***Aug 4 2024***

1. ***Lean AI and Small Language Models: <br>
   The article discusses the shift towards lean AI, which aims to optimize efficiency and minimize resource consumption. This shift is driven by the high costs and resource demands of large language models (LLMs). Small language models (SLMs) are becoming more popular due to their lower operational costs, faster deployment cycles, and specialized applications. Open-source initiatives are making advanced AI more accessible and affordable for organizations.***
   
   Aug 2, [InfoWorld](https://www.infoworld.com/article/3480593/small-language-models-and-open-source-are-transforming-ai.html) published an article Small language models (SLM) and open source are transforming AI. The shift towards lean AI emphasizes optimizing efficiency and minimizing resource consumption, addressing the high costs and resource demands of large language models (LLMs). Enterprises are increasingly adopting SLMs for their lower operational costs, faster deployment cycles, and ability to deliver specialized applications. Open-source initiatives and tools are democratizing AI capabilities, enabling more organizations to incorporate advanced AI without relying on expensive proprietary solutions. <br><br>

3. ***Meta's SAM 2 for Visual Segmentation: <br>
   Meta introduced the Segment Anything Model 2 (SAM 2) for visual segmentation in images and videos. SAM 2 uses a transformer architecture with streaming memory for real-time video processing and has the largest video segmentation dataset. It provides better accuracy with fewer interactions and is significantly faster and more accurate than its predecessor, SAM. The model, dataset, and an interactive demo are being released.***
   
   Aug 1, Meta published a [paper](https://arxiv.org/pdf/2408.00714) “SAM 2: Segment Anything in Images and Videos”. The research presents Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. The  authors build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. The model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on the data provides strong performance across a wide range of tasks. In video segmentation, it is observed better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, the model is more accurate and 6x faster than the Segment Anything Model (SAM). The authors believe that the data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. The authors are releasing a version of [the model](https://github.com/facebookresearch/segment-anything-2), the dataset and an interactive demo. <br><br>

5. ***Microsoft's OmniParser for GUI Agents: <br>
   Microsoft presented OmniParser, a method for parsing user interface screenshots into structured elements. OmniParser enhances GPT-4V's ability to generate actions accurately grounded in the corresponding regions of the interface. By using curated datasets for icon detection and description, OmniParser significantly improves performance on benchmarks and outperforms existing models that require additional information.***
   Aug 1, Microsoft published a [paper](https://arxiv.org/pdf/2408.00203) “OmniParser for Pure Vision Based GUI Agent”. The recent success of large vision language models shows great potential in driving the agent system operating on user interfaces. However, the power multimodal models like GPT-4V as a general agent on multiple operating systems across different applications is largely underestimated due to the lack of a robust screen parsing technique capable of: 1) reliably identifying interactable icons within the user interface, and 2) understanding the semantics of various elements in a screenshot and accurately associate the intended action with the corresponding region on the screen. To fill these gaps, the paper introduces OmniParser, a comprehensive method for parsing user interface screenshots into structured elements, which significantly enhances the ability of GPT-4V to generate actions that can be accurately grounded in the corresponding regions of the interface. The authors first curated an interactable icon detection dataset using popular webpages and an icon description dataset. These datasets were utilized to fine-tune specialized models: a detection model to parse interactable regions on the screen and a caption model to extract the functional semantics of the detected elements. OmniParser significantly improves GPT-4V's performance on ScreenSpot benchmark. And on Mind2Web and AITW benchmark, OmniParser with screenshot only input outperforms the GPT-4V baselines requiring additional information outside of screenshot. <br><br>

8. ***Scaling Inference Compute with Repeated Sampling: <br>
   Researchers from Stanford, Oxford, and Google explored scaling inference compute by increasing the number of generated samples. They found that coverage, or the fraction of problems solved, scales with the number of samples. Repeated sampling significantly improves performance in domains with verifiable answers, and it is cost-effective. However, identifying correct samples in domains without automatic verifiers remains a challenge.***
   
   Jul 31, Stanford Uni, Uni of Oxford and Google published a [paper](https://arxiv.org/pdf/2407.21787) “Large Language Monkeys: Scaling Inference Compute with Repeated Sampling”. Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. This paper explores inference compute as another axis for scaling by increasing the number of generated samples. Across multiple tasks and models, the authors observe that coverage - the fraction of problems solved by any attempt - scales with the number of samples over four orders of magnitude. In domains like coding and formal proofs, where all answers can be automatically verified, these increases in coverage directly translate into improved performance. When applying repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-attempt state-of-the-art of 43% which uses more capable frontier models. Moreover, using current API pricing, amplifying the cheaper DeepSeek model with five samples is more cost-effective and solves more issues than paying a premium for one sample from GPT-4o or Claude 3.5 Sonnet. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. Finally, the work finds that identifying correct samples out of many generations remains an important direction for future research in domains without automatic verifiers. When solving math word problems from GSM8K and MATH, coverage with Llama-3 models grows to over 95% with 10,000 samples. However, common methods to pick correct solutions from a sample collection, such as majority voting or reward models, plateau beyond several hundred samples and fail to fully scale with the sample budget. <br><br>

9. ***Australia's Privacy Concerns with Social Media Platform X: <br>
    Australia's privacy watchdog is concerned that social media platform X (formerly Twitter) may be breaching privacy laws by automatically opting users into having their posts used to train AI systems. Platforms are required to ensure default settings enable user control and seek consent for data use. The watchdog is investigating practices across the industry as other major platforms also harvest user data to train AI.***
   
    Jul 31, according to [abc.new.au](https://www.abc.net.au/news/science/2024-07-31/elon-musk-x-breach-privacy-law-data-harvest-grok-ai/104054400), Australia's privacy watchdog says social media platform X (formerly Twitter) may be in breach of Australian privacy law after it emerged users were automatically opted in to having their posts used to build artificial intelligence (AI) systems. The Office of the Australian Information Commissioner stopped short of saying it would launch an inquiry into the platform's data collection, similar to the one it is currently undertaking in relation to TikTok. On Friday, an X user pointed out the X app privacy settings includes a pre-ticked box that permits X to use the account holder's posts to train the Grok AI chatbot built by Elon Musk's company xAI. The default setting states that you "allow your posts as well as your interactions, inputs and results with Grok to be used for training and fine-tuning". Under Australian law, platforms are required to ensure default settings enable user control, and to either seek an individuals' consent for how the platform will use the data, or be satisfied the user would reasonably expect the organisation to use their data for this purpose. In recent months, it also emerged other platforms, such as Meta and Slack, were harvesting user data to train AI as part of a global race to build bigger and better large language models (LLMs). Last month, it was revealed xAI was trying to build the world's largest supercomputer in the US city of Memphis to fuel its AI ambitions. The Commissioner's Office says it's "looking at such practices across the industry" as other major platforms, also competing to build their own AIs, harvest user data. <br><br>

11. ***Safetywashing in AI Safety Benchmarks: <br>
    A study by various universities analyzed AI safety benchmarks and found many are highly correlated with general capabilities, leading to "safetywashing." The study calls for more meaningful safety metrics that are empirically separable from generic capabilities. The authors propose a rigorous framework for AI safety research to advance the science of safety evaluations and clarify measurable progress.***
    
    Jul 31, Center of AI Safety, Uni of Penn., UC Berkeley, Stanford Uni, Yale Uni and Keio Uni published a [paper](https://arxiv.org/pdf/2407.21792) “Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?”. As artificial intelligence systems grow more powerful, there has been increasing interest in "AI safety" research to address emerging and future risks. However, the field of AI safety remains poorly defined and inconsistently measured, leading to confusion about how researchers can contribute. This lack of clarity is compounded by the unclear relationship between AI safety benchmarks and upstream general capabilities (e.g., general knowledge and reasoning). To address these issues, the study conducts a comprehensive meta-analysis of AI safety benchmarks, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety. The findings reveal that many safety benchmarks highly correlate with upstream model capabilities, potentially enabling "safetywashing" -- where capability improvements are misrepresented as safety advancements. Based on these findings, the authors propose an empirical foundation for developing more meaningful safety metrics and define AI safety in a machine learning research context as a set of clearly delineated research goals that are empirically separable from generic capabilities advancements. In doing so, the authors aim to provide a more rigorous framework for AI safety research, advancing the science of safety evaluations and clarifying the path towards measurable progress. <br><br>

13. ***Google's Gemma 2 AI Models: <br>
    Google introduced the Gemma 2 family, including Gemma 2 2B, ShieldGemma, and Gemma Scope. Gemma 2 2B is a lightweight model with superior performance and efficiency. ShieldGemma offers safety content classifier models for various tasks, while Gemma Scope provides insights into model operations. These additions enhance AI capabilities, safety, and innovation.***
    
    Jul 31, Google released [Gemma 2 family](https://developers.googleblog.com/en/smaller-safer-more-transparent-advancing-responsible-ai-with-gemma/#:~:text=A%20Future%20Built%20on%20Responsible,developing%20safe%20and%20beneficial%20AI.) members Gemma 2 2B, SheldGemma and Gemma Scope. [Gemma 2 2B](https://huggingface.co/collections/google/gemma-2-2b-release-66a20f3796a2ff2a7c76f98f) – a brand-new version of the popular 2 billion (2B) parameter model, featuring built-in safety advancements and a powerful balance of performance and efficiency. This lightweight model produces outsized results by learning from larger models through distillation. In fact, Gemma 2 2B surpasses all GPT-3.5 models on the Chatbot Arena, demonstrating its exceptional conversational AI abilities. ShieldGemma – a suite of safety content classifier models, built upon Gemma 2, to filter the input and outputs of AI models and keep the user safe. [ShieldGemma](https://huggingface.co/google/shieldgemma-2b (/9b/27b)) offers various model sizes to meet diverse needs. The 2B model is ideal for online classification tasks, while the 9B and 27B versions provide higher performance for offline applications where latency is less of a concern. [Gemma Scope](https://huggingface.co/google/gemma-scope) – a new model interpretability tool that offers unparalleled insight into our models' inner workings. With these additions, researchers and developers can now create safer customer experiences, gain unprecedented insights into the models, and confidently deploy powerful AI responsibly, right on device, unlocking new possibilities for innovation. <br><br>

15. ***ShieldGemma for Content Moderation: <br>
    Google presented ShieldGemma, a suite of LLM-based safety content moderation models built on Gemma2. These models offer robust predictions of safety risks and outperform existing models on benchmarks. The paper introduces a novel data curation pipeline and demonstrates strong generalization performance. ShieldGemma advances LLM safety and content moderation solutions.***
    
    Jul 31, Google published a [paper](https://arxiv.org/pdf/2407.21772) “ShieldGemma: Generative AI Content Moderation Based on Gemma”. The paper presents ShieldGemma, a comprehensive suite of LLM-based safety content moderation models built upon Gemma2. These models provide robust, state-of-the-art predictions of safety risks across key harm types (sexually explicit, dangerous content, harassment, hate speech) in both user input and LLM-generated output. By evaluating on both public and internal benchmarks, the work demonstrates superior performance compared to existing models, such as Llama Guard (+10.8\% AU-PRC on public benchmarks) and WildCard (+4.3\%). Additionally, the paper presents a novel LLM-based data curation pipeline, adaptable to a variety of safety-related tasks and beyond. The authors have shown strong generalization performance for model trained mainly on synthetic data. By releasing ShieldGemma, the paper provides a valuable resource to the research community, advancing LLM safety and enabling the creation of more effective content moderation solutions for developers. Models are [available here](https://huggingface.co/google/shieldgemma-2b (/9b/27b)). <br><br>

17. ***Meta's Self-Improving Language Models: <br>
    Meta, UC Berkeley, and NYU introduced a Meta-Rewarding step for self-improving language models. This method improves models' judgment skills by having them judge their own responses. The approach enhances both judgment and instruction-following abilities without human supervision, showing significant performance improvements on benchmarks.***
    
    Jul 30, Meta, UC Berkeley, and NYU published a [paper](https://arxiv.org/pdf/2407.19594) “Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge”. Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs can improve by judging their own responses instead of relying on human labelers. However, existing methods have primarily focused on improving model responses rather than judgment capabilities, resulting in rapid saturation during iterative training. To address this issue, the paper introduces a novel Meta-Rewarding step to the self-improvement process, where the model judges its own judgements and uses that feedback to refine its judgment skills. Surprisingly, this unsupervised approach improves the model's ability to judge {\em and} follow instructions, as demonstrated by a win rate improvement of Llama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on Arena-Hard. These results strongly suggest the potential for self-improving models without human supervision. <br><br>

19. ***Google's MoNE for Efficient Visual Processing: <br>
    Google and the University of Washington presented the Mixture of Nested Experts (MoNE) model, which uses a nested structure for experts to process visual tokens efficiently. MoNE reduces inference time compute while maintaining performance, making it adaptable to different compute budgets. The approach is validated on standard image and video datasets.***
    
    Jul 30, Google and Uni of Washington published a [paper](https://arxiv.org/pdf/2407.19985) “Mixture of Nested Experts: Adaptive Processing of Visual Tokens”. The visual medium (images and videos) naturally contains a large amount of information redundancy, thereby providing a great opportunity for leveraging efficiency in processing. While Vision Transformer (ViT) based models scale effectively to large data regimes, they fail to capitalize on this inherent redundancy, leading to higher computational costs. Mixture of Experts (MoE) networks demonstrate scalability while maintaining same inference-time costs, but they come with a larger parameter footprint. The paper presents Mixture of Nested Experts (MoNE), which utilizes a nested structure for experts, wherein individual experts fall on an increasing compute-accuracy curve. Given a compute budget, MoNE learns to dynamically choose tokens in a priority order, and thus redundant tokens are processed through cheaper nested experts. Using this framework, MoNE achieves equivalent performance as the baseline models, while reducing inference time compute by over two-fold. The authors validate the approach on standard image and video datasets - ImageNet-21K, Kinetics400, and Something-Something-v2. The authors further highlight MoNE's adaptability by showcasing its ability to maintain strong performance across different inference-time compute budgets on videos, using only a single trained model. <br><br>

21. ***Diffusion Augmented Agents for RL: <br>
    Google introduced Diffusion Augmented Agents (DAAG), a framework leveraging language models, vision language models, and diffusion models to improve sample efficiency and transfer learning in reinforcement learning. DAAG enhances learning by transforming past experiences to align with target instructions, reducing the need for reward-labeled data and improving lifelong learning capabilities.***
    
    Jul 30, Google published a [paper](https://arxiv.org/pdf/2407.20798) “Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning”. The paper introduces Diffusion Augmented Agents (DAAG), a novel framework that leverages large language models, vision language models, and diffusion models to improve sample efficiency and transfer learning in reinforcement learning for embodied agents. DAAG hindsight relabels the agent's past experience by using diffusion models to transform videos in a temporally and geometrically consistent way to align with target instructions with a technique called as Hindsight Experience Augmentation. A large language model orchestrates this autonomous process without requiring human supervision, making it well-suited for lifelong learning scenarios. The framework reduces the amount of reward-labeled data needed to 1) finetune a vision language model that acts as a reward detector, and 2) train RL agents on new tasks. The paper demonstrates the sample efficiency gains of DAAG in simulated robotics environments involving manipulation and navigation. The results show that DAAG improves learning of reward detectors, transferring past experience, and acquiring new tasks - key abilities for developing efficient lifelong learning agents. Supplementary material and visualizations are available on the website [this https URL](https://sites.google.com/view/diffusion-augmented-agents/) <br><br>

23. ***Meta's SAM 2 for Video and Image Segmentation: <br>
    Meta's Segment Anything Model 2 (SAM 2) is designed for promptable visual segmentation in images and videos. It uses a transformer architecture with streaming memory for real-time processing and has the largest video segmentation dataset. SAM 2 provides better accuracy and speed compared to its predecessor, SAM, and is being released with a dataset and demo.***
    
    Jul 29, Meta published a [paper](https://scontent.fcbr1-1.fna.fbcdn.net/v/t39.2365-6/453323338_287900751050452_6064535069828837026_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=iEdf_eLLDBIQ7kNvgHvSueV&_nc_ht=scontent.fcbr1-1.fna&gid=AbMZVotuhlaDUcgiZQmipJh&oh=00_AYCMj0UJk4ZJNT-OM4nxXp6vgeWLO9SHo56ZlmA1qZHoaQ&oe=66B0FCB9) “SAM 2: Segment Anything in Images and Videos”.  The paper presents Segment Anything Model 2 (SAM 2 ), a foundation model towards solving promptable visual segmentation in images and videos. The authors build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. The model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on the data provides strong performance across a wide range of tasks. In video segmentation, it’s observed better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, the model is more accurate and 6x faster than the Segment Anything Model (SAM). The authors believe that the data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. The authors are releasing [a version of the model](https://ai.meta.com/sam2/), the dataset and an interactive demo. <br><br>

25. ***SaulLM Models for Legal Domain: <br>
    MICS and CINES introduced SaulLM-54B and SaulLM-141B, large language models tailored for the legal sector. These models are based on the Mixtral architecture and use domain adaptation strategies for legal tasks. They outperform previous models on LegalBench-Instruct and are released under the MIT License to facilitate reuse and research.***
    
    Jul 28, MICS and CINES published a [paper](https://arxiv.org/pdf/2407.19584) “SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain”. The paper introduces SaulLM-54B and SaulLM-141B, two large language models (LLMs) tailored for the legal sector. These models, which feature architectures of 54 billion and 141 billion parameters, respectively, are based on the Mixtral architecture. The development of SaulLM-54B and SaulLM-141B is guided by large-scale domain adaptation, divided into three strategies: (1) the exploitation of continued pretraining involving a base corpus that includes over 540 billion of legal tokens, (2) the implementation of a specialized legal instruction-following protocol, and (3) the alignment of model outputs with human preferences in legal interpretations. The integration of synthetically generated data in the second and third steps enhances the models' capabilities in interpreting and processing legal texts, effectively reaching state-of-the-art performance and outperforming previous open-source models on LegalBench-Instruct. This work explores the trade-offs involved in domain-specific adaptation at this scale, offering insights that may inform future studies on domain adaptation using strong decoder models. Building upon SaulLM-7B, this study refines the approach to produce an LLM better equipped for legal tasks. The authors are releasing base, instruct, and aligned versions on top of SaulLM-54B and SaulLM-141B under the MIT License to facilitate reuse and collaborative research. <br><br>

27. ***Amazon's REAPER for RAG Systems: <br>
    Amazon presented REAPER, a reasoning-based retrieval planner for complex RAG (Retrieval Augmented Generation) systems. REAPER generates retrieval plans for conversational systems, significantly reducing latency and scaling easily to new use cases. The method is shown to improve performance in a conversational shopping assistant context.***
    
    Jul 26, Amazon published a [paper](https://arxiv.org/abs/2407.18553) “REAPER: Reasoning based Retrieval Planning for Complex RAG Systems”. Complex dialog systems often use retrieved evidence to facilitate factual responses. Such RAG (Retrieval Augmented Generation) systems retrieve from massive heterogeneous data stores that are usually architected as multiple indexes or APIs instead of a single monolithic source. For a given query, relevant evidence needs to be retrieved from one or a small subset of possible retrieval sources. Complex queries can even require multi-step retrieval. For example, a conversational agent on a retail site answering customer questions about past orders will need to retrieve the appropriate customer order first and then the evidence relevant to the customer's question in the context of the ordered product. Most RAG Agents handle such Chain-of-Thought (CoT) tasks by interleaving reasoning and retrieval steps. However, each reasoning step directly adds to the latency of the system. For large models this latency cost is significant -- in the order of multiple seconds. Multi-agent systems may classify the query to a single Agent associated with a retrieval source, though this means that a (small) classification model dictates the performance of a large language model. This paper presents REAPER (REAsoning-based PlannER) - an LLM based planner to generate retrieval plans in conversational systems. The paper shows significant gains in latency over Agent-based systems and are able to scale easily to new and unseen use cases as compared to classification-based planning. Though the method can be applied to any RAG system, the authors show the results in the context of a conversational shopping assistant. <br><br>

29. ***Apple's MMAU Benchmark for LLM Agents: <br>
    Apple introduced the Massive Multitask Agent Understanding (MMAU) benchmark, evaluating models across five domains and capabilities. MMAU provides a comprehensive framework for assessing LLM agents' strengths and limitations with detailed analyses of 18 models. The benchmark enhances interpretability and understanding of model performance.***
    
    Jul 18, Apple published a [paper](https://arxiv.org/pdf/2407.18961) “MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains”. Recent advances in large language models (LLMs) have increased the demand for comprehensive benchmarks to evaluate their capabilities as human-like agents. Existing benchmarks, while useful, often focus on specific application scenarios, emphasizing task completion but failing to dissect the underlying skills that drive these outcomes. This lack of granularity makes it difficult to deeply discern where failures stem from. Additionally, setting up these environments requires considerable effort, and issues of unreliability and reproducibility sometimes arise, especially in interactive tasks. To address these limitations, the authors introduce the Massive Multitask Agent Understanding (MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need for complex environment setups. It evaluates models across five domains, including Tool-use, Directed Acyclic Graph (DAG) QA, Data Science and Machine Learning coding, Contest-level programming and Mathematics, and covers five essential capabilities: Understanding, Reasoning, Planning, Problem-solving, and Self-correction. With a total of 20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU provides a comprehensive framework for evaluating the strengths and limitations of LLM agents. By testing 18 representative models on MMAU, the paper provides deep and insightful analyses. Ultimately, MMAU not only sheds light on the capabilities and limitations of LLM agents but also enhances the interpretability of their performance. Datasets and evaluation scripts of MMAU are released at https://github.com/apple/axlearn/blob/main/docs/research/mmau.
 <br><br><br>

***28 Jul 2024***

1. ***Google’s AI achieves silver-medal standard solving International Mathematical Olympiad problems <br>
Google's AlphaProof and AlphaGeometry teams developed new AI systems capable of solving complex math problems. The systems, AlphaProof for algebra and number theory, and AlphaGeometry 2 for geometry, achieved a silver-medal standard at the 2023 International Mathematical Olympiad, solving four out of six problems. AlphaProof handled algebra and number theory, including the competition's hardest problem, while AlphaGeometry 2 solved the geometry problem. The systems scored 28 points out of a possible 42, just below the gold-medal threshold of 29 points, showcasing advanced mathematical reasoning capabilities.*** <br>
   Jul 25, Google’s AlphaProof  and AlphaGeometry teams published a [blog](https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/) “AI achieves silver-medal standard solving International Mathematical Olympiad problems”. Artificial general intelligence (AGI) with advanced mathematical reasoning has the potential to unlock new frontiers in science and technology. AlphaProof is a new reinforcement-learning based system for formal math reasoning, and AlphaGeometry 2, is an improved version of our geometry-solving system. Together, these systems solved four out of six problems from this year’s International Mathematical Olympiad (IMO), achieving the same level as a silver medalist in the competition for the first time. The IMO is the oldest, largest and most prestigious competition for young mathematicians, held annually since 1959. More recently, the annual IMO competition has also become widely recognised as a grand challenge in machine learning and an aspirational benchmark for measuring an AI system’s advanced mathematical reasoning capabilities. AlphaProof solved two algebra problems and one number theory problem by determining the answer and proving it was correct. This included the hardest problem in the competition, solved by only five contestants at this year’s IMO. AlphaGeometry 2 proved the geometry problem, while the two combinatorics problems remained unsolved. Each of the six problems can earn seven points, with a total maximum of 42. [Google’s system achieved a final score of 28 points](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/imo-2024-solutions/index.html), earning a perfect score on each problem solved — equivalent to the top end of the silver-medal category. This year, the gold-medal threshold starts at 29 points, and was achieved by 58 of 609 contestants at the official competition. <br><br>

3. ***AI models collapse when trained on recursively generated data <br>
Researchers from several universities published findings on the risks of training AI models on data generated by other AI models. They identified a phenomenon called 'model collapse,' where the quality of models degrades due to recursive training on AI-generated content, leading to the loss of rare content features. This study emphasizes the need for human-generated data to maintain the integrity and performance of AI models over time.*** <br>
   Jul 25, Uni of Oxford, Uni of Cambridge,  Imperial College London, Uni of Toronto, Uni of Edinburgh published a [paper](https://www.nature.com/articles/s41586-024-07566-y.pdf) on Nature “AI models collapse when trained on recursively generated data”. Stable diffusion revolutionized image creation from descriptive text. GPT-2), GPT-3(.5) and GPT-4 demonstrated high performance across a variety of language tasks. ChatGPT introduced such language models to the public. It is now clear that generative artificial intelligence (AI) such as large language models (LLMs) is here to stay and will substantially change the ecosystem of online text and images. Here the authors consider what may happen to GPT-{n} once LLMs contribute much of the text found online. The paper finds that indiscriminate use of model-generated content in training causes irreversible defects in the resulting models, in which tails of the original content distribution disappear. The authors refer to this effect as ‘model collapse’ and show that it can occur in LLMs as well as in variational autoencoders (VAEs) and Gaussian mixture models (GMMs). The study builds theoretical intuition behind the phenomenon and portray its ubiquity among all learned generative models. The work demonstrates that it must be taken seriously if people are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of LLM-generated content in data crawled from the Internet. <br><br>

5. ***OpenAI's SearchGPT prototype <br>
OpenAI launched SearchGPT, a prototype integrating web information with AI model responses to provide users with fast, sourced answers. Currently in testing with a small group, SearchGPT aims to enhance ChatGPT by incorporating real-time web data. The system allows users to ask follow-up questions and aims to improve search interactions. While separate from AI training, SearchGPT ensures that publishers can manage their content's appearance in search results.*** <br>
   Jul 25, [OpenAI released](https://openai.com/index/searchgpt-prototype/) it SearchGPT, a prototype of new search features designed to combine the strength of OpenAI’s AI models with information from the web to give users fast and timely answers with clear and relevant sources. OpenAI is launching to a small group of users and publishers to get feedback. While this prototype is temporary, it plans to integrate the best of these features directly into ChatGPT in the future. SearchGPT will quickly and directly respond to users questions with up-to-date information from the web while giving clear links to relevant sources. Users will be able to ask follow-up questions, like one would in a conversation with a person, with the shared context building with each query. OpenAI has partnered with publishers to build this experience and continue to seek their feedback. In addition to launching the SearchGPT prototype, OpenAI is also launching a way for publishers to manage how they appear in SearchGPT, so publishers have more choices. Importantly, SearchGPT is about search and is separate from training OpenAI’s generative AI foundation models. Sites can be surfaced in search results even if they opt out of generative AI training. However, it is found that the answers of SearchGPT are mostly incorrect. <br><br>

7. ***Data Provenance Initiative's audit on AI data consent protocols <br>
The Data Provenance Initiative highlighted a growing crisis in data consent for AI training. Their audit of 14,000 web domains showed increasing restrictions on the use of web data for AI, with significant portions of commonly used datasets becoming inaccessible. This trend threatens the diversity and scalability of AI systems, prompting a call for more effective data consent protocols.*** <br>
   Jul 24, Data Provenance Initiative (a collective of independent and academic researchers) published a [paper](https://arxiv.org/pdf/2407.14933) “Consent in Crisis: The Rapid Decline of the AI Data Commons”. General-purpose artificial intelligence (AI) systems are built on massive swathes of public web data, assembled into corpora such as C4, RefinedWeb, and Dolma. To the authors’ knowledge, they conduct the first, large-scale, longitudinal audit of the consent protocols for the web domains underlying AI training corpora. The audit of 14,000 web domains provides an expansive view of crawlable web data and how consent preferences to use it are changing over time. The authors observe a proliferation of AI-specific clauses to limit use, acute differences in restrictions on AI developers, as well as general inconsistencies between websites' expressed intentions in their Terms of Service and their robots.txt. The paper diagnoses these as symptoms of ineffective web protocols, not designed to cope with the widespread re-purposing of the internet for AI. The longitudinal analyses show that in a single year (2023-2024) there has been a rapid crescendo of data restrictions from web sources, rendering ~5%+ of all tokens in C4, or 28%+ of the most actively maintained, critical sources in C4, fully restricted from use. For Terms of Service crawling restrictions, a full 45% of C4 is now restricted. If respected or enforced, these restrictions are rapidly biasing the diversity, freshness, and scaling laws for general-purpose AI systems. The authors hope to illustrate the emerging crisis in data consent, foreclosing much of the open web, not only for commercial AI, but non-commercial AI and academic purposes. <br><br>

9. ***Mistral releases Mistral Large 2 AI model <br>
Mistral unveiled Mistral Large 2, an advanced AI model with a 128k context window and support for multiple languages and coding languages. The model excels in single-node inference and boasts high accuracy on various benchmarks, including coding and reasoning. It sets a new standard for performance and cost efficiency in AI models, offering robust performance on par with leading models like GPT-4.*** <br>
    Jul 24, [Mistral released Mistral Large English](https://mistral.ai/news/mistral-large-2407/), the latest generation continues to push the boundaries of cost efficiency, speed, and performance. Mistral Large 2 is exposed on la Plateforme and enriched with new features to facilitate building innovative AI applications. Mistral Large 2 has a 128k context window and supports dozens of languages including French, German, Chinese, etc, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash. Mistral Large 2 is designed for single-node inference with long-context applications in mind – its size of 123 billion parameters allows it to run at large throughput on a single node. Mistral Research License allows usage and modification for research and non-commercial usages. Mistral Large 2 sets a new frontier in terms of performance / cost of serving on evaluation metrics. In particular, on MMLU, the pretrained version achieves an accuracy of 84.0%, and sets a new point on the performance/cost Pareto front of open models. Mistral Large 2 vastly outperforms the previous Mistral Large on coding and reasoning, and performs on par with leading models such as GPT-4o, Claude 3 Opus, and Llama 3 405B. Additionally, the new Mistral Large 2 is trained to acknowledge when it cannot find solutions or does not have sufficient information to provide a confident answer. This commitment to accuracy is reflected in the improved model performance on popular mathematical benchmarks, demonstrating its enhanced reasoning and problem-solving skills. The new Mistral Large 2 is particularly better at following precise instructions and handling long multi-turn conversations. <br><br>

11. ***Data mixture inference study using BPE tokenizers <br>
Researchers developed a method to infer the data mixture in language model training sets using byte-pair encoding (BPE) tokenizers. By analyzing token frequency patterns, they accurately estimated the proportions of various data types in training sets. This method revealed new insights into the multilingual and code-focused nature of recent models like GPT-4o and Llama3, contributing to better understanding and transparency in AI model training.*** <br>
    Jul 24, Uni of Washington and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2407.16607) “Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?”. The pretraining data of today's strongest language models is opaque. In particular, little is known about the proportions of various domains or languages represented. This work tackles a task which is called data mixture inference, which aims to uncover the distributional make-up of training data. The work introduces a novel attack based on a previously overlooked source of information -- byte-pair encoding (BPE) tokenizers, used by the vast majority of modern language models. The key insight is that the ordered list of merge rules learned by a BPE tokenizer naturally reveals information about the token frequencies in its training data: the first merge is the most common byte pair, the second is the most common pair after merging the first token, and so on. Given a tokenizer's merge list along with data samples for each category of interest, the authors formulate a linear program that solves for the proportion of each category in the tokenizer's training set. Importantly, to the extent to which tokenizer training data is representative of the pretraining data, the work indirectly learns about the pretraining data. In controlled experiments, the paper shows that the attack recovers mixture ratios with high precision for tokenizers trained on known mixtures of natural languages, programming languages, and data sources. The paper then applies the approach to off-the-shelf tokenizers released with recent LMs. The authors confirm much publicly disclosed information about these models, and also make several new inferences: GPT-4o's tokenizer is much more multilingual than its predecessors, training on 39% non-English data; Llama3 extends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and Claude's tokenizers are trained on predominantly code (~60%). The work sheds light on current design practices for pretraining data, and inspires continued research into data mixture inference for LMs. <br><br>

13. ***Elon Musk's xAI Memphis Supercluster <br>
Elon Musk announced the creation of the Memphis Supercluster, a powerful AI training cluster using 100,000 Nvidia H100 GPUs. This initiative aims to develop the world's most powerful AI, Grok 3, by December 2023. The supercluster's scale surpasses current top supercomputers, highlighting Musk's ambitious plans for AI advancements.*** <br>
    Jul 23, according to [tomshardware.com](https://www.tomshardware.com/pc-components/gpus/elon-musk-fires-up-the-most-powerful-ai-training-cluster-in-the-world-uses-100000-nvidia-h100-gpus-on-a-single-fabric), Tech baron Elon Musk has taken to Twitter/X to boast of starting up “the most powerful AI training cluster in the world,” which he will use to create the self-professed "world’s most powerful AI by every metric by December of this year.” Today, xAI’s Memphis Supercluster began AI training using 100,000 liquid-cooled Nvidia H100 GPUs connected with a single RDMA (remote direct memory access) fabric. In a follow-up Tweet, Musk explains that the new supercluster will be “training the world’s most powerful AI by every metric.” From previous statements of intent, it is assumed that the power of xAI’s 100,000 H100 GPU installation will now be targeted at Grok 3 training. Musk said the refined LLM should be finished with the training stage “by December this year.” To put the Memphis Supercluster compute resources in some context, certainly, going by scale, the new xAI Memphis Supercluster easily outclasses anything in the most recent Top500 list in terms of GPU horsepower. The world’s most powerful supercomputers such as Frontier (37,888 AMD GPUs), Aurora (60,000 Intel GPUs), and Microsoft Eagle (14,400 Nvidia H100 GPUs) seem to be significantly outgunned by the xAI machine. <br><br>

15. ***Meta's release of Llama 3.1 <br>
Meta introduced Llama 3, a new set of foundation models supporting multilinguality, coding, reasoning, and tool usage. The largest model, with 405 billion parameters, rivals leading AI models like GPT-4. Llama 3 integrates capabilities across image, video, and speech tasks and includes safety features. The release aims to push the boundaries of AI performance and versatility.*** <br>
    Jul 23, Meta released llama 3.1 and the corresponding [paper](https://scontent.fcbr1-1.fna.fbcdn.net/v/t39.2365-6/452387774_1036916434819166_4173978747091533306_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=7qSoXLG5aAYQ7kNvgHS5YSv&_nc_ht=scontent.fcbr1-1.fna&oh=00_AYDrMk-4WCS9Y2Sa1syLLvRW05gfxbKWvz1mJleRSHbpBg&oe=66AA6F8D) “The Llama 3 Herd of Models”. Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. The largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. The authors find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. The authors publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and the Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which integrates image, video, and speech capabilities into Llama 3 via a compositional approach. The authors observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development. <br><br>

17. ***Switzerland mandates open-source software for government <br>
Switzerland passed a law requiring all government software to be open-source, promoting transparency, security, and efficiency. This legislation, part of a broader European trend, mandates public disclosure of software code and the release of non-sensitive government data as Open Government Data (OGD). The move aims to foster greater openness and practical reuse of software and data in the public sector.*** <br>
    Jul 23, [according to zdnet.com](https://www.zdnet.com/article/switzerland-now-requires-all-government-software-to-be-open-source/), Switzerland now requires all government software to be open source. Several European countries are betting on open-source software. In the United States, eh, not so much. In the latest news from across the Atlantic, Switzerland has taken a major step forward with its "Federal Law on the Use of Electronic Means for the Fulfillment of Government Tasks" (EMBAG). This groundbreaking legislation mandates using open-source software (OSS) in the public sector. This new law requires all public bodies to disclose the source code of software developed by or for them unless third-party rights or security concerns prevent it. This "public money, public code" approach aims to enhance government operations' transparency, security, and efficiency. In addition to mandating OSS, the EMBAG also requires the release of non-personal and non-security-sensitive government data as Open Government Data (OGD). This dual "open by default" approach marks a significant paradigm shift towards greater openness and practical reuse of software and data. Other countries in Europe have long supported open source. For example, in 2023, French President Macron stated, "We love open source," and France's National Gendarmerie (Think FBI if you're an American) uses Linux on its PCs. The European Union (EU) has long worked on securing OSS via the EU's Free and Open Source Software Auditing (FOSSA) project. <br><br>

19. ***OpenDevin platform for AI software developers <br>
A new platform, OpenDevin, was introduced to facilitate the development of AI agents that interact with their environment similarly to human developers. OpenDevin supports code writing, command line interaction, and web browsing, allowing for flexible and powerful AI agent development. The platform, open-sourced and community-driven, aims to enhance AI capabilities across various challenging tasks.*** <br>
    Jul 23, UIUC, CMU, Yale, UC Berkeley, ContextualAI, et al. published a [paper](https://arxiv.org/pdf/2407.16741) “OpenDevin: An Open Platform for AI Software Developers as Generalist Agents”. Software is one of the most powerful tools that humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. This paper introduces OpenDevin, a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to those of a human developer: by writing code, interacting with a command line, and browsing the web. The authors describe how the platform allows for the implementation of new agents, safe interaction with sandboxed environments for code execution, coordination between multiple agents, and incorporation of evaluation benchmarks. Based on the currently incorporated benchmarks, the authors perform an evaluation of agents over 15 challenging tasks, including software engineering (e.g., SWE-Bench) and web browsing (e.g., WebArena), among others. Released under the permissive MIT license, [OpenDevin](https://github.com/OpenDevin/OpenDevin) is a community project spanning academia and industry with more than 1.3K contributions from over 160 contributors and will improve going forward. <br><br>

21. ***Comparison of KAN and MLP models <br>
Researchers conducted a comprehensive comparison of KAN and MLP models, finding that MLP generally outperforms KAN across various tasks, except for symbolic formula representation. The study also identified the B-spline activation function as a key factor in KAN's performance in symbolic tasks. These findings offer insights for future research on model alternatives and improvements.*** <br>
    Jul 23, Nation Uni of Singapore published a [paper](https://arxiv.org/pdf/2407.16674) “KAN or MLP: A Fairer Comparison”. This paper does not introduce a novel method. Instead, it offers a fairer and more comprehensive comparison of KAN and MLP models across various tasks, including machine learning, computer vision, audio processing, natural language processing, and symbolic formula representation. Specifically, the authors control the number of parameters and FLOPs to compare the performance of KAN and MLP. The main observation is that, except for symbolic formula representation tasks, MLP generally outperforms KAN. The authors also conduct ablation studies on KAN and find that its advantage in symbolic formula representation mainly stems from its B-spline activation function. When B-spline is applied to MLP, performance in symbolic formula representation significantly improves, surpassing or matching that of KAN. However, in other tasks where MLP already excels over KAN, B-spline does not substantially enhance MLP's performance. Furthermore, the study finds that KAN's forgetting issue is more severe than that of MLP in a standard class-incremental continual learning setting, which differs from the findings reported in the KAN paper. The authors hope these results provide insights for future research on KAN and other MLP alternatives. Project link: https://github.com/yu-rp/KANbeFair <br><br>

23. ***Specialization for legal tasks using Llama 3 <br>
A study demonstrated that fine-tuned Llama 3 models significantly outperform GPT-4 on legal text classification tasks. The research showed that even light fine-tuning on specific tasks could achieve high accuracy, suggesting a viable alternative to using commercial models for legal research. This approach could reduce reliance on costly human annotation.*** <br>
    Jul 23, Tubingen AI Center, Harvard Uni, ETH Zurich, Washington Uni and Uni of Virginia published a [paper](https://arxiv.org/pdf/2407.16615) “Lawma: The Power of Specialization for Legal Tasks”. Annotation and classification of legal text are central components of empirical legal research. Traditionally, these tasks are often delegated to trained research assistants. Motivated by the advances in language modeling, empirical legal scholars are increasingly turning to prompting commercial models, hoping that it will alleviate the significant cost of human annotation. Despite growing use, the understanding of how to best utilize large language models for legal tasks remains limited. The work conducts a comprehensive study of 260 legal text classification tasks, nearly all new to the machine learning community. Starting from GPT-4 as a baseline, the paper shows that it has non-trivial but highly varied zero-shot accuracy, often exhibiting performance that may be insufficient for legal work. The authors then demonstrate that a lightly fine-tuned Llama 3 model vastly outperforms GPT-4 on almost all tasks, typically by double-digit percentage points. The authors find that larger models respond better to fine-tuning than smaller models. A few tens to hundreds of examples suffice to achieve high classification accuracy. Notably, the authors can fine-tune a single model on all 260 tasks simultaneously at a small loss in accuracy relative to having a separate model for each task. The work points to a viable alternative to the predominant practice of prompting commercial models. For concrete legal tasks with some available labeled data, researchers are better off using a fine-tuned open-source model. <br><br>

25. ***Local vs global continual learning <br>
Researchers explored continual learning strategies, comparing local and global approximations. They classified existing algorithms based on these strategies and assessed their practical effects. The study highlighted the importance of understanding continual learning mechanisms to develop effective strategies for integrating new information while retaining past knowledge.*** <br>
    Jul 23, ETH Zurich published a [paper](https://arxiv.org/pdf/2407.16611) on CoLLAs 2024 “Local vs Global continual learning”.  Continual learning is the problem of integrating new information in a model while retaining the knowledge acquired in the past. Despite the tangible improvements achieved in recent years, the problem of continual learning is still an open one. A better understanding of the mechanisms behind the successes and failures of existing continual learning algorithms can unlock the development of new successful strategies. This work views continual learning from the perspective of the multi-task loss approximation, and compares two alternative strategies, namely local and global approximations. The authors classify existing continual learning algorithms based on the approximation used, and assess the practical effects of this distinction in common continual learning settings. Additionally, the authors study optimal continual learning objectives in the case of local polynomial approximations and provide examples of existing algorithms implementing the optimal objectives. <br><br>

27. ***Shared hallucinations in LLMs <br>
Salesforce's study on large language models (LLMs) revealed that these models share a 'shared imagination space,' enabling them to answer each other's imaginary questions with high success. This phenomenon suggests a commonality in the models' training and hallucination processes, raising questions about model homogeneity and computational creativity.*** <br>
    Jul 23, Salesforce published a [paper](https://arxiv.org/pdf/2407.16604) “Shared Imagination: LLMs Hallucinate Alike”. Despite the recent proliferation of large language models (LLMs), their training recipes -- model architecture, pre-training data and optimization algorithm -- are often very similar. This naturally raises the question of the similarity among the resulting models. This paper proposes a novel setting, imaginary question answering (IQA), to better understand model similarity. In IQA, the authors ask one model to generate purely imaginary questions (e.g., on completely made-up concepts in physics) and prompt another model to answer. Surprisingly, despite the total fictionality of these questions, all models can answer each other's questions with remarkable success, suggesting a "shared imagination space" in which these models operate during such hallucinations. The authors conduct a series of investigations into this phenomenon and discuss implications on model homogeneity, hallucination, and computational creativity. <br><br>

29. ***Training-free baseline for video LLMs <br>
Apple proposed SlowFast-LLaVA, a training-free video LLM that captures detailed spatial semantics and long-range temporal context. The two-stream design effectively aggregates features from video frames, outperforming existing training-free methods on video tasks. This approach offers a strong baseline for video LLMs without the need for extensive training.*** <br>
    Jul 22, Apple published a [paper](https://arxiv.org/pdf/2407.15841) “SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models”. The paper proposes SlowFast-LLaVA (or SF-LLaVA for short), a training-free video large language model (LLM) that can jointly capture the detailed spatial semantics and long-range temporal context without exceeding the token budget of commonly used LLMs. This is realized by using a two-stream SlowFast design of inputs for Video LLMs to aggregate features from sampled video frames in an effective way. Specifically, the Slow pathway extracts features at a low frame rate while keeping as many spatial details as possible (e.g., with 24x24 tokens), and the Fast pathway operates on a high frame rate but uses a larger spatial pooling stride (e.g., downsampling 6x) to focus on the motion cues. As a result, this design allows adequately capturing both spatial and temporal features that are beneficial for understanding details along the video. Experimental results show that SF-LLaVA outperforms existing training-free methods on a wide range of video tasks. On some benchmarks, it achieves comparable or even better performance compared to state-of-the-art Video LLMs that are fine-tuned on video datasets. <br><br>

31. ***AI-based startups analysis by High Signal AI <br>
An analysis of AI-based startups backed by YCombinator highlighted trends in AI innovation, investment, and founder backgrounds. Key findings include a focus on B2B solutions, opportunities in underserved sectors, and the importance of technical expertise. The study also noted the growing interest in generative AI and the need for ethical AI considerations.*** <br>
    Jul 19, High Signal AI published a [blog](https://highsignalai.substack.com/p/what-i-learned-from-looking-at-400) “What I learned from looking at 400 AI-based Startups backed by YCombinator”. YCombinator's (YC) track record in identifying and nurturing successful startups is unparalleled in the tech industry. Their selection process has consistently surfaced companies that go on to reshape entire sectors, making their portfolio a valuable indicator of emerging trends and technologies.  The author was looking for answers to questions like which sectors are seeing the most AI innovation? What types of AI applications are attracting investment? What backgrounds do successful AI founders have? This study aims to provide insights into: 1) The hottest industries and sectors for AI startups. 2) Areas ripe for AI disruption. 3) AI in emerging technologies like blockchain and quantum computing. 4) Companies working in AI Safety, Accessibility, Explainability. 5) Common traits among YC-backed AI founders. 6) How to find what you should build with AI using the above insights. Key finds are: 1) Focus on B2B: With 81.1% of YC-backed AI startups targeting businesses, consider enterprise solutions for higher chances of funding and success. 2) Explore Underserved Sectors: While healthcare/biotech (10.8%), fintech (9.1%), and developer tools (8.9%) dominate, look for opportunities in neglected areas like manufacturing (1%) or agriculture (0.7%). 3) Prioritize Technical Expertise: Ensure your founding team includes strong technical talent, as 74.8% of YC-backed AI companies have at least one founder with a robust technical background. 4) Capitalize on Generative AI: With 18.7% of startups in this space, generative AI is hot. However, consider how you can apply it innovatively to stand out. 5) Address Ethical Concerns: Only 1.2% of startups focus on ethical AI. This gap represents a significant opportunity for forward-thinking founders. <br><br>

33. ***Pruning and knowledge distillation for LLMs <br>
Nvidia's study on compressing language models through pruning and knowledge distillation demonstrated significant compute cost savings and performance improvements. The approach reduced training data requirements and maintained high accuracy, offering an efficient alternative to training large models from scratch. The compressed models performed well compared to other community models and state-of-the-art compression techniques.*** <br>
    Jul 19, Nvidia published a [paper](https://arxiv.org/pdf/2407.14679) “Compact Language Models via Pruning and Knowledge Distillation”. Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. This paper investigates if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, the study develops a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; and arrives at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. The paper uses this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using the approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. The authors have [open-sourced Minitron](https://github.com/NVlabs/Minitron) model weights on Huggingface, with corresponding supplementary material including example code available on GitHub. <br><br>

35. ***Long-context language models benchmark <br>
Researchers created NoCha, a benchmark for long-context language models, testing their ability to retrieve, synthesize, and reason over book-length inputs. The dataset consists of pairs of true and false claims about books, requiring global reasoning. The study found that current LLMs struggle with these tasks, highlighting the need for further improvements in long-context comprehension.*** <br>
    Jul 18, UMass Amherst, Allen Inst of AI, and Princeton Uni published a [paper](https://arxiv.org/pdf/2406.16264) “One Thousand and One Pairs: A "novel" challenge for long-context language models”. Synthetic long-context LLM benchmarks (e.g., "needle-in-the-haystack") test only surface-level retrieval capabilities, but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? The paper addresses this question by creating NoCha, a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, the annotators confirm that the largest share of pairs in NoCha require global reasoning over the entire book to verify. The experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that evaluated: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest accuracy at 55.8%. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NoCha allows for the evolution of the benchmark dataset and the easy analysis of future models. <br><br>

37. ***AI Agents That Matter <br>
The paper highlights several issues with current AI agent benchmarks, such as an overemphasis on accuracy, lack of attention to costs, conflation of benchmarking needs, inadequate holdout sets, and poor standardization in evaluation practices. The authors propose optimizing both accuracy and cost, distinguishing benchmarking needs, addressing overfitting, and standardizing evaluations to develop more practical and reliable AI agents.*** <br>
    Jul 1, Princeton Uni published [paper](https://arxiv.org/abs/2407.01502) “AI Agents That Matter”. AI agents are an exciting new research direction, and agent development is driven by benchmarks. An analysis of current agent benchmarks and evaluation practices reveals several shortcomings that hinder their usefulness in real-world applications. First, there is a narrow focus on accuracy without attention to other metrics. As a result, SOTA agents are needlessly complex and costly, and the community has reached mistaken conclusions about the sources of accuracy gains. The focus on cost in addition to accuracy motivates the new goal of jointly optimizing the two metrics. The authors design and implement one such optimization, showing its potential to greatly reduce cost while maintaining accuracy. Second, the benchmarking needs of model and downstream developers have been conflated, making it hard to identify which agent would be best suited for a particular application. Third, many agent benchmarks have inadequate holdout sets, and sometimes none at all. This has led to agents that are fragile because they take shortcuts and overfit to the benchmark in various ways. The research prescribes a principled framework for avoiding overfitting. Finally, there is a lack of standardization in evaluation practices, leading to a pervasive lack of reproducibility. The authors hope that the steps introduced for addressing these shortcomings will spur the development of agents that are useful in the real world and not just accurate on benchmarks.

 <br><br><br>

***21 Jul 2024***

1. ***Mistral NeMo Released: <br>
Mistral released Mistral NeMo, a 12B model with a 128k context length, under the Apache 2.0 license. It’s a global, multilingual model with strengths in English, French, Chinese, and Hindi, and uses a new tokenizer, Tekken, which outperforms the previous SentencePiece tokenizer. Mistral NeMo is better at instruction following, reasoning, multi-turn conversations, and code generation compared to its predecessor, Mistral 7B.*** <br>
   Jul 18, [Mistral released Mistral NeMo](https://mistral.ai/news/mistral-nemo/), a state-of-the-art 12B model with 128k context length, and released under the Apache 2.0 license. As it relies on standard architecture, Mistral NeMo is easy to use and a drop-in replacement in any system using Mistral 7B. The model is designed for global, multilingual applications. It is trained on function calling, has a large context window, and is particularly strong in English, French, Chinese, and Hindi. Mistral NeMo uses a new tokenizer, Tekken, based on Tiktoken, that was trained on over more than 100 languages, and compresses natural language text and source code more efficiently than the SentencePiece tokenizer used in previous Mistral models. Compared to the Llama 3 tokenizer, Tekken proved to be more proficient in compressing text for approximately 85% of all languages. Mistral NeMO underwent an advanced fine-tuning and alignment phase. Compared to Mistral 7B, it is much better at following precise instructions, reasoning, handling multi-turn conversations, and generating code. <br><br>

3. ***OpenAI Paper on LLM Legibility: <br>
OpenAI published a paper on improving LLM output legibility using Prover-Verifier Games. This method enhances the clarity and checkability of solutions in math problems by training verifiers and provers. Results show increased human accuracy in verifying solutions, suggesting this method could align LLMs more closely with human-checkable outputs.*** <br>
   Jul 18, OpenAI published a [paper](https://arxiv.org/abs/2407.13692) “Prover-Verifier Games improve legibility of LLM outputs”. One way to increase confidence in the outputs of Large Language Models (LLMs) is to support them with reasoning that is clear and easy to check -- a property call legibility. The paper studies legibility in the context of solving grade-school math problems and show that optimizing chain-of-thought solutions only for answer correctness can make them less legible. To mitigate the loss in legibility, the authors propose a training algorithm inspired by Prover-Verifier Game from Anil et al. (2021). The algorithm iteratively trains small verifiers to predict solution correctness, "helpful" provers to produce correct solutions that the verifier accepts, and "sneaky" provers to produce incorrect solutions that fool the verifier. The study finds that the helpful prover's accuracy and the verifier's robustness to adversarial attacks increase over the course of training. Furthermore, the paper shows that legibility training transfers to time-constrained humans tasked with verifying solution correctness. Over course of LLM training human accuracy increases when checking the helpful prover's solutions, and decreases when checking the sneaky prover's solutions. Hence, training for checkability by small verifiers is a plausible technique for increasing output legibility. The results suggest legibility training against small verifiers as a practical avenue for increasing legibility of large LLMs to humans, and thus could help with alignment of superhuman models. <br><br>

5. ***MetaSumPerceiver for Fact-Checking: <br>
Virginia Tech introduced MetaSumPerceiver, a model designed for fact-checking claims by summarizing multimodal, multi-document datasets. Using a dynamic perceiver-based model and reinforcement learning, it outperforms existing approaches on the MOCHEG dataset and demonstrates strong performance on a new fact-checking dataset.*** <br>
   Jul 18, Virginia Tech published a [paper](https://arxiv.org/pdf/2407.13089) on ACM 2024 “MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for Fact-Checking”. Fact-checking real-world claims often requires reviewing multiple multimodal documents to assess a claim's truthfulness, which is a highly laborious and time-consuming task. This paper presents a summarization model designed to generate claim-specific summaries useful for fact-checking from multimodal, multi-document datasets. The model takes inputs in the form of documents, images, and a claim, with the objective of assisting in fact-checking tasks. The paper introduces a dynamic perceiver-based model that can handle inputs from multiple modalities of arbitrary lengths. To train the model, the authors leverage a novel reinforcement learning-based entailment objective to generate summaries that provide evidence distinguishing between different truthfulness labels. To assess the efficacy of the approach, the authors conduct experiments on both an existing benchmark and a new dataset of multi-document claims. The proposed approach outperforms the SOTA approach by 4.6% in the claim verification task on the MOCHEG dataset and demonstrates strong performance on the new Multi-News-Fact-Checking dataset. <br><br>

7. ***OpenAI's GPT-4o Mini: <br>
OpenAI released GPT-4o mini, a cost-effective model scoring 82% on the MMLU. Priced significantly lower than previous models, it supports a wide range of tasks and has a large context window. The model is designed for efficient and low-latency applications, with future support for multiple input and output types.*** <br>
   Jul 18, OpenAI [released GPT-4o mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/?utm_source=substack&utm_medium=email), scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo. GPT-4o mini enables a broad range of tasks with its low cost and latency, such as applications that chain or parallelize multiple model calls (e.g., calling multiple APIs), pass a large volume of context to the model (e.g., full code base or conversation history), or interact with customers through fast, real-time text responses (e.g., customer support chatbots).  GPT-4o mini supports text and vision in the API, with support for text, image, video and audio inputs and outputs coming in the future. The model has a context window of 128K tokens, supports up to 16K output tokens per request, and has knowledge up to October 2023. <br><br>

9. ***IBM and MIT on Benchmark Agreement Testing: <br>
IBM and MIT highlighted issues in Benchmark Agreement Testing (BAT) for LLMs and proposed best practices to ensure robust and valid evaluations. They introduced BenchBench, a tool for BAT, and demonstrated the importance of standardized procedures in improving the reliability of benchmark evaluations.*** <br>
    Jul 18, IBM and MIT published a [paper](https://arxiv.org/pdf/2407.13696) “Benchmark Agreement Testing Done Right: A Guide for LLM Benchmark Evaluation”. Recent advancements in Language Models (LMs) have catalyzed the creation of multiple benchmarks, designed to assess these models' general capabilities. A crucial task, however, is assessing the validity of the benchmarks themselves. This is most commonly done via Benchmark Agreement Testing (BAT), where new benchmarks are validated against established ones using some agreement metric (e.g., rank correlation). Despite the crucial role of BAT for benchmark builders and consumers, there are no standardized procedures for such agreement testing. This deficiency can lead to invalid conclusions, fostering mistrust in benchmarks and upending the ability to properly choose the appropriate benchmark to use. By analyzing over 40 prominent benchmarks, the authors demonstrate how some overlooked methodological choices can significantly influence BAT results, potentially undermining the validity of conclusions. To address these inconsistencies, the study proposes a set of best practices for BAT and demonstrate how utilizing these methodologies greatly improves BAT robustness and validity. To foster adoption and facilitate future research, the paper introduces BenchBench, a python package for BAT, and release the BenchBench-leaderboard, a meta-benchmark designed to evaluate benchmarks using their peers. Findings of the paper underscore the necessity for standardized BAT, ensuring the robustness and validity of benchmark evaluations in the evolving landscape of language model research. BenchBench Package: https://github.com/IBM/BenchBench Leaderboard: https://huggingface.co/spaces/per/BenchBench <br><br>

11. ***Open-Source LLMs in Biomedical Tasks: <br>
A study explored the performance of open-source LLMs like Mixtral 8x7B compared to commercial models in biomedical tasks. While competitive in few-shot settings, open-source models struggled in zero-shot scenarios. The research suggests that domain-specific few-shot examples can close the performance gap between commercial and open-source models.*** <br>
    Jul 18, Uni of Regensburg published a [paper](https://arxiv.org/pdf/2407.13511) “Can Open-Source LLMs Compete with Commercial Models? Exploring the Few-Shot Performance of Current GPT Models in Biomedical Tasks”. Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPT and Anthropic's Claude 3 Opus, have dominated natural language processing (NLP) benchmarks across different domains. New competing Open-Source alternatives like Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap while often offering higher throughput and being less costly to use. Open-Source LLMs can also be self-hosted, which makes them interesting for enterprise and clinical use cases where sensitive data should not be processed by third parties. The authors participated in the 12th BioASQ challenge, which is a retrieval augmented generation (RAG) setting, and explored the performance of current GPT models Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning (zero-shot, few-shot) and QLoRa fine-tuning. The study also explored how additional relevant knowledge from Wikipedia added to the context-window of the LLM might improve their performance. Mixtral 8x7b was competitive in the 10-shot setting, both with and without fine-tuning, but failed to produce usable results in the zero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead to measurable performance gains. The results indicate that the performance gap between commercial and open-source models in RAG setups exists mainly in the zero-shot setting and can be closed by simply collecting few-shot examples for domain-specific use cases. The code needed to rerun these experiments is available through [GitHub](https://github.com/SamyAteia/bioasq2024).  <br><br>

13. ***AI and Universal Basic Income: <br>
An article in Forbes discussed the impact of AI and robotics on job displacement, suggesting a shift towards human-AI collaboration. It proposes that universal basic income may become necessary as AI makes many traditional jobs obsolete, emphasizing the enduring need for human skills in the workforce.*** <br>
    Jul 17, Forbes published an [article](https://www.forbes.com/sites/jackkelly/2024/07/17/ai-robot-job-displacement-universal-basic-income/) “How AI And Robot Job Displacements Could Lead Us Down The Road Of Universal Basic Income And Loss Of Identity”. The article argues that in today’s evolving workplace landscape, AI and robotics are already automating tasks across almost all sectors, including manufacturing, data analysis, customer service and administration. As it stands, repetitive and routine tasks are the most susceptible to automation. While AI and robotics will undoubtedly change the nature of work, it's unlikely that these technologies will eradicate the existence of all jobs. The focus will likely shift toward human-AI collaboration and jobs requiring uniquely human skills. The future of work could involve a combination of paid employment, universal basic income and a renewed focus on finding meaning and fulfillment outside of traditional work structures. Many jobs require creativity, critical thinking, social skills, problem-solving under pressure and the ability to handle unforeseen situations. Because these are areas where AI is still limited, it demonstrates the need for continued human skills in job functions. Elon Musk stated this May in Pairs that AI will eventually make workers obsolete—a prediction he doesn’t necessarily see as pernicious. Highly advanced AI capabilities will dispel the need for human labor, rendering traditional jobs unnecessary, in what he frames as a likely "benign scenario" for the future of work. Musk sees people working simply out of personal interest or creative satisfaction. For work to become optional, we would need to live in an “age of abundance” achieved by "universal high income," Musk said. Geoffrey Hinton told BBC that universal basic income would need to be provided by the government to provide a safety net, if automation catalyzes widespread job displacement. <br><br>

15. ***Manipulating LLM Uncertainty: <br>
Research from Rutgers, NYU, and Meta investigated the fragility of uncertainty estimation in LLMs. They demonstrated that backdoor attacks could manipulate model uncertainty without changing the output, highlighting a significant threat to LLM reliability and the need for defenses against such vulnerabilities.*** <br>
    Jul 17, Rutgers Uni, NYU, Meta, et al. published a [paper](https://arxiv.org/pdf/2407.11282) “Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models”. Large Language Models (LLMs) are employed across various high-stakes domains, where the reliability of their outputs is crucial. One commonly used method to assess the reliability of LLMs' responses is uncertainty estimation, which gauges the likelihood of their answers being correct. While many studies focus on improving the accuracy of uncertainty estimations for LLMs, this research investigates the fragility of uncertainty estimation and explores potential attacks. The authors demonstrate that an attacker can embed a backdoor in LLMs, which, when activated by a specific trigger in the input, manipulates the model's uncertainty without affecting the final output. Specifically, the proposed backdoor attack method can alter an LLM's output probability distribution, causing the probability distribution to converge towards an attacker-predefined distribution while ensuring that the top-1 prediction remains unchanged. Experimental results demonstrate that this attack effectively undermines the model's self-evaluation reliability in multiple-choice questions. For instance, the study achieved a 100 attack success rate (ASR) across three different triggering strategies in four models. Further, the authors investigate whether this manipulation generalizes across different prompts and domains. This work highlights a significant threat to the reliability of LLMs and underscores the need for future defenses against such attacks. The code is available at https://github.com/qcznlp/uncertainty_attack. <br><br>

17. ***Goldfish for Long Video Understanding: <br>
KAUST, Harvard, and Swiss AI Lab introduced Goldfish, a methodology for understanding long videos. It uses efficient retrieval mechanisms and a new benchmark, TVQA-long, to improve comprehension of lengthy video content. Goldfish outperforms previous methods in both long and short video understanding.*** <br>
    Jul 17, KAUST, Harvard Uni and The Swiss AI Lab published a [paper](https://arxiv.org/pdf/2407.12679) “Goldfish: Vision-Language Understanding of Arbitrarily Long Videos”. Most current LLM-based models for video understanding can process videos within minutes. However, they struggle with lengthy videos due to challenges such as "noise and redundancy", as well as "memory and computation" constraints. This paper presents Goldfish, a methodology tailored for comprehending videos of arbitrary lengths. The work also introduces the TVQA-long benchmark, specifically designed to evaluate models' capabilities in understanding long videos with questions in both vision and text content. Goldfish approaches these challenges with an efficient retrieval mechanism that initially gathers the top-k video clips relevant to the instruction before proceeding to provide the desired response. This design of the retrieval mechanism enables the Goldfish to efficiently process arbitrarily long video sequences, facilitating its application in contexts such as movies or television series. To facilitate the retrieval process, the study developed MiniGPT4-Video that generates detailed descriptions for the video clips. In addressing the scarcity of benchmarks for long video evaluation, the paper adapted the TVQA short video benchmark for extended content analysis by aggregating questions from entire episodes, thereby shifting the evaluation from partial to full episode comprehension. The study attained a 41.78% accuracy rate on the TVQA-long benchmark, surpassing previous methods by 14.94%. The MiniGPT4-Video also shows exceptional performance in short video comprehension, exceeding existing state-of-the-art methods by 3.23%, 2.03%, 16.5% and 23.59% on the MSVD, MSRVTT, TGIF, and TVQA short video benchmarks, respectively. These results indicate that these models have significant improvements in both long and short-video understanding. The models and code have been made publicly available at https://vision-cair.github.io/Goldfish_website/ <br><br>

19. ***Foundation Model Transparency Index v1.1: <br>
A follow-up study from Stanford, Princeton, and MIT assessed the transparency of foundation model developers. The updated index shows improved transparency, with developers now scoring 58 out of 100 on average. The study highlights areas of ongoing opacity and suggests that increased transparency can be achieved through policy interventions.*** <br>
    Jul 17, Stanford Uni, Princeton Uni and MIT published a [paper](https://arxiv.org/pdf/2407.12929) “The Foundation Model Transparency Index v1.1: May 2024”. Foundation models are increasingly consequential yet extremely opaque. To characterize the status quo, the Foundation Model Transparency Index was launched in October 2023 to measure the transparency of leading foundation model developers. The October 2023 Index (v1.0) assessed 10 major foundation model developers (e.g. OpenAI, Google) on 100 transparency indicators (e.g. does the developer disclose the wages it pays for data labor?). At the time, developers publicly disclosed very limited information with the average score being 37 out of 100. To understand how the status quo has changed, the researchers conduct a follow-up study (v1.1) after 6 months: the report scores 14 developers against the same 100 indicators. While in v1.0 the authors searched for publicly available information, in v1.1 developers submit reports on the 100 transparency indicators, potentially including information that was not previously public. The report finds that developers now score 58 out of 100 on average, a 21 point improvement over v1.0. Much of this increase is driven by developers disclosing information during the v1.1 process: on average, developers disclosed information related to 16.6 indicators that was not previously public. The authors observe regions of sustained (i.e. across v1.0 and v1.1) and systemic (i.e. across most or all developers) opacity such as on copyright status, data access, data labor, and downstream impact. The authors publish transparency reports for each developer that consolidate information disclosures: these reports are based on the information disclosed to the authors via developers. The findings demonstrate that transparency can be improved in this nascent ecosystem, the Foundation Model Transparency Index likely contributes to these improvements, and policymakers should consider interventions in areas where transparency has not improved. <br><br>

21. ***AI revolutionises business document management: <br>
Professionals spend a significant amount of time searching for information, highlighting the need for better tools. Adobe’s AI Assistant, a generative AI conversational engine, integrates into document workflows to boost productivity. It helps generate high-quality insights and quickly create emails, reports, and presentations from various document types. In the legal sector, it summarizes judgments to save time for legal counsel. Wealth advisors also benefit from this technology by consolidating information for client advice. AI integration into workflows is becoming essential for businesses.*** <br>
    Jul 16, Financial Review published an [article](https://www.afr.com/technology/ai-revolutionises-business-document-management-20240711-p5jst3) “AI revolutionises business document management”. On average, professionals spend about 8.2 hours a week just searching for information within their documents. This inefficiency underscores the need for smarter, more effective tools to handle the deluge of data and documents that businesses process daily. Deeply integrated into document workflows, Adobe’s AI Assistant is a generative AI-powered conversational engine that can be deployed in minutes, unlocking new levels of document productivity for every knowledge worker across the enterprise. With AI Assistant, users can gain high-quality insights with intelligent citations and quickly generate emails, reports, presentations, and more from the information in their PDFs and other types of documents, including Word, PowerPoint, and meeting transcripts. One practical application of this technology is seen in the legal sector. The AI Assistant helps by summarising four or five judgments, enabling the legal counsel to quickly grasp the essentials and make informed decisions faster, saving them 30 minutes a day, and 2.5 hours a week. The AI Assistant also caters to wealth advisors who need to consolidate information from various reports to provide comprehensive advice to clients. The AI model is integrating AI capabilities into existing workflows, make AI a necessity, not a luxury. <br><br>

23. ***High Performance RWKV/Transformer and Extreme KV-Cache Compression:  <br>
The paper introduces GoldFinch, a hybrid model combining Linear Attention and Transformer techniques to efficiently generate a compressed KV-Cache. GoldFinch enhances performance relative to previous models and saves cache size significantly. Despite the complexity of autoregressive generation, the pre-fill computation remains efficient due to the use of RNNs. The trained weights and codes are released for community use.*** <br>
    Jul 16, EleutherAI et al. published a [paper](https://arxiv.org/pdf/2407.12077) “GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression”. The paper introduces GoldFinch, a hybrid Linear Attention/Transformer sequence model that uses a new technique to efficiently generate a highly compressed and reusable KV-Cache in linear time and space with respect to sequence length. GoldFinch stacks a new GOLD transformer on top of an enhanced version of the Finch (RWKV-6) architecture. The authors train up to 1.5B parameter class models of the Finch, Llama, and GoldFinch architectures, and find dramatically improved modeling performance relative to both Finch and Llama. The cache size savings increase linearly with model layer count, ranging from 756-2550 times smaller than the traditional transformer cache for common sizes, enabling inference of extremely large context lengths even on limited hardware. Although autoregressive generation has O(n) time complexity per token because of attention, pre-fill computation of the entire initial cache state for a submitted context costs only O(1) time per token due to the use of a recurrent neural network (RNN) to generate this cache. Trained weights and training [codes](https://github.com/recursal/GoldFinch-paper) are released under the Apache 2.0 license for community use. <br><br>

25. ***A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval: <br>
Existing benchmarks focus on simple queries, but BRIGHT addresses the need for benchmarks requiring deep reasoning. BRIGHT includes 1,398 real-world queries across various domains and shows that current retrieval models perform poorly on it. The use of Chain-of-Thought reasoning improves performance. BRIGHT is robust against data leakage and encourages research on more challenging retrieval tasks. Code and data are publicly available.*** <br>
    Jul 16, The Uni of Hong Kong, Princeton Uni, Uni of Washington and Google published a [paper](https://arxiv.org/pdf/2407.12883) “BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval”. Existing retrieval benchmarks primarily consist of information-seeking queries (e.g., aggregated questions from search engines) where keyword or semantic-based retrieval is usually sufficient. However, many complex real-world queries require in-depth reasoning to identify relevant documents that go beyond surface form matching. For example, finding documentation for a coding question requires understanding the logic and syntax of the functions involved. To better benchmark retrieval on such challenging queries, the paper introduces BRIGHT, the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398 real-world queries collected from diverse domains (such as economics, psychology, robotics, software engineering, earth sciences, etc.), sourced from naturally occurring or carefully curated human data. Extensive evaluation reveals that even state-of-the-art retrieval models perform poorly on BRIGHT. The leading model on the MTEB leaderboard, which achieves a score of 59.0 nDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. The authors further demonstrate that augmenting queries with Chain-of-Thought reasoning generated by large language models (LLMs) improves performance by up to 12.2 points. Moreover, BRIGHT is robust against data leakage during pretraining of the benchmarked models as the authors validate by showing similar performance even when documents from the benchmark are included in the training data. BRIGHT paves the way for future research on retrieval systems in more realistic and challenging settings. Code and data are available at https://brightbenchmark.github.io. <br><br>

27. ***All Large Language Models can be Fully Sparsely-Activated: <br>
Q-Sparse enables efficient training of sparsely-activated large language models (LLMs) through top-K sparsification and the straight-through-estimator. It achieves comparable results to baseline LLMs with significant efficiency gains in inference. Q-Sparse is versatile across different training settings and is effective for both full-precision and 1-bit LLMs, promising to revolutionize future LLM efficiency.*** <br>
    Jul 15, Microsoft published a [paper](https://arxiv.org/pdf/2407.10969) “Q-Sparse: All Large Language Models can be Fully Sparsely-Activated”.  The paper introduces, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) the paper presents an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different settings, including training-from-scratch, continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the cornerstone and a clear path to revolutionize the efficiency, including cost and energy consumption, of future LLMs. <br><br>

29. ***Taming Large Language Models for Better Automatic Evaluation: <br>
FLAMe, a family of autorater models, enhances the evaluation of LLMs by training on a diverse set of quality assessment tasks. FLAMe outperforms proprietary models like GPT-4 on many tasks and serves as a robust starting point for fine-tuning. It is also more efficient and less biased than other models, excelling in identifying high-quality responses for various tasks.*** <br>
    Jul 15, Google published a [paper](https://arxiv.org/pdf/2407.10817) “Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation”. As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their output due to the high costs of human evaluation. To make progress towards better LLM autoraters, the paper introduces FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on Google’s large and diverse collection of 100+ quality assessment tasks comprising 5M+ human judgments, curated and standardized using publicly released human evaluations from previous research. FLAMe significantly improves generalization to a wide variety of held-out tasks, outperforming LLMs trained on proprietary data like GPT-4 and Claude-3 on many tasks. The study shows that FLAMe can also serve as a powerful starting point for further downstream fine-tuning, using reward modeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, the FLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative model trained exclusively on permissively licensed data, outperforming both GPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, the study explores a more computationally efficient approach using a novel tail-patch fine-tuning strategy to optimize the FLAMe multitask mixture for reward modeling evaluation (FLAMe-Opt-RM), offering competitive RewardBench performance while requiring approximately 25x less training datapoints. Overall, the FLAMe variants outperform all popular proprietary LLM-as-a-Judge models considered across 8 out of 12 autorater evaluation benchmarks, encompassing 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, analysis reveals that FLAMe is significantly less biased than these LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark, while effectively identifying high-quality responses for code generation.  <br><br>

31. ***How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients: <br>
The paper studies low-rank structures in LLMs and introduces WeLore for weight compression and memory-efficient fine-tuning. WeLore leverages gradient dynamics to identify suitable rank reduction ratios, enabling significant performance and efficiency improvements. The technique outperforms full finetuning while reducing memory and computational requirements, making it a powerful tool for optimizing LLMs.*** <br>
    Jul 15, Uni of Texas at Austin Uni of Oxford, Meta, et al published a [paper](https://arxiv.org/pdf/2407.11239) “From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients”. Modern Large Language Models (LLMs) are composed of matrices with billions of elements, making their storage and processing quite demanding in terms of computational resources and memory usage. Being significantly large, such matrices can often be expressed in low-rank format with potential to relax resource requirements. Unlike prior works which focus on developing novel matrix decomposition algorithms, this work first studies the emergence of low-rank structures across matrices within different layers of LLMs and establish a consequential relationship between the gradient dynamics and emerging low-rank expressiveness of matrices. Findings reveal that different layers exhibit varying levels of converged low-rank structure, necessitating a non-uniform rank reduction across them to minimize performance drop due to compression. In view of that, the paper presents Weight Low-Rank Projection (WeLore) that unifies weight compression and memory-efficient fine-tuning as ONE, in a data-agnostic and one-shot way. WeLore capitalizes the heavy-tail distribution of singular values to identify a suitable rank reduction ratio for matrices within LLMs. Going beyond only as a compression technique, WeLore categorizes weight matrices into Low-rank Components (LRCs) and Non-Low-rank Components (N-LRCs) based on their ability to express themselves as low-rank. The gradient perspective and extensive experiments illustrate that LRCs tend to have better finetuning capabilities and can closely mimic (sometimes outperform) the training loss trajectory and performance of full-finetuning with notable memory and compute footprint reduction. For example, finetuning a 50% compressed LLaMa-2 7B model using only a fraction of parameters in LRCs (WeLore) can outperform its full finetuning with ~3x better throughput and ~0.6x GPU requirement. The codes are available at https://github.com/VITA-Group/welore <br><br>

33. ***Practical Unlearning for Large Language Models: <br>
Addressing security issues in LLMs, the O3 framework provides a solution for machine unlearning without compromising model utility. It includes an OOD detector and an Orthogonal LoRA for continuous unlearning. O3 smartly decides on unlearning actions during inference and shows superior performance across various tasks and datasets, especially with continuous unlearning requests.*** <br>
    Jul 14, Northwestern Uni and Arizona State Uni published a [paper](https://arxiv.org/pdf/2407.10223) “Practical Unlearning for Large Language Models”. While LLMs have demonstrated impressive performance across various domains and tasks, their security issues have become increasingly severe. Machine unlearning (MU) has emerged as a promising solution to address these issues by removing the influence of undesired data on the target model without compromising its utility in other aspects. MU typically assumes full access to the original training data to preserve utility, which is difficult to achieve in LLM unlearning. Existing LLM unlearning methods often assume access to data most affected by undesired data unlearning. However, this assumption underestimates the entanglement among various LLM capabilities and ignores data access limitations due to various issues. Moreover, these LLM unlearning methods do not sufficiently consider that unlearning requests in real-world scenarios are continuously emerging. To overcome these challenges and achieve practical LLM unlearning, the authors propose the O3 framework. The O3 framework includes an Out-Of-Distribution (OOD) detector to measure the similarity between input and unlearning data, and an Orthogonal low-rank adapter (LoRA) for continuously unlearning requested data. The OOD detector is trained with a novel contrastive entropy loss and utilizes a local-global layer-aggregated scoring mechanism. The orthogonal LoRA achieves parameter disentanglement among continual unlearning requests. During inference, the O3 framework can smartly decide whether and to what extent to load the unlearning LoRA based on the OOD detector's predictions. Notably, O3's effectiveness does not rely on any retained data. The authors conducted extensive experiments on O3 and state-of-the-art LLM unlearning methods across three tasks and seven datasets. The results indicate that O3 consistently achieves the best trade-off between unlearning effectiveness and utility preservation, especially when facing continuous unlearning requests. <br><br>

35. ***Vocabulary used by ChatGPT in academic writing:  <br>
The paper examines LLM usage in academic writing, identifying a significant impact on scientific literature. By analyzing vocabulary changes in PubMed abstracts, the study estimates that at least 10% of 2024 abstracts involved LLMs. This impact varies across disciplines and regions, with notable increases in certain fields. The study highlights the widespread adoption of LLMs in academia.*** <br>
    Jul 3, researchers from Uni of Tubingen and Northwestern Uni published a [paper](https://arxiv.org/pdf/2406.07016) “Delving into ChatGPT usage in academic writing through excess vocabulary”. Recent large language models (LLMs) can generate and revise text with human-level performance, and have been widely commercialized in systems like ChatGPT. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists have been using them to assist their scholarly writing. How wide-spread is LLM usage in the academic literature currently? To answer this question, the work uses an unbiased, large-scale approach, free from any assumptions on academic LLM usage. The authors study vocabulary changes in 14 million PubMed abstracts from 2010-2024, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. The analysis based on excess words usage suggests that at least 10% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, and was as high as 30% for some PubMed sub-corpora. The paper shows that the appearance of LLM-based writing assistants has had an unprecedented impact in the scientific literature, surpassing the effect of major world events such as the Covid pandemic. The paper also shows the six most widely used words by ChatGPT: delves, crucial, potential, these, significant, and important. <br><br>

37. ***Self-replicating Programs Emerge from Simple Interaction: <br>
The study explores how self-replicating programs arise from simple computational interactions. It demonstrates that self-replicators emerge in environments without explicit fitness landscapes due to random interactions and self-modification. The findings contribute to understanding the dynamics of self-replication and complex behavior emergence in computational substrates.*** <br>
    Jun 27, Google and Uni of Chicago published a [paper](https://arxiv.org/pdf/2406.19108) “Computational Life: How Well-formed, Self-replicating Programs Emerge from Simple Interaction”. The fields of Origin of Life and Artificial Life both question what life is and how it emerges from a distinct set of "pre-life" dynamics. One common feature of most substrates where life emerges is a marked shift in dynamics when self-replication appears. While there are some hypotheses regarding how self-replicators arose in nature, people know very little about the general dynamics, computational principles, and necessary conditions for self-replicators to emerge. This is especially true on "computational substrates" where interactions involve logical, mathematical, or programming rules. This paper takes a step towards understanding how self-replicators arise by studying several computational substrates based on various simple programming languages and machine instruction sets. The authors show that when random, non self-replicating programs are placed in an environment lacking any explicit fitness landscape, self-replicators tend to arise. The study demonstrates how this occurs due to random interactions and self-modification, and can happen with and without background random mutations. The paper also shows how increasingly complex dynamics continue to emerge following the rise of self-replicators. Finally, the authors show a counterexample of a minimalistic programming language where self-replicators are possible, but so far have not been observed to arise.
 <br><br>

***14 Jul 2024***

1. ***OpenAI's Novel Approach with "Strawberry"<br>
OpenAI is developing "Strawberry," a novel post-training method to enhance AI reasoning, enabling complex long-horizon tasks and deep internet research. The project aims to address common sense issues in current AI models and improve their autonomous research capabilities.*** <br><br>
   Jul 13, according to [Reuters](https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/), ChatGPT maker OpenAI is working on a novel approach to its artificial intelligence models in a project code-named “Strawberry,” according to a person familiar with the matter and internal documentation reviewed by Reuters. The project, details of which have not been previously reported, comes as the Microsoft-backed startup races to show that the types of models it offers are capable of delivering advanced reasoning capabilities. There is a project that uses Strawberry models with the aim of enabling the company’s AI to not just generate answers to queries but to plan ahead enough to navigate the internet autonomously and reliably to perform what OpenAI terms “deep research,” according to the source. Two sources described viewing earlier this year what OpenAI staffers told them were Q* demos, capable of answering tricky science and math questions out of reach of today’s commercially-available models. OpenAI hopes the innovation will improve its AI models’ reasoning capabilities dramatically, the person familiar with it said, adding that Strawberry involves a specialized way of processing an AI model after it has been pre-trained on very large datasets. While large language models can already summarize dense texts and compose elegant prose far more quickly than any human, the technology often falls short on common sense problems whose solutions seem intuitive to people, like recognizing logical fallacies and playing tic-tac-toe. When the model encounters these kinds of problems, it often “hallucinates” bogus information. Strawberry includes a specialized way of what is known as “post-training” OpenAI’s generative AI models, or adapting the base models to hone their performance in specific ways after they have already been “trained” on reams of generalized data, one of the sources said. Among the capabilities OpenAI is aiming Strawberry at is performing long-horizon tasks (LHT), the document says, referring to complex tasks that require a model to plan ahead and perform a series of actions over an extended period of time, the first source explained.
 <br><br>
3. ***Progress Towards Artificial General Intelligence (AGI)<br>
OpenAI tracks its progress towards AGI using an internal scale from Level 1 to Level 5. Current models are at Level 1, with Level 2 near completion, aiming for human PhD-level problem-solving. Achieving AGI will require massive computing power and time, with predictions varying widely on when it will be achieved.*** <br><br>
   Jul 12, according to [theverge.com](https://www.theverge.com/2024/7/11/24196746/heres-how-openai-will-determine-how-powerful-its-ai-systems-are?utm_source=substack&utm_medium=email), OpenAI has created an internal scale to track the progress its large language models are making toward artificial general intelligence, or AI with human-like intelligence. Today’s chatbots, like ChatGPT, are at Level 1. OpenAI claims it is nearing Level 2, defined as a system that can solve basic problems at the level of a person with a PhD. Level 3 refers to AI agents capable of taking actions on a user’s behalf. Level 4 involves AI that can create new innovations. Level 5, the final step to achieving AGI, is AI that can perform the work of entire organizations of people. OpenAI has previously defined AGI as “a highly autonomous system surpassing humans in most economically valuable tasks.” OpenAI’s unique structure is centered around its mission of achieving AGI, and how OpenAI defines AGI is important. Still, AGI is still quite a ways away: it will take billions upon billions of dollars worth of computing power to reach AGI, if at all. Timelines from experts, and even at OpenAI, vary wildly. In October 2023, OpenAI CEO Sam Altman said we are “five years, give or take,” before reaching AGI. OpenAI hasn’t provided details on how it assigns models to these internal. However, company leaders demonstrated a research project using the GPT-4 AI model during an all-hands meeting on Thursday and believe this project showcases some new skills that exhibit human-like reasoning. 

5. ***Advancements in Attention Mechanisms<br>
A paper by Colfax Research and collaborators introduces "FlashAttention-3," improving attention mechanisms in Transformer models. By leveraging new GPU capabilities and optimizing computation, the method achieves significant speedup and accuracy, making it more efficient for large language models.*** <br><br>
   Jul 11, Colfax Research, Meta, Nvidia, Georgia Tech, Princeton Uni, and TogetherAI published a [paper](https://arxiv.org/pdf/2407.08608v1) “FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision”. Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU. The work develops three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. The paper demonstrates that FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0× with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. The paper validates that FP8 FlashAttention-3 achieves 2.6× lower numerical error than a baseline FP8 attention.

7. ***Concerns Over Safety and Profit at OpenAI<br>
Former OpenAI employee William Saunders criticizes the company for prioritizing profit over safety. His resignation highlights concerns about the potential risks of developing AGI without adequate safety measures, drawing parallels to the Titanic's insufficient lifeboats.*** <br><br>
   Jul 11, according to [furturism.com](https://futurism.com/openai-researcher-quit-realized-upsetting-truth), A former OpenAI worker says he quit the company after realizing that it was putting safety on the back burner to pursue profit. "I really didn't want to end up working for the Titanic of AI, and so that's why I resigned," former Superalignment team member William Saunders said. "During my three years at OpenAI, I would sometimes ask myself a question. Was the path that OpenAI was on more like the Apollo program or more like the Titanic?" Saunders argued that the Titanic may have been called "unsinkable, but at the same time there weren't enough lifeboats for everyone and so when disaster struck, a lot of people died." His comments highlight growing concerns over companies like OpenAI developing AI systems that are capable of superseding the abilities of humans, an idea dubbed artificial general intelligence (AG) — something that currently remains entirely theoretical, but is a source of great interest to executives like OpenAI's Sam Altman. "Even when big problems happened, like Apollo 13, they had enough sort of like redundancy, and were able to adapt to the situation in order to bring everyone back safely," "It is not possible to develop AGI or any new technology with zero risk," he added in an email to Business Insider after the publication pointed out that the Apollo program saw its own fair share of safety oversights. "What I would like to see is the company taking all possible reasonable steps to prevent these risks." Saunders' comments paint a worrying picture of how OpenAI is being run and the changes Altman has made over the last couple of years.

9. ***Efficient Training with Q-GaLore<br>
Researchers introduced "Q-GaLore," a method combining quantization and low-rank projection to reduce memory usage in training large language models. This approach significantly cuts memory consumption while maintaining performance, enabling efficient training on less powerful hardware.*** <br><br>
    Jul 11, Uni of Texas at Austin, Uni of Oxford, Meat et al. published a [paper](https://arxiv.org/pdf/2407.08296) “Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients”. Training Large Language Models (LLMs) is memory-intensive due to the large number of parameters and associated optimization states. GaLore, a recent method, reduces memory usage by projecting weight gradients into a low-rank subspace without compromising performance. However, GaLore relies on time-consuming Singular Value Decomposition (SVD) operations to identify the subspace, and the frequent subspace updates lead to significant training time overhead. Moreover, GaLore offers minimal improvements in accuracy and efficiency compared to LoRA in more accessible fine-tuning scenarios. To address these limitations, the paper introduces Q-Galore, a novel approach that substantially reduces memory usage by combining quantization and low-rank projection, surpassing the benefits of GaLore. The proposed method is based on two key observations: (i) the gradient subspace exhibits diverse properties, with some layers converging early in training while others are subject to frequent changes; (ii) the projection matrices are highly resilient to low-bit quantization. Leveraging these insights, Q-GaLore adaptively updates the gradient subspace based on its convergence statistics, achieving comparable performance while significantly reducing the number of SVD operations. The study maintains the projection matrices in INT4 format and weights in INT8 format, incorporating stochastic rounding to capture accumulated gradient information. This approach enables a high-precision training trajectory using only low-precision weights. The study demonstrates that Q-GaLore achieves highly competitive performance with exceptional memory efficiency. At pre-training, Q-GaLore facilitates training a LLaMA-7B model from scratch on a single NVIDIA RTX 4060 Ti with only 16 GB memory. At fine-tuning, it reduces memory consumption by up to 50% compared to LoRA and GaLore, while consistently outperforming QLoRA at the same memory cost.

11. ***Teaching Transformers Causal Reasoning<br>
Microsoft and MIT propose an axiomatic training method to teach transformers causal reasoning from passive data. The study demonstrates that models can generalize causal reasoning to new scenarios, matching or surpassing the performance of larger models.*** <br><br>
    Jun 10, Microsoft and MIT published a [paper](https://arxiv.org/pdf/2407.07612) “Teaching Transformers Causal Reasoning through Axiomatic Training”. For text-based AI systems to interact in the real world, causal reasoning is an essential skill. Since interventional data is costly to generate, the word studies to what extent an agent can learn causal reasoning from passive data. Specifically, the paper considers an axiomatic training setup where an agent learns from multiple demonstrations of a causal axiom (or rule), rather than incorporating the axiom as an inductive bias or inferring it from data values. A key question is whether the agent would learn to generalize from the axiom demonstrations to new scenarios. For example, if a transformer model is trained on demonstrations of the causal transitivity axiom over small graphs, would it generalize to applying the transitivity axiom over large graphs? Experimental results, based on a novel axiomatic training scheme, indicate that such generalization is possible. The paper considers the task of inferring whether a variable causes another variable, given a causal graph structure. The paper finds that a 67 million parameter transformer model, when trained on linear causal chains (along with some noisy variations) can generalize well to new kinds of graphs, including longer causal chains, causal chains with reversed order, and graphs with branching; even when it is not explicitly trained for such settings. The model performs at par (or even better) than many larger language models such as GPT-4, Gemini Pro, and Phi-3. Overall, the axiomatic training framework provides a new paradigm of learning causal reasoning from passive data that can be used to learn arbitrary axioms, as long as sufficient demonstrations can be generated.

6. ***Detecting Contextual Hallucinations<br>
A new method called "Lookback Lens" detects and mitigates hallucinations in large language models by analyzing attention weights. This approach effectively reduces hallucinations across various tasks and models, enhancing the accuracy of generated content.*** <br><br>
   Jul 9, MIT and Uni of Washington published a [paper](https://arxiv.org/pdf/2407.07071) “Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps”. When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such contextual hallucinations. The authors hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, the work proposes a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). The study finds that a linear classifier based on these lookback ratio features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector -- Lookback Lens -- is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. The authors further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.

8. ***Framework for Composable Interventions<br>
A paper introduces a framework for combining interventions in language models to improve factual accuracy and mitigate harmful outputs. The study reveals the interactions between different interventions, emphasizing the need for new multi-objective methods.*** <br><br>
   Jul 9, Uni of Virginia, EleutherAI, Microsoft Harvard Uni, Uni of Oxford et al. published a [paper](https://arxiv.org/pdf/2407.06483) “Composable Interventions for Language Models”. Test-time interventions for language models can enhance factual accuracy, mitigate harmful outputs, and improve model efficiency without costly retraining. But despite a flood of new methods, different types of interventions are largely developing independently. In practice, multiple interventions must be applied sequentially to the same model, yet we lack standardized ways to study how interventions interact. The paper fills this gap by introducing composable interventions, a framework to study the effects of using multiple interventions on the same language models, featuring new metrics and a unified codebase. Using the framework, the paper conducts extensive experiments and composes popular methods from three emerging intervention categories—knowledge editing, model compression, and machine unlearning. Experimental results from 310 different compositions uncover meaningful interactions: compression hinders editing and unlearning, composing interventions hinges on their order of application, and popular general-purpose metrics are inadequate for assessing composability. Taken together, the findings showcase clear gaps in composability, suggesting a need for new multi-objective interventions.

10. ***Lawyers' Perceptions of AI-Generated Documents<br>
A study shows that lawyers prefer documents perceived as human-crafted over AI-generated ones, despite expecting future AI involvement in legal document generation. This perception could influence the adoption of AI in legal processes.*** <br><br>
    Jul 9, Masaryk Uni and CMU published a [paper](https://www.arxiv.org/pdf/2407.06798) “It Cannot Be Right If It Was Written by AI: On Lawyers' Preferences of Documents Perceived as Authored by an LLM vs a Human”. Large Language Models (LLMs) enable a future in which certain types of legal documents may be generated automatically. This has a great potential to streamline legal processes, lower the cost of legal services, and dramatically increase access to justice. While many researchers focus their efforts on proposing and evaluating LLM-based applications supporting tasks in the legal domain, there is a notable lack of investigations into how legal professionals perceive content if they believe it has been generated by an LLM. Yet, this is a critical point as over-reliance or unfounded skepticism may influence whether such documents bring about appropriate legal consequences. This study is the necessary analysis in the context of the ongoing transition towards mature generative AI systems. Specifically, the paper examined whether the perception of legal documents' by lawyers (n=75) varies based on their assumed origin (human-crafted vs AI-generated). The participants evaluated the documents focusing on their correctness and language quality. The analysis revealed a clear preference for documents perceived as crafted by a human over those believed to be generated by AI. At the same time, most of the participants are expecting the future in which documents will be generated automatically. These findings could be leveraged by legal practitioners, policy makers and legislators to implement and adopt legal document generation technology responsibly, and to fuel the necessary discussions into how legal processes should be updated to reflect the recent technological developments.

12. ***Fallback Behaviors of Language Models<br>
Tel Aviv University research categorizes fallback behaviors in language models, showing that advanced models shift from sequence repetitions to hallucinations under uncertainty. The study highlights the need for improved techniques to manage these behaviors.*** <br><br>
    Jul 8, Tel Aviv Uni published a [paper](https://arxiv.org/pdf/2407.06071) “From Loops to Oops: Fallback Behaviors of Language Models Under Uncertainty”. Large language models (LLMs) often exhibit undesirable behaviors, such as hallucinations and sequence repetitions. The paper proposes to view these behaviors as fallbacks that models exhibit under uncertainty, and investigate the connection between them. The work categorizes fallback behaviors -- sequence repetitions, degenerate text, and hallucinations -- and extensively analyzes them in models from the same family that differ by the amount of pretraining tokens, parameter count, or the inclusion of instruction-following training. The experiments reveal a clear and consistent ordering of fallback behaviors, across all these axes: the more advanced an LLM is (i.e., trained on more tokens, has more parameters, or instruction-tuned), its fallback behavior shifts from sequence repetitions, to degenerate text, and then to hallucinations. Moreover, the same ordering is observed throughout a single generation, even for the best-performing models; as uncertainty increases, models shift from generating hallucinations to producing degenerate text and then sequence repetitions. Lastly, the paper demonstrates that while common decoding techniques, such as random sampling, might alleviate some unwanted behaviors like sequence repetitions, they increase harder-to-detect hallucinations.

14. ***LMs and Vision Models Conceptual Alignment<br>
The study by the University of Copenhagen and MBZU evaluates the conceptual alignment between large-scale pretrained language models (LMs) and vision models. Contrary to the claim that LMs lack the ability to connect utterances to the world, the experiments show partial convergence in representations between LMs and vision models. This has significant implications for multi-modal processing and the understanding of LMs.*** <br><br>
    Jul 6, Uni of Copenhagen and MBZU published a [paper](https://arxiv.org/pdf/2302.06555) “Do Vision and Language Models Share Concepts? A Vector Space Alignment Study”. Large-scale pretrained language models (LMs) are said to `lack the ability to connect utterances to the world’ (Bender and Koller, 2020), because they do not have ‘mental models of the world’ (Mitchell and Krakauer, 2023). If so, one would expect LM representations to be unrelated to representations induced by vision models. The paper presents an empirical evaluation across four families of LMs (BERT, GPT-2, OPT and LLaMA-2) and three vision model architectures (ResNet, SegFormer, and MAE). The experiments show that LMs partially converge towards representations isomorphic to those of vision models, subject to dispersion, polysemy and frequency. This has important implications for both multi-modal processing and the LM understanding debate (Mitchell and Krakauer, 2023).

16. ***Google Study on Scalable Oversight<br>
Google's paper explores scalable oversight protocols to enable accurate human supervision of superhuman AI. Comparing debate, consultancy, and direct question-answering, the study finds that debate generally outperforms consultancy and direct QA in specific tasks, particularly those with information asymmetry. Allowing AI agents to choose their stance in debates reduces the chances of judges being convinced by incorrect answers.*** <br><br>
    Jul 5, Google published a [paper](https://arxiv.org/pdf/2407.04622) “On scalable oversight with weak LLMs judging strong LLMs”. Scalable oversight protocols aim to enable humans to accurately supervise superhuman AI. This paper studies debate, where two AI's compete to convince a judge; consultancy, where a single AI tries to convince a judge that asks questions; and compare to a baseline of direct question-answering, where the judge just answers outright without the AI. The authors use large language models (LLMs) as both AI agents and as stand-ins for human judges, taking the judge models to be weaker than agent models. The authors benchmark on a diverse range of asymmetries between judges and agents, extending previous work on a single extractive QA task with information asymmetry, to also include mathematics, coding, logic and multimodal reasoning asymmetries. The paper finds that debate outperforms consultancy across all tasks when the consultant is randomly assigned to argue for the correct/incorrect answer. Comparing debate to direct question answering, the results depend on the type of task: in extractive QA tasks with information asymmetry debate outperforms direct question answering, but in other tasks without information asymmetry the results are mixed. Previous work assigned debaters/consultants an answer to argue for. When the authors allow them to instead choose which answer to argue for, the study finds judges are less frequently convinced by the wrong answer in debate than in consultancy. Further, the paper finds that stronger debater models increase judge accuracy, though more modestly than in previous studies.

18. ***New RNN Layer with Linear Complexity<br>
Stanford, UC San Diego, UC Berkeley, and Meta propose a new sequence modeling layer with linear complexity and an expressive hidden state, named Test-Time Training (TTT) layers. TTT layers update the hidden state via self-supervised learning even during test sequences. The study shows that TTT-Linear and TTT-MLP models outperform traditional RNNs and Transformers in handling long contexts, pointing towards future research potential.*** <br><br>
    Jul 5, Stanford Uni., UC San Diego, UC Berkeley and Meta published a [paper](https://arxiv.org/pdf/2407.04620) “Learning to (Learn at Test Time): RNNs with Expressive Hidden States”. Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. The authors propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, the layers are called Test-Time Training (TTT) layers. The researchers consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. The study evaluates the instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.

20. ***MJ-Bench for Evaluating Multimodal Judges<br>
A collaborative study introduces MJ-Bench to evaluate multimodal judges used for text-to-image generation models. It benchmarks judges on alignment, safety, image quality, and bias, revealing that closed-source VLMs like GPT-4o provide superior feedback. The findings suggest that VLM judges offer more accurate and stable feedback in natural language compared to numerical scales.*** <br><br>
    Jul 5, UNC-Chapel Hill, Uni of Chicago, Stanford Uni, Duke Uni and 13 institutes published a [paper](https://arxiv.org/pdf/2407.04842) “MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?”. While text-to-image models like DALLE-3 and Stable Diffusion are rapidly proliferating, they often encounter challenges such as hallucination, bias, and the production of unsafe, low-quality output. To effectively address these issues, it is crucial to align these models with desired behaviors based on feedback from a multimodal judge. Despite their significance, current multimodal judges frequently undergo inadequate evaluation of their capabilities and limitations, potentially leading to misalignment and unsafe fine-tuning outcomes. To address this issue, the paper introduces MJ-Bench, a novel benchmark which incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. Specifically, the paper evaluates a large variety of multimodal judges including smaller-sized CLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and close-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of a preference dataset. Experiments reveal that close-source VLMs generally provide better feedback, with GPT-4o outperforming other judges in average. Compared with open-source VLMs, smaller-sized scoring models can provide better feedback regarding text-image alignment and image quality, while VLMs provide more accurate feedback regarding safety and generation bias due to their stronger reasoning capabilities. Further studies in feedback scale reveal that VLM judges can generally provide more accurate and stable feedback in natural language (Likert-scale) than numerical scales. Notably, human evaluations on end-to-end fine-tuned models using separate feedback from these multimodal judges provide similar conclusions, further confirming the effectiveness of MJ-Bench. All data, code, models are available at https://huggingface.co/MJ-Bench.

22. ***SWiM Framework for Long Context Models<br>
Snorkel AI and the University of Wisconsin-Madison propose SWiM, an evaluation framework for long context models in LLMs. The study identifies the "lost-in-the-middle" effect where performance degrades with information in the middle of the context window. It introduces medoid voting as an effective training-free method to mitigate this effect, showing a significant improvement in single document QA tasks.*** <br><br>
    Jul 4, Snorkel AI and Uni of Wisconsin-Madison published a [paper](https://arxiv.org/pdf/2407.03651) “Evaluating Language Model Context Windows: A "Working Memory" Test and Inference-time Correction”. Large language models are prominently used in real-world applications, often tasked with reasoning over large volumes of documents. An exciting development in this space is models boasting extended context capabilities, with some accommodating over 2 million tokens. Such long context model capabilities remain uncertain in production systems, motivating the need to benchmark their performance on real world use cases. The paper addresses this challenge by proposing SWiM, an evaluation framework that addresses the limitations of standard tests. Testing the framework on eight long context models, the paper finds that even strong models such as GPT-4 and Claude 3 Opus degrade in performance when information is present in the middle of the context window (lost-in-the-middle effect). Next, in addition to the benchmark, the paper proposes medoid voting, a simple, but effective training-free approach that helps alleviate this effect, by generating responses a few times, each time randomly permuting documents in the context, and selecting the medoid answer. The authors evaluate medoid voting on single document QA tasks, achieving up to a 24% lift in accuracy.

24. ***ChartGemma for Chart Understanding<br>
York University, MILA, Salesforce, and Nanyang Technological University present ChartGemma, a model for understanding and reasoning with charts. Trained on instruction-tuning data from chart images, ChartGemma captures visual trends and patterns, achieving state-of-the-art results in chart summarization, question answering, and fact-checking. The model offers realistic and factually correct summaries of real-world charts.*** <br><br>
    Jul 4, York Uni., MILA, Saleforce, and Nanyang Tech Uni. published a [paper](https://arxiv.org/pdf/2407.04172) “ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild”. Given the ubiquity of charts as a data analysis, visualization, and decision-making tool across industries and sciences, there has been a growing interest in developing pre-trained foundation models as well as general purpose instruction-tuned models for chart understanding and reasoning. However, existing methods suffer crucial drawbacks across two critical axes affecting the performance of chart representation models: they are trained on data generated from underlying data tables of the charts, ignoring the visual trends and patterns in chart images, and use weakly aligned vision-language backbone models for domain-specific training, limiting their generalizability when encountering charts in the wild. The paper addresses these important drawbacks and introduces ChartGemma, a novel chart understanding and reasoning model developed over PaliGemma. Rather than relying on underlying data tables, ChartGemma is trained on instruction-tuning data generated directly from chart images, thus capturing both high-level trends and low-level visual information from a diverse set of charts. The simple approach achieves state-of-the-art results across 5 benchmarks spanning chart summarization, question answering, and fact-checking, and the elaborate qualitative studies on real-world charts show that ChartGemma generates more realistic and factually correct summaries compared to its contemporaries. The authors release the code, model checkpoints, dataset, and demos at https://github.com/vis-nlp/ChartGemma.

26. ***BM25S: Efficient Lexical Search<br>
McGill University introduces BM25S, an efficient implementation of BM25 for lexical search that achieves up to 500x speedup by precomputing and storing BM25 scores in sparse matrices. This approach significantly outperforms existing Python and Java-based implementations, although it may encounter performance limitations and out-of-memory errors.*** <br><br>
    Jul 4, McGill Uni published a [paper](https://arxiv.org/pdf/2407.03618) “BM25S: Orders of magnitude faster lexical search via eager sparse scoring”. The paper introduces BM25S, an efficient Python-based implementation of BM25 that only depends on Numpy and Scipy. BM25S achieves up to a 500x speedup compared to the most popular Python-based framework by eagerly computing BM25 scores during indexing and storing them into sparse matrices. It also achieves considerable speedups compared to highly optimized Java-based implementations, which are used by popular commercial products. Finally, BM25S reproduces the exact implementation of five BM25 variants based on Kamphuis et al. (2020) by extending eager scoring to non-sparse variants using a novel score shifting method. Limitations of the work include it may not achieve the highest possible performance, and may terminate with OOM errors. The code can be found at https://github.com/xhluca/bm25s

28. ***Holistic Evaluation of Multimodal Models<br>
CMU's paper on HEMM proposes a framework for evaluating multimodal foundation models across basic skills, information flow, and real-world use cases. The study identifies challenges and trends in multimodal modeling, offering insights into the impacts of data and model scale, and the benefits of instruction tuning for future multimodal models.*** <br><br>
    Jul 3, CMU published a [paper](https://arxiv.org/pdf/2407.03418) “HEMM: Holistic Evaluation of Multimodal Foundation Models”. Multimodal foundation models that can holistically process text alongside images, video, audio, and other sensory modalities are increasingly used in a variety of real-world applications. However, it is challenging to characterize and study progress in multimodal foundation models, given the range of possible modeling decisions, tasks, and domains. This paper introduces Holistic Evaluation of Multimodal Models (HEMM) to systematically evaluate the capabilities of multimodal foundation models across a set of 3 dimensions: basic skills, information flow, and real-world use cases. Basic multimodal skills are internal abilities required to solve problems, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and the ability to handle external knowledge. Information flow studies how multimodal content changes during a task through querying, translation, editing, and fusion. Use cases span domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and human-computer interaction applications. Through comprehensive experiments across the 30 tasks in HEMM, the authors (1) identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today's models, and (2) distill performance trends regarding how different modeling dimensions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence performance. The conclusions regarding challenging multimodal interactions, use cases, and tasks requiring reasoning and external knowledge, the benefits of data and model scale, and the impacts of instruction tuning yield actionable insights for future work in multimodal foundation models. [Here is the project link](https://github.com/pliang279/HEMM).

30. ***LLMs in Time Series Forecasting<br>
Research by the University of Virginia and University of Washington questions the utility of LLMs in time series forecasting. Ablation studies show that removing the LLM component often improves performance. The study concludes that LLMs do not enhance sequential dependency representation or few-shot learning in time series tasks compared to simpler models.*** <br><br>
    Jun 22, Uni of Virginia and Uni of Washington published a [paper](https://arxiv.org/pdf/2406.16964) “Are Language Models Actually Useful for Time Series Forecasting”. Large language models (LLMs) are being applied to time series tasks, particularly time series forecasting. However, are language models actually useful for time series? After a series of ablation studies on three recent and popular LLM-based time series forecasting methods, the authors find that removing the LLM component or replacing it with a basic attention layer does not degrade the forecasting results -- in most cases the results even improved. The authors also find that despite their significant computational cost, pretrained LLMs do no better than models trained from scratch, do not represent the sequential dependencies in time series, and do not assist in few-shot settings. Additionally, the authors explore time series encoders and reveal that patching and attention structures perform similarly to state-of-the-art LLM-based forecasters.

32. ***Flexibility of Neural Networks<br>
NYU, University of Maryland, Capital One, and Meta investigate the practical flexibility of neural networks. The findings reveal that standard optimizers limit the model's ability to fit training data, convolutional networks are more parameter-efficient, and stochastic training with SGD finds more fitting minima. The study offers insights into the relationship between network architecture and generalization.*** <br><br>
    Jun 17, NYU, Uni of Maryland, Capital One and Meta’s LeCun published a [paper](https://arxiv.org/pdf/2406.11463) “Just How Flexible are Neural Networks in Practice?”. It is widely believed that a neural network can fit a training set containing at least as many samples as it has parameters, underpinning notions of overparameterized and underparameterized models. In practice, however, we only find solutions accessible via our training procedure, including the optimizer and regularizers, limiting flexibility. Moreover, the exact parameterization of the function class, built into an architecture, shapes its loss surface and impacts the minima we find. This work examines the ability of neural networks to fit data in practice. The findings of the work indicate that: (1) standard optimizers find minima where the model can only fit training sets with significantly fewer samples than it has parameters; (2) convolutional networks are more parameter-efficient than MLPs and ViTs, even on randomly labeled data; (3) while stochastic training is thought to have a regularizing effect, SGD actually finds minima that fit more training data than full-batch gradient descent; (4) the difference in capacity to fit correctly labeled and incorrectly labeled samples can be predictive of generalization; (5) ReLU activation functions result in finding minima that fit more data despite being designed to avoid vanishing and exploding gradients in deep architectures.

34. ***ZeRO++ for Efficient Model Training<br>
Microsoft, OpenAI, Snowflake, University of Houston, and University of Nevada introduce ZeRO++, an optimizer that significantly reduces communication overhead in large model training. By implementing techniques like low-precision all-gather and data remapping, ZeRO++ achieves up to 2.16x better throughput and maintains accuracy comparable to original ZeRO. The model's natural weight-quantization also facilitates efficient inference.*** <br><br>
    Mar 9,  Microsoft, OpenAI, Snowflake, Uni of Houston, and Nui of Nevada published a [paper](https://openreview.net/pdf?id=gx2BT0a9MQ) on ICLR2024 “ZeRO++: Extremely Efficient Collective Communication for Large Model Training”. Zero Redundancy Optimizer (ZeRO) has been used to train a wide range of large language models on massive GPU clusters due to its ease of use, efficiency, and good scalability. However, when training on low-bandwidth clusters, and/or when small batch size per GPU is used, ZeRO’s effective throughput is limited by communication overheads. To alleviate this limitation, this paper introduces ZeRO++ composing of three communication volume reduction techniques (lowprecision all-gather, data remapping, and low-precision gradient averaging) to significantly reduce the communication volume up to 4x that enables up to 2.16x better throughput at 384 GPU scale. Experimental results also show ZeRO++ can speedup the RLHF training by 3.3x compared to vanilla ZeRO. To verify the convergence of ZeRO++, the authors test up to 13B model for pretraining with 8/6-bits all gather and up to 30B model for finetuning with 4/2-bits all gather, and demonstrate on-par accuracy as original ZeRO (aka standard training). As a byproduct, the model trained with ZeRO++ is naturally weight-quantized, which can be directly used for inference without post-training quantization or quantization-aware training.
 <br><br><br>

***7 Jul 2024***

1. ***Highlighting AI Companies' Data Vulnerability <br>
A recent breach at OpenAI, which affected only an employee discussion forum, underscores that AI companies are prime targets for hackers. AI firms possess three types of valuable data: high-quality training data, bulk user interactions, and sensitive customer data. The complexity of AI systems increases breach risks, highlighting the need for robust security practices and continuous vigilance.*** <br><br>
   Jul 5, according to [techcrunch.com](https://techcrunch.com/2024/07/05/openai-breach-is-a-reminder-that-ai-companies-are-treasure-troves-for-hackers/), A recent breach at OpenAI was minor, affecting only an employee discussion forum, but it underscores that AI companies are prime targets for hackers. OpenAI and similar companies hold three types of highly valuable data: 1) High-Quality Training Data: Secretive, labor-intensive, and crucial for AI model development. 2) Bulk User Interactions: Billions of ChatGPT conversations offering deep insights, more valuable than simple search data. 3) Customer Data: Sensitive information from companies using AI tools, which often includes internal databases and proprietary information. AI companies handle significant industrial secrets, necessitating robust security measures. Despite their capabilities, the novelty and complexity of AI increase the risk of breaches. Good security practices are essential, but the constantly evolving threat landscape, now amplified by AI, requires continuous vigilance. While there's no need for immediate panic, the breach highlights the unique and substantial risks AI companies face. Enhanced security and awareness are crucial as they continue to manage vast amounts of sensitive data.

3. ***Kyutai's Voice-Enabled AI Chat-Bot <br>
French start-up Kyutai introduced Moshi, an AI model with advanced vocal capabilities. Developed in six months by a team of eight, Moshi facilitates natural and expressive AI communication. It has potential applications as a coach or companion and excels in text-to-speech. Kyutai plans to share Moshi’s code and model weights for open research and development.*** <br><br>
   Jul 3, according to [kyutai.org](https://kyutai.org/cp_moshi.pdf), the French AI start-up unveiled its first voice-enabled AI chat-bot which is openly accessible to all. In just 6 months, with a team of 8, the Kyutai research lab developed from scratch an artificial intelligence (AI) model with unprecedented vocal capabilities called Moshi. The interactive demo of the AI will be accessible from the Kyutai website. This new type of technology makes it possible for the first time to communicate in a smooth, natural and expressive way with an AI. During the presentation, the Kyutai team interacted with Moshi to illustrate its potential as a coach or companion for example, and its creativity through the incarnation of characters in roleplays. More broadly, Moshi has the potential to revolutionize the use of speech in the digital world. For instance, its text-to-speech capabilities are exceptional in terms of emotion and interaction between multiple voices. Moshi can also be installed locally and therefore run safely on an unconnected device. With Moshi, Kyutai intends to contribute to open research in AI and to the development of the entire ecosystem. The code and weights of the models will soon be freely shared, which is also unprecedented for such technology. They will be useful both to researchers in the field and to developers working on voice-based products and services.

5. ***Advancements in Vision Language Models <br>
InternLM-XComposer-2.5 (IXC-2.5), developed by SAIL, CUHK, SenseTime Group, and Tsinghua University, is a versatile vision language model supporting long-contextual input and output. IXC-2.5 features ultra-high resolution understanding, fine-grained video understanding, and multi-turn multi-image dialogue. It outperforms existing models on multiple benchmarks and is publicly available.*** <br><br>
   Jul 3, SAIL, CUHK, SenseTime Group and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2407.03320) “InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output”. The paper presents InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output. IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output contexts. Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In addition to comprehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks. The InternLM-XComposer-2.5 is publicly available at https://github.com/InternLM/InternLM-XComposer.

7. ***Unsafe Information Leakage in AI Responses <br>
Researchers from the University of Toronto, Vector Institute, and University of Oxford highlight vulnerabilities in large language models (LLMs) to jailbreak attacks. Current defenses are insufficient, and the paper introduces inferential adversaries and an information censorship criterion to ensure safety, revealing an intrinsic safety-utility trade-off.*** <br><br>
   Jul 2, Uni of Toronto, Vector Institute and Uni of Oxford published a [paper](https://arxiv.org/pdf/2407.02551) “A False Sense of Safety: Unsafe Information Leakage in 'Safe' AI Responses”. Large Language Models (LLMs) are vulnerable to jailbreaks–methods to elicit harmful or generally impermissible outputs. Safety measures are developed and assessed on their effectiveness at defending against jailbreak attacks, indicating a belief that safety is equivalent to robustness. The authors assert that current defense mechanisms, such as output filters and alignment fine-tuning, are, and will remain, fundamentally insufficient for ensuring model safety. These defenses fail to address risks arising from dual-intent queries and the ability to composite innocuous outputs to achieve harmful goals. To address this critical gap, the paper introduces an information-theoretic threat model called inferential adversaries who exploit impermissible information leakage from model outputs to achieve malicious goals. The work distinguishes these from commonly studied security adversaries who only seek to force victim models to generate specific impermissible outputs. The study demonstrates the feasibility of automating inferential adversaries through question decomposition and response aggregation. To provide safety guarantees, the researchers define an information censorship criterion for censorship mechanisms, bounding the leakage of impermissible information. The paper proposes a defense mechanism which ensures this bound and reveals an intrinsic safety-utility trade-off. The work provides the first theoretically grounded understanding of the requirements for releasing safe LLMs and the utility costs involved.

9. ***RankRAG: Enhancing Context Ranking and Generation <br>
Georgia Tech and Nvidia propose RankRAG, a framework that instruction-tunes LLMs for context ranking and answer generation in retrieval-augmented generation (RAG). RankRAG outperforms existing models on various benchmarks, including biomedical domains, demonstrating its superior generalization capability without additional fine-tuning.*** <br><br>
    Jul 2, Georgia Tech and Nvidia published a [paper](https://arxiv.org/pdf/2407.02485v1) “RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs”. Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). This work proposes a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, the work compares the model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, the Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains. Zero-shot QA performance over nine datasets is about 56% on average.

11. ***Accelerating LLM Inference with Sparse Attention <br>
Microsoft and the University of Surrey introduce MInference 1.0, a method to accelerate pre-filling for long-context LLMs using dynamic sparse attention. By leveraging specific attention matrix patterns, MInference reduces inference latency by up to 10x while maintaining accuracy, applicable to existing LLMs without modifications.*** <br><br>
    Jul 2, Microsoft and Uni of Surrey published a [paper](https://arxiv.org/pdf/2407.02490) “MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention”. The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, the paper introduces MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, the authors identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. The work determines the optimal pattern for each attention head offline and dynamically builds sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, the authors perform efficient sparse attention calculations via the optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. The proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, the study demonstrates that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. The code is available at https://aka.ms/MInference.

13. ***Optimizing Data Mixture for LLM Pre-training <br>
SeaAI, SMU, and others present RegMix, a method to identify high-performing data mixtures for LLM pre-training through regression. RegMix demonstrates superior performance compared to human-selected mixtures, highlighting the significant impact of data mixtures on model performance and the need for automatic approaches.*** <br><br>
    Jul 1, SeaAI, SMU, et al published a [paper](https://arxiv.org/pdf/2407.01492) “RegMix: Data Mixture as Regression for Language Model Pre-training”. The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. The authors propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix involves training a set of small models with diverse data mixtures and fitting a regression model to predict their performance given their respective mixtures. With the fitted regression model, the study simulates the top-ranked mixture and uses it to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, the authors train 512 models with 1M parameters for 1B tokens of different mixtures to fit the regression model and find the optimal mixture. Using this mixture the work trains a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which is found to perform best among 64 candidate 1B parameter models with other mixtures. Further, the method demonstrates superior performance compared to human selection and achieves results that match or surpass DoReMi, while utilizing only 10% of the compute budget. The experiments also show that (1) Data mixtures significantly impact performance with single-task performance variations of up to 14.6%; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed; (4) Data mixture effects transcend scaling laws, and the approach captures the complexity by considering all domains together. The code is available at https://github.com/sail-sg/regmix.

15. ***Mechanistic Interpretation in Transformers <br>
UC Berkeley, MIT, and others introduce contextual decomposition for transformers (CD-T), a method for interpreting transformer models' internal mechanisms. CD-T offers computational efficiency and reliable interpretation, improving understanding and trust in model outputs compared to existing methods like SHAP and LIME.*** <br><br>
    Jul 1, UC Berkeley, MIT et al. published a [paper](https://arxiv.org/pdf/2407.00886) “Mechanistic Interpretation through Contextual Decomposition in Transformers”. Transformers exhibit impressive capabilities but are often regarded as black boxes due to challenges in understanding the complex nonlinear relationships between features. Interpreting machine learning models is of paramount importance to mitigate risks, and mechanistic interpretability is in particular of current interest as it opens up a window for guiding manual modifications and reverse-engineering solutions. This work introduces contextual decomposition for transformers (CD-T), extending a prior work on CD for RNNs and CNNs, to address mechanistic interpretation computationally efficiently. CD-T is a flexible interpretation method for transformers. It can capture contributions of combinations of input features or source internal components (e.g. attention heads, feed-forward networks) to (1) final predictions or (2) the output of any target internal component. Using CD-T, the authors propose a novel algorithm for circuit discovery. On a real-world pathology report classification task: the paper shows CD-T distills a more faithful circuit of attention heads with improved computational efficiency (speed up 2x) than a prior benchmark, path patching. As a versatile interpretation method, CD-T also exhibits exceptional capabilities for local interpretations. CD-T is shown to reliably find words and phrases of contrasting sentiment/topic on SST-2 and AGNews datasets. Through human experiments, the paper demonstrates CD-T enables users to identify the more accurate of two models and to better trust a model's outputs compared to alternative interpretation methods such as SHAP and LIME.

17. ***Universal Approach for Sentence Segmentation <br>
Researchers from Johannes Kepler University and the University of Cambridge introduce Segment Any Text (SaT), a model for robust, adaptable, and efficient sentence segmentation. SaT outperforms existing methods across diverse domains and languages, providing a universal solution for text segmentation challenges.*** <br><br>
    Jun 30, Johannes Kepler Uni and Uni of Cambridge published a [paper](https://arxiv.org/pdf/2406.16678) “Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation”. Segmenting text into sentences plays an early and crucial role in many NLP systems. This is commonly achieved by using rule-based or statistical methods relying on lexical features such as punctuation. Although some recent works no longer exclusively rely on punctuation, it is found that no prior method achieves all of (i) robustness to missing punctuation, (ii) effective adaptability to new domains, and (iii) high efficiency. The paper introduces a new model - Segment any Text (SaT) - to solve this problem. To enhance robustness, the work proposes a new pretraining scheme that ensures less reliance on punctuation. To address adaptability, the paper introduces an extra stage of parameter-efficient fine-tuning, establishing state-of-the-art performance in distinct domains such as verses from lyrics and legal documents. Along the way, the work introduces architectural modifications that result in a threefold gain in speed over the previous state of the art and solve spurious reliance on context far in the future. Finally, the work introduces a variant of the model with fine-tuning on a diverse, multilingual mixture of sentence-segmented data, acting as a drop-in replacement and enhancement for existing segmentation tools. Overall, the contributions provide a universal approach for segmenting any text. The method outperforms all baselines - including strong LLMs - across 8 corpora spanning diverse domains and languages, especially in practically relevant situations where text is poorly formatted. The models and code, including documentation, are available at [this https URL](https://huggingface.co/segment-any-text) under the MIT license.

19. ***Token Erasure in LLMs <br>
Northeastern University explores how LLMs process multi-token words and named entities through a "token erasure" effect. This study reveals how LLMs convert arbitrary token groups into meaningful representations, providing insights into the implicit vocabulary of LLMs.*** <br><br>
    Jun 28, Northeastern Uni published a [paper](https://arxiv.org/pdf/2406.20086) “Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs”. LLMs process text as sequences of tokens that roughly correspond to words, where less common words are represented by multiple tokens. However, individual tokens are often semantically unrelated to the meanings of the words/concepts they comprise. For example, Llama-2-7b's tokenizer splits the word "northeastern" into the tokens ['_n', 'ort', 'he', 'astern'], none of which correspond to semantically meaningful units like "north" or "east." Similarly, the overall meanings of named entities like "Neil Young" and multi-word expressions like "break a leg" cannot be directly inferred from their constituent tokens. Mechanistically, how do LLMs convert such arbitrary groups of tokens into useful higher-level representations? This work finds that last token representations of named entities and multi-token words exhibit a pronounced "erasure" effect, where information about previous and current tokens is rapidly forgotten in early layers. Using this observation, the authors propose a method to "read out" the implicit vocabulary of an autoregressive LLM by examining differences in token representations across layers, and present results of this method for Llama-2-7b and Llama-3-8B. To the authors knowledge, this is the first attempt to probe the implicit vocabulary of an LLM.

21. ***Zuckerberg on AI Development <br>
Mark Zuckerberg advocates for diverse AI models instead of a single overarching AI. He emphasizes the importance of businesses and creators developing their own AI, reflecting varied interests and avoiding the concentration of AI capabilities in one entity.*** <br><br>
    Jun 28, according to [Entrepreneur](https://www.entrepreneur.com/business-news/mark-zuckerberg-reveals-the-future-meta-ai-tech-industry/476368), Mark Zuckerberg Sounds Off on Developing AI: 'I Don't Think AI Technology Is a Thing That Should Be Hoarded'. In a talk with Kallaway, Zuckerberg said that the most effective use of AI in the future is to have businesses and creators create their own AI instead of focusing on one big overarching model. "It's almost as if they think they're creating God or something, and that's just not what we're doing". Zuckerberg spoke about most major companies and their desire to build one main AI, using Google's Gemini or OpenAI's Chat GPT as examples. But for Meta, the strategy isn't to develop one central AI — the company wants to create multiple programs. "Our overall view is that this isn't the type of thing that there should just be one of, people want to interact with lots of different people and businesses and there need to be a lot of different AIs that get created to reflect people's different interests," he explained.

23. ***Unsupervised Segmentation Model <br>
UC Berkeley introduces Unsupervised SAM (UnSAM), a model for whole-image segmentation without human annotations. Using a hierarchical structure discovery approach, UnSAM achieves competitive results with supervised models, offering an effective solution for unsupervised segmentation tasks.*** <br><br>
    Jun 28, UC Berkeley published a [paper](https://arxiv.org/pdf/2406.20081v1) “Segment Anything without Supervision”. The Segmentation Anything Model (SAM) requires labor-intensive data labeling. The paper presents Unsupervised SAM (UnSAM) for promptable and automatic wholeimage segmentation that does not require human annotations. UnSAM utilizes a divide-and-conquer strategy to “discover” the hierarchical structure of visual scenes. The study first leverages top-down clustering methods to partition an unlabeled image into instance/semantic level segments. For all pixels within a segment, a bottom-up clustering method is employed to iteratively merge them into larger groups, thereby forming a hierarchical structure. These unsupervised multi-granular masks are then utilized to supervise model training. Evaluated across seven popular datasets, UnSAM achieves competitive results with the supervised counterpart SAM, and surpasses the previous state-of-the-art in unsupervised segmentation by 11% in terms of AR. Moreover, the work shows that supervised SAM can also benefit from the self-supervised labels. By integrating the unsupervised pseudo masks into SA-1B’s ground-truth masks and training UnSAM with only 1% of SA-1B, a lightly semisupervised UnSAM can often segment entities overlooked by supervised SAM, exceeding SAM’s AR by over 6.7% and AP by 3.9% on SA-1B.

25. ***Leveraging Internal Knowledge for Complex Reasoning <br>
KAIST and the University of Richmond investigate how LLMs utilize internal knowledge for complex reasoning. The study proposes DepthQA, a dataset for deconstructing complex questions, highlighting the importance of structured intermediate steps in enhancing LLMs' reasoning abilities.*** <br><br>
    Jun 27, KAIST and Uni of Richmond published a [paper](https://arxiv.org/pdf/2406.19502) “Investigating How Large Language Models Leverage Internal Knowledge to Perform Complex Reasoning”. Despite significant advancements, there is a limited understanding of how large language models (LLMs) utilize knowledge for reasoning. To address this, the paper proposes a method that deconstructs complex real-world questions into a graph, representing each question as a node with parent nodes of background knowledge needed to solve the question. The word develops the DepthQA dataset, deconstructing questions into three depths: (i) recalling conceptual knowledge, (ii) applying procedural knowledge, and (iii) analyzing strategic knowledge. Based on a hierarchical graph, the authors quantify forward discrepancy, discrepancies in LLMs' performance on simpler sub-problems versus complex questions. The work also measures backward discrepancy, where LLMs answer complex questions but struggle with simpler ones. The analysis shows that smaller models have more discrepancies than larger models. Additionally, guiding models from simpler to complex questions through multi-turn interactions improves performance across model sizes, highlighting the importance of structured intermediate steps in knowledge reasoning. This work enhances the understanding of LLM reasoning and suggests ways to improve their problem-solving abilities.

27. ***Limitations of Unlearning in Content Regulation <br>
Google examines the limitations of unlearning as a method for content regulation in LLMs. The study introduces "ununlearning," where unlearned knowledge can be reintroduced during inference, emphasizing the need for content filtering to ensure effective regulation.*** <br><br>
    Jun 27, Google published a [paper](https://arxiv.org/pdf/2407.00106) “UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI”. Exact unlearning was first introduced as a privacy mechanism that allowed a user to retract their data from machine learning models on request. Shortly after, inexact schemes were proposed to mitigate the impractical costs associated with exact unlearning. More recently unlearning is often discussed as an approach for removal of impermissible knowledge i.e. knowledge that the model should not possess such as unlicensed copyrighted, inaccurate, or malicious information. The promise is that if the model does not have a certain malicious capability, then it cannot be used for the associated malicious purpose. This paper revisits the paradigm in which unlearning is used for in Large Language Models (LLMs) and highlight an underlying inconsistency arising from in-context learning. Unlearning can be an effective control mechanism for the training phase, yet it does not prevent the model from performing an impermissible act during inference. The authors introduce a concept of ununlearning, where unlearned knowledge gets reintroduced in-context, effectively rendering the model capable of behaving as if it knows the forgotten knowledge. As a result, the authors argue that content filtering for impermissible knowledge will be required and even exact unlearning schemes are not enough for effective content regulation. The paper discusses feasibility of ununlearning for modern LLMs and examine broader implications.

29. ***Meta 3D Gen: Advanced 3D Asset Generation <br>
Meta introduces 3DGen, a state-of-the-art pipeline for text-to-3D asset generation. 3DGen excels in prompt fidelity and visual quality, supporting high-quality 3D shapes and textures in under a minute, outperforming industry baselines.*** <br><br>
    Jun 25, Meta published a [paper](https://scontent.fcbr1-1.fna.fbcdn.net/v/t39.2365-6/449707112_509645168082163_2193712134508658234_n.pdf?_nc_cat=111&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=tlSuCzsxrocQ7kNvgHDhUqB&_nc_ht=scontent.fcbr1-1.fna&oh=00_AYDZlYUAcKARsxlM8vnT6D3vG_iobn2IbT4Za8o92Vmk3w&oe=668FCED1) “Meta 3D Gen”. The paper introduces Meta 3D Gen (3DGen), a new state-of-the-art, fast pipeline for text-to-3D asset generation. 3DGen offers 3D asset creation with high prompt fidelity and high-quality 3D shapes and textures in under a minute. It supports physically-based rendering (PBR), necessary for 3D asset relighting in real-world applications. Additionally, 3DGen supports generative retexturing of previously generated (or artist-created) 3D shapes using additional textual inputs provided by the user. 3DGen integrates key technical components, Meta 3D AssetGen and Meta 3D TextureGen, that is developed for text-to-3D and text-to-texture generation, respectively. By combining their strengths, 3DGen represents 3D objects simultaneously in three ways: in view space, in volumetric space, and in UV (or texture) space. The integration of these two techniques achieves a win rate of 68% with respect to the single-stage model. The authors compare 3DGen to numerous industry baselines, and show that it outperforms them in terms of prompt fidelity and visual quality for complex textual prompts, while being significantly faster.

31. ***Critical View on AI Hallucinations <br>
Researchers from the University of Glasgow critique large language models for producing inaccurate outputs, likening them to "bullshit" as defined by Frankfurt. The study argues that understanding AI misrepresentations in this way is more useful for predicting and discussing their behavior.*** <br><br>
    Jun 8, Ethics and Information Technology published a [paper](https://link.springer.com/article/10.1007/s10676-024-09775-5) by researchers from Uni of Glasgow “ChatGPT is bullshit”. Recently, there has been considerable interest in large language models: machine learning systems which produce humanlike text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called “AI hallucinations”. The authors argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. The researchers distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. The authors further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.

33. ***Human Expectations of LLM Performance <br>
Harvard, MIT, and the University of Chicago explore how humans generalize LLM performance across tasks. The study reveals that human generalization can be predicted using NLP methods, showing discrepancies between human expectations and actual LLM performance, especially in high-cost mistake scenarios.*** <br><br>
    Jun 6, Harvard Uni, MIT, and Uni of Chicago published a [paper](https://arxiv.org/pdf/2406.01382) accepted by ICML 2024 “Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function”. What makes large language models (LLMs) impressive is also what makes them hard to evaluate: their diversity of uses. To evaluate these models, people must understand the purposes they will be used for. The authors consider a setting where these deployment decisions are made by people, and in particular, people's beliefs about where an LLM will perform well. The paper models such beliefs as the consequence of a human generalization function: having seen what an LLM gets right or wrong, people generalize to where else it might succeed. The study collects a dataset of 19K examples of how humans make generalizations across 79 tasks from the MMLU and BIG-Bench benchmarks. The work shows that the human generalization function can be predicted using NLP methods: people have consistently structured ways to generalize. The authors then evaluate LLM alignment with the human generalization function. The results show that -- especially for cases where the cost of mistakes is high -- more capable models (e.g. GPT-4) can do worse on the instances people choose to use them for, exactly because they are not aligned with the human generalization function.

35. ***Impact of ChatGPT on Human Skills <br>
A study published in Technological Forecasting & Social Change analyzes the impact of ChatGPT on human skills using Twitter data. The study identifies four emerging skills for using ChatGPT and highlights the significant impact of the technology on various human skills.*** <br><br>
    Jun 5, Technological Forecasting & Social Change published a [paper](https://www.sciencedirect.com/science/article/pii/S0040162524001859) “The impact of ChatGPT on human skills: A quantitative study on twitter data”. The novel generative Artificial Intelligence (AI) developed by OpenAI, i.e., ChatGPT, rised a great interest in both scientific and business contexts. This new wave of technological advancement typically produces deep transformation in the workplace, requiring new skills. However, none of the studies in literature provide quantitative analysis and measures on the impact of ChatGPT on human skills. To address this gap, the paper collected a database of 616,073 tweets about ChatGPT, and used Natural Language Processing techniques to identify the tasks users requested ChatGPT to perform, and the sentiment related to these tasks. Then, the work compared these tasks with a standard taxonomy of skills (i.e., ESCO) using BERT. The results of the study underline that ChatGPT impacts 185 different skills. Moreover, the researchers proposed a model to represent the interaction of the user and ChatGPT, useful to define four skills which are emerging for using this new technology. The four skills are: defining task goals; using prompt language; knowing the principles of generative AI; and conducting performance measurement.
 
