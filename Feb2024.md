# Weekly AI-News - Feb 2024
Welcome to the Weekly-AI-news!
I mainly focuses more on Generative AI, LLMs and the related, so may miss import AI news in other areas. <br><br>

**25 Feb 2024**

1. ***Gemini Image Generation Issues:***
* Google's conversational app, Gemini, faced problems with its image generation feature, generating inaccurate and offensive images.
* Identified issues: Failure to show diversity for specific prompts, and the model being overly cautious and refusing certain prompts.
* Actions taken: Temporary disablement of image generation, commitment to significant improvement before re-enabling.
* Lessons learned: Gemini may not be reliable for sensitive topics; AI systems require constant improvement; reliance on Google Search for factual information is advised.<br><br>
   23 Feb, according to a [Google blog](https://blog.google/products/gemini/gemini-image-generation-issue/) “Gemini image generation got it wrong. We'll do better”. Gemini, a conversational app, launched an image generation feature that included people. The feature generated inaccurate and offensive images in some cases. Two main issues were identified: 1) Tuning to show diverse people failed for specific prompts (e.g., "Black teacher"). 2) The model became overly cautious and refused certain prompts entirely. Actions taken include 1) Image generation of people in Gemini is temporarily disabled. 2) The feature will be significantly improved before being re-enabled. Some lessons learned are 1) Gemini may not be reliable for generating images or text about sensitive topics. 2) AI systems are prone to mistakes and require constant improvement. 3) Rely on Google Search for factual information on current events. 4) Google is committed to rolling out AI technology safely and responsibly. The blog also indicates that this was an unintentional issue and not the intended behavior of the feature, Google is taking steps to prevent similar issues from happening again and users should be aware of the limitations of AI technology.

2. ***Amazon's Warning on AI Tools:***<br>
- Amazon advises its employees against using third-party generative AI tools for work due to confidentiality concerns.
- Employees can use such tools for personal use but should avoid them for confidential work.
- Concerns about ownership claims over inputs by generative AI services are highlighted.
- Amazon emphasizes safeguards for employee use of generative AI technologies.<br><br>
   23 Feb, according to [analyticsindiamag](https://analyticsindiamag.com/amazon-warns-employees-not-to-use-generative-ai-tools/), Amazon has advised its employees against using third-party generative AI tools for work, citing concerns about confidentiality. Internal memos state that while employees may use such tools for personal purposes, they should avoid using them for confidential Amazon work and refrain from sharing proprietary information. The company's policy warns that generative AI services may claim ownership over inputs, including emails, documentation, and strategy materials. Amazon joins other major companies like Samsung and Apple in restricting the use of tools like ChatGPT internally, with some concerns related to competitors like Microsoft, which invested in OpenAI. Amazon's spokesperson, Adam Montgomery, stated that the company has safeguards in place for employee use of generative AI technologies and emphasizes the importance of protecting confidential information.

3. ***Stability.AI's Stable Diffusion 3:***<br>
- Stability.AI released Stable Diffusion 3, a text-to-image model with improved performance in prompts, image quality, and spelling.
- Not broadly available yet, a waitlist is opening for users.
- Aims to offer adaptable solutions for creativity, aligning with the mission to activate humanity's potential.<br><br>
   22 Feb, Stability.AI released [Stable Diffusion 3](https://stability.ai/news/stable-diffusion-3), Stablity.AI’s most capable text-to-image model with greatly improved performance in multi-subject prompts, image quality, and spelling abilities.  Stabile Diffusion 3 is still not broadly available but a waitlist is opening for all users. Stable Diffusion 3 aims to offer adaptable solutions that enable individuals, developers, and enterprises to unleash their creativity, aligning with our mission to activate humanity’s potential.

4. ***HKUST's Subobject-level Image Tokenization:***<br>
- HKUST published a [paper](https://arxiv.org/pdf/2402.14327.pdf) introducing subobject-level image tokenization as an alternative to traditional patch-level tokenization.
- Proposes a system using Sequence-to-sequence AutoEncoder for efficient learning of image translation.
- Results show significant improvements over traditional patch-level tokenization.<br><br>
   22 Feb, HKUST published a paper “Subobject-level Image Tokenization”. The authors point out that transformer-based vision models typically tokenize images into fixed-size square patches as input units, which lacks the adaptability to image content and overlooks the inherent pixel grouping structure. Inspired by the subword tokenization widely adopted in language models, we propose an image tokenizer at a subobject level, where the subobjects are represented by semantically meaningful image segments obtained by segmentation models (e.g., segment anything models). To implement a learning system based on subobject tokenization, the paper first introduced a Sequence-to-sequence AutoEncoder (SeqAE) to compress subobject segments of varying sizes and shapes into compact embedding vectors, then fed the subobject embeddings into a large language model for vision language learning. Empirical results demonstrated that the subobject-level tokenization significantly facilitates efficient learning of translating images into object and attribute descriptions compared to the traditional patch-level tokenization. Codes and models will be open-sourced at [this https URL](https://github.com/ChenDelong1999/subobjects).

5.	***OpenCodeInterpreter for Code Generation:***
- Collaboration between Uni of Waterloo, Allen Inst. Of AI, and HKUST introduces OpenCodeInterpreter for code generation.
- Addresses the limitations of open-source models by integrating code generation with execution and refinement.
- Exceptional performance compared to GPT-4's Code Interpreter; narrows the gap between open-source and proprietary systems.<br><br>
  22 Feb, Uni of Waterloo, Allen Inst. Of AI,HKUST et. al published a [paper](https://arxiv.org/pdf/2402.14658.pdf) “OpenCodeInterpreter Integrating Code Generation with Execution and Refinement”. The authors indicates that the introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, the paper introduces OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from GPT-4. OpenCodeInterpreter brings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter. Project [link is here](https://github.com/OpenCodeInterpreter/OpenCodeInterpreter).

6.	***Google and CMU's OmniPred Framework:***
- ntroduces OmniPred, a framework training language models as universal regressors for diverse real-world experiments.
- Demonstrates precise numerical regression capabilities using textual representations of mathematical parameters.
- Outperforms traditional regression models when trained over multiple tasks using data from Google Vizier.<br><br>
  22 Feb, Google and CMU published a [paper](https://arxiv.org/pdf/2402.14547.pdf) “OmniPred: Language Models as Universal Regressors”. The authors indicate that Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. This study proposes OmniPred, a framework for training language models as universal end-to-end regressors over (x,y) evaluation data from diverse real-world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, the extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.

7.	***US Department of Commerce's Request for Comment on Open-weight AI Models:***
- The NTIA is soliciting comments on open-weight AI models to assess risks, benefits, and potential policies.
- Focus on striking a balance between promoting innovation and ensuring safety in AI technology.
- Comments invited within 30 days, contributing to a report informing NTIA's findings and policy recommendations.<br><br>
 21 Feb, US Department of Commerce is [soliciting comments on open-weight AI models](https://www.commerce.gov/news/press-releases/2024/02/ntia-solicits-comments-open-weight-ai-models). The Department of Commerce’s National Telecommunications and Information Administration (NTIA) has initiated a Request for Comment on the risks, benefits, and potential policies related to advanced artificial intelligence (AI) models with widely available model weights. These "open-weight" models enable developers to build on and adapt previous work, making AI tools more accessible to small companies, researchers, nonprofits, and individuals. While this approach may accelerate AI benefits and safety research, it also raises concerns about potential harms from advanced models. The initiative, in response to President Biden's Executive Order on Artificial Intelligence, aims to assess the risks and benefits of large AI models with widely available weights and develop policy recommendations. The Request for Comment seeks public input on issues such as the levels of openness of AI models, the benefits and risks associated with open versus closed models, and considerations related to innovation, competition, safety, security, trustworthiness, equity, and national security. The comments will contribute to a report to the President, informing NTIA's findings and policy recommendations. The focus is on striking a balance between promoting innovation and ensuring safety in the rapidly evolving field of AI technology. Comments are invited within 30 days of the Request for Comment's publication in the Federal Register. The NTIA, as part of the U.S. Department of Commerce, advises the President on telecommunications and information policy issues, emphasizing areas such as broadband internet access, spectrum usage, public safety communications, and fostering innovation and economic growth on the internet.

8.	***Google's User-LLM for Contextualization:***
- Google presents User-LLM, a framework leveraging user embeddings to contextualize Large Language Models (LLMs).
- User embeddings capture latent user preferences, enabling dynamic adaptation of LLMs to user context.
- Outperforms text-prompt-based contextualization on certain tasks while being computationally efficient.<br><br>
  21 Feb, Google published a [paper](https://arxiv.org/pdf/2402.13598.pdf) “User-LLM: Efficient LLM Contextualization with User Embeddings”. The study finds that Large language models (LLMs) have revolutionized natural language processing. However, effectively incorporating complex and potentially noisy user interaction data remains a challenge. To address this, the authors propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs. These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time. The stuady integrates these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context. The comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks. Notably, the approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient. The researchers further incorporate Perceiver layers to streamline the integration between user encoders and LLMs, reducing computational demands.

9.	***Microsoft's LongRoPE Extending LLM Context Window:***
- Introduces LongRoPE, extending the context window of pre-trained LLMs to an impressive 2048k tokens.
- Achieves extension through efficient search, progressive extension strategy, and readjustment on 8k length.
- Retains original architecture with minor modifications, showcasing effectiveness across various tasks.<br><br>
  21 Feb, Microsoft publish a [paper](https://arxiv.org/pdf/2402.13753.pdf) “LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens”. The paper states that large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) identifying and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) introducing a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) readjusting LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of the suggested method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations.

10.	***Neural Network Diffusion for Parameter Generation:***
- Demonstrates that diffusion models can generate high-performing neural network parameters.
- Utilizes an autoencoder and a latent diffusion model to synthesize network parameter representations.
- Generates models with comparable or improved performance over trained networks at minimal additional cost.<br><br>
  20 Feb, UNS, UC Berkeley and Meta published a [paper](https://arxiv.org/pdf/2402.13144.pdf) “Neural Network Diffusion”. The authors indicates that Diffusion models have achieved remarkable success in image and video generation. This work demonstrates that diffusion models can also \textit{generate high-performing neural network parameters}. The proposed approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, the diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, experimental results show that the generated models perform differently with the trained networks. These results encourage more exploration on the versatile use of diffusion models.

11.	***Google DeepMind's Language Model Predictive Control:***
- Addresses limitations of Large Language Models (LLMs) in long-term interactions through Language Model Predictive Control (LMPC).
- Fine-tunes robot code-writing LLMs for improved teachability and adapts classic robotics techniques.
- LMPC framework improves non-expert teaching success rates and produces strong meta-learners across various tasks.<br><br>
  18 Feb, Google DeepMind published a [paper](https://arxiv.org/pdf/2402.11450.pdf) “Learning to Learn Faster from Human Feedback with Language Model Predictive Control”. The researchers of the paper point out that Large language models (LLMs) have been shown to exhibit a wide range of capabilities, such as writing robot code from language commands -- enabling non-experts to direct robot behaviors, modify them based on feedback, or compose them to perform new tasks. However, these capabilities (driven by in-context learning) are limited to short-term interactions, where users' feedback remains relevant for only as long as it fits within the context size of the LLM, and can be forgotten over longer interactions. This paper investigates fine-tuning the robot code-writing LLMs, to remember their in-context interactions and improve their teachability i.e., how efficiently they adapt to human inputs (measured by average number of corrections before the user considers the task successful). The key observation is that when human-robot interactions are formulated as a partially observable Markov decision process (in which human language inputs are observations, and robot code outputs are actions), then training an LLM to complete previous interactions can be viewed as training a transition dynamics model -- that can be combined with classic robotics techniques such as model predictive control (MPC) to discover shorter paths to success. This gives rise to Language Model Predictive Control (LMPC), a framework that fine-tunes PaLM 2 to improve its teachability on 78 tasks across 5 robot embodiments -- improving non-expert teaching success rates of unseen tasks by 26.9% while reducing the average number of human corrections from 2.4 to 1.9. Experiments show that LMPC also produces strong meta-learners, improving the success rate of in-context learning new tasks on unseen robot embodiments and APIs by 31.5%. See videos, code, and demos at: [this https URL](https://robot-teaching.github.io/).<br>



**18 Feb 2024**

1.	***OpenAI's Sora: Text-to-Video AI Model<br>
Released on Feb 15, Sora is a text-to-video AI model by OpenAI that generates minute-long realistic scenes from textual prompts, maintaining visual quality and user adherence. Sora uses a diffusion model and transformer architecture, akin to GPT models, with a deep understanding of language for accurate interpretation. It excels in creating complex scenes but may struggle with physics simulation, spatial details, and precise event descriptions.
The model utilizes the recaptioning technique from DALL·E 3, enhancing fidelity to user instructions in the generated video.***<br><br>
15 Feb, OpenAI release [Sora](https://openai.com/sora), a text-to-video AI model that can create minute-long realistic and imaginative scenes from text instructions while maintaining visual quality and adherence to the user’s prompt. “Sora is able to generate complex scenes with multiple characters, specific types of motion, and accurate details of the subject and background. The model understands not only what the user has asked for in the prompt, but also how those things exist in the physical world. The model has a deep understanding of language, enabling it to accurately interpret prompts and generate compelling characters that express vibrant emotions. Sora can also create multiple shots within a single generated video that accurately persist characters and visual style.” Current model may struggle with accurately simulating the physics of a complex scene, and may not understand specific instances of cause and effect. The model may also confuse spatial details of a prompt, for example, mixing up left and right, and may struggle with precise descriptions of events that take place over time, like following a specific camera trajectory. Technically, Sora is a diffusion model, which generates a video by starting off with one that looks like static noise and gradually transforms it by removing the noise over many steps. Similar to GPT models, Sora uses a transformer architecture, unlocking superior scaling performance. It builds on past research in DALL·E and GPT models. It uses the recaptioning technique from DALL·E 3, which involves generating highly descriptive captions for the visual training data. As a result, the model is able to follow the user’s text instructions in the generated video more faithfully. Learn more in the [technical report](https://openai.com/research/video-generation-models-as-world-simulators).

2.	***Google's Gemini 1.5 Announcement:<br>
Google introduces Gemini 1.5, showing significant improvements in quality compared to 1.0 Ultra while utilizing less compute power. Built on a new Mixture-of-Experts (MoE) architecture, Gemini 1.5 can process long-context, reasoning about events in vast datasets like Apollo 11 transcripts or a silent movie.
Gemini 1.5 Pro exhibits impressive "in-context learning" skills and aims to expand safety tests.***<br><br>
15 Feb, Google announced [Gemini 1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/) in a blog with title “Our next-generation model: Gemini 1.5”. According to the blog by CEO Sundar Pichai and Demis Hassabis, Gemini 1.5 shows dramatic improvements across a number of dimensions and 1.5 Pro achieves comparable quality to 1.0 Ultra, while using less compute. It’s built upon research and engineering innovations across nearly every part of Google’s foundation model development and infrastructure. This includes making Gemini 1.5 more efficient to train and serve, with a new Mixture-of-Experts (MoE) architecture. It can process long-context, starting with standard 128k, up to 1 million tokens. For example, when given the 402-page transcripts from Apollo 11’s mission to the moon, it can reason about conversations, events and details found across the document. When given a 44-minute silent Buster Keaton movie, the model can accurately analyze various plot points and events, and even reason about small details in the movie that could easily be missed. When given a prompt with more than 100,000 lines of code, it can better reason across examples, suggest helpful modifications and give explanations about how different parts of the code works. Gemini 1.5 Pro also shows impressive “in-context learning” skills, meaning that it can learn a new skill from information given in a long prompt, without needing additional fine-tuning. Google will continue to expand safety tests.

3.	***Google and UCSD's Data-Efficient LLM Training:<br>
Google and UCSD publish a paper on data-efficient approaches for training Large Language Models (LLMs). Techniques like Ask-LLM and Density sampling are introduced to optimize the Pareto frontier of model quality and training resource consumption. Ask-LLM and Density emerge as the best methods for data-efficient training, outperforming full-data training in certain scenarios.***<br><br>
15 Feb, Google and UCSD published a [paper](https://arxiv.org/pdf/2402.09668.pdf) “How to Train Data-Efficient LLMs”. The authors indicate that the training of large language models (LLMs) is expensive. The paper studies data-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model quality and training resource/data consumption. The researchers seek to understand the tradeoffs associated with data selection routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and diversity-based measures in the feature space. The first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of instruction-tuned LLMs to directly assess the quality of a training example. To target coverage, the authors propose Density sampling, which models the data distribution to select a diverse sample. In the comparison of 19 samplers, involving hundreds of evaluation tasks and pre-training runs, the research finds that Ask-LLM and Density are the best methods in their respective categories. Coverage sampling can recover the performance of the full data, while models trained on Ask-LLM data consistently outperform full-data training -- even when we reject 90% of the original dataset, while converging up to 70% faster.

4.	***Scaling Language Models to 128K Context:<br>
University of Edinburgh, MIT, and others publish a paper on scaling language models' context lengths to 128K through continual pretraining. The study focuses on data engineering, emphasizing the quantity and quality of data for effective pretraining. Results highlight the importance of domain balance and length upsampling in achieving optimal performance.***<br><br>
15 Feb, University of Edinburgh, MIT, inter alia published a [paper](https://arxiv.org/pdf/2402.10171.pdf) “Data Engineering for Scaling Language Models to 128K Context”. The authors study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. The researchers hypothesize that long context modeling, in particular \textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. The study investigates the \textit{quantity} and \textit{quality} of the data for continual pretraining: (1) for quantity, the authors show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, the results equally emphasize \textit{domain balance} and \textit{length upsampling}. Concretely, the researchers find that naively upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important. The authors demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. The proposed recipe outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K. Experiments were conducted with 8 A100 80GB Gpus, run 5 days.

5.	***BitDelta: Compression of Fine-Tuned Models:<br>
MIT, Princeton Uni, and Together AI publish a paper on BitDelta, a method to quantize fine-tuned Large Language Models (LLMs) down to 1 bit without compromising performance. This finding suggests potential redundancy in information added during fine-tuning, with significant implications for GPU memory requirements and generation latency in multi-tenant settings.***<br><br>
15 Feb, MIT, Princeton Uni, and Together AI published a [paper](https://arxiv.org/pdf/2402.10193.pdf) “BitDelta: Your Fine-Tune May Only Be Worth One Bit”. The researchers point out that Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it's intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. The authors explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. The paper introduces a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, which can also be translated to enhanced generation latency in multi-tenant settings. The authors validate BitDelta through experiments across Llama-2 and Mistral model families, and on models up to 70B parameters, showcasing minimal performance degradation over all tested settings. The authors used 6 A100 80GB Gpus in their experiments.

6.	***Meta AI's Feature Prediction for Visual Representations:<br>
Meta AI publishes a paper exploring feature prediction as a stand-alone objective for unsupervised learning from video. V-JEPA, a collection of vision models, is introduced, trained solely using feature prediction without pretrained image encoders or text. Results show versatile visual representations performing well on motion and appearance-based tasks.***<br><br>
14 Feb, Meta AI published a [paper](https://scontent.fcbr1-1.fna.fbcdn.net/v/t39.2365-6/427986745_768441298640104_1604906292521363076_n.pdf?_nc_cat=103&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=Lpq5IeF5ftUAX9GJm-9&_nc_ht=scontent.fcbr1-1.fna&oh=00_AfDL6KWpDyoVaBBKFZ-zLYHLxohnVdSx85in-r4loH4kyg&oe=65D69EB1) “Revisiting Feature Prediction for Learning Visual Representations from Video”. This paper explores feature prediction as a stand-alone objective for unsupervised learning from video and introduces V-JEPA, a collection of vision models trained solely using a feature prediction objective, without the use of pretrained image encoders, text, negative examples, reconstruction, or other sources of supervision. The models are trained on 2 million videos collected from public datasets and are evaluated on downstream image and video tasks. Experimental results show that learning by predicting video features leads to versatile visual representations that perform well on both motion and appearance-based tasks, without adaption of the model’s parameters; e.g., using a frozen backbone. The largest model, a ViT-H/16 trained only on videos, obtains 81.9% on Kinetics-400, 72.2% on Something-Something-v2, and 77.9% on ImageNet1K.

7.	***Compute Governance in AI Development:<br>
OpenAI and other institutes publish a paper emphasizing the role of computing power in governing AI development. Compute is considered a crucial intervention point for achieving common policy objectives in AI safety and beneficial use. The paper suggests guardrails to minimize risks associated with compute-based policies.***<br><br>
14 Feb, OpenAI and list of institutes published a  [paper](https://arxiv.org/pdf/2402.08797.pdf) “Computing Power and the Governance of Artificial Intelligence”. The paper states that computing power, or "compute," is crucial for the development and deployment of artificial intelligence (AI) capabilities. As a result, governments and companies have started to leverage compute as a means to govern AI. For example, governments are investing in domestic compute capacity, controlling the flow of compute to competing countries, and subsidizing compute access to certain sectors. However, these efforts only scratch the surface of how compute can be used to govern AI development and deployment. Relative to other key inputs to AI (data and algorithms), AI-relevant compute is a particularly effective point of intervention: it is detectable, excludable, and quantifiable, and is produced via an extremely concentrated supply chain. These characteristics, alongside the singular importance of compute for cutting-edge AI models, suggest that governing compute can contribute to achieving common policy objectives, such as ensuring the safety and beneficial use of AI. More precisely, policymakers could use compute to facilitate regulatory visibility of AI, allocate resources to promote beneficial outcomes, and enforce restrictions against irresponsible or malicious AI development and usage. However, while compute-based policies and technologies have the potential to assist in these areas, there is significant variation in their readiness for implementation. Some ideas are currently being piloted, while others are hindered by the need for fundamental research. Furthermore, naive or poorly scoped approaches to compute governance carry significant risks in areas like privacy, economic impacts, and centralization of power. We end by suggesting guardrails to minimize these risks from compute governance.

8.	***Reassessing Link Prediction Metrics:<br>
Researchers from UC Santa Cruz argue that the metric (AUC) used for measuring link prediction performance lacks crucial information. Link prediction tasks are found to perform significantly worse than indicated by popular literature, prompting the proposal for a new, comprehensive metric.***<br><br>
13 Feb, according to [techxplore.com](https://techxplore.com/news/2024-02-widespread-machine-methods-link-poorly.html), researchers from UC Santa Cruz published a paper on PNAS argue that the metric used to measure link prediction performance is missing crucial information, and link prediction tasks are performing significantly worse than popular literature indicates. The paper recommends that ML researchers stop using the standard practice metric for measuring link prediction, known as AUC, and introduce a new, more comprehensive metric for this problem. In this research, the authors discovered that there are fundamental mathematical limitations to using low dimensional embeddings for link predictions, and that AUC can not measure these limitations. The inability to measure these limitations caused the authors to conclude that AUC does not accurately measure link prediction performance.

9.	***Nvidia's Chat with RTX for AI Chatbot:<br>
Nvidia introduces Chat with RTX, allowing users to run an AI chatbot on Windows PCs powered by NVIDIA RTX. The tool leverages retrieval-augmented generation (RAG) and NVIDIA TensorRT-LLM software, enabling personalization and contextually relevant queries. It aims to reduce GPU memory requirements and enhance generation latency in multi-tenant settings.***<br><br>
13 Feb, According to Nvidia, Nvidia’s [Chat with RTX](https://blogs.nvidia.com/blog/chat-with-rtx-available-now/) lets you run an AI chatbot on your GPU. The tool is a free-to-download tech demo bringing generative AI capabilities to Windows PCs powered by NVIDIA RTX. This tool allows users to personalize a chatbot with their own content, utilizing retrieval-augmented generation (RAG) and NVIDIA TensorRT-LLM software. Users can connect local files as a dataset, enabling quick and contextually relevant queries, bypassing the need for cloud-based services. The application supports various file formats, including .txt, .pdf, .doc/.docx, and .xml, and allows integration of information from YouTube videos. Running locally on Windows RTX PCs ensures fast results while keeping user data on the device. The tool requires a GeForce RTX 30 Series GPU or higher with at least 8GB of VRAM, Windows 10 or 11, and the latest NVIDIA GPU drivers. Developers can explore the potential of accelerating Large Language Models (LLMs) with RTX GPUs using the TensorRT-LLM RAG developer reference project available on GitHub. The article also mentions an identified issue during installation, which will be addressed in a future release. Additionally, there's an ongoing NVIDIA Generative AI on NVIDIA RTX developer contest, offering prizes such as a GeForce RTX 4090 GPU and a conference pass to NVIDIA GTC for participants.

10.	***World Model on Million-Length Video and Language:<br>
UC Berkeley publishes a paper on training one of the largest context size transformers on long video and language sequences. The study addresses challenges in learning from millions of tokens, utilizing the RingAttention technique for scalable training. The work paves the way for developing understanding of both human knowledge and the multimodal world.***<br><br>
13 Feb, UC Berkeley published a [paper](https://arxiv.org/abs/2402.08268) “World Model on Million-Length Video And Language With RingAttention”. The study indicates that current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, the researchers curate a large dataset of diverse videos and books, utilize the RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest context size neural network: the authors train one of the largest context size transformers on long video and language sequences, setting new benchmarks in difficult retrieval tasks and long video understanding. (b) Solutions for overcoming vision-language training challenges, including using masked sequence packing for mixing different sequence lengths, loss weighting to balance language and vision, and model-generated QA dataset for long sequence chat. (c) A highly-optimized implementation with RingAttention, masked sequence packing, and other key features for training on millions-length multimodal sequences. (d) Fully open-sourced a family of 7B parameter models capable of processing long text documents (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M tokens. This work paves the way for training on massive datasets of long video and language to develop understanding of both human knowledge and the multimodal world, and broader capabilities.

11.	***GhostWriter: AI-Enhanced Writing Design:<br>
Harvard Uni and Microsoft introduce GhostWriter, an AI-enhanced writing design probe that leverages Large Language Models (LLMs) for personalized text generation. Users can exercise enhanced agency and personalization, providing multiple ways to control the system's writing style.***<br><br>
13 Feb, Harvard Uni and Microsoft published a [paper](https://arxiv.org/pdf/2402.08855.pdf) “GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency”. The authors states that Large language models (LLMs) are becoming more prevalent and have found a ubiquitous use in providing different forms of writing assistance. However, LLM-powered writing systems can frustrate users due to their limited personalization and control, which can be exacerbated when users lack experience with prompt engineering. The researchers see design as one way to address these challenges and introduce GhostWriter, an AI-enhanced writing design probe where users can exercise enhanced agency and personalization. GhostWriter leverages LLMs to learn the user's intended writing style implicitly as they write, while allowing explicit teaching moments through manual style edits and annotations. The authers study 18 participants who use GhostWriter on two different writing tasks, observing that it helps users craft personalized text generations and empowers them by providing multiple ways to control the system's writing style. The study presents insights regarding people's relationship with AI-assisted writing and offer design recommendations for future work.

12.	***PIVOT: Visual Prompting for VLMs:<br>
Google publishes a paper on PIVOT, a visual prompting approach for Vision Language Models (VLMs) that enables zero-shot control of robotic systems. PIVOT uses iterative visual optimization, allowing VLMs to handle spatial tasks without fine-tuning on task-specific data.***<br><br>
12 Feb, Google published a [paper](https://arxiv.org/pdf/2402.07872.pdf) “PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs”. The authors indicates that Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data? In this paper, the researchers propose a novel visual prompting approach for VLMs that is called Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. The authors investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. The researchers find, perhaps surprisingly, that the proposed approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, the work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: [this http URL](http://pivot-prompt.github.io/).

13.	***Generative AI and the Future of Work in Australia:<br>
McKinsey & Company's updated report indicates that 62% of existing task hours in Australia could be automated using generative AI, rising to 79-98% by 2030. The report models various adoption scenarios, considering factors such as technical potential, implementation costs, and societal dynamics.***<br><br>
12 Feb, McKinsey & Company published a [report](https://www.mckinsey.com/industries/public-sector/our-insights/generative-ai-and-the-future-of-work-in-australia) “Generative AI and the future of work in Australia”. This is an updated version of 2019, which estimated that 44 percent of Australians’ time at work could be automated by adopting the technology of the time. The new report revisited the topic to assess how the rapid emergence of gen AI has accelerated machines’ capabilities, finding that 62 percent of existing task hours could be automated using the technology available at the time of analysis. This potential could rise further to between 79 and 98 percent by 2030. However, there could be a substantial time lag between technical potential and realized change—developing capabilities into technical solutions takes time, the cost of implementing solutions may exceed the cost of human labor, and the pace of adoption could be influenced by social or regulatory dynamics. Accounting for these potential sources of friction, the new report modeled a series of adoption scenarios. While the early scenario suggests that just above 50 percent of activities could be automated by 2030, the late scenario could see just 2 percent in the same year. The midpoint of these scenarios would imply that around one-quarter of work hours could be automated by 2030.

14.	***Debating for Truthful Answers in LLMs:<br>
UCL and others publish a paper on using debate as a method for aligning Large Language Models (LLMs) with desired behavior. The study shows that debate helps non-expert models and humans answer questions more accurately, providing empirical evidence for model alignment.***<br><br>
9 Feb, UCL and others published a [paper](https://arxiv.org/pdf/2402.06782.pdf) “Debating with More Persuasive LLMs Leads to More Truthful Answers”. The authors point out that common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? The study investigates this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method evaluated is \textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. The study finds that debate consistently helps both non-expert models and humans answer questions, achieving 76\% and 88\% accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. The results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.


**11 Feb 2024**

1.	***OpenAI seeks $7 trillion for global chip project: OpenAI CEO seeks massive investment to overhaul the global semiconductor industry by building new chip factories worldwide.***<br><br>
9 Feb, according to [CNBC](https://www.msn.com/en-us/money/companies/openai-ceo-sam-altman-seeks-as-much-as-7-trillion-for-new-ai-chip-project-report/ar-BB1i285k), OpenAI CEO Sam Altman seeks as much as $7 trillion for new AI chip project to overhaul the global semiconductor industry with trillions of dollars in investment. Altman has said AI chip limitations hinder OpenAI's growth, and as this project would increase chip-building capacity globally, he is in talks with investors, including the United Arab Emirates government. Altman could need to raise between $5 trillion and $7 trillion for the endeavor, the WSJ reported, citing one source. CNBC could not confirm the exact number.

2.	***Cloud repatriation trend in UK: A study reveals 25% of UK organizations are moving workloads back on-premises due to security concerns, unmet expectations, and cost overruns in the cloud.***<br><br>
9 Feb, according to [InfoWorld](https://www.infoworld.com/article/3712861/why-companies-are-leaving-the-cloud.html), a recent study by Citrix reveals that 25% of surveyed organizations in the United Kingdom have moved at least half of their cloud-based workloads back to on-premises infrastructures. The survey, which involved 350 IT leaders, highlights security issues and unmet expectations as the main drivers (33%) for repatriating workloads. Additionally, 24% cited the failure to meet internal expectations as a significant motivator. Cost emerges as a predominant factor, with over 43% of IT leaders finding the migration to the cloud more expensive than anticipated. The article suggests that the cloud's inability to deliver on the promised lower costs, better agility, and innovation from 2010 to 2015 contributes to this shift. Traditional infrastructure patterns, where enterprises run workloads and data sets similar to on-premises methods, lead to unexpected costs in the public cloud. The cloud is deemed suitable for modern applications but may not align with most enterprise applications. Despite repatriation challenges, public cloud providers are expected to maintain growth due to their role in hosting generative AI applications and data. The article concludes that the ongoing focus on AI systems and expanding cloud usage for AI-related infrastructure contributes positively to the cloud providers' growth, making the situation healthy for the industry. (NOTE: AI systems are more suitable to run on-premises !)

3.	***Australian law firm embraces AI: Lander & Rogers utilizes Copilot for various legal tasks and launches an AI Lab to develop custom tools and integrate AI into workflows.***<br><br>
9 Feb, according to [itnews.com.au](https://www.itnews.com.au/news/australian-law-firm-lander-rogers-finds-legal-uses-for-copilot-604842), Australian law firm Lander & Rogers finds legal uses for Copilot and also sets up an AI Lab.  The firm has initiated a pilot program with a core group and plans to expand its usage across the organization in the coming months. Copilot is being utilized for contract drafting, data extraction, legal research, and simpler tasks like email composition and meeting summarization. The law firm has found Copilot particularly helpful in correspondence drafting and collaborating with its knowledge in the Power Platform. Lander & Rogers has encouraged its 500 staff members to use Copilot, especially for email composition, where the AI tool can extract key points from documents and assist with spelling, grammar, and conciseness. The firm had previously assessed tasks suitable for automation, partnering with Monash University for AI testing. Recently, the law firm launched an AI Lab with a focus on developing specialized tools for legal tasks. While no prototypes are completed yet, the AI Lab is concentrating on proofs-of-concept to bridge theoretical AI capabilities with practical, usable tools in legal practice. The AI Lab aims to integrate AI into operational frameworks and legal workflows to enhance efficiency, accuracy, and client satisfaction. Lander & Rogers acknowledges the importance of educating staff on the limitations and ethical considerations of emerging technologies. The firm has a nine-point AI policy, emphasizing responsible AI use, clear and unambiguous prompts, confidentiality, and privacy considerations. The AI Lab plays a crucial role in ensuring the firm's adherence to privacy obligations and promoting ethical AI practices.

4.	***Interactive Agent Foundation Model proposed: Microsoft, Stanford, and UCLA research proposes a new training paradigm for building versatile AI agents capable of handling diverse tasks and applications.***<br><br>
9 Feb, Microsoft, Standford Uni and UCLA published a [paper](https://arxiv.org/pdf/2402.05929) “An Interactive Agent Foundation Model”. The paper indicates that the development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. The research proposes an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. The training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. The study demonstrates the performance of the framework across three separate domains -- Robotics, Gaming AI, and Healthcare. The proposed model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of the approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effective multimodal and multi-task learning. The proposed approach provides a promising avenue for developing generalist, action-taking, multimodal systems.

5.	***Question Aware Vision Transformer for multimodal reasoning: AWS introduces QA-ViT, a method that embeds question awareness directly into vision encoding, improving visual and scene-text understanding in multimodal AI models.***<br><br>
8 Feb, AWS published a [paper](https://arxiv.org/pdf/2402.05472.pdf) “Question Aware Vision Transformer for Multimodal Reasoning”. The paper states that Vision-Language (VL) models have gained significant research focus, enabling remarkable advances in multimodal reasoning. These architectures typically comprise a vision encoder, a Large Language Model (LLM), and a projection module that aligns visual features with the LLM's representation space. Despite their success, a critical limitation persists: the vision encoding process remains decoupled from user queries, often in the form of image-related questions. Consequently, the resulting visual features may not be optimally attuned to the query-specific elements of the image. To address this, we introduce QA-ViT, a Question Aware Vision Transformer approach for multimodal reasoning, which embeds question awareness directly within the vision encoder. This integration results in dynamic visual features focusing on relevant image aspects to the posed question. QA-ViT is model-agnostic and can be incorporated efficiently into any VL architecture. Extensive experiments demonstrate the effectiveness of applying the proposed method to various multimodal architectures, leading to consistent improvement across diverse tasks and showcasing its potential for enhancing visual and scene-text understanding.

6.	***Repurposing LLMs for specialized domains: Researchers develop a framework to "tag" general LLMs with specialized information, enabling them to perform tasks in previously underrepresented domains like medicine and chemistry.***<br><br>
7 Feb, MSR, CMU, and Harvard Uni published a [paper](https://arxiv.org/pdf/2402.05140) “Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains”. The researchers point out that Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. The research introduces a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. The researchers design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. The studay develops a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from task functions, the proposed method enables zero-shot generalization to unseen problems through diverse combinations of the input tags. It also boosts LLM's performance in various specialized domains, such as predicting protein or chemical properties and modeling drug-target interactions, outperforming expert models tailored to these tasks.

7.	***Britain invests in AI research hubs: UK launches nine new research hubs with $120 million funding to promote AI development and train regulators on managing its risks.***<br><br>
6 Feb, according to [channelnewsasia.com](https://www.channelnewsasia.com/business/britain-invests-100-million-pounds-ai-research-and-regulation-4102071), Britain invests 100 million pounds to launch nine new research hubs in artificial intelligence (AI) and train regulators about the technology, by taking an agile, sector-specific approach to grip the risks immediately. Nearly 90 million pounds would go towards the hubs, which will focus on using AI in areas including healthcare, chemistry and mathematics, and a partnership with the United States on responsible AI, the government said. Another 10 million pounds would help regulators address the risks and harness the opportunities of AI, it said, such as developing practical tools to monitor risks in sectors from telecoms and healthcare to finance and education.

8.	***LLMs learn to self-compose reasoning structures: USC and Google introduce SELF-DISCOVER, a framework that allows LLMs to learn and utilize reasoning structures, significantly improving their performance on complex tasks.***<br><br>
6 Feb, University of South California and Google published a [paper](https://arxiv.org/pdf/2402.03620.pdf) “Self-Discover: Large Language Models Self-Compose Reasoning Structures”. The researchers introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, the study shows that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.

9.	***Ex-Google engineer critiques Agile methodology: A former Google engineer argues that the Agile software development methodology can lead to micromanagement and poorly maintained code, sparking debate among developers.***<br><br>
1 Feb, [devclass.com](https://devclass.com/2024/02/01/agile-process-can-spur-micromanagement-and-poorly-maintained-code-says-ex-google-software-engineer/) published an article “Agile process can spur micromanagement and poorly maintained code, says ex-Google software engineer”. A former Google software engineer, Murat Guler, contends in his book, "Defending Software Quality," that the Agile methodology, often hailed for improving productivity and code quality, can lead to micromanagement and poorly maintained code. Guler, who worked at Google for over 13 years, argues that Agile's emphasis on daily collaboration between business people and developers may encourage micromanagement, leading to issues such as unrealistic deadlines, reduced trust in engineers, and increased scrutiny. Additionally, Guler criticizes aspects of Agile, including 2-week sprints, welcoming changing requirements, and practices like pair programming and shared code ownership. He points out that even when used correctly, Agile may not fit all types of software development, and its principles can be misapplied, resulting in negative consequences. The post has garnered support from developers who share similar concerns about the imposition of Agile methodology, describing it as "cult-like" and emphasizing the importance of team dynamics and mutual trust over processes and tools.




**4 Feb 2024**

1.	2 Feb, according to [Theverge.com](https://www.theverge.com/2024/2/1/24058095/open-ai-bioweapon-study-preparedness-team), OpenAI's study suggests its GPT-4 language model offers only a "slight" advantage over the internet for bioweapon research, contradicting concerns and some external research. While participants with GPT-4 achieved slightly higher accuracy, the increase wasn't statistically significant. Concerns remain due to GPT-4's vast data access and multimodal nature, suggesting the potential for a larger advantage than reported.
 
2.	1 Feb, NYU published a [paper](https://www.science.org/doi/reader/10.1126/science.adi1374) on Science “Grounded language acquisition through the eyes and ears of a single child”. How do young children learn to associate new words with specific objects or visually represented concepts? This hotly debated question in early language acquisition has been traditionally examined in laboratories, limiting generalizability to real-world settings. This study investigated the question in an unprecedented, longitudinal manner using head-mounted video recordings from a single child’s first-person experiences in naturalistic settings. By applying machine learning, they introduced the Child’s View for Contrastive Learning (CVCL) model, pairing video frames that co-occurred with uttered words, and embedded the images and words in shared representational spaces. CVCL represents sets of visually similar things from one concept (e.g., puzzles) through distinct subclusters (animal versus alphabet puzzles). It combines associative and representation learning that fills gaps in language acquisition research and theories. 

3.	1 Feb, researchers from Georgetown Uni and Apple published a [paper](https://arxiv.org/pdf/2402.00858.pdf) “Can Large Language Models Understand Context?”. The study indicates that understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent. However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features. This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models. This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models' ability to understand context. First, the researcher evaluates the performance of LLMs under the in-context learning pretraining scenario. Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. Second, as LLM compression holds growing significance in both research and real-world applications, the researchers assess the context understanding of quantized models under in-context-learning settings. The study finds that 3-bit post-training quantization leads to varying degrees of performance reduction on the benchmark. The researchers conduct an extensive analysis of these scenarios to substantiate the experimental results.

4.	31 Jan, AI2, UCB, CMU, MIT etc published a [paper](https://arxiv.org/pdf/2402.00159.pdf) “Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research”. The paper indicates that language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed:  commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, the researchers release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, the researchers open source the data curation toolkit to enable further experimentation and reproduction of the work. In this report, the researchers document Dolma, including its design principles, details about its construction, and a summary of its contents. The paper interleaves this report with analyses and experimental results from training language models on intermediate states of Dolma to share what the researchers have learned about important data curation practices, including the role of content or quality filters, deduplication, and multi-source mixing. Dolma has been used to train OLMo, a state-of-the-art, open language model and framework designed to build and study the science of language modelling.

5.	31 Jan, according to [Arstechnica](https://arstechnica.com/security/2024/01/ars-reader-reports-chatgpt-is-sending-him-conversations-from-unrelated-ai-users/), ChatGPT is leaking private conversations that include login credentials and other personal details of unrelated users, screenshots submitted by an Ars reader on Monday indicated. Two of the seven screenshots the reader submitted stood out in particular. Both contained multiple pairs of usernames and passwords that appeared to be connected to a support system used by employees of a pharmacy prescription drug portal. An employee using the AI chatbot seemed to be troubleshooting problems they encountered while using the portal. User shocked to find chats naming unpublished research papers, and other private data. OpenAI officials say that the ChatGPT histories a user reported result from his ChatGPT account being compromised. The unauthorized logins came from Sri Lanka, an Open AI representative said. The user said he logs into his account from Brooklyn, New York. The user, Chase Whiteside, has since changed his password, but he doubted his account was compromised. He said he used a nine-character password with upper- and lower-case letters and special characters. He said he didn’t use it anywhere other than for a Microsoft account. He said the chat histories belonging to other people appeared all at once on Monday morning during a brief break from using his account.

6.	30 Jan, UIUC published a [paper](https://arxiv.org/pdf/2401.17263.pdf) “Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks”. The paper states that despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which the researchers posit should be effective, universal, and practical. To achieve this, the authors of the paper propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, the study finds that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success rate of the strongest attack on GPT-4 from 92% to 6%. The code is available [here](https://github.com/andyz245/rpo).

7.	30 Jan, according to [Reuters](https://www.reuters.com/technology/cybersecurity/italy-regulator-notifies-openai-privacy-breaches-chatgpt-2024-01-29/), Italy's data protection authority has told OpenAI that its artificial intelligence chatbot application ChatGPT breaches data protection rules, as it presses ahead with an investigation started last year. Italy was the first West European country to curb ChatGPT, whose rapid development has attracted attention from lawmakers and regulators. Under the EU's General Data Protection Regulation (GDPR) introduced in 2018, any company found to have broken rules faces fines of up to 4% of its global turnover.

8.	29 Jan, researchers from Italy and Israel published a [paper](https://arxiv.org/pdf/2401.14887.pdf) “The Power of Noise: Redefining Retrieval for RAG Systems”. The paper argues that Retrieval-Augmented Generation (RAG) systems represent a significant advancement over traditional Large Language Models (LLMs). RAG systems enhance their generation ability by incorporating external data retrieved through an Information Retrieval (IR) phase, overcoming the limitations of standard LLMs, which are restricted to their pre-trained knowledge and limited context window. Most research in this area has predominantly concentrated on the generative aspect of LLMs within RAG systems. This study fills this gap by thoroughly and critically analyzing the influence of IR components on RAG systems. This paper analyzes which characteristics a retriever should possess for an effective RAG's prompt formulation, focusing on the type of documents that should be retrieved. We evaluate various elements, such as the relevance of the documents to the prompt, their position, and the number included in the context. The findings reveal, among other insights, that including irrelevant documents can unexpectedly enhance performance by more than 30% in accuracy, contradicting people initial assumption of diminished quality. These results underscore the need for developing specialized strategies to integrate retrieval with language generation models, thereby laying the groundwork for future research in this field.

9.	29 Jan, CMU & Apple published a [paper](https://arxiv.org/pdf/2401.16380.pdf) “Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling”. The paper states that Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. This study proposes Web Rephrase Augmented Pre-training (WRAP) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as “like Wikipedia” or in “question-answer format” to jointly pre-train LLMs on real and synthetic rephrases. First, the researchers show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by ∼ 3×. At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, the researchers investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. The gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher ‘quality’ than web-scraped data.

10.	26 Jan, University of Edinburgh published a [paper](https://arxiv.org/pdf/2401.15241.pdf) “Unlearning Reveals the Influential Training Data of Language Models”. The paper indicates that in order to enhance the performance of language models while mitigating the risks of generating harmful content, it is crucial to identify which training dataset affects the model's outputs. Ideally, researchers can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac, which estimates the influence of a training dataset by unlearning it from the trained model. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and the researchers evaluate how much the model's predictions change after unlearning. The paper empirically examines if the proposed methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Experimental results demonstrate that the method estimates their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple model checkpoints.

11.	26 Jan, Harvard Business Review published a [paper](https://hbr.org/2024/01/using-prompt-engineering-to-better-communicate-with-people) “Using Prompt Engineering to Better Communicate with People”. The paper argues that it seems like everyone knows, or wants to know, how to produce the best possible prompts for generative AI. But, among the buzz and promise of masterful prompt engineering, there is an emerging risk that managers will view generative AI as a one-stop-shop for gathering information. In doing so, they may neglect their most valuable information resources: employees, partners, and customers. These stakeholders offer contextual information and tacit knowledge that is beyond the capability of any generative AI tool. As we get better at speaking to robots, we should remember how to most effectively speak to our colleagues and customers, too. Following are a summary of the six guidelines to consider when initiating discussions with stakeholders, whether they’re customers, partners, or employees: 1) Structure your prompts the right way. 2) Utilize reflective and thoughtful probing. 3) Convey empathetic language and humility. 4) Harness humor, playfulness, and emotions. 5) Acknowledge key challenges. 6) Be a good (and patient) listener.

12.	23 Jan, according to the [androidcentral](https://www.androidcentral.com/apps-software/google-leaked-2024-goals), Google’s 2024 goals are: 1) Deliver the world’s most advanced, safe, and responsible Al. 2) Improve knowledge, learning, creativity, and productivity. 3) Build the most helpful personal computing platforms and devices. 4) Enable organizations and developers to innovate on Google Cloud. 5) Provide the world’s most trusted products and platforms. 6) Build a Google that’s extraordinary for Googlers and the world. 7) Improve company velocity, efficiency, and productivity, and deliver durable cost savings.
