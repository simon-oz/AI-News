# Weekly AI-News - May 2024
Welcome to the Weekly-AI-news!
I mainly focuses more on Generative AI, LLMs and the related, so may miss import AI news in other areas. <br><br>

***26 May 2024***

1. ***Forbes on Google’s AI-Generated Summaries:
Google’s new AI-generated summary feature for search results, revealed at the Google I/O developer conference, is under scrutiny for providing misleading information. Users, including journalists, noted that the AI summary, known as 'AI Overview', has cited unreliable sources like jokes from Reddit or articles from The Onion, leading to the spread of misinformation. Computer scientist Melanie Mitchell highlighted an instance where the AI cited a conspiracy theory about President Obama. The AI also has been accused of plagiarizing text from blogs without modifying personal details. Forbes found other errors, such as incorrect geographical facts and biological classifications. Some of the erroneous AI summaries have been replaced by news articles discussing the AI issues.*** <br><br>
   24 May, [according to Forbes](https://www.forbes.com/sites/siladityaray/2024/05/24/googles-ai-overview-appears-to-produce-misleading-answers/?sh=9e9d4d232252), One of the key new features unveiled at the Google I/O developer conference last week—AI-generated summaries on search results—has become the subject of scrutiny and jokes on social media after users appeared to show the feature displaying misleading answers and, in some cases, dangerous misinformation. 1) Several Google users, including journalists, have shared what appear to be multiple examples of the AI summary, called ‘AI Overview’ citing dubious sources, such as Reddit posts written as jokes, or failing to understand that articles on the Onion aren’t factual. 2) Computer scientist Melanie Mitchell shared an example of the feature displaying in an answer a right-wing conspiracy theory that President Barack Obama is Muslim, in what appears to be a failed attempt to summarize a book from the Oxford University Press’ research platform. 3) In other instances, the AI summary appears to be plagiarizing text from blogs and failing to remove or alter mentions of the author’s children. 4) Many other examples shared on social media include the platform getting basic facts wrong, such as failing to acknowledge countries in Africa starting with K and suggesting pythons are mammals—both results that Forbes was able to replicate. 5) Other inaccurate results that have gone viral—like the one about Obama or putting glue on pizza—no longer display an AI summary but rather news articles referencing the AI search issues.

3. ***KAIST’s Study on LLVMs:
KAIST's paper discusses the progress in large language and vision models (LLVMs), focusing on visual instruction tuning which enhances model performance. Open-source LLVMs have improved by using high-quality datasets and multiple vision models. The study introduces Meteor, an LLVM that uses the Mamba architecture for efficient processing and embedding of detailed rationales to improve understanding and answering capabilities. Meteor achieves better performance without increasing model size or adding extra vision encoders. The findings are supported by benchmark evaluations, indicating significant improvements.*** <br><br>
   24 May, Korea Advanced Institute of Science and Technology (KAIST) published a [paper](https://arxiv.org/pdf/2405.15574v1) “Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models”. The paper finds that the rapid development of large language and vision models (LLVMs) has been driven by advances in visual instruction tuning. Recently, open-source LLVMs have curated high-quality visual instruction tuning datasets and utilized additional vision encoders or multiple computer vision models in order to narrow the performance gap with powerful closed-source LLVMs. These advancements are attributed to multifaceted information required for diverse capabilities, including fundamental image understanding, real-world knowledge about common-sense and non-object concepts (e.g., charts, diagrams, symbols, signs, and math problems), and step-by-step procedures for solving complex questions. Drawing from the multifaceted information, the study presents a new efficient LLVM, Mamba-based traversal of rationales (Meteor), which leverages multifaceted rationale to enhance understanding and answering capabilities. To embed lengthy rationales containing abundant information, the authors employ the Mamba architecture, capable of processing sequential data with linear time complexity. The paper introduces a new concept of traversal of rationale that facilitates efficient embedding of rationale. Subsequently, the backbone multimodal language model (MLM) is trained to generate answers with the aid of rationale. Through these steps, Meteor achieves significant improvements in vision language performances across multiple evaluation benchmarks requiring diverse capabilities, without scaling up the model size or employing additional vision encoders and computer vision models. Github [link is here](https://github.com/ByungKwanLee/Meteor).

5. ***Meta and Google on Self-Supervised Learning:
Meta and Google’s paper addresses the automatic curation of datasets for self-supervised learning, a process typically requiring extensive human effort. They propose a clustering-based approach to create large, diverse, and balanced datasets. This method involves using k-means clustering on a data repository followed by balanced sampling. Experiments show that features trained on these curated datasets outperform those from uncurated ones and are comparable or superior to those from manually curated data.*** <br><br>
   24 May, Meta, Google et al. published a [paper](https://arxiv.org/pdf/2405.15613) “Automatic Data Curation for Self-Supervised Learning:
A Clustering-Based Approach”. The paper indicates that self-supervised features are the cornerstone of modern machine learning systems. They are typically pre-trained on data collections whose construction and curation typically require extensive human effort. This manual process has some limitations similar to those encountered in supervised learning, e.g., the crowd-sourced selection of data is costly and time-consuming, preventing scaling the dataset size. This work considers the problem of automatic curation of high-quality datasets for self-supervised pre-training. The authors posit that such datasets should be large, diverse and balanced, and propose a clustering-based approach for building ones satisfying all these criteria. The method involves successive and hierarchical applications of k-means on a large and diverse data repository to obtain clusters that distribute uniformly among data concepts, followed by a hierarchical, balanced sampling step from these clusters. Extensive experiments on three different data domains including web-based images, satellite images and text show that features trained on the automatically curated datasets outperform those trained on uncurated data while being on par or better than ones trained on manually curated data.

7. ***MIT on Language Model Features:
MIT’s paper challenges the linear representation hypothesis in language models, proposing that some model features are inherently multi-dimensional. The study introduces a method using sparse autoencoders to discover multi-dimensional features in models like GPT-2 and Mistral 7B, finding interpretable examples such as circular representations of days and months. The research suggests that these multi-dimensional features are fundamental for certain computational tasks and provides evidence through experiments on models like Mistral 7B and Llama 3 8B.*** <br><br>
   23 May, MIT published a [paper](https://arxiv.org/pdf/2405.14860) “Not All Language Model Features Are Linear”. Recent work has proposed the linear representation hypothesis: that language models perform computation by manipulating one-dimensional representations of concepts ("features") in activation space. In contrast, the paper explores whether some language model representations may be inherently multi-dimensional. The study begins by developing a rigorous definition of irreducible multi-dimensional features based on whether they can be decomposed into either independent or non-co-occurring lower-dimensional features. Motivated by these definitions, the authors design a scalable method that uses sparse autoencoders to automatically find multi-dimensional features in GPT-2 and Mistral 7B. These auto-discovered features include strikingly interpretable examples, e.g. circular features representing days of the week and months of the year. The authors identify tasks where these exact circles are used to solve computational problems involving modular arithmetic in days of the week and months of the year. Finally, the work provides evidence that these circular features are indeed the fundamental unit of computation in these tasks with intervention experiments on Mistral 7B and Llama 3 8B, and the researchers find further circular representations by breaking down the hidden states for these tasks into interpretable components.

9. ***CohereAI’s Aya 23 Report:
CohereAI introduces Aya 23, a new family of multilingual language models, which builds on the previous Aya model by focusing on fewer languages with more depth. Aya 23 covers 23 languages, improving performance in both discriminative and generative tasks compared to broader multilingual models like Aya 101. The release includes open weights for 8B and 35B models, demonstrating CohereAI’s commitment to expanding multilingual capabilities.*** <br><br>
    23 May, CohereAI published its [tech report](https://arxiv.org/pdf/2405.15032) “Aya 23: Open Weight Releases to Further Multilingual Progress”. This technical report introduces Aya 23, a family of multilingual language models. Aya 23 builds on the recent release of the Aya model (Üstün et al., 2024), focusing on pairing a highly performant pre-trained model with the recently released Aya collection (Singh et al., 2024). The result is a powerful multilingual large language model serving 23 languages, expanding state-of-art language modeling capabilities to approximately half of the world's population. The Aya model covered 101 languages whereas Aya 23 is an experiment in depth vs breadth, exploring the impact of allocating more capacity to fewer languages that are included during pre-training. Aya 23 outperforms both previous massively multilingual models like Aya 101 for the languages it covers, as well as widely used models like Gemma, Mistral and Mixtral on an extensive range of discriminative and generative tasks. CohereAI released the open weights for both the [8B](https://huggingface.co/CohereForAI/aya-23-8B) and [35B](https://huggingface.co/CohereForAI/aya-23-35B) models as part of our continued commitment for expanding access to multilingual progress.

11. ***SimPO:
Researchers from the University of Virginia and Princeton University propose SimPO, a new preference optimization method for reinforcement learning from human feedback. SimPO uses the average log probability of sequences as rewards, enhancing training efficiency and stability. The method outperforms existing approaches like Direct Preference Optimization (DPO) on various benchmarks without increasing response length, showing significant improvements in models like Llama3-8B-Instruct.*** <br><br>
    23 May, Uni of Virginia and Princeton Uni published a [paper](https://arxiv.org/pdf/2405.14734) “SimPO Simple Preference Optimization with a Reference-Free Reward”. The authors indicate that Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability. This work proposes SimPO, a simpler yet more effective approach. The effectiveness of SimPO is attributed to a key design: using the average log probability of a sequence as the implicit reward. This reward formulation better aligns with model generation and eliminates the need for a reference model, making it more compute and memory efficient. Additionally, the authors introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further enhancing the algorithm's performance. The researchers compare SimPO to DPO and its latest variants across various state-of-the-art training setups, including both base and instruction-tuned models like Mistral and Llama3. The study evaluated on extensive instruction-following benchmarks, including AlpacaEval 2, MT-Bench, and the recent challenging Arena-Hard benchmark. Experimental results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length. Specifically, SimPO outperforms DPO by up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. The top-performing model, built on Llama3-8B-Instruct, achieves a remarkable 44.7 length-controlled win rate on AlpacaEval 2 -- surpassing Claude 3 Opus on the leaderboard, and a 33.8 win rate on Arena-Hard -- making it the strongest 8B open-source model.

13. ***AutoCoder by University of Connecticut:
The University of Connecticut introduces AutoCoder, a large language model that surpasses GPT-4 Turbo in code interpretation tasks. AutoCoder’s training involves a multi-turn dialogue dataset created through a method called AIEV-Instruct, which ensures execution-validated code. This approach reduces dependence on proprietary models and provides a versatile code interpreter capable of using external packages.*** <br><br>
    23 May, Uni of Connecticut published a [paper}(https://arxiv.org/pdf/2405.14906) “AutoCoder: Enhancing Code Large Language Model with AIEV-INSTRUCT”. The research introduces AutoCoder, the first Large Language Model to surpass GPT-4 Turbo (April 2024) and GPT-4o in pass@1 on the Human Eval benchmark test (90.9% vs. 90.2%). In addition, AutoCoder offers a more versatile code interpreter compared to GPT-4 Turbo and GPT-4o. It's code interpreter can install external packages instead of limiting to built-in packages. AutoCoder's training data is a multi-turn dialogue dataset created by a system combining agent interaction and external code execution verification, a method termed as \textsc{AIEV-Instruct} (Instruction Tuning with Agent-Interaction and Execution-Verified). Compared to previous large-scale code dataset generation methods, AIEV-Instruct reduces dependence on proprietary large models and provides execution-validated code dataset. The code and the demo video is available in https://github.com/bin123apple/AutoCoder.

15. ***YOLOv10 released:
Tsinghua University’s paper presents YOLOv10, a new generation of real-time object detection models. YOLOv10 improves on previous models by eliminating the need for non-maximum suppression and optimizing model components for better efficiency and performance. The model achieves state-of-the-art results, demonstrating significant speed and parameter reductions compared to previous versions like YOLOv9.*** <br><br>
    23 May, Tsinghua Uni published a [paper](https://arxiv.org/pdf/2405.14458v1) “YOLOv10: Real-Time End-to-End Object Detection”. Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model’s capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. This work aims to further advance the performance-efficiency boundary of YOLOs from both the post-processing and the model architecture. To this end, the paper first presents the consistent dual assignments for NMS-free training of YOLOs, which brings the competitive performance and low inference latency simultaneously. Moreover, the study introduces the holistic efficiency-accuracy driven model design strategy for YOLOs. The authors comprehensively optimize various components of YOLOs from both the efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of the effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves the state-of-the-art performance and efficiency across various model scales. For example, YOLOv10-S is 1.8×  faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8× smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has 46% less latency and 25% fewer parameters for the same performance. Code: https://github.com/THU-MIG/yolov10.

17. ***Grokked Transformers Study:
Researchers from Ohio State University and CMU explore transformers’ ability to implicitly reason using extended training, termed grokking. They find that transformers can generalize well for comparison tasks but not for composition tasks. The study reveals mechanisms behind grokking and suggests architectural improvements to enhance reasoning capabilities. Transformers that achieve near-perfect accuracy for complex reasoning tasks showcase the potential of parametric memory.*** <br><br>
    23 May, Ohio State Uni and CMU published a [paper](https://arxiv.org/pdf/2405.15071) “Grokked Transformers are Implicit Reasoners A Mechanistic Journey to the Edge of Generalization”. The researchers study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, the authors consistently find that transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting. The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison. The study delves into the model's internals throughout training, conducting analytical experiments that reveal: 1) the mechanism behind grokking, such as the formation of the generalizing circuit and its relation to the relative efficiency of generalizing and memorizing circuits, and 2) the connection between systematicity and the configuration of the generalizing circuit. Our findings guide data and training setup to better induce implicit reasoning and suggest potential improvements to the transformer architecture, such as encouraging cross-layer knowledge sharing. Furthermore, we demonstrate that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning.

19. ***Microsoft CEO on AI:
Microsoft CEO Satya Nadella emphasizes viewing AI as a tool rather than a human-like entity. He criticizes the term "artificial intelligence" and suggests "different intelligence" as a more appropriate term. Nadella warns against anthropomorphizing AI and highlights the practical focus of AI development. Microsoft-backed OpenAI faces backlash for replicating actress Scarlett Johansson’s voice, showcasing ethical concerns in AI technology.*** <br><br>
    22 May, [according to MSN](https://www.msn.com/en-us/money/news/microsoft-ceo-satya-nadella-says-stop-treating-ai-like-humans-i-sort-of-believe-its-a-tool/ar-BB1mORVQ), Microsoft Corporation CEO Satya Nadella has emphasized that artificial intelligence should be viewed as a tool, not a human-like entity. Nadella’s comments come in the wake of ChatGPT-parent OpenAI’s unveiling of a personal assistant with human-like capabilities. He expressed his disapproval of anthropomorphizing AI during an interview with Bloomberg on Monday, stating, “I sort of believe it’s a tool.”  Nadella also criticized the term “artificial intelligence,” "I think one of the most unfortunate names is 'artificial intelligence' — I wish we had called it 'different intelligence'," he said. "Because I have my intelligence. I don’t need any artificial intelligence." He stressed that AI’s abilities are not equivalent to human intelligence and should be recognized as such. Last week, a Google executive also said that although it is feasible to create AI tools capable of displaying emotion, the company prefers to prioritize it being “super helpful and super useful." Even before ChatGPT’s integration into mainstream awareness of AI, tech firms frequently imbued AI programs with human-like traits. They often gave them female-associated names and characteristics to foster familiarity and connection with users, the report noted. Meanwhile, Microsoft-backed OpenAI is facing backlash for its new voice assistant after actress Scarlett Johansson hired legal counsel accusing ChatGPT-parent of replicating her voice using AI technology.

21. ***Attention as an RNN by Mila and Borealis AI:
Researchers propose viewing attention mechanisms in transformers as a form of Recurrent Neural Network (RNN) to improve efficiency. They introduce Aaren, an attention-based module that combines parallel training capabilities of transformers with efficient updates typical of RNNs. Aaren achieves comparable performance to transformers while being more time and memory efficient, validated by empirical results across multiple sequential problem settings.*** <br><br>
    22 May, Mila and Borealis AI published a [paper](https://arxiv.org/pdf/2405.13956) “Attention as an RNN”. The researchers from Yoshua Bengio’s team point out that the advent of Transformers marked a significant breakthrough in sequence modelling, providing a highly performant architecture capable of leveraging GPU parallelism. However, Transformers are computationally expensive at inference time, limiting their applications, particularly in low-resource settings (e.g., mobile and embedded devices). Addressing this, the authors (1) begin by showing that attention can be viewed as a special Recurrent Neural Network (RNN) with the ability to compute its many-to-one RNN output efficiently. (2) show that popular attention-based models such as Transformers can be viewed as RNN variants. However, unlike traditional RNNs (e.g., LSTMs), these models cannot be updated efficiently with new tokens, an important property in sequence modelling. Tackling this, the researchers (3) introduce a new efficient method of computing attention’s many-to-many RNN output based on the parallel prefix scan algorithm. Building on the new attention formulation, the authors (4) introduce Aaren, an attention-based module that can not only (i) be trained in parallel (like Transformers) but also (ii) be updated efficiently with new tokens, requiring only constant memory for inferences (like traditional RNNs). Empirical results show Aarens achieve comparable performance to Transformers on 38 datasets spread across four popular sequential problem settings: reinforcement learning, event forecasting, time series classification, and time series forecasting tasks while being more time and memory-efficient.

23. ***Time 100 Voices on AI Sentience:
F. Li and J. Etchemendy argue that current AI, including advanced models like ChatGPT, is not sentient. True general intelligence involves subjective experiences, which AI lacks due to its non-biological nature. AI’s expressions of emotions are probabilistic outputs rather than genuine experiences. Achieving sentient AI requires understanding biological sentience, which current technology does not possess.*** <br><br>
    22 May, according to [Time 100 Voices](https://time.com/collection/time100-voices/6980134/ai-llm-not-sentient/), “No, Today’s AI Isn’t Sentient. Here’s How We Know”. In this article, F. Li and J. Etchemendy  argue that Artificial General Intelligence (AGI) refers to an AI that matches human intelligence in all its forms. The term arose to differentiate from AI systems that are intelligent but specialized, like Deep Blue in chess. True general intelligence, which humans supposedly possess, involves not just problem-solving but also sentience—the ability to have subjective experiences like hunger or pain. The launch of ChatGPT in 2022 reignited debates on AI sentience. Some argue that because AI like ChatGPT can express subjective experiences, it might be sentient. However, this reasoning is flawed. Human declarations of hunger are based on physiological states, while an LLM’s similar statements are just probabilistic text completions without any real experience. AI lacks the biological makeup necessary for true subjective experiences. Despite some believing otherwise, LLMs cannot be sentient. They are mathematical models without bodies, and their word sequences do not reflect genuine experiences. Achieving sentient AI requires a deeper understanding of how sentience emerges in biological systems, which current LLMs do not possess.

25. ***MIT and IBM on Reducing KV Cache Size:
MIT and IBM’s paper introduces Cross-Layer Attention (CLA) to reduce the size of key-value (KV) caches in transformer models. CLA shares key and value heads between adjacent layers, reducing memory requirements without significant accuracy loss. Experiments with 1B- and 3B-parameter models show CLA’s effectiveness in enabling longer sequence lengths and larger batch sizes, improving upon traditional Multi-Query Attention (MQA) methods.*** <br><br>
    21 May, MIT and IBM  published a [paper](https://arxiv.org/pdf/2405.12981) “Reducing Transformer Key-Value Cache Size with Cross-Layer Attention”. The authors state that Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs). However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes. Since the invention of the transformer, two of the most effective interventions discovered for reducing the size of the KV cache have been Multi-Query Attention (MQA) and its generalization, Grouped-Query Attention (GQA). MQA and GQA both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy. This paper shows that it is possible to take Multi-Query Attention a step further by also sharing key and value heads between adjacent layers, yielding a new attention design we call Cross-Layer Attention (CLA). With CLA, the authors find that it is possible to reduce the size of the KV cache by another 2x while maintaining nearly the same accuracy as unmodified MQA. In experiments training 1B- and 3B-parameter models from scratch, the study demonstrates that CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, enabling inference with longer sequence lengths and larger batch sizes than would otherwise be possible.

27. ***Techspot Report on Microsoft Recall Feature:
Desktop users often struggle to locate specific past activities on their computers, but Microsoft's new Recall feature aims to solve this by using AI to track and present a timeline of all user activities. CEO Satya Nadella highlighted that Recall offers an advanced semantic search that can recreate past moments by taking constant screenshots and processing data locally, ensuring privacy. Users can control or delete this logging, though sensitive data like passwords might not be proactively hidden.*** <br><br>
    21 May, according to [Techspot](https://www.techspot.com/news/103087-microsoft-new-ai-powered-recall-feature-log-everything.html), Desktop users will swear they've been in situations where they had to desperately search through emails, browser histories, and file folders trying to find that one thing they vaguely remember seeing or working on a few days prior. It's frustrating, but Microsoft may have cracked this problem with a new feature called Recall. Recall is essentially an all-seeing eye that keeps tabs on everything one do on a computer. It then presents a scrollable timeline one can search through, using AI. In an interview with The Wall Street Journal, Microsoft CEO Satya Nadella described the feature as an upgrade over traditional keyword search, dubbing it was "semantic search over all your history." "It's not just about any document. We can recreate moments from the past," he said. He also provided a glimpse into what happens behind the scenes when the feature's turned on. Essentially, Windows takes screenshots of your screen constantly. It then uses a generative AI model right on the device along with the NPU to process all that data, including photos. Recall data stays local and private on a PC, and one can pause or delete logging as needed. However, it added that Recall won't proactively hide sensitive stuff like passwords or payment details.

29. ***University of Chicago Paper on Financial Statement Analysis:
The University of Chicago's study tested GPT-4's ability to analyze financial statements, finding it outperformed human analysts in predicting earnings changes even without narrative or industry-specific information. The LLM provided accurate insights and trading strategies based on its predictions yielded better financial metrics than other models, suggesting LLMs could play a significant role in financial decision-making.*** <br><br>
    20 May, Uni of Chicago published a [paper](https://bfi.uchicago.edu/wp-content/uploads/2024/05/BFI_WP_2024-65.pdf) “Financial Statement Analysis with Large Language Models”. The authors investigate whether an LLM can successfully perform financial statement analysis in a way similar to a professional human analyst. The research provides standardized and anonymous financial statements to GPT4 and instruct the model to analyze them to determine the direction of future earnings. Even without any narrative or industry specific information, the LLM outperforms financial analysts in its ability to predict earnings changes. The LLM exhibits a relative advantage over human analysts in situations when the analysts tend to struggle. Furthermore, the study finds that the prediction accuracy of the LLM is on par with the performance of a narrowly trained state-of-the-art ML model. LLM prediction does not stem from its training memory. Instead, the authors find that the LLM generates useful narrative insights about a company’s future performance. Lastly, the trading strategies based on GPT’s predictions yield a higher Sharpe ratio and alphas than strategies based on other models. Taken together, experimental results suggest that LLMs may take a central role in decision-making.

31. ***Collaborative Paper on INDUS:
A paper by IBM, NASA, and other institutions introduced INDUS, a suite of LLMs trained on scientific corpora to excel in Earth science, biology, and other fields. INDUS models include domain-specific encoders and smaller, efficient versions for resource-constrained applications. They outperform general-purpose and existing domain-specific models on newly created and existing scientific benchmarks, enhancing scientific research capabilities.*** <br><br>
    20 May, IBM, NASA, UAH, Navteca, Harvard-Smithsonian, etc published a [paper](https://arxiv.org/pdf/2405.10725) “INDUS: Effective and Efficient Language Models for Scientific Applications”. The paper argues that Large language models (LLMs) trained on general domain corpora showed remarkable results on natural language processing (NLP) tasks. However, previous research demonstrated LLMs trained using domain-focused corpora perform better on specialized tasks. Inspired by this pivotal insight, the research developed INDUS, a comprehensive suite of LLMs tailored for the Earth science, biology, physics, heliophysics, planetary sciences and astrophysics domains and trained using curated scientific corpora drawn from diverse data sources. The suite of models include: (1) an encoder model trained using domain-specific vocabulary and corpora to address natural language understanding tasks, (2) a contrastive-learning-based general text embedding model trained using a diverse set of datasets drawn from multiple sources to address information retrieval tasks and (3) smaller versions of these models created using knowledge distillation techniques to address applications which have latency or resource constraints. The researchers also created three new scientific benchmark datasets namely, CLIMATE-CHANGE-NER (entity-recognition), NASA-QA (extractive QA) and NASA-IR (IR) to accelerate research in these multi-disciplinary fields. Finally, the study shows that these models outperform both general-purpose encoders (RoBERTa) and existing domain-specific encoders (SciBERT) on these new tasks as well as existing benchmark tasks in the domains of interest.

33. ***Nature Human Behaviour Paper on Theory of Mind in LLMs:
Researchers compared LLMs like GPT-4 and LLaMA2 to humans on theory of mind tasks. GPT-4 performed at or above human levels in tasks like identifying false beliefs and indirect requests but struggled with detecting faux pas, which LLaMA2 handled better. These findings indicate that LLMs can exhibit human-like mental inference behaviors, though with some limitations and biases.*** <br><br>
    20 May, Nature Human Behaviour published a [paper](https://www.nature.com/articles/s41562-024-01882-z) written by researchers from Germany, Italy, the UK and the USA, “Testing theory of mind in large language models and humans”. The paper indicates that at the core of what defines us as humans is the concept of theory of mind: the ability to track other people’s mental states. The recent development of large language models (LLMs) such as ChatGPT has led to intense debate about the possibility that these models exhibit behaviour that is indistinguishable from human behaviour in theory of mind tasks. Here we compare human and LLM performance on a comprehensive battery of measurements that aim to measure different theory of mind abilities, from understanding false beliefs to interpreting indirect requests and recognizing irony and faux pas. We tested two families of LLMs (GPT and LLaMA2) repeatedly against these measures and compared their performance with those from a sample of 1,907 human participants. Across the battery of theory of mind tests, we found that GPT-4 models performed at, or even sometimes above, human levels at identifying indirect requests, false beliefs and misdirection, but struggled with detecting faux pas. Faux pas, however, was the only test where LLaMA2 outperformed humans. Follow-up manipulations of the belief likelihood revealed that the superiority of LLaMA2 was illusory, possibly reflecting a bias towards attributing ignorance. By contrast, the poor performance of GPT originated from a hyperconservative approach towards committing to conclusions rather than from a genuine failure of inference. These findings not only demonstrate that LLMs exhibit behaviour that is consistent with the outputs of mentalistic inference in humans but also highlight the importance of systematic testing to ensure a non-superficial comparison between human and artificial intelligences.

35. ***The Register Report on Google Cloud Outage:
Google Cloud experienced a major outage due to an automation bug during maintenance, affecting network connectivity across multiple services for nearly three hours. The incident disrupted services like Compute Engine and Kubernetes Engine. Google assured customers that the issue was resolved and unlikely to recur, though recent reliability issues remain a concern.*** <br><br>
    20 May, according to [Theregister](https://www.theregister.com/2024/05/20/google_cloud_network_outage/), in the week after its astounding deletion of Australian pension fund UniSuper's entire account, you might think Google Cloud would be on its very best behavior. NOPE, at 15:22 last Thursday, US Pacific Time, Google Cloud ran "maintenance automation intended to shutdown an unused network control component in a single location." Which is fair enough. It worked! Unfortunately, it also worked in other locations – about 40 of them, by The Register's count. The result was two hours and forty-eight minutes during which users of 33 Google Cloud Services – including biggies like the Compute Engine and Kubernetes Engine – experienced the following symptoms: 1) New VM instances were provisioned without network connectivity, hence unable to establish network connections; 2) Migrated/restarted VMs lost network connectivity; 3) Virtual networking configurations (firewalls, network load balancers etc.) could not be updated; 4)Partial packet loss for certain VPC network flows was observed in us-central1 and us-east1; 5)Cloud NAT Dynamic Port Allocation (DPA) experienced allocation failures; 6) Creation of new GKE nodes and nodepools experienced failures. Other services that needed VMs in Google Cloud Engine or network configuration updates "were not able to successfully complete operations during this time." The incident was over by 18:10 Pacific Time. Google has told customers that the incident was caused by a bug in the automation it used to shut down networks, and that once the flawed component was restarted the problem went away. And Google has told clients "There is no risk of a recurrence of this outage at the moment." Which isn't very reassuring, given the org's recent rotten track record.

37. ***Microsoft Copilot+ PC Announcement:
Microsoft introduced Copilot+ PCs, advanced Windows PCs designed for AI tasks with powerful processors and all-day battery life. Features include Recall for searching past activities, Cocreator for real-time image generation, and Live Captions for translating audio. These PCs support advanced AI applications and offer enhanced security and user experience, available from June 18.*** <br><br>
    20 May, Microsoft [introduced Copilot+ PC](https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/), a new category of Windows PCs designed for AI, Copilot+ PCs. Copilot+ PCs are the fastest, most intelligent Windows PCs ever built. With powerful new silicon capable of an incredible 40+ TOPS (trillion operations per second), all–day battery life and access to the most advanced AI models, Copilot+ PCs will enable one to do things one can’t on any other PC. Easily find and remember what one have seen in a PC with Recall, generate and refine AI images in near real-time directly on the device using Cocreator, and bridge language barriers with Live Captions, translating audio from 40+ languages into English.  It is the fastest, most secure Windows PCs ever built, and allows users to scroll across time to find the content in the timeline across any application, website, document, or more. Recall leverages a personal semantic index, built and stored entirely on the device. As Satya Nadella said in the [keynote speech](https://news.microsoft.com/wp-content/uploads/prod/2024/05/Satya-Nadella_Transcript_KEY01_Build2024.pdf), the other thing is that the Copilot Library also makes it easy for one to incorporate RAG inside of applications on device data. It gives one the right tools to build a vector store within their app. It enables one to do that semantic search that one saw with Recall, but now, in one’s application, one can construct these prompts using local data for RAG applications. Other features include image creator, and innovative AI experiences from creative apps such as Adobe, DaVinci Resolve Studio, CapCut, and to copilot everyday AI Companion. The new Copilot+ PCs from the biggest brands are available starting June 18.

39. ***Modular LLMs Paper by Microsoft and Collaborators:
A collaborative study explored the reuse of parameter-efficient LLM adapters for new tasks. The authors developed a library of adapters and a method, MBC, for grouping tasks based on adapter similarity. The Arrow routing mechanism allows dynamic adapter selection for new inputs without retraining. Experiments showed improved generalization, suggesting modular LLMs could match or outperform traditional training methods.*** <br><br>
    18 May, Microsoft, MILA, Uni de Montreal, Uni of Copenhagen, Uni of Edinburgh etc. published a [paper](https://arxiv.org/pdf/2405.11157) “Towards Modular LLMs by Building and Reusing a Library of LoRAs”. The authors indicate that the growing number of parameter-efficient adaptations of a base large language model (LLM) calls for studying whether we can reuse such trained adapters to improve performance for new tasks. This work studies how to best build a library of adapters given multi-task data and devise techniques for both zero-shot and supervised task generalization through routing in such library. The authors benchmark existing approaches to build this library and introduce model-based clustering, MBC, a method that groups tasks based on the similarity of their adapter parameters, indirectly optimizing for transfer across the multi-task dataset. To re-use the library, the study presents a novel zero-shot routing mechanism, Arrow, which enables dynamic selection of the most relevant adapters for new inputs without the need for retraining. The authors experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verifying that MBC-based adapters and Arrow routing lead to superior generalization to new tasks. The research makes steps towards creating modular, adaptable LLMs that can match or outperform traditional joint training.

41. ***Stanford University and Collaborators' Paper on Scaling Laws:
Researchers proposed an observational approach to understand language model performance scaling using data from ~80 publicly available models. They identified a generalized scaling law linking model performance to training compute efficiency. The study demonstrated predictability in scaling phenomena and the impact of post-training interventions, aiding in benchmarking and algorithm development.*** <br><br>
    17 May, Stanford Uni, Uni of Toronto and Vector Institute published a [paper](https://arxiv.org/pdf/2405.10938) “Observational Scaling Laws and the Predictability of Language Model Performance”. The authors state that understanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different scales has limited their use. The study proposes an alternative, observational approach that bypasses model training and instead builds scaling laws from ~80 publically available models. Building a single scaling law from multiple model families is challenging due to large variations in their training compute efficiencies and capabilities. However, the work shows that these variations are consistent with a simple, generalized scaling law where language model performance is a function of a low-dimensional capability space, and model families only vary in their efficiency in converting training compute to capabilities. Using this approach, the paper shows the surprising predictability of complex scaling phenomena: 1) several emergent phenomena follow a smooth, sigmoidal behavior and are predictable from small models; 2) the agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks; and the authors also show how to predict the impact of post-training interventions like Chain-of-Thought and Self-Consistency as language model capabilities continue to improve.

43. ***CNBC Report on AI Risks:
A survey revealed that data leaks are the primary concern for companies using generative AI, surpassing issues like hallucinations. While many organizations report successful AI pilots, data privacy and security remain significant challenges. Experts advise cautious and incremental AI adoption to mitigate risks, emphasizing the importance of understanding data management and regulatory changes.*** <br><br>
    16 May, according to [CNBC](https://www.cnbc.com/2024/05/16/the-no-1-risk-companies-see-in-gen-ai-usage-isnt-hallucinations.html), The biggest risk corporations see in gen AI usage isn’t hallucinations, but what organizations worry about most is data leaks. Most companies (77%) report successful gen AI pilots, according to a recent survey from Alteryx, but 80% cite data privacy and security concerns as the top challenges in scaling AI. Meanwhile, 45% of organizations encountered unintended data exposure when implementing AI solutions, according to AvePoint’s 2024 AI and Information Management Report. Microsoft AI’s leak of 38 terabytes of data late last year is just one example of how big this problem can get. “AI has certainly amplified and accelerated some of the challenges that we see around data management,” said Dana Simberkoff, chief risk, privacy and information security officer at AvePoint, which provides technology to help organizations manage, migrate and protect their data in the cloud and on premise. Simberkoff explains that much of this leaked information is unstructured data that’s sitting in collaboration spaces, unprotected but previously undiscovered due to the difficulty around finding it. “It’s often what we call dark data,” Simberkoff said. Arvind Jain, CEO and cofounder of enterprise search platform Glean, which creates gen AI-powered enterprise-wide search tools and was named this week to the 2024 CNBC Disruptor 50 list, says there’s immense pressure on chief information officers and related roles to deploy AI, leaving a lot of room for error in the race to modernity. “It was so hard to find anything. Nobody knows where to look,” said Jain. “That’s the thing that AI fundamentally changes. We don’t have to go and look anywhere anymore. You just have to ask a question.”  One thing Simberkoff says many leaders forget is that it’s okay to pause in the AI adoption journey. “Organizations may sort of rush to adopt AI and then have to pause. That’s okay,” she said. “One of the things that we’ve seen that can be very effective is thinking about this in incremental steps, which is that you can start off with something like an acceptable use policy and a strategy, but it’s always good to test the waters with a pilot.” Plus, Simberkoff says regulation and laws are changing, so having a good understanding of your data over time just makes sense.

45. ***Nature Article on Understanding LLMs:
An article discussed the complexity of understanding LLMs like ChatGPT. Researchers use explainable AI techniques and psychological methods to study LLM behavior, revealing human-like reasoning and erratic actions. Neuroscientific approaches help identify and tweak crucial neural network areas. While progress is made, much about LLMs remains unknown, and transparency is crucial for accountability.*** <br><br>
    14 May, Nature published a [paper](https://www.nature.com/articles/d41586-024-01314-y) “How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models”. The article discusses the challenges and efforts to understand the internal workings of AI large language models (LLMs) like ChatGPT. David Bau, a computer scientist, highlights the complexity of modern AI systems compared to conventional software, which can often be understood by insiders. The latest AI systems, using machine learning and neural networks, are often referred to as 'black boxes' due to their opaque inner workings. Researchers are employing techniques from explainable AI (XAI) to demystify these systems. Standard XAI methods involve highlighting influential parts of data or building decision trees to approximate AI behavior. These methods have had some success but remain a work in progress, particularly for LLMs, which have billions of parameters. Some studies suggest that LLMs exhibit human-like reasoning and erratic behavior. For instance, a team at Anthropic demonstrated that LLMs might role-play, drawing on training data to answer questions. Other researchers, like those at Harvard, have shown that LLMs can construct internal models of the world, as seen in experiments with the board game Othello. Researchers also use psychological methods, treating LLMs like human subjects to study their behavior. Techniques like 'chain-of-thought prompting' improve AI's reasoning abilities by guiding it through step-by-step logic. However, some researchers caution that these explanations might not reflect the true internal processes of LLMs. Studies have shown that models can fabricate logical explanations for their answers, similar to how humans justify decisions based on biases. Neuroscientific approaches are also being applied to LLMs. For example, researchers have developed methods to scan and edit neural networks, identifying crucial areas responsible for specific responses and tweaking them to improve accuracy. While progress has been made in understanding LLMs, much remains unknown. Regulations, like the European Union’s AI Act, require explainability for high-risk AI systems, but LLMs might not always fall under these categories. Researchers argue that companies should be responsible for ensuring the transparency and explainability of their models to avoid a lack of accountability.

47. ***FinalSpark Paper on Neuroplatform:
FinalSpark introduced a Neuroplatform for remote research in wetware computing using living neurons for computations. The system supports large-scale electrophysiological experiments on neural organoids, providing tools for automated and remote research. The platform facilitates complex experiments and data collection, advancing the field of organoid intelligence.*** <br><br>
    2 May, Frontiers in Artificial Intelligence published a [paper](https://www.frontiersin.org/articles/10.3389/frai.2024.1376042/full) from FinalSpark, an institute from Switzerland, “Open and remotely accessible Neuroplatform for research in wetware computing”. Wetware computing and organoid intelligence is an emerging research field at the intersection of electrophysiology and artificial intelligence. The core concept involves using living neurons to perform computations, similar to how Artificial Neural Networks (ANNs) are used today. However, unlike ANNs, where updating digital tensors (weights) can instantly modify network responses, entirely new methods must be developed for neural networks using biological neurons. Discovering these methods is challenging and requires a system capable of conducting numerous experiments, ideally accessible to researchers worldwide. For this reason, we developed a hardware and software system that allows for electrophysiological experiments on an unmatched scale. The Neuroplatform enables researchers to run experiments on neural organoids with a lifetime of even more than 100 days. To do so, we streamlined the experimental process to quickly produce new organoids, monitor action potentials 24/7, and provide electrical stimulations. We also designed a microfluidic system that allows for fully automated medium flow and change, thus reducing the disruptions by physical interventions in the incubator and ensuring stable environmental conditions. Over the past three years, the Neuroplatform was utilized with over 1,000 brain organoids, enabling the collection of more than 18 terabytes of data. A dedicated Application Programming Interface (API) has been developed to conduct remote research directly via our Python library or using interactive compute such as Jupyter Notebooks. In addition to electrophysiological operations, our API also controls pumps, digital cameras and UV lights for molecule uncaging. This allows for the execution of complex 24/7 experiments, including closed-loop strategies and processing using the latest deep learning or reinforcement learning libraries. Furthermore, the infrastructure supports entirely remote use. Currently in 2024, the system is freely available for research purposes, and numerous research groups have begun using it for their experiments. This article outlines the system’s architecture and provides specific examples of experiments and results.

49. ***Google Paper on Mitigating LLM Hallucinations:
Google proposed a method to reduce LLM hallucinations by having models abstain from answering uncertain queries. Using self-consistency and conformal prediction, the approach ensures rigorous error rate control. Experiments showed effective hallucination rate reduction while maintaining performance, offering a promising strategy for improving LLM reliability.*** <br><br>
    4 Apr, Google published a [paper](https://arxiv.org/pdf/2405.01563) “Mitigating LLM Hallucinations via Conformal Abstention”. The authors develop a principled procedure for determining when a large language model (LLM) should abstain from responding (e.g., by . saying "I don't know") in a general domain, instead of resorting to possibly "hallucinating" a non-sensical or incorrect answer. Building on earlier approaches that use self-consistency as a more reliable measure of model confidence, we propose using the LLM itself to self-evaluate the similarity between each of its sampled responses for a given query. We then further leverage conformal prediction techniques to develop an abstention procedure that benefits from rigorous theoretical guarantees on the hallucination rate (error rate). Experimentally, the resulting conformal abstention method reliably bounds the hallucination rate on various closed-book, open-domain generative question answering datasets, while also maintaining a significantly less conservative abstention rate on a dataset with long responses (Temporal Sequences) compared to baselines using log-probability scores to quantify uncertainty, while achieveing comparable performance on a dataset with short answers (TriviaQA). To evaluate the experiments automatically, one needs to determine if two responses are equivalent given a question. Following standard practice, the authors use a thresholded similarity function to determine if two responses match, but also provide a method for calibrating the threshold based on conformal prediction, with theoretical guarantees on the accuracy of the match prediction, which might be of independent interest.
 <br><br><br>

***19 May 2024***

1. ***The complexity of large language models (LLMs) like ChatGPT has prompted researchers to investigate their inner workings using psychology, neuroscience, and explainable AI (XAI). Unlike traditional software, LLMs function as 'black boxes,' with their decision-making processes being opaque even to their developers. Efforts in XAI aim to clarify these processes, ensuring safer and more efficient AI. Despite LLMs mimicking human reasoning, they can act unpredictably, sometimes spreading misinformation or biases. Advanced techniques like causal tracing and neuron-level analysis have provided insights, but understanding LLMs remains challenging, requiring ongoing research and regulatory measures for transparency and accountability.*** <br><br>
   17 May, Nature published [an article](https://www.nature.com/articles/d41586-024-01314-y) “How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models - 
Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.” The complexity of large language models (LLMs), such as ChatGPT, has led researchers to explore their inner workings using methods from psychology, neuroscience, and explainable AI (XAI). Unlike traditional software, LLMs, which use neural networks inspired by the brain, often operate as 'black boxes' whose decision-making processes are opaque even to their developers. Efforts in XAI aim to illuminate these processes, revealing why LLMs make certain decisions and helping to build safer, more efficient AI. Studies have shown LLMs can mimic reasoning and exhibit behaviors similar to humans, but they can also act unpredictably, sometimes generating misinformation or exhibiting biases. Advanced techniques, including causal tracing and neuron-level analysis, have provided insights into LLM behavior and potential for model editing. Despite progress, understanding LLMs remains challenging, necessitating ongoing research and regulatory measures to ensure transparency and accountability in AI applications.

3. ***The resurgence of private cloud strategies among enterprises is driven by the growing complexity and cost of AI workloads. A 2023 Forrester survey shows that 79% of enterprise cloud decision-makers are implementing private clouds, with IDC predicting significant growth in spending on private cloud services. While public cloud providers dominate the market, AI's cost control and data security concerns are shifting infrastructures towards hybrid clouds. Despite challenges, solutions like colocation data centers are emerging to support these infrastructures. This trend is expected to continue as AI technologies advance, providing cost-effective solutions for enterprises.*** <br><br>
   17 May, InfoWorld published [an article](https://www.infoworld.com/article/3715483/is-generative-ai-bringing-back-private-clouds.html) “Is generative AI bringing back private clouds?” The resurgence of private cloud strategies among enterprises is driven by the growing complexity and cost of AI workloads. Forrester's 2023 survey reveals that 79% of enterprise cloud decision-makers are implementing private clouds, with IDC forecasting significant growth in spending on private cloud services and infrastructure. While public cloud providers dominate the market, AI is prompting a shift towards hybrid cloud infrastructures due to cost control and data security concerns. Despite the challenges, such as the need for specialized hardware, solutions like colocation data centers are emerging to support these infrastructures. As AI technologies advance, the trend towards private clouds is expected to continue, offering cost-effective and valuable solutions for enterprises.

5. ***Generative AI technology is advancing rapidly, offering new capabilities for those who understand how to use them. Generative AI agents—AI-powered entities that perform tasks or assist humans—present great opportunities for organizations. However, widespread adoption has been hindered by data quality, employee distrust, and implementation costs. As gen AI technologies progress, more use cases will emerge, deployment costs will decrease, and automation will scale across enterprise processes, employee experiences, and customer interfaces. This evolution requires investing in strong AI trust, risk management practices, and platforms for managing agent-based systems.*** <br><br>
   17 May, Mckinsey published [an article](https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/the-promise-and-the-reality-of-gen-ai-agents-in-the-enterprise) “The promise and the reality of gen AI agents in the enterprise”. Generative AI technology is improving so quickly that a range of new capabilities are rapidly coming online, but only for those who can understand how to use them. The evolution of generative AI (gen AI) has opened the door to great opportunities across organizations, particularly regarding gen AI agents—AI-powered software entities that plan and perform tasks or aid humans by delivering specific services on their behalf. So far, adoption at scale across businesses has faced difficulties because of data quality, employee distrust, and cost of implementation. In addition, capabilities have raced ahead of leaders’ capacity to imagine how these agents could be used to transform work. However, as gen AI technologies progress and the next-generation agents emerge, we expect more use cases to be unlocked, deployment costs to decrease, long-tail use cases to become economically viable, and more at-scale automation to take place across a wider range of enterprise processes, employee experiences, and customer interfaces. This evolution will demand investing in strong AI trust and risk management practices and policies as well as platforms for managing and monitoring agent-based systems.

7. ***Arup, a British multinational design and engineering firm, fell victim to a deepfake scam, resulting in a Hong Kong employee transferring $25 million to fraudsters. Fake voices and images deceived the finance worker into believing he was communicating with the company's CFO and other staff. Despite initial suspicion of a phishing email, the realistic video call convinced the employee. Arup stated that its financial stability and internal systems remained unaffected. The scam underscores the rising sophistication of cyberattacks, with Arup experiencing regular threats like invoice fraud, phishing, and deepfakes, emphasizing the need for vigilance.*** <br><br>
   17 May, [according to CNN](https://edition.cnn.com/2024/05/16/tech/arup-deepfake-scam-loss-hong-kong-intl-hnk/index.html), A British multinational design and engineering firm, Arup, confirmed it was the victim of a deepfake scam, resulting in a Hong Kong employee transferring $25 million to fraudsters. The incident involved the use of fake voices and images to deceive the finance worker into believing he was communicating with the company's chief financial officer and other staff. The employee, initially suspicious of a phishing email, was convinced after a realistic video call. Despite the loss, Arup stated that its financial stability and internal systems remained unaffected. The scam highlights the rising sophistication of cyberattacks globally, with Arup experiencing regular threats such as invoice fraud, phishing, and deepfakes. Arup's leadership emphasized the growing concern over such technology and the need for vigilance.

9. ***Hugging Face, a leading machine learning company, is committing $10 million in free shared GPUs to help developers create new AI technologies. This initiative aims to support small developers, academics, and startups in competing with tech giants. Hugging Face's CEO, Delangue, expressed concern about AI startups' ability to compete due to the centralization of AI advancements within major tech companies. By donating GPUs through the ZeroGPU program, Hugging Face aims to enable open AI development, fostering collaboration and transparency in the AI community.*** <br><br>
    16 May, [according to theverge](https://www.theverge.com/2024/5/16/24156755/hugging-face-celement-delangue-free-shared-gpus-ai), Hugging Face, one of the biggest names in machine learning, is committing $10 million in free shared GPUs to help developers create new AI technologies. The goal is to help small developers, academics, and startups counter the centralization of AI advancements. Delangue, CEO of Huggingface, is concerned about AI startups’ ability to compete with the tech giants. Most significant advancements in artificial intelligence — like GPT-4, the algorithms behind Google Search, and Tesla’s Full Self-Driving system — remain hidden within the confines of major tech companies. Not only are these corporations financially incentivized to keep their models proprietary, but with billions of dollars at their disposal for computational resources, they can compound those gains and race ahead of competitors, making it impossible for startups to keep up. Hugging Face aims to level the playing field by donating these shared GPUs to the community through a new program called ZeroGPU. With AI rapidly advancing behind closed doors, the goal of Hugging Face is to allow people to build more AI tech in the open. “AI should not be held in the hands of the few. With this commitment to open-source developers, we’re excited to see what everyone will cook up next in the spirit of collaboration and transparency,” Delangue said in a press release.

11. ***A paper by Stanford University explores the capabilities of multimodal foundation models in many-shot in-context learning (ICL). Evaluating models like GPT-4o and Gemini 1.5 Pro across various datasets and tasks, the study finds that many-shot ICL significantly improves performance compared to few-shot ICL. Techniques like batching multiple queries in a single API call enhance performance and reduce costs. The research shows that Gemini 1.5 Pro has higher ICL data efficiency than GPT-4o, suggesting that many-shot ICL can efficiently adapt multimodal models to new applications and domains.*** <br><br>
    16 May, Stanford Uni published a [paper](https://arxiv.org/pdf/2405.09798) “Many-Shot In-Context Learning in Multimodal Foundation Models”. The paper states that large language models are well-known to be effective at few-shot in-context learning (ICL). Recent advancements in multimodal foundation models have enabled unprecedentedly long context windows, presenting an opportunity to explore their capability to perform ICL with many more demonstrating examples. This work evaluates the performance of multimodal foundation models scaling from few-shot to many-shot ICL. The authors benchmark GPT-4o and Gemini 1.5 Pro across 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification). The authors observe that many-shot ICL, including up to almost 2,000 multimodal demonstrating examples, leads to substantial improvements compared to few-shot (<100 examples) ICL across all of the datasets. Further, Gemini 1.5 Pro performance continues to improve log-linearly up to the maximum number of tested examples on many datasets. Given the high inference costs associated with the long prompts required for many-shot ICL, the authors also explore the impact of batching multiple queries in a single API call. The paper shows that batching up to 50 queries can lead to performance improvements under zero-shot and many-shot ICL, with substantial gains in the zero-shot setting on multiple datasets, while drastically reducing per-query cost and latency. Finally, the work measures ICL data efficiency of the models, or the rate at which the models learn from more demonstrating examples. The authors find that while GPT-4o and Gemini 1.5 Pro achieve similar zero-shot performance across the datasets, Gemini 1.5 Pro exhibits higher ICL data efficiency than GPT-4o on most datasets. The results suggest that many-shot ICL could enable users to efficiently adapt multimodal foundation models to new applications and domains. Codebase is publicly available at [this https URL](https://github.com/stanfordmlgroup/ManyICL).

13. ***Columbia University and Databricks published a paper comparing Low-Rank Adaptation (LoRA) and full finetuning methods for large language models. LoRA, which saves memory by training low-rank perturbations, generally underperforms compared to full finetuning but better maintains the base model's performance on tasks outside the target domain. LoRA provides stronger regularization, maintaining diverse generations. The paper suggests that the rank of perturbations learned in full finetuning explains the performance gap and offers best practices for finetuning with LoRA.*** <br><br>
    16 May, Columbia Uni and Databricks published a [paper](https://arxiv.org/pdf/2405.09673) “LoRA Learns Less and Forgets Less”. The paper argues that Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. This work compares the performance of LoRA and full finetuning on two target domains, programming and mathematics. The authors consider both the instruction finetuning (≈100K prompt-response pairs) and continued pretraining (≈10B unstructured tokens) data regimes. The results show that, in most settings, LoRA substantially underperforms full finetuning. Nevertheless, LoRA exhibits a desirable form of regularization: it better maintains the base model's performance on tasks outside the target domain. The paper shows that LoRA provides stronger regularization compared to common techniques such as weight decay and dropout; it also helps maintain more diverse generations. The study shows that full finetuning learns perturbations with a rank that is 10-100X greater than typical LoRA configurations, possibly explaining some of the reported gaps. The paper concludes by proposing best practices for finetuning with LoRA.

15. ***Meta's paper on "Chameleon" presents a family of early-fusion, token-based mixed-modal models capable of understanding and generating images and text. Chameleon models are evaluated on tasks such as visual question answering, image captioning, text generation, and image generation, demonstrating state-of-the-art performance in various tasks. Chameleon matches or exceeds the performance of larger models like Gemini Pro and GPT-4V, marking a significant step in unified multimodal modeling.*** <br><br>
    16 May, Meta published a [paper](https://arxiv.org/pdf/2405.09818) “Chameleon: Mixed-Modal Early-Fusion Foundation Models”. Chameleon is a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. The paper outlines a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.

17. ***Google's release of PaliGemma, an open vision-language model, is designed for versatile vision-language tasks. Built with open components, PaliGemma integrates the SigLIP vision model and Gemma language model, enabling tasks like image captioning, visual question answering, and object detection. Google provides pretrained and transfer checkpoints, allowing PaliGemma to be fine-tuned for specific tasks. The model aims to enhance document understanding and reasoning capabilities through its unique architecture.*** <br><br>
    15 May, Google released [PaliGemma](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md), an open vision-language model (VLM) inspired by PaLI-3 recipe, built with open components, such as the SigLIP vision model (So400m/14) and the Gemma 2B language model. PaliGemma is designed as a versatile model for transfer to a wide range of vision-language tasks such as image and short video caption, visual question answering, text reading, object detection and object segmentation. Together with the pretrained and transfer checkpoints at multiple resolutions, Google provides a checkpoint transferred to a mixture of tasks that can be used for off-the-shelf exploration. PaliGemma takes as input one or more images, which are turned into "soft tokens" by the SigLIP encoder, and input text (codenamed the "prefix") that is tokenized by Gemma's tokenizer. The image tokens and prefix tokens are concatenated (in this order) and passed to the Gemma decoder with full block-attention, which then generates an output text (the "suffix") auto-regressively with masked attention. Gemma is a decoder-only model for text generation. Combining the image encoder of SigLIP with Gemma using a linear adapter makes PaliGemma a powerful vision language model. The PaliGemma release comes with three types of models: 1) PT checkpoints: Pretrained models that can be fine-tuned to downstream tasks. 2) Mix checkpoints: PT models fine-tuned to a mixture of tasks. They are suitable for general-purpose inference with free-text prompts, and have great document understanding and reasoning capabilities. 3) FT checkpoints: A set of fine-tuned models, each one specialized on a different academic benchmark. They are available in various resolutions and are intended for research purposes only.

19. ***Microsoft's Project ALPINE investigates the planning capabilities of Transformer-based language models through autoregressive learning. The study abstracts planning as a network path-finding task and shows that Transformers can execute path-finding by embedding adjacency and reachability matrices within their weights. However, Transformers have limitations in identifying reachability relationships through transitivity. The study's theoretical and empirical analyses contribute to understanding the planning capabilities of autoregressive learning in networks.*** <br><br>
    15 May, Microsoft published a [paper](https://arxiv.org/pdf/2405.09220) “ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models”. The paper presents the findings of Project ALPINE which stands for ``Autoregressive Learning for Planning In NEtworks." Project ALPINE initiates a theoretical investigation into the development of planning capabilities in Transformer-based language models through their autoregressive learning mechanisms, aiming to identify any potential limitations in their planning abilities. The study abstracts planning as a network path-finding task where the objective is to generate a valid path from a specified source node to a designated target node. In terms of expressiveness, the paper shows that the Transformer is capable of executing path-finding by embedding the adjacency and reachability matrices within its weights. The theoretical analysis of the gradient-based learning dynamic of the Transformer reveals that the Transformer is capable of learning both the adjacency matrix and a limited form of the reachability matrix. These theoretical insights are then validated through experiments, which demonstrate that the Transformer indeed learns the adjacency matrix and an incomplete reachability matrix, which aligns with the predictions made in the theoretical analysis. Additionally, when applying the methodology to a real-world planning benchmark, called Blocksworld, the authors’ observations remain consistent. The theoretical and empirical analyses further unveil a potential limitation of Transformer in path-finding: it cannot identify reachability relationships through transitivity, and thus would fail when path concatenation is needed to generate a path. In summary, the findings shed new light on how the internal mechanisms of autoregressive learning enable planning in networks. This study may contribute to the understanding of the general planning capabilities in other related domains.

21. ***A paper by Google examines the performance gap between online and offline alignment algorithms in reinforcement learning from human feedback (RLHF). The study finds that online methods have a clear advantage over offline methods in reward optimization. Hypotheses such as offline data coverage and quality do not fully explain the performance difference. The research highlights the importance of on-policy sampling in AI alignment and the challenges of offline alignment algorithms, emphasizing the unique interplay between discriminative and generative capabilities.*** <br><br>
    14 May, Google published a [paper](https://arxiv.org/pdf/2405.08448) “Understanding the performance gap between online and offline alignment algorithms”. The paper states that reinforcement learning from human feedback (RLHF) is the canonical framework for large language model alignment. However, rising popularity in offline alignment algorithms challenge the need for on-policy sampling in RLHF. Within the context of reward over-optimization, the study starts with an opening set of experiments that demonstrate the clear advantage of online methods over offline methods. This prompts the authors to investigate the causes to the performance discrepancy through a series of carefully designed experimental ablations. This work shows empirically that hypotheses such as offline data coverage and data quality by itself cannot convincingly explain the performance difference. The authors also find that while offline algorithms train policy to become good at pairwise classification, it is worse at generations; in the meantime the policies trained by online algorithms are good at generations while worse at pairwise classification. This hints at a unique interplay between discriminative and generative capabilities, which is greatly impacted by the sampling process. Lastly, the research observes that the performance discrepancy persists for both contrastive and non-contrastive loss functions, and appears not to be addressed by simply scaling up policy networks. Taken together, this study sheds light on the pivotal role of on-policy sampling in AI alignment, and hints at certain fundamental challenges of offline alignment algorithms.

23. ***Salesforce and UIUC's paper details the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF), aiming to outperform offline methods. The study constructs preference models using open-source datasets to approximate human feedback and discusses theoretical insights and algorithmic principles of online RLHF. The trained model achieves impressive performance on various benchmarks. The paper provides detailed implementation guides, datasets, and code for open-source communities, demonstrating the potential of supervised fine-tuning and iterative RLHF.*** <br><br>
    13 May, Salesforce and UIUC published a [paper](https://arxiv.org/pdf/2405.07863) “RLHF Workflow: From Reward Modeling to Online RLHF”. The authors present the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF) in this technical report, which is widely reported to outperform its offline counterpart by a large margin in the recent large language model (LLM) literature. However, existing open-source RLHF projects are still largely confined to the offline learning setting. This paper aims to fill in this gap and provide a detailed recipe that is easy to reproduce for online iterative RLHF. In particular, since online human feedback is usually infeasible for open-source communities with limited resources, the study starts by constructing preference models using a diverse set of open-source datasets and using the constructed proxy preference model to approximate human feedback. Then, the authors discuss the theoretical insights and algorithmic principles behind online iterative RLHF, followed by a detailed practical implementation. The trained LLM, SFR-Iterative-DPO-LLaMA-3-8B-R, achieves impressive performance on LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarks such as HumanEval and TruthfulQA. The study has shown that supervised fine-tuning (SFT) and iterative RLHF can obtain state-of-the-art performance with fully open-source datasets. Further, the authors have made the models, curated datasets, and comprehensive step-by-step code guidebooks publicly available. Please refer to https://github.com/RLHFlow/RLHF-Reward-Modeling and https://github.com/RLHFlow/Online-RLHF for more detailed information.

25. ***A survey paper by MIT, ACU, and others discusses AI deception, defining it as inducing false beliefs for outcomes other than the truth. The authors review empirical examples of AI deception, the associated risks, and potential solutions. They recommend regulatory frameworks, bot-or-not laws, and prioritizing research funding to detect and reduce AI deception. The paper emphasizes the importance of proactive measures to prevent AI deception from destabilizing society.*** <br><br>
    10 May, MIT, ACU and other published a [paper](https://www.courthousenews.com/wp-content/uploads/2024/05/PATTER100988_proof.pdf) on Patterns “AI Deception: A Survey of Examples, Risks, and Potential Solutions”. This paper argues that a range of current AI systems have learned how to deceive humans. The authors define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. The authors first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, the paper outlines several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.

27. ***Technion–Israel Institute of Technology's paper on autonomous LLM-driven research explores whether AI can adhere to scientific values like transparency, traceability, and verifiability. The study presents an automation platform that guides LLM agents through the research process, generating research papers with programmatic information tracing. While fully autonomous research shows promise, human oversight is crucial for complex goals. The work demonstrates AI's potential to accelerate scientific discovery while enhancing traceability and transparency.*** <br><br>
    24 Apr, Technion–Israel Institute of Technology published a [paper](https://arxiv.org/pdf/2404.17605) “Autonomous LLM-driven research from data to human-verifiable research papers”. The paper indicates that as AI promises to accelerate scientific discovery, it remains unclear whether fully AI-driven research is possible and whether it can adhere to key scientific values, such as transparency, traceability and verifiability. Mimicking human scientific practices, the authors built data-to-paper, an automation platform that guides interacting LLM agents through a complete stepwise research process, while programmatically back-tracing information flow and allowing human oversight and interactions. In autopilot mode, provided with annotated data alone, data-to-paper raised hypotheses, designed research plans, wrote and debugged analysis codes, generated and interpreted results, and created complete and information-traceable research papers. Even though research novelty was relatively limited, the process demonstrated autonomous generation of de novo quantitative insights from data. For simple research goals, a fully-autonomous cycle can create manuscripts which recapitulate peer-reviewed publications without major errors in about 80-90%, yet as goal complexity increases, human co-piloting becomes critical for assuring accuracy. Beyond the process itself, created manuscripts too are inherently verifiable, as information-tracing allows to programmatically chain results, methods and data. The work thereby demonstrates a potential for AI-driven acceleration of scientific discovery while enhancing, rather than jeopardizing, traceability, transparency and verifiability. Here is [the github link](https://github.com/technion-kishony-lab/data-to-paper).

29. ***New York University's paper investigates the hidden computation in Transformer language models, particularly through chain-of-thought responses. The study finds that intermediate tokens can act as filler tokens, providing computational benefits independent of their content. This raises concerns about unauditable, hidden computations in large language models. The findings suggest that additional tokens can enhance performance but highlight the need for careful supervision to ensure transparency and accountability in AI computations.*** <br><br>
    24 Apr, New York Uni published a [paper](https://arxiv.org/pdf/2404.15758) “Let's Think Dot by Dot: Hidden Computation in Transformer Language Models”. The authors find that Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. The study shows that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, the authors find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. The paper also provides a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, the results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.
 <br><br><br>

***12 May 2024***

1. ***Researchers from several institutions published a paper in Science detailing a high-resolution reconstruction of a portion of the human cerebral cortex. The reconstruction, which comprises 57,000 cells, 230 millimeters of blood vessels, and 150 million synapses, offers valuable insights into the brain's structure and function. Glia cells outnumber neurons, and the study highlights rare powerful axonal inputs among thousands of weaker connections. Further exploration using this resource promises to deepen our understanding of the human brain.*** <br><br>
   10 May, Harvard Uni, Uni of London, Google, Princeton Uni. etc published a [paper](https://www.science.org/doi/10.1126/science.adk4858) on Science, “A petavoxel fragment of human cerebral cortex reconstructed at nanoscale resolution”. The authors indicate that to fully understand how the human brain works, knowledge of its structure at high resolution is needed. Presented here is a computationally intensive reconstruction of the ultrastructure of a cubic millimeter of human temporal cortex that was surgically removed to gain access to an underlying epileptic focus. It contains about 57,000 cells, about 230 millimeters of blood vessels, and about 150 million synapses and comprises 1.4 petabytes. Our analysis showed that glia outnumber neurons 2:1, oligodendrocytes were the most common cell, deep layer excitatory neurons could be classified on the basis of dendritic orientation, and among thousands of weak connections to each neuron, there exist rare powerful axonal inputs of up to 50 synapses. Further studies using this resource may bring valuable insights into the mysteries of the human brain. (**Comments: [Some](https://news.ycombinator.com/item?id=36413296) believe GPT4 has about 1.7T parameters, and the human brain has about 3000 trillion synapses, so GPT4 is  about 0.00057 of human brain if take synapses as parameters in GPT4**)

3. ***The UK’s AI Safety Institute has released its AI testing and evaluation platform, Inspect, to the global AI community. Inspect aims to facilitate safe innovation in AI models by enabling users to assess specific capabilities and produce scores based on their results. This open-source software library supports diverse applications, including core knowledge assessment, reasoning ability evaluation, and autonomous capabilities testing, contributing to consistent AI safety evaluations worldwide.*** <br><br>
   10 May, according to [infosecurity-magazine](https://www.infosecurity-magazine.com/news/platform-to-accelerate-safe-ai/), The UK’s AI Safety Institute has made its AI testing and evaluation platform available to the global AI community as of 10 May, 2024. The platform, called Inspect, is set to pave the way for the safe innovation of AI models, according to the AI Safety Institute and Department for Science, Innovation and Technology (DIST). By making Inspect available to the global community, the Institute said it is helping accelerate the work on AI safety evaluations carried out internationally. The aim is that this leads to better safety testing and the development of more secure models. It also allows for a consistent approach to AI safety evaluations around the world. according to the government. Inspect is a software library which enables testers – from start-ups, academia and AI developers to international governments – to assess specific capabilities of individual models and then produce a score based on their results. Inspect can be used to evaluate models in a range of areas, including their core knowledge, ability to reason, and autonomous capabilities. Released through an open-source license, it means Inspect it is now freely available for the AI community to use. AI Safety Institute Chair, Ian Hogarth, commented: “We have been inspired by some of the leading open source AI developers - most notably projects like GPT-NeoX, OLMo or Pythia which all have publicly available training data and OSI-licensed training and evaluation code, model weights, and partially trained checkpoints. This is our effort to contribute back.”

5. ***A study by Google and the Israel Institute of Technology investigates the impact of fine-tuning large language models (LLMs) on new knowledge acquisition and its potential to induce factual inaccuracies or hallucinations. The research reveals that LLMs struggle to incorporate new factual knowledge efficiently during fine-tuning, leading to a linear increase in hallucinations as new knowledge is acquired. These findings underscore the challenges of introducing new knowledge through fine-tuning and emphasize the reliance of LLMs on pre-existing knowledge acquired during pre-training.*** <br><br>
   9 May, Google and Israel Inst. of Tech. published a [paper](https://arxiv.org/pdf/2405.05904) “Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?” The paper argues that when large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. This paper studies the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, the authors design a controlled setup, focused on closed-book QA, where the authors vary the proportion of the fine-tuning examples that introduce new knowledge. The study demonstrates that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, the study also found that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate. Taken together, the results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently.

7. ***Court documents from a class-action lawsuit against OpenAI reveal the removal of two extensive datasets, referred to as "books1" and "books2," which were used to train the GPT-3 AI model. These datasets, containing over 100,000 published books, were central to claims of copyright infringement. OpenAI ceased using the datasets in late 2021 and subsequently deleted them in mid-2022. The departure of the researchers responsible for creating the datasets adds complexity to the legal dispute.*** <br><br>
   7 May, BusinessInsider published an [article](https://www.businessinsider.com/openai-destroyed-ai-training-datasets-lawsuit-authors-books-copyright-2024-5) “OpenAI destroyed a trove of books used to train AI models. The employees who collected the data are gone.”. Recently unsealed court documents in the class-action lawsuit filed by the Authors Guild against OpenAI reveal that the startup removed two extensive datasets, referred to as “books1” and “books2,” which had been utilized to train its GPT-3 artificial intelligence model. Attorneys representing the Authors Guild asserted in legal filings that these datasets likely contained “over 100,000 published books” and were pivotal to their claims that OpenAI employed copyrighted materials for AI model training. In a 2020 white paper, OpenAI characterized the “books1” and “books2” datasets as “internet-based book corpora,” constituting 16% of the training data used in developing GPT-3. The same white paper indicated that the combined “books1” and “books2” datasets comprised 67 billion tokens, roughly equivalent to 50 billion words. Notably, the use of “books1” and “books2” for model training ceased in late 2021, and the datasets were subsequently deleted in mid-2022 due to disuse. Additionally, the two researchers responsible for creating “books1” and “books2” are no longer affiliated with OpenAI.

9. ***Researchers from JKU Linz, Austria, introduce xLSTM, an extension of Long Short-Term Memory (LSTM) models designed to address their limitations in language modeling compared to Transformer technology. By incorporating exponential gating and modifying LSTM memory structures, xLSTM demonstrates improved performance and scalability in language modeling tasks, rivaling state-of-the-art Transformer models and outperforming LSTMs at scale.*** <br><br>
    7 May, JKU Linz Austria published a [paper](https://arxiv.org/pdf/2405.04517) “xLSTM: Extended Long Short-Term Memory”. The authors indicate that in the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. The authors now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, the study introduces exponential gating with appropriate normalization and stabilization techniques. Secondly, the authors modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.

11. ***The International Conference on Learning Representations (ICLR) 2024 announces Outstanding Paper Awards and a Test of Time Award. Recognized papers cover a range of topics, including generalization in diffusion models, learning interactive real-world simulators, and protein discovery. The Test of Time Award goes to "Auto-Encoding Variational Bayes," highlighting the enduring impact of the paper in the field.*** <br><br>
    7 May, ICLR 2024 announced the [Outstanding Paper](https://blog.iclr.cc/2024/05/06/iclr-2024-outstanding-paper-awards/) Awards and [Test of Time Award](https://blog.iclr.cc/2024/05/07/iclr-2024-test-of-time-award/). The titles of the outstanding paper awards include: Generalization in diffusion models arises from geometry-adaptive harmonic representations; Learning Interactive Real-World Simulators; Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors; Protein Discovery with Discrete Walk-Jump Sampling; and Vision Transformers Need Registers. The Test of Time Award is “Auto-Encoding Variational Bayes”, and the Runner Up is “Intriguing properties of neural networks” 

13. ***IBM introduces the Granite series of decoder-only code models for software development tasks, leveraging Large Language Models (LLMs) trained on code. The Granite Code models offer a wide range of capabilities, including code generation, bug fixing, and documentation, catering to enterprise software development workflows. Released under an open-source license, these models demonstrate state-of-the-art performance across various coding tasks, facilitating improved productivity for developers.*** <br><br>
    7 May, IBM published a [paper](https://arxiv.org/pdf/2405.04324v1) “Granite Code Models: A Family of Open Foundation Models for Code Intelligence”. The paper indicates that Large Language Models (LLMs) trained on code are revolutionizing the software development process. Increasingly, code LLMs are being integrated into software development environments to improve the productivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously. Realizing the full potential of code LLMs requires a wide range of capabilities, including code generation, fixing bugs, explaining and documenting code, maintaining repositories, and more. This work introduces the Granite series of decoder-only code models for code generative tasks, trained with code written in 116 programming languages. The Granite Code models family consists of models ranging in size from 3 to 34 billion parameters, suitable for applications ranging from complex application modernization tasks to on-device memory-constrained use cases. Evaluation on a comprehensive set of tasks demonstrates that Granite Code models consistently reaches state-of-the-art performance among available open-source code LLMs. The Granite Code model family was optimized for enterprise software development workflows and performs well across a range of coding tasks (e.g. code generation, fixing and explanation), making it a versatile all around code model. The Granite Code [models are released](https://github.com/ibm-granite/granite-code-models) under an Apache 2.0 license for both research and commercial use.

15. ***Microsoft proposes vAttention, a dynamic memory management approach for serving Large Language Models (LLMs) without PagedAttention. Unlike prior systems that reserve memory for key-value caches ahead of time, vAttention enables on-demand physical memory allocation for key-value caches, reducing fragmentation and improving throughput for LLM inference. The approach leverages existing system support for demand paging, resulting in faster token generation and processing speeds compared to previous methods.*** <br><br>
    7 May, Microsoft published a [paper](https://arxiv.org/pdf/2405.04437) “vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention”. The authors argue that Efficient use of GPU memory is essential for high throughput LLM inference. Prior systems reserved memory for the KV-cache ahead-of-time, resulting in wasted capacity due to internal fragmentation. Inspired by OS-based virtual memory systems, vLLM proposed PagedAttention to enable dynamic memory allocation for KV-cache. This approach eliminates fragmentation, enabling high-throughput LLM serving with larger batch sizes. However, to be able to allocate physical memory dynamically, PagedAttention changes the layout of KV-cache from contiguous virtual memory to non-contiguous virtual memory. This change requires attention kernels to be rewritten to support paging, and serving framework to implement a memory manager. Thus, the PagedAttention model leads to software complexity, portability issues, redundancy and inefficiency. This paper proposes vAttention for dynamic KV-cache memory management. In contrast to PagedAttention, vAttention retains KV-cache in contiguous virtual memory and leverages low-level system support for demand paging, that already exists, to enable on-demand physical memory allocation. Thus, vAttention unburdens the attention kernel developer from having to explicitly support paging and avoids re-implementation of memory management in the serving framework. The paper shows that vAttention enables seamless dynamic memory management for unchanged implementations of various attention kernels. vAttention also generates tokens up to 1.97x faster than vLLM, while processing input prompts up to 3.92x and 1.45x faster than the PagedAttention variants of FlashAttention and FlashInfer.

17. ***Researchers from CMU and the University of Waterloo present MAmmoTH2, a method for efficiently harvesting large-scale instruction data from the web to enhance reasoning abilities in Large Language Models (LLMs). By leveraging naturally existing instruction data and open-source LLMs, MAmmoTH2 significantly boosts performance on reasoning benchmarks without costly human annotation or model distillation. The approach provides a new paradigm for building better instruction tuning data for LLMs.*** <br><br>
    6 May, CMU and Uni of Waterloo published a [paper](https://arxiv.org/pdf/2405.03548) “MAmmoTH2: Scaling Instructions from the Web”. The authors state that instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. The paper proposes a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. The proposed approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, the authors build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 34% on MATH and from 36% to 67% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. The work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data.

19. ***Google and the University of Massachusetts Amherst propose an adaptive retrieval and scalable indexing method for k-nearest neighbors (k-NN) search using Cross-Encoders. By efficiently computing latent query and item embeddings to approximate Cross-Encoder similarity scores, the method achieves improved recall and speed compared to existing approaches. The indexing approach enables fast and accurate k-NN search with reduced computational overhead, benefiting various information retrieval tasks.*** <br><br>
    6 May, Google and Uni Massachusetts Amherst published a [paper](https://arxiv.org/pdf/2405.03651) “Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders”. The authors indicate that Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than embedding-based models (dual-encoders) at estimating query-item relevance. Existing approaches perform k-NN search with CE by approximating the CE similarity with a vector embedding space fit either with dual-encoders (DE) or CUR matrix factorization. DE-based retrieve-and-rerank approaches suffer from poor recall on new domains and the retrieval with DE is decoupled from the CE. While CUR-based approaches can be more accurate than the DE-based approach, they require a prohibitively large number of CE calls to compute item embeddings, thus making it impractical for deployment at scale. This paper addresses these shortcomings with a proposed sparse-matrix factorization based method that efficiently computes latent query and item embeddings to approximate CE scores and performs k-NN search with the approximate CE similarity. The authors compute item embeddings offline by factorizing a sparse matrix containing query-item CE scores for a set of train queries. The method produces a high-quality approximation while requiring only a fraction of CE calls as compared to CUR-based methods, and allows for leveraging DE to initialize the embedding space while avoiding compute- and resource-intensive finetuning of DE via distillation. At test time, the item embeddings remain fixed and retrieval occurs over rounds, alternating between a) estimating the test query embedding by minimizing error in approximating CE scores of items retrieved thus far, and b) using the updated test query embedding for retrieving more items. The proposed k-NN search method improves recall by up to 5% (k=1) and 54% (k=100) over DE-based approaches. Additionally, the indexing approach achieves a speedup of up to 100x over CUR-based and 5x over DE distillation methods, while matching or improving k-NN search recall over baselines.

21. ***Google and the University of Washington introduce ImageInWords (IIW), a framework for curating hyper-detailed image descriptions and a corresponding dataset. IIW addresses the challenge of generating accurate image descriptions by involving human annotators in the annotation process. Models trained on IIW data demonstrate superior performance in generating image descriptions and vision-language reasoning tasks compared to prior approaches, underscoring the dataset's quality and utility for training Vision-Language models.*** <br><br>
    5 May, Google and Uni Washington published a [paper](https://arxiv.org/pdf/2405.02793) “ImageInWords: Unlocking Hyper-Detailed Image Descriptions”. The authors argue that despite the longstanding adage "an image is worth a thousand words," creating accurate and hyper-detailed image descriptions for training Vision-Language models remains challenging. Current datasets typically have web-scraped descriptions that are short, low-granularity, and often contain details unrelated to the visual content. As a result, models trained on such data generate descriptions replete with missing information, visual inconsistencies, and hallucinations. To address these issues, the paper introduces ImageInWords (IIW), a carefully designed human-in-the-loop annotation framework for curating hyper-detailed image descriptions and a new dataset resulting from this process. The study validates the framework through evaluations focused on the quality of the dataset and its utility for fine-tuning with considerations for readability, comprehensiveness, specificity, hallucinations, and human-likeness. The dataset significantly improves across these dimensions compared to recently released datasets (+66%) and GPT-4V outputs (+48%). Furthermore, models fine-tuned with IIW data excel by +31% against prior work along the same human evaluation dimensions. Given the fine-tuned models, the study also evaluates text-to-image generation and vision-language reasoning. The proposed model's descriptions can generate images closest to the original, as judged by both automated and human metrics. The researchers also find the model produces more compositionally rich descriptions, outperforming the best baseline by up to 6% on ARO, SVO-Probes, and Winoground datasets. [Link to the dataset](https://huggingface.co/datasets/google/imageinwords).

23. ***Huggingface and Sorbonne University conduct extensive experiments to identify critical design decisions in building vision-language models (VLMs). The study highlights the importance of architecture choice, data, and training methods in achieving state-of-the-art performance in VLMs. The development of Idefics2, an efficient VLM with 8 billion parameters, exemplifies these findings, achieving competitive performance while being computationally efficient.*** <br><br>
    3 May, Huggingface and Sorbonne Uni. published a [paper](https://arxiv.org/pdf/2405.02246) “What matters when building vision-language models?”. The authors find that the growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, the paper observes that critical decisions regarding the design of VLMs are often not justified. The researchers argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, the study conducts extensive experiments around pre-trained models, architecture choice, data, and training methods. The consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. Finds include 1) The cross-attention architecture performs better than the fully autoregressive one when unimodal pre-trained backbones are kept frozen. However, when training the unimodal backbones, the fully autoregressive architecture outperforms the cross-attention one, even though the latter has more parameters. 2) Reducing the number of visual tokens with learned pooling significantly improves compute efficiency at training and inference while improving performance on downstream tasks. 3) Splitting images into sub-images during training allow trading compute efficiency for more performance during inference. The authors [release the model](https://huggingface.co/collections/HuggingFaceM4/idefics2-661d1971b7c50831dd3ce0fe) (base, instructed, and chat) along with the datasets created for its training.

25. ***Apple proposes superposition prompting as a novel methodology to improve retrieval-augmented generation (RAG) in large language models (LLMs). The method allows LLMs to process input documents in parallel prompt paths, enhancing time efficiency and accuracy in question-answering benchmarks. Superposition prompting significantly reduces compute time while improving accuracy, addressing key drawbacks of LLMs in processing long contexts and handling irrelevant information.*** <br><br>
    10 Apr, Apple published a [paper](https://arxiv.org/pdf/2404.06910) “Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation”. The paper argues that despite the successes of large language models (LLMs), they exhibit significant drawbacks, particularly when processing long contexts. Their inference cost scales quadratically with respect to sequence length, making it expensive for deployment in some real-world text processing applications, such as retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the "distraction phenomenon," where irrelevant context in the prompt degrades output quality. To address these drawbacks, the research proposes a novel RAG prompting methodology, superposition prompting, which can be directly applied to pre-trained transformer-based LLMs without the need for fine-tuning. At a high level, superposition prompting allows the LLM to process input documents in parallel prompt paths, discarding paths once they are deemed irrelevant. The paper demonstrates the capability of the proposed method to simultaneously enhance time efficiency across a variety of question-answering benchmarks using multiple pre-trained LLMs. Furthermore, the technique significantly improves accuracy when the retrieved context is large relative to the context the model was trained on. For example, the approach facilitates an 93x reduction in compute time while improving accuracy by 43\% on the NaturalQuestions-Open dataset with the MPT-7B instruction-tuned model over naive RAG.

27. ***Enkrypt AI investigates the impact of fine-tuning and quantization on vulnerabilities in Large Language Models (LLMs). The study reveals that fine-tuning and quantization decrease jailbreak resistance in LLMs, increasing their susceptibility to attacks. The findings underscore the importance of considering security implications when fine-tuning LLMs for specialized tasks, highlighting the need for external guardrails to mitigate vulnerabilities.*** <br><br>
    5 Apr, Enkrypt AI published a [paper](https://arxiv.org/pdf/2404.04392) “Increased LLM Vulnerabilities from Fine-tuning and Quantization”. The paper finds that Large Language Models (LLMs) have become very popular and have found use cases in many domains, such as chatbots, auto-task completion agents, and much more. However, LLMs are vulnerable to different types of attacks, such as jailbreaking, prompt injection attacks, and privacy leakage attacks. Foundational LLMs undergo adversarial and alignment training to learn not to generate malicious and toxic content. For specialized use cases, these foundational LLMs are subjected to fine-tuning or quantization for better performance and efficiency. The authors examine the impact of downstream tasks such as fine-tuning and quantization on LLM vulnerability. The study tests foundation models like Mistral, Llama, MosaicML, and their fine-tuned versions. The research shows that fine-tuning and quantization reduces jailbreak resistance significantly, leading to increased LLM vulnerabilities. Finally, the authors demonstrate the utility of external guardrails in reducing LLM vulnerabilities.
 <br><br><br>

***5 May 2024***

1. ***The KANs paper introduces Kolmogorov-Arnold Networks (KANs) as alternatives to Multi-Layer Perceptrons (MLPs), utilizing learnable activation functions on edges instead of fixed ones on nodes. KANs outperform MLPs in accuracy and interpretability, offering promising opportunities for improving deep learning models.*** <br><br>
   2 May, MIT, California Inst of Tech, inter alia published a [paper](https://arxiv.org/pdf/2404.19756) “KAN: Kolmogorov-Arnold Networks”. Inspired by the Kolmogorov-Arnold representation theorem, the authors propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons"), KANs have learnable activation functions on edges ("weights"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. The study shows that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs. Codes are available at [this https URL] (https://github.com/KindXiaoming/pykan). Here is some [discussions on Kans](https://www.reddit.com/r/MachineLearning/comments/1clcu5i/d_kolmogorovarnold_network_is_just_an_mlp/).

3. ***NeMo-Aligner, a toolkit for model alignment, is introduced to efficiently align Large Language Models (LLMs) with human values and preferences. It supports various alignment paradigms and is designed for scalability and extensibility, aiming to make LLMs safer and more helpful.*** <br><br>
   2 May, Nvidia published a [paper](https://arxiv.org/pdf/2405.01481) “NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment”. The authors argue that Aligning Large Language Models (LLMs) with human values and preferences is essential for making them helpful and safe. However, building efficient tools to perform alignment can be challenging, especially for the largest and most competent LLMs which often contain tens or hundreds of billions of parameters. The study creates NeMo-Aligner, a toolkit for model alignment that can efficiently scale to using hundreds of GPUs for training. NeMo-Aligner comes with highly optimized and scalable implementations for major paradigms of model alignment such as: Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally, the toolkit supports running most of the alignment techniques in a Parameter Efficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed for extensibility, allowing support for other alignment techniques with minimal effort. It is open-sourced with Apache 2.0 License and Nvidia invites community contributions at https://github.com/NVIDIA/NeMo-Aligner

5. ***Oracle Database 23ai, formerly 23c, introduces AI Vector Search, enabling the generation and storage of multi-dimensional representations of various data types. This allows users to perform similarity searches combined with SQL queries, enhancing data analysis capabilities.*** <br><br>
   2 May, according to a [blog released by Oracle](https://blogs.oracle.com/database/post/oracle-23ai-now-generally-available), Oracle renamed its new database from  Oracle Database 23c to Oracle Database 23ai. The new version introduces AI Vector Search, a powerful new technology that enables one to leverage a new generation of AI models to generate and store vectors.  These vectors, sometimes referred to as embeddings, are multi-dimensional representations of documents, images, videos, sound, etc.  By encoding these objects as vectors, a user gains the ability to look for similarities between them using mathematical calculations.  The real power of Oracle Database23ai's solution is that one can combine these similarity searches with searches on business data using simple SQL.. (Comments: last year, traditional DB products such as PostgreSQL, No-SQL databases such as Cassandra, and traditional Information Retrieval products such as ElasticSearch, Solr, etc already have integrated vector search in their products. There are also specific vector database products such as Milvus, Chroma Qdrant, etc that provide search functions for both structured and unstructured data).

7. ***Maximum diffusion reinforcement learning addresses correlations in experiences of embodied agents, overcoming challenges in traditional machine learning techniques. It enables single-shot learning and outperforms existing methods, facilitating transparent and reliable decision-making.*** <br><br>
   2 May, [Nature Machine Intelligence](https://www.nature.com/articles/s42256-024-00829-3) published a paper “Maximum diffusion reinforcement learning”. The paper finds that robots and animals both experience the world through their bodies and senses. Their embodiment constrains their experiences, ensuring that they unfold continuously in space and time. As a result, the experiences of embodied agents are intrinsically correlated. Correlations create fundamental challenges for machine learning, as most techniques rely on the assumption that data are independent and identically distributed. In reinforcement learning, where data are directly collected from an agent’s sequential experiences, violations of this assumption are often unavoidable. This study derives a method that overcomes this issue by exploiting the statistical mechanics of ergodic processes, which is termed as maximum diffusion reinforcement learning. By decorrelating agent experiences, the approach provably enables single-shot learning in continuous deployments over the course of individual task attempts. Moreover, the researchers prove the approach generalizes well-known maximum entropy techniques and robustly exceeds state-of-the-art performance across popular benchmarks. The results at the nexus of physics, learning and control form a foundation for transparent and reliable decision-making in embodied reinforcement learning agents. 

9. ***Tech engineering faces challenges with layoffs and AI's impact on job roles, as highlighted by tech influencer Deedy Das. Startups prefer experienced hires over fresh graduates, and AI threatens traditional roles, leading to unemployment among Computer Science graduates.*** <br><br>
    2 May, according to [The Economic Times](https://economictimes.indiatimes.com/jobs/fresher/influencer-predicts-tough-times-ahead-for-techies-says-software-engineering-no-longer-a-guarantee-of-success-and-wealth/articleshow/109778330.cms?from=mdr), Tech engineering, once a booming sector, is facing challenges as big tech companies, such as Microsoft and Google, have undertaken mass layoffs in recent years. Deedy Das, a prominent tech influencer on X (formerly Twitter), has raised alarms about an impending "winter" for tech engineering. Through a series of Twitter threads, Deedy has highlighted stark differences between the current landscape and the industry's state two decades ago. Contrary to earlier notions, where pursuing Computer Science (CS) seemed a sure path to employment in the burgeoning field of Edtech, Deedy suggests a grim reality. Despite significant layoffs, companies exhibit reluctance towards new hiring initiatives. Startups, in particular, are leaning towards experienced hires, shunning fresh graduates. Moreover, the rise of artificial intelligence (AI) poses a looming threat to traditional job roles. Deedy underscores how AI algorithms are progressively replacing tasks previously performed by humans. With AI's growing prominence, the demand for human-centric software engineering roles may dwindle further. While acknowledging the cyclical nature of technological trends, Deedy remains skeptical about a swift resurgence in the tech job market. He observes a reluctance among startups to invest in training new graduates, citing the high costs involved. Consequently, a significant percentage of Computer Science graduates find themselves unemployed, highlighting a mismatch between academic training and industry demands.

11. ***Self-Play Preference Optimization proposes a method for language model alignment, treating it as a game to identify Nash equilibrium policy. It achieves state-of-the-art performance without external supervision, showcasing its effectiveness in language model alignment.*** <br><br>
    1 May, UCLA and CMU published a [paper](https://arxiv.org/pdf/2405.00675) “Self-Play Preference Optimization for Language Model Alignment”. The paper finds that traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. This study proposes a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. The proposed approach, dubbed Self-Play Preference Optimization (SPPO), approximates the Nash equilibrium through iterative policy updates and enjoys theoretical convergence guarantee. The method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO). In the experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and the Open LLM Leaderboard. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models.

13. ***AI Chatbots are increasingly involved in scientific publishing, with some papers showing signs of AI involvement. While detection methods are limited, concerns arise regarding the potential impact of AI-generated content on scientific integrity.*** <br><br>
    1 May, Scientific American published a [paper](https://www.scientificamerican.com/article/chatbots-have-thoroughly-infiltrated-scientific-publishing/) “AI Chatbots Have Thoroughly Infiltrated Scientific Publishing”. The paper finds that one percent of scientific articles published in 2023 showed signs of generative AI’s potential involvement, according to a recent analysis. Some of these tells—such as the inadvertent inclusion of “certainly, here is a possible introduction for your topic” in a recent paper in Surfaces and Interfaces, a journal published by Elsevier—are reasonably obvious evidence that a scientist used an AI chatbot known as a large language model (LLM). But “that’s probably only the tip of the iceberg,” says scientific integrity consultant Elisabeth Bik. In most other cases AI involvement isn’t as clear-cut, and automated AI text detectors are unreliable tools for analyzing a paper. Researchers from several fields have, however, identified a few key words and phrases (such as “complex and multifaceted”) that tend to appear more often in AI-generated sentences than in typical human writing. “When you’ve looked at this stuff long enough, you get a feel for the style,” says Andrew Gray, a librarian and researcher at University College London. The paper also finds that AI certainly has been used to produce scientific diagrams and illustrations that have often been included in academic papers—including, notably, one bizarrely endowed rodent—and even to replace human participants in experiments. The paper ends by saying that if AI-generated judgments creep into academic papers alongside AI text, that concerns experts, including Matt Hodgkinson, a council member of the Committee on Publication Ethics, a U.K.-based nonprofit organization that promotes ethical academic research practices. Chatbots are “not good at doing analysis,” he says, “and that’s where the real danger lies.”

15. ***In-Context Learning explores the behavior of learning with long-context models, showing performance gains with larger datasets. It identifies properties of in-context learning and long-context models, providing insights into their effectiveness.*** <br><br>
    30 Apr, CMU and Tel Aviv Uni published a [paper](https://arxiv.org/pdf/2405.00200) “In-Context Learning with Long-Context Models: An In-Depth Exploration”. The paper finds that as model context lengths continue to increase, the number of demonstrations that can be provided in-context approaches the size of entire training datasets. The paper studies the behavior of in-context learning (ICL) at this extreme scale on multiple datasets and models. The authors show that, for many datasets with large label spaces, performance continues to increase with hundreds or thousands of demonstrations. The paper contrasts this with example retrieval and finetuning: example retrieval shows excellent performance at low context lengths but has diminished gains with more demonstrations; finetuning is more data hungry than ICL but can sometimes exceed long-context ICL performance with additional data. The researchers use this ICL setting as a testbed to study several properties of both in-context learning and long-context models. The study shows that long-context ICL is less sensitive to random input shuffling than short-context ICL, that grouping of same-label examples can negatively impact performance, and that the performance boosts the authors see do not arise from cumulative gain from encoding many examples together. The research concludes that although long-context ICL can be surprisingly effective, most of this gain comes from attending back to similar examples rather than task learning.

17. ***Multi-token prediction improves large language model training efficiency and downstream capabilities without additional training time overhead. It enhances performance on coding tasks and algorithmic reasoning, offering faster inference times.*** <br><br>
    30 Apr, Meta published a [paper](https://arxiv.org/pdf/2404.19737) “Better & Faster Large Language Models via Multi-token Prediction”. The paper finds that large language models such as GPT and Llama are trained with a next-token prediction loss. The study suggests that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, the authors ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, the paper measures improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where the models consistently outperform strong baselines by several percentage points. The 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.

19. ***AdvPrompter introduces a method for generating adversarial prompts to assess language model vulnerabilities. It efficiently generates human-readable adversarial prompts, improving model robustness against jailbreaking attacks.*** <br><br>
    29 Apr, Meta and Max-Planck-Institute publish a [paper](https://arxiv.org/pdf/2404.16873) “AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs”. The authors indicate that while recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires finding adversarial prompts that cause such jailbreaking, e.g. by appending a suffix to a given instruction, which is inefficient and time-consuming. On the other hand, automatic adversarial prompt generation often leads to semantically meaningless attacks that can easily be detected by perplexity-based filters, may require gradient information from the TargetLLM, or do not scale well due to time-consuming discrete optimization processes over the token space. This study presents a novel method that uses another LLM, called the AdvPrompter, to generate human-readable adversarial prompts in seconds, sim800times faster than existing optimization-based approaches. The authors train the AdvPrompter using a novel algorithm that does not require access to the gradients of the TargetLLM. This process alternates between two steps: (1) generating high-quality target adversarial suffixes by optimizing the AdvPrompter predictions, and (2) low-rank fine-tuning of the AdvPrompter with the generated adversarial suffixes. The trained AdvPrompter generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show state-of-the-art results on the AdvBench dataset, that also transfer to closed-source black-box LLM APIs. Further, the researchers demonstrate that by fine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores.

21. ***LoRA Land presents fine-tuned large language models using Low Rank Adaptation (LoRA), showing improved performance over base models and even GPT-4. It also introduces LoRAX, a multi-LoRA inference server for efficient model deployment.*** <br><br>
    29 Apr, a start-up named Predibase published a [paper](https://arxiv.org/pdf/2405.00732) “LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report”. The authors argue that Low Rank Adaptation (LoRA) has emerged as one of the most widely adopted methods for Parameter Efficient Fine-Tuning (PEFT) of Large Language Models (LLMs). LoRA reduces the number of trainable parameters and memory usage while achieving comparable performance to full fine-tuning. The study aims to assess the viability of training and serving LLMs fine-tuned with LoRA in real-world applications. First, the authors measure the quality of LLMs fine-tuned with quantized low rank adapters across 10 base models and 31 tasks for a total of 310 models. The paper finds that 4-bit LoRA fine-tuned models outperform base models by 34 points and GPT-4 by 10 points on average. Second, the authors investigate the most effective base models for fine-tuning and assess the correlative and predictive capacities of task complexity heuristics in forecasting the outcomes of fine-tuning. Finally, the research evaluates the latency and concurrency capabilities of LoRAX, an open-source Multi-LoRA inference server that facilitates the deployment of multiple LoRA fine-tuned models on a single GPU using shared base model weights and dynamic adapter loading. LoRAX powers LoRA Land, a web application that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA A100 GPU with 80GB memory. LoRA Land highlights the quality and cost-effectiveness of employing multiple specialized LLMs over a single, general-purpose LLM.

23. ***PCAST's report outlines recommendations for responsible AI use in scientific discovery, emphasizing equity in resource sharing, secure data access, collaboration, and transparent AI use to mitigate risks in research.*** <br><br>
    29 Apr, the President’s Council of Advisors on Science and Technology [(PCAST)](https://www.whitehouse.gov/pcast/briefing-room/2024/04/29/pcast-releases-report-on-supercharging-research-harnessing-artificial-intelligence-to-meet-global-challenges/) released a [report](https://www.whitehouse.gov/wp-content/uploads/2024/04/AI-Report_Letter-ExSumm-29APRIL2024_SEND.pdf) that recommends new actions to help USA responsibly harness the power of Artificial Intelligence (AI) to accelerate scientific discovery. This report comes in response to President Biden’s charge in his Executive Order on the Safe, Secure, Trustworthy Development and Use of Artificial Intelligence. The report outlines a forward-looking approach to advance the safe and effective use of AI to empower human researchers, explore scientific possibilities, and to mitigate risks in an increasingly digital world. Specifically, PCAST’s recommendations include:  1) Expanding existing efforts to broadly and equitably share basic AI resources to benefit the full diversity of academic researchers, national laboratories, and smaller companies and non-profit organizations. 2) Expanding secure and responsible access of anonymized federal data sets for critical research needs with appropriate protections and safeguards. 3) Supporting both basic and applied research in AI that involves collaborations across academia, industry, national laboratories, and federal agencies as outlined in the vision for the NAIRR developed by the NAIRR Task Force. 4) Adopting principles of responsible, transparent, and trustworthy AI use throughout all stages of the scientific research process to manage the risks of inaccurate, biased, harmful, or non-replicable findings from scientific uses of AI. 5) Encouraging innovative approaches to integrating AI assistance into scientific workflows to facilitate human researchers creatively conducting high quality science that utilizes AI assistance responsibly.

25. ***"We Need Structured Output" discusses the importance of constraining large language model outputs to specific formats or standards for integration into developer workflows. User-centered constraints can enhance user experience and streamline development processes.*** <br><br>
    10 Apr, Google published a [paper](https://arxiv.org/pdf/2404.07362v1) “"We Need Structured Output": Towards User-centered Constraints on Large Language Model Output”. The paper finds that Large language models can produce creative and diverse responses. However, to integrate them into current developer workflows, it is essential to constrain their outputs to follow specific formats or standards. In this work, the authors surveyed 51 experienced industry professionals to understand the range of scenarios and motivations driving the need for output constraints from a user-centered perspective. The authors identified 134 concrete use cases for constraints at two levels: low-level, which ensures the output adhere to a structured format and an appropriate length, and high-level, which requires the output to follow semantic and stylistic guidelines without hallucination. Critically, applying output constraints could not only streamline the currently repetitive process of developing, testing, and integrating LLM prompts for developers, but also enhance the user experience of LLM-powered features and applications. The researchers conclude with a discussion on user preferences and needs towards articulating intended constraints for LLMs, alongside an initial design for a constraint prototyping tool.
<br><br><br>

