# Weekly AI-News - Jan 2024
Welcome to the Weekly-AI-news!
I mainly focuses more on Generative AI, LLMs and the related, so may miss import AI news in other areas. <br><br>

**28 Jan 2024**

1.	28 Jan, according to [this post](https://twitter.com/owencm/status/1751409104713826666), OpenAI’s new embedding strategy is based on an MRL technique, which is stemmed from the [paper](https://openreview.net/pdf?id=9njZa1fm35) “Matryoshka Representation Learning”. Owen Campbell-Moore from OpenAI posted “I was responsible for the blog post and it’s my bad not thinking / remembering to cite. We’re updating the blog post to add a citation now!”, but this happened after [others said](https://twitter.com/jainprateek_/status/1751291366515384354) “Closed science companies like OpenAI and Anthropic parasitically extract value from open science and open source without giving credit to people or organizations building them. Open science with citations would’ve addressed that, but alas that’s too much to ask.”

2.	25 Jan, He, K. from Meta and NYU published a [paper](https://arxiv.org/pdf/2401.14404.pdf) “Deconstructing Denoising Diffusion Models for Self-Supervised Learning”. In this study, the researchers examine the representation learning abilities of Denoising Diffusion Models (DDM) that were originally purposed for image generation. The auther’s philosophy is to deconstruct a DDM, gradually transforming it into a classical Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore how various components of modern DDMs influence self-supervised representation learning. The researchers observe that only a very few modern components are critical for learning good representations, while many others are nonessential. The study ultimately arrives at an approach that is highly simplified and to a large extent resembles a classical DAE. The authors hope the study will rekindle interest in a family of classical methods within the realm of modern self-supervised learning.

3.	25 Jan, University of Edinburgh and NTU Singapore published a [paper](https://arxiv.org/pdf/2401.14351.pdf) “ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models”. This paper presents ServerlessLLM, a locality-enhanced serverless inference system for Large Language Models (LLMs). ServerlessLLM exploits the substantial capacity and bandwidth of storage and memory devices available on GPU servers, thereby reducing costly remote checkpoint downloads and achieving efficient checkpoint loading. ServerlessLLM achieves this through three main contributions: (i) fast LLM checkpoint loading via a novel loading-optimized checkpoint format design, coupled with an efficient multi-tier checkpoint loading system; (ii) locality-driven LLM inference with live migration, which allows ServerlessLLM to effectively achieve locality-driven server allocation while preserving the low latency of ongoing LLM inference; and (iii) locality-aware server allocation, enabling ServerlessLLM to evaluate the status of each server in a cluster and effectively schedule model startup time to capitalize on local checkpoint placement. Comprehensive experiments, which include microbenchmarks and real-world traces, show that ServerlessLLM surpasses state-of-the-art systems by 10 - 200X in latency performance when running various LLM inference workloads.

4.	24 Jan, Cornell Uni published a [paper](https://arxiv.org/pdf/2401.13660.pdf) “MambaByte: Token-free Selective State Space Model”. The paper states that Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. The study experiments with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences. Experiments indicate the computational efficiency of MambaByte compared to other byte-level models. The research also finds MambaByte to be competitive with and even outperform state-of-the-art subword Transformers. Furthermore, owing to linear scaling in length, MambaByte benefits from fast inference compared to Transformers. These findings establish the viability of MambaByte in enabling token-free language modeling.
 
5.	23 Jan, Google published a [paper](https://arxiv.org/pdf/2401.12945.pdf) “Lumiere: A Space-Time Diffusion Model for Video Generation”.  The paper introduces Lumiere -- a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion -- a pivotal challenge in video synthesis. To this end, the researchers introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution -- an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, Lumiere learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. The researchers demonstrate state-of-the-art text-to-video generation results, and show that the design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation. Project link is [here](https://lumiere-video.github.io/).

6.	23 Jan, Stanford and OpenAI published a [paper](https://arxiv.org/pdf/2401.12954.pdf) “Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding”. The paper introduces meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to deconstruct complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct "expert" instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly enhancing its performance across a wide array of tasks. The zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction by obviating the need for detailed, task-specific instructions. Furthermore, the research demonstrates the seamless integration of external tools, such as a Python interpreter, into the meta-prompting framework, thereby broadening its applicability and utility. Through rigorous experimentation with GPT-4, the researchers establish the superiority of meta-prompting over conventional scaffolding methods: When averaged across all tasks, including the Game of 24, Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented with a Python interpreter functionality, surpasses standard prompting by 17.1%, expert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.

7.	23 Jan, Google DeepMind published a [paper](https://arxiv.org/pdf/2401.12963.pdf) “AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents”. The paper finds that foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. This research proposes AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. The study demonstrates AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. The researchers experimentally show that such "in-the-wild" data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences.

8.	23 Jan, Nvidia published a [paper](https://arxiv.org/pdf/2401.10225.pdf) “ChatQA: Building GPT-4 Level Conversational QA Models”. In this work, Nvidia introduces ChatQA, a family of conversational question answering (QA) models that obtain GPT-4 level accuracies. Specifically, the study proposes a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval-augmented generation in conversational QA, the researchers fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, the ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.

9.	23 Jan, Chatdoc.com published the research [paper](https://arxiv.org/pdf/2401.12599.pdf) “Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition” for the chatdco project. With the rapid development of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) has become a predominant method in the field of professional knowledge-based question answering. Presently, major foundation model companies have opened up Embedding and Chat API interfaces, and frameworks like LangChain have already integrated the RAG process. It appears that the key models and steps in RAG have been resolved, leading to the question: are professional knowledge QA systems now approaching perfection? This article discovers that current primary methods depend on the premise of accessing high-quality text corpora. However, since professional documents are mainly stored in PDFs, the low accuracy of PDF parsing significantly impacts the effectiveness of professional knowledge-based QA. The study conducted an empirical RAG experiment across hundreds of questions from the corresponding real-world professional documents. The results show that, ChatDOC, a RAG system equipped with a panoptic and pinpoint PDF parser, retrieves more accurate and complete segments, and thus better answers. Empirical experiments show that ChatDOC is superior to baseline on nearly 47% of questions, ties for 38% of cases, and falls short on only 15% of cases. It shows that it possible to revolutionize RAG with enhanced PDF structure recognition.

10.	22 Jan, MIT FutureTech published a working [paper](https://futuretech-site.s3.us-east-2.amazonaws.com/2024-01-18+Beyond_AI_Exposure.pdf) “Beyond AI Exposure: Which Tasks are Cost-Effective to Automate with Computer Vision?”. The faster AI automation spreads through the economy, the more profound its potential impacts, both positive (improved productivity) and negative (worker displacement). The previous literature on “AI Exposure” cannot predict this pace of automation since it attempts to measure an overall potential for AI to affect an area, not the technical feasibility and economic attractiveness of building such systems. This article presents a new type of AI task automation model that is end-to-end, estimating: the level of technical performance needed to do a task, the characteristics of an AI system capable of that performance, and the economic choice of whether to build and deploy such a system. The result is a first estimate of which tasks are technically feasible and economically attractive to automate - and which are not. The research focuses on computer vision, where cost modeling is more developed. The study finds that at today’s costs U.S. businesses would choose not to automate most vision tasks that have “AI Exposure,” and that only 23% of worker wages being paid for vision tasks would be attractive to automate. This slower roll-out of AI can be accelerated if costs falls rapidly or if it is deployed via AI-as-a-service platforms that have greater scale than individual firms, both of which we quantify. Overall, the findings suggest that AI job displacement will be substantial, but also gradual – and therefore there is room for policy and retraining to mitigate unemployment impacts.

11.	22 Jan, National Uni of Singapore published a [paper](https://arxiv.org/pdf/2401.11817.pdf) “Hallucination is Inevitable: An Innate Limitation of Large Language Models”. The paper indicates that hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. This study formalizes the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, the researchers define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, the research shows that LLMs cannot learn all of the computable functions and will therefore always hallucinate. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, the researchers describe the hallucination-prone tasks and empirically validate their claims. Finally, using the formal world framework, the paper discusses the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.

12.	19 Jan, Princeton Uni and other published a [paper](https://arxiv.org/pdf/2401.10774.pdf) “MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads”. The article states that the inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. The paper presents MEDUSA, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, MEDUSA constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, MEDUSA introduces only minimal overhead in terms of single-step latency while substantially reducing the number of decoding steps required. The researchers present two levels of fine-tuning procedures for MEDUSA to meet the needs of different use cases: • MEDUSA-1: M EDUSA is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. • MEDUSA-2: M EDUSA is fine-tuned together with the backbone LLM, enabling better prediction accuracy of MEDUSA heads and higher speedup but needing a special training recipe that preserves the backbone model’s capabilities. Moreover, the study proposes several extensions that improve or expand the utility of MEDUSA, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. The researchers evaluate MEDUSA on models of various sizes and training procedures. The experiments demonstrate that M EDUSA-1 can achieve over 2.2× speedup without compromising generation quality, while MEDUSA-2 further improves the speedup to 2.3-3.6×. The code for this implementation is available at https: //github.com/FasterDecoding/Medusa.

13.	19 Jan, University of Texas and Microsoft published a [paper](https://arxiv.org/pdf/2401.10464.pdf) “PhotoScout: Synthesis-Powered Multi-Modal Image Search”. The paper indicates that due to the availability of increasingly large amounts of visual data, there is a growing need for tools that can help users find relevant images. While existing tools can perform image retrieval based on similarity or metadata, they fall short in scenarios that necessitate semantic reasoning about the content of the image. This paper explores a new multi-modal image search approach that allows users to conveniently specify and perform semantic image search tasks. With the new tool, PhotoScout, the user interactively provides natural language descriptions, positive and negative examples, and object tags to specify their search tasks. Under the hood, PhotoScout is powered by a program synthesis engine that generates visual queries in a domain-specific language and executes the synthesized program to retrieve the desired images. In a study with 25 participants, the researchers observed that PhotoScout allows users to perform image retrieval tasks more accurately and with less manual effort.

14.	16 Jan, Ritsumeikan University published a [paper](https://arxiv.org/pdf/2401.08273.pdf) “Large Language Models are Null-Shot Learners”. The paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the "Examples" section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, the study proposes that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with six LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets. The researchers also perform ablation studies, including experimenting with a modified version of null-shot prompting that incorporates ideas from zero-shot chain-of-thought prompting, which shows different trends of results.



**21 Jan 2024**

1.	19 Jan, [ICLR 2024](https://openreview.net/group?id=ICLR.cc/2024/Conference#tab-accept-oral) released paper acceptation notifications. There are total 7304 submissions, acceptance rate is 30.81%, with Poster 24.63%, Spotlight 5.01% and oral 1.16%. The conference will in Vienna Austria, 7th- 11th May.

2.	18 Jan, Meta published a [paper](https://arxiv.org/pdf/2401.10020.pdf) “Self-Rewarding Language Models”. The researchers posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. This work studies Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. The researchers show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of the proposed approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continually improve in both axes.

3.	18 Jan, UCAS published a [paper](https://arxiv.org/pdf/2401.10166.pdf) “VMamba: Visual State Space Model”. The researchers indicate that Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) stand as the two most popular foundation models for visual representation learning. While CNNs exhibit remarkable scalability with linear complexity w.r.t. image resolution, ViTs surpass them in fitting capabilities despite contending with quadratic complexity. A closer inspection reveals that ViTs achieve superior visual modeling performance through the incorporation of global receptive fields and dynamic weights. This observation motivates the researchers to propose a novel architecture that inherits these components while enhancing computational efficiency. To this end, the researchers draw inspiration from the recently introduced state space model and propose the Visual State Space Model (VMamba), which achieves linear complexity without sacrificing global receptive fields. To address the encountered direction-sensitive issue, the study introduces the Cross-Scan Module (CSM) to traverse the spatial domain and converts any non-causal visual image into order patch sequences. Extensive experimental results substantiate that VMamba not only demonstrates promising capabilities across various visual perception tasks, but also exhibits more pronounced advantages over established benchmarks as the image resolution increases. Source code has been available at this [https URL](https://github.com/MzeroMiko/VMamba).

4.	17 Jan, Nature published a [paper](https://www.nature.com/articles/s41586-023-06747-5.pdf) from Google “Solving olympiad geometry without human demonstrations”. The paper states that proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning, owing to their reputed difficulty among the world’s best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges, resulting in severe scarcity of training data. The researchers propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on Google’s large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004. 

5.	16 Jan, OpenAI [posted](https://twitter.com/OpenAI/status/1746986660892741927) on X on how they are preparing for 2024’s world elections. These include: • Working to prevent abuse, including misleading deepfakes • Providing transparency on AI-generated content • Improving access to authoritative voting information.

6.	15 Jan, according to [BBC News](https://www.bbc.com/news/business-67977967), AI to hit 40% of jobs and worsen inequality, IMF says. The International Monetary Fund (IMF) predicts that nearly 40% of all jobs will be affected by artificial intelligence (AI), with the technology likely worsening overall inequality. In advanced economies, AI is projected to impact around 60% of jobs, benefiting workers in half of these cases through enhanced productivity. However, AI may also replace human-executed tasks, potentially lowering demand for labor, impacting wages, and leading to job elimination. The IMF suggests that the technology will affect only 26% of jobs in low-income countries, emphasizing the need for infrastructure and skilled workforces to harness AI benefits. Managing Director Kristalina Georgieva calls for comprehensive social safety nets and retraining programs to ensure an inclusive AI transition and curb inequality. The IMF analysis coincides with discussions on AI regulation at the World Economic Forum in Davos.

7.	14 Jan, AWS published a [paper](https://www.amazon.science/publications/panda-performance-debugging-for-databases-using-llm-agents) “Panda: Performance Debugging for Databases using LLM Agents”. Debugging a performance issue in databases is notoriously hard. Wouldn’t it be convenient if there exists an oracle or a co-pilot for every database system which users can query in natural language (NL) — ‘what’s wrong?’, or even better— ‘How to fix it?’. Large Language Models (LLMs), like ChatGPT, seem to be a natural surrogate to this oracle given their ability to answer a wide range of questions by efficiently encoding vast amount of knowledge for e.g., a major chunk of the internet. However, prompting ChatGPT with database performance queries often results in ‘technically correct’ but highly ‘vague’ or ‘generic’ recommendations typically rendered useless and untrustworthy by experienced Database Engineers (DBEs).  This work proposes Panda, a framework to provide context grounding to pre-trained LLMs in order to generate more ‘useful’ and ‘in-context’ troubleshooting recommendations. Panda draws inspiration from the way experienced DBEs perform debugging, and puts a system in place with necessary components required to robustly deploy pre-trained LLMs in production for debugging. The 4 key components of Panda are: (1) Grounding; (2) Verification;(3) Affordance; and (4) Feedback. The researchers describe the necessity and usefulness of each component and how they communicate internally to transform a given pre-trained LLM into generating in-context, actionable, useful and accurate recommendation for debugging a given database system.

8.	13 Jan, according to [futurism](https://futurism.com/the-byte/microsoft-cherrypicked-ai-examples), IN LEAKED AUDIO, MICROSOFT CHERRY-PICKED EXAMPLES TO MAKE ITS AI SEEM FUNCTIONAL "IT WASN'T THAT EASY TO GET GOOD ANSWERS." The article says Microsoft's internal presentation on an early version of its Security Copilot, a ChatGPT-like AI tool for cybersecurity, revealed challenges with hallucinating incorrect responses. The leaked audio exposed that cherry-picked examples were used to showcase the tool's capabilities as it frequently provided different answers and struggled to offer accurate responses. Security Copilot, based on OpenAI's GPT-4, faced hallucination issues common in large language models, exacerbated by using the model without cybersecurity-specific training data. Microsoft clarified that the discussed technology was exploratory, preceding Security Copilot, and used simulations from public datasets, ensuring no customer data was involved.
   
=====================================

1)	11 Jan, Google published a [paper](https://arxiv.org/pdf/2401.05654.pdf) “Towards Conversational Diagnostic AI”. The paper indicates that at the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians' expertise is an outstanding grand challenge. This research introduces AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue. AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. The researchers designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. The study compared AMIE's performance to that of primary care physicians (PCPs) in a randomized, double-blind crossover study of text-based consultations with validated patient actors in the style of an Objective Structured Clinical Examination (OSCE). The study included 149 case scenarios from clinical providers in Canada, the UK, and India, 20 PCPs for comparison with AMIE, and evaluations by specialist physicians and patient actors. AMIE demonstrated greater diagnostic accuracy and superior performance on 28 of 32 axes according to specialist physicians and 24 of 26 axes according to patient actors. This research has several limitations and should be interpreted with appropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale LLM-patient interactions but is not representative of usual clinical practice. While further research is required before AMIE could be translated to real-world settings, the results represent a milestone towards conversational diagnostic AI.

2)	11 Jan, NYU and UC Berkeley published a [paper](https://arxiv.org/pdf/2401.06209.pdf) “Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs”. Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). This research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, the researchers explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. The researchers identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, the study constructs the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. The paper further evaluates various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, the researchers propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, the research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems. The project link is [here](https://tsb0601.github.io/mmvp_blog/).



**14 Jan 2024**

1.	12 Jan, according to [TheIntercept](https://theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/), OPENAI QUIETLY DELETES BAN ON USING CHATGPT FOR “MILITARY AND WARFARE”, The Pentagon has its eye on the leading AI company, which this week softened its ban on military use. The article indicates that OpenAI quietly removed language from its usage policy that explicitly prohibited the use of its technology for military purposes. The updated policy, which aimed to be clearer and more readable, retains an injunction not to use the service to harm oneself or others and gives "develop or use weapons" as an example. However, the specific ban on "military and warfare" use has been removed. OpenAI stated that any use of its technology, including by the military, to develop or use weapons, injure others, or engage in unauthorized activities violating the security of any service or system, is disallowed. Critics argue that the shift may indicate a silent weakening of OpenAI's stance against doing business with militaries, especially given the company's close partnership with Microsoft, a major defense contractor. The change comes as militaries worldwide explore the incorporation of machine learning techniques, with the Pentagon tentatively exploring the use of OpenAI's models for analysis and decision-making.

2.  12 Jan, Anthropic published a [paper](https://arxiv.org/pdf/2401.05566.pdf) "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training". The researchers believe humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive
strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, the paper constructs proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, the researchers train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. The researchers find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, the study finds that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. The results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a
false impression of safety.

3.	11 Jan, IST Austria published a [paper](https://arxiv.org/pdf/2401.04679.pdf) “RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation”. The research investigates parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). The paper presents a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains low-rank and highly-sparse components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, the study shows that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. The research provides system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memory- and computationally-efficient training.

4.	10 Jan, Google published a [paper](https://arxiv.org/pdf/2401.04858.pdf) “User Embedding Model for Personalized Language Prompting”. The paper states that Modeling long histories plays a pivotal role in enhancing recommendation systems, allowing to capture user's evolving preferences, resulting in more precise and personalized recommendations. This study tackles the challenges of modeling long user histories for preference understanding in natural language. Specifically, the research introduces a new User Embedding Module (UEM) that efficiently processes user history in free-form text by compressing and representing them as embeddings, to use them as soft prompts to a LM. Experiments demonstrate the superior capability of this approach in handling significantly longer histories compared to conventional text based prompting methods, yielding substantial improvements in predictive performance. The main contribution of this research is to demonstrate the ability to bias language models with user signals represented as embeddings.

5.	10 Jan, OpenAI launched [GPT Store](https://openai.com/blog/introducing-the-gpt-store) to help user find useful and popular custom versions of ChatGPT. The store features a diverse range of GPTs developed by partners and the community. Browse popular and trending GPTs on the community leaderboard, with categories like DALL·E, writing, research, programming, education, and lifestyle. Some of our first featured GPTs include: Personalized trail recommendations from AllTrails; Search and synthesize results from 200M academic papers with Consensus; Expand your coding skills with Khan Academy’s Code Tutor; Design presentations or social posts with Canva; Find your next read with Books; Learn math and science anytime, anywhere with the CK-12 Flexi AI tutor. It also encourage user publish their own GPT in the store simply by 1) Save one’s GPT for Everyone (Anyone with a link will not be shown in the store); 2) verify user Builder Profile (Settings → Builder profile → Enable your name or a verified website). In addition, Team and Enterprise customers can manage GPTs, a new ChatGPT Team plan for teams of all sizes. Team customers have access to a private section of the GPT Store which includes GPTs securely published to the team/enterprise workspace. The GPT Store will be available soon for ChatGPT Enterprise customers and will include enhanced admin controls like choosing how internal-only GPTs are shared and which external GPTs may be used inside business. Like all usage on ChatGPT Team and Enterprise, OpenAI do not use conversations with GPTs to improve our models.

6.	10 Jan, Thenewstack published an [article](https://thenewstack.io/7-guiding-principles-for-working-with-llms/) “7 Guiding Principles for Working with LLMs”. The article outlines seven guiding principles for effectively working with Large Language Models (LLMs), which have revolutionized programming. These principles are based on the author's experience in using LLMs, such as ChatGPT and Claude, for various tasks. The guiding principles are as follows: 1) Think Out Loud: Encourages verbalizing thoughts and discussions with LLMs to generate ideas, validate, refute, or clarify concepts. Applies to technical topics and extends to non-technical domains, fostering a habit of narrating work. 2) Never Trust, Always Verify: Advocates for verification of LLM-generated content, especially in technical domains, by either using tests or autonomous self-directed loops. Emphasizes the importance of human judgment and fact-checking when relying on LLMs for information. 3) Recruit a Team of Assistants: Suggests using multiple LLMs and coding assistants in different situations to leverage diverse insights and solutions. Highlights the benefits of comparing results and seeking a consensus from a team of LLMs. 4) Ask for Choral Explanations: Encourages seeking explanations from multiple LLMs on a given topic to gain diverse insights and understanding. Draws parallels with the concept of "Choral Explanations" on platforms like StackExchange and Quora. 5) Exploit Pattern Recognition: Acknowledges the complementary pattern recognition abilities of both humans and LLMs. Describes instances where LLMs excel in recognizing patterns in data, aiding in problem-solving and understanding. 6) Automate Transformations: Advocates using LLMs for pattern-based transformations in knowledge work, such as converting formats or summarizing data. Highlights the efficiency gains in automating mundane tasks and focusing on higher-order intellectual tasks. 7) Learn by Doing: Discusses the role of LLMs as effective teachers in facilitating on-demand learning during projects. Emphasizes the immediate and tangible nature of learning through task-oriented interactions with LLMs. 
The author concludes by noting that these principles serve as a guide in the evolving era of LLMs, recognizing that continued exploration and adaptation will be crucial as features and capabilities emerge in AI assistants.

7.	9 Jan, meta published a [paper](https://arxiv.org/pdf/2401.04577.pdf) “Masked Audio Generation using a Single Non-Autoregressive Transformer”. The paper introduces MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, the model predicts spans of masked tokens obtained from a masking scheduler, while during inference the model gradually constructs the output sequence using several decoding steps. To further enhance the quality of the generated audio, the researchers introduce a novel rescoring method in which, the researchers leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, the study explores a hybrid version of MAGNeT, in which the researchers fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. The researchers demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline). Through ablation studies and analysis, the research shed light on the importance of each of the components comprising MAGNeT, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality. Samples are available on the [demo page URL](https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT).
                                                                                                                                                                                                                            
8.	8 Jan, Princeton Uni. published a [paper](https://arxiv.org/pdf/2401.04151.pdf) “Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning”. The paper indicates that fine-tuning is the primary methodology for tailoring pre-trained large language models to specific tasks. As the model's scale and the diversity of tasks expand, parameter-efficient fine-tuning methods are of paramount importance. One of the most widely used family of methods is low-rank adaptation (LoRA) and its variants. LoRA encodes weight update as the product of two low-rank matrices. Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks. The study introduces Chain of LoRA (COLA), an iterative optimization framework inspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full parameter fine-tuning, without incurring additional computational costs or memory overheads. COLA employs a residual learning procedure where it merges learned LoRA modules into the pre-trained language model parameters and re-initilize optimization for new born LoRA modules. The paper provides theoretical convergence guarantees as well as empirical results to validate the effectiveness of the algorithm. Across various models (OPT and llama-2) and seven benchmarking tasks, the research demonstrates that COLA can consistently outperform LoRA without additional computational or memory costs.

9.	8 Jan, ABC published an [opinion article](https://www.abc.net.au/religion/university-research-funding-and-declining-public-trust/103293026) “Research funding has been politicised and universities are losing public trust — is this the year that will reverse those trends?”. The author reflects on the release of ATARs and the concerns of students, including their worries about gaining entry into tertiary courses and funding their studies in the context of the Jobs Ready Graduate package. The focus then shifts to the Australian Universities Accord, a significant review of higher education billed as a "once-in-a-generation opportunity." The Accord aims to reimagine Australian higher education and address challenges faced by universities, such as the long-term effects of COVID-19, budget constraints, and concerns about international student markets. The Accord's interim report highlighted five priority areas, with a final report containing 47 recommendations awaiting release. Parallelly, the Australian Research Council (ARC) underwent a review to address governance issues and political interference. The article emphasizes the need for increased investment in research, as universities face challenges related to funding gaps and public perceptions of their value. Public trust in universities has declined, partly influenced by the politicization of higher education, and the article underscores the importance of transparent communication about the funding systems and the societal contributions of research. The Universities Accord and ARC Review are seen as opportunities to address these issues and rebuild public confidence.

10.	8 Jan, Mistral published a [paper](https://arxiv.org/pdf/2401.04088.pdf) “Mixtral of Experts”. The paper introduces Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. The paper also provides a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.


**7 Jan 2024**
1.	4 Jan, Google published a [paper](https://arxiv.org/pdf/2401.02412.pdf) “LLM Augmented LLMs: Expanding Capabilities through Composition”. The paper points out that foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. This research studies the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. The researchers illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13\% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, there is a relative improvement of 40\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.

2.  4 Jan, Google and DeepMind published a [paper](https://arxiv.org/pdf/2401.02412.pdf) “LLM Augmented LLMs: Expanding Capabilities through Composition”. The paper states that foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. This research studies the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. The paper proposes CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. The researchers illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13\% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, there is a relative improvement of 40\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.

3. 4 Jan, researchers from Uni. of Hongkong published a [paper](https://arxiv.org/pdf/2401.02415.pdf) “LLaMA Pro: Progressive LLaMA with Block Expansion”. The research point out that humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, the study proposes a new post-pretraining method for LLMs with an expansion of Transformer blocks. The researchers tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. This research experiments on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. The findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.

4.  4 Jan, University of Cambridge and College London published a [paper](https://arxiv.org/pdf/2401.02994.pdf) “Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM”. In conversational AI research, there's a noticeable trend towards developing models with a larger number of parameters, exemplified by models like ChatGPT. While these expansive models tend to generate increasingly better chat responses, they demand significant computational resources and memory. This study explores a pertinent question: Can a combination of smaller models collaboratively achieve comparable or enhanced performance relative to a singular large model? The paper introduces an approach termed "blending", a straightforward yet effective method of integrating multiple chat AIs. The empirical evidence suggests that when specific smaller models are synergistically blended, they can potentially outperform or match the capabilities of much larger counterparts. For instance, integrating just three models of moderate size (6B/13B paramaeters) can rival or even surpass the performance metrics of a substantially larger model like ChatGPT (175B+ paramaters). This hypothesis is rigorously tested using A/B testing methodologies with a large user base on the Chai research platform over a span of thirty days. The findings underscore the potential of the "blending" strategy as a viable approach for enhancing chat AI efficacy without a corresponding surge in computational demands.

5.	4 Jan, MIT Technology Review published [an article](https://www.technologyreview.com/2024/01/04/1086046/whats-next-for-ai-in-2024/) “What’s next for AI in 2024”.  The article claims that last year's predictions on AI trends largely held true, and now the focus is on 2024. The anticipated trends include: 1) Customized Chatbots: Google and OpenAI are making AI more accessible to the non-tech user. In 2024, people might create personalized chatbots with ease, utilizing multimodal capabilities of models like GPT-4 and Gemini. This could lead to practical applications in various fields, such as real estate. 2) Generative AI's Second Wave in Video: The next frontier for generative AI is text-to-video. Tools like Runway's Gen-2 are producing high-quality video content, catching the attention of major studios like Paramount and Disney. The impact extends beyond entertainment, with applications in marketing and training, raising concerns about the changing landscape of filmmaking and actors' roles. 3) AI-Generated Election Disinformation: With the rise of AI, the risk of AI-generated disinformation during elections is growing. Examples from Argentina, Slovakia, and the US illustrate how politicians are leveraging AI to create misleading content. This trend poses a challenge to recognizing real information online and could have severe consequences in polarized political climates. 4) Robots That Multitask: Inspired by generative AI techniques, roboticists are developing more general-purpose robots capable of multitasking. Models like DeepMind's Robocat and RT-X aim to enable robots to perform a range of tasks without specialized training. Challenges include the scarcity of diverse data sources for robot training, but initiatives like data collection from volunteers and large datasets from companies are addressing this issue.
These trends reflect ongoing efforts to make AI more accessible, versatile, and impactful, but they also raise concerns related to reliability, ethical considerations, and potential misuse. The coming year is expected to be pivotal in addressing these challenges.

6.	3rd Jan, Codec Avatars, Meta and UCLA published a [paper](https://arxiv.org/pdf/2401.01885.pdf) “From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations”. The paper presents a framework for generating full-bodied photorealistic avatars that gesture according to the conversational dynamics of a dyadic interaction. Given speech audio, the proposed model outputs multiple possibilities of gestural motion for an individual, including face, body, and hands. The key behind the method is in combining the benefits of sample diversity from vector quantization with the high-frequency details obtained through diffusion to generate more dynamic, expressive motion. The researchers visualize the generated motion using highly photorealistic avatars that can express crucial nuances in gestures (e.g. sneers and smirks). To facilitate this line of research, the study introduces a first-of-its-kind multi-view conversational dataset that allows for photorealistic reconstruction. Experiments show the model generates appropriate and diverse gestures, outperforming both diffusion- and VQ-only methods. Furthermore, the perceptual evaluation highlights the importance of photorealism (vs. meshes) in accurately assessing subtle motion details in conversational gestures. Code and dataset available [online](https://github.com/facebookresearch/audio2photoreal/).

7.	3rd Jan, Forbes published an [article](https://www.forbes.com/sites/jodiecook/2024/01/03/6-strategies-for-better-results-from-chatgpt-according-to-openai/?sh=7ee27a6c66ad) “6 Strategies For Better Results From ChatGPT, According To OpenAI”. The six strategies are 1) Write clear instructions – make the prompt super simple and instruct as if you were explaining a task to a junior member of the team, asking the model to adopt a persona, using line breaks or extra formatting to clearly indicate distinct parts, specifying the steps required to complete a text, providing examples, and specifying the desired length of the output; 2) Provide reference text - instructing the model to answer using a reference text, making absolutely sure it uses the text’s content in the response; 3) Split complex tasks - ask something, then use a key part of the response to ask the next thing. Don’t lose context in complicated prompts, don’t leave ChatGPT to decide which parts of your request are important; 4) Give the model time - asking for a ‘chain of thought’ before an answer can help the model reason its way toward correct answers more reliably; 5) Use external tools - upload spreadsheets or documents and ask for information based on the stats and numbers, ChatGPT will call tools do the job more reliably and efficiently; 6)  Test changes systematically - in some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples, it may be necessary to define a “comprehensive test suite,” making incremental changes to ensure the results mean what you think they do.

8.	2nd Jan, UCLA published a [paper](https://arxiv.org/pdf/2401.01335.pdf) “Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models”. The researchers indicate that harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). The paper delves into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. The research proposes a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. The proposed method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, the researchers prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution. Empirically, the researchers evaluate the method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Experimental results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents.

9.	2nd Jan, Forbes published an [article](https://www.forbes.com/sites/janakirammsv/2024/01/02/exploring-the-future-5-cutting-edge-generative-ai-trends-in-2024/?sh=791d4ac1206e) “Exploring The Future: 5 Cutting-Edge Generative AI Trends In 2024”. The articles states that as we delve into 2024, generative AI is poised for significant evolution, bringing forth trends that promise to reshape technology. Key trends include the emergence of multimodal AI models, going beyond text to incorporate diverse data types like images, language, and audio. Small Language Models (SLMs) will gain prominence, trained on high-quality datasets and offering comparable content quality to larger counterparts with lower resource requirements. Autonomous agents, employing advanced algorithms and machine learning, will contribute to AI models producing content without extensive human intervention. Open generative AI models are anticipated to approach proprietary models in performance, narrowing the gap and providing viable alternatives for enterprises. Additionally, the cloud-native approach, particularly leveraging Kubernetes, is set to become crucial for hosting generative AI models, with frameworks and tools maturing to efficiently manage the entire lifecycle of foundation models. The year 2024 is expected to witness rapid evolution in generative AI, bringing forth novel capabilities for consumers and enterprises alike.

10.	1st Jan, TheNewStack published an [article](https://thenewstack.io/open-source-in-2024-more-volatility-more-risk-more-ai/) “Open Source in 2024: More Volatility, More Risk, More AI”. The article argues that the open-source landscape in 2024 may witness more companies transitioning from open source licenses to business licenses, increased regulatory actions, and continued evolution in generative AI. Despite economic uncertainty and recent developments, there is cautious optimism among industry insiders. The shift in licensing models, exemplified by HashiCorp's move to the Business Source License, has sparked discussions about the future of open source and the role of foundations. Additionally, the potential impact of regulatory frameworks on AI, especially generative AI, is gaining attention. The year 2024 is expected to bring forth challenges and opportunities, prompting the industry to reevaluate licensing and foster greater collaboration.

11.	1st Jan, Google and Harvard University published a [paper](https://arxiv.org/pdf/2401.00935.pdf) “Boundary Attention: Learning to Find Faint Boundaries at Any Resolution”. The study presents a differentiable model that explicitly models boundaries -- including contours, corners and junctions -- using a new mechanism called boundary attention. The paper shows that the model provides accurate results even when the boundary signal is very weak or is swamped by noise. Compared to previous classical methods for finding faint boundaries, the proposed model has the advantages of being differentiable; being scalable to larger images; and automatically adapting to an appropriate level of geometric detail in each part of an image. Compared to previous deep methods for finding boundaries via end-to-end training, it has the advantages of providing sub-pixel precision, being more resilient to noise, and being able to process any image at its native resolution and aspect ratio. The project is available here.

12.	1st Jan, researchers from Monash Uni, CSIRO etc published a [paper](https://arxiv.org/pdf/2401.00788.pdf) “A STRAIOS: Parameter-Efficient Instruction Tuning Code Large Language Models”. The research finds that The high cost of full-parameter fine-tuning (FFT) of Large Language Models (LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods. However, it remains unclear which methods provide the best cost-performance trade-off at different model scales. This paper introduces ASTRAIOS, a suite of 28 instruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up to 16 billion parameters. Through investigations across 5 tasks and 8 different datasets encompassing both code comprehension and code generation tasks, the study finds that FFT generally leads to the best downstream performance across all scales, and PEFT methods differ significantly in their efficacy based on the model scale. LoRA usually offers the most favorable trade-off between cost and performance. Further investigation into the effects of these methods on both model robustness and code security reveals that larger models tend to demonstrate reduced robustness and less security. At last, the researchers explore the relationships among updated parameters, cross-entropy loss, and task performance. We find that the tuning effectiveness observed in small models generalizes well to larger models, and the validation loss in instruction tuning can be a reliable indicator of overall downstream performance.

13.	31 Dec, JPMorgan AI Research published a [paper](https://arxiv.org/pdf/2401.00908v1.pdf) “DocLLM: A layout-aware generative language model for multimodal document understanding”. The study indicates that enterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. This paper presents DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. The proposed model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows to address irregular layouts and heterogeneous content frequently encountered in visual documents. The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks. The researchers demonstrate that the solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets. However, this is not an open-source project, no code or pretrained models are available.

14.	29 Dec, Upstage AI published a [paper](https://arxiv.org/pdf/2312.15166.pdf) “SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling”. The paper introduces SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. Inspired by recent efforts to efficiently up-scale LLMs, the researchers present a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining. In contrast to other LLM up-scaling methods that use mixture-of-experts, DUS does not require complex changes to train and inference efficiently. The study shows experimentally that DUS is simple yet effective in scaling up high-performance LLMs from small ones. Building on the DUS model, the paper also presents SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, surpassing Mixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0 license, promoting broad access and application in the LLM field. 

15.	29 Dec, Stanford Uni and Meta published a [paper](https://arxiv.org/pdf/2312.17661.pdf) “Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models”. The study indicates that the burgeoning interest in Multimodal Large Language Models (MLLMs), such as OpenAI's GPT-4V(ision), has significantly impacted both academic and industrial realms. These models enhance Large Language Models (LLMs) with advanced visual understanding capabilities, facilitating their application in a variety of multimodal tasks. Recently, Google introduced Gemini, a cutting-edge MLLM designed specifically for multimodal integration. Despite its advancements, preliminary benchmarks indicate that Gemini lags behind GPT models in commonsense reasoning tasks. However, this assessment, based on a limited dataset (i.e., HellaSWAG), does not fully capture Gemini's authentic commonsense reasoning potential. To address this gap, this study undertakes a thorough evaluation of Gemini's performance in complex reasoning tasks that necessitate the integration of commonsense knowledge across modalities. The researchers carry out a comprehensive analysis of 12 commonsense reasoning datasets, ranging from general to domain-specific tasks. This includes 11 datasets focused solely on language, as well as one that incorporates multimodal elements. The experiments across four LLMs and two MLLMs demonstrate Gemini's competitive commonsense reasoning capabilities. Additionally, the research identifies common challenges faced by current LLMs and MLLMs in addressing commonsense problems, underscoring the need for further advancements in enhancing the commonsense reasoning abilities of these models.

1)	21 Dec, Science published an [article](https://www.science.org/doi/full/10.1126/science.adm9788) “AI is transforming how science is done. Science education must reflect this change”. The discussion on artificial intelligence (AI) in science education often focuses on its role in achieving science learning objectives, but there's a crucial aspect receiving insufficient attention—how AI is transforming the nature of science (NOS). AI influences not only teaching tools but also the fundamental aspects of scientific inquiry. The advantages of AI include personalized education through simulations and tailored content, while concerns involve potential drawbacks like reliance on AI for composing homework. The shift in focus from memorization to contemporary learning skills, such as critical thinking and innovation, challenges AI's ability to replicate sophisticated human skills. AI is already reshaping professional science, influencing hypothesis generation, experiment design, and data interpretation. Recommendations for responsible AI use in scientific research stress transparency, risk management, and participatory methods. This shift in scientific practices necessitates a comprehensive approach to science education reform, including curriculum restructuring, teacher training, and new teaching tools to align with the evolving landscape of AI-informed science.

