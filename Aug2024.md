

***Sep 8 2024***


1. ***Key Findings on Few-Shot Learning and Fine-Tuning in LLMs: <br>A study published by Area Science Park Trieste compares In-context Learning (ICL) and Supervised Fine-Tuning (SFT) in large language models (LLMs). While both strategies improve performance on specific tasks, the internal representations they induce are markedly different. ICL creates hierarchical, semantically organized representations, while SFT's are fuzzier and more mixed. Despite this, SFT develops probability modes better suited for encoding answers. This highlights the diverse computational strategies of LLMs.*** <br><br>
   Sep 7, Area Science Park Trieste published a [paper](https://arxiv.org/pdf/2409.03662) “The representation landscape of few-shot learning and fine-tuning in large language models”. In-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks. Despite their different natures, these strategies often lead to comparable performance gains. However, little is known about whether they induce similar representations inside LLMs. The authors approach this problem by analyzing the probability landscape of their hidden representations in the two cases. More specifically, the study compares how LLMs solve the same question-answering task, finding that ICL and SFT create very different internal structures, in both cases undergoing a sharp transition in the middle of the network. In the first half of the network, ICL shapes interpretable representations hierarchically organized according to their semantic content. In contrast, the probability landscape obtained with SFT is fuzzier and semantically mixed. In the second half of the model, the fine-tuned representations develop probability modes that better encode the identity of answers, while the landscape of ICL representations is characterized by less defined peaks. The approach reveals the diverse computational strategies developed inside LLMs to solve the same task across different conditions, allowing researchers to make a step towards designing optimal methods to extract information from language models.

3. ***Salesforce Introduces xLAM for AI Agent Systems: <br>Salesforce's paper presents the xLAM family of large action models, designed to enhance AI agent tasks. These models, ranging from 1B to 8x22B parameters, are trained using a scalable pipeline unifying various datasets. xLAM models outperform GPT-4 and Claude-3 in tool use, placing first on the Berkeley Function-Calling Leaderboard. By releasing xLAM, Salesforce aims to democratize access to high-performance models for autonomous agents.*** <br><br>
   Sep 5, Salesforce published a paper “xLAM: A Family of Large Action Models to Empower AI Agent Systems”. Autonomous agents powered by large language models (LLMs) have attracted significant research interest. However, the open-source community faces many challenges in developing specialized models for agent tasks, driven by the scarcity of high-quality agent datasets and the absence of standard protocols in this area. The study introduces and publicly releases xLAM, a series of large action models designed for AI agent tasks. The xLAM series includes five models with both dense and mixture-of-expert architectures, ranging from 1B to 8x22B parameters, trained using a scalable, flexible pipeline that unifies, augments, and synthesizes diverse datasets to enhance AI agents' generalizability and performance across varied environments. Experimental results demonstrate that xLAM consistently delivers exceptional performance across multiple agent ability benchmarks, notably securing the 1st position on the Berkeley Function-Calling Leaderboard, outperforming GPT-4, Claude-3, and many other models in terms of tool use. By releasing the xLAM series, the authors aim to advance the performance of open-source LLMs for autonomous AI agents, potentially accelerating progress and democratizing access to high-performance models for agent tasks. Models are available at [this https URL](https://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4)

5. ***Google’s Framework for Enhancing Mathematical Reasoning in LLMs: <br>Google’s paper introduces a multi-turn direct preference learning framework to improve LLMs' mathematical reasoning. Tailored for tasks integrating external tools like code interpreters, this method boosts performance, particularly in models fine-tuned for multi-turn reasoning. Experimentation shows marked improvements in benchmarks such as GSM8K and MATH, demonstrating the framework’s effectiveness in optimizing model trajectories.*** <br><br>
   Sep 4, Google published a [paper](https://arxiv.org/pdf/2409.03215) “Building Math Agents with Multi-Turn Iterative Preference Learning”. Recent studies have shown that large language models' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, the paper introduces a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of the framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Experimental results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model's performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.

7. ***Simula Research Lab Examines LLMs in Log Parsing: <br>A study published by Simula Research Lab evaluates six LLMs, including GPT-3.5 and CodeLlama, in log parsing tasks. The results show that free-to-use models can compete with proprietary models, with CodeLlama outperforming GPT-3.5 by 10% in correct template extraction. These findings highlight the potential of open-source models in log parsing and their usability advantages.*** <br><br>
   Sep 4, Simula Research Lab published a [paper](https://arxiv.org/pdf/2409.02474) “A Comparative Study on Large Language Models for Log Parsing”. Log messages provide valuable information about the status of software systems. This information is provided in an unstructured fashion and automated approaches are applied to extract relevant parameters. To ease this process, log parsing can be applied, which transforms log messages into structured log templates. Recent advances in language models have led to several studies that apply ChatGPT to the task of log parsing with promising results. However, the performance of other state-of-the-art large language models (LLMs) on the log parsing task remains unclear. This study investigates the current capability of state-of-the-art LLMs to perform log parsing. The authors select six recent LLMs, including both paid proprietary (GPT-3.5, Claude 2.1) and four free-to-use open models, and compare their performance on system logs obtained from a selection of mature open-source projects. The study designs two different prompting approaches and apply the LLMs on 1, 354 log templates across 16 different projects. The study evaluates their effectiveness, in the number of correctly identified templates, and the syntactic similarity between the generated templates and the ground truth. The paper found that free-to-use models are able to compete with paid models, with CodeLlama extracting 10% more log templates correctly than GPT-3.5. Moreover, the authors provide qualitative insights into the usability of language models (e.g., how easy it is to use their responses). The results reveal that some of the smaller, free-to-use LLMs can considerably assist log parsing compared to their paid proprietary competitors, especially code-specialized models.

9. ***OLMoE: A High-Performance Open Mixture-of-Experts Model: <br>A collaborative paper introduces OLMoE, a sparse Mixture-of-Experts (MoE) language model with 7 billion parameters. Trained on 5 trillion tokens, OLMoE outperforms larger models like Llama2-13B-Chat, proving the efficacy of sparse MoE architectures. The authors open-source all components, aiming to advance the capabilities of open models.*** <br><br>
    Sep 3, Allen Inst for AI, Contextual AI, Uni of Washington and Princeton Uni published a [paper](https://arxiv.org/pdf/2409.02060) “OLMoE: Open Mixture-of-Experts Language Models”. The paper introduces OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. The paper pretrains it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. The models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. The paper presents various experiments on MoE training, analyze routing in the model showing high specialization, and open-source all aspects of the work: model weights, training data, [code](https://github.com/allenai/OLMoE), and logs.

11. ***GOT Model Aims to Revolutionize OCR: <br>researchers introduced the General OCR Theory (GOT) and its model, OCR-2.0. GOT handles a broad spectrum of artificial optical characters, offering region-specific recognition and interactive features. Experiments demonstrate its superiority over traditional OCR systems, positioning it as a comprehensive solution for modern OCR needs.*** <br><br>
    Sep 3, StepFun, Megvii Tech, UCAS, and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2409.01704) “General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model”. Traditional OCR systems (OCR-1.0) are increasingly unable to meet people's usage due to the growing demand for intelligent processing of man-made optical characters. This paper collectively refers to all artificial optical signals (e.g., plain texts, math/molecular formulas, tables, charts, sheet music, and even geometric shapes) as "characters" and propose the General OCR Theory along with an excellent model, namely GOT, to promote the arrival of OCR-2.0. The GOT, with 580M parameters, is a unified, elegant, and end-to-end model, consisting of a high-compression encoder and a long-contexts decoder. As an OCR-2.0 model, GOT can handle all the above "characters" under various OCR tasks. On the input side, the model supports commonly used scene- and document-style images in slice and whole-page styles. On the output side, GOT can generate plain or formatted results (markdown/tikz/smiles/kern) via an easy prompt. Besides, the model enjoys interactive OCR features, i.e., region-level recognition guided by coordinates or colors. Furthermore, the authors also adapt dynamic resolution and multi-page OCR technologies to GOT for better practicality. In experiments, the paper provides sufficient results to prove the superiority of the model.

13. ***Political DEBATE: Efficient Classifiers for Political Texts: <br>Princeton University and collaborators published a paper introducing Political DEBATE models for zero-shot and few-shot classification of political texts. These models outperform state-of-the-art classifiers, offering efficient, open-source solutions. Additionally, the release of the PolNLI dataset facilitates further research in political document classification.*** <br><br>
    Sep 2, Princeton Uni, Pennsylvania State Uni and Louisiana State Uni published a [paper](https://arxiv.org/pdf/2409.02078) “Political DEBATE: Efficient Zero-shot and Few-shot Classifiers for Political Text”. Social scientists quickly adopted large language models due to their ability to annotate documents without supervised training, an ability known as zero-shot learning. However, due to their compute demands, cost, and often proprietary nature, these models are often at odds with replication and open science standards. This paper introduces the Political DEBATE (DeBERTa Algorithm for Textual Entailment) language models for zero-shot and few-shot classification of political documents. These models are not only as good, or better than, state-of-the art large language models at zero and few-shot classification, but are orders of magnitude more efficient and completely open source. By training the models on a simple random sample of 10-25 documents, they can outperform supervised classifiers trained on hundreds or thousands of documents and state-of-the-art generative models with complex, engineered prompts. Additionally, the authors release the PolNLI dataset used to train these models -- a corpus of over 200,000 political documents with highly accurate labels across over 800 classification tasks.

15. ***Using Report Cards to Qualitatively Evaluate LLMs: <br>The University of Toronto and the Vector Institute published a paper introducing "Report Cards"—natural language summaries of LLM behavior. These qualitative evaluations provide clearer insights into model performance than traditional benchmarks, enabling more interpretable and holistic assessments of LLMs.*** <br><br>
    Sep 1, Uni of Toronto and Vector Inst published a [paper](https://arxiv.org/pdf/2409.00844) “Report Cards: Qualitative Evaluation of Language Models Using Natural Language Summaries”. The rapid development and dynamic nature of large language models (LLMs) make it difficult for conventional quantitative benchmarks to accurately assess their capabilities. The paper proposes report cards, which are human-interpretable, natural language summaries of model behavior for specific skills or topics. The authors develop a framework to evaluate report cards based on three criteria: specificity (ability to distinguish between models), faithfulness (accurate representation of model capabilities), and interpretability (clarity and relevance to humans). The paper also proposes an iterative algorithm for generating report cards without human supervision and explore its efficacy by ablating various design choices. Through experimentation with popular LLMs, the authors demonstrate that report cards provide insights beyond traditional benchmarks and can help address the need for a more interpretable and holistic evaluation of LLMs.

17. ***MIT Investigates Dataset Licensing in AI: <br>MIT's study systematically audits over 1,800 datasets to address legal and ethical concerns related to dataset licensing and attribution in AI. The findings reveal widespread misattribution, underscoring the need for better transparency in dataset usage. To facilitate this, the study releases an interactive tool, the Data Provenance Explorer, for tracing dataset lineage.*** <br><br>
    Aug 30, MIT published a [paper](https://www.nature.com/articles/s42256-024-00878-8) on Nature Machine Intelligence “A large-scale audit of dataset licensing and attribution in AI”. The race to train language models on vast, diverse and inconsistently documented datasets raises pressing legal and ethical concerns. To improve data transparency and understanding, the study convenes a multi-disciplinary effort between legal and machine learning experts to systematically audit and trace more than 1,800 text datasets. The authors develop tools and standards to trace the lineage of these datasets, including their source, creators, licences and subsequent use. The landscape analysis highlights sharp divides in the composition and focus of data licenced for commercial use. Important categories including low-resource languages, creative tasks and new synthetic data all tend to be restrictively licenced. The authors observe frequent miscategorization of licences on popular dataset hosting sites, with licence omission rates of more than 70% and error rates of more than 50%. This highlights a crisis in misattribution and informed use of popular datasets driving many recent breakthroughs. The analysis of data sources also explains the application of copyright law and fair use to finetuning data. As a contribution to continuing improvements in dataset transparency and responsible use, the authors release the audit, with an interactive user interface, the Data Provenance Explorer, to enable practitioners to trace and filter on data provenance for the most popular finetuning data collections: www.dataprovenance.org.

19. ***Reassessing AI Alignment Beyond Preferences: <br>A collaborative paper published challenges the preferentist approach to AI alignment, arguing that aligning AI with human values requires going beyond preferences. The authors propose aligning AI with normative standards suited to their social roles, advocating for stakeholder negotiation to ensure alignment promotes mutual benefit across diverse values.*** <br><br>
    Aug 30, MIT, UC Berkeley, Uni of College London, and Uni of Cambridge published a [paper](https://arxiv.org/pdf/2408.16984) “Beyond Preferences in AI Alignment”. The dominant practice of AI alignment assumes (1) that preferences are an adequate representation of human values, (2) that human rationality can be understood in terms of maximizing the satisfaction of preferences, and (3) that AI systems should be aligned with the preferences of one or more humans to ensure that they behave safely and in accordance with our values. Whether implicitly followed or explicitly endorsed, these commitments constitute what we term a preferentist approach to AI alignment. This paper characterizes and challenges the preferentist approach, describing conceptual and technical alternatives that are ripe for further research. The authors first survey the limits of rational choice theory as a descriptive model, explaining how preferences fail to capture the thick semantic content of human values, and how utility representations neglect the possible incommensurability of those values. The authors then critique the normativity of expected utility theory (EUT) for humans and AI, drawing upon arguments showing how rational agents need not comply with EUT, while highlighting how EUT is silent on which preferences are normatively acceptable. Finally, the researchers argue that these limitations motivate a reframing of the targets of AI alignment: Instead of alignment with the preferences of a human user, developer, or humanity-writ-large, AI systems should be aligned with normative standards appropriate to their social roles, such as the role of a general-purpose assistant. Furthermore, these standards should be negotiated and agreed upon by all relevant stakeholders. On this alternative conception of alignment, a multiplicity of AI systems will be able to serve diverse ends, aligned with normative standards that promote mutual benefit and limit harm despite our plural and divergent values.

21. ***Arctic-SnowCoder: Improving Code Pretraining with High-Quality Data: <br>Researchers from Snowflake and universities introduced Arctic-SnowCoder, a data-efficient model that achieves state-of-the-art performance in code pretraining. Through progressively refined data phases, the model demonstrates the importance of high-quality data aligned with downstream applications, outperforming larger models despite being trained on fewer tokens.*** <br><br>
    Aug 30, Snowflake, UIUC and Seul Nat Uni published a [paper](https://arxiv.org/pdf/2409.02326) “Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining”. Recent studies have been increasingly demonstrating that high-quality data is crucial for effective pretraining of language models. However, the precise definition of "high-quality" remains underexplored. Focusing on the code domain, the paper introduces Arctic-SnowCoder-1.3B, a data-efficient base code model pretrained on 555B tokens through three phases of progressively refined data: (1) general pretraining with 500B standard-quality code tokens, preprocessed through basic filtering, deduplication, and decontamination, (2) continued pretraining with 50B high-quality tokens, selected from phase one by a BERT-style quality annotator trained to distinguish good code from random data, using positive examples drawn from high-quality code files, along with instruction data from Magicoder and StarCoder2-Instruct, and (3) enhanced pretraining with 5B synthetic data created by Llama-3.1-70B using phase two data as seeds, adapting the Magicoder approach for pretraining. Despite being trained on a limited dataset, Arctic-SnowCoder achieves state-of-the-art performance on BigCodeBench, a coding benchmark focusing on practical and challenging programming tasks, compared to similarly sized models trained on no more than 1T tokens, outperforming Phi-1.5-1.3B by 36%. Across all evaluated benchmarks, Arctic-SnowCoder-1.3B beats StarCoderBase-3B pretrained on 1T tokens. Additionally, it matches the performance of leading small base code models trained on trillions of tokens. For example, Arctic-SnowCoder-1.3B surpasses StarCoder2-3B, pretrained on over 3.3T tokens, on HumanEval+, a benchmark that evaluates function-level code generation, and remains competitive on BigCodeBench. The evaluation presents a comprehensive analysis justifying various design choices for Arctic-SnowCoder. Most importantly, the authors find that the key to high-quality data is its alignment with the distribution of downstream applications.

23. ***Jina-ColBERT-v2: Advancing Multilingual Retrieval: <br>A paper presents Jina-ColBERT-v2, an optimized version of the ColBERT model for multilingual retrieval tasks. By improving efficiency and cutting storage requirements, the model demonstrates strong performance across multiple retrieval benchmarks while maintaining its effectiveness in a bi-encoder architecture.*** <br><br>
    Aug 30, Uni of Texas at Austin and Jina AI GmbH published a [paper](https://arxiv.org/pdf/2408.16672) “Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever”. Multi-vector dense models, such as ColBERT, have proven highly effective in information retrieval. ColBERT's late interaction scoring approximates the joint query-document attention seen in cross-encoders while maintaining inference efficiency closer to traditional dense retrieval models, thanks to its bi-encoder architecture and recent optimizations in indexing and search. This paper introduces several improvements to the ColBERT model architecture and training pipeline, leveraging techniques successful in the more established single-vector embedding model paradigm, particularly those suited for heterogeneous multilingual data. The new model, Jina-ColBERT-v2, demonstrates strong performance across a range of English and multilingual retrieval tasks, while also cutting storage requirements by up to 50% compared to previous models.

25. ***Improving OCR with Context Leveraging Models: <br>University College London's paper introduces CLOCR-C, a model that leverages context-adaptive abilities of transformer-based language models to correct OCR errors in digitized historical archives. By incorporating socio-cultural context, CLOCR-C significantly reduces error rates and improves the quality of OCR for downstream tasks like Named Entity Recognition.*** <br><br>
    Aug 30, Uni of College London published a [paper](https://arxiv.org/pdf/2408.17428) “CLOCR-C: Context Leveraging OCR Correction with Pre-trained Language Models”. The digitisation of historical print media archives is crucial for increasing accessibility to contemporary records. However, the process of Optical Character Recognition (OCR) used to convert physical records to digital text is prone to errors, particularly in the case of newspapers and periodicals due to their complex layouts. This paper introduces Context Leveraging OCR Correction (CLOCR-C), which utilises the infilling and context-adaptive abilities of transformer-based language models (LMs) to improve OCR quality. The study aims to determine if LMs can perform post-OCR correction, improve downstream NLP tasks, and the value of providing the socio-cultural context as part of the correction process. Experiments were conducted using seven LMs on three datasets: the 19th Century Serials Edition (NCSE) and two datasets from the Overproof collection. The results demonstrate that some LMs can significantly reduce error rates, with the top-performing model achieving over a 60% reduction in character error rate on the NCSE dataset. The OCR improvements extend to downstream tasks, such as Named Entity Recognition, with increased Cosine Named Entity Similarity. Furthermore, the study shows that providing socio-cultural context in the prompts improves performance, while misleading prompts lower performance. In addition to the findings, this study releases a dataset of 91 transcribed articles from the NCSE, containing a total of 40 thousand words, to support further research in this area. The findings suggest that CLOCR-C is a promising approach for enhancing the quality of existing digital archives by leveraging the socio-cultural information embedded in the LMs and the text requiring correction.

27. ***PrivacyLens: Evaluating Privacy Norm Awareness in LLMs: <br>a paper proposes PrivacyLens, a framework to evaluate LLMs’ awareness of privacy norms in communication scenarios. Results show that even state-of-the-art LLMs, like GPT-4 and Llama-3, often leak sensitive information despite privacy-enhancing prompts. PrivacyLens provides a structured evaluation tool to measure and mitigate privacy risks in LLMs.*** <br><br>
    Aug 29, Stanford Uni, Northeastern Uni and Harvard Uni published a [paper](https://arxiv.org/pdf/2409.00138) “PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action”. As language models (LMs) are widely utilized in personalized communication scenarios (e.g., sending emails, writing social media posts) and endowed with a certain level of agency, ensuring they act in accordance with the contextual privacy norms becomes increasingly critical. However, quantifying the privacy norm awareness of LMs and the emerging privacy risk in LM-mediated communication is challenging due to (1) the contextual and long-tailed nature of privacy-sensitive cases, and (2) the lack of evaluation approaches that capture realistic application scenarios. To address these challenges, the authors propose PrivacyLens, a novel framework designed to extend privacy-sensitive seeds into expressive vignettes and further into agent trajectories, enabling multi-level evaluation of privacy leakage in LM agents' actions. The authors instantiate PrivacyLens with a collection of privacy norms grounded in privacy literature and crowdsourced seeds. Using this dataset, the paper reveals a discrepancy between LM performance in answering probing questions and their actual behavior when executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4 and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even when prompted with privacy-enhancing instructions. The authors also demonstrate the dynamic nature of PrivacyLens by extending each seed into multiple trajectories to red-team LM privacy leakage risk. Dataset and code are available at https://github.com/SALT-NLP/PrivacyLens.

29. ***LiGNN: LinkedIn’s Large-Scale Graph Neural Networks: <br>LinkedIn's award-winning paperdetails LiGNN, a framework for deploying large-scale graph neural networks (GNNs). With algorithmic improvements and scalable solutions, LiGNN enhances the quality of GNN representation learning, driving measurable improvements in user engagement metrics, such as job application rates and ad click-through rates.*** <br><br>
    Aug 29, one of KDD2024 Best [paper](https://dl.acm.org/doi/10.1145/3637528.3671566) is LinkedIn’s “LiGNN: Graph Neural Networks at LinkedIn”. This paper presents LiGNN, a deployed large-scale Graph Neural Networks (GNNs) Framework. The authors share the insight on developing and deployment of GNNs at large scale at LinkedIn. The paper presents a set of algorithmic improvements to the quality of GNN representation learning including temporal graph architectures with long term losses, effective cold start solutions via graph densification, ID embeddings and multi-hop neighbor sampling. The authors explain how to build and speed up by 7x the large-scale training on LinkedIn graphs with adaptive sampling of neighbors, grouping and slicing of training data batches, specialized shared-memory queue and local gradient optimization. The paper summarizes the deployment lessons and learnings gathered from A/B test experiments. The techniques presented in this work have contributed to an approximate relative improvements of 1% of Job application hearing back rate, 2% Ads CTR lift, 0.5% of Feed engaged daily active users, 0.2% session lift and 0.1% weekly active user lift from people recommendation. The authors believe that this work can provide practical solutions and insights for engineers who are interested in applying Graph neural networks at large scale.

31. ***Introduction of SurveySum Dataset for Scientific Article Summarization: <br>The paper titled "SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section" presents a novel dataset, SurveySum, which fills a gap in domain-specific summarization tools. The paper introduces two pipelines for summarizing scientific articles into a survey section and evaluates these pipelines using various metrics. The results emphasize the critical role of high-quality retrieval stages and different configurations for improving the quality of generated summaries.*** <br><br>
    Aug 29, Brasília-DF published a [paper](https://www.arxiv.org/pdf/2408.16444) “SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section”. Document summarization is a task to shorten texts into concise and informative summaries. This paper introduces a novel dataset designed for summarizing multiple scientific articles into a section of a survey. The contributions of the paper are: (1) SurveySum, a new dataset addressing the gap in domain-specific summarization tools; (2) two specific pipelines to summarize scientific articles into a section of a survey; and (3) the evaluation of these pipelines using multiple metrics to compare their performance. The results highlight the importance of high-quality retrieval stages and the impact of different configurations on the quality of generated summaries.

33. ***Generative Verifiers in Reward Modeling and Next-Token Prediction: <br>Google’s paper, "Generative Verifiers: Reward Modeling as Next-Token Prediction," discusses improving the performance of large language models (LLMs) using verifiers trained through next-token prediction. This method, referred to as generative verifiers (GenRM), combines verification with solution generation. GenRM enhances chain-of-thought reasoning and integrates seamlessly with instruction tuning, outperforming standard discriminative verifiers and LLM-as-a-Judge by 16-64% on various reasoning tasks.*** <br><br>
    Aug 28, Google published a [paper](https://arxiv.org/pdf/2408.15240) “Generative Verifiers: Reward Modeling as Next-Token Prediction”. Verifiers or reward models are often used to enhance the reasoning performance of large language models (LLMs). A common approach is the Best-of-N method, where N candidate solutions generated by the LLM are ranked by a verifier, and the best one is selected. While LLM-based verifiers are typically trained as discriminative classifiers to score solutions, they do not utilize the text generation capabilities of pretrained LLMs. To overcome this limitation, the study instead proposes training verifiers using the ubiquitous next-token prediction objective, jointly on verification and solution generation. Compared to standard verifiers, such generative verifiers (GenRM) can benefit from several advantages of LLMs: they integrate seamlessly with instruction tuning, enable chain-of-thought reasoning, and can utilize additional inference-time compute via majority voting for better verification. The authors demonstrate that when using Gemma-based verifiers on algorithmic and grade-school math reasoning tasks, GenRM outperforms discriminative verifiers and LLM-as-a-Judge, showing a 16-64% improvement in the percentage of problems solved with Best-of-N. Furthermore, the study shows that GenRM scales favorably across dataset size, model capacity, and inference-time compute.

35. ***Engaged Human Learning Through Language Model Agent Conversations: <br>Stanford and Yale Universities introduced "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations." The study proposes Co-STORM, an LM-powered system where users observe and interact with conversations between multiple LM agents to discover unknown unknowns. Co-STORM organizes information into dynamic mind maps and outperforms traditional search engines and RAG chatbots in human evaluation.*** <br><br>
    Aug 27, Stanford Uni and Yale Uni published a [paper](https://www.arxiv.org/pdf/2408.15232) “Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations”. While language model (LM)-powered chatbots and generative search engines excel at answering concrete queries, discovering information in the terrain of unknown unknowns remains challenging for users. To emulate the common educational scenario where children/students learn by listening to and participating in conversations of their parents/teachers, the study creates Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all the questions, Co-STORM lets users observe and occasionally steer the discourse among several LM agents. The agents ask questions on the user's behalf, allowing the user to discover unknown unknowns serendipitously. To facilitate user interaction, Co-STORM assists users in tracking the discourse by organizing the uncovered information into a dynamic mind map, ultimately generating a comprehensive report as takeaways. For automatic evaluation, the authors construct the WildSeek dataset by collecting real information-seeking records with user goals. Co-STORM outperforms baseline methods on both discourse trace and report quality. In a further human evaluation, 70% of participants prefer Co-STORM over a search engine, and 78% favor it over a RAG chatbot.

37. ***Loss of Plasticity in Deep Continual Learning: <br>The University of Alberta’s paper published in Nature explores the limitations of deep learning in continual learning settings. It shows that deep learning models gradually lose plasticity, learning no better than shallow networks. The study highlights that only algorithms injecting random variability, such as continual backpropagation, can maintain plasticity indefinitely, suggesting that gradient-descent-based methods alone are insufficient for sustained deep learning.*** <br><br>
    Aug 21, Uni of Alberta published a [paper](https://www.nature.com/articles/s41586-024-07711-7) on Nature “Loss of plasticity in deep continual learning”. Artificial neural networks, deep-learning methods and the backpropagation algorithm form the foundation of modern machine learning and artificial intelligence. These methods are almost always used in two phases, one in which the weights of the network are updated and one in which the weights are held constant while the network is used or evaluated. This contrasts with natural learning and many applications, which require continual learning. It has been unclear whether or not deep learning methods work in continual learning settings. Here the authors show that they do not—that standard deep-learning methods gradually lose plasticity in continual-learning settings until they learn no better than a shallow network. The study shows such loss of plasticity using the classic ImageNet dataset and reinforcement-learning problems across a wide range of variations in the network and the learning algorithm. Plasticity is maintained indefinitely only by algorithms that continually inject diversity into the network, such as the proposed continual backpropagation algorithm, a variation of backpropagation in which a small fraction of less-used units are continually and randomly reinitialized. The results indicate that methods based on gradient descent are not enough—that sustained deep learning requires a random, non-gradient component to maintain variability and plasticity.
 <br><br><br>

***Sep 1 2024***

1. ***Stanford Uni and UC Berkeley on "Law of Vision Representation": <br>This paper introduces the "Law of Vision Representation" in multimodal large language models (MLLMs). The authors find a strong correlation between cross-modal alignment, vision representation, and model performance. They develop the AC score, which quantifies these factors and shows a linear relationship with model performance. By identifying optimal vision representations without needing to fine-tune the language model, they reduce computational costs by 99.7%.*** <br><br>
   Aug 29, Stanford Uni and UC Berkeley published a [paper](https://arxiv.org/pdf/2408.16357) “Law of Vision Representation in MLLMs”. The paper presents the "Law of Vision Representation" in multimodal large language models (MLLMs). It reveals a strong correlation between the combination of cross-modal alignment, correspondence in vision representation, and MLLM performance. The authors quantify the two factors using the cross-modal Alignment and Correspondence score (AC score). Through extensive experiments involving thirteen different vision representation settings and evaluations across eight benchmarks, the paper finds that the AC score is linearly correlated to model performance. By leveraging this relationship, the authors are able to identify and train the optimal vision representation only, which does not require finetuning the language model every time, resulting in a 99.7% reduction in computational cost.

3. ***HKUST and Huggingface on "LlamaDuo": <br>"LlamaDuo" is introduced as an LLMOps pipeline that enables migration from service-oriented LLMs to smaller, local models. This approach addresses challenges like operational failures, privacy concerns, and offline requirements. By iteratively fine-tuning smaller models with synthetic datasets from service LLMs, it ensures smaller models can match or surpass the capabilities of service LLMs for specific tasks.*** <br><br>
   Aug 29, HKUST and Huggingface published a [paper](https://www.arxiv.org/pdf/2408.13467) “LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMs”. The widespread adoption of cloud-based proprietary large language models (LLMs) has introduced significant challenges, including operational dependencies, privacy concerns, and the necessity of continuous internet connectivity. This paper introduces an LLMOps pipeline, "LlamaDuo", for the seamless migration of knowledge and abilities from service-oriented LLMs to smaller, locally manageable models. This pipeline is crucial for ensuring service continuity in the presence of operational failures, strict privacy policies, or offline requirements. LlamaDuo involves fine-tuning a small language model against the service LLM using a synthetic dataset generated by the latter. If the performance of the fine-tuned model falls short of expectations, it is enhanced by further fine-tuning with additional similar data created by the service LLM. This iterative process guarantees that the smaller model can eventually match or even surpass the service LLM's capabilities in specific downstream tasks, offering a practical and scalable solution for managing AI deployments in constrained environments. Extensive experiments with leading edge LLMs are conducted to demonstrate the effectiveness, adaptability, and affordability of LlamaDuo across various downstream tasks. The pipeline implementation is available at https://github.com/deep-diver/llamaduo.

5. ***Cambridge and University of Hong Kong on "GRAB": <br>The paper presents "GRAB," a graph analysis benchmark for large multimodal models (LMMs). The benchmark consists of 2,170 questions covering 23 graph properties, challenging current LMMs, with the top-performing model scoring only 21.7%. The goal is to push LMMs' capabilities in graph analysis.*** <br><br>
   Aug 29, Uni of Cambridge and the Uni of Hong Kong published a [paper](https://www.arxiv.org/pdf/2408.11817) “GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models”. Large multimodal models (LMMs) have exhibited proficiencies across many visual tasks. Although numerous well-known benchmarks exist to evaluate model performance, they increasingly have insufficient headroom. As such, there is a pressing need for a new generation of benchmarks challenging enough for the next generation of LMMs. One area that LMMs show potential is graph analysis, specifically, the tasks an analyst might typically perform when interpreting figures such as estimating the mean, intercepts or correlations of functions and data series. This paper introduces GRAB, a graph analysis benchmark, fit for current and future frontier LMMs. The benchmark is entirely synthetic, ensuring high-quality, noise-free questions. GRAB is comprised of 2170 questions, covering four tasks and 23 graph properties. The authors evaluate 20 LMMs on GRAB, finding it to be a challenging benchmark, with the highest performing model attaining a score of just 21.7%. Finally, the authors conduct various ablations to investigate where the models succeed and struggle. The authors [release GRAB](https://grab-benchmark.github.io/) to encourage progress in this important, growing domain.

7. ***Nvidia and collaborators on "Eagle": <br>This paper explores multimodal LLM design using a mixture of vision encoders. It shows that concatenating visual tokens from multiple vision encoders is as effective as more complex architectures. The authors also introduce Pre-Alignment, improving model coherence and performance on major MLLM benchmarks, with the Eagle model family outperforming leading models.*** <br><br>
   Aug 28, Nvidia, Georgia Tech, UMD and HKPU published a [paper](https://arxiv.org/pdf/2408.15998) “Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders”. The ability to accurately interpret complex visual information is a crucial topic of multimodal large language models (MLLMs). Recent work indicates that enhanced visual perception significantly reduces hallucinations and improves performance on resolution-sensitive tasks, such as optical character recognition and document analysis. A number of recent MLLMs achieve this goal using a mixture of vision encoders. Despite their success, there is a lack of systematic comparisons and detailed ablation studies addressing critical aspects, such as expert selection and the integration of multiple vision experts. This study provides an extensive exploration of the design space for MLLMs using a mixture of vision encoders and resolutions. The findings reveal several underlying principles common to various existing strategies, leading to a streamlined yet effective design approach. The paper discovers that simply concatenating visual tokens from a set of complementary vision encoders is as effective as more complex mixing architectures or strategies. The authors additionally introduce Pre-Alignment to bridge the gap between vision-focused encoders and language tokens, enhancing model coherence. The resulting family of MLLMs, Eagle, surpasses other leading open-source models on major MLLM benchmarks. Models and code: https://github.com/NVlabs/Eagle
 
9. ***Bar-Ilan and Allen Institute for AI on "Knowledge Navigator": <br>"Knowledge Navigator" is a system for enhancing exploratory search in scientific literature by organizing search results into a two-level hierarchy of topics and subtopics. This structured approach aids users in refining searches and discovering deeper knowledge across scientific domains.*** <br><br>
    Aug 28, Bar-Ilan and Allen Inst for AI published a [paper](https://www.arxiv.org/pdf/2408.15836) “Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature”. The exponential growth of scientific literature necessitates advanced tools for effective knowledge exploration. The paper presents Knowledge Navigator, a system designed to enhance exploratory search abilities by organizing and structuring the retrieved documents from broad topical queries into a navigable, two-level hierarchy of named and descriptive scientific topics and subtopics. This structured organization provides an overall view of the research themes in a domain, while also enabling iterative search and deeper knowledge discovery within specific subtopics by allowing users to refine their focus and retrieve additional relevant documents. Knowledge Navigator combines LLM capabilities with cluster-based methods to enable an effective browsing method. The paper demonstrates the approach's effectiveness through automatic and manual evaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. The code, prompts, and benchmarks are made [publicly available](https://knowledge-navigators.github.io/).

11. ***Writer, Inc. on "Writing in the Margins": <br>"Writing in the Margins" (WiM) is a new inference pattern for improving LLM performance in long-context retrieval tasks. By segmenting and inferring information in chunks, it boosts accuracy by 7.5% for reasoning tasks and over 30% for aggregation tasks, without fine-tuning models. WiM is implemented using Hugging Face's Transformers library.*** <br><br>
    Aug 27, Writer, Inc. published a [paper](https://www.arxiv.org/pdf/2408.14906) “Writing in the Margins: Better Inference Pattern for Long Context Retrieval”. The paper introduces Writing in the Margins (WiM), a new inference pattern for Large Language Models designed to optimize the handling of long input sequences in retrieval-oriented tasks. This approach leverages the chunked prefill of the key-value cache to perform segment-wise inference, which enables efficient processing of extensive contexts along with the generation and classification of intermediate information ("margins") that guide the model towards specific tasks. This method increases computational overhead marginally while significantly enhancing the performance of off-the-shelf models without the need for fine-tuning. Specifically, the authors observe that WiM provides an average enhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG) and more than a 30.0% increase in the F1-score for aggregation tasks (CWE). Additionally, the paper shows how the proposed pattern fits into an interactive retrieval design that provides end-users with ongoing updates about the progress of context processing, and pinpoints the integration of relevant information into the final response. The authors release the implementation of WiM using Hugging Face Transformers library at https://github.com/writer/writing-in-the-margins.

13. ***UC Berkeley and Stanford on "Text2SQL is Not Enough": <br>The "Table-Augmented Generation" (TAG) paradigm is introduced to expand the scope of natural language questions over databases, going beyond Text2SQL and Retrieval-Augmented Generation methods. TAG enables broader interactions between LMs and databases, offering new research opportunities in data query handling.*** <br><br>
    Aug 27, UC Berkeley and Stanford Uni. Published a [paper](https://www.arxiv.org/pdf/2408.14717) “Text2SQL is Not Enough: Unifying AI and Databases with TAG”. AI systems that serve natural language questions over databases promise to unlock tremendous value. Such systems would allow users to leverage the powerful reasoning and knowledge capabilities of language models (LMs) alongside the scalable computational power of data management systems. These combined capabilities would empower users to ask arbitrary natural language questions over custom data sources. However, existing methods and benchmarks insufficiently explore this setting. Text2SQL methods focus solely on natural language questions that can be expressed in relational algebra, representing a small subset of the questions real users wish to ask. Likewise, Retrieval-Augmented Generation (RAG) considers the limited subset of queries that can be answered with point lookups to one or a few data records within the database. The paper proposes Table-Augmented Generation (TAG), a unified and general-purpose paradigm for answering natural language questions over databases. The TAG model represents a wide range of interactions between the LM and database that have been previously unexplored and creates exciting research opportunities for leveraging the world knowledge and reasoning capabilities of LMs over data. The paper systematically develops benchmarks to study the TAG problem and find that standard methods answer no more than 20% of queries correctly, confirming the need for further research in this area. The authors release code for the benchmark at https://github.com/TAG-Research/TAG-Bench.

15. ***Cornell, Geneva, Together AI, and Princeton on "Mamba in the Llama": <br>This paper discusses distilling large Transformers into linear RNNs like Mamba. The hybrid model created, using a quarter of the original attention layers, matches or exceeds Transformer performance in chat benchmarks. The authors introduce a speculative decoding algorithm, improving the model's inference speed and deployment efficiency.*** <br><br>
    Aug 27, Uni of Cornell, Uni of Geneva, together AI and Uni of Princeton published a [paper](https://arxiv.org/pdf/2408.15237) “The Mamba in the Llama: Distilling and Accelerating Hybrid Models”. Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics. Given the focus on training large-scale Transformer models, the paper considers the challenge of converting these pretrained models for deployment. The paper demonstrates that it is feasible to distill large Transformers into linear RNNs by reusing the linear projection weights from attention layers with academic GPU resources. The resulting hybrid model, which incorporates a quarter of the attention layers, achieves performance comparable to the original Transformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch with trillions of tokens in both chat benchmarks and general benchmarks. Moreover, the work introduces a hardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid models. Overall the paper shows how, with limited computation resources, can remove many of the original attention layers and generate from the resulting model more efficiently. The top-performing model, distilled from Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and 7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.

17. ***Cerebras on "Cerebras Inference": <br>Cerebras introduces a new AI inference solution that delivers significantly faster token processing speeds and lower costs compared to NVIDIA-based solutions. Using its Wafer Scale Engine, Cerebras Inference handles Llama3 models at 20x the speed of GPU solutions, offering a scalable, cost-effective platform for high-speed inference.*** <br><br>
    Aug 27, Cerebras announced its [Cerebras Inference](https://cerebras.ai/blog/introducing-cerebrcas-inference-ai-at-instant-speed), “Introducing Cerebras Inference: AI at Instant Speed”.  Cerebras inference delivers 1,800 tokens per second for Llama3.1 8B and 450 tokens per second for Llama3.1 70B, which is 20x faster than NVIDIA GPU-based hyperscale clouds. Cerebras inference offers the industry’s best pricing at 10c per million tokens for Lama 3.1 8B and 60c per million tokens for Llama 3 70B. Cerebras inference is open to developers today via API access. Powered by the third generation Wafer Scale Engine, Cerebras inference runs Llama3.1 20x faster than GPU solutions at 1/5 the price. At 1,800 tokens/s, Cerebras Inference is 2.4x faster than Groq in Llama3.1-8B. For Llama3.1-70B, Cerebras is the only platform to enable instant responses at a blistering 450 tokens/sec. All this is achieved using native 16-bit weights for the model, ensuring the highest accuracy responses. Cerebras solves the memory bandwidth bottleneck by building the largest chip in the world and storing the entire model on-chip. With the unique wafer-scale design, Cerebrate is able to integrate 44GB of SRAM on a single chip – eliminating the need for external memory and for the slow lanes linking external memory to compute. In total, the WSE-3 has 21 petabytes/s of aggregate memory bandwidth – 7,000x that of an H100. It is the only AI chip with both petabyte-scale compute and petabyte-scale memory bandwidth, making it a near ideal design for high-speed inference.

19. ***Nous Research on "DisTrO": <br>"DisTrO" is a distributed optimizer that drastically reduces inter-GPU communication requirements, enabling large neural network training without high-speed interconnects. This allows for more efficient and scalable training of large models on low-bandwidth networks, matching the performance of existing optimization methods.*** <br><br>
    Aug 26, Nous Research published its [report](https://github.com/NousResearch/DisTrO) “A Preliminary Report on DisTro”. Training large scale neural networks typically involves sharing gradients between all accelerators, which necessitates specialized, high-speed interconnects. To address this, the paper introduces DisTrO, a family of architecture-agnostic and network-agnostic distributed optimizers that reduces the inter-GPU communication requirements by four to five orders of magnitude without relying on amortized analysis, enabling low-latency training of large neural networks on slow internet bandwidths with heterogeneous networking hardware. In this preliminary report the authors are excited to show the first and earliest empirical proof that DisTrO-AdamW matches standard AdamW+All-Reduce in convergence rate while massively reducing the required bandwidth during pre-training of a 1.2B LLM. When using Distributed Data Parallelism, DisTrO may enable future large scale foundation model training to bypass the need for high-speed interconnects entirely.

21. ***University of Hong Kong and collaborators on Transformer Efficiency: <br>The paper presents a method for approximating gradients in multi-layer Transformers with near-linear time complexity. This approach significantly reduces the computational demands of training and inference, making it more efficient to handle large sequences and long-context language models.*** <br><br>
    Aug 23, Uni of HK, Uni of Wisconsin-Madison, Tsinghua Uni, and Adobe published a [paper](https://www.arxiv.org/pdf/2408.13233) “Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time”. The quadratic computational complexity in the self-attention mechanism of popular transformer architectures poses significant challenges for training and inference, particularly in terms of efficiency and memory requirements. Towards addressing these challenges, this paper introduces a novel fast computation method for gradient calculation in multi-layer transformer models. The approach enables the computation of gradients for the entire multi-layer transformer model in almost linear time n^{1+o(1)}, where n is the input sequence length. This breakthrough significantly reduces the computational bottleneck associated with the traditional quadratic time complexity. The theory holds for any loss function and maintains a bounded approximation error across the entire model. Furthermore, the analysis can hold when the multi-layer transformer model contains many practical sub-modules, such as residual connection, casual mask, and multi-head attention. By improving the efficiency of gradient computation in large language models, the authors hope that the work will facilitate the more effective training and deployment of long-context language models based on the theoretical results.

23. ***West Pharmaceutical, Stanford, and Amazon on "RoundTable": <br>"RoundTable" introduces a dynamic schema and contextual autocomplete system to enhance query precision in databases. By leveraging full-text search and suggesting queries based on data in the table, this framework improves LLM accuracy when interacting with complex datasets.*** <br><br>
    Aug 23 West Pharmaceutical Services, Inc., Stanford Uni, and Amazon published a [paper](https://arxiv.org/pdf/2408.12369v1) “RoundTable: Leveraging Dynamic Schema and Contextual Autocomplete for Enhanced Query Precision in Tabular Question Answering”. With advancements in Large Language Models (LLMs), a major use case that has emerged is querying databases in plain English, translating user questions into executable database queries, which has improved significantly. However, real-world datasets often feature a vast array of attributes and complex values, complicating the LLMs task of accurately identifying relevant columns or values from natural language queries. Traditional methods cannot fully relay the datasets size and complexity to the LLM. To address these challenges, the paper proposes a novel framework that leverages Full-Text Search (FTS) on the input table. This approach not only enables precise detection of specific values and columns but also narrows the search space for language models, thereby enhancing query accuracy. Additionally, it supports a custom auto-complete feature that suggests queries based on the data in the table. This integration significantly refines the interaction between the user and complex datasets, offering a sophisticated solution to the limitations faced by current table querying capabilities. This work is accompanied by an application for both Mac and Windows platforms, which readers can try out themselves on their own data.

 <br><br>

***Aug 25 2024***


1. ***"Real-Time Video Generation with Pyramid Attention Broadcast": <br>The paper introduces Pyramid Attention Broadcast (PAB), a real-time, training-free method for DiT-based video generation. PAB addresses redundancy in the diffusion process by broadcasting attention outputs in a pyramid style, applying different strategies based on attention variance. The method enables efficient distributed inference, achieving real-time generation for up to 720p videos and is expected to serve as a robust baseline for future research.*** <br><br>
   Aug 22, National Uni of Singapore, VideoSys and Purdue Uni published a [paper](https://arxiv.org/pdf/2408.12588) “Real-Time Video Generation with Pyramid Attention Broadcast”. The paper presents Pyramid Attention Broadcast (PAB), a real-time, high quality and training-free approach for DiT-based video generation. The method is founded on the observation that attention difference in the diffusion process exhibits a U-shaped pattern, indicating significant redundancy. The authors mitigate this by broadcasting attention outputs to subsequent steps in a pyramid style. It applies different broadcast strategies to each attention based on their variance for best efficiency. The paper further introduces broadcast sequence parallel for more efficient distributed inference. PAB demonstrates superior results across three models compared to baselines, achieving real-time generation for up to 720p videos. The authors anticipate that this simple yet effective method will serve as a robust baseline and facilitate future research and application for video generation. <br><br>

3. ***"Great Memory, Shallow Reasoning: Limits of kNN-LMs": <br>This paper critically evaluates k-nearest neighbor language models (kNN-LMs), which excel in memory-intensive tasks but struggle with reasoning tasks that require integrating multiple information pieces. Even with perfect retrieval, kNN-LMs fail at determining correct answers in complex tasks, suggesting a limit to their reasoning capabilities.*** <br><br>
   Aug 21, Cornell Uni published a [paper](https://arxiv.org/pdf/2408.11815) “Great Memory, Shallow Reasoning: Limits of kNN-LMs”. K-nearest neighbor language models (kNN-LMs), which integrate retrieval with next-word prediction, have demonstrated strong performance in language modeling as well as downstream NLP benchmarks. These results have led researchers to argue that models trained on poor quality or outdated data could perform well by employing a kNN extension that has access to a higher-quality datastore. This work asks whether this improved ability to recall information really translates into downstream abilities. The paper extensively evaluates kNN-LMs on a diverse set of tasks, ranging from sentiment classification and commonsense reasoning to multi-hop reasoning. Results show that kNN-LMs excel at memory-intensive tasks, where utilizing the patterns in the input is sufficient for determining the output, but struggle with reasoning tasks that require integrating multiple pieces of information to derive new knowledge. The paper further demonstrates through oracle experiments and qualitative analysis that even with perfect retrieval, kNN-LMs still fail to determine the correct answers, placing an upper bound on their reasoning performance. Code and datastores are released at [this https URL](https://github.com/GSYfate/knnlm-limits/). <br><br>

5. ***"Anthropic Faces Copyright Lawsuit Over AI Training Data": <br>A lawsuit has been filed against Anthropic by three authors accusing the company of using their copyrighted content without permission to train its Claude AI chatbot. This reflects ongoing legal challenges in the AI industry concerning the use of copyrighted material, raising concerns about copyright infringement and the implications for AI training quality and accuracy.*** <br><br>
   Aug 21, windowscentral.com published an [article](https://www.windowscentral.com/software-apps/openai-ceo-sam-altmans-words-haunt-claude-ai) “OpenAI CEO Sam Altman's words haunt Claude AI: ‘Anthropic’s model seeks to profit from strip-mining the human expression and ingenuity behind each one of those works’”. A lawsuit has been filed against Anthropic by three authors, accusing the company of using their copyrighted content to train its Claude AI chatbot without permission. This follows a pattern of legal challenges in the AI industry, including similar cases against OpenAI and Microsoft. The lawsuit claims that Anthropic's model profits from exploiting human creativity without compensating creators, raising concerns about copyright infringement in AI training. While tech companies argue that using copyrighted content is "fair use," restricting AI from such content could diminish the quality and accuracy of chatbot responses, potentially leading to further issues. OpenAI CEO Sam Altman had previously admitted it's impossible to create ChatGPT-like tools without copyrighted content. <br><br>

7. ***"Xinyu: Efficient LLM-based System for Commentary Generation": <br>The paper introduces Xinyu, a system designed to assist in generating Chinese commentaries by deconstructing the process into sequential steps and using strategies like supervised fine-tuning and retrieval augmented generation. Xinyu significantly increases commentary creation efficiency without compromising quality, reducing the time needed from 4 hours to 20 minutes.*** <br><br>
   Aug 21, Zhejiang Uni etc published a [paper](https://arxiv.org/pdf/2408.11609) on KDD24 “Xinyu: An Efficient LLM-based System for Commentary Generation”. Commentary provides readers with a deep understanding of events by presenting diverse arguments and evidence. However, creating commentary is a time-consuming task, even for skilled commentators. Large language models (LLMs) have simplified the process of natural language generation, but their direct application in commentary creation still faces challenges due to unique task requirements. These requirements can be categorized into two levels: 1) fundamental requirements, which include creating well-structured and logically consistent narratives, and 2) advanced requirements, which involve generating quality arguments and providing convincing evidence. This paper introduces Xinyu, an efficient LLM-based system designed to assist commentators in generating Chinese commentaries. To meet the fundamental requirements, the paper deconstructs the generation process into sequential steps, proposing targeted strategies and supervised fine-tuning (SFT) for each step. To address the advanced requirements, the paper presents an argument ranking model for arguments and establish a comprehensive evidence database that includes up-to-date events and classic books, thereby strengthening the substantiation of the evidence with retrieval augmented generation (RAG) technology. To evaluate the generated commentaries more fairly, corresponding to the two-level requirements, the paper introduces a comprehensive evaluation metric that considers five distinct perspectives in commentary generation. The experiments confirm the effectiveness of the proposed system. The paper also observes a significant increase in the efficiency of commentators in real-world scenarios, with the average time spent on creating a commentary dropping from 4 hours to 20 minutes. Importantly, such an increase in efficiency does not compromise the quality of the commentaries. <br><br>

9. ***"Transfusion: Multi-Modal Model Training with Unified Approach": <br>The paper presents Transfusion, a multi-modal model that combines next token prediction with diffusion to handle both discrete and continuous data. Transfusion models, pre-trained on a mixture of text and image data, show improved scaling and performance, offering benefits for both image and text generation, particularly when scaled to 7B parameters and 2T multi-modal tokens.*** <br><br>
    Aug 20, Meta, Waymo and Uni of Southern California published a [paper](https://www.arxiv.org/pdf/2408.11039) “Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model”. The paper introduces Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences. The authors pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. The experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific encoding and decoding layers, The study can further improve the performance of Transfusion models, and even compress each image to just 16 patches. The authors further demonstrate that scaling the Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds. <br><br>

11. ***"Impact of Code in LLM Pre-training: A Systematic Analysis": This paper investigates the impact of including code data in LLM pre-training, finding that code significantly enhances performance across a broad range of tasks beyond coding. The inclusion of code data results in notable improvements in natural language reasoning, world knowledge, generative win-rates, and code performance, highlighting the value of preserving code in pre-training.*** <br><br>
    Aug 20, Cohere published a [paper](https://arxiv.org/pdf/2408.10914) “To Code, or Not To Code? Exploring Impact of Code in Pre-training”. Including code in the pre-training data mixture, even for models not specifically designed for code, has become a common practice in LLMs pre-training. While there has been anecdotal consensus among practitioners that code data plays a vital role in general LLMs' performance, there is only limited work analyzing the precise impact of code on non-code tasks. This paper systematically investigates the impact of code data on general performance. The research asks "what is the impact of code data used in pre-training on a large variety of downstream tasks beyond code generation". The authors conduct extensive ablations and evaluate across a broad range of natural language reasoning tasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for models with sizes ranging from 470M to 2.8B parameters. Across settings, the paper finds a consistent results that code is a critical building block for generalization far beyond coding tasks and improvements to code quality have an outsized impact across all tasks. In particular, compared to text-only pre-training, the addition of code results in up to relative increase of 8.2% in natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement in generative win-rates, and a 12x boost in code performance respectively. The work suggests investments in code quality and preserving code during pre-training have positive impacts. <br><br>

13. ***"Ferret: Faster and Effective Automated Red Teaming": <br>The paper introduces Ferret, a new method for automated red teaming that builds upon Rainbow Teaming by generating multiple adversarial prompt mutations and ranking them with a reward-based scoring technique. Ferret achieves a 95% attack success rate, improving efficiency and effectiveness in generating adversarial prompts for large language models.*** <br><br>
    Aug 20, Singapore Uni of Tech and Design published a [paper](https://arxiv.org/pdf/2408.10701) “Ferret: Faster and Effective Automated Red Teaming with Reward-Based Scoring Technique”. In today's era, where large language models (LLMs) are integrated into numerous real-world applications, ensuring their safety and robustness is crucial for responsible AI usage. Automated red-teaming methods play a key role in this process by generating adversarial attacks to identify and mitigate potential vulnerabilities in these models. However, existing methods often struggle with slow performance, limited categorical diversity, and high resource demands. While Rainbow Teaming, a recent approach, addresses the diversity challenge by framing adversarial prompt generation as a quality-diversity search, it remains slow and requires a large fine-tuned mutator for optimal performance. To overcome these limitations, the paper proposes Ferret, a novel approach that builds upon Rainbow Teaming by generating multiple adversarial prompt mutations per iteration and using a scoring function to rank and select the most effective adversarial prompt. The authors explore various scoring functions, including reward models, Llama Guard, and LLM-as-a-judge, to rank adversarial mutations based on their potential harm to improve the efficiency of the search for harmful mutations. The results demonstrate that Ferret, utilizing a reward model as a scoring function, improves the overall attack success rate (ASR) to 95%, which is 46% higher than Rainbow Teaming. Additionally, Ferret reduces the time needed to achieve a 90% ASR by 15.2% compared to the baseline and generates adversarial prompts that are transferable i.e. effective on other LLMs of larger size. The codes are available at https://github.com/declare-lab/ferret. <br><br>

15. ***"Challenges with Dataset Construction in Computer Vision": <br>The paper argues that constructing representative image datasets for model testing is statistically implausible, making performance metrics unreliable for real-world deployment. The authors recommend focusing on assessing models' decision-making processes rather than accuracy metrics, as larger datasets or bias-aware datasets do not solve this issue.*** <br><br>
    Aug 20, McGill Uni and York University published a [paper](https://arxiv.org/pdf/2408.11160) “Statistical Challenges with Dataset Construction: Why You Will Never Have Enough Images”.  Deep neural networks have achieved impressive performance on many computer vision benchmarks in recent years. However, can we be confident that impressive performance on benchmarks will translate to strong performance in real-world environments? Many environments in the real world are safety critical, and even slight model failures can be catastrophic. Therefore, it is crucial to test models rigorously before deployment. The authors argue, through both statistical theory and empirical evidence, that selecting representative image datasets for testing a model is likely implausible in many domains. Furthermore, performance statistics calculated with non-representative image datasets are highly unreliable. As a consequence, there is no guarantee that models which perform well on withheld test images will also perform well in the real world. Creating larger and larger datasets will not help, and bias aware datasets cannot solve this problem either. Ultimately, there is little statistical foundation for evaluating models using withheld test sets. The paper recommends that future evaluation methodologies focus on assessing a model's decision-making process, rather than metrics such as accuracy. <br><br>

17. ***"Plan-based Retrieval for Grounded Text Generation": <br>This study examines how planning can guide retrieval to reduce hallucinations in text generation. By improving the coverage of relevant facts, the proposed plan-guided retrieval approach enhances the informativeness and attribution of generated responses, offering a promising method to mitigate hallucinations in language models.*** <br><br>
    Aug 20, Uni of Southern California and Google published a [paper](https://arxiv.org/pdf/2408.10490) “Analysis of Plan-based Retrieval for Grounded Text Generation”. In text generation, hallucinations refer to the generation of seemingly coherent text that contradicts established knowledge. One compelling hypothesis is that hallucinations occur when a language model is given a generation task outside its parametric knowledge (due to rarity, recency, domain, etc.). A common strategy to address this limitation is to infuse the language models with retrieval mechanisms, providing the model with relevant knowledge for the task. This paper leverages the planning capabilities of instruction-tuned LLMs and analyzes how planning can be used to guide retrieval to further reduce the frequency of hallucinations. The authors empirically evaluate several variations of the proposed approach on long-form text generation tasks. By improving the coverage of relevant facts, plan-guided retrieval and generation can produce more informative responses while providing a higher rate of attribution to source documents. <br><br>

19. ***"KAN 2.0: Bridging AI and Science with Kolmogorov-Arnold Networks": <br>The paper introduces a framework to integrate Kolmogorov-Arnold Networks (KANs) with scientific discovery, enhancing their ability to identify features, modular structures, and symbolic formulas. The framework allows for a bidirectional synergy between KANs and scientific knowledge, demonstrating KANs' potential in discovering various physical laws.*** <br><br>
    Aug 19, MIT, California Inst of Tech and NSF published a [paper](https://arxiv.org/pdf/2408.10205) “KAN 2.0: Kolmogorov-Arnold Networks Meet Science”. A major challenge of AI + Science lies in their inherent incompatibility: today's AI is primarily based on connectionism, while science depends on symbolism. To bridge the two worlds, the paper proposes a framework to seamlessly synergize Kolmogorov-Arnold Networks (KANs) and science. The framework highlights KANs' usage for three aspects of scientific discovery: identifying relevant features, revealing modular structures, and discovering symbolic formulas. The synergy is bidirectional: science to KAN (incorporating scientific knowledge into KANs), and KAN to science (extracting scientific insights from KANs). The authors highlight major new functionalities in the pykan package: (1) MultKAN: KANs with multiplication nodes. (2) kanpiler: a KAN compiler that compiles symbolic formulas into KANs. (3) tree converter: convert KANs (or any neural networks) to tree graphs. Based on these tools, the authors demonstrate KANs' capability to discover various types of physical laws, including conserved quantities, Lagrangians, symmetries, and constitutive laws.<br><br>

21. ***"Generative AI Hype and Its Practical Evolution": <br>The article discusses the gradual evolution of generative AI technology, emphasizing the shift from hype to practical utility. It highlights the emergence of smaller, more efficient models, the increasing focus on AI literacy, and the continuous improvement of generative AI, suggesting that AI will gradually transform human activities rather than replace them.*** <br><br>
    Aug 19, TheConversation.com published an [article](https://theconversation.com/generative-ai-hype-is-ending-and-now-the-technology-might-actually-become-useful-236940) “Generative AI hype is ending – and now the technology might actually become useful”. Large language models such as GPT-4 do not always match what people expect of them, such as the [experimental results](https://arxiv.org/pdf/2406.01382) from Harvard Uni etc. Experience from successful projects shows it is also tough to make a generative model follow instructions. However, GenAI hype isn’t over yet because first generative AI technology, despite its challenges, is rapidly improving, with scale and size being the primary drivers of the improvement. Second, studies have found sufficiently complex large language models can develop the ability to reason by analogy and even reproduce optical illusions like those experienced by humans. Next, 1) AI is being used to support humans, rather than replace them. 2) we also see a rise in [smaller (and cheaper) generative AI models](https://www.bloomberg.com/news/articles/2024-08-08/move-over-llms-small-ai-models-are-the-next-big-thing), trained on specific data and deployed locally to reduce costs and optimise efficiency. Even OpenAI, which has led the race for ever-larger models, has released the GPT-4o Mini model to reduce costs and improve performance. 3) we see a strong focus on providing AI literacy training and educating the workforce on how AI works, its potentials and limitations, and best practices for ethical AI use. Therefore, the AI revolution will look more like an evolution. Its use will gradually grow over time and, little by little, alter and transform human activities. Which is much better than replacing them. <br><br>

23. ***"IDEA: Enhancing Rule Learning in Interactive Environments": <br>The paper introduces IDEA, an agent designed to improve rule-learning abilities in large language models through induction, deduction, and abduction processes. Evaluated on the RULEARN benchmark, IDEA shows improved performance in interactive settings, offering insights for developing agents capable of human-like rule learning.*** <br><br>
    Aug 19, Uni of Texas at Dallas published a [paper](https://arxiv.org/pdf/2408.10455) “IDEA: Enhancing the rule learning ability of language agent through Induction, DEuction, and Abduction”. While large language models (LLMs) have been thoroughly evaluated for deductive and inductive reasoning, their proficiency in abductive reasoning and holistic rule learning in interactive environments remains less explored. This work introduces RULEARN, a novel benchmark specifically designed to assess the rule-learning ability of LLMs in interactive settings. In RULEARN, agents interact with the environment to gather observations and discern patterns, using these insights to solve problems. To further enhance the rule-learning capabilities of LLM agents within this benchmark, the paper proposes IDEA agent, which integrates Induction, Deduction, and Abduction processes. IDEA agent refines this approach by leveraging a structured reasoning sequence: generating hypotheses through abduction, testing them via deduction, and refining them based on induction feedback. This sequence enables agents to dynamically establish and apply rules, mimicking human-like reasoning processes. The evaluation of five representative LLMs indicates that while these models can generate plausible initial hypotheses, they often struggle with strategic interaction within the environment, effective incorporation of feedback, and adaptive refinement of their hypotheses. IDEA agent demonstrates significantly improved performance on the RULEARN benchmark, offering valuable insights for the development of agents capable of human-like rule-learning in real-world scenarios. We will release our code and data. <br><br>

25. ***"In-Context Learning with Transformers: A Theoretical Analysis": <br>This paper explores the training dynamics of transformers in in-context learning (ICL), showing that transformers can generalize to unseen examples by learning template functions in-context. The study provides a theoretical foundation for understanding ICL and demonstrates that transformers can effectively perform ridge regression over basis functions.*** <br><br>
    Aug 19, CMU, Upenn et al. published a [paper](https://arxiv.org/pdf/2408.10147) “In-Context Learning with Representations: Contextual Generalization of Trained Transformers”. In-context learning (ICL) refers to a remarkable capability of pretrained large language models, which can learn a new task given a few examples during inference. However, theoretical understanding of ICL is largely under-explored, particularly whether transformers can be trained to generalize to unseen examples in a prompt, which will require the model to acquire contextual knowledge of the prompt for generalization. This paper investigates the training dynamics of transformers by gradient descent through the lens of non-linear regression tasks. The contextual generalization here can be attained via learning the template function for each task in-context, where all template functions lie in a linear space with m basis functions. The paper analyzes the training dynamics of one-layer multi-head transformers to in-contextly predict unlabeled inputs given partially labeled prompts, where the labels contain Gaussian noise and the number of examples in each prompt are not sufficient to determine the template. Under mild assumptions, the paper shows that the training loss for a one-layer multi-head transformer converges linearly to a global minimum. Moreover, the transformer effectively learns to perform ridge regression over the basis functions. To the authors’ knowledge, this study is the first provable demonstration that transformers can learn contextual (i.e., template) information to generalize to both unseen examples and tasks when prompts contain only a small number of query-answer pairs. <br><br>

27. ***"xGen-MM (BLIP-3): Open Large Multimodal Models Framework": <br>The report introduces xGen-MM, a framework for developing large multimodal models that expands on Salesforce's xGen initiative. The models show strong in-context learning capabilities, competitive performance, and improved safety, with all resources made publicly available to advance research in multimodal models.*** <br><br>
    Aug 16, Salesforce and Uni of Washington published a [paper](https://arxiv.org/pdf/2408.08872) “xGen-MM (BLIP-3): A Family of Open Large Multimodal Models”. This report introduces xGen-MM (also known as BLIP-3), a framework for developing Large Multimodal Models (LMMs). The framework comprises meticulously curated datasets, a training recipe, model architectures, and a resulting suite of LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen initiative on foundation AI models. The models undergo rigorous evaluation across a range of tasks, including both single and multi-image benchmarks. The pre-trained base model exhibits strong in-context learning capabilities and the instruction-tuned model demonstrates competitive performance among open-source LMMs with similar model sizes. In addition, the paper introduces a safety-tuned model with DPO, aiming to mitigate harmful behaviors such as hallucinations and improve safety. The authors open source the models, curated large-scale datasets, and the fine-tuning codebase to facilitate further advancements in LMM research. Associated resources will be available on the project page. <br><br>

29. ***"Cybench: Evaluating Cybersecurity Capabilities of Language Models": <br>The paper presents Cybench, a framework for evaluating language models' cybersecurity capabilities through professional-level Capture the Flag (CTF) tasks. The evaluation reveals that current models struggle with complex tasks, but subtasks improve performance measurement, highlighting the need for further research in cybersecurity-focused language models.*** <br><br>
    Aug 15, Stanford Uni published a [paper](https://arxiv.org/pdf/2408.08926) “Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk of Language Models”. Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have the potential to cause real-world impact. Policymakers, model providers, and other researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration testing. Toward that end, the paper introduces Cybench, a framework for specifying cybersecurity tasks and evaluating agents on those tasks. The authors include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute bash commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, the paper introduces subtasks, which break down a task into intermediary steps for more gradated evaluation; adds subtasks for 17 of the 40 tasks. To evaluate agent capabilities, the authors construct a cybersecurity agent and evaluate 7 models: GPT-4o, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. Without guidance, the research finds that agents are able to solve only the easiest complete tasks that took human teams up to 11 minutes to solve, with Claude 3.5 Sonnet and GPT-4o having the highest success rates. Finally, subtasks provide more signal for measuring performance compared to unguided runs, with models achieving a 3.2\% higher success rate on complete tasks with subtask-guidance than without subtask-guidance. All code and data are publicly available at https://cybench.github.io <br><br>

31. ***"Automated Design of Agentic Systems (ADAS): A New Research Area": <br>The paper introduces ADAS, a research area focused on automatically creating powerful agentic systems by combining novel building blocks and programming agents in code. The Meta Agent Search algorithm demonstrates the potential for automatically designing high-performing agents, offering a new direction for AI research and development.*** <br><br>
    Aug 15, Uni of British Columbia, Vector Inst. And CIFAR AI published a [paper](https://arxiv.org/pdf/2408.08435) “Automated Design of Agentic Systems”. Researchers are investing substantial effort in developing powerful general-purpose agents, wherein Foundation Models are used as modules within agentic systems (e.g. Chain-of-Thought, Self-Reflection, Toolformer). However, the history of machine learning teaches us that hand-designed solutions are eventually replaced by learned solutions. The research formulates a new research area, Automated Design of Agentic Systems (ADAS), which aims to automatically create powerful agentic system designs, including inventing novel building blocks and/or combining them in new ways. The work further demonstrates that there is an unexplored yet promising approach within ADAS where agents can be defined in code and new agents can be automatically discovered by a meta agent programming ever better ones in code. Given that programming languages are Turing Complete, this approach theoretically enables the learning of any possible agentic system: including novel prompts, tool use, control flows, and combinations thereof. The paper presents a simple yet effective algorithm named Meta Agent Search to demonstrate this idea, where a meta agent iteratively programs interesting new agents based on an ever-growing archive of previous discoveries. Through extensive experiments across multiple domains including coding, science, and math, the research shows that the proposed algorithm can progressively invent agents with novel designs that greatly outperform state-of-the-art hand-designed agents. Importantly, the authors consistently observe the surprising result that agents invented by Meta Agent Search maintain superior performance even when transferred across domains and models, demonstrating their robustness and generality. Provided the authors develop it safely, the work illustrates the potential of an exciting new research direction toward automatically designing ever-more powerful agentic systems to benefit humanity.
<br><br><br>

***Aug 18 2024***

1. ***ACL 2024 Best Paper Awards: <br>On August 15, ACL 2024 announced the recipients of its best paper awards, recognizing various research contributions including topics such as SMS spam detection, language model explainability, and causal estimation. Notable awards include the Test of Time Award for "GloVe: Global Vectors for Word Representation" and the Best Social Impact Paper Award, which highlighted research on AI safety and cultural bias in large language models.***<br><br>
   Aug 15, ACL 2024 announced best paper awards. <br>***The best paper*** include: 1) [ExplainableDetector](https://arxiv.org/abs/2405.08026): Exploring Transformer-based Language Modeling Approach for SMS Spam Detection with Explainability Analysis, 2) [Deciphering Oracle Bone Language](https://arxiv.org/abs/2406.00684) with Diffusion Models, 3) [Causal Estimation of Memorisation Profiles](https://arxiv.org/abs/2406.04327), 4) [Aya Model](https://arxiv.org/abs/2402.07827): An Instruction Finetuned Open-Access Multilingual Language Model, 5) [Mission](https://arxiv.org/abs/2401.06416): Impossible Language Models, 6) [Semisupervised Neural Proto-Language Reconstructior](https://arxiv.org/abs/2406.05930), 7) [Why are Sensitive Functions Hard for Transformers](https://arxiv.org/abs/2402.09963). <br>***Test of time Award paper*** is “[GloVe](https://aclanthology.org/D14-1162.pdf): Global Vectors for Word Representation”. <br>***Best Social Impact Paper Award papers***: 1) [How Johnny Can Persuade LLMs to Jailbreak Them](https://arxiv.org/abs/2401.06373): Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs, 2) [DIALECTBENCH](https://arxiv.org/abs/2403.11009): A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages, 3) [Having Beer after Prayer](https://arxiv.org/abs/2305.14456)? Measuring Cultural Bias in Large Language Models. <br>***Theme Paper Award paper*** is [OLMo](https://arxiv.org/abs/2402.00838): Accelerating the Science of Language Models. <br>***The new Open science, open data, and open models for reproducible NLP research award papers*** are 1) [AppWorld](https://arxiv.org/abs/2407.18901): A Controllable World of Apps and People for Benchmarking Interactive Coding Agents, 2) [Latxa](https://arxiv.org/abs/2403.20266): An Open Language Model and Evaluation Suite for Basque, 3) [Dolma](https://arxiv.org/abs/2402.00159): an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. <br><br>

3. ***Google's Visual Memory Innovation: <br>Google published a paper "Towards flexible perception with visual memory," proposing a novel approach to neural network training. Instead of the traditional monolithic model, this method combines the power of deep learning with a flexible database structure. It allows for scalable data integration, data removal, and interpretable decision-making, challenging the idea that neural networks must be static and unchangeable once trained.***<br>

   Aug 15, Google published a [paper](https://arxiv.org/pdf/2408.08172) “Towards flexible perception with visual memory”.  Training a neural network is a monolithic endeavor, akin to carving knowledge into stone: once the process is completed, editing the knowledge in a network is nearly impossible, since all information is distributed across the network's weights. The paper explores a simple, compelling alternative by marrying the representational power of deep neural networks with the flexibility of a database. Decomposing the task of image classification into image similarity (from a pre-trained embedding) and search (via fast nearest neighbor retrieval from a knowledge database), the work builds a simple and flexible visual memory that has the following key capabilities: (1.) The ability to flexibly add data across scales: from individual samples all the way to entire classes and billion-scale data; (2.) The ability to remove data through unlearning and memory pruning; (3.) An interpretable decision-mechanism on which the authors can intervene to control its behavior. Taken together, these capabilities comprehensively demonstrate the benefits of an explicit visual memory. The authors hope that it might contribute to a conversation on how knowledge should be represented in deep vision models -- beyond carving it in “stone” weights.<br><br>

5. ***Study on Language Models and Hallucinations: <br>Google's paper explores how training language models (LMs) on knowledge graphs affects their hallucination tendencies. The research reveals that larger, longer-trained LMs hallucinate less, but controlling hallucinations at scale is costly. Interestingly, as models grow, hallucinations become harder to detect, indicating a complex relationship between model size and performance.***<br>

   Aug 14, Google published a [paper](https://arxiv.org/pdf/2408.07852v1) “Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability”. While many capabilities of language models (LMs) improve with increased training budget, the influence of scale on hallucinations is not yet fully understood. Hallucinations come in many forms, and there is no universally accepted definition. The study thus focuses on studying only those hallucinations where a correct answer appears verbatim in the training set. To fully control the training data content, the authors construct a knowledge graph (KG)-based dataset, and use it to train a set of increasingly large LMs. The study finds that for a fixed dataset, larger and longer-trained LMs hallucinate less. However, hallucinating on <= 5% of the training data requires an order of magnitude larger model, and thus an order of magnitude more compute, than Hoffmann et al. (2022) reported was optimal. Given this costliness, the authors study how hallucination detectors depend on scale. While the study sees detector size improves performance on fixed LM's outputs, it is found an inverse relationship between the scale of the LM and the detectability of its hallucinations.<br><br>

7. ***MIT's AI Risk Repository: <br>MIT's AI Risk Repository is a comprehensive database and taxonomy of 777 AI risks, aimed at unifying the understanding and management of AI-related dangers. It classifies risks by causal factors and domains, providing a detailed framework to help policymakers, researchers, and developers navigate the complex landscape of AI safety.***<br>

   Aug 14, MIT release an [AI Risk Repository](https://airisk.mit.edu/) with a [paper](https://cdn.prod.website-files.com/669550d38372f33552d2516e/66bc918b580467717e194940_The%20AI%20Risk%20Repository_13_8_2024.pdf) “The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence”. The risks posed by Artificial Intelligence (AI) are of considerable concern to academics, auditors, policymakers, AI companies, and the public. However, a lack of shared understanding of AI risks can impede our ability to comprehensively discuss, research, and react to them. This paper addresses this gap by creating an AI Risk Repository to serve as a common frame of reference. This comprises a living database of 777 risks extracted from 43 taxonomies, which can be filtered based on two overarching taxonomies and easily accessed, modified, and updated via the authors’ [website](https://airisk.mit.edu/) and [online spreadsheets](https://docs.google.com/spreadsheets/d/1evwjF4XmpykycpeZFq0FUteEAt7awx2i2oE6kMrV_xE/copy). The authors construct a Repository with a systematic review of taxonomies and other structured classifications of AI risk followed by an expert consultation. The study develops the taxonomies of AI risk using a best-fit framework synthesis. The high-level Causal Taxonomy of AI Risks classifies each risk by its causal factors (1) Entity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3) Timing: Pre-deployment; Post-deployment. The mid-level Domain Taxonomy of AI Risks classifies risks into seven AI risk domains: (1) Discrimination & toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors & misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and (7) AI system safety, failures, & limitations. These are further divided into 23 subdomains. The AI Risk Repository is, to authors’ knowledge, the first attempt to rigorously curate, analyze, and extract AI risk frameworks into a publicly accessible, comprehensive, extensible, and categorized risk database. This creates a foundation for a more coordinated, coherent, and complete approach to defining, auditing, and managing the risks posed by AI systems.<br><br>

9. ***Advancements in Text-to-SQL Pipelines: <br>A paper by Distyl AI and Polytechnique Montreal questioned the necessity of schema linking in Text-to-SQL pipelines. The research found that modern language models can bypass this step, directly identifying relevant schema elements, which improves accuracy and simplifies the pipeline.***<br>

    Aug 14, Distyl AI and Polytechnique Montreal published a [paper](https://www.arxiv.org/pdf/2408.07702) “The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models”. Schema linking is a crucial step in Text-to-SQL pipelines, which translate natural language queries into SQL. The goal of schema linking is to retrieve relevant tables and columns (signal) while disregarding irrelevant ones (noise). However, imperfect schema linking can often exclude essential columns needed for accurate query generation. This work revisits the need for schema linking when using the latest generation of large language models (LLMs). The work finds empirically that newer models are adept at identifying relevant schema elements during generation, without the need for explicit schema linking. This allows Text-to-SQL pipelines to bypass schema linking entirely and instead pass the full database schema to the LLM, eliminating the risk of excluding necessary information. Furthermore, as alternatives to schema linking, the authors propose techniques that improve Text-to-SQL accuracy without compromising on essential schema information. This approach achieves 71.83\% execution accuracy on the BIRD benchmark, ranking first at the time of submission.<br><br>

11. ***Agent Q: Autonomous AI Advances: <br>A paper by MultiOn and Stanford introduces "Agent Q," a framework that significantly enhances the reasoning capabilities of large language models in dynamic environments. This method integrates guided Monte Carlo Tree Search with a self-critique mechanism, enabling AI agents to improve their performance in tasks like web navigation and real-world booking scenarios.***<br>

    Aug 13, MultiOn and Stanford Uni published a [paper](https://arxiv.org/pdf/2408.07199) “Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents”. Large Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex reasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a difficult challenge. Traditional supervised pre-training on static datasets falls short in enabling autonomous agent capabilities needed to perform complex decision-making in dynamic settings like web navigation. Previous attempts to bridge this gap through supervised fine-tuning on curated expert demonstrations often suffer from compounding errors and limited exploration data, resulting in sub-optimal policy outcomes. To overcome these challenges, the work proposes a framework that combines guided Monte Carlo Tree Search (MCTS) search with a self-critique mechanism and iterative fine-tuning on agent interactions using an off-policy variant of the Direct Preference Optimization (DPO) algorithm. The method allows LLM agents to learn effectively from both successful and unsuccessful trajectories, thereby improving their generalization in complex, multi-step reasoning tasks. The authors validate the approach in the WebShop environment, a simulated e-commerce platform—where it consistently outperforms behavior cloning and reinforced fine-tuning baseline, and beats average human performance when equipped with the capability to do online search. In real-world booking scenarios, the methodology boosts Llama-3 70B model’s zero-shot performance from 18.6% to 81.7% success rate (a 340% relative increase) after a single day of data collection and further to 95.4% with online search. The authors believe this represents a substantial leap forward in the capabilities of autonomous agents, paving the way for more sophisticated and reliable decision-making in real-world settings.<br><br>

13. ***xAI's Grok-2 Model Preview: <br>xAI released an early preview of Grok-2, a new AI model with enhanced capabilities in chat, coding, and reasoning. Grok-2 outperforms several leading models on various benchmarks and excels in tasks like visual math reasoning and document-based question answering.***<br>

    Aug 13, [xAI released Grok-2](https://x.ai/blog/grok-2), an early preview of Grok-2, a significant step forward from its previous model Grok-1.5, featuring frontier capabilities in chat, coding, and reasoning. At the same time, xAI is introducing Grok-2 mini, a small but capable sibling of Grok-2. An early version of Grok-2 has been tested on the LMSYS leaderboard under the name "sus-column-r." At the time of this blog post, it is outperforming both Claude 3.5 Sonnet and GPT-4-Turbo. Internally, xAI employs a comparable process to evaluate the models. xAI’s AI Tutors engage with the models across a variety of tasks that reflect real-world interactions with Grok. During each interaction, the AI Tutors are presented with two responses generated by Grok. They select the superior response based on specific criteria outlined in the guidelines. xAI focused on evaluating model capabilities in two key areas: following instructions and providing accurate, factual information. Grok-2 has shown significant improvements in reasoning with retrieved content and in its tool use capabilities, such as correctly identifying missing information, reasoning through sequences of events, and discarding irrelevant posts. Grok-2 achieves performance levels competitive to other frontier models in areas such as graduate-level science knowledge (GPQA), general knowledge (MMLU, MMLU-Pro), and math competition problems (MATH). Additionally, Grok-2 excels in vision-based tasks, delivering state-of-the-art performance in visual math reasoning (MathVista) and in document-based question answering (DocVQA).<br><br>

15. ***Google's Imagen 3 Release: <br>Google introduced Imagen 3, a latent diffusion model that generates high-quality images from text prompts. While the model is preferred over other state-of-the-art models in terms of quality, the release includes discussions on safety and responsibility, although detailed information and model weights were not disclosed.***<br>

    Aug 13, Google published a [paper](https://arxiv.org/pdf/2408.07009) “Imagen 3” to introduce Imagen 3, a latent diffusion model that generates high quality images from text prompts. The paper describes the quality and responsibility evaluations. Imagen 3 is preferred over other state-of-the-art (SOTA) models at the time of evaluation. In addition, the paper discusses issues around safety and representation, as well as methods we used to minimize the potential harm of our models. But it looks like no much details are released in the paper, and model weights are not released neither.<br><br>

17. ***Evolution of AI Agents: <br>An article by venturebeat discusses the shift from AI assistants to proactive, autonomous agents. These agents are capable of making independent decisions and handling complex tasks, signaling a significant evolution in AI. While promising, the article notes that the infrastructure to support these agents is still under development, and organizations must address challenges like data quality and trust.***<br>

    Aug 13, [venturebeat.com](https://venturebeat.com/ai/beyond-assistants-ai-agents-are-transforming-the-paradigm/) published an article “Beyond assistants: AI agents are transforming the paradigm”. The article discusses the evolution of AI from reactive assistants to proactive, autonomous agents. By 2028, a significant portion of human interactions with AI is expected to shift from prompting large language models (LLMs) to interfacing with intent-driven agents. Unlike AI assistants that respond to user requests, these agents can make decisions and act independently, handling complex tasks in real time. This shift is seen as the next step in generative AI, with major tech companies like Google, Microsoft, and AWS already developing such agents. AI agents are compared to specialized employees who collaborate to solve business problems. They are already showing success in areas like customer service and marketing, and their use is expected to expand across various industries. However, the infrastructure needed to support these agents, such as a system for seamless communication and coordination between them, is still in development. Organizations looking to adopt AI agents must first overcome challenges associated with generative AI, such as data quality and managing trust and security issues. Starting with simple, well-defined use cases and focusing on data hygiene are recommended steps. As LLMs improve and more industries adopt AI agents, the benefits of this technology are expected to become more widespread, positioning organizations to fully leverage the potential of agentic AI.<br><br>

19. ***AI Scientist Framework for Autonomous Research: <br>A paper by Sakana AI and partners introduces "The AI Scientist," a framework for fully automated scientific discovery. The AI can generate research ideas, conduct experiments, and write scientific papers, potentially transforming the research process by reducing costs and increasing creativity.***<br>

    Aug 12, Sakana AI, FLAIR, Uni of Oxford, Uni of British Columbia et al. published a [paper](https://arxiv.org/pdf/2408.06292) “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery”. One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aids to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. The paper introduces The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. The work demonstrates its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than $15 per paper. To evaluate the generated papers, the authors design and validate an automated reviewer, which the authors show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by the automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking people closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. The code is open-sourced at https://github.com/SakanaAI/AI-Scientist<br><br>

21. ***Falcon Mamba 7B: A Revolutionary SSLM: <br>TII released Falcon Mamba 7B, the first open-source State Space Language Model (SSLM), noted for its low memory cost and superior performance over traditional models like Meta's Llama 3.1. This model reflects Abu Dhabi's innovation in AI research and is expected to be included in future Hugging Face releases.***<br>

    Aug 12, TII released first SSLM with [Falcon Mamba 7B](https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html) model. Falcon Mamba 7B is the first open source released State Space Language Model (SSLM), a new revolutionary architecture for Falcon models. Falcon Mamba 7B is the no. 1 globally performing open source SSLM in the world, as independently verified by Hugging Face. SSLMs have a low memory cost and don’t require additional memory to generate arbitrary long blocks of text. Falcon Mamba 7B also outperforms traditional transformer architecture models such as Meta’s Llama 3.1 8B and Mistral’s 7B. New model reflects the innovation and pioneering approach of Abu Dhabi in AI research and development. Falcon Mamba was trained with ~ 5500GT of data, mainly composed of RefinedWeb data with addition of high-quality technical data and code data from public sources. TII used constant learning rate for the most of the training, followed by a relatively short learning rate decay stage. In this last stage, TII also added a small portion of high-quality curated data to further enhance model performance. The Falcon Mamba architecture will be available in the next release of the Hugging Face transformers library (>4.45.0).<br><br>

23. ***Mutual Reasoning in Small Language Models: <br>A paper by Microsoft and Harvard introduces rStar, a method that enhances reasoning in small language models through mutual reasoning and self-play. This approach significantly improves problem-solving accuracy across various benchmarks, making smaller models more effective without requiring extensive fine-tuning.***<br>

    Aug 12, Microsoft and Harvard Uni published a [paper](https://arxiv.org/pdf/2408.06195) “Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers”. This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will be available at https://github.com/zhentingqi/rStar.<br><br>

25. ***Debunking Emergent Abilities in LLMs: <br>A paper challenges the notion that large language models develop complex intelligent behaviors, attributing their performance to in-context learning rather than true emergent abilities. The study emphasizes that despite improvements, LLMs still require explicit instructions to perform tasks effectively.***<br>

    Aug 12, [techxplore.com](https://techxplore.com/news/2024-08-emergent-abilities-large-language-context.html) published an article introduced an ACL 2024 [paper](https://arxiv.org/pdf/2309.01809) “Are Emergent Abilities in Large Language Models just In-Context Learning?”.  According to the study, there is no evidence that what are known as large language models (LLMs) are beginning to develop a general "intelligent" behavior that would enable them to proceed in a planned or intuitive manner or to think in a complex way. The research focuses on unforeseen and sudden leaps in the performance of language models, which are referred to as "emergent abilities." After the models were introduced, scientists found that they became more powerful with increasing size and the growing amount of data with which they were trained (scaling). On the one hand, this raised hopes that further scaling would make the models even better. On the other hand, there was also concern that these abilities could become dangerous, as the LLMs could become independent and possibly escape human control. In response, AI laws were introduced worldwide, including in the European Union and the U.S.. However, the authors of the current study have now come to the conclusion that there is no evidence for the presumed development of differentiated thinking in the models. Instead, the LLMs acquired the superficial skill of following relatively simple instructions, as the researchers showed. The systems are still a long way from what humans are capable of. Users should explicitly state what the systems should do and, if possible, give examples. The important thing is: The tendency of these models to produce plausible-sounding but false results—known as confabulation—is likely to persist, even if the quality of the models has improved dramatically in recent times.<br><br>

27. ***AI's Impact on IT Jobs: <br>According to a report, 92% of IT jobs will undergo transformation due to AI, with mid- and low-level positions being most affected. The report stresses the growing importance of skills like AI literacy and rapid engineering, as traditional roles like data management and basic programming become less relevant.***<br>

    Aug 12, according to [cio.com](https://www.cio.com/article/3485322/92-of-it-jobs-will-be-transformed-by-ai.html), 92% of IT jobs will be transformed by AI, and the biggest change will be experienced by mid- and low-level positions, as manual tasks become less relevant or easily replaceable by this technology. To get a better idea how AI will change the labor market for technology professionals, the recently formed AI-Enabled ICT Workforce Consortium has published its inaugural report, “The Transformational Opportunity of AI on ICT Jobs,” which reveals that 92% of IT jobs will see a high or moderate transformation due to advances in AI. The study argues that the biggest changes will be seen in mid-level (40%) and entry-level (37%) technology jobs, as certain skills and capabilities become more or less relevant. AI ethics, responsible AI, rapid engineering, AI literacy, and large language model (LLM) architecture are expected to rise in importance in this new era, while traditional data management, content creation, documentation maintenance, basic programming and languages, and research information will become less relevant. That’s why, the report says, critical skills are needed in all IT jobs, including AI literacy, data analytics, and rapid engineering. That’s why the consortium is seeking to empower workers to reskill and upskill.<br><br>

29. ***Meta's UniBench for Vision-Language Models: <br>Meta introduced UniBench, a unified benchmark for evaluating vision-language models across 50+ tasks. The study reveals that while scaling models improves some capabilities, it falls short in areas like reasoning and counting, suggesting that more targeted interventions are needed for VLM progress.***<br>

    Aug 9, Meta published a [paper](https://arxiv.org/pdf/2408.04810) “UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling”. Significant research efforts have been made to scale and improve vision-language model (VLM) training approaches. Yet, with an ever-growing number of benchmarks, researchers are tasked with the heavy burden of implementing each protocol, bearing a non-trivial computational cost, and making sense of how all these benchmarks translate into meaningful axes of progress. To facilitate a systematic evaluation of VLM progress, the authors introduce UniBench: a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more. The authors showcase the utility of UniBench for measuring progress by evaluating nearly 60 publicly available vision-language models, trained on scales of up to 12.8B samples. While scaling training data or model size can boost many vision-language model capabilities, scaling offers little benefit for reasoning or relations. Surprisingly, the authors also discover today's best VLMs struggle on simple digit recognition and counting tasks, e.g. MNIST, which much simpler networks can solve. Where scale falls short, the work finds that more precise interventions, such as data quality or tailored-learning objectives offer more promise. For practitioners, the authors also offer guidance on selecting a suitable VLM for a given application. Finally, the authors release an easy-to-run UniBench [code-base](https://github.com/facebookresearch/unibench) with the full set of 50+ benchmarks and comparisons across 59 models as well as a distilled, representative set of benchmarks that runs in 5 minutes on a single GPU.<br><br>

31. ***Elon Musk vs. OpenAI Lawsuit: <br>Elon Musk filed a new lawsuit against OpenAI and its CEO, Sam Altman, accusing them of betraying the company's original non-profit mission. The lawsuit alleges manipulation and violation of agreements, highlighting the deteriorating relationship between Musk and Altman since the founding of OpenAI in 2015.***<br>

    Aug 6, according to [theguardian.com](https://www.theguardian.com/technology/article/2024/aug/05/elon-musk-openai-lawsuit?utm_source=substack&utm_medium=email), Elon Musk has filed a new lawsuit against OpenAI and its CEO, Sam Altman, alleging they manipulated him into co-founding the company and betrayed its original non-profit mission by turning it into a for-profit entity. This lawsuit follows a similar one Musk filed earlier this year but later withdrew. Musk's complaint accuses Altman and other co-founders of deceiving him and violating the "founding agreement" meant to prioritize the betterment of humanity. OpenAI denies the allegations, arguing that Musk supported the shift to a for-profit model and is motivated by jealousy. The new lawsuit also includes accusations of federal racketeering and wire fraud against Altman and his associates. The legal battle underscores the deteriorating relationship between Musk and Altman, who co-founded OpenAI in 2015 but later parted ways due to an internal power struggle. Musk has since founded his own AI company, xAI, which has struggled to match the success of OpenAI’s ChatGPT.<br><br>

33. ***The Consistent Reasoning Paradox (CRP): <br>Researchers introduced the Consistent Reasoning Paradox (CRP), which suggests that consistent reasoning, a core component of human intelligence, implies fallibility. The paradox asserts that an AI striving to mimic human intelligence through consistent reasoning will inevitably produce incorrect but plausible answers, especially in basic arithmetic. This paradox reveals that a trustworthy AI, which reasons consistently and never answers incorrectly, must be capable of acknowledging uncertainty by saying "I don't know.", a function, which current AI lacks.***<br>

    Aug 5, King’s College London, Uni of Cambridge and Simon Fraser Uni published a [paper](https://arxiv.org/pdf/2408.02357) “On the consistent reasoning paradox of intelligence and optimal trust in AI: The power of 'I don't know'”. The paper introduces the Consistent Reasoning Paradox (CRP). Consistent reasoning, which lies at the core of human intelligence, is the ability to handle tasks that are equivalent, yet described by different sentences ('Tell me the time!' and 'What is the time?'). The CRP asserts that consistent reasoning implies fallibility -- in particular, human-like intelligence in AI necessarily comes with human-like fallibility. Specifically, it states that there are problems, e.g. in basic arithmetic, where any AI that always answers and strives to mimic human intelligence by reasoning consistently will hallucinate (produce wrong, yet plausible answers) infinitely often. The paradox is that there exists a non-consistently reasoning AI (which therefore cannot be on the level of human intelligence) that will be correct on the same set of problems. The CRP also shows that detecting these hallucinations, even in a probabilistic sense, is strictly harder than solving the original problems, and that there are problems that an AI may answer correctly, but it cannot provide a correct logical explanation for how it arrived at the answer. Therefore, the CRP implies that any trustworthy AI (i.e., an AI that never answers incorrectly) that also reasons consistently must be able to say 'I don't know'. Moreover, this can only be done by implicitly computing a new concept that the paper introduces, termed the 'I don't know' function -- something currently lacking in modern AI. In view of these insights, the CRP also provides a glimpse into the behaviour of Artificial General Intelligence (AGI). An AGI cannot be 'almost sure', nor can it always explain itself, and therefore to be trustworthy it must be able to say 'I don't know'.<br><br>

35. ***McKinsey's "Technology Trends Outlook 2024": <br>McKinsey's The report identifies five key tech trends: the AI revolution, digital future building, compute and connectivity frontiers, cutting-edge engineering, and sustainability. Generative AI (Gen AI) is highlighted as a rapidly advancing area, with significant organizational adoption and the potential to generate up to $4.4 trillion in annual value. The report underscores the importance of addressing risks such as bias, misinformation, and deepfakes as organizations invest in the capabilities required to scale Gen AI, leading to a heightened demand for talent in data science, software engineering, and data engineering.***<br>

    Jul 30, [McKinsey published](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-top-trends-in-tech#/) “Technology Trends Outlook 2024”. The five tech trends categories are: the AI revolution, building the digital future, compute and connectivity frontiers, cutting-edge engineering, and a sustainable world. With regarding to GenAI, the report indicates that Generative AI (gen AI) has been making significant strides, pushing the boundaries of machine capabilities. Gen AI has sparked widespread interest, with individuals and organizations across different regions and industries exploring its potential. According to the latest McKinsey Global Survey on the state of AI, 65 percent of respondents say their organizations are regularly using gen AI in at least one business function, up from one-third last year, and gen AI use cases have the potential to generate an annual value of $2.6 trillion to $4.4 trillion. However, it’s important to recognize the risks that accompany the use of this powerful technology, including bias, misinformation, and deepfakes. Progressing through 2024 and beyond, McKinsey anticipates organizations investing in the risk mitigation, operating model, talent, and technological capabilities required to scale gen AI. Organizations are now focusing on scaling and expanding their internal capabilities to harness the potential of gen AI, leading to a sharp increase in demand for data scientists, software engineers, and data engineers.<br><br>

37. ***Impact of Output Length on LLM Reasoning: <br>The study highlights the trade-off between generating detailed reasoning in outputs and the time required. The researcher proposes Constrained-CoT (CCoT), a refined prompt engineering strategy that limits output length while maintaining accuracy. Experiments showed that applying CCoT to LLaMA2-70b improved accuracy and reduced output length, demonstrating the effectiveness of concise reasoning in LLMs.***<br>

    Jul 29, researcher from Italy published a [paper](https://arxiv.org/pdf/2407.19825) “Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost”. Today's large language models (LLMs) can solve challenging question-answering tasks, and prompt engineering techniques, such as chain-of-thought (CoT), have gained attention for enhancing the explanation and correctness of outputs. Nevertheless, models require significant time to generate answers augmented with lengthy reasoning details. To address this issue, this paper analyzes the impact of output lengths on LLM inference pipelines and proposes novel metrics to evaluate them in terms of correct conciseness. It also examines the impact of controlling output length through a refined prompt engineering strategy, Constrained-CoT (CCoT), which encourages the model to limit output length. Experiments on pre-trained LLMs demonstrated the benefit of the proposed metrics and the effectiveness of CCoT across different models. For instance, constraining the reasoning of LLaMA2-70b to 100 words improves the accuracy from 36.01% (CoT) to 41.07% (CCoT) on the GSM8K dataset, while reducing the average output length by 28 words.<br><br>

37. ***The Winds of AI Winter: The AI industry is facing significant challenges, with doubts about the ongoing AI Summer due to declining GPT-4 usage, canceled projects, and financial concerns. Key issues include user dissatisfaction, economic viability, and the need for AI engineers to bridge the gap between technological advances and practical benefits to avoid a potential "AI Winter."***<br>

    Jul 22, latent space published an [article](https://www.latent.space/p/mar-jun-2024) "The Winds of AI Winter". The AI industry is experiencing significant turbulence as doubts emerge about the ongoing AI Summer. Key issues include: 1) Decline in GPT-4 Use: Many users have switched from OpenAI’s GPT-4 to alternatives like Claude due to perceived superior performance. This trend, coupled with OpenAI's rumored financial losses, reflects growing dissatisfaction. 2)Failures and Cancellations: Several high-profile AI projects and products, such as Google’s AI overviews and McDonald’s drive-thru AI, have been announced and then abruptly canceled. 3) Financial and Industry Concerns: Goldman Sachs and Sequoia Capital highlight potential economic issues, with Goldman Sachs questioning the viability of AI investments given their high costs, while Sequoia critiques the AI industry's revenue shortfall compared to infrastructure costs. 4) Investment vs. Returns: Predictions vary widely on AI’s economic impact, with some expecting massive returns on investments and others warning of diminishing returns and substantial costs with unclear benefits. 5) Engineering Focus: The article emphasizes the need for AI engineers to address the imbalance between technological advancements and practical, widespread benefits to avoid a potential downturn in the industry. Overall, the industry faces a complex situation with both significant technological strides and serious financial and practical challenges. The key takeaway is the urgent need for effective engineering to bridge the gap between capability and real-world application to prevent a potential "AI Winter."
    <br><br>

    



***Aug 11 2024***

1. ***Meta's Self-Taught Evaluators: <br>
   Meta introduced a novel approach to model evaluation called "Self-Taught Evaluators," which uses synthetic training data to improve models without human annotations. This method iteratively generates and judges model outputs, significantly enhancing a large language model (LLM) without relying on costly and quickly outdated human preference data. The results show that the Self-Taught Evaluator can outperform commonly used judges and match the performance of top models trained with labeled data.*** <br><br>
   Aug 10, Meta published a [paper](https://arxiv.org/pdf/2408.02666) “Self-Taught Evaluators”. Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. This paper presents an approach that aims to improve evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, the iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, the Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples. <br><br>

3. ***Generative AI Business Use Cases:  <br>An article from cio.com highlights four transformative business applications of generative AI: virtual assistants, intelligent search, content summarization, and document processing. These AI-powered tools enhance productivity, streamline data management, and improve customer interactions, giving businesses a competitive edge by reducing costs and optimizing operations.*** <br><br>
   Aug 9, cio.com published [an article](https://www.cio.com/article/3478850/four-generative-ai-use-cases-for-businesses.html) “Four generative AI use cases for businesses”. The article outlines four key use cases where generative AI is transforming business operations: 1) Virtual Assistants: Generative AI powers tools like chatbots and virtual assistants, enhancing both employee productivity and customer experiences by automating tasks and providing personalized interactions. 2) Intelligent Search: Leveraging large language models (LLMs), generative AI enables enterprises to process and search proprietary data more effectively, offering precise and relevant information tailored to business-specific needs. 3) Content Summarization: AI models can quickly summarize documents, meetings, and videos, saving time and improving decision-making in sectors like healthcare and finance. 4) Document Processing: Generative AI streamlines document management by automating tasks such as translation, proofreading, and data extraction, particularly benefiting industries that handle large volumes of documents, like legal and financial sectors. Overall, generative AI boosts productivity, reduces costs, and provides businesses with a competitive edge by enabling more efficient data processing and customer interaction. <br><br>

5. ***OpenAI Leadership Changes:  <br>Futurism reported significant leadership changes at OpenAI, with key figures like co-founder John Schulman and VP Peter Deng leaving the company. This has sparked speculation about OpenAI's ability to achieve its goal of safe artificial general intelligence (AGI). Critics suggest that these departures may indicate challenges in fulfilling its vision, with some even predicting a potential "Generative AI bubble" burst.*** <br><br>
   Aug 8, Futurism published [an article](https://futurism.com/openai-prominent-employees-leaving) “Why Are OpenAI's Most Prominent Employees Leaving?”. Cofounder John Schulman announced he'd left the company this week to join rival AI company Anthropic, while president Greg Brockman is taking a leave of absence. VP of consumer product Peter Deng has also quit, indicating major shifts in OpenAI's upper ranks. The Sam Altman-led company has made its core mission to realize safe artificial general intelligence, the still entirely hypothetical point at which point AI can keep up with humans across a wide variety of intellectual tasks. But how far the company is from doing just that remains a heavily debated subject, with critics pointing out that OpenAI is shoring up billions of dollars in investment by making empty promises — an "AI bubble" that may be set to burst. Could the latest departures show that the venture is struggling to fulfill its long-term vision, let alone turn a profit from generative AI? Critics say OpenAI's brain drain problem could be the canary in the coal mine. "Calling it," leading AI skeptic Gary Marcus tweeted. "August 2024 will be known as the month in which the Generative AI bubble burst." Other critics questioned OpenAI's repeated claims that AGI is right around the corner. "If OpenAI is right on the verge of AGI, why do prominent people keep leaving?" AI developer Benjamin de Kraker tweeted. <br><br>
 
7. ***LLM-DetectAIve for Machine-Generated Text Detection:  <br>A new tool called "LLM-DetectAIve" was introduced for detecting machine-generated texts (MGTs). Unlike previous binary classifiers, it categorizes texts into four distinct types, offering fine-grained detection. This tool is particularly valuable in academic and educational settings, where identifying the degree of LLM involvement in text creation is crucial.*** <br><br>
   Aug 8, MBZUAI, Uni of Florida, NYU, et al. published a [paper](https://arxiv.org/pdf/2408.04284) “LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection”. The widespread accessibility of large language models (LLMs) to the general public has significantly amplified the dissemination of machine-generated texts (MGTs). Advancements in prompt manipulation have exacerbated the difficulty in discerning the origin of a text (human-authored vs machinegenerated). This raises concerns regarding the potential misuse of MGTs, particularly within educational and academic domains. This paper presents LLM-DetectAIve -- a system designed for fine-grained MGT detection. It is able to classify texts into four categories: human-written, machine-generated, machine-written machine-humanized, and human-written machine-polished. Contrary to previous MGT detectors that perform binary classification, introducing two additional categories in LLM-DetectiAIve offers insights into the varying degrees of LLM intervention during the text creation. This might be useful in some domains like education, where any LLM intervention is usually prohibited. Experiments show that LLM-DetectAIve can effectively identify the authorship of textual content, proving its usefulness in enhancing integrity in education, academia, and other domains. LLM-DetectAIve is publicly accessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video describing our system is available at https://youtu.be/E8eT_bE7k8c. <br><br>

9. ***Optical Neural Networks and FFM Learning:  <br>A paper in Nature presents a new method called fully forward mode (FFM) learning for training optical neural networks. FFM learning allows most machine learning operations to be conducted efficiently on physical systems rather than digital simulations, leading to faster learning processes and significant advancements in deep learning, ultrasensitive perception, and topological photonics.*** <br><br>
    Aug 7, Nature published a [paper](https://www.nature.com/articles/s41586-024-07687-4.pdf) “Fully forward mode training for optical neural networks”. Optical computing promises to improve the speed and energy efficiency of machine learning applications. However, current approaches to efficiently train these models are limited by in silico emulation on digital computers. This research develops a method called fully forward mode (FFM) learning, which implements the compute-intensive training process on the physical system. The majority of the machine learning operations are thus efficiently conducted in parallel on site, alleviating numerical modelling constraints. In free-space and integrated photonics, the researchers experimentally demonstrate optical systems with state-of-the-art performances for a given network size. FFM learning shows training the deepest optical neural networks with millions of parameters achieves accuracy equivalent to the ideal model. It supports all-optical focusing through scattering media with a resolution of the diffraction limit; it can also image in parallel the objects hidden outside the direct line of sight at over a kilohertz frame rate and can conduct all-optical processing with light intensity as weak as subphoton per pixel (5.40 × 1018- operations-per-second-per-watt energy efficiency) at room temperature. Furthermore, the study proves that FFM learning can automatically search non-Hermitian exceptional points without an analytical model. FFM learning not only facilitates orders-of-magnitude-faster learning processes, but can also advance applied and theoretical fields such as deep neural networks, ultrasensitive perception and topological photonics. <br><br>

11. ***Human-Level Robot Table Tennis:  <br>Google researchers developed a robot that achieved amateur human-level performance in competitive table tennis. The robot uses a hierarchical policy architecture and zero-shot sim-to-real techniques to adapt to new opponents in real time. It performed well against beginners and intermediate players, showcasing significant progress toward human-level robotic performance in physical tasks.*** <br><br>
    Aug 7, Google published a [paper](https://arxiv.org/pdf/2408.03906) “Achieving Human Level Competitive Robot Table Tennis”. Achieving human-level speed and performance on real world tasks is a north star for the robotics research community. This work takes a step towards that goal and presents the first learned robot agent that reaches amateur human-level performance in competitive table tennis. Table tennis is a physically demanding sport which requires human players to undergo years of training to achieve an advanced level of proficiency. The paper contributes (1) a hierarchical and modular policy architecture consisting of (i) low level controllers with their detailed skill descriptors which model the agent's capabilities and help to bridge the sim-to-real gap and (ii) a high level controller that chooses the low level skills, (2) techniques for enabling zero-shot sim-to-real including an iterative approach to defining the task distribution that is grounded in the real-world and defines an automatic curriculum, and (3) real time adaptation to unseen opponents. Policy performance was assessed through 29 robot vs. human matches of which the robot won 45% (13/29). All humans were unseen players and their skill level varied from beginner to tournament level. Whilst the robot lost all matches vs. the most advanced players it won 100% matches vs. beginners and 55% matches vs. intermediate players, demonstrating solidly amateur human-level performance. Videos of the matches can be viewed at https://sites.google.com/view/competitive-robot-table-tennis <br><br>

13. ***WalledEval Safety Evaluation Toolkit:  <br>Walled AI Lab introduced "WalledEval," a comprehensive toolkit for evaluating the safety of large language models (LLMs). It includes over 35 safety benchmarks and features like WalledGuard for content moderation and SGXSTest for exaggerated safety in cultural contexts. The toolkit is designed to test and improve LLM safety across various models and scenarios.*** <br><br>
    Aug 7, Walled AI Lab published a [paper](https://arxiv.org/pdf/2408.03837) “WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models”. WalledEval is a comprehensive AI safety testing toolkit designed to evaluate large language models (LLMs). It accommodates a diverse range of models, including both open-weight and API-based ones, and features over 35 safety benchmarks covering areas such as multilingual safety, exaggerated safety, and prompt injections. The framework supports both LLM and judge benchmarking, and incorporates custom mutators to test safety against various text-style mutations such as future tense and paraphrasing. Additionally, WalledEval introduces WalledGuard, a new, small and performant content moderation tool, and SGXSTest, a benchmark for assessing exaggerated safety in cultural contexts. We make WalledEval publicly available at https://github.com/walledai/walledevalA. <br><br>

15. ***CoverBench for Complex Claim Verification:  <br>Google and Tel Aviv University released "CoverBench," a benchmark for verifying the correctness of language models' outputs in complex reasoning tasks. It provides a diversified evaluation for complex claim verification across various domains, ensuring high data quality and challenging baseline results. The benchmark aims to advance the accuracy and reliability of language models in handling complex queries.*** <br><br>
    Aug 6, Google and Tel Aviv Uni published a [paper](https://arxiv.org/pdf/2408.03325) “CoverBench: A Challenging Benchmark for Complex Claim Verification”. There is a growing line of research on verifying the correctness of language models' outputs. At the same time, LMs are being used to tackle complex queries that require reasoning. The paper introduces CoverBench, a challenging benchmark focused on verifying LM outputs in complex reasoning settings. Datasets that can be used for this purpose are often designed for other complex reasoning tasks (e.g., QA) targeting specific use-cases (e.g., financial tables), requiring transformations, negative sampling and selection of hard examples to collect such a benchmark. CoverBench provides a diversified evaluation for complex claim verification in a variety of domains, types of reasoning, relatively long inputs, and a variety of standardizations, such as multiple representations for tables where available, and a consistent schema. The authors manually vet the data for quality to ensure low levels of label noise. Finally, the paper reports a variety of competitive baseline results to show CoverBench is challenging and has very significant headroom. The data is available at https://huggingface.co/datasets/google/coverbench. <br><br>

17. ***Scaling LLM Test-Time Compute:  <br>A joint study by UC Berkeley and Google explored the effectiveness of scaling test-time computation for large language models (LLMs). The research suggests that optimizing test-time compute allocation can significantly enhance LLM performance, potentially outperforming models with more parameters. This approach may lead to more efficient and self-improving AI agents.*** <br><br>
    Aug 6, UC Berkeley and Google published a [paper](https://arxiv.org/pdf/2408.03314) “Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters”. Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. The paper studies the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. This study analyzes two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. The authors find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a "compute-optimal" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy will improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, it is found that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model. <br><br>

19. ***Google's Antitrust Ruling:  <br>A US judge ruled that Google illegally maintained its monopoly on online search and advertising by paying billions to be the default search engine on smartphones and browsers. This landmark decision could have significant implications for how tech giants operate, as antitrust authorities aim to strengthen competition. The penalties Google may face will be decided in a future hearing.*** <br><br>
    Aug 6, [according to BBC](https://www.bbc.com/news/articles/c0k44x6mge3o), A US judge has ruled Google acted illegally to crush its competition and maintain a monopoly on online search and related advertising. The landmark decision on Monday is a major blow to Alphabet, Google's parent company, and could reshape how technology giants do business. Google was sued by the US Department of Justice in 2020 over its control of about 90% of the online search market. It is one of several lawsuits that have been filed against the big tech companies as US antitrust authorities attempt to strengthen competition in the industry. This case has at times been described as posing an existential threat to Google and its owner given its dominance of the search and online advertising business. It is unclear yet what penalties Google and Alphabet will face as a result of the decision. The fines or other remedies will be decided in a future hearing. In his decision, US District Judge Amit Mehta said Google had paid billions to ensure it is the default search engine on smartphones and browsers. “Google is a monopolist, and it has acted as one to maintain its monopoly,” Judge Mehta wrote in his 277-page opinion. Alphabet said it plans to appeal against the ruling. US Attorney General Merrick Garland, the country's top prosecutor, hailed the ruling as a "historic win for the American people". Another case against the technology company over its advertising technology is scheduled to go to trial in September. In Europe, meanwhile, Google has been fined billions in monopoly cases. <br><br>

21. ***Privacy in AI Assistants:  <br>Google proposed operationalizing contextual integrity (CI) in AI assistants to address privacy concerns. By aligning information-sharing actions with privacy expectations, the framework aims to prevent AI assistants from inappropriately sharing user information. The study's evaluation shows that CI-based reasoning improves privacy compliance in AI assistants.*** <br><br>
    Aug 5, Google published a [paper](https://arxiv.org/pdf/2408.02373) “Operationalizing Contextual Integrity in Privacy-Conscious Assistants”. Advanced AI assistants combine frontier LLMs and tool access to autonomously perform complex tasks on behalf of users. While the helpfulness of such assistants can increase dramatically with access to user information including emails and documents, this raises privacy concerns about assistants sharing inappropriate information with third parties without user supervision. To steer information-sharing assistants to behave in accordance with privacy expectations, the study proposes to operationalize contextual integrity (CI), a framework that equates privacy with the appropriate flow of information in a given context. In particular, the authors design and evaluate a number of strategies to steer assistants' information-sharing actions to be CI compliant. The evaluation is based on a novel form filling benchmark composed of synthetic data and human annotations, and it reveals that prompting frontier LLMs to perform CI-based reasoning yields strong results. <br><br>

23. ***RAG Foundry Framework:  <br>Intel Labs introduced "RAG Foundry," an open-source framework for enhancing large language models with Retrieval-Augmented Generation (RAG). The framework integrates data creation, training, inference, and evaluation, allowing for rapid prototyping and experimentation with RAG techniques. RAG Foundry has shown consistent improvements in knowledge-intensive tasks.*** <br><br>
    Aug 5, Inter Labs published a [paper](https://arxiv.org/pdf/2408.02545) “RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation”. Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. The paper introduces RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. The paper demonstrates the framework's effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry.  <br><br>

25. ***Jailbreaking LLMs:  <br>NYU and Meta published a study analyzing the jailbreaking of large language models (LLMs) from a statistical perspective. The research introduces E-RLHF, a modification to the existing RLHF objective, which increases the likelihood of safe responses. E-RLHF outperforms RLHF in preventing harmful behavior while maintaining model performance.*** <br><br>
    Aug 2, NYU and Meta published a [paper](https://arxiv.org/pdf/2408.01420) “Mission Impossible: A Statistical Perspective on Jailbreaking LLMs”. Large language models (LLMs) are trained on a deluge of text data with limited quality control. As a result, LLMs can exhibit unintended or even harmful behaviours, such as leaking information, fake news or hate speech. Countermeasures, commonly referred to as preference alignment, include fine-tuning the pretrained LLMs with carefully crafted text examples of desired behaviour. Even then, empirical evidence shows preference aligned LLMs can be enticed to harmful behaviour. This so called jailbreaking of LLMs is typically achieved by adversarially modifying the input prompt to the LLM. The paper provides theoretical insights into the phenomenon of preference alignment and jailbreaking from a statistical perspective. Under the framework, the authors first show that pretrained LLMs will mimic harmful behaviour if present in the training corpus. Under that same framework, the paper then introduces a statistical notion of alignment, and lower-bound the jailbreaking probability, showing that it is unpreventable under reasonable assumptions. Based on the insights, the authors propose an alteration to the currently prevalent alignment strategy RLHF. Specifically, the research introduces a simple modification to the RLHF objective, called E-RLHF, that aims to increase the likelihood of safe responses. E-RLHF brings no additional training cost, and is compatible with other methods. Empirically, the study demonstrates that E-RLHF outperforms RLHF on all alignment problems put forward by the AdvBench and HarmBench project without sacrificing model performance as measured by the MT-Bench project. <br><br>

27. ***RELBENCH for Relational Databases:  <br>Stanford University and Kumo.AI introduced "RELBENCH," a benchmark for predictive tasks over relational databases using graph neural networks. The study demonstrates that relational deep learning (RDL) models outperform traditional manual feature engineering, reducing human effort and improving predictive accuracy.*** <br><br>
    Jul 29, Stanford Uni, Kumo.AI etc published a [paper](https://arxiv.org/pdf/2407.20060) “RELBENCH: A Benchmark for Deep Learning on Relational Databases”.  The paper presents RELBENCH, a public benchmark for solving predictive tasks over relational databases with graph neural networks. RELBENCH provides databases and tasks spanning diverse domains and scales, and is intended to be a foundational infrastructure for future research. The work uses RELBENCH to conduct the first comprehensive study of Relational Deep Learning (RDL) (Fey et al., 2024), which combines graph neural network predictive models with (deep) tabular models that extract initial entity-level representations from raw tables. End-to-end learned RDL models fully exploit the predictive signal encoded in primary-foreign key links, marking a significant shift away from the dominant paradigm of manual feature engineering combined with tabular models. To thoroughly evaluate RDL against this prior gold-standard, the authors conduct an in-depth user study where an experienced data scientist manually engineers features for each task. In this study, RDL learns better models whilst reducing human work needed by more than an order of magnitude. This demonstrates the power of deep learning for solving predictive tasks over relational databases, opening up many new research opportunities enabled by RelBench. <br><br>

29. ***Chain of Code for Reasoning:  <br>A paper from Google, Stanford, and UC Berkeley proposed "Chain of Code" (CoC), an extension to improve language model reasoning by integrating code emulation. CoC outperforms existing reasoning methods like Chain of Thought, broadening the scope of questions LMs can answer by simulating code execution.*** <br><br>
    Jul 29, Google, Stanford Uni, and UC Berkley published a [paper](https://chain-of-code.github.io/paper.pdf) on ICML2024 “Chain of Code: Reasoning with a Language Model-Augmented Code Emulator”. Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter – it’s hypothesized that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for semantic ones (and in particular, those that are a mix of both). For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for "detect_sarcasm(string)" that can be executed by the interpreter (handling the edge cases would be insurmountable). However, LMs may still produce a valid solution if they not only write code, but also selectively "emulate" the interpreter by generating the expected output of "detect_sarcasm(string)". This paper proposes Chain of Code (CoC), a simple yet surprisingly effective extension that improves LM code-driven reasoning. The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode that the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as an "LMulator"). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over Chain of Thought. In a nutshell, CoC broadens the scope of reasoning questions that LMs can answer by "thinking in code". <br><br>

30. ***AdaCoder for Visual Question Answering:  <br>Researchers from Tokyo Institute of Technology and OMRON SINIC X Corp developed "AdaCoder," a framework for adaptive prompt compression in visual programmatic models (VPMs). AdaCoder reduces input token length while maintaining or improving performance in visual question answering tasks, demonstrating its effectiveness in optimizing VPMs.*** <br><br>
    Jul 28, Tokyo Inst. Of Tech, OMRON SINIC X Corp et al published a [paper](https://arxiv.org/pdf/2407.19410) “AdaCoder: Adaptive Prompt Compression for Programmatic Visual Question Answering”. Visual question answering aims to provide responses to natural language questions given visual input. Recently, visual programmatic models (VPMs), which generate executable programs to answer questions through large language models (LLMs), have attracted research interest. However, they often require long input prompts to provide the LLM with sufficient API usage details to generate relevant code. To address this limitation, the paper proposes AdaCoder, an adaptive prompt compression framework for VPMs. AdaCoder operates in two phases: a compression phase and an inference phase. In the compression phase, given a preprompt that describes all API definitions in the Python language with example snippets of code, a set of compressed preprompts is generated, each depending on a specific question type. In the inference phase, given an input question, AdaCoder predicts the question type and chooses the appropriate corresponding compressed preprompt to generate code to answer the question. Notably, AdaCoder employs a single frozen LLM and pre-defined prompts, negating the necessity of additional training and maintaining adaptability across different powerful black-box LLMs such as GPT and Claude. In experiments, the authors apply AdaCoder to ViperGPT and demonstrate that it reduces token length by 71.1%, while maintaining or even improving the performance of visual question answering. <br><br>

32. ***Modular RAG Framework:  <br>A modular framework for Retrieval-Augmented Generation (RAG) systems is proposed to enable highly reconfigurable and specialized operators. This framework addresses the limitations of existing RAG paradigms, facilitating the development of more efficient and adaptable RAG systems.*** <br><br>
    Jul 26, Tonji Uni and Fudan Uni published a [paper](https://arxiv.org/pdf/2407.21059) “Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks”. Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of "retrieve-then-generate". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies. <br><br>

34. ***Limitations of Instruction Tuning:  <br>the ICML2024 paper finds that while IT is widely used to convert pre-trained LLMs into conversational agents, it doesn't improve knowledge or skills and may even degrade them. Key issues include a decline in response quality, increased hallucination, and the ineffectiveness of popular IT improvement methods. The authors emphasize that responses based on pre-trained knowledge outperform those generated from IT and hope these insights guide future research.*** <br><br>
    Jul 14, Uni of Maryland, Adobe, and Nvidia published a [paper](https://arxiv.org/pdf/2402.05119) on ICML2024 “A Closer Look at the Limitations of Instruction Tuning”. Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, the authors reveal various limitations of IT. In particular, the research shows that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating responses. (4) Popular methods to improve IT do not lead to performance improvements over a simple LoRA fine-tuned model. The findings reveal that responses generated solely from pre-trained knowledge consistently outperform responses by models that learn any form of new knowledge from IT on open-source datasets. The authors hope the insights and challenges revealed in this paper inspire future work in related directions.
 <br><br><br>

***Aug 4 2024***

1. ***Lean AI and Small Language Models: <br>
   The article discusses the shift towards lean AI, which aims to optimize efficiency and minimize resource consumption. This shift is driven by the high costs and resource demands of large language models (LLMs). Small language models (SLMs) are becoming more popular due to their lower operational costs, faster deployment cycles, and specialized applications. Open-source initiatives are making advanced AI more accessible and affordable for organizations.***
   
   Aug 2, [InfoWorld](https://www.infoworld.com/article/3480593/small-language-models-and-open-source-are-transforming-ai.html) published an article Small language models (SLM) and open source are transforming AI. The shift towards lean AI emphasizes optimizing efficiency and minimizing resource consumption, addressing the high costs and resource demands of large language models (LLMs). Enterprises are increasingly adopting SLMs for their lower operational costs, faster deployment cycles, and ability to deliver specialized applications. Open-source initiatives and tools are democratizing AI capabilities, enabling more organizations to incorporate advanced AI without relying on expensive proprietary solutions. <br><br>

3. ***Meta's SAM 2 for Visual Segmentation: <br>
   Meta introduced the Segment Anything Model 2 (SAM 2) for visual segmentation in images and videos. SAM 2 uses a transformer architecture with streaming memory for real-time video processing and has the largest video segmentation dataset. It provides better accuracy with fewer interactions and is significantly faster and more accurate than its predecessor, SAM. The model, dataset, and an interactive demo are being released.***
   
   Aug 1, Meta published a [paper](https://arxiv.org/pdf/2408.00714) “SAM 2: Segment Anything in Images and Videos”. The research presents Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. The  authors build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. The model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on the data provides strong performance across a wide range of tasks. In video segmentation, it is observed better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, the model is more accurate and 6x faster than the Segment Anything Model (SAM). The authors believe that the data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. The authors are releasing a version of [the model](https://github.com/facebookresearch/segment-anything-2), the dataset and an interactive demo. <br><br>

5. ***Microsoft's OmniParser for GUI Agents: <br>
   Microsoft presented OmniParser, a method for parsing user interface screenshots into structured elements. OmniParser enhances GPT-4V's ability to generate actions accurately grounded in the corresponding regions of the interface. By using curated datasets for icon detection and description, OmniParser significantly improves performance on benchmarks and outperforms existing models that require additional information.***
   Aug 1, Microsoft published a [paper](https://arxiv.org/pdf/2408.00203) “OmniParser for Pure Vision Based GUI Agent”. The recent success of large vision language models shows great potential in driving the agent system operating on user interfaces. However, the power multimodal models like GPT-4V as a general agent on multiple operating systems across different applications is largely underestimated due to the lack of a robust screen parsing technique capable of: 1) reliably identifying interactable icons within the user interface, and 2) understanding the semantics of various elements in a screenshot and accurately associate the intended action with the corresponding region on the screen. To fill these gaps, the paper introduces OmniParser, a comprehensive method for parsing user interface screenshots into structured elements, which significantly enhances the ability of GPT-4V to generate actions that can be accurately grounded in the corresponding regions of the interface. The authors first curated an interactable icon detection dataset using popular webpages and an icon description dataset. These datasets were utilized to fine-tune specialized models: a detection model to parse interactable regions on the screen and a caption model to extract the functional semantics of the detected elements. OmniParser significantly improves GPT-4V's performance on ScreenSpot benchmark. And on Mind2Web and AITW benchmark, OmniParser with screenshot only input outperforms the GPT-4V baselines requiring additional information outside of screenshot. <br><br>

8. ***Scaling Inference Compute with Repeated Sampling: <br>
   Researchers from Stanford, Oxford, and Google explored scaling inference compute by increasing the number of generated samples. They found that coverage, or the fraction of problems solved, scales with the number of samples. Repeated sampling significantly improves performance in domains with verifiable answers, and it is cost-effective. However, identifying correct samples in domains without automatic verifiers remains a challenge.***
   
   Jul 31, Stanford Uni, Uni of Oxford and Google published a [paper](https://arxiv.org/pdf/2407.21787) “Large Language Monkeys: Scaling Inference Compute with Repeated Sampling”. Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. This paper explores inference compute as another axis for scaling by increasing the number of generated samples. Across multiple tasks and models, the authors observe that coverage - the fraction of problems solved by any attempt - scales with the number of samples over four orders of magnitude. In domains like coding and formal proofs, where all answers can be automatically verified, these increases in coverage directly translate into improved performance. When applying repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-attempt state-of-the-art of 43% which uses more capable frontier models. Moreover, using current API pricing, amplifying the cheaper DeepSeek model with five samples is more cost-effective and solves more issues than paying a premium for one sample from GPT-4o or Claude 3.5 Sonnet. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. Finally, the work finds that identifying correct samples out of many generations remains an important direction for future research in domains without automatic verifiers. When solving math word problems from GSM8K and MATH, coverage with Llama-3 models grows to over 95% with 10,000 samples. However, common methods to pick correct solutions from a sample collection, such as majority voting or reward models, plateau beyond several hundred samples and fail to fully scale with the sample budget. <br><br>

9. ***Australia's Privacy Concerns with Social Media Platform X: <br>
    Australia's privacy watchdog is concerned that social media platform X (formerly Twitter) may be breaching privacy laws by automatically opting users into having their posts used to train AI systems. Platforms are required to ensure default settings enable user control and seek consent for data use. The watchdog is investigating practices across the industry as other major platforms also harvest user data to train AI.***
   
    Jul 31, according to [abc.new.au](https://www.abc.net.au/news/science/2024-07-31/elon-musk-x-breach-privacy-law-data-harvest-grok-ai/104054400), Australia's privacy watchdog says social media platform X (formerly Twitter) may be in breach of Australian privacy law after it emerged users were automatically opted in to having their posts used to build artificial intelligence (AI) systems. The Office of the Australian Information Commissioner stopped short of saying it would launch an inquiry into the platform's data collection, similar to the one it is currently undertaking in relation to TikTok. On Friday, an X user pointed out the X app privacy settings includes a pre-ticked box that permits X to use the account holder's posts to train the Grok AI chatbot built by Elon Musk's company xAI. The default setting states that you "allow your posts as well as your interactions, inputs and results with Grok to be used for training and fine-tuning". Under Australian law, platforms are required to ensure default settings enable user control, and to either seek an individuals' consent for how the platform will use the data, or be satisfied the user would reasonably expect the organisation to use their data for this purpose. In recent months, it also emerged other platforms, such as Meta and Slack, were harvesting user data to train AI as part of a global race to build bigger and better large language models (LLMs). Last month, it was revealed xAI was trying to build the world's largest supercomputer in the US city of Memphis to fuel its AI ambitions. The Commissioner's Office says it's "looking at such practices across the industry" as other major platforms, also competing to build their own AIs, harvest user data. <br><br>

11. ***Safetywashing in AI Safety Benchmarks: <br>
    A study by various universities analyzed AI safety benchmarks and found many are highly correlated with general capabilities, leading to "safetywashing." The study calls for more meaningful safety metrics that are empirically separable from generic capabilities. The authors propose a rigorous framework for AI safety research to advance the science of safety evaluations and clarify measurable progress.***
    
    Jul 31, Center of AI Safety, Uni of Penn., UC Berkeley, Stanford Uni, Yale Uni and Keio Uni published a [paper](https://arxiv.org/pdf/2407.21792) “Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?”. As artificial intelligence systems grow more powerful, there has been increasing interest in "AI safety" research to address emerging and future risks. However, the field of AI safety remains poorly defined and inconsistently measured, leading to confusion about how researchers can contribute. This lack of clarity is compounded by the unclear relationship between AI safety benchmarks and upstream general capabilities (e.g., general knowledge and reasoning). To address these issues, the study conducts a comprehensive meta-analysis of AI safety benchmarks, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety. The findings reveal that many safety benchmarks highly correlate with upstream model capabilities, potentially enabling "safetywashing" -- where capability improvements are misrepresented as safety advancements. Based on these findings, the authors propose an empirical foundation for developing more meaningful safety metrics and define AI safety in a machine learning research context as a set of clearly delineated research goals that are empirically separable from generic capabilities advancements. In doing so, the authors aim to provide a more rigorous framework for AI safety research, advancing the science of safety evaluations and clarifying the path towards measurable progress. <br><br>

13. ***Google's Gemma 2 AI Models: <br>
    Google introduced the Gemma 2 family, including Gemma 2 2B, ShieldGemma, and Gemma Scope. Gemma 2 2B is a lightweight model with superior performance and efficiency. ShieldGemma offers safety content classifier models for various tasks, while Gemma Scope provides insights into model operations. These additions enhance AI capabilities, safety, and innovation.***
    
    Jul 31, Google released [Gemma 2 family](https://developers.googleblog.com/en/smaller-safer-more-transparent-advancing-responsible-ai-with-gemma/#:~:text=A%20Future%20Built%20on%20Responsible,developing%20safe%20and%20beneficial%20AI.) members Gemma 2 2B, SheldGemma and Gemma Scope. [Gemma 2 2B](https://huggingface.co/collections/google/gemma-2-2b-release-66a20f3796a2ff2a7c76f98f) – a brand-new version of the popular 2 billion (2B) parameter model, featuring built-in safety advancements and a powerful balance of performance and efficiency. This lightweight model produces outsized results by learning from larger models through distillation. In fact, Gemma 2 2B surpasses all GPT-3.5 models on the Chatbot Arena, demonstrating its exceptional conversational AI abilities. ShieldGemma – a suite of safety content classifier models, built upon Gemma 2, to filter the input and outputs of AI models and keep the user safe. [ShieldGemma](https://huggingface.co/google/shieldgemma-2b (/9b/27b)) offers various model sizes to meet diverse needs. The 2B model is ideal for online classification tasks, while the 9B and 27B versions provide higher performance for offline applications where latency is less of a concern. [Gemma Scope](https://huggingface.co/google/gemma-scope) – a new model interpretability tool that offers unparalleled insight into our models' inner workings. With these additions, researchers and developers can now create safer customer experiences, gain unprecedented insights into the models, and confidently deploy powerful AI responsibly, right on device, unlocking new possibilities for innovation. <br><br>

15. ***ShieldGemma for Content Moderation: <br>
    Google presented ShieldGemma, a suite of LLM-based safety content moderation models built on Gemma2. These models offer robust predictions of safety risks and outperform existing models on benchmarks. The paper introduces a novel data curation pipeline and demonstrates strong generalization performance. ShieldGemma advances LLM safety and content moderation solutions.***
    
    Jul 31, Google published a [paper](https://arxiv.org/pdf/2407.21772) “ShieldGemma: Generative AI Content Moderation Based on Gemma”. The paper presents ShieldGemma, a comprehensive suite of LLM-based safety content moderation models built upon Gemma2. These models provide robust, state-of-the-art predictions of safety risks across key harm types (sexually explicit, dangerous content, harassment, hate speech) in both user input and LLM-generated output. By evaluating on both public and internal benchmarks, the work demonstrates superior performance compared to existing models, such as Llama Guard (+10.8\% AU-PRC on public benchmarks) and WildCard (+4.3\%). Additionally, the paper presents a novel LLM-based data curation pipeline, adaptable to a variety of safety-related tasks and beyond. The authors have shown strong generalization performance for model trained mainly on synthetic data. By releasing ShieldGemma, the paper provides a valuable resource to the research community, advancing LLM safety and enabling the creation of more effective content moderation solutions for developers. Models are [available here](https://huggingface.co/google/shieldgemma-2b (/9b/27b)). <br><br>

17. ***Meta's Self-Improving Language Models: <br>
    Meta, UC Berkeley, and NYU introduced a Meta-Rewarding step for self-improving language models. This method improves models' judgment skills by having them judge their own responses. The approach enhances both judgment and instruction-following abilities without human supervision, showing significant performance improvements on benchmarks.***
    
    Jul 30, Meta, UC Berkeley, and NYU published a [paper](https://arxiv.org/pdf/2407.19594) “Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge”. Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs can improve by judging their own responses instead of relying on human labelers. However, existing methods have primarily focused on improving model responses rather than judgment capabilities, resulting in rapid saturation during iterative training. To address this issue, the paper introduces a novel Meta-Rewarding step to the self-improvement process, where the model judges its own judgements and uses that feedback to refine its judgment skills. Surprisingly, this unsupervised approach improves the model's ability to judge {\em and} follow instructions, as demonstrated by a win rate improvement of Llama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on Arena-Hard. These results strongly suggest the potential for self-improving models without human supervision. <br><br>

19. ***Google's MoNE for Efficient Visual Processing: <br>
    Google and the University of Washington presented the Mixture of Nested Experts (MoNE) model, which uses a nested structure for experts to process visual tokens efficiently. MoNE reduces inference time compute while maintaining performance, making it adaptable to different compute budgets. The approach is validated on standard image and video datasets.***
    
    Jul 30, Google and Uni of Washington published a [paper](https://arxiv.org/pdf/2407.19985) “Mixture of Nested Experts: Adaptive Processing of Visual Tokens”. The visual medium (images and videos) naturally contains a large amount of information redundancy, thereby providing a great opportunity for leveraging efficiency in processing. While Vision Transformer (ViT) based models scale effectively to large data regimes, they fail to capitalize on this inherent redundancy, leading to higher computational costs. Mixture of Experts (MoE) networks demonstrate scalability while maintaining same inference-time costs, but they come with a larger parameter footprint. The paper presents Mixture of Nested Experts (MoNE), which utilizes a nested structure for experts, wherein individual experts fall on an increasing compute-accuracy curve. Given a compute budget, MoNE learns to dynamically choose tokens in a priority order, and thus redundant tokens are processed through cheaper nested experts. Using this framework, MoNE achieves equivalent performance as the baseline models, while reducing inference time compute by over two-fold. The authors validate the approach on standard image and video datasets - ImageNet-21K, Kinetics400, and Something-Something-v2. The authors further highlight MoNE's adaptability by showcasing its ability to maintain strong performance across different inference-time compute budgets on videos, using only a single trained model. <br><br>

21. ***Diffusion Augmented Agents for RL: <br>
    Google introduced Diffusion Augmented Agents (DAAG), a framework leveraging language models, vision language models, and diffusion models to improve sample efficiency and transfer learning in reinforcement learning. DAAG enhances learning by transforming past experiences to align with target instructions, reducing the need for reward-labeled data and improving lifelong learning capabilities.***
    
    Jul 30, Google published a [paper](https://arxiv.org/pdf/2407.20798) “Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning”. The paper introduces Diffusion Augmented Agents (DAAG), a novel framework that leverages large language models, vision language models, and diffusion models to improve sample efficiency and transfer learning in reinforcement learning for embodied agents. DAAG hindsight relabels the agent's past experience by using diffusion models to transform videos in a temporally and geometrically consistent way to align with target instructions with a technique called as Hindsight Experience Augmentation. A large language model orchestrates this autonomous process without requiring human supervision, making it well-suited for lifelong learning scenarios. The framework reduces the amount of reward-labeled data needed to 1) finetune a vision language model that acts as a reward detector, and 2) train RL agents on new tasks. The paper demonstrates the sample efficiency gains of DAAG in simulated robotics environments involving manipulation and navigation. The results show that DAAG improves learning of reward detectors, transferring past experience, and acquiring new tasks - key abilities for developing efficient lifelong learning agents. Supplementary material and visualizations are available on the website [this https URL](https://sites.google.com/view/diffusion-augmented-agents/) <br><br>

23. ***Meta's SAM 2 for Video and Image Segmentation: <br>
    Meta's Segment Anything Model 2 (SAM 2) is designed for promptable visual segmentation in images and videos. It uses a transformer architecture with streaming memory for real-time processing and has the largest video segmentation dataset. SAM 2 provides better accuracy and speed compared to its predecessor, SAM, and is being released with a dataset and demo.***
    
    Jul 29, Meta published a [paper](https://scontent.fcbr1-1.fna.fbcdn.net/v/t39.2365-6/453323338_287900751050452_6064535069828837026_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=iEdf_eLLDBIQ7kNvgHvSueV&_nc_ht=scontent.fcbr1-1.fna&gid=AbMZVotuhlaDUcgiZQmipJh&oh=00_AYCMj0UJk4ZJNT-OM4nxXp6vgeWLO9SHo56ZlmA1qZHoaQ&oe=66B0FCB9) “SAM 2: Segment Anything in Images and Videos”.  The paper presents Segment Anything Model 2 (SAM 2 ), a foundation model towards solving promptable visual segmentation in images and videos. The authors build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. The model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on the data provides strong performance across a wide range of tasks. In video segmentation, it’s observed better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, the model is more accurate and 6x faster than the Segment Anything Model (SAM). The authors believe that the data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. The authors are releasing [a version of the model](https://ai.meta.com/sam2/), the dataset and an interactive demo. <br><br>

25. ***SaulLM Models for Legal Domain: <br>
    MICS and CINES introduced SaulLM-54B and SaulLM-141B, large language models tailored for the legal sector. These models are based on the Mixtral architecture and use domain adaptation strategies for legal tasks. They outperform previous models on LegalBench-Instruct and are released under the MIT License to facilitate reuse and research.***
    
    Jul 28, MICS and CINES published a [paper](https://arxiv.org/pdf/2407.19584) “SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain”. The paper introduces SaulLM-54B and SaulLM-141B, two large language models (LLMs) tailored for the legal sector. These models, which feature architectures of 54 billion and 141 billion parameters, respectively, are based on the Mixtral architecture. The development of SaulLM-54B and SaulLM-141B is guided by large-scale domain adaptation, divided into three strategies: (1) the exploitation of continued pretraining involving a base corpus that includes over 540 billion of legal tokens, (2) the implementation of a specialized legal instruction-following protocol, and (3) the alignment of model outputs with human preferences in legal interpretations. The integration of synthetically generated data in the second and third steps enhances the models' capabilities in interpreting and processing legal texts, effectively reaching state-of-the-art performance and outperforming previous open-source models on LegalBench-Instruct. This work explores the trade-offs involved in domain-specific adaptation at this scale, offering insights that may inform future studies on domain adaptation using strong decoder models. Building upon SaulLM-7B, this study refines the approach to produce an LLM better equipped for legal tasks. The authors are releasing base, instruct, and aligned versions on top of SaulLM-54B and SaulLM-141B under the MIT License to facilitate reuse and collaborative research. <br><br>

27. ***Amazon's REAPER for RAG Systems: <br>
    Amazon presented REAPER, a reasoning-based retrieval planner for complex RAG (Retrieval Augmented Generation) systems. REAPER generates retrieval plans for conversational systems, significantly reducing latency and scaling easily to new use cases. The method is shown to improve performance in a conversational shopping assistant context.***
    
    Jul 26, Amazon published a [paper](https://arxiv.org/abs/2407.18553) “REAPER: Reasoning based Retrieval Planning for Complex RAG Systems”. Complex dialog systems often use retrieved evidence to facilitate factual responses. Such RAG (Retrieval Augmented Generation) systems retrieve from massive heterogeneous data stores that are usually architected as multiple indexes or APIs instead of a single monolithic source. For a given query, relevant evidence needs to be retrieved from one or a small subset of possible retrieval sources. Complex queries can even require multi-step retrieval. For example, a conversational agent on a retail site answering customer questions about past orders will need to retrieve the appropriate customer order first and then the evidence relevant to the customer's question in the context of the ordered product. Most RAG Agents handle such Chain-of-Thought (CoT) tasks by interleaving reasoning and retrieval steps. However, each reasoning step directly adds to the latency of the system. For large models this latency cost is significant -- in the order of multiple seconds. Multi-agent systems may classify the query to a single Agent associated with a retrieval source, though this means that a (small) classification model dictates the performance of a large language model. This paper presents REAPER (REAsoning-based PlannER) - an LLM based planner to generate retrieval plans in conversational systems. The paper shows significant gains in latency over Agent-based systems and are able to scale easily to new and unseen use cases as compared to classification-based planning. Though the method can be applied to any RAG system, the authors show the results in the context of a conversational shopping assistant. <br><br>

29. ***Apple's MMAU Benchmark for LLM Agents: <br>
    Apple introduced the Massive Multitask Agent Understanding (MMAU) benchmark, evaluating models across five domains and capabilities. MMAU provides a comprehensive framework for assessing LLM agents' strengths and limitations with detailed analyses of 18 models. The benchmark enhances interpretability and understanding of model performance.***
    
    Jul 18, Apple published a [paper](https://arxiv.org/pdf/2407.18961) “MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains”. Recent advances in large language models (LLMs) have increased the demand for comprehensive benchmarks to evaluate their capabilities as human-like agents. Existing benchmarks, while useful, often focus on specific application scenarios, emphasizing task completion but failing to dissect the underlying skills that drive these outcomes. This lack of granularity makes it difficult to deeply discern where failures stem from. Additionally, setting up these environments requires considerable effort, and issues of unreliability and reproducibility sometimes arise, especially in interactive tasks. To address these limitations, the authors introduce the Massive Multitask Agent Understanding (MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need for complex environment setups. It evaluates models across five domains, including Tool-use, Directed Acyclic Graph (DAG) QA, Data Science and Machine Learning coding, Contest-level programming and Mathematics, and covers five essential capabilities: Understanding, Reasoning, Planning, Problem-solving, and Self-correction. With a total of 20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU provides a comprehensive framework for evaluating the strengths and limitations of LLM agents. By testing 18 representative models on MMAU, the paper provides deep and insightful analyses. Ultimately, MMAU not only sheds light on the capabilities and limitations of LLM agents but also enhances the interpretability of their performance. Datasets and evaluation scripts of MMAU are released at https://github.com/apple/axlearn/blob/main/docs/research/mmau.
 <br><br><br>
