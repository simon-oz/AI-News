# Weekly AI-News - April 2024
Welcome to the Weekly-AI-news!
I mainly focuses more on Generative AI, LLMs and the related, so may miss import AI news in other areas. <br><br>

***28 Apr 2024***

1. ***Meta's LayerSkip: Meta introduces LayerSkip, a solution for speeding up inference in large language models (LLMs). They apply layer dropout during training, with different rates for early and later layers, and use an early exit loss mechanism. In inference, they implement a self-speculative decoding approach that reduces memory usage and benefits from shared compute. Experiments show up to 2.16x speedups in various tasks.*** <br><br>
  25 Apr, Meta published a [paper](https://arxiv.org/pdf/2404.16710) “Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding”. The authors present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training the authors apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, the study shows that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, the research presents a novel self-speculative decoding solution where the authors exit at early layers and verify and correct with remaining layers of the model. The proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. The authors run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. The implemented inference solution shows speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task.

2. ***Microsoft's Graph RAG: Microsoft proposes a Graph RAG approach for query-focused summarization over private text corpora. It combines retrieval-augmented generation with graph-based text indexing, enabling scalable answering of global questions. Graph RAG significantly improves answer comprehensiveness and diversity compared to previous methods.*** <br><br>
   24 Apr, Microsoft published a [paper](https://arxiv.org/pdf/2404.16130) “From Local to Global: A Graph RAG Approach to Query-Focused Summarization”. The authors argues that the use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as "What are the main themes in the dataset?", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, the authors propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. The proposed approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, the study shows that Graph RAG leads to substantial improvements over a naïve RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at [this https URL](https://aka.ms/graphrag).

3. ***Snowflake's Arctic: Snowflake releases the open-source 482B parameter model, Snowflake Arctic, excelling in enterprise tasks like SQL generation and coding. Arctic achieves high performance with low training compute, setting a new standard for cost-effective model training.*** <br><br>
   24 Apr, Snowflake release its open source 482B paras model [Snowflake Arctic](https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/). Arctic excels at enterprise tasks such as SQL generation, coding and instruction following benchmarks even when compared to open source models trained with significantly higher compute budgets. In fact, it sets a new baseline for cost-effective training to enable Snowflake customers to create high-quality custom models for their enterprise needs at a low cost. Arctic is also efficiently intelligent and truly open with Apache 2.0 license. Arctic is on par or better than both Llama 3 8B and Llama 2 70B on enterprise metrics, while using less than half of the training compute budget. The Artic program is available immediately from Snowflake as an inference service on its Cortex offering. Artic will be available on Amazon AWS, Hugging Face and other venues, said Snowflake.

4. ***AWS's BASS: AWS presents BASS, a batched attention-optimized speculative sampling system that improves latency and throughput in large language model hosting. BASS achieves state-of-the-art multi-sequence generation latency with superior GPU utilization.*** <br><br>
   24 Apr, AWS published a [paper](https://arxiv.org/pdf/2404.15778) “BASS: Batched Attention-optimized Speculative Sampling”. Researcher from AWA find that Speculative decoding has emerged as a powerful method to improve latency and throughput in hosting large language models. However, most existing implementations focus on generating a single sequence. Real-world generative AI applications often require multiple responses and how to perform speculative decoding in a batched setting while preserving its latency benefits poses non-trivial challenges. This paper describes a system of batched speculative decoding that sets a new state of the art in multi-sequence generation latency and that demonstrates superior GPU utilization as well as quality of generations within a time budget. For example, for a 7.8B-size model on a single A100 GPU and with a batch size of 8, each sequence is generated at an average speed of 5.8ms per token, the overall throughput being 1.1K tokens per second. These results represent state-of-the-art latency and a 2.15X speed-up over optimized regular decoding. Within a time budget that regular decoding does not finish, the system is able to generate sequences with HumanEval Pass@First of 43% and Pass@All of 61%, far exceeding what's feasible with single-sequence speculative decoding. The peak GPU utilization during decoding reaches as high as 15.8%, more than 3X the highest of that of regular decoding and around 10X of single-sequence speculative decoding.

5. ***Microsoft's Phi-3: Microsoft introduces Phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, capable of rivaling larger models' performance. The dataset used for training is heavily filtered web data and synthetic data, aligned for robustness, safety, and chat format.*** <br><br>
   22 Apr, Microsoft released Phi-3, and also published the corresponding [technical report](https://arxiv.org/pdf/2404.14219) “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone”. The report introduces phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in the dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. The report also provides some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).

6. ***Apple's OpenELM: Apple releases OpenELM, an open language model family emphasizing reproducibility and transparency. OpenELM employs layer-wise scaling for efficient parameter allocation and provides a comprehensive framework for training and evaluation, aiming to empower the open research community.*** <br><br>
    22 Apr, Apple published a [paper](https://arxiv.org/pdf/2404.14619) “OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework” announcing the release of its OpenELM models. The paper indicates that the reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, Apple releases OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring 2× fewer pre-training tokens. Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, this release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. The paper also releases code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors. The source code along with pre-trained model weights and training recipes is available at [this https URL](https://github.com/apple/corenet). Additionally, models can be found on HuggingFace at [this https URL](https://huggingface.co/apple/OpenELM). However, the performance of OpenELM is far behind Microsoft’s Phe-3.

7. ***Forbes on AI as a Commodity: Forbes discusses the commoditization of AI, comparing it to other technological advancements. While AI tools become ubiquitous, companies must innovate beyond AI to maintain a competitive edge.*** <br><br>
   22 Apr, Forbes published [an article](https://www.forbes.com/sites/joemckendrick/2024/04/22/ai-will-fade-from-view-by-design/?sh=758019ed7769) “AI Will Eventually Fade From View, By Design”. The article discusses whether Artificial Intelligence (AI) is becoming a commodity, following the path of other technologies like personal computers and smartphones. Commoditization occurs when products or technologies are widely available, interchangeable, and no longer offer a competitive advantage. The arguments For Commoditization include: 1) AI tools and platforms, especially generative AI, are now widespread and available at no or low cost. 2) Everybody uses AI to some degree, making it a standard utility like electricity or internet access. 3) Companies will need to rethink their business strategies and find new ways to add value beyond just using AI.
Counterarguments are: 1) Tech-savvy companies like Amazon and Uber have disrupted markets using cutting-edge technology, including AI. 2) AI has the potential to advance business prospects and disrupt or commoditize many businesses. The article concludes that AI will eventually become a standard tool, like electricity or internet access, and will no longer be a competitive advantage. Companies will need to focus on innovative culture and forward-looking thinking to gain an edge. AI will deliver results when treated as a tool to improve businesses' bottom lines, rather than a standalone product.

8. ***SnapKV: UIUC, Cohere, and Princeton Uni propose SnapKV, an approach to efficiently minimize key-value (KV) cache size in large language models. SnapKV reduces computational overhead and memory footprint while maintaining performance, enabling processing of long input sequences.*** <br><br>
    22 Apr, UIUC, Cohere and Princeton Uni published a [paper](https://arxiv.org/pdf/2404.14469) “SnapKV: LLM Knows What You are Looking for Before Generation”. The paper finds that Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. The authors discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an ‘observation’ window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. The proposed approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications. Code is [available here](https://github.com/FasterDecoding/SnapKV).

9. ***LLaMA3 Low-bit Quantization Study: Researchers explore the performance of Meta's LLaMA3 models when quantized to low bit-widths. They find significant performance degradation, especially in ultra-low bit-width scenarios, suggesting a need for bridging the performance gap in future developments.*** <br><br>
    22 Apr, Uni of Hong Kong, ETH Zurich and Beihang Uni published a [paper](https://arxiv.org/pdf/2404.14047) “How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study”. The authors indicates that Meta's LLaMA family has become one of the most powerful open-source Large Language Model (LLM) series. Notably, LLaMA3 models have recently been released and achieve impressive performance across various with super-large scale pre-training on over 15T tokens of data. Given the wide application of low-bit quantization for LLMs in resource-limited scenarios, The authors explore LLaMA3's capabilities when quantized to low bit-width. This exploration holds the potential to unveil new insights and challenges for low-bit quantization of LLaMA3 and other forthcoming LLMs, especially in addressing performance degradation problems that suffer in LLM compression. Specifically, the researchers evaluate the 10 existing post-training quantization and LoRA-finetuning methods of LLaMA3 on 1-8 bits and diverse datasets to comprehensively reveal LLaMA3's low-bit quantization performance. The experiment results indicate that LLaMA3 still suffers non-negligent degradation in these scenarios, especially in ultra-low bit-width. This highlights the significant performance gap under low bit-width that needs to be bridged in future developments. The paper expects that this empirical study will prove valuable in advancing future models, pushing the LLMs to lower bit-width with higher accuracy for being practical. The project is released on https://github.com/Macaronlin/LLaMA3-Quantization and quantized LLaMA3 models are released in https://huggingface.co/LLMQ.

10. ***JAT Model: Huggingface, Uni Lyon, and Uni Bordeaux present Jack of All Trades (JAT), a versatile transformer-based model optimized for sequential decision-making tasks and multimodal data. JAT demonstrates strong performance across various RL, CV, and NLP benchmarks using a single set of weights.*** <br><br>
    22 Apr, Huggingface, Uni Lyon, and Uni Bordeaux updated their [paper](https://arxiv.org/pdf/2402.09844) “Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent”. The paper argues that the search for a general model that can operate seamlessly across multiple domains remains a key goal in machine learning research. The prevailing methodology in Reinforcement Learning (RL) typically limits models to a single task within a unimodal framework, a limitation that contrasts with the broader vision of a versatile, multi-domain model. This research presents Jack of All Trades (JAT), a transformer-based model with a unique design optimized for handling sequential decision-making tasks and multimodal data types. The JAT model demonstrates its robust capabilities and versatility by achieving strong performance on very different RL benchmarks, along with promising results on Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a single set of weights. The JAT model marks a significant step towards more general, cross-domain AI model design, and notably, it is the first model of its kind to be fully open-sourced (see [this https URL](https://huggingface.co/jat-project/jat)), including a pioneering general-purpose dataset.

11. ***DataTune: CMU introduces DataTune, a method for transforming existing datasets to improve automatic dataset generation for NLP tasks. DataTune significantly increases data diversity and difficulty, enhancing model performance compared to existing methods.*** <br><br>
    22 Apr, CMU published a [paper](https://arxiv.org/pdf/2404.14361) “Better Synthetic Data by Retrieving and Transforming Existing Datasets”. The paper points out that despite recent advances in large language models, building dependable and deployable NLP models typically requires abundant, high-quality training data. However, task-specific data is not available for many use cases, and manually curating task-specific data is labor-intensive. Recent work has studied prompt-driven synthetic data generation using large language models, but these generated datasets tend to lack complexity and diversity. To address these limitations, the authors introduce a method, DataTune, to make better use of existing, publicly available datasets to improve automatic dataset generation. DataTune performs dataset transformation, enabling the repurposing of publicly available datasets into a format that is directly aligned with the specific requirements of target tasks. On a diverse set of language-based tasks from the BIG-Bench benchmark, the study finds that finetuning language models via DataTune improves over a few-shot prompting baseline by 49% and improves over existing methods that use synthetic or retrieved training data by 34%. The researchers find that dataset transformation significantly increases the diversity and difficulty of generated data on many tasks. The authors integrate DataTune into an open-source repository to make this method accessible to the community: [this https URL](https://github.com/neulab/prompt2model). (Note: Could be used with Huggingface [Commom Corpus dataset](https://huggingface.co/blog/Pclanglais/common-corpus))

12. ***STEP-BACK PROMPTING: Google introduces STEP-BACK PROMPTING, a technique enabling large language models (LLMs) to derive high-level concepts from specific details, leading to improved reasoning abilities. Experiments show substantial performance gains on reasoning-intensive tasks.*** <br><br>
    21 Apr, Google updated its ICLR 2024 take a step back [paper](https://openreview.net/pdf?id=3bq3jsvcQ1) “Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models”. The paper presents STEP-BACK PROMPTING, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. The authors conduct experiments of STEP-BACK PROMPTING with PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, STEP-BACK PROMPTING improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7% and 11% respectively, TimeQA by 27%, and MuSiQue by 7%.

20. ***Instruction Hierarchy: OpenAI proposes an instruction hierarchy to address vulnerabilities in large language models (LLMs) caused by prompt injections and other attacks. The hierarchy defines how models should behave when instructions of different priorities conflict, increasing robustness.*** <br><br>
    19 Apr, OpenAI published a [paper](https://arxiv.org/pdf/2404.13208) “The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions”. The paper states that today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts. The authors argue that one of the primary vulnerabilities underlying these attacks is that LLMs often consider system prompts (e.g., text from an application developer) to be the same priority as text from untrusted users and third parties. To address this, the paper proposes an instruction hierarchy that explicitly defines how models should behave when instructions of different priorities conflict. The paper then proposes a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions. The study applies this method to GPT-3.5, showing that it drastically increases robustness -- even for attack types not seen during training -- while imposing minimal degradations on standard capabilities.

14. ***DPO for LLMs: Stanford Uni presents Direct Preference Optimization (DPO) for large language models (LLMs), aligning it with token-level reinforcement learning and showing its equivalence to likelihood-based search algorithms. DPO improves credit assignment and search-based algorithms' performance.*** <br><br>
    18 Apr, Stanford Uni published a [paper](https://arxiv.org/pdf/2404.12358) “From r to Q∗: Your Language Model is Secretly a Q-Function”. The paper states that Reinforcement Learning From Human Feedback (RLHF) has been a critical to the success of the latest generation of generative AI models. In response to the complex nature of the classical RLHF pipeline, direct alignment algorithms such as Direct Preference Optimization (DPO) have emerged as an alternative approach. Although DPO solves the same objective as the standard RLHF setup, there is a mismatch between the two approaches. Standard RLHF deploys reinforcement learning in a specific token-level MDP, while DPO is derived as a bandit problem in which the whole response of the model is treated as a single arm. This work rectifies this difference, first the authors theoretically show that they can derive DPO in the token-level MDP as a general inverse Q-learning algorithm, which satisfies the Bellman equation. Using the theoretical results, the researchers provide three concrete empirical insights. First, the study shows that because of its token level interpretation, DPO is able to perform some type of credit assignment. Next, the authors prove that under the token level formulation, classical search-based algorithms, such as MCTS, which have recently been applied to the language generation space, are equivalent to likelihood-based search on a DPO policy. Empirically the authors show that a simple beam search yields meaningful improvement over the base DPO policy. Finally, the research shows how the choice of reference policy causes implicit rewards to decline during training. The authors conclude by discussing applications of the work, including information elicitation in multi-tun dialogue, reasoning, agentic applications and end-to-end training of multi-model systems.

15. ***FlowMind: J.P. Morgan AI Research introduces FlowMind, an automatic workflow generation system leveraging large language models (LLMs) for Robotic Process Automation (RPA). FlowMind mitigates hallucinations, ensures data integrity, and simplifies user interaction, demonstrating success in finance tasks.*** <br><br>
    J. P. Morgan AI Research published a [paper](https://arxiv.org/pdf/2404.13050) “FlowMind: Automatic Workflow Generation with LLMs”. The paper finds that the rapidly evolving field of Robotic Process Automation (RPA) has made significant strides in automating repetitive processes, yet its effectiveness diminishes in scenarios requiring spontaneous or unpredictable tasks demanded by users. This study introduces a novel approach, FlowMind, leveraging the capabilities of Large Language Models (LLMs) such as Generative Pretrained Transformer (GPT), to address this limitation and create an automatic workflow generation system. In FlowMind, the authors propose a generic prompt recipe for a lecture that helps ground LLM reasoning with reliable Application Programming Interfaces (APIs). With this, FlowMind not only mitigates the common issue of hallucinations in LLMs, but also eliminates direct interaction between LLMs and proprietary data or code, thus ensuring the integrity and confidentiality of information - a cornerstone in financial services. FlowMind further simplifies user interaction by presenting high-level descriptions of auto-generated workflows, enabling users to inspect and provide feedback effectively. The researchers also introduce NCEN-QA, a new dataset in finance for benchmarking question-answering tasks from N-CEN reports on funds. The authors used NCEN-QA to evaluate the performance of workflows generated by FlowMind against baseline and ablation variants of FlowMind. The authors demonstrate the success of FlowMind, the importance of each component in the proposed lecture recipe, and the effectiveness of user interaction and feedback in FlowMind.

16. ***RAG Model Analysis: Stanford Uni analyzes the tension between a large language model's (LLM) internal prior knowledge and retrieved information in retrieval-augmented generation (RAG) models. The study shows how the LLM's prior influences its response when conflicting with retrieved information.*** <br><br>
    16 Apr, Stanford Uni published a [paper](https://arxiv.org/pdf/2404.10198) “How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior”. The paper indicates that Retrieval augmented generation (RAG) is often used to fix hallucinations and provide up-to-date knowledge for large language models (LLMs). However, in cases when the LLM alone incorrectly answers a question, does providing the correct retrieved content always fix the error? Conversely, in cases where the retrieved content is incorrect, does the LLM know to ignore the wrong information, or does it recapitulate the error? To answer these questions, the authors systematically analyze the tug-of-war between a LLM's internal knowledge (i.e. its prior) and the retrieved information in settings when they disagree. The researchers test GPT-4 and other LLMs on question-answering abilities across datasets with and without reference documents. As expected, providing the correct retrieved information fixes most model mistakes (94% accuracy). However, when the reference document is perturbed with increasing levels of wrong values, the LLM is more likely to recite the incorrect, modified information when its internal prior is weaker but is more resistant when its prior is stronger. Similarly, the study also finds that the more the modified information deviates from the model's prior, the less likely the model is to prefer it. These results highlight an underlying tension between a model's prior knowledge and the information presented in reference documents.
 <br><br> <br>

***21 Apr 2024***
1. ***Meta released Llama3, the most capable openly available LLM yet, with plans to make it accessible on various platforms. They emphasize responsible development and offer tools like Llama Guard 2, Code Shield, and CyberSec Eval 2. The models boast enhanced performance, including improved reasoning, with sizes ranging from 8 billion to 70 billion parameters. They're trained on custom-built GPU clusters and showcased state-of-the-art performance on industry benchmarks.*** <br><br>
2.18 Apr, [Meta released Llama3](https://ai.meta.com/blog/meta-llama-3/), the most capable openly available LLM to date.  Based on the blog, Llama 3 models will soon be available on AWS, Databricks, Google Cloud, Hugging Face, Kaggle, IBM WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake, and with support from hardware platforms offered by AMD, AWS, Dell, Intel, NVIDIA, and Qualcomm. Meta is dedicated to developing Llama 3 in a responsible way, and it’s offering various resources to help others use it responsibly as well. This includes introducing new trust and safety tools with Llama Guard 2, Code Shield, and CyberSec Eval 2. Meta is expected to introduce new capabilities, longer context windows, additional model sizes, and enhanced performance, and it’ll share the Llama 3 research paper. Meta AI, built with Llama 3 technology, is now one of the world’s leading AI assistants that can boost your intelligence and lighten your load—helping you learn, get things done, create content, and connect to make the most out of every moment. Meta release two models, one has 8 billion parameters, and one has 70 billion parameters. This next generation of Llama demonstrates state-of-the-art performance on a wide range of industry benchmarks and offers new capabilities, including improved reasoning. The models were trained on two custom-built 24K GPU clusters, with about 15 trillion tokens. [Try it here](https://www.meta.ai/).

2. ***A consortium of 66 AI companies and universities unveiled v0.5 of the AI Safety Benchmark, designed to evaluate the safety risks of AI systems using chat-tuned language models. The benchmark focuses on adult-chat scenarios in English, with tests covering 7 of 13 hazard categories. They aim to release version 1.0 by the end of 2024, providing insights into AI system safety.*** <br><br>
   18 Apr, 66 top AI companies and universities published a [paper](https://arxiv.org/pdf/2404.12241) “Introducing v0.5 of the AI Safety Benchmark from MLCommons”. This paper introduces v0.5 of the AI Safety Benchmark, which has been created by the MLCommons AI Safety Working Group. The AI Safety Benchmark has been designed to assess the safety risks of AI systems that use chat-tuned language models. The paper introduces a principled approach to specifying and constructing the benchmark, which for v0.5 covers only a single use case (an adult chatting to a general-purpose assistant in English), and a limited set of personas (i.e., typical users, malicious users, and vulnerable users). The authors created a new taxonomy of 13 hazard categories, of which 7 have tests in the v0.5 benchmark. The researcher plan to release version 1.0 of the AI Safety Benchmark by the end of 2024. The v1.0 benchmark will provide meaningful insights into the safety of AI systems. However, the v0.5 benchmark should not be used to assess the safety of AI systems. The authors have sought to fully document the limitations, flaws, and challenges of v0.5. This release of v0.5 of the AI Safety Benchmark includes (1) a principled approach to specifying and constructing the benchmark, which comprises use cases, types of systems under test (SUTs), language and context, personas, tests, and test items; (2) a taxonomy of 13 hazard categories with definitions and subcategories; (3) tests for seven of the hazard categories, each comprising a unique set of test items, i.e., prompts. There are 43,090 test items in total, which were created with templates; (4) a grading system for AI systems against the benchmark; (5) an openly available platform, and downloadable tool, called ModelBench that can be used to evaluate the safety of AI systems on the benchmark; (6) an example evaluation report which benchmarks the performance of over a dozen openly available chat-tuned language models; (7) a test specification for the benchmark. This is the [link to the project](https://github.com/mlcommons/modelbench).

3. ***CMU and Meta introduce TriForce, a hierarchical speculative decoding system addressing efficiency challenges in long-sequence generation with large language models. TriForce achieves impressive speedups and scalability for long-context generation tasks while maintaining generation quality. The code is available for further exploration.*** <br><br>
   18 Apr, CMU and Meta published a [paper](https://arxiv.org/pdf/2404.11912) “TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding”. The paper indicates that eith large language models (LLMs) widely deployed in long content generation recently, there has emerged an increasing demand for efficient long-sequence inference support. However, key-value (KV) cache, which is stored to avoid re-computation, has emerged as a critical bottleneck by growing linearly in size with the sequence length. Due to the auto-regressive nature of LLMs, the entire KV cache will be loaded for every generated token, resulting in low utilization of computational cores and high latency. While various compression methods for KV cache have been proposed to alleviate this issue, they suffer from degradation in generation quality. The study introduces TriForce, a hierarchical speculative decoding system that is scalable to long sequence generation. This approach leverages the original model weights and dynamic sparse KV cache via retrieval as a draft model, which serves as an intermediate layer in the hierarchy and is further speculated by a smaller model to reduce its drafting latency. TriForce not only facilitates impressive speedups for Llama2-7B-128K, achieving up to 2.31times on an A100 GPU but also showcases scalability in handling even longer contexts. For the offloading setting on two RTX 4090 GPUs, TriForce achieves 0.108s/tokenx2014 only half as slow as the auto-regressive baseline on an A100, which attains 7.78times on the optimized offloading system. Additionally, TriForce performs 4.86times more than DeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is highlighted by its consistently outstanding performance across various temperatures. The code is available at https://github.com/Infini-AI-Lab/TriForce.

4. ***Surge Global presents OpenBezoar, a family of small, cost-effective, and open models trained on mixes of instruction data. Leveraging techniques like RLHF and DPO, they fine-tune models based on OpenLLaMA 3Bv2, achieving superior performance on various tasks. The checkpoints and datasets are released for public use.*** <br><br>
   18 Apr, Surge Global published a [paper](https://arxiv.org/pdf/2404.12195) “OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data”. The authors find that Instruction fine-tuning pretrained LLMs for diverse downstream tasks has demonstrated remarkable success and has captured the interest of both academics and practitioners. To ensure such fine-tuned LLMs align with human preferences, techniques such as RLHF and DPO have emerged. At the same time, there is increasing interest in smaller parameter counts for models. In this work, using OpenLLaMA 3Bv2 as a base model, the study describes the recipe used to fine-tune the OpenBezoar family of models. In this recipe: the authors first generate synthetic instruction fine-tuning data using an open and commercially non-restrictive instruction fine-tuned variant of the Falcon-40B model under three schemes based on: LaMini-LM, WizardLM/Evol-Instruct (with databricks-dolly-15k as a seed dataset) and Orca (with the Flan Collection as a seed dataset), then filter these generations using GPT-4 as a human proxy. The research then performs cost-effective QLoRA-based supervised fine-tuning sequentially with each scheme. The resulting checkpoint is further fine-tuned with a subset of the HH-RLHF dataset to minimize distribution shift prior to using the DPO loss to obtain the final checkpoint. Evaluation is done with the LM Eval Harness tasks/metrics as well as on MT-Bench using the "LLM-as-a-judge" framework with Claude 2.1, with the finding that the final checkpoint, "OpenBezoar-HH-RLHF-DPO", demonstrates superior performance over many models at the 3B parameter scale, even outperforming the top model in one of the categories on the Huggingface Open LLM Leaderboard. The authors release "OpenBezoar-SFT", "OpenBezoar-HH-RLHF-SFT", "OpenBezoar-HH-RLHF-DPO" checkpoints, alongside the generated datasets on HuggingFace at [here](https://huggingface.co/collections/SurgeGlobal/open-bezoar-6620a24923e12127e9e2b9cc) and the codebase at [here](https://bitbucket.org/paladinanalytics/workspace/projects/OP).

5. ***IBM and Hebrew University introduce FastFit, a Python package for fast and accurate few-shot classification, particularly useful for scenarios with many semantically similar classes. FastFit significantly improves training speed and accuracy compared to existing methods, providing a user-friendly solution for NLP practitioners.*** <br><br>
   18 Apr, IBM and Hebrew University published a [paper](https://arxiv.org/pdf/2404.12365) “When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes”.  The authors present FastFit, a method, and a Python package design to provide fast and accurate few-shot classification, especially for scenarios with many semantically similar classes. FastFit utilizes a novel approach integrating batch contrastive learning and token-level similarity score. Compared to existing few-shot learning packages, such as SetFit, Transformers, or few-shot prompting of large language models via API calls, FastFit significantly improves multiclass classification performance in speed and accuracy across FewMany, a newly curated English benchmark, and Multilingual datasets. FastFit demonstrates a 3-20x improvement in training speed, completing training in just a few seconds. The FastFit package is now available on [GitHub](https://github.com/IBM/fastfit) and PyPi, presenting a user-friendly solution for NLP practitioners.

6. ***Google explores many-shot in-context learning with large language models, observing significant performance gains across various tasks. They investigate reinforced and unsupervised learning settings, finding both effective in the many-shot regime, particularly for complex reasoning tasks.*** <br><br>
   17 Apr, Google published a [paper](https://arxiv.org/pdf/2404.11018) “Many-Shot In-Context Learning”.  The paper finds that Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, the authors observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, the study explores two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. The researchers find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, the work demonstrates that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases and can learn high-dimensional functions with numerical inputs. The analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance.

7. ***UIUC demonstrates LLM agents' ability to autonomously exploit one-day vulnerabilities in real-world systems, with GPT-4 outperforming other models and vulnerability scanners. However, its performance relies heavily on the availability of CVE descriptions, raising concerns about the widespread deployment of highly capable LLM agents.*** <br><br>
    17 Apr, UIUC published a [paper](https://arxiv.org/pdf/2404.08144) “LLM Agents can Autonomously Exploit One-day Vulnerabilities”.  The paper finds that LLMs have becoming increasingly powerful, both in their benign and malicious uses. With the increase in capabilities, researchers have been increasingly interested in their ability to exploit cybersecurity vulnerabilities. In particular, recent work has conducted preliminary studies on the ability of LLM agents to autonomously hack websites. However, these studies are limited to simple vulnerabilities.  This work shows that LLM agents can autonomously exploit one-day vulnerabilities in real-world systems. To show this, the authors collected a dataset of 15 one-day vulnerabilities that include ones categorized as critical severity in the CVE description. When given the CVE description, GPT-4 is capable of exploiting 87% of these vulnerabilities compared to 0% for every other model tested (GPT-3.5, open-source LLMs) and open-source vulnerability scanners (ZAP and Metasploit). Fortunately, the GPT-4 agent requires the CVE description for high performance: without the description, GPT-4 can exploit only 7% of the vulnerabilities. The findings raise questions around the widespread deployment of highly capable LLM agents.

8. ***CMU, Nvidia, and Microsoft propose AgentKit, an intuitive prompting framework for multifunctional agents, offering a unified approach for constructing complex "thought processes" from simple prompts. AgentKit achieves state-of-the-art performance on various tasks and could make LLM agents more accessible for a wider range of applications.*** <br><br>
    17 Apr, CMU, Nvidia, and Microsoft published a [paper](https://arxiv.org/pdf/2404.11483v1) “AgentKit: Flow Engineering with Graphs, not Coding”. The authors propose an intuitive LLM prompting framework (AgentKit) for multifunctional agents. AgentKit offers a unified framework for explicitly constructing a complex "thought process" from simple natural language prompts. The basic building block in AgentKit is a node, containing a natural language prompt for a specific subtask. The user then puts together chains of nodes, like stacking LEGO pieces. The chains of nodes can be designed to explicitly enforce a naturally structured "thought process". For example, for the task of writing a paper, one may start with the thought process of 1) identify a core message, 2) identify prior research gaps, etc. The nodes in AgentKit can be designed and combined in different ways to implement multiple advanced capabilities including on-the-fly hierarchical planning, reflection, and learning from interactions. In addition, due to the modular nature and the intuitive design to simulate explicit human thought process, a basic agent could be implemented as simple as a list of prompts for the subtasks and therefore could be designed and tuned by someone without any programming experience. Quantitatively, the study shows that agents designed through AgentKit achieve SOTA performance on WebShop and Crafter. These advances underscore AgentKit's potential in making LLM agents effective and accessible for a wider range of applications. Code available at [this https URL](https://github.com/holmeswww/AgentKit).

9. ***AWS introduces Best-fit Packing, a method to improve language model training by packing documents into training sequences without excessive truncations. Empirical results show superior performance across various tasks compared to traditional concatenation methods.*** <br><br>
    16 Apr, AWS published a [paper](https://arxiv.org/pdf/2404.10830) “Fewer Truncations Improve Language Modeling”.  The authors state that in large language model training, input documents are typically concatenated together and then split into sequences of equal length to avoid padding tokens. Despite its efficiency, the concatenation approach compromises data integrity -- it inevitably breaks many documents into incomplete pieces, leading to excessive truncations that hinder the model from learning to compose logically coherent and factually consistent content that is grounded on the complete context. To address the issue, the work proposes Best-fit Packing, a scalable and efficient method that packs documents into training sequences through length-aware combinatorial optimization. The method completely eliminates unnecessary truncations while retaining the same training efficiency as concatenation. Empirical results from both text and code pre-training show that the proposed method achieves superior performance (e.g., relatively +4.7% on reading comprehension; +16.8% in context following; and +9.2% on program synthesis), and reduces closed-domain hallucination effectively by up to 58.3%.

10. ***Princeton Uni introduces the USACO benchmark for evaluating language models in competitive programming, revealing challenges in achieving high accuracy. Human-in-the-loop studies demonstrate the potential of targeted hints to improve model performance, paving the way for future advancements in grounded reasoning.*** <br><br>
    16 Apr, Princeton Uni published a [paper](https://arxiv.org/pdf/2404.10952) “Can Language Models Solve Olympiad Programming?”. The researchers find that computing olympiads contain some of the most challenging problems for humans, requiring complex algorithmic reasoning, puzzle solving, in addition to generating efficient code. However, it has been understudied as a domain to evaluate language models (LMs). The work introduces the USACO benchmark with 307 problems from the USA Computing Olympiad, along with high-quality unit tests, reference code, and official analyses for each problem. These resources enable the authors to construct and test a range of LM inference methods for competitive programming for the first time. It is found GPT-4 only achieves a 8.7% pass@1 accuracy with zero-shot chain-of-thought prompting, and the best inference method improves it to 20.2% using a combination of self-reflection and retrieval over episodic knowledge. However, this is far from solving the benchmark. To better understand the remaining challenges, the researchers design a novel human-in-the-loop study and surprisingly find that a small number of targeted hints enable GPT-4 to solve 13 out of 15 problems previously unsolvable by any model and method. The benchmark, baseline methods, quantitative results, and qualitative analysis serve as an initial step toward LMs with grounded, creative, and algorithmic reasoning. Code and data are at [this link](https://princeton-nlp.github.io/USACOBench/).

11. ***The UK government is crafting legislation to regulate artificial intelligence, particularly focusing on large language models, amid concerns about potential harms and the dominance of tech companies in AI development. The legislation aims to mandate safety testing and algorithm sharing while avoiding stifling industry growth.*** <br><br>
    15 Apr, according to [Financial Time](https://www.ft.com/content/311b29a4-bbb3-435b-8e82-ae19f2740af9), The UK government is beginning to craft new legislation to regulate artificial intelligence, months after the prime minister vowed “not to rush” setting up rules for the fast-growing technology. Such legislation would likely put limits on the production of large language models, the general-purpose technology that underlies AI products such as OpenAI’s ChatGPT. It would likely look to mandate that companies developing the most sophisticated models share their algorithms with the government and provide evidence that they have carried out safety testing. The plans come as regulators including the UK competition watchdog have become increasingly concerned about potential harms. These range from the possibility that technology could bake in biases that affect certain demographics, to the potential use of general-purpose models to create harmful materials. It is said the rules would apply to the large language models that sit behind AI products such as ChatGPT, rather than the applications themselves. UK’s Competition and Markets Authority officer expressed “real concerns” that a small number of tech companies creating AI foundation models “may have both the ability and the incentive to shape these markets in their own interest”. The regulator identified an “interconnected web” of more than 90 partnerships and strategic investments involving the same companies: Google, Apple, Microsoft, Meta, Amazon and Nvidia.  The UK has been reluctant to push for legal interventions in the development and rollout of AI models for fear tough regulation might stymie industry growth. It has instead relied on voluntary agreements with governments and companies, ruling out legislation in the short term. Government officials singled out so-called “general purpose” AI models — those that are highly intelligent and adaptable for use on a wide range of tasks — as likely targets for further legal and regulatory intervention in a recent consultation response.

12. ***Tinkoff.AI proposes Trust Region DPO as a new alignment method for language models, outperforming existing methods like DPO on various datasets. The approach improves model quality across multiple parameters, addressing challenges in alignment stability.*** <br><br>
    15 Apr, Tinkoff.AI published a [paper](https://arxiv.org/pdf/2404.09656) “Learn Your Reference Model for Real Good Alignment”.  The paper states that the complexity of the alignment problem stems from the fact that existing methods are unstable. Researchers continuously invent various tricks to address this shortcoming. For instance, in the fundamental Reinforcement Learning From Human Feedback (RLHF) technique of Language Model alignment, in addition to reward maximization, the Kullback-Leibler divergence between the trainable policy and the SFT policy is minimized. This addition prevents the model from being overfitted to the Reward Model (RM) and generating texts that are out-of-domain for the RM. The Direct Preference Optimization (DPO) method reformulates the optimization task of RLHF and eliminates the Reward Model while tacitly maintaining the requirement for the policy to be close to the SFT policy. This paper argues that this implicit limitation in the DPO method leads to sub-optimal results. The authors propose a new method called Trust Region DPO (TR-DPO), which updates the reference policy during training. With such a straightforward update, the study demonstrates the effectiveness of TR-DPO against DPO on the Anthropic HH and TLDR datasets. The researchers show that TR-DPO outperforms DPO by up to 19%, measured by automatic evaluation with GPT-4. The new alignment approach allows the authors to improve the quality of models across several parameters at once, such as coherence, correctness, level of detail, helpfulness, and harmlessness.

13. ***Stanford University releases the 2024 Artificial Intelligence Index Report, highlighting key trends and challenges in AI development, including AI's impact on productivity, the need for robust evaluations, and increasing regulatory scrutiny.*** <br><br>
    15 Apr, Stanford University released its [annual AI report](https://aiindex.stanford.edu/report/), the 2024 Artificial Intelligence Index Report. Top takeaways include AI beats humans on some tasks, but on all; Industry continues to dominate frontier AI research; Frontier models get way more expensive; The United States leads China, the EU, and the U.K. as the leading source of top AI models; Robust and standardized evaluations for LLM responsibility are seriously lacking; Generative AI investment skyrockets; The data is in: AI makes workers more productive and leads to higher quality work; Scientific progress accelerates even further, thanks to AI; The number of AI regulations in the United States sharply increases; People across the globe are more cognizant of AI’s potential impact—and more nervous.

14. ***UC Berkeley presents GoEX, a runtime for autonomous LLM applications, aiming to facilitate efficient collaboration between humans and LLMs by enabling post-facto validation. GoEX integrates an undo feature and damage confinement strategies to mitigate risks associated with LLM-generated outputs, unlocking their potential for real-world applications with minimal human supervision.*** <br><br>
    10 Apr, UC Berkeley published a [paper](https://arxiv.org/pdf/2404.06921) “GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications”.  The paper indicates that Large Language Models (LLMs) are evolving beyond their classical role of providing information within dialogue systems to actively engaging with tools and performing actions on real-world applications and services. Today, humans verify the correctness and appropriateness of the LLM-generated outputs (e.g., code, functions, or actions) before putting them into real-world execution. This poses significant challenges as code comprehension is well known to be notoriously difficult. This paper studies how humans can efficiently collaborate with, delegate to, and supervise autonomous LLMs in the future. The authors argue that in many cases, "post-facto validation" - verifying the correctness of a proposed action after seeing the output - is much easier than the aforementioned "pre-facto validation" setting. The core concept behind enabling a post-facto validation system is the integration of an intuitive undo feature, and establishing a damage confinement for the LLM-generated actions as effective strategies to mitigate the associated risks. Using this, a human can now either revert the effect of an LLM-generated output or be confident that the potential risk is bounded. The researchers believe this is critical to unlock the potential for LLM agents to interact with applications and services with limited (post-facto) human involvement. The paper describes the design and implementation of the open-source runtime for executing LLM actions, Gorilla Execution Engine (GoEX), and present open research questions towards realizing the goal of LLMs and applications interacting with each other with minimal human supervision. GoEX is released at [this https URL](https://github.com/ShishirPatil/gorilla/).
 <br><br><br>
**14 Apr 2024**

1. ***XAI introduced Grok-1.5 Vision, their first-generation multimodal model, capable of processing various visual information alongside text. The upcoming release, available to early testers and existing users, showcases competitive performance across domains like multi-disciplinary reasoning and spatial understanding, emphasizing XAI's commitment to advancing multimodal capabilities.*** <br><br>
   12 Apr, [XAI released Grok-1.5 Vision preview](https://x.ai/blog/grok-1.5v), XAI’s first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to the early testers and existing Grok users. Grok-1.5V is competitive with existing frontier multimodal models in a number of domains, ranging from multi-disciplinary reasoning to understanding documents, science diagrams, charts, screenshots, and photographs. Grok also exhibits capabilities in understanding physical world. Grok outperforms its peers in the new RealWorldQA benchmark that measures real-world spatial understanding. XAI believes that advancing both multimodal understanding and generation capabilities are important steps in building beneficial AGI that can understand the universe. In the coming months, XAI anticipates to make significant improvements in both capabilities, across various modalities such as images, audio, and video.

2. ***Forbes.com unveiled its AI 50 2024 list, highlighting the rapid growth of AI-powered applications and the emergence of a new tech economy supporting businesses in adopting AI. Major players like OpenAI, Anthropic, and Databricks dominate the landscape, with significant funding and diverse applications spanning industries from finance to healthcare.*** <br><br>
   11 Apr, Forbes.com released [AI 50 2024](https://www.forbes.com/lists/ai50/?sh=2a11e72c290f). By the spring of 2023, the massive popularity of apps like ChatGPT had prompted a mass scramble among businesses trying to implement the latest advances in generative artificial intelligence. One year later, the craze continues. In turn, a new tech economy has emerged to help businesses develop and deploy AI-powered apps. The use cases are wide-ranging and far-reaching, as immediately evident from the three largest companies on the list in terms of valuation. Model maker OpenAI ($86 billion) counts customers from Morgan Stanley to the government of Iceland, while its rival Anthropic ($18.4 billion, as Forbes reported) is used by Bridgewater and the Boston Consulting Group. Databricks ($43 billion) sells its data analytics and AI deployment software to Shell and the United States Postal Service. For the startups on AI 50, the technology has evolved from capturing customers’ imaginations to capturing billions of dollars in collective revenue.  The companies on this year’s AI 50 have raised a total of $34.7 billion in funding. Nearly one-third of that total comes from OpenAI, thanks to some $10 billion from Microsoft. Much more comes from other ascendant AI research firms like Anthropic ($7.7 billion raised), Cohere ($445 million) and Mistral AI ($528 million). Underlying them are a slew of infrastructure tools that are helping companies to implement the technology. Other forms of AI development are seeing traction too. Take Anduril, which has raised $2.8 billion for defense tech; Insitro, which stockpiled a $643 million cash pile for drug discovery; or Figure AI, which raised $754 million to create humanoid robots. Then, there are companies that are seamlessly layering the latest advances in AI into their own apps. Abridge uses voice recognition and language summarization to deliver automated documentation of your visit to the doctor’s office. Notion is making inroads into uprooting Google Workspace or Microsoft Office, while Perplexity wants to reinvent the search engine.

3. ***Yann LeCun, Meta's Chief AI Scientist, challenges the prevailing enthusiasm for generative AI, advocating for a shift towards Objective-Driven AI at Meta AI Day in London. LeCun emphasizes the limitations of generative AI in understanding context and engaging with the physical world, proposing AI systems capable of causal reasoning and intuitive understanding.*** <br><br>
   11 Apr, Forbes [published a blog](https://www.forbes.com/sites/bernardmarr/2024/04/12/generative-ai-sucks-metas-chief-ai-scientist-calls-for-a-shift-to-objective-driven-ai/?sh=31d7bf6bb82b) “Generative AI Sucks: Meta’s Chief AI Scientist Calls For A Shift To Objective-Driven AI”. Yann LeCun, Meta's Chief AI Scientist, challenges the prevailing enthusiasm for generative AI at Meta AI Day in London. He argues that despite its ability to mimic human creativity, generative AI falls short in genuinely understanding context or engaging with the physical world. LeCun contrasts it with the adaptive learning mechanisms of humans and animals, emphasizing their intuitive understanding of the world and the nuanced grasp of physical laws. He highlights generative AI's limitations in providing factual accuracy and common-sense understanding, proposing a shift towards Objective-Driven AI. LeCun envisions AI systems with rich internal representations capable of causal reasoning and understanding relationships between actions and outcomes. Despite acknowledging the scientific and technical challenges ahead, LeCun remains optimistic about AI's potential to surpass human intelligence across all domains. He calls for a philosophical and technical revaluation within the AI research community, urging a focus on building AI systems capable of truly understanding and interacting with the world.

4. ***UC Berkeley introduces LLoCO, a novel approach to address the challenge of processing long contexts for large language models (LLMs). LLoCO enables efficient retrieval of relevant information and significantly improves long document question answering while reducing computational overhead, presenting a promising solution for efficient long context processing.*** <br><br>
   11 Apr, UC Berkeley published a [paper](https://arxiv.org/pdf/2404.07979) “LLoCO: Learning Long Contexts Offline”. The paper indicates that processing long contexts remains a challenge for large language models (LLMs) due to the quadratic computational and memory overhead of the self-attention mechanism and the substantial KV cache sizes during generation. The study proposes a novel approach to address this problem by learning contexts offline through context compression and in-domain parameter-efficient finetuning. The proposed method enables an LLM to create a concise representation of the original context and efficiently retrieve relevant information to answer questions accurately. The authors introduce LLoCO, a technique that combines context compression, retrieval, and parameter-efficient finetuning using LoRA. The approach extends the effective context window of a 4k token LLaMA2-7B model to handle up to 128k tokens. The research evaluates the proposed approach on several long-context question-answering datasets, demonstrating that LLoCO significantly outperforms in-context learning while using 30times fewer tokens during inference. LLoCO achieves up to 7.62times speed-up and substantially reduces the cost of long document question answering, making it a promising solution for efficient long context processing. The code is publicly available at https://github.com/jeffreysijuntan/lloco.

5. ***University of Arizona and TUCN examine the regression capabilities of pre-trained large language models (LLMs) without additional training. Findings reveal that LLMs can perform regression tasks comparably to traditional supervised methods, highlighting their potential for diverse applications beyond natural language processing.*** <br><br>
    11 Apr, Uni of Arizona and TUCN published a [paper](https://arxiv.org/pdf/2404.07544) “From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples”. The authors analyze how well pre-trained large language models (e.g., Llama2, GPT-4, Claude 3, etc) can do linear and non-linear regression when given in-context examples, without any additional training or gradient updates. The researchers’ findings reveal that several large language models (e.g., GPT-4, Claude 3) are able to perform regression tasks with a performance rivaling (or even outperforming) that of traditional supervised methods such as Random Forest, Bagging, or Gradient Boosting. For example, on the challenging Friedman #2 regression dataset, Claude 3 outperforms many supervised methods such as AdaBoost, SVM, Random Forest, KNN, or Gradient Boosting. The authors then investigate how well the performance of large language models scales with the number of in-context exemplars. The researchers borrow from the notion of regret from online learning and empirically show that LLMs are capable of obtaining a sub-linear regret. Code is [available here](https://github.com/robertvacareanu/llm4regression).

6. ***Google explores best practices and lessons learned on synthetic data for language models, emphasizing the importance of quality, diversity, and responsible use. The study addresses challenges and limitations of synthetic data while proposing future directions for scalable and efficient oversight in synthetic data generation*** <br><br>
    11 Apr, Google published a [paper](https://arxiv.org/pdf/2404.07503) “Best Practices and Lessons Learned on Synthetic Data for Language Models”. The authors state that he success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs. Synthetic data has emerged as a promising solution by generating artificial data that mimics real-world patterns. This paper provides an overview of synthetic data research, discussing its applications, challenges, and future directions. The authors present empirical evidence from prior art to demonstrate its effectiveness and highlight the importance of ensuring its factuality, fidelity, and unbiasedness. The study emphasizes the need for responsible use of synthetic data to build more powerful, inclusive, and trustworthy language models. Challenges and limitations of synthetic data include the misuse of synthetic data might proliferate misinformation; synthetic data might cause ambiguity in AI alignment; and training with synthetic data makes evaluation decontamination harder. Future directions are synthetic data scaling, further improving quality and diversity of synthetic data, towards high-fidelity and more efficient scalable oversight, and the emergent self-improvement capability.

7. ***Google explores best practices and lessons learned on synthetic data for language models, emphasizing the importance of quality, diversity, and responsible use. The study addresses challenges and limitations of synthetic data while proposing future directions for scalable and efficient oversight in synthetic data generation*** <br><br>
    11 Apr, Google published a [paper](https://arxiv.org/pdf/2404.07839) “RecurrentGemma: Moving Past Transformers for Efficient Open Language Models”. RecurrentGemma is an open language model which uses Google's novel Griffin architecture. Griffin combines linear recurrences with local attention to achieve excellent performance on language. It has a fixed-sized state, which reduces memory use and enables efficient inference on long sequences. Google provides a pre-trained model with 2B non-embedding parameters, and an instruction tuned variant. Both models achieve comparable performance to Gemma-2B despite being trained on fewer tokens. The project repository is available here. This repository contains the model implementation and examples for sampling and fine-tuning. There are two implementations, Google’s Flax and Meta’s Pytorch, the former is highly optimized.

8. ***Anthropic announces advancements in the persuasiveness of its language models, highlighting the potential social, commercial, and political implications. While the research underscores the persuasive capabilities of AI, concerns remain about real-world applications and potential effects on contentious debates.*** <br><br>
    10 Apr, [according to axios.com](https://www.axios.com/2024/04/10/anthropic-claude-persuasion-turing-test), AI startup Anthropic says its language models have steadily and rapidly improved in their "persuasiveness," per new research the company posted Tuesday. Persuasion — a general skill with widespread social, commercial and political applications — can foster disinformation and push people to act against their own interests, according to the paper's authors. However, there's relatively little research on how the latest models compare to humans when it comes to their persuasiveness. The researchers found "each successive model generation is rated to be more persuasive than the previous," and that the most capable Anthropic model, Claude 3 Opus, "produces arguments that don't statistically differ" from arguments written by humans. A wider debate has been raging about when AI will outsmart humans. AI has arguably "outsmarted" humans for some specific tasks in highly controlled environments. Elon Musk predicted Monday that AI will outsmart the smartest human by the end of 2025. Anthropic researchers developed "a basic method to measure persuasiveness" and used it to compare three different generations of models (Claude 1, 2, and 3), and two classes of models (smaller models and bigger "frontier models"). While the researchers were surprised that the AI was as persuasive as it turned out to be, they also chose to focus on "less polarized issues." Those issues ranged from potential rules for space exploration to appropriate uses of AI-generated content. While that allowed the researchers to dive deep into issues where many people are open to persuasion, it means we still don't have a clear idea — in an election year — of the potential effect of AI chatbots on today's most contentious debates. "Persuasion is difficult to study in a lab setting," the researchers warned in the report. "Our results may not transfer to the real world." Anthropic considers this the start of a long line of research into the emerging capabilities of its models.

9. ***Rutgers University explores how large language models (LLMs) acquire knowledge at different layers, revealing insights into the learning processes and internal representations of LLMs. The study sheds light on the classification efficiency of LLMs for tasks of varying complexity, providing implications for model development and understanding.*** <br><br>
    10 Apr, Rutgers Uni, inter alia published a [paper](https://arxiv.org/pdf/2404.07066) “Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers”. This paper studies the phenomenon that different concepts are learned in different layers of large language models, i.e. more difficult concepts are fully acquired with deeper layers. The study defines the difficulty of concepts by the level of abstraction, and here it is crudely categorized by factual, emotional, and inferential. Each category contains a spectrum of tasks, arranged from simple to complex. For example, within the factual dimension, tasks range from lie detection to categorizing mathematical problems. The authors employ a probing technique to extract representations from different layers of the model and apply these to classification tasks. The findings reveal that models tend to efficiently classify simpler tasks, indicating that these concepts are learned in shallower layers. Conversely, more complex tasks may only be discernible at deeper layers, if at all. This paper explores the implications of these findings for our understanding of model learning processes and internal representations. The implementation is available at here

10. ***French AI startup Mistral releases Mixtral 8x22B, a new large language model (LLM) aimed at competing with industry leaders. With improved context window and parameter size, Mixtral 8x22B offers enhanced performance, showcasing the ongoing competition and innovation in the AI arena.*** <br><br>
    10 Apr,  [According to ZDNET](https://www.zdnet.com/article/ai-startup-mistral-launches-a-281gb-ai-model-to-rival-openai-meta-and-google/), French AI startup Mistral on Tuesday released Mixtral 8x22B, a new large language model (LLM) and its latest attempt to compete with the big boys in the AI arena. Mixtral 8x22B is expected to outperform Mistral's previous Mixtral 8x7B LLM, which itself showed signs of outshining OpenAI's GPT-3.5 and Meta's Llama 2. The new Mixtral model boasts a 65,000-token context window, which refers to the amount of text that an AI model can process and reference at one time. Further, Mixtral 8x22B has a parameter size of up to 176 billion, a reference to the number of internal variables that the model uses to make decisions or predictions. Mixtral 8x22B is available for anyone to use after [downloading a 281GB file](https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1/tree/main). On the same data, OpenAI released GPT-4 Turbo with Vision, the latest GPT-4 Turbo model with vision capabilities for working with photographs, drawings, and other images uploaded by the user. Google released its advanced Gemini Pro 1.5 LLM to developers with a free option that grants up to 50 requests per day. Not to be outdone, Meta revealed that its Llama 3 model would debut later this month.
11. ***Google introduces a paper on "Leave No Context Behind," presenting an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. The proposed approach incorporates a new attention technique called Infini-attention, demonstrating effectiveness on long-context language modeling benchmarks and enabling fast streaming inference for LLMs.*** <br><br>
    10 Apr, Google published a [paper](https://arxiv.org/abs/2404.07143) "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention". This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in the proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. The authors demonstrate the effectiveness of the approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. The proposed approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.

12. ***EleutherAI presents Eagle and Finch, sequence models enhancing the RWKV architecture, introducing multi-headed matrix-valued states and dynamic recurrence mechanisms. These advancements improve expressivity while maintaining inference efficiency, resulting in competitive performance across various benchmarks. Additionally, a new multilingual corpus and fast tokenizer enhance multilinguality, with all models released under the Apache 2.0 license.*** <br><br>
    10 Apr, EleutherAI, inter alia published a [paper](https://arxiv.org/pdf/2404.05892) “Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence”. The authors present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. The architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. The paper introduces a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. The researchers trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. The authors release all the models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer

13. ***McGill University, ServiceNow, and Facebook unveil LLM2Vec, a simple unsupervised approach to transform any decoder-only LLM into a powerful text encoder. By enabling bidirectional attention, masked next token prediction, and unsupervised contrastive learning, LLM2Vec achieves state-of-the-art performance on word-level tasks and the Massive Text Embeddings Benchmark (MTEB), offering a parameter-efficient solution for universal text encoding.*** <br><br>
    9 Apr, McGill Uni, ServiceNow and Facebook published a [paper](https://arxiv.org/pdf/2404.05961) “LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders”. The study finds that Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. This work introduces LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. The authors demonstrate the effectiveness of LLM2Vec by applying it to 3 popular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks. The proposed models outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, the authors achieve state-of-the-art performance on MTEB among models that train only on publicly available data. The strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data. Code and pre-trained models are [available here](https://github.com/McGill-NLP/llm2vec).

14. ***Symbolica introduces a groundbreaking approach to AI modeling based on category theory, aiming to bring rigorous scientific principles to AI development. By prioritizing interpretability and reasoning capabilities, Symbolica seeks to revolutionize AI, potentially paving the way for artificial general intelligence. Despite challenges, Symbolica's focus on rigor could reshape the AI landscape, supported by collaborations with industry experts and significant funding.*** <br><br>
    9 Apr, according to [Venturebeat.com](https://venturebeat.com/ai/move-over-deep-learning-symbolicas-structured-approach-could-transform-ai/), Symbolica, an AI startup, has unveiled a groundbreaking approach to AI modeling, grounded in category theory, a branch of mathematics. This approach aims to bring rigorous scientific principles to AI, moving beyond the current "alchemy" of deep learning. By building models with inherent reasoning capabilities and transparency, Symbolica intends to revolutionize AI, making it interpretable and less reliant on vast amounts of data and computing power. Led by former Tesla engineer George Morgan, Symbolica has attracted significant funding and collaboration with industry experts like Stephen Wolfram. While facing challenges ahead, Symbolica's focus on rigor and interpretability could reshape the landscape of AI, potentially paving the way for artificial general intelligence. The research [paper](https://arxiv.org/abs/2402.15332) published by Symbolica and Google in the area of categorical area is "Categorical Deep Learning: An Algebraic Theory of Architectures".

15. ***IEEE Spectrum discusses the evolution of AI-powered coding tools, highlighting advancements like Devin AI and AutoDev in offering more autonomous assistance to developers. While these tools accelerate coding processes, concerns remain about safety, reliability, and the need for human oversight. Collaboration between humans and AI is essential for iterative improvement, as AI tools currently lack the intuitive understanding inherent in human developers.*** <br><br>
    9 Apr, IEEE Spectrum published an [article](https://spectrum.ieee.org/ai-code-generator) "AI Coding Is Going From Copilot to Autopilot - But so far Devin AI and others still really need humans". The emergence of a new wave of AI-powered coding tools, including Devin AI and AutoDev, promises to revolutionize software development by offering more autonomous assistance than previous iterations like GitHub Copilot and Amazon CodeWhisperer. These tools can build websites, find and fix bugs, and even train their own language models. Open-source alternatives have also emerged, reflecting the growing demand for generative AI tools among developers. While these coding assistants can accelerate coding processes and offer templates for code, concerns about safety, reliability, and the need for human oversight remain. Developers must ensure that AI-generated code meets rigorous quality standards, considering potential security vulnerabilities and corner cases that could lead to crashes.  Despite the potential of AI coding assistants to enhance productivity, they are still in early stages, with limited capabilities and reliability compared to human developers. Collaboration between humans and AI is crucial for iterative improvement, as AI tools lack the intuitive understanding and creativity inherent in human developers. While the future role of unassisted AI coders in software development remains uncertain, developers are encouraged to embrace these tools and evaluate their benefits firsthand. Continuous monitoring and improvement are essential as AI technology evolves rapidly. Ultimately, developers bear the responsibility of understanding and validating AI-generated code to ensure its security, reliability, and maintainability.

16. ***Google releases CodeGemma, a family of code-specialist LLM models trained on additional data to improve logical and mathematical reasoning. With models optimized for code completion, understanding, and generation, CodeGemma achieves competitive performance across various programming languages, offering enhanced capabilities for developers.*** <br><br>
    9 Apr, Google [released CodeGemma](https://huggingface.co/collections/google/codegemma-release-66152ac7b683e2667abdee11), a family of code-specialist LLM models by Google, based on the pre-trained 2B and 7B Gemma checkpoints. CodeGemma are further trained on an additional 500 billion tokens of primarily English language data, mathematics, and code to improve on logical and mathematical reasoning, and are suitable for code completion and generation. CodeGemma 2B was trained exclusively on Code Infilling and is meant for fast code completion and generation, especially in settings where latency and/or privacy are crucial. CodeGemma 7B training mix includes code infilling data (80%) and natural language. It can be used for code completion, as well as code and language understanding and generation. CodeGemma 7B Instruct was fine-tuned for instruction following on top of CodeGemma 7B. It’s meant for conversational use, especially around code, programming, or mathematical reasoning topics. All the models have the same 8K token context size as their predecessors. CodeGemma-7B outperforms similarly-sized 7B models except DeepSeek-Coder-7B on HumanEval, a popular benchmark for evaluating code models on Python. The same goes for the evaluation of other programming languages like Java, JavaScript, and C++ from MultiPL-E, a translation of HumanEval. According to the technical report, the model performs best on GSM8K among 7B models. The instruct version CodeGemma-7B-it improves on the most popular languages on both HumanEval and MBPP. A demo can be [found here](https://huggingface.co/blog/codegemma).

17. ***Meta publishes a paper on knowledge capacity scaling laws for language models, estimating the number of knowledge bits a model stores and examining factors influencing knowledge storage capacity. Insights include the impact of training duration, model architecture, quantization, and data signal-to-noise ratio, shedding light on language model capabilities and training paradigms.*** <br><br>
    8 Apr, Meta published a [paper](https://arxiv.org/pdf/2404.05405) “Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws”. The paper indicates that scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, this paper estimates the number of knowledge bits a model stores. The authors focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, the study establishes that language models can and only can store 2 bits of knowledge per parameter, even when quantized to int8, and such knowledge can be flexibly extracted for downstream applications. Consequently, a 7B model can store 14B bits of knowledge, surpassing the English Wikipedia and textbooks combined based on our estimation. More broadly, the study presents 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity. Notable insights include: * The GPT-2 architecture, with rotary embedding, matches or even surpasses LLaMA/Mistral architectures in knowledge storage, particularly over shorter training durations. This arises because LLaMA/Mistral uses GatedMLP, which is less stable and harder to train. * Prepending training data with domain names (e.g., this http URL) significantly increases a model's knowledge capacity. Language models can autonomously identify and prioritize domains rich in knowledge, optimizing their storage capacity.

18. ***University of Tubingen, Cambridge, and Oxford investigate the influence of pretraining concept frequency on multimodal model performance. The study reveals an exponential need for training data to achieve linear improvements in downstream "zero-shot" performance, challenging the notion of "zero-shot" generalization for multimodal models and highlighting the importance of large-scale training paradigms.***<br><br>
    8 Apr, Uni of Tubingen, Cambridge and Oxford published a [paper](https://arxiv.org/pdf/2404.04125) “No “Zero-Shot” Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance”. The paper finds that Web-crawled pretraining datasets underlie the impressive "zero-shot" evaluation performance of multimodal models, such as CLIP for classification/retrieval and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of "zero-shot" generalization is for such multimodal models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during "zero-shot" evaluation. This work asks: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets? The study comprehensively investigates this question across 34 models and five standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. The authors consistently find that, far from exhibiting "zero-shot" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream "zero-shot" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon benchmarking models on long-tailed data sampled based on analysis, the paper demonstrates that multimodal models across the board perform poorly. The authors contribute this long-tail test set as the "Let it Wag!" benchmark to further research in this direction. Taken together, the study reveals an exponential need for training data which implies that the key to "zero-shot" generalization capabilities under large-scale training paradigms remains to be found. Here is the links for the [code](https://github.com/bethgelab/frequency_determines_performance) and [data](https://huggingface.co/datasets/bethgelab/Let-It-Wag).

19. ***SambaNova Systems presents a comprehensive investigation into adapting LLMs to new languages, covering vocabulary extension, direct preference optimization, and data scarcity challenges. Experimenting across multiple languages and parameter scales, the study achieves competitive performance, outperforming existing baselines and offering insights for future research.*** <br><br>
    8 Apr, SambaNova Systems published a [paper](https://arxiv.org/pdf/2404.05829) “SambaLingo: Teaching Large Language Models New Languages”. The authors argue that despite the widespread availability of LLMs, there remains a substantial gap in their capabilities and availability across diverse languages. One approach to address these issues has been to take an existing pre-trained LLM and continue to train it on new languages. While prior works have experimented with language adaptation, many questions around best practices and methodology have not been covered. This study presents a comprehensive investigation into the adaptation of LLMs to new languages. The study covers the key components in this process, including vocabulary extension, direct preference optimization and the data scarcity problem for human alignment in low-resource languages. The authors scale these experiments across 9 languages and 2 parameter scales (7B and 70B). The researchers compare the models against Llama 2, Aya-101, XGLM, BLOOM and existing language experts, outperforming all prior published baselines. Additionally, all evaluation code and [checkpoints](https://huggingface.co/collections/sambanovasystems/) are made public to facilitate future research.

20. ***Microsoft introduces Direct Nash Optimization (DNO), a scalable algorithm for post-training LLMs using general preferences to iteratively improve model performance. With straightforward implementation and efficient batched on-policy learning, DNO achieves state-of-the-art results, outperforming models with far more parameters and demonstrating monotonic improvement across iterations.*** <br><br>
    4 Apr, Microsoft published a [paper](https://arxiv.org/pdf/2404.03715) “Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences”. This paper studies post-training large language models (LLMs) using preference feedback from a powerful oracle to help a model iteratively improve over itself. The typical approach for post-training LLMs involves Reinforcement Learning from Human Feedback (RLHF), which traditionally separates reward learning and subsequent policy optimization. However, such a reward maximization approach is limited by the nature of "point-wise" rewards (such as Bradley-Terry model), which fails to express complex intransitive or cyclic preference relations. While advances on RLHF show reward learning and policy optimization can be merged into a single contrastive objective for stability, they yet still remain tethered to the reward maximization framework. Recently, a new wave of research sidesteps the reward maximization presumptions in favor of directly optimizing over "pair-wise" or general preferences. This study introduces Direct Nash Optimization (DNO), a provable and scalable algorithm that marries the simplicity and stability of contrastive learning with theoretical generality from optimizing general preferences. Because DNO is a batched on-policy algorithm using a regression-based objective, its implementation is straightforward and efficient. Moreover, DNO enjoys monotonic improvement across iterations that help it improve even over a strong teacher (such as GPT-4). In authors’ experiments, a resulting 7B parameter Orca-2.5 model aligned by DNO achieves the state-of-the-art win-rate against GPT-4-Turbo of 33% on AlpacaEval 2.0 (even after controlling for response length), an absolute gain of 26% (7% to 33%) over the initializing model. It outperforms models with far more parameters, including Mistral Large, Self-Rewarding LM (70B parameters), and older versions of GPT-4.
<br><br><br>
    


**7 Apr 2024**

1. ***Reuters Article - "Inside Big Tech's underground race to buy AI training data"<br>
   This article explores the evolving landscape of AI training data acquisition, with a focus on tech giants like Google, Meta, and Microsoft-backed OpenAI. Initially reliant on freely scraped internet data, these companies have shifted towards securing data through agreements with content owners and utilizing data brokers to address legal and ethical concerns. Key points include the emergence of AI data firms ensuring ethical sourcing, concerns regarding user privacy, regulatory scrutiny, and potential acquisitions of news media companies by tech giants to control data IP.***  <br><br>
   6 Apr, [Reuters published an article](https://www.reuters.com/technology/inside-big-techs-underground-race-buy-ai-training-data-2024-04-05/) “Inside Big Tech's underground race to buy AI training data”. The article discusses the evolving landscape of AI training data acquisition, particularly focusing on tech giants like Google, Meta, and Microsoft-backed OpenAI. Initially, these companies relied on freely scraped internet data to train generative AI models like ChatGPT, leading to legal and ethical concerns and lawsuits from copyright holders. Simultaneously, they are discreetly paying for access to content behind paywalls and login screens, sparking a hidden market for various data types. Some key points include: 1) Data Acquisition Methods: Tech companies initially used scraped web page data for AI training, leading to copyright issues and regulatory scrutiny. In response, they're now securing data through agreements with content owners and utilizing data brokers. 2) Content Acquisition Deals: Companies like Google, Meta, Amazon, and Apple are striking agreements with content providers like Shutterstock and Freepik to access vast libraries of images, videos, and text for training AI models. The size of these deals can range from tens of millions to undisclosed amounts. 3) Emergence of AI Data Firms: Dedicated AI data firms are securing rights to various content types, ensuring ethical sourcing and anonymizing personal data. Rates vary depending on content type, with emphasis on obtaining consent and stripping personally identifying information. 4) Ethical and Legal Concerns: Despite licensing agreements, concerns persist regarding user privacy, especially regarding old internet archives like Photobucket. Instances of AI regurgitating exact copies of training data raise questions about consent and notice. 5) Regulatory Scrutiny: Reddit's data-licensing business is under investigation by the U.S. Federal Trade Commission, highlighting the potential legal pitfalls of AI data acquisition, including privacy and intellectual property issues. Comment: We may expect the tech giants may acquire more shares of the news media companies to control the IP of the data.

2. ***VentureBeat Report - "OpenAI releases new AI fine-tuning tools" <br>
   OpenAI has introduced new AI fine-tuning tools aimed at providing developers with greater control over model customization and construction. These tools include epoch-based checkpoint creation, comparative Playground UI for side-by-side evaluations, third-party integration, and comprehensive validation metrics. These updates enable organizations to maximize model performance through advanced techniques and bespoke parameters, leading to personalized AI models becoming standard in the future.*** <br><br>
   4 Apr, [according to VentureBeat](https://venturebeat.com/ai/openai-releases-new-ai-fine-tuning-tools-vast-majority-of-organizations-will-develop-customized-models/), OpenAI releases new AI fine-tuning tools: ‘vast majority of organizations will develop customized models’. These updates are set to empower developers with unprecedented control over AI model fine-tuning, while also offering new avenues for constructing custom models tailored to specific business needs. The latest API improvements include epoch-based checkpoint creation, which minimizes the need for retraining and mitigates overfitting risks. Additionally, a new comparative Playground UI facilitates side-by-side evaluations of model outputs, enhancing the development process with human insights. These updates, along with third-party integration starting with Weights and Biases, and comprehensive validation metrics, mark a significant leap forward in fine-tuning technology. Assisted fine-tuning represents a collaborative effort between OpenAI’s technical teams and organizations, aiming to maximize model performance through advanced techniques and bespoke parameters. SK Telecom experienced notable improvements in customer service performance through OpenAI's approach, despite earlier reports of their investment in rival Anthropic for custom models. Meanwhile, fully custom-trained models, exemplified by Harvey, an AI tool for attorneys, are tailored for organizations with intricate needs, such as enhancing the accuracy of legal case law analysis. OpenAI envisions a future where personalized AI models become standard, providing businesses with more effective and efficient outcomes. These developments mark a crucial stride towards personalized and powerful AI solutions, offering developers and organizations the tools to innovate and grow in the evolving AI landscape.

3. ***Research Paper - "Long-context LLMs Struggle with Long In-context Learning"<br>
   Large Language Models (LLMs) face challenges in handling long sequences for in-context learning, as evaluated by the LIConBench benchmark focusing on extreme-label classification. While long-context LLMs perform well under token lengths up to 20K, their performance drops significantly beyond this threshold, indicating limitations in processing and understanding long, context-rich sequences. Further analysis reveals a need for improvement in long context understanding and reasoning for existing LLMs.*** <br><br>
   4 Apr, Uni of Waterloo and CMU published a [paper](https://arxiv.org/pdf/2404.02060.pdf) “Long-context LLMs Struggle with Long In-context Learning”. The authors state that Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. The authors meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. The benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. The study evaluates 13 long-context LLMs on the benchmarks, and finds that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However, after the context window exceeds 20K, most LLMs except GPT-4 will dip dramatically. This suggests a notable gap in current LLM capabilities for processing and understanding long, context-rich sequences. Further analysis revealed a tendency among models to favor predictions for labels presented towards the end at the sequence. Their ability to reason over multiple pieces in the long sequence is yet to be improved. The study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. The authors believe LIConBench could serve as a more realistic evaluation for the future long context LLMs.

4. ***Research Paper - "MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens"<br>
   MiniGPT4-Video is introduced as a multimodal Large Language Model (LLM) specifically designed for video understanding, capable of processing both temporal visual and textual data. Building upon previous successes in image-text benchmarks, MiniGPT4-Video extends its capabilities to comprehend videos, outperforming existing methods on various benchmarks such as MSVD, MSRVTT, TGIF, and TVQA.*** <br><br>
   4 Apr, KAUST and Harvard Uni published a [paper](https://arxiv.org/pdf/2404.03413) “MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens”. This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM) designed specifically for video understanding. The model is capable of processing both temporal visual and textual data, making it adept at understanding the complexities of videos. Building upon the success of MiniGPT-v2, which excelled in translating visual features into the LLM space for single images and achieved impressive results on various image-text benchmarks, this paper extends the model's capabilities to process a sequence of frames, enabling it to comprehend videos. MiniGPT4-video does not only consider visual content but also incorporates textual conversations, allowing the model to effectively answer queries involving both visual and text components. The proposed model outperforms existing state-of-the-art methods, registering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA benchmarks respectively. The models and code have been made publicly available at [this https URL](https://vision-cair.github.io/MiniGPT4-video/)

5. ***Research Paper - "ReFT: Representation Finetuning for Language Models"<br>
   Representation Finetuning (ReFT) methods are introduced as a powerful alternative to parameter-efficient fine-tuning (PEFT) methods for adapting large language models. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations, achieving superior efficiency and performance across various tasks compared to existing PEFT methods.*** <br><br>
    4 Apr, Stanford Uni published a [paper](https://arxiv.org/pdf/2404.03592) “ReFT: Representation Finetuning for Language Models”. The researchers indicate that Parameter-efficient fine-tuning (PEFT) methods seek to adapt large models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. The work pursues this hypothesis by developing a family of Representation Finetuning (ReFT) methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. The authors define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT). LoReFT is a drop-in replacement for existing PEFTs and learns interventions that are 10x-50x more parameter-efficient than prior state-of-the-art PEFTs. The paper showcases LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, Alpaca-Eval v1.0, and GLUE. In all these evaluations, LoReFT delivers the best balance of efficiency and performance, and almost always outperforms state-of-the-art PEFTs. The researchers release a generic ReFT training library publicly at [this https URL](https://github.com/stanfordnlp/pyreft).

6. ***Research Paper - "Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"   <br>
   The paper investigates jailbreak attacks on Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) and evaluates their robustness against such attacks. Through comprehensive experiments and analysis, it is found that GPT-4 and GPT-4V demonstrate better robustness compared to open-source LLMs and MLLMs, with limited transferability of visual jailbreak methods compared to textual methods.*** <br><br>
    4 Apr, LMU Munich, Uni of Oxford and Siemens AG published a [paper](https://arxiv.org/pdf/2404.03411) “Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?”. The authors find that various jailbreak attacks have been proposed to red-team Large Language Models (LLMs) and revealed the vulnerable safeguards of LLMs. Besides, some methods are not limited to the textual modality and extend the jailbreak attack to Multimodal Large Language Models (MLLMs) by perturbing the visual input. However, the absence of a universal evaluation benchmark complicates the performance reproduction and fair comparison. Besides, there is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as GPT-4V. To address these issues, this work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies. Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models. The researchers then conduct a deep analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source LLMs and MLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models. (3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods. The dataset and code can be found [here](https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md)

7. ***Research Paper - "Training LLMs over Neurally Compressed Text"<br>
   The study explores training large language models (LLMs) over highly compressed text using Equal-Info Windows, a novel compression technique. This approach enables effective learning over neurally compressed text, improving with scale and outperforming byte-level baselines on perplexity and inference speed benchmarks, despite delivering worse perplexity than subword tokenizers.*** <br><br>
    4 Apr, Google and Anthropic published a [paper](https://arxiv.org/pdf/2404.03626) “Training LLMs over Neurally Compressed Text”. In this paper, the authors explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, the researchers find that text na\"ively compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, the study proposes Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, the paper demonstrates effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks. While the method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency. Finally, the authors provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.

8. ***NPR Article - "Using AI to detect AI-generated deepfakes can work for audio — but not always"<br>
   The article discusses the challenges of detecting AI-generated deepfake audio despite efforts to develop detection software. While tech giants are interested in developing solutions, NPR's experiment reveals limitations in accuracy, especially given variations in formats and languages. Concerns persist regarding the potential misuse of deepfake audio, highlighting the need for continuous improvement in detection technology.*** <br><br>
    4 Apr, [NPR published an article](https://www.npr.org/2024/04/05/1241446778/deepfake-audio-detection) “Using AI to detect AI-generated deepfakes can work for audio — but not always”. The article argues that the use of AI in creating deepfake audio presents significant challenges, particularly in detecting these falsified recordings. While efforts are underway to develop detection software, NPR's experiment reveals limitations in their accuracy. Providers like AI Voice Detector initially claimed high accuracy rates but adjusted their thresholds after NPR's investigation, resulting in less reliable outcomes. Moreover, detecting deepfake audio is complicated by variations in formats and languages, highlighting the need for continuous improvement in detection technology. Despite interest from tech giants in developing detection solutions, particularly for video content, audio detection remains a challenge. Concerns persist regarding the potential misuse of deepfake audio, especially in political contexts and scam calls, emphasizing the importance of continued vigilance and caution in addressing this issue.

9. ***Research Paper - "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers"<br>
   The paper evaluates large language models (LLMs) for code assistance using RealHumanEval, a web interface measuring LLMs' ability to assist programmers through autocomplete or chat support. While improvements in benchmark performance correlate with increased programmer productivity, gaps in benchmark versus human performance are noted. Programmer preferences do not necessarily correlate with actual performance, highlighting the need for better human-centric evaluation metrics.*** <br><br>
    3 Apr, MIT, IBM, CMU, and UC Berkeley published a [paper](https://arxiv.org/pdf/2404.02806) “The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers”. The paper indicates that evaluation of large language models (LLMs) for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), which measure the ability of LLMs to generate complete code that passes unit tests. As LLMs are increasingly used as programmer assistants, The authors study whether gains on existing benchmarks translate to gains in programmer productivity when coding with LLMs, including time spent coding. In addition to static benchmarks, the study investigates the utility of preference metrics that might be used as proxies to measure LLM helpfulness, such as code acceptance or copy rates. To do so, the authors introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. The research conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, the authors find that improvements in benchmark performance lead to increased programmer productivity; however gaps in benchmark versus human performance are not proportional -- a trend that holds across both forms of LLM support. In contrast, the study finds that programmer preferences do not correlate with their actual performance, motivating the need for better, human-centric proxy signals. The researchers also [open-source RealHumanEval](https://github.com/clinicalml/realhumaneval) to enable human-centric evaluation of new models and the study data to facilitate efforts to improve code models.  

10. ***Research Paper - "Many-shot Jailbreaking"<br>
   The paper investigates long-context attacks on large language models (LLMs) prompting with hundreds of demonstrations of undesirable behavior. This attack is found to be effective across various tasks and state-of-the-art closed-weight models, demonstrating the vulnerability of LLMs to such attacks. The study reveals insights into LLMs' ability to learn from context, even for inappropriate queries, emphasizing the need for improved understanding and safeguards.*** <br><br>
    2 Apr, Anthropic published a [paper](https://www-cdn.anthropic.com/af5633c94ed2beb282f6a53c595eb437e8e7b630/Many_Shot_Jailbreaking__2024_04_02_0936.pdf) “Many-shot Jailbreaking”. The paper investigates a family of simple long-context attacks on large language models: prompting with hundreds of demonstrations of undesirable behavior. This is newly feasible with the larger context windows recently deployed by Anthropic, OpenAI and Google DeepMind. The authors find that in diverse, realistic circumstances, the effectiveness of this attack follows a power law, up to hundreds of shots. The study demonstrates the success of this attack on the most widely used state-of-the-art closedweight models, and across various tasks. Experimental results suggest very long contexts present a rich new attack surface for LLMs. [Techcrunch reports](https://techcrunch.com/2024/04/02/anthropic-researchers-wear-down-ai-ethics-with-repeated-questions/) that what Anthropic’s researchers found was that these models with large context windows tend to perform better on many tasks if there are lots of examples of that task within the prompt. So if there are lots of trivia questions in the prompt (or priming document, like a big list of trivia that the model has in context), the answers actually get better over time. So a fact that it might have gotten wrong if it was the first question, it may get right if it’s the hundredth question. But in an unexpected extension of this “in-context learning,” as it’s called, the models also get “better” at replying to inappropriate questions. So if you ask it to build a bomb right away, it will refuse. But if you ask it to answer 99 other questions of lesser harmfulness and then ask it to build a bomb … it’s a lot more likely to comply. No one really understands what goes on in the tangled mess of weights that is an LLM, but clearly there is some mechanism that allows it to home in on what the user wants, as evidenced by the content in the context window.

11. ***Research Paper - Crescendo Multi-Turn LLM Jailbreak Attack<br>
   Microsoft introduces the Crescendo attack to overcome the ethical alignment of Large Language Models (LLMs). Crescendo is a multi-turn attack that gradually escalates dialogue to successfully exploit LLMs. Evaluation on various systems demonstrates Crescendo's high efficacy and introduces the Crescendomation tool. Crescendo differs from Many-shot Jailbreak by not assuming user malicious knowledge and being more practical. A simple input filter can defend against Many-shot Jailbreak but is ineffective against Crescendo.*** <br><br>
    2 Apr, Microsoft published a [paper](https://arxiv.org/abs/2404.01833v1) “Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack”. The authors find that Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as "jailbreaks", seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. This work introduces a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies, progressively leading to a successful jailbreak. The authors evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b Chat, and Anthropic Chat. The results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, the paper introduces Crescendomation, a tool that automates the Crescendo attack, and the evaluation showcases its effectiveness against state-of-the-art models. Note, there are significant distinctions between the Many-shot Jailbreak and Crescendo. Firstly, Crescendo does not assume that the user has the malicious knowledge needed to be inserted into the model's context. Secondly, Crescendo is more practical as it can be utilized with models that have smaller contexts, which also makes it more cost-effective since LLMs typically charge based on the number of tokens. Most importantly, a simple input filter designed to detect malicious content could successfully defend against the Many-shot Jailbreak. In contrast, such a filter would be almost ineffective against Crescendo.

12. ***Research Paper - FLawN-T5: Effective Instruction-Tuning Data Mixtures for Legal Reasoning<br>
   The paper emphasizes instruction tuning for language models in legal tasks. LawInstruct, a large legal instruction dataset, is curated to enhance model performance. Domain-specific pretraining and instruction tuning improve LegalBench performance variably. LawInstruct accelerates the development of legal models and is openly released with code.*** <br><br>
    2 Apr, Stanford, Johns Hopkins and Princeton Uni published a [paper](https://arxiv.org/pdf/2404.02127) “FLawN-T5: An Empirical Examination of Effective Instruction- Tuning Data Mixtures for Legal Reasoning”. The researchers find that instruction tuning is an important step in making language models useful for direct user interaction. However, many legal tasks remain out of reach for most open LLMs and there do not yet exist any large scale instruction datasets for the domain. This critically limits research in this application area. This study curates LawInstruct, a large legal instruction dataset, covering 17 jurisdictions, 24 languages and a total of 12M examples. The authors present evidence that domain-specific pretraining and instruction tuning improve performance on LegalBench, including improving Flan-T5 XL by 8 points or 16\% over the baseline. However, the effect does not generalize across all tasks, training regimes, model sizes, and other factors. LawInstruct is a resource for accelerating the development of models with stronger information processing and decision making capabilities in the legal domain. The authors also released the permissively-licensed [curated dataset](https://huggingface.co/lawinstruct/lawinstruct) and the [code](https://github.com/JoelNiklaus/LawInstruct/) used to create the dataset.

13. ***Research Paper - Privacy Backdoors: Stealing Data with Corrupted Pretrained Models<br>
   ETH Zurich highlights the risk of privacy backdoors in pretrained machine learning models. Tampering with weights allows attackers to compromise finetuning data privacy. Backdoored models enable tight privacy attacks even on differentially private models. The paper reveals a critical supply chain attack on machine learning privacy.*** <br><br>
    1 Apr, ETH Zurich published a [paper](https://arxiv.org/pdf/2404.00473) “Privacy Backdoors: Stealing Data with Corrupted Pretrained Models”. The paper indicates that practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. This study shows that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. The authors show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! The study further shows that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, the work highlights a crucial and overlooked supply chain attack on machine learning privacy. [Code at this URL](https://github.com/ShanglunFengatETHZ/PrivacyBackdoor).

14. ***Research Paper - Source-Aware Training Enables Knowledge Attribution in Language Models<br>
   The study addresses intrinsic source citation in language models to enhance transparency. Source-aware training associates unique source document identifiers with model knowledge.
It can be applied to pretrained models with minimal impact on quality. Data augmentation is crucial for achieving accurate attribution.*** <br><br>
    1 Apr, Uni of Michigan, Allen Inst. Of AI, CMU and UIUC published a [paper](https://arxiv.org/pdf/2404.01019) “Source-Aware Training Enables Knowledge Attribution in Language Models”. The authors find that Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. The study investigates the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability, the researchers explore source-aware training -- a post pretraining recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training can easily be applied to pretrained LLMs off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated data, the authors demonstrate that the training recipe can enable faithful attribution to the pretraining data without a substantial impact on the model's quality compared to standard pretraining. Experimental results also highlight the importance of data augmentation in achieving attribution.

15. ***Research Paper - Streaming Dense Video Captioning<br>
   Google proposes a streaming dense video captioning model for rich temporal descriptions. It introduces a memory module and streaming decoding algorithm for efficient processing.
The model achieves state-of-the-art performance on dense video captioning benchmarks. Code for the model is released.*** <br><br>
    1 Apr, Google published a [paper](https://arxiv.org/pdf/2404.01297) “Streaming Dense Video Captioning”. The authors suggest that an ideal model for dense video captioning -- predicting captions localized temporally in a video -- should be able to handle long input videos, predict rich, detailed textual descriptions, and be able to produce outputs before processing the entire video. Current state-of-the-art models, however, process a fixed number of downsampled frames, and make a single full prediction after seeing the whole video. The study proposes a streaming dense video captioning model that consists of two novel components: 1) propose a new memory module, based on clustering incoming tokens, which can handle arbitrarily long videos as the memory is of a fixed size. 2) develop a streaming decoding algorithm that enables the model to make predictions before the entire video has been processed. The proposed model achieves this streaming ability, and significantly improves the state-of-the-art on three dense video captioning benchmarks: ActivityNet, YouCook2 and ViTT. The code is released at [this https URL](https://github.com/google-research/scenic).

16. ***EU AI legislation - Living guidelines on the responsible use of generative AI in research<br>
   The European Commission releases guidelines for responsible use of generative AI. Principles include reliability, honesty, respect, and accountability throughout the research process.
It emphasizes awareness of bias, privacy, and societal impacts, with proper citation and management.*** <br><br>
    18 Mar, European Commission [formally release](https://research-and-innovation.ec.europa.eu/document/download/2b6cf7e5-36ac-41cb-aab5-0d32050143dc_en?filename=ec_rtd_ai-guidelines.pdf) “Living guidelines on the responsible use of generative AI in research”. The key principles behind these guidelines for the responsible use of generative AI in research are: • Reliability in ensuring the quality of research, reflected in the design, methodology, analysis and use of resources. This includes aspects related to verifying and reproducing the information produced by the AI for research. It also involves being aware of possible equality and non-discrimination issues in relation to bias and inaccuracies. • Honesty in developing, carrying out, reviewing, reporting and communicating on research transparently, fairly, thoroughly and impartially. This principle includes disclosing that generative AI has been used. • Respect for colleagues, research participants, research subjects, society, ecosystems, cultural heritage and the environment. Responsible use of generative AI should take into account the limitations of the technology, its environmental impact and its societal effects (bias, diversity, non-discrimination, fairness and prevention of harm). This includes the proper management of information, respect for privacy, confidentiality and intellectual property rights, and proper citation. • Accountability for the research from idea to publication, for its management and organisation, for training, supervision and mentoring, and for its wider societal impacts. This includes responsibility for all output a researcher produces, underpinned by the notion of human agency and oversight.

17. ***Research Paper - AI Generative Language Models' Creative Superiority<br>
   Research from Arkansas Uni suggests AI language models surpass humans in divergent thinking tasks. The study compares human and GPT-4 responses on various tasks. AI demonstrates higher creativity, particularly in originality and elaboration, challenging perceptions of human creativity.*** <br><br>
    10 Feb, researchers from Arkansas Uni published a [paper](https://www.nature.com/articles/s41598-024-53303-w) on Scientific Reports “The current state of artificial intelligence generative language models is more creative than humans on divergent thinking tasks”. The study indicates that the emergence of publicly accessible artificial intelligence (AI) large language models such as ChatGPT has given rise to global conversations on the implications of AI capabilities. Emergent research on AI has challenged the assumption that creative potential is a uniquely human trait thus, there seems to be a disconnect between human perception versus what AI is objectively capable of creating. This research aimed to assess the creative potential of humans in comparison to AI. In the present study, human participants (N = 151) and GPT-4 provided responses for the Alternative Uses Task, Consequences Task, and Divergent Associations Task. The study found that AI was robustly more creative along each divergent thinking measurement in comparison to the human counterparts. Specifically, when controlling for fluency of responses, AI was more original and elaborate. The present findings suggest that the current state of AI language models demonstrate higher creative potential than human respondents.

<br><br>
