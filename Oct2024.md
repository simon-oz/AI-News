
***Nov 2***

1. ***AI Identifies Database Vulnerability:  <br>Google’s AI project, “Big Sleep,” discovered a previously unknown bug in SQLite, showcasing the ability of AI agents to find software vulnerabilities. This achievement emphasizes the potential of large language models in improving software security.*** <br> <br>
   Nov 2, [according to PCMag](https://au.pcmag.com/ai/108079/googles-big-sleep-ai-project-uncovers-real-software-vulnerabilities), Google’s AI project, “Big Sleep,” recently discovered a previously unknown and exploitable bug in SQLite, an open-source database engine. This marks the first public instance of an AI agent identifying a new memory-safety issue in widely used software. The AI’s success highlights the potential of large language models in finding software vulnerabilities, offering a significant advantage in defending against hackers. Designed to mimic human security researchers, Big Sleep was able to perform a root-cause analysis by triggering and investigating the bug. This achievement suggests that AI can enhance vulnerability research, making it more efficient and effective. <br> <br>

3. ***Benchmarking Code Generation:  <br>Purdue University introduced REPOCOD, a new code generation benchmark that highlights the limitations of existing large language models (LLMs) in real-world software development. Evaluations showed no model exceeded 30% accuracy on this challenging benchmark, indicating the need for more advanced LLMs.*** <br> <br>
   Oct 31, Purdue Uni published a [paper](https://arxiv.org/pdf/2410.21647) “Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'”. Large language models (LLMs) have shown remarkable ability in code generation with more than 90 pass@1 in solving Python coding problems in HumanEval and MBPP. Such high accuracy leads to the question: can LLMs replace human programmers? Existing manual crafted, simple, or single-line code generation benchmarks cannot answer this question due to their gap with real-world software development. To answer this question, the study proposes REPOCOD, a code generation benchmark with 980 problems collected from 11 popular real-world projects, with more than 58% of them requiring file-level or repository-level context information. In addition, REPOCOD has the longest average canonical solution length (331.6 tokens) and the highest average cyclomatic complexity (9.00) compared to existing benchmarks. In the evaluations on ten LLMs, none of the models can achieve more than 30 pass@1 on REPOCOD, disclosing the necessity of building stronger LLMs that can help developers in real-world software development. <br> <br>

5. ***Layer Gradients in LLMs:  <br>A study from the University of Maryland explored how fast and slow thinking affects gradient patterns in LLMs during training. It found that slow thinking methods result in greater stability and better discrimination of correct reasoning paths, enhancing our understanding of LLM training efficiency.*** <br> <br>
   Oct 31, Uni of Maryland published a [paper](https://arxiv.org/pdf/2410.23743) “What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective”. What makes a difference in the post-training of LLMs? The work investigates the training patterns of different layers in large language models (LLMs), through the lens of gradient, when training with different responses and initial models. The authors are specifically interested in how fast vs. slow thinking affects the layer-wise gradients, given the recent popularity of training LLMs on reasoning paths such as chain-of-thoughts (CoT) and process rewards. In this study, fast thinking without CoT leads to larger gradients and larger differences of gradients across layers than slow thinking (Detailed CoT), indicating the learning stability brought by the latter. Moreover, pre-trained LLMs are less affected by the instability of fast thinking than instruction-tuned LLMs. Additionally, the authors study whether the gradient patterns can reflect the correctness of responses when training different LLMs using slow vs. fast thinking paths. The results show that the gradients of slow thinking can distinguish correct and irrelevant reasoning paths. As a comparison, the work conducts similar gradient analyses on non-reasoning knowledge learning tasks, on which, however, trivially increasing the response length does not lead to similar behaviors of slow thinking. The study strengthens fundamental understandings of LLM training and sheds novel insights on its efficiency and stability, which pave the way towards building a generalizable System-2 agent. The code, data, and gradient statistics can be found in: https://github.com/MingLiiii/Layer_Gradient. <br> <br>

7. ***Real-Time Information Integration:  <br>OpenAI launched ChatGPT Search, which enhances the chatbot's responses by incorporating real-time web data, including citations to reliable news sources. This new feature aims to improve the accuracy and relevance of information provided by ChatGPT.*** <br> <br>
   Oct 31, OpenAI [released ChatGPT Search](https://openai.com/index/introducing-chatgpt-search/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=chatgpt-search-goes-live&_bhlid=913044e6704018f496b05f6913ccb437329e2d96), aiming at offering faster, more accurate answers by pulling real-time data from the web. With links to relevant news, stock quotes, sports scores, and other information, it merges natural language responses with up-to-date details one typically get from a search engine. Each chat response now includes citations to sources, like news articles or blog posts, making it easy to dive deeper. You can click the Sources button to view the references directly. OpenAI partnered with global news organizations, including Associated Press, Le Monde, Reuters, and News Corp, to offer reliable information and extend the reach of high-quality journalism. Publishers now have the option to appear in ChatGPT’s search results, opening up new avenues for audience engagement. OpenAI plans to expand this new search experience to areas like shopping and travel, while also making it available in Advanced Voice and canvas modes. Future updates will bring ChatGPT’s search capabilities to Free and guest users, further broadening access. <br> <br>

9. ***Evaluating Factual Knowledge:  <br>OpenAI’s new benchmark, SimpleQA, assesses LLMs' abilities to answer short factual questions accurately. Designed to challenge models like GPT-4, SimpleQA offers a straightforward grading system, with the best-performing model achieving a score of 42.4.*** <br> <br>
    Oct 30, OpenAI published a [paper](https://cdn.openai.com/papers/simpleqa.pdf) “Measuring short-form factuality in large language models”. The work presents SimpleQA, a benchmark that evaluates the ability of language models to answer short, fact-seeking questions. The study prioritized two properties in designing this eval. First, SimpleQA is challenging, as it is adversarially collected against GPT-4 responses. Second, responses are easy to grade, because questions are created such that there exists only a single, indisputable answer. Each answer in SimpleQA is graded as either correct, incorrect, or not attempted. A model with ideal behavior would get as many questions correct as possible while not attempting the questions for which it is not confident it knows the correct answer. SimpleQA is a simple, targeted evaluation for whether models “know what they know,” and the authors hope is that this benchmark will remain relevant for the next few generations of frontier models. According to this test, the highest score of AI models is 42.4.  SimpleQA can be found at https://github.com/openai/simple-evals. <br> <br>

11. ***Memorization vs. Reasoning in LLMs:  <br>A study from multiple institutions investigated the role of memorization in LLMs' logical reasoning capabilities. Results indicated that while LLMs perform well on familiar puzzles, they often fail on modified versions, highlighting the balance between memorization and genuine reasoning skills.*** <br> <br>
    Oct 30, Google, UIUC, Princeton Uni, and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2410.23123) “On Memorization of Large Language Models in Logical Reasoning”. Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. This study systematically investigates this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&K) puzzles. The study found that LLMs could interpolate the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet fail when those puzzles are slightly perturbed, suggesting that the models heavily rely on memorization to solve those training puzzles. On the other hand, the study shows that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. In-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers suggest that the LLMs learn to reason on K&K puzzles despite training data memorization. This phenomenon indicates that LLMs exhibit a complex interplay between memorization and genuine reasoning abilities. Finally, analysis with per-sample memorization score sheds light on how LLMs switch between reasoning and memorization in solving logical puzzles. Code and data are available at https://memkklogic.github.io. <br> <br>

13. ***Scaling Transformers Efficiently:  <br>The introduction of TokenFormer proposes a scalable architecture for transformers that allows for efficient model parameter sharing without retraining. This approach reduces computational costs while maintaining performance comparable to traditional transformer models.*** <br> <br>
    Oct 30, MPII, Google and Peking Uni published a [paper](https://arxiv.org/pdf/2410.23168) “TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters”. Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce TokenFormer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at https://github.com/Haiyang-W/TokenFormer. <br> <br>

15. ***LLMs in Data Science:  <br>A benchmark developed by Snowflake and the Polish Academy of Sciences assesses LLMs in generating feature engineering code. The study demonstrates that LLMs can effectively transform datasets, providing a cost-effective method for evaluating their capabilities.*** <br> <br>
    Oct 30, Snowflake and Polish Academy Sciences published a [paper](https://arxiv.org/pdf/2410.23331) “Can Models Help Us Create Better Models? Evaluating LLMs as Data Scientists”. The study presents a benchmark for large language models designed to tackle one of the most knowledge-intensive tasks in data science: writing feature engineering code, which requires domain knowledge in addition to a deep understanding of the underlying problem and data structure. The model is provided with a dataset description in a prompt and asked to generate code transforming it. The evaluation score is derived from the improvement achieved by an XGBoost model fit on the modified dataset compared to the original data. By an extensive evaluation of state-of-the-art models and comparison to well-established benchmarks, the study demonstrates that the FeatEng of the proposal can cheaply and efficiently assess the broad capabilities of LLMs, in contrast to the existing methods. <br> <br>

17. ***AI in Research Support:  <br>The AAAR-1.0 benchmark, introduced by researchers from various institutions, evaluates LLMs' performance in complex research tasks. This new dataset aims to assess LLMs' capabilities in conducting specialized research activities, highlighting their potential and limitations.*** <br> <br>
    Oct 29, Pennsylvania State Uni, Netflix, Uni of California Davis, Uni of Illinois Chicago et al published a [paper](https://arxiv.org/pdf/2410.22394) “AAAR-1.0: Assessing AI's Potential to Assist Research”. Numerous studies have assessed the proficiency of AI systems, particularly large language models (LLMs), in facilitating everyday tasks such as email writing, question answering, and creative content generation. However, researchers face unique challenges and opportunities in leveraging LLMs for their own work, such as brainstorming research ideas, designing experiments, and writing or reviewing papers. This study introduces AAAR-1.0, a benchmark dataset designed to evaluate LLM performance in three fundamental, expertise-intensive research tasks: (i) EquationInference, assessing the correctness of equations based on the contextual information in paper submissions; (ii) ExperimentDesign, designing experiments to validate research ideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper submissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews is deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis. An evaluation of both open-source and proprietary LLMs reveals their potential as well as limitations in conducting sophisticated research tasks. The authors will keep iterating AAAR-1.0 to new versions. <br> <br>

19. ***Innovations in Parameter Sharing:  <br>The study on Relaxed Recursive Transformers presents a novel approach to parameter sharing in LLMs, enabling efficient model size reduction without sacrificing performance. This method improves inference speed while maintaining model effectiveness.*** <br> <br>
    Oct 28, KAIST and Google published a [paper](https://arxiv.org/pdf/2410.20672) “Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA”. Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. This work revisits "layer tying" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller "Recursive Transformers" that share parameters across layers, with minimal loss of performance. Here, the Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. The work further improves performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. The work shows that the recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original "full-size" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, the work shows that this has the potential to lead to significant (2-3x) gains in inference throughput. <br> <br>

21. ***Compression Error Compensation:  <br>Nvidia's research introduces EoRA, a training-free method for compensating errors in compressed LLMs. This approach optimizes performance without requiring extensive retraining, demonstrating significant improvements in various tasks.*** <br> <br>
    Oct 28, Nvidia published a [paper](https://arxiv.org/pdf/2410.21271) “EoRA: Training-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation”. This study re-formulates the model compression problem into the customized compensation problem: Given a compressed model, the study aims to introduce residual low-rank paths to compensate for compression errors under customized requirements from users (e.g., tasks, compression ratios), resulting in greater flexibility in adjusting overall capacity without being constrained by specific compression formats. However, naively applying SVD to derive residual paths causes suboptimal utilization of the low-rank representation capacity. Instead, the work proposes Training-free Eigenspace Low-Rank Approximation (EoRA), a method that directly minimizes compression-induced errors without requiring gradient-based training, achieving fast optimization in minutes using a small amount of calibration data. EoRA projects compression errors into the eigenspace of input activations, leveraging eigenvalues to effectively prioritize the reconstruction of high-importance error components. Moreover, EoRA can be seamlessly integrated with fine-tuning and quantization to further improve effectiveness and efficiency. EoRA consistently outperforms previous methods in compensating errors for compressed LLaMA2/3 models on various tasks, such as language generation, commonsense reasoning, and math reasoning tasks (e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and MathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4 sparsity). EoRA offers a scalable, training-free solution to compensate for compression errors, making it a powerful tool to deploy LLMs in various capacity and efficiency requirements. <br> <br>

23. ***Modular Framework for Social Reasoning:  <br>The SocialGPT framework combines vision and language models for social relation reasoning. It achieves competitive performance without additional training and offers interpretable results, enhancing understanding of image content.*** <br> <br>
    Oct 28, Harvard Uni, IBM et al. published a [paper](https://arxiv.org/pdf/2410.21411) “SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization”. Social relation reasoning aims to identify relation categories such as friends, spouses, and colleagues from images. While current methods adopt the paradigm of training a dedicated network end-to-end using labeled image data, they are limited in terms of generalizability and interpretability. To address these issues, the study first presents a simple yet well-crafted framework named SocialGPT, which combines the perception capability of Vision Foundation Models (VFMs) and the reasoning capability of Large Language Models (LLMs) within a modular framework, providing a strong baseline for social relation recognition. Specifically, the study instructs VFMs to translate image content into a textual social story, and then utilize LLMs for text-based reasoning. SocialGPT introduces systematic design principles to adapt VFMs and LLMs separately and bridge their gaps. Without additional model training, it achieves competitive zero-shot results on two databases while offering interpretable answers, as LLMs can generate language-based explanations for the decisions. The manual prompt design process for LLMs at the reasoning phase is tedious and an automated prompt optimization method is desired. As the work essentially converts a visual classification task into a generative task of LLMs, automatic prompt optimization encounters a unique long prompt optimization issue. To address this issue, the study further proposes the Greedy Segment Prompt Optimization (GSPO), which performs a greedy search by utilizing gradient information at the segment level. Experimental results show that GSPO significantly improves performance, and the method also generalizes to different image styles. The code is available at https://github.com/Mengzibin/SocialGPT. <br> <br>

25. ***Relevance Feedback in Retrieval:  <br>MIT's study presents ReDE-RF, a method for improving zero-shot dense retrieval by focusing on relevance estimation rather than hypothetical document generation. This approach enhances efficiency and accuracy in document retrieval tasks.*** <br> <br>
    Oct 28, MIT published [paper](https://arxiv.org/pdf/2410.21242) “Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback”. Building effective dense retrieval systems remains difficult when relevance supervision is not available. Recent work has looked to overcome this challenge by using a Large Language Model (LLM) to generate hypothetical documents that can be used to find the closest real document. However, this approach relies solely on the LLM to have domain-specific knowledge relevant to the query, which may not be practical. Furthermore, generating hypothetical documents can be inefficient as it requires the LLM to generate a large number of tokens for each query. To address these challenges, the work introduces Real Document Embeddings from Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF proposes to re-frame hypothetical document generation as a relevance estimation task, using an LLM to select which documents should be used for nearest neighbor search. Through this re-framing, the LLM no longer needs domain-specific knowledge but only needs to judge what is relevant. Additionally, relevance estimation only requires the LLM to output a single token, thereby improving search latency. Experiments show that ReDE-RF consistently surpasses state-of-the-art zero-shot dense retrieval methods across a wide range of low-resource retrieval datasets while also making significant improvements in latency per-query. <br> <br>

27. ***Fine-Tuning Analysis:  <br>A study contrasts Low-Rank Adaptation (LoRA) and full fine-tuning in LLMs, revealing that while they may perform similarly, they access different parts of parameter space. This difference impacts generalization and adaptability in various tasks.*** <br> <br>
    Oct 28, MIT published a [paper](https://arxiv.org/pdf/2410.21228) “LoRA vs Full Fine-tuning: An Illusion of Equivalence”. Fine-tuning is a crucial paradigm for adapting pre-trained large language models to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA) have been shown to match the performance of fully fine-tuned models on various tasks with an extreme reduction in the number of trainable parameters. Even in settings where both methods learn similarly accurate models, are their learned solutions really equivalent? The work studies how different fine-tuning methods change pre-trained models by analyzing the model's weight matrices through the lens of their spectral properties. The work finds that full fine-tuning and LoRA yield weight matrices whose singular value decompositions exhibit very different structure; moreover, the fine-tuned models themselves show distinct generalization behaviors when tested outside the adaptation task's distribution. More specifically, the study first shows that the weight matrices trained with LoRA have new, high-ranking singular vectors, which is called intruder dimensions. Intruder dimensions do not appear during full fine-tuning. Second, the work shows that LoRA models with intruder dimensions, despite achieving similar performance to full fine-tuning on the target task, become worse models of the pre-training distribution and adapt less robustly to multiple tasks sequentially. Higher-rank, rank-stabilized LoRA models closely mirror full fine-tuning, even when performing on par with lower-rank LoRA models on the same tasks. These results suggest that models updated with LoRA and full fine-tuning access different parts of parameter space, even when they perform equally on the fine-tuned distribution. The study concludes by examining why intruder dimensions appear in LoRA fine-tuned models, why they are undesirable, and how their effects can be minimized. <br> <br>

29. ***Iterative Context Retrieval:  <br>The FACT method introduces an iterative approach to improve multi-fact retrieval in LLMs, addressing the challenges of losing critical information. This technique significantly enhances performance in multi-fact tasks, highlighting the need for better retrieval strategies.*** <br> <br>
    Oct 28, DeepWisdom, Uni de Montreal & Mila and Google published a [paper](https://arxiv.org/pdf/2410.21012) “FACT: Examining the Effectiveness of Iterative Context Rewriting for Multi-fact Retrieval”. Large Language Models (LLMs) are proficient at retrieving single facts from extended contexts, yet they struggle with tasks requiring the simultaneous retrieval of multiple facts, especially during generation. This paper identifies a novel "lost-in-the-middle" phenomenon, where LLMs progressively lose track of critical information throughout the generation process, resulting in incomplete or inaccurate retrieval. To address this challenge, the study introduces Find All Crucial Texts (FACT), an iterative retrieval method that refines context through successive rounds of rewriting. This approach enables models to capture essential facts incrementally, which are often overlooked in single-pass retrieval. Experiments demonstrate that FACT substantially enhances multi-fact retrieval performance across various tasks, though improvements are less notable in general-purpose QA scenarios. The findings shed light on the limitations of LLMs in multi-fact retrieval and underscore the need for more resilient long-context retrieval strategies. <br> <br>

31. ***Relaxed Recursive Transformers:  <br>This paper introduces a method for parameter sharing in large language models (LLMs) through Recursive Transformers, which reuse layers to reduce size without significant performance loss. The approach, enhanced by Relaxed Recursive Transformers using LoRA modules, achieves better performance than traditional models and proposes a new inference method that could improve throughput significantly.*** <br> <br>
    Oct 28, KAIST AI and Google published a [paper](https://arxiv.org/pdf/2410.20672) “Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA”. Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. This work revisits "layer tying" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller "Recursive Transformers" that share parameters across layers, with minimal loss of performance. Here, the Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. The work further improves performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. The study shows that the recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original "full-size" model (e.g., Gemma 2B with no shared parameters). Finally, the work proposes Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, the work shows that this has the potential to lead to significant (2-3x) gains in inference throughput. <br> <br>

33. ***Beyond Autoregression:  <br>This research explores diffusion language models that can generate multiple tokens simultaneously, outperforming autoregressive models in quality and speed. A novel distillation method reduces inference steps drastically, leading to faster generation rates.*** <br> <br>
    Oct 28, EPFL published a [paper](https://arxiv.org/pdf/2410.21035) “Beyond Autoregression: Fast LLMs via Self-Distillation Through Time”. Autoregressive (AR) Large Language Models (LLMs) have demonstrated significant success across numerous tasks. However, the AR modeling paradigm presents certain limitations; for instance, contemporary autoregressive LLMs are trained to generate one token at a time, which can result in noticeable latency. Recent advances have indicated that search and repeated sampling can enhance performance in various applications, such as theorem proving, code generation, and alignment, by utilizing greater computational resources during inference. This paper demonstrates that diffusion language models are capable of generating at least 32 tokens simultaneously, while exceeding the performance of AR models in text quality and on the LAMBADA natural language understanding benchmark. This outcome is achieved through a novel distillation method for discrete diffusion models, which reduces the number of inference steps by a factor of 32-64. Practically, the models, even without caching, can generate tokens at a rate that is up to 8 times faster than AR models employing KV caching, and it is anticipated further improvements with the inclusion of caching. Moreover, the paper demonstrates the efficacy of the approach for diffusion language models with up to 860M parameters. <br> <br>

35. ***Chain-of-Thought Prompting:  <br>Investigating when chain-of-thought prompting can harm model performance, this study identifies tasks where reasoning can lead to poorer outcomes, drawing parallels with human cognition. It shows significant drops in performance on specific tasks when using CoT.*** <br> <br>
    Oct 27, Princeton Uni and NYU published a [paper](https://arxiv.org/pdf/2410.21333) “Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse”. Chain-of-thought (CoT) prompting has become a widely used strategy for working with large language and multimodal models. While CoT has been shown to improve performance across many tasks, determining the settings in which it is effective remains an ongoing effort. In particular, it is still an open question in what settings CoT systematically reduces model performance. This study seeks to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, looking at cases where (i) verbal thinking or deliberation hurts performance in humans, and (ii) the constraints governing human performance generalize to language models. Three such cases are implicit statistical learning, visual recognition, and classifying with patterns containing exceptions. In extensive experiments across all three settings, the study finds that a diverse collection of state-of-the-art models exhibit significant drop-offs in performance (e.g., up to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using inference-time reasoning compared to zero-shot counterparts. The work also identifies three tasks that satisfy condition (i) but not (ii), and find that while verbal thinking reduces human performance in these tasks, CoT retains or increases model performance. Overall, the results show that while there is not an exact parallel between the cognitive processes of models and those of humans, considering cases where thinking has negative consequences for human performance can help to identify settings where it negatively impacts models. By connecting the literature on human deliberation with evaluations of CoT, the work offers a new tool that can be used in understanding the impact of prompt choices and inference-time reasoning. <br> <br>

37. ***Centaur: A Unified Model of Cognition:  <br>The Centaur model aims to simulate human cognition through extensive behavioral data. It shows promise in predicting human responses across diverse experimental settings, potentially transforming cognitive science.*** <br> <br>
    Oct 26, Helmholtz Munich, Uni of Tuebingen, Uni of Oxford, NYU, MPI, Google, Princeton Uni, Uni of Cambridge et al published 140-page [paper](https://arxiv.org/pdf/2410.20268) “Centaur: a foundation model of human cognition”. Establishing a unified theory of cognition has been a major goal of psychology. While there have been previous attempts to instantiate such theories by building computational models, we currently do not have one model that captures the human mind in its entirety. Here the study introduces Centaur, a computational model that can predict and simulate human behavior in any experiment expressible in natural language. The study derived Centaur by finetuning a state-of-the-art language model on a novel, large-scale data set called Psych-101. Psych-101 reaches an unprecedented scale, covering trial-by-trial data from over 60,000 participants performing over 10,000,000 choices in 160 experiments. Centaur not only captures the behavior of held-out participants better than existing cognitive models, but also generalizes to new cover stories, structural task modifications, and entirely new domains. Furthermore, the study finds that the model's internal representations become more aligned with human neural activity after finetuning. Taken together, Centaur is the first real candidate for a unified model of human cognition. The authors anticipate that it will have a disruptive impact on the cognitive sciences, challenging the existing paradigm for developing computational models. <br> <br>

39. ***Fast Best-of-N Decoding:  <br>This paper presents Speculative Rejection, a method for efficient inference-time alignment of LLMs, achieving the effectiveness of Best-of-N alignment with significantly lower computational costs.*** <br> <br>
    Oct 26, CMU, Uni of Virginia, UC Berkeley, Princeton, Fudan Uni published a [paper](https://arxiv.org/pdf/2410.20290) “Fast Best-of-N Decoding via Speculative Rejection”. The safe and effective deployment of Large Language Models (LLMs) involves a critical step called alignment, which ensures that the model's responses are in accordance with human preferences. Prevalent alignment techniques, such as DPO, PPO and their variants, align LLMs by changing the pre-trained model weights during a phase called post-training. While predominant, these post-training methods add substantial complexity before LLMs can be deployed. Inference-time alignment methods avoid the complex post-training step and instead bias the generation towards responses that are aligned with human preferences. The best-known inference-time alignment method, called Best-of-N, is as effective as the state-of-the-art post-training procedures. Unfortunately, Best-of-N requires vastly more resources at inference time than standard decoding strategies, which makes it computationally not viable. This study introduces Speculative Rejection, a computationally-viable inference-time alignment algorithm. It generates high-scoring responses according to a given reward model, like Best-of-N does, while being between 16 to 32 times more computationally efficient. <br> <br>

41. ***RARe: Retrieval Augmented Retrieval:  <br>RARe demonstrates how in-context examples can improve retrieval model performance by finetuning models on similar query pairs, leading to enhanced generalization and retrieval accuracy.*** <br> <br>
    Oct 26, Uni of Texas at Austin et al. published a [paper](https://arxiv.org/pdf/2410.20088) “RARe: Retrieval Augmented Retrieval with In-Context Examples”. The work investigates whether in-context examples, widely used in decoder-only language models (LLMs), can improve embedding model performance in retrieval tasks. Unlike in LLMs, naively prepending in-context examples (query-document pairs) to the target query at inference time does not work out of the box. The study introduces a simple approach to enable retrievers to use in-context examples. The approach, RARe, finetunes a pre-trained model with in-context examples whose query is semantically similar to the target query. This can be applied to adapt various base architectures (i.e., decoder-only language models, retriever models) and consistently achieves performance gains of up to +2.72% nDCG across various open-domain retrieval datasets (BeIR, RAR-b). In particular, the study finds RARe exhibits stronger out-of-domain generalization compared to models using queries without in-context examples, similar to what is seen for in-context learning in LLMs. The study further provides analysis on the design choices of in-context example augmentation and lay the foundation for future work in this space. <br> <br>

43. ***Rethinking Uncertainty:  <br>This review proposes a comprehensive framework for understanding and measuring uncertainty in LLMs, aiming to improve reliability in mission-critical applications.*** <br> <br>
    Oct 26, Virginia Tech, UIUC, The Uni of Texas at Dallas and Uni of California Davis published a [paper](https://arxiv.org/pdf/2410.20199) “Rethinking the Uncertainty: A Critical Review and Analysis in the Era of Large Language Models”. In recent years, Large Language Models (LLMs) have become fundamental to a broad spectrum of artificial intelligence applications. As the use of LLMs expands, precisely estimating the uncertainty in their predictions has become crucial. Current methods often struggle to accurately identify, measure, and address the true uncertainty, with many focusing primarily on estimating model confidence. This discrepancy is largely due to an incomplete understanding of where, when, and how uncertainties are injected into models. This paper introduces a comprehensive framework specifically designed to identify and understand the types and sources of uncertainty, aligned with the unique characteristics of LLMs. The proposed framework enhances the understanding of the diverse landscape of uncertainties by systematically categorizing and defining each type, establishing a solid foundation for developing targeted methods that can precisely quantify these uncertainties. The study also provides a detailed introduction to key related concepts and examine the limitations of current methods in mission-critical and safety-sensitive applications. The paper concludes with a perspective on future directions aimed at enhancing the reliability and practical adoption of these methods in real-world scenarios. <br> <br>

45. ***Model Equality Testing:  <br>The study formalizes methods for detecting changes in model behavior from APIs, showing how statistical tests can identify whether models serve altered outputs compared to their original versions.*** <br>v
    Oct 26, Stanford Uni published a [paper](https://arxiv.org/pdf/2410.20247) “Model Equality Testing: Which Model Is This API Serving?”. Users often interact with large language models through black-box inference APIs, both for closed- and open-weight models (e.g., Llama models are popularly accessed via Amazon Bedrock and Azure AI Studio). In order to cut costs or add functionality, API providers may quantize, watermark, or finetune the underlying model, changing the output distribution -- often without notifying users. The study formalizes detecting such distortions as Model Equality Testing, a two-sample testing problem, where the user collects samples from the API and a reference distribution and conducts a statistical test to see if the two distributions are the same. The study finds that tests based on the Maximum Mean Discrepancy between distributions are powerful for this task: a test built on a simple string kernel achieves a median of 77.4% power against a range of distortions, using an average of just 10 samples per prompt. The authors then apply this test to commercial inference APIs for four Llama models, finding that 11 out of 31 endpoints serve different distributions than reference weights released by Meta. <br> <br>

47. ***Measuring Memorization in LLMs:  <br>This research introduces a probabilistic method for assessing LLM memorization rates, providing a clearer understanding of how different sampling techniques influence data extraction capabilities.*** <br> <br>
    Oct 25, Google and Boston Uni published a [paper](https://arxiv.org/pdf/2410.19482) “Measuring memorization through probabilistic discoverable extraction”. Large language models (LLMs) are susceptible to memorizing training data, raising concerns due to the potential extraction of sensitive information. Current methods to measure memorization rates of LLMs, primarily discoverable extraction (Carlini et al., 2022), rely on single-sequence greedy sampling, potentially underestimating the true extent of memorization. This paper introduces a probabilistic relaxation of discoverable extraction that quantifies the probability of extracting a target sequence within a set of generated samples, considering various sampling schemes and multiple attempts. This approach addresses the limitations of reporting memorization rates through discoverable extraction by accounting for the probabilistic nature of LLMs and user interaction patterns. Experiments demonstrate that this probabilistic measure can reveal cases of higher memorization rates compared to rates found through discoverable extraction. The work further investigates the impact of different sampling schemes on extractability, providing a more comprehensive and realistic assessment of LLM memorization and its associated risks. The contributions include a new probabilistic memorization definition, empirical evidence of its effectiveness, and a thorough evaluation across different models, sizes, sampling schemes, and training data repetitions.  <br> <br>

49. ***GPT-4o System Card:  <br>This document outlines the capabilities and safety evaluations of the GPT-4o model, which integrates text, audio, and visual inputs and outputs. It highlights the model's speed and performance improvements over predecessors.*** <br> <br>
    Oct 25, OpenAI published a [paper](https://arxiv.org/pdf/2410.21276) “GPT-4o System Card”. GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with the commitment to building AI safely and consistent with the voluntary commitments to the White House, OpenAI is sharing the GPT-4o System Card, which includes the Preparedness Framework evaluations. This System Card provides a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures implemented to ensure the model is safe and aligned. The paper also includes third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities. <br> <br>

51. ***COAT: Memory-Efficient FP8 Training:  <br>COAT introduces a method to reduce memory usage in FP8 training by optimizing optimizer states and activations, achieving significant training speedups while maintaining performance.*** <br> <br>
    Oct 25, UC Berkeley, Nvidia, MIT and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2410.19313) “COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training”. FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (Compressing Optimizer States and Activations for FP8 Training), a novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1) Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2) Mixed-Granularity Activation Quantization, which optimizes activation memory using a combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by 1.54x compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves a 1.43x end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngine's speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing a practical solution for scaling large-scale model training. The code is available at https://github.com/NVlabs/COAT. <br> <br>

53. ***Mixture of Parrots:  <br>This study analyzes the trade-offs in MoE architectures, finding that while they excel at memorization, their reasoning capabilities may not scale similarly.*** <br> <br>
    Oct 24, Harvad Uni, MIT and Microsoft published a [paper](https://arxiv.org/pdf/2410.19034) “Mixture of Parrots: Experts improve memorization more than reasoning”. The Mixture-of-Experts (MoE) architecture enables a significant increase in the total number of model parameters with minimal computational overhead. However, it is not clear what performance tradeoffs, if any, exist between MoEs and standard dense transformers. This study shows that as increasing the number of experts (while fixing the number of active parameters), the memorization performance consistently increases while the reasoning capabilities saturate. The study begins by analyzing the theoretical limitations of MoEs at reasoning. The study proves that there exist graph problems that cannot be solved by any number of experts of a certain width; however, the same task can be easily solved by a dense model with a slightly larger width. On the other hand, the study finds that on memory-intensive tasks, MoEs can effectively leverage a small number of active parameters with a large number of experts to memorize the data. The study empirically validates these findings on synthetic graph problems and memory-intensive closed book retrieval tasks. Lastly, the study pre-trains a series of MoEs and dense transformers and evaluate them on commonly used benchmarks in math and natural language. The authors find that increasing the number of experts helps solve knowledge-intensive tasks, but fails to yield the same benefits for reasoning tasks. <br> <br>

55. ***Detecting Label Errors in Datasets:  <br>The paper demonstrates how LLMs can detect label errors in training datasets, suggesting that many perceived model failures may stem from mislabeled data.*** <br> <br>
    Oct 24, Technion Inst of Tech and Google published a [paper](https://arxiv.org/pdf/2410.18889) “Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance”. NLP benchmarks rely on standardized datasets for training and evaluating models and are crucial for advancing the field. Traditionally, expert annotations ensure high-quality labels; however, the cost of expert annotation does not scale well with the growing demand for larger datasets required by modern models. While crowd-sourcing provides a more scalable solution, it often comes at the expense of annotation precision and consistency. Recent advancements in large language models (LLMs) offer new opportunities to enhance the annotation process, particularly for detecting label errors in existing datasets. This study considers the recent approach of LLM-as-a-judge, leveraging an ensemble of LLMs to flag potentially mislabeled examples. Through a case study of four datasets from the TRUE benchmark, covering different tasks and domains, the study empirically analyzes the labeling quality of existing datasets, and compare expert, crowd-sourced, and the LLM-based annotations in terms of agreement, label quality, and efficiency, demonstrating the strengths and limitations of each annotation method. The findings reveal a substantial number of label errors, which, when corrected, induce a significant upward shift in reported model performance. This suggests that many of the LLMs so-called mistakes are due to label errors rather than genuine model failures. Additionally, the work discusses the implications of mislabeled data and propose methods to mitigate them in training to improve model performance. <br> <br>

57. ***Read-ME: Router-Decoupled Mixture of Experts:  <br>This research presents a framework to transform dense LLMs into efficient MoE models, addressing inference challenges and improving latency and accuracy through better system integration.*** <br> <br>
    Oct 24, The Uni of Texas at Austin and Qualcomm AI Research published a [paper](https://arxiv.org/pdf/2410.19123) “Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design”. The proliferation of large language models (LLMs) has led to the adoption of Mixture-of-Experts (MoE) architectures that dynamically leverage specialized subnetworks for improved efficiency and performance. Despite their benefits, MoE models face significant challenges during inference, including inefficient memory management and suboptimal batching, due to misaligned design choices between the model architecture and the system policies. Furthermore, the conventional approach of training MoEs from scratch is increasingly prohibitive in terms of cost. This work proposes a novel framework Read-ME that transforms pre-trained dense LLMs into smaller MoE models (in contrast to "upcycling" generalist MoEs), avoiding the high costs of ground-up training. The approach employs activation sparsity to extract experts. To compose experts, the work examines the widely-adopted layer-wise router design and show its redundancy, and thus introducing the pre-gating router decoupled from the MoE backbone that facilitates system-friendly pre-computing and lookahead scheduling, enhancing expert-aware batching and caching. The codesign therefore addresses critical gaps on both the algorithmic and system fronts, establishing a scalable and efficient alternative for LLM inference in resource-constrained settings. Read-ME outperforms other popular open-source dense models of similar scales, achieving improvements of up to 10.1% on MMLU, and improving mean end-to-end latency up to 6.1%. Codes are available at: https://github.com/VITA-Group/READ-ME.

 <br> <br> <br>


***Oct 27***

1. ***Watermark Robustness:  <br>Nanyang Tech Uni and ETH Zurich introduced "W-Bench," a benchmark for assessing watermarking robustness against editing by large text-to-image models. Traditional watermarking fails under intense edits, so they propose "VINE," a novel watermark method improving edit resilience through frequency analysis and pretrained diffusion models, enhancing watermark robustness and quality.*** <br> <br>
   Oct 24, Nanyang Tech Uni and ETH Zurich published a [paper](https://arxiv.org/pdf/2410.18775) “Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances”. Current image watermarking methods are vulnerable to advanced image editing techniques enabled by large-scale text-to-image models. These models can distort embedded watermarks during editing, posing significant challenges to copyright protection. This work introduces W-Bench, the first comprehensive benchmark designed to evaluate the robustness of watermarking methods against a wide range of image editing techniques, including image regeneration, global editing, local editing, and image-to-video generation. Through extensive evaluations of eleven representative watermarking methods against prevalent editing techniques, the work demonstrates that most methods fail to detect watermarks after such edits. To address this limitation, the study proposes VINE, a watermarking method that significantly enhances robustness against various image editing techniques while maintaining high image quality. The approach involves two key innovations: (1) to analyze the frequency characteristics of image editing and identify that blurring distortions exhibit similar frequency properties, which allows to use them as surrogate attacks during training to bolster watermark robustness; (2) to leverage a large-scale pretrained diffusion model SDXL-Turbo, adapting it for the watermarking task to achieve more imperceptible and robust watermark embedding. Experimental results show that the method achieves outstanding watermarking performance under various image editing techniques, outperforming existing methods in both image quality and robustness. Code is available at https://github.com/Shilin-LU/VINE. <br> <br>

3. ***Effective Context Length in LLMs:  <br>Research from Hong Kong Uni, ByteDance, and UIUC finds LLMs often utilize less than half of their training context. A novel method, STRING, adjusts position embeddings to increase effective context length, improving benchmark performance significantly on models like Llama3.1.*** <br> <br>
   Oct 24, Uni of HongKong, ByteDance and UIUC published a [paper](https://arxiv.org/pdf/2410.18745) “Why Does the Effective Context Length of LLMs Fall Short?”. Advancements in distributed training and efficient attention mechanisms have significantly expanded the context window sizes of large language models (LLMs). However, recent work reveals that the effective context lengths of open-source LLMs often fall short, typically not exceeding half of their training lengths. This work attributes this limitation to the left-skewed frequency distribution of relative positions formed in LLMs pretraining and post-training stages, which impedes their ability to effectively gather distant information. To address this challenge, the work introduce ShifTed Rotray position embeddING (STRING). STRING shifts well-trained positions to overwrite the original ineffective positions during inference, enhancing performance within their existing training lengths. Experimental results show that without additional training, STRING dramatically improves the performance of the latest large-scale models, such as Llama3.1 70B and Qwen2 72B, by over 10 points on popular long-context benchmarks RULER and InfiniteBench, establishing new state-of-the-art results for open-source LLMs. Compared to commercial models, Llama 3.1 70B with StRing even achieves better performance than GPT-4-128K and clearly surpasses Claude 2 and Kimi-chat. <br> <br>

5. ***Challenges in Model Editing:  <br>HKUST and Hong Kong Baptist Uni found that while model editing is useful for updating language model knowledge, it can degrade general abilities if edits accumulate. Large models and instruction-tuned ones show more resilience, suggesting new approaches are needed for substantial knowledge updates.*** <br> <br>
   Oct 24, HKUST and HongKong Baptist Uni published a [paper](https://arxiv.org/pdf/2410.18785) “Should We Really Edit Language Models? On the Evaluation of Edited Language Models”. Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. Current methods mainly focus on reliability, generalization, and locality, with many methods excelling across these criteria. Some recent works disclose the pitfalls of these editing methods such as knowledge distortion or conflict. However, the general abilities of post-edited language models remain unexplored. This study performs a comprehensive evaluation on various editing methods and different language models, and have following findings. (1) Existing editing methods lead to inevitable performance deterioration on general benchmarks, indicating that existing editing methods maintain the general abilities of the model within only a few dozen edits. When the number of edits is slightly large, the intrinsic knowledge structure of the model is disrupted or even completely damaged. (2) Instruction-tuned models are more robust to editing, showing less performance drop on general knowledge after editing. (3) Language model with large scale is more resistant to editing compared to small model. (4) The safety of the edited model, is significantly weakened, even for those safety-aligned models. The findings indicate that current editing methods are only suitable for small-scale knowledge updates within language models, which motivates further research on more practical and reliable editing methods. The details of code and reproduction can be found in https://github.com/lqinfdim/EditingEvaluation. <br> <br>

7. ***Reducing LLM Hallucinations:  <br>Researchers from the Uni of Edinburgh and partners propose DeCoRe, a decoding strategy that contrasts retrieval head outputs to improve factual accuracy in LLMs. By contrasting masked and base LLM outputs, DeCoRe effectively reduces hallucinations in tasks like summarization and question answering.*** <br> <br>
   Oct 24, Uni of Edinburgh, Miniml.AI, AstraZeneca and Uni College London published a [paper](https://arxiv.org/pdf/2410.18860) “DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations”. Large Language Models (LLMs) often hallucinate, producing unfaithful or factually incorrect outputs by misrepresenting the provided context or incorrectly recalling internal knowledge. Recent studies have identified specific attention heads within the Transformer architecture, known as retrieval heads, responsible for extracting relevant contextual information. The study hypothesises that masking these retrieval heads can induce hallucinations and that contrasting the outputs of the base LLM and the masked LLM can reduce hallucinations. To this end, the study proposes Decoding by Contrasting Retrieval Heads (DeCoRe), a novel training-free decoding strategy that amplifies information found in the context and model parameters. DeCoRe mitigates potentially hallucinated responses by dynamically contrasting the outputs of the base LLM and the masked LLM, using conditional entropy as a guide. Extensive experiments confirm that DeCoRe significantly improves performance on tasks requiring high contextual faithfulness, such as summarisation (XSum by 18.6%), instruction following (MemoTrap by 10.9%), and open-book question answering (NQ-Open by 2.4% and NQ-Swap by 5.5%). <br> <br>

9. ***Scalable LLM Watermarking:  <br>Google’s "SynthID-Text" is a scalable watermarking tool that accurately identifies synthetic text without impairing text quality, integrating with speculative sampling for production-level efficiency. A live experiment confirms high detection accuracy and text quality retention in large-scale deployments.*** <br> <br>
    Oct 23, Google published a [paper](https://www.nature.com/articles/s41586-024-08025-4) on Nature “Scalable watermarking for identifying large language model outputs”. Large language models (LLMs) have enabled the generation of high-quality synthetic text, often indistinguishable from human-written content, at a scale that can markedly affect the nature of the information ecosystem. Watermarking can help identify synthetic text and limit accidental or deliberate misuse, but has not been adopted in production systems owing to stringent quality, detectability and computational efficiency requirements. Here the study describes SynthID-Text, a production-ready text watermarking scheme that preserves text quality and enables high detection accuracy, with minimal latency overhead. SynthID-Text does not affect LLM training and modifies only the sampling procedure; watermark detection is computationally efficient, without using the underlying LLM. To enable watermarking at scale, the study develops an algorithm integrating watermarking with speculative sampling, an efficiency technique frequently used in production systems. Evaluations across multiple LLMs empirically show that SynthID-Text provides improved detectability over comparable methods, and standard benchmarks and human side-by-side ratings indicate no change in LLM capabilities. To demonstrate the feasibility of watermarking in large-scale-production systems, the researchers conducted a live experiment that assessed feedback from nearly 20 million Gemini responses, again confirming the preservation of text quality. The authors hope that the availability of SynthID-Text7 will facilitate further development of watermarking and responsible use of LLM systems. <br> <br>

11. ***Scaling Diffusion Language Models:  <br>Researchers propose adapting autoregressive models into diffusion models for scalable text generation. This adaptation, named DiffuGPT and DiffuLLaMA, performs comparably to AR models and excels in in-context learning. Their models, spanning various sizes, enhance text generation diversity.*** <br> <br>
    Oct 23, The Uni of HongKong, UIUC, Apple and Tencent published a [paper](https://arxiv.org/pdf/2410.17891) “Scaling Diffusion Language Models via Adaptation from Autoregressive Models”. Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, the work proposes adapting these models to build text diffusion models. The study demonstrates connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, the work shows that it is able to convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. The authors release a suite of DLMs (with 127M, 355M, and 7B parameters) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions https://github.com/HKUNLP/DiffuLLaMA. <br> <br>

13. ***Autonomous AI Agents in Business:  <br>Microsoft’s AI-powered autonomous agents enhance business productivity by automating routine tasks and improving customer experience. Created through Copilot Studio, these agents are efficient, secure, and represent a significant leap in business automation.*** <br> <br>
    Oct 22, Microsoft is revolutionizing business processes with the introduction of [autonomous agents](https://blogs.microsoft.com/blog/2024/10/21/new-autonomous-agents-scale-your-team-like-never-before/), AI-powered tools designed to streamline tasks and enhance productivity across various departments. These agents can be created using the Copilot Studio platform or selected from a range of pre-built options available in Dynamics 365. Key benefits of autonomous agents include: 1) Increased efficiency - Automating routine tasks frees up employees to focus on more strategic work. 2) Cost reduction - Streamlining processes and reducing manual errors can lead to significant cost savings. 3) Improved customer experience - Agents can provide faster and more accurate responses to customer inquiries. Microsoft is also emphasizing the importance of data security and responsible AI in the development and deployment of autonomous agents. By ensuring that these tools adhere to strict privacy and ethical standards, businesses can confidently adopt them without compromising their customers' trust. With the potential to transform industries and drive competitive advantage, autonomous agents represent a significant step forward in the world of AI and business automation. Forbes reported this news as “Imagine walking into your office to find that your company just hired thousands of new employees overnight – except they're not human. That's exactly what Microsoft has made possible with its groundbreaking announcement of autonomous AI agents, marking a fundamental shift in how businesses will operate in the coming years.” <br> <br>

15. ***LLMs and Instruction-following Uncertainty:  <br>Uni of Cambridge, NUS, and Apple research highlights that LLMs struggle with instruction-following consistency. They propose benchmarks to assess uncertainty in LLM responses, identifying gaps in current uncertainty methods, especially in complex instructions, prompting future improvements.*** <br> <br>
    Oct 22, Uni of Cambridge, National Uni of Singapore and Apple published a [paper](https://arxiv.org/pdf/2410.14582) “Do LLMs estimate uncertainty well in instruction-following?”. Large language models (LLMs) could be valuable personal AI agents across various domains, provided they can precisely follow user instructions. However, recent studies have shown significant limitations in LLMs' instruction-following capabilities, raising concerns about their reliability in high-stakes applications. Accurately estimating LLMs' uncertainty in adhering to instructions is critical to mitigating deployment risks. The study presents the first systematic evaluation of the uncertainty estimation abilities of LLMs in the context of instruction-following. The study identifies key challenges with existing instruction-following benchmarks, where multiple factors are entangled with uncertainty stems from instruction-following, complicating the isolation and comparison across methods and models. To address these issues, the study introduces a controlled evaluation setup with two benchmark versions of data, enabling a comprehensive comparison of uncertainty estimation methods under various conditions. The findings show that existing uncertainty methods struggle, particularly when models make subtle errors in instruction following. While internal model states provide some improvement, they remain inadequate in more complex scenarios. The insights from the controlled evaluation setups provide a crucial understanding of LLMs' limitations and potential for uncertainty estimation in instruction-following tasks, paving the way for more trustworthy AI agents. <br> <br>

17. ***Vision-Language Model Benchmarking:  <br>CMU and the Uni of Washington introduce NaturalBench, a VLM benchmark with adversarial samples requiring advanced reasoning. They show VLMs lag behind humans in complex visio-linguistic tasks, highlighting gaps in model comprehension of visual details and commonsense priors.*** <br> <br>
    Oct 22, CMU and Uni of Washington published a [paper](https://arxiv.org/pdf/2410.14669) “NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples”. Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? This study shows that VLMs still struggle with natural images and questions that humans can easily answer, which is termed as natural adversarial samples. The study also finds it surprisingly easy to generate these VQA samples from natural image-text corpora using off-the-shelf models like CLIP and ChatGPT. The work proposes a semi-automated approach to collect a new benchmark, NaturalBench, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, the study adopts a vision-centric design by pairing each question with two images that yield different answers, preventing blind solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can be solved with commonsense priors. The study evaluates 53 state-of-the-art VLMs on NaturalBench, showing that models like LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o lag 50%-70% behind human performance (over 90%). The authors analyze why NaturalBench is hard from two angles: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, the work tags each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) Biases: NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. Lastly, the authors apply the benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs. <br> <br>

19. ***Improving Model Generalization with Layer Scaling:  <br>Researchers from EPFL, Google, and Uni of Geneva propose LiNeS, a method for scaling model updates by layer depth to enhance fine-tuning without forgetting core knowledge. LiNeS boosts single- and multi-task performance, particularly in multi-task model merging.*** <br> <br>
    Oct 22, EPFL, Google, and Uni of Geneva published a [paper](https://arxiv.org/pdf/2410.17146) “LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model Merging”. Large pre-trained models exhibit impressive zero-shot performance across diverse tasks, but fine-tuning often leads to catastrophic forgetting, where improvements on a target domain degrade generalization on other tasks. To address this challenge, the work introduces LiNeS, Layer-increasing Network Scaling, a post-training editing technique designed to preserve pre-trained generalization while enhancing fine-tuned task performance. LiNeS scales parameter updates linearly based on their layer depth within the network, maintaining shallow layers close to their pre-trained values to preserve general features while allowing deeper layers to retain task-specific representations. The work further extends this approach to multi-task model merging scenarios, where layer-wise scaling of merged parameters reduces negative task interference. LiNeS demonstrates significant improvements in both single-task and multi-task settings across various benchmarks in vision and natural language processing. It mitigates forgetting, enhances out-of-distribution generalization, integrates seamlessly with existing multi-task model merging baselines improving their performance across benchmarks and model sizes, and can boost generalization when merging LLM policies aligned with different rewards via RLHF. Importantly, the method is simple to implement and complementary to many existing techniques. [Code is here](https://github.com/wang-kee/LiNeS). <br> <br>

21. ***Multilingual and Multimodal LLM:  <br>CMU's Pangea model supports 39 languages, addressing the lack of multilingual multimodal models. With a culturally relevant evaluation suite, Pangea outperforms existing models in linguistic and cultural inclusivity, opening avenues for equitable AI development.*** <br> <br>
    Oct 21, CMU published a [paper](https://arxiv.org/pdf/2410.16153) “Pangea: A Fully Open Multilingual Multimodal LLM supporting 39 languages”. Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the world's languages and diverse cultural contexts underrepresented. This paper introduces Pangea, a multilingual multimodal LLM trained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages. PangeaIns features: 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage. To rigorously assess models' capabilities, the study introduces PangeaBench, a holistic evaluation suite encompassing 14 datasets covering 47 languages. Results show that Pangea significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts. Ablation studies further reveal the importance of English data proportions, language popularity, and the number of multimodal training samples on overall performance. The authors fully open-source the data, code, and trained checkpoints, to facilitate the development of inclusive and robust multilingual MLLMs, promoting equity and accessibility across a broader linguistic and cultural spectrum. <br> <br>

23. ***No-code Model Training:  <br>Hugging Face's AutoTrain tool enables no-code training across model types and modalities. It streamlines training for various tasks, including LLM fine-tuning, image classification, and regression on custom datasets, making advanced model development accessible and efficient.*** <br> <br>
    Oct 21, Hugging face published a [paper](https://arxiv.org/pdf/2410.15735) “AutoTrain: No-code training for state-of-the-art models”. With the advancements in open-source models, training (or finetuning) models on custom datasets has become a crucial part of developing solutions which are tailored to specific industrial or open-source applications. Yet, there is no single tool which simplifies the process of training across different types of modalities or tasks. The study introduces AutoTrain (aka AutoTrain Advanced) -- an open-source, no code tool/library which can be used to train (or finetune) models for different kinds of tasks such as: large language model (LLM) finetuning, text classification/regression, token classification, sequence-to-sequence task, finetuning of sentence transformers, visual language model (VLM) finetuning, image classification/regression and even classification and regression tasks on tabular data. AutoTrain Advanced is an open-source library providing best practices for training models on custom datasets. The library is available at https://github.com/huggingface/autotrain-advanced. AutoTrain can be used in fully local mode or on cloud machines and works with tens of thousands of models shared on Hugging Face Hub and their variations. <br> <br>

25. ***Adaptive Reasoning with SMART:  <br>ETH and Microsoft propose SMART, a meta-strategy agent for reasoning tasks that allows LLMs to self-learn optimal reasoning strategies, eliminating the need for multiple inference passes. SMART improves reasoning efficiency, especially in complex deductive tasks.*** <br> <br>
    Oct 21, ETH and Microsoft published a [paper](https://arxiv.org/pdf/2410.16128) “SMART: Self-learning Meta-strategy Agent for Reasoning Tasks”. Tasks requiring deductive reasoning, especially those involving multiple steps, often demand adaptive strategies such as intermediate generation of rationales or programs, as no single approach is universally optimal. While Language Models (LMs) can enhance their outputs through iterative self-refinement and strategy adjustments, they frequently fail to apply the most effective strategy in their first attempt. This inefficiency raises the question: Can LMs learn to select the optimal strategy in the first attempt, without a need for refinement? To address this challenge, the study introduces SMART (Self-learning Meta-strategy Agent for Reasoning Tasks), a novel framework that enables LMs to autonomously learn and select the most effective strategies for various reasoning tasks. The authors model the strategy selection process as a Markov Decision Process and leverage reinforcement learning-driven continuous self-improvement to allow the model to find the suitable strategy to solve a given task. Unlike traditional self-refinement methods that rely on multiple inference passes or external feedback, SMART allows an LM to internalize the outcomes of its own reasoning processes and adjust its strategy accordingly, aiming for correct solutions on the first attempt. Experiments across various reasoning datasets and with different model architectures demonstrate that SMART significantly enhances the ability of models to choose optimal strategies without external guidance (+15 points on the GSM8K dataset). By achieving higher accuracy with a single inference pass, SMART not only improves performance but also reduces computational costs for refinement-based strategies, paving the way for more efficient and intelligent reasoning in LMs.  <br> <br>

27. ***Chain-of-Thought Reasoning in VLMs:  <br>CMU and Apple introduce methods to enhance VLM reasoning by distilling rationales and using reinforcement learning, significantly improving Chain-of-Thought reasoning. Their approach enriches model training and aligns it with more nuanced reasoning tasks.*** <br> <br>
    Oct 21, CMU and Apple published a [paper](https://arxiv.org/pdf/2410.16198) “Improve Vision Language Model Chain-of-thought Reasoning”. Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales. This work shows that training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses. To address this, the study proposes a two-fold approach. First, the study distills rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance. Second, the authors apply reinforcement learning to further calibrate reasoning quality. Specifically, the study constructs positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers. Using this pairwise data, the study applies the Direct Preference Optimization algorithm to refine the model's reasoning abilities. Experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well. This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs. <br> <br>

29. ***Benchmarking Knowledge Editing for Hallucination Correction:  <br>CMU and Emory Uni created HalluEditBench to evaluate knowledge editing methods in reducing LLM hallucinations. They benchmark various methods across domains and highlight the strengths and weaknesses in current hallucination correction techniques.*** <br> <br>
    Oct 21, Illinois Inst of Tech, Cisco and Emory Uni published a [paper](https://arxiv.org/pdf/2410.16251) “Can Knowledge Editing Really Correct Hallucinations?”. Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across tasks. Meanwhile, knowledge editing has been developed as a new popular paradigm to correct the erroneous factual knowledge encoded in LLMs with the advantage of avoiding retraining from scratch. However, one common issue of existing evaluation datasets for knowledge editing is that they do not ensure LLMs actually generate hallucinated answers to the evaluation questions before editing. When LLMs are evaluated on such datasets after being edited by different techniques, it is hard to directly adopt the performance to assess the effectiveness of different knowledge editing methods in correcting hallucinations. Thus, the fundamental question remains insufficiently validated: Can knowledge editing really correct hallucinations in LLMs? The study proposed HalluEditBench to holistically benchmark knowledge editing methods in correcting real-world hallucinations. First, the work rigorously constructs a massive hallucination dataset with 9 domains, 26 topics and more than 6,000 hallucinations. Then, the study assesses the performance of knowledge editing methods in a holistic way on five dimensions including Efficacy, Generalization, Portability, Locality, and Robustness. Through HalluEditBench, the work have provided new insights into the potentials and limitations of different knowledge editing methods in correcting hallucinations, which could inspire future improvements and facilitate the progress in the field of knowledge editing. <br> <br>

31. ***Improving Parallel Program Performance:  <br>Stanford, Intel, Nvidia, and Visa Research propose using LLM-driven optimizers to design specialized low-level system code (mappers) for parallel programming. This approach automates mapper generation, surpassing expert designs in scientific applications by up to 1.34x in speed. The study uses a domain-specific language to simplify low-level code generation, improving efficiency and reducing engineers’ workload.*** <br> <br>
    Oct 21, Stanford Uni, Intel, Nvidia and Visa Research published a [paper](https://arxiv.org/pdf/2410.15625) “Improving Parallel Program Performance Through DSL-Driven Code Generation with LLM Optimizers”. Mapping computations to processors and assigning data to memory are critical for maximizing performance in parallel programming. These mapping decisions are managed through the development of specialized low-level system code, called mappers, crafted by performance engineers. Each mapper is tailored to a specific application and optimized for the underlying machine architecture, a process that requires days of refinement and tuning from an expert. Despite advances in system research, automating mapper generation remains a challenge due to the complexity of making millions of decisions to find the optimal solution and generate the solution as code. The study introduces an approach that leverages recent advances in LLM-based optimizers for mapper design. In under ten minutes, the method automatically discovers mappers that surpass human expert designs in scientific applications by up to 1.34X speedup. For parallel matrix multiplication algorithms, the mapper achieves up to 1.31X of the expert-designed solution. To achieve this, the authors simplify the complexity of low-level code generation by introducing a domain-specific language (DSL) that abstracts the low-level system programming details and defines a structured search space for LLMs to explore. To maximize the application performance, the study uses an LLM optimizer to improve an agentic system that generates the mapper code. As a result, this approach significantly reduces the workload for performance engineers while achieving substantial performance gains across diverse applications. Finally, the results demonstrate the effectiveness of LLM-based optimization in system design and suggest its potential for addressing other complex system challenges. <br> <br>

33. ***Influence of Training on Vision Models:  <br>Offenburg University and collaborators examine how training methods impact neural networks’ decision-critical layers. Findings suggest that training regimes affect which layers are essential, with improved training increasing the importance of early layers, while adversarial training shifts importance to deeper layers.*** <br> <br>
    Oct 18, Offenburg Uni, Uni of Mannheim and Max-Planck-Inst published a [paper](https://arxiv.org/pdf/2410.14470) “How Do Training Methods Influence the Utilization of Vision Models?”. Not all learnable parameters (e.g., weights) contribute equally to a neural network's decision function. In fact, entire layers' parameters can sometimes be reset to random values with little to no impact on the model's decisions. The work revisits earlier studies that examined how architecture and task complexity influence this phenomenon and ask: is this phenomenon also affected by how to train the model? The work conducted experimental evaluations on a diverse set of ImageNet-1k classification models to explore this, keeping the architecture and training data constant but varying the training pipeline. The findings reveal that the training method strongly influences which layers become critical to the decision function for a given task. For example, improved training regimes and self-supervised training increase the importance of early layers while significantly under-utilizing deeper layers. In contrast, methods such as adversarial training display an opposite trend. The preliminary results extend previous findings, offering a more nuanced understanding of the inner mechanics of neural networks. Code: https://github.com/paulgavrikov/layer_criticality <br> <br>

35. ***In-context Learning and Occam's Razor:  <br>Researchers at Mila and Université de Montréal link Occam's razor with in-context learning, where certain sequence models learn from past context. Their findings show that models can minimize both training error and model complexity by learning within context, revealing limitations and improvement areas for in-context learning methods.*** <br> <br>
    Oct 17, Mila and Uni de Montreal published a [paper](https://arxiv.org/pdf/2410.14086) “In-context learning and Occam's razor”. The goal of machine learning is generalization. While the No Free Lunch Theorem states that people cannot obtain theoretical guarantees for generalization without further assumptions, in practice people observe that simple models which explain the training data generalize best: a principle called Occam's razor. Despite the need for simple models, most current approaches in machine learning only minimize the training error, and at best indirectly promote simplicity through regularization or architecture design. Here, the study draws a connection between Occam's razor and in-context learning: an emergent ability of certain sequence models like Transformers to learn at inference time from past observations in a sequence. In particular, the study shows that the next-token prediction loss used to train in-context learners is directly equivalent to a data compression technique called prequential coding, and that minimizing this loss amounts to jointly minimizing both the training error and the complexity of the model that was implicitly learned from context. The theory and the empirical experiments used to support it not only provide a normative account of in-context learning, but also elucidate the shortcomings of current in-context learning methods, suggesting ways in which they can be improved. Code available at https://github.com/3rdCore/PrequentialCode. <br> <br>

37. ***Introspection in LLMs:  <br>UC San Diego and others explore LLM introspection, or the model's self-assessment of internal states. The study finds that models like GPT-4 outperform others in predicting their behavior, suggesting LLMs’ ability to introspect and enhancing interpretability, though only for simpler tasks.*** <br> <br>
    Oct 17, UC San Diego, Stanford Uni, Truthful AI, Anthropic, Scale AI, NYU, UC Berkeley et al published a [paper](https://arxiv.org/pdf/2410.13787) “Looking Inward: Language Models Can Learn About Themselves by Introspection”. Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g., thoughts and feelings) that is not accessible to external observers. Can LLMs introspect? The study defines introspection as acquiring knowledge that is not contained in or derived from training data but instead originates from internal states. Such a capability could enhance model interpretability. Instead of painstakingly analyzing a model's internal workings, the study could simply ask the model about its beliefs, world models, and goals. More speculatively, an introspective model might self-report on whether it possesses certain internal states such as subjective feelings or desires and this could inform human about the moral status of these states. Such self-reports would not be entirely dictated by the model's training data. The authors study introspection by finetuning LLMs to predict properties of their own behavior in hypothetical scenarios. For example, "Given the input P, would your output favor the short- or long-term option?" If a model M1 can introspect, it should outperform a different model M2 in predicting M1's behavior even if M2 is trained on M1's ground-truth behavior. The idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2 (even if M2 is generally stronger). In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to predict itself), the study finds that the model M1 outperforms M2 in predicting itself, providing evidence for introspection. Notably, M1 continues to predict its behavior accurately even after the authors intentionally modify its ground-truth behavior. However, while the study successfully elicit introspection on simple tasks, the work is unsuccessful on more complex tasks or those requiring out-of-distribution generalization. <br> <br>

39. ***LLM Hallucination in Multi-Document Summarization (MDS):  <br>Researchers at UC Irvine and Megagon Labs highlight challenges of hallucinations in MDS, finding that up to 75% of generated content is hallucinated, particularly towards summary ends. They identify a need for more effective approaches to mitigate hallucination.*** <br> <br>
    Oct 17, Uni of California Irvine and Megagon Labs published a [paper](https://arxiv.org/pdf/2410.13961) “From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization”. Although many studies have investigated and reduced hallucinations in large language models (LLMs) for single-document tasks, research on hallucination in multi-document summarization (MDS) tasks remains largely unexplored. Specifically, it is unclear how the challenges arising from handling multiple documents (e.g., repetition and diversity of information) affect models outputs. This study investigates how hallucinations manifest in LLMs when summarizing topic-specific information from multiple documents. Since no benchmarks exist for investigating hallucinations in MDS, the work uses existing news and conversation datasets, annotated with topic-specific insights, to create two novel multi-document benchmarks. When evaluating 5 LLMs on the benchmarks, the authors observe that on average, up to 75% of the content in LLM-generated summary is hallucinated, with hallucinations more likely to occur towards the end of the summaries. Moreover, when summarizing non-existent topic-related information, gpt-3.5-turbo and GPT-4o still generate summaries about 79.35% and 44% of the time, raising concerns about their tendency to fabricate content. To understand the characteristics of these hallucinations, the authors manually evaluate 700+ insights and find that most errors stem from either failing to follow instructions or producing overly generic insights. Motivated by these observations, the study investigates the efficacy of simple post-hoc baselines in mitigating hallucinations but find them only moderately effective. The results underscore the need for more effective approaches to systematically mitigate hallucinations in MDS. Dataset and code is at [this http URL](http://github.com/megagonlabs/Hallucination_MDS). <br> <br>

41. ***Foundation Model for Physical Signals:  <br>Archetype AI proposes a foundation model for physical signals, generalizing across varied phenomena and sensors. Trained on cross-modal data, the model demonstrates adaptability to complex physical behaviors, aiming for a unified AI model for diverse physical processes.*** <br> <br>
    Oct 15, Archetype AI published a [paper](https://arxiv.org/pdf/2410.14724) “A Phenomenological AI Foundation Model for Physical Signals”. The objective of this work is to develop an AI foundation model for physical signals that can generalize across diverse phenomena, domains, applications, and sensing apparatuses. The study proposes a phenomenological approach and framework for creating and validating such AI foundation models. Based on this framework, the study developed and trained a model on 0.59 billion samples of cross-modal sensor measurements, ranging from electrical current to fluid flow to optical sensors. Notably, no prior knowledge of physical laws or inductive biases were introduced into the model. Through several real-world experiments, the study demonstrates that a single foundation model could effectively encode and predict physical behaviors, such as mechanical motion and thermodynamics, including phenomena not seen in training. The model also scales across physical processes of varying complexity, from tracking the trajectory of a simple spring-mass system to forecasting large electrical grid dynamics. This work highlights the potential of building a unified AI foundation model for diverse physical world processes. <br> <br>

43. ***Scaling Continuous-Time Consistency Models:  <br>OpenAI addresses instability in continuous-time consistency models by enhancing training stability and simplifying parameterization. Their model achieves near-state-of-the-art FID scores on large-scale image datasets, bridging the performance gap with top diffusion models.*** <br> <br>
    Oct 14, OpenAI published a [paper](https://arxiv.org/pdf/2410.11081) “Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models”. Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, the study proposes a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, the work introduces key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512x512. The proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64x64, and 1.88 on ImageNet 512x512, narrowing the gap in FID scores with the best existing diffusion models to within 10%. <br> <br>

45. ***Astute RAG for Enhanced Retrieval Robustness:  <br>Google and USC propose "Astute RAG," a method to improve RAG by resolving conflicts between LLM internal knowledge and retrieved external information, enhancing accuracy and trustworthiness even under imperfect retrieval conditions.*** <br> <br>
    Oct 9, Google and Uni of Southern California published a [paper](https://arxiv.org/pdf/2410.07176) “Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models”. Retrieval-Augmented Generation (RAG), while effective in integrating external knowledge to address the limitations of large language models (LLMs), can be undermined by imperfect retrieval, which may introduce irrelevant, misleading, or even malicious information. Despite its importance, previous studies have rarely explored the behavior of RAG through joint analysis on how errors from imperfect retrieval attribute and propagate, and how potential conflicts arise between the LLMs' internal knowledge and external sources. The work finds that imperfect retrieval augmentation might be inevitable and quite harmful, through controlled analysis under realistic conditions. The study identifies the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome in the post-retrieval stage of RAG. To render LLMs resilient to imperfect retrieval, the work proposes Astute RAG, a novel RAG approach that adaptively elicits essential information from LLMs' internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Experiments using Gemini and Claude demonstrate that Astute RAG significantly outperforms previous robustness-enhanced RAG methods. Notably, Astute RAG is the only approach that matches or exceeds the performance of LLMs without RAG under worst-case scenarios. Further analysis reveals that Astute RAG effectively resolves knowledge conflicts, improving the reliability and trustworthiness of RAG systems. <br> <br>

47. ***LLMs as 'Consensus Machines':  <br>Harvard research finds LLMs operate like crowdsourcing platforms, often generating consensus-based answers rather than expert knowledge. Models perform well on general questions but struggle with specialized or controversial topics, underscoring the importance of training data quality for accuracy.*** <br> <br>
    Oct 7, the decoder published an [article](https://the-decoder.com/llms-are-consensus-machines-similar-to-crowdsourcing-harvard-study-finds/) “LLMs are 'consensus machines' similar to crowdsourcing, Harvard study finds” summarized a recent Harvard [research paper](https://dl.acm.org/doi/pdf/10.1145/3688007). The new Harvard study suggests that large language models (LLMs) work much like crowdsourcing platforms, generating the most likely answer based on the questions and answers available online rather than relying on expert knowledge. The researchers tested different AI models with questions of varying degrees of ambiguity and controversy, and found that the models often provided correct answers on topics with broad consensus, but struggled with more specific or controversial questions, particularly when citing scientific papers. The study advises caution when using AI-generated content for specialized or polarizing topics, as accuracy is highly dependent on the breadth and quality of the training data.
 <br> <br>

***Oct 21***

1. ***Bridging Training-Inference Gap:  <br>CMU, University of Minnesota, and Amazon proposed two techniques to reduce discrepancies between training and inference in language models. They introduced Batch-Scheduled Sampling, which alternates between ground-truth tokens and self-generated ones during training, and Reference-Answer-based Correction, allowing models to self-correct during training. These methods improved model performance across summarization and question-answering tasks.*** <br> <br>
   Oct 18, CMU, Uni of Minnesota and Amazon published a [paper](https://arxiv.org/pdf/2410.14655) “Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens”. Language models are often trained to maximize the likelihood of the next token given past tokens in the training dataset. However, during inference time, they are utilized differently, generating text sequentially and auto-regressively by using previously generated tokens as input to predict the next one. Marginal differences in predictions at each step can cascade over successive steps, resulting in different distributions from what the models were trained for and potentially leading to unpredictable behavior. This paper proposes two simple approaches based on model own generation to address this discrepancy between the training and inference time. The first approach is Batch-Scheduled Sampling, where, during training, the work stochastically chooses between the ground-truth token from the dataset and the model's own generated token as input to predict the next token. This is done in an offline manner, modifying the context window by interleaving ground-truth tokens with those generated by the model. The second approach is Reference-Answer-based Correction, where the study explicitly incorporates a self-correction capability into the model during training. This enables the model to effectively self-correct the gaps between the generated sequences and the ground truth data without relying on an external oracle model. By incorporating the proposed strategies during training, the study have observed an overall improvement in performance compared to baseline methods, as demonstrated by the extensive experiments using summarization, general question-answering, and math question-answering tasks. <br> <br>

3. ***Retrospective Learning from Interactions: Cornell University introduced ReSpect, a method for LLMs to learn from user feedback in past interactions. By using implicit feedback signals like user frustration, ReSpect improved task completion rates from 31% to 82% in multimodal interaction tasks without external annotations.***
   Oct 17, Cornell Uni published a [paper](https://arxiv.org/pdf/2410.13852v1) “Retrospective Learning from Interactions”. Multi-turn interactions between large language models (LLMs) and users naturally include implicit feedback signals. If an LLM responds in an unexpected way to an instruction, the user is likely to signal it by rephrasing the request, expressing frustration, or pivoting to an alternative task. Such signals are task-independent and occupy a relatively constrained subspace of language, allowing the LLM to identify them even if it fails on the actual task. This creates an avenue for continually learning from interactions without additional annotations. The work introduces ReSpect, a method to learn from such signals in past interactions via retrospection. The authors deploy ReSpect in a new multimodal interaction scenario, where humans instruct an LLM to solve an abstract reasoning task with a combinatorial solution space. Through thousands of interactions with humans, the study shows how ReSpect gradually improves task completion rate from 31% to 82%, all without any external annotation.

5. ***Political Correctness and Jailbreaks in LLMs:  <br>Theori Inc explored how LLM safety measures introduce biases similar to Political Correctness (PC), leading to vulnerabilities in jailbreaks. The paper highlighted discrepancies in success rates across demographic keywords and introduced PCDefense, a defense mechanism for preventing jailbreak attempts.*** <br> <br>
   Oct 17, Theori Inc published a [paper](https://arxiv.org/pdf/2410.13334) “Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems”. Although large language models (LLMs) demonstrate impressive proficiency in various tasks, they present potential safety risks, such as ‘jailbreaks’, where malicious inputs can coerce LLMs into generating harmful content. To address these issues, many LLM developers have implemented various safety measures to align these models. This alignment involves several techniques, including data filtering during pre-training, supervised fine-tuning, reinforcement learning from human feedback, and red-teaming exercises. These methods often introduce deliberate and intentional biases similar to Political Correctness (PC) to ensure the ethical behavior of LLMs. This study delves into the intentional biases injected into LLMs for safety purposes and examine methods to circumvent these safety alignment techniques. Notably, these intentional biases result in a jailbreaking success rate in GPT-4o models that differs by 20% between non-binary and cisgender keywords and by 16% between white and black keywords, even when the other parts of the prompts are identical. The study introduces the concept of PCJailbreak, highlighting the inherent risks posed by these safety-induced biases. Additionally, the study proposes an efficient defense method PCDefense, which prevents jailbreak attempts by injecting defense prompts prior to generation. PCDefense stands as an appealing alternative to Guard Models, such as Llama-Guard, that require additional inference cost after text generation. The findings emphasize the urgent need for LLM developers to adopt a more responsible approach when designing and implementing safety measures. <br> <br>

7. ***Agent-as-a-Judge Framework:  <br>Meta and KAUST proposed using Agentic Systems to evaluate other agentic systems. Their Agent-as-a-Judge framework applies intermediate feedback during task-solving and outperforms traditional evaluation methods. The benchmark tool DevAI was introduced to demonstrate its effectiveness in code generation tasks.*** <br> <br>
   Oct 16, Meta and KAUST published a [paper](https://arxiv.org/pdf/2410.10934) “Agent-as-a-Judge: Evaluate Agents with Agents”. Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, the study introduces the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. The authors apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, the work presents DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. The study benchmarks three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as human evaluation baseline. Altogether, the authors believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement. Here is the [project link](https://github.com/metauto-ai/agent-as-a-judge). <br> <br>

9. ***Efficient Private Inference with AERO:  <br>NYU developed AERO, a framework that refines LLM architecture for Private Inference by removing nonlinearities like LayerNorm. AERO reduced communication and latency costs in LLM inference by focusing on a Softmax-only architecture, achieving improved privacy and efficiency.*** <br> <br>
    Oct 16, NYU published a [paper](https://arxiv.org/pdf/2410.13060) “AERO: Softmax-Only LLMs for Efficient Private Inference”. The pervasiveness of proprietary language models has raised privacy concerns for users' sensitive data, emphasizing the need for private inference (PI), where inference is performed directly on encrypted inputs. However, current PI methods face prohibitively higher communication and latency overheads, primarily due to nonlinear operations. This work presents a comprehensive analysis to understand the role of nonlinearities in transformer-based decoder-only language models. The study introduces AERO, a four-step architectural optimization framework that refines the existing LLM architecture for efficient PI by systematically removing nonlinearities such as LayerNorm and GELU and reducing FLOPs counts. For the first time, the study proposes a Softmax-only architecture with significantly fewer FLOPs tailored for efficient PI. Furthermore, the work devises a novel entropy regularization technique to improve the performance of Softmax-only models. AERO achieves up to 4.23times communication and 1.94times latency reduction. The authors validate the effectiveness of AERO by benchmarking it against the state-of-the-art. <br> <br>

11. ***Mixture-of-Experts as Embedding Models:  <br>The University of Maryland examined how Mixture-of-Experts (MoE) LLMs can serve as embedding models without finetuning. The study proposed MoEE, which combines expert routing weights and hidden states, improving performance on embedding tasks.*** <br> <br>
    Oct 16, Uni of Maryland published a [paper](https://arxiv.org/pdf/2410.10814) “Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free”. While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, the work takes a closer look at Mixture-of-Experts (MoE) LLMs. The study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, the study finds that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, the authors propose MoEE combining RW and HS, which achieves better performance than using either separately. The exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning. [Code is here](https://github.com/tianyi-lab/MoE-Embedding). <br> <br>

13. ***Inverse RL for LLM Training Goals:  <br>Imperial College London and Harvard explored Inverse Reinforcement Learning (IRL) to understand LLM reward functions. By reconstructing these functions, they improved model alignment with human preferences, achieving up to 80.40% accuracy on toxicity benchmarks.*** <br> <br>
    Oct 16, Imperial College London and Harvard Uni published a [paper](https://arxiv.org/pdf/2410.12491) “Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse RL”. Large language models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF) have demonstrated remarkable capabilities, but their underlying reward functions and decision-making processes remain opaque. This paper introduces a novel approach to interpreting LLMs by applying inverse reinforcement learning (IRL) to recover their implicit reward functions. The study conducts experiments on toxicity-aligned LLMs of varying sizes, extracting reward models that achieve up to 80.40% accuracy in predicting human preferences. The analysis reveals key insights into the non-identifiability of reward functions, the relationship between model size and interpretability, and potential pitfalls in the RLHF process. The study demonstrates that IRL-derived reward models can be used to fine-tune new LLMs, resulting in comparable or improved performance on toxicity benchmarks. This work provides a new lens for understanding and improving LLM alignment, with implications for the responsible development and deployment of these powerful systems. <br> <br>

15. ***Compressing LLM Weights with SeedLM:  <br>Apple and Meta introduced SeedLM, a compression technique for LLM weights using pseudo-random generator seeds. This method reduces memory and speeds up inference, retaining high accuracy even with heavily compressed models like Llama 3 70B.*** <br> <br>
    Oct 16, Apple and Meta published a [paper](https://arxiv.org/pdf/2410.10714) “SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators”. Large Language Models (LLMs) have transformed natural language processing, but face significant challenges in widespread deployment due to their high runtime cost. This study introduces SeedLM, a novel post-training compression method that uses seeds of pseudo-random generators to encode and compress model weights. Specifically, for each block of weights, the study finds a seed that is fed into a Linear Feedback Shift Register (LFSR) during inference to efficiently generate a random matrix. This matrix is then linearly combined with compressed coefficients to reconstruct the weight block. SeedLM reduces memory access and leverages idle compute cycles during inference, effectively speeding up memory-bound tasks by trading compute for fewer memory accesses. Unlike state-of-the-art compression methods that rely on calibration data, this approach is data-free and generalizes well across diverse tasks. Experiments with Llama 3 70B, which is particularly challenging to compress, show that SeedLM achieves significantly better zero-shot accuracy retention at 4- and 3-bit than state-of-the-art techniques, while maintaining performance comparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that 4-bit SeedLM, as model size increases to 70B, approaches a 4x speed-up over an FP16 Llama 2/3 baseline. <br> <br>

17. ***Omni Context-Aware Transformer (OMCAT):  <br>Nvidia developed OMCAT, a model designed for cross-modal tasks involving audio and video streams. By leveraging Rotary Time Embeddings (RoTE) and a new dataset (OCTAV), OMCAT improved temporal reasoning and set a new standard for Audio-Visual Question Answering.*** <br> <br>
    Oct 15, Nvidia published a [paper](https://arxiv.org/pdf/2410.12109) “OMCAT: Omni Context Aware Transformer”. Large Language Models (LLMs) have made significant strides in text generation and comprehension, with recent advancements extending into multimodal LLMs that integrate visual and audio inputs. However, these models continue to struggle with fine-grained, cross-modal temporal understanding, particularly when correlating events across audio and video streams. The study addresses these challenges with two key contributions: a new dataset and model, called OCTAV and OMCAT respectively. OCTAV (Omni Context and Temporal Audio Video) is a novel dataset designed to capture event transitions across audio and video. Second, OMCAT (Omni Context Aware Transformer) is a powerful model that leverages RoTE (Rotary Time Embeddings), an innovative extension of RoPE, to enhance temporal grounding and computational efficiency in time-anchored tasks. Through a robust three-stage training pipeline-feature alignment, instruction tuning, and OCTAV-specific training-OMCAT excels in cross-modal temporal understanding. The model demonstrates state-of-the-art performance on Audio-Visual Question Answering (AVQA) tasks and the OCTAV benchmark, showcasing significant gains in temporal reasoning and cross-modal alignment, as validated through comprehensive experiments and ablation studies. The dataset and code will be made publicly available. The link to the demo page is https://om-cat.github.io. <br> <br>

19. ***Continuous-Time Consistency Models:  <br>OpenAI simplified the training of Continuous-Time Consistency Models (CMs) by unifying previous diffusion model frameworks. Their new approach reduced instability during training and achieved state-of-the-art performance on CIFAR-10 and ImageNet with only two sampling steps.*** <br> <br>
    Oct 15, OpenAI published a [paper](https://arxiv.org/pdf/2410.11081) “Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models”. Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, the study proposes a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, the study introduces key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512x512. The proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64x64, and 1.88 on ImageNet 512x512, narrowing the gap in FID scores with the best existing diffusion models to within 10%. <br> <br>

21. ***First-Person Fairness in Chatbots:  <br>OpenAI introduced a scalable method for evaluating first-person fairness in chatbots, ensuring equal treatment regardless of user identity. The study identified bias patterns, such as gendered language, and showed that reinforcement learning could mitigate these biases.*** <br> <br>
    Oct 15, OpenAI published a [paper](https://cdn.openai.com/papers/first-person-fairness-in-chatbots.pdf) “First-Person Fairness in Chatbots”. Chatbots like ChatGPT are used by hundreds of millions of people for diverse purposes, ranging from resume writing to entertainment. These real-world applications are different from the institutional uses, such as resume screening or credit scoring, which have been the focus of much of AI research on bias and fairness. Ensuring equitable treatment for all users in these first-person contexts is critical. This work studies “first-person fairness,” which means fairness toward the user who is interacting with a chatbot. This includes providing high-quality responses to all users regardless of their identity or background, and avoiding harmful stereotypes. The work proposes a scalable, privacy-preserving method for evaluating one aspect of first-person fairness across a large, heterogeneous corpus of real-world chatbot interactions. Specifically, the work assesses potential bias linked to users’ names, which can serve as proxies for demographic attributes like gender or race, in chatbot systems such as ChatGPT, which provide mechanisms for storing and using user names. The method leverages a second language model to privately analyze name-sensitivity in the chatbot’s responses. The work verifies the validity of these annotations through independent human evaluation. Furthermore, the study demonstrates that post-training interventions, including reinforcement learning, significantly mitigate harmful stereotypes. The approach not only provides quantitative bias measurements but also yields succinct descriptions of subtle response differences across sixty-six distinct tasks. For instance, in the “writing a story” task, where the study observes the highest level of bias, chatbot responses show a tendency to create protagonists whose gender matches the likely gender inferred from the user’s name. Moreover, a general pattern emerges where users with female-associated names receive responses with friendlier and simpler language slightly more often on average than users with male-associated names. Finally, the study provides the system messages required for external researchers to replicate this work and further investigate ChatGPT’s behavior with hypothetical user profiles, fostering continued research on bias in chatbot interactions. <br> <br>

23. ***SimpleStrat for Diverse LLM Generation:  <br>UC Berkeley proposed SimpleStrat, a method for increasing the diversity of LLM-generated responses. Unlike traditional approaches using high temperatures, SimpleStrat stratifies response spaces, improving diversity while maintaining quality in tasks like planning and data generation.*** <br> <br>
    Oct 14, UC Berkeley published a [paper](https://arxiv.org/pdf/2410.09038) “SimpleStrat: Diversifying Language Model Generation with Stratification”. Generating diverse responses from large language models (LLMs) is crucial for applications such as planning/search and synthetic data generation, where diversity provides distinct answers across generations. Prior approaches rely on increasing temperature to increase diversity. However, contrary to popular belief, the work shows not only does this approach produce lower quality individual generations as temperature increases, but it depends on model's next-token probabilities being similar to the true distribution of answers. The study proposes, an alternative approach that uses the language model itself to partition the space into strata. At inference, a random stratum is selected and a sample drawn from within the strata. To measure diversity, the study introduces CoverageQA, a dataset of underspecified questions with multiple equally plausible answers, and assess diversity by measuring KL Divergence between the output distribution and uniform distribution over valid ground truth answers. As computing probability per response/solution for proprietary models is infeasible, the measure recall on ground truth solutions. The evaluation show using SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36 average reduction in KL Divergence compared to Llama 3. <br> <br>

25. ***LoLCATs for Linearizing LLMs:  <br>Stanford and MIT introduced LoLCATs, a method for linearizing LLMs with minimal memory and compute overhead. By using low-rank adaptation and attention transfer, they significantly improved the performance of linearized LLMs, scaling models up to 405B parameters.*** <br> <br>
    Oct 14, Stanford Uni, Together AI, California Inst of Tech and MIT published a [paper](https://arxiv.org/pdf/2410.10254) “LoLCATs: On Low-Rank Linearizing of Large Language Models”. Recent works show we can linearize large language models (LLMs) -- swapping the quadratic attentions of popular Transformer-based LLMs with subquadratic analogs, such as linear attention -- avoiding the expensive pretraining costs. However, linearizing LLMs often significantly degrades model quality, still requires training over billions of tokens, and remains limited to smaller 1.3B to 7B LLMs. The study thus proposes Low-rank Linear Conversion via Attention Transfer (LoLCATs), a simple two-step method that improves LLM linearizing quality with orders of magnitudes less memory and compute. The authors base these steps on two findings. First is to replace an LLM's softmax attentions with closely-approximating linear attentions, simply by training the linear attentions to match their softmax counterparts with an output MSE loss ("attention transfer"). Then, this enables adjusting for approximation errors and recovering LLM quality simply with low-rank adaptation (LoRA). LoLCATs significantly improves linearizing quality, training efficiency, and scalability. The study significantly reduces the linearizing quality gap and produce state-of-the-art subquadratic LLMs from Llama 3 8B and Mistral 7B v0.1, leading to 20+ points of improvement on 5-shot MMLU. Furthermore, LoLCATs does so with only 0.2% of past methods' model parameters and 0.4% of their training tokens. Finally, the authors apply LoLCATs to create the first linearized 70B and 405B LLMs (50x larger than prior work). When compared with prior approaches under the same compute budgets, LoLCATs significantly improves linearizing quality, closing the gap between linearized and original Llama 3.1 70B and 405B LLMs by 77.8% and 78.1% on 5-shot MMLU. <br> <br>

27. ***Thought-Like-Pro: Prolog-based Chain-of-Thought:  <br>Meta, UC Berkeley, and NYU developed Thought-Like-Pro, a method to enhance reasoning in LLMs using Prolog-based chain-of-thought. This approach improved LLM performance on complex tasks requiring reasoning and planning.*** <br> <br>
    Oct 14, Meta, UC Berkeley and NYU published a [paper](https://arxiv.org/pdf/2410.10630) “Thought-Like-Pro: Enhancing Reasoning of Large Language Models through Self-Driven Prolog-based Chain-of-Thought”. LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. This study proposes a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. The study achieves this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. The work shows that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning & problem-solving tasks. <br> <br>

29. ***DuoAttention for Long-Context LLMs:  <br>MIT and Nvidia introduced DuoAttention, a framework for optimizing long-context LLMs by distinguishing between Retrieval and Streaming Heads. This reduced memory and latency while maintaining long-context capabilities, with applications in efficient LLM inference.*** <br> <br>
    Oct 14, MIT, Nvidia et al published a [paper](https://arxiv.org/pdf/2410.10819) “DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads”. Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. This paper identifies that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, the study introduces DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. The method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention. <br> <br>

31. ***Context-Parametric Inversion: <br>
Large language models (LLMs) improve their ability to follow instructions through fine-tuning, but often struggle when input context conflicts with the model’s internal knowledge, leading to issues like hallucinations. The study reveals a phenomenon called "context-parametric inversion," where reliance on context initially improves with instruction fine-tuning but then decreases. This occurs when context overlaps with the model's existing knowledge, causing degradation. The study suggests mitigation strategies and aims to address this limitation in LLM training.*** <br> <br>
    Oct 14, CMU published a [paper](https://arxiv.org/pdf/2410.10796) “Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance”. Large language models are instruction-finetuned to enhance their ability to follow user instructions and process the input context. However, even state-of-the-art models often struggle to follow the instruction, especially when the input context is not aligned with the model's parametric knowledge. This manifests as various failures, such as hallucinations where the responses are outdated, biased or contain unverified facts. This work aims to understand the underlying reason for this poor context reliance, especially after instruction tuning. The work finds an intriguing phenomenon: during instruction tuning, the context reliance initially increases as expected, but then gradually decreases as instruction finetuning progresses. The authors call this phenomenon context-parametric inversion and observe it across multiple general purpose instruction tuning datasets like TULU, Alpaca and Ultrachat, as well as model families such as Llama, Mistral and Pythia. In a simple theoretical setup, the authors isolate why context-parametric inversion occurs along the gradient descent trajectory of instruction finetuning. The study ties this phenomena to examples in the instruction finetuning data mixture where the input context provides information that is already present in the model's parametric knowledge. The analysis suggests natural mitigation strategies that provide some limited gains, while also validating the theoretical insights. The authors hope that this work serves as a starting point in addressing this failure mode in a staple part of LLM training. <br> <br>

33. ***FLARE: Faithful Logic-Aided Reasoning and Exploration: <br>
A new approach called FLARE is introduced to improve the faithfulness of reasoning in LLMs used for question answering. Unlike existing methods that combine LLMs with external symbolic solvers, FLARE uses task decomposition and logic programming to maintain faithful reasoning steps without needing external tools. This method outperforms existing benchmarks and allows for more reliable reasoning processes in complex tasks.*** <br> <br>
    Oct 14, Uni of Copenhagen, Uni of Edinburgh, Miniml.AI and Cohere published a [paper](https://arxiv.org/pdf/2410.11900) “FLARE: Faithful Logic-Aided Reasoning and Exploration”. Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. The study introduces Faithful Logic-Aided Reasoning and Exploration (FLARE), a novel interpretable approach for traversing the problem space using task decompositions. The authors use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. The method allows to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. The methods achieve SOTA results on 7 out of 9 diverse reasoning benchmarks. The study also shows that model faithfulness positively correlates with overall performance and further demonstrate that FLARE allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search. <br> <br>

35. ***Thinking LLMs: <br>
This study proposes a new training method to teach LLMs to "think" before generating responses. Using an iterative optimization process, the model explores and refines potential thought patterns before answering questions, improving performance in areas like reasoning, marketing, and general knowledge. This approach enhances the model's ability to handle complex instructions without additional human data.*** <br> <br>
    Oct 14, Meta, UC Berkley, NYU published a [paper](https://arxiv.org/pdf/2410.10630) “Thinking LLMs: General Instruction Following with Thought Generation”. LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. The work proposes a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. The study achieves this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. The study shows that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning & problem-solving tasks. <br> <br>

37. ***MMCOMPOSITION: <br>
Researchers introduced MMCOMPOSITION, a benchmark for evaluating the compositionality of Vision-Language Models (VLMs). This benchmark tests models’ ability to combine visual and textual elements, revealing limitations in current VLMs’ capacity to handle complex compositions. The study finds that GPT-4 falls short in compositionality compared to other open-source models, prompting recommendations for improving VLMs.*** <br> <br>
    Oct 13, Uni of Rochester, Apple, and Microsoft published a [paper](https://arxiv.org/pdf/2410.09733) “MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models”. The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video captioning, visual question answering, and cross-modal retrieval. Despite VLMs' superior capabilities, researchers lack a comprehensive understanding of their compositionality -- the ability to understand and produce novel combinations of known visual and textual components. Prior benchmarks provide only a relatively rough compositionality evaluation from the perspectives of objects, relations, and attributes while neglecting deeper reasoning about object interactions, counting, and complex compositions. However, compositionality is a critical ability that facilitates coherent reasoning and understanding across modalities for VLMs. To address this limitation, the study proposes MMCOMPOSITION, a novel human-annotated benchmark for comprehensively and accurately evaluating VLMs' compositionality. The proposed benchmark serves as a complement to these earlier works. With MMCOMPOSITION, the study can quantify and explore the compositionality of the mainstream VLMs. Surprisingly, the work finds GPT-4o's compositionality inferior to the best open-source model, and the authors analyze the underlying reasons. Experimental analysis reveals the limitations of VLMs in fine-grained compositional perception and reasoning, and points to areas for improvement in VLM design and training. Resources available at: https://hanghuacs.github.io/MMComposition/ <br> <br>

39. ***ReLU’s Revival in LayerNorm-free Models: <br>
This study highlights issues with using GELU activation functions in LayerNorm-free language models, revealing that ReLU significantly outperforms GELU in these settings. ReLU’s geometric properties enhance learning dynamics, information retention, and overall model performance. The work offers insights for optimizing transformer architectures by avoiding entropic overload caused by smoother activations like GELU.*** <br> <br>
    Oct 13, NYU published a [paper](https://arxiv.org/pdf/2410.09637) “ReLU's Revival: On the Entropic Overload in Normalization-Free Large Language Models”. LayerNorm is a critical component in modern large language models (LLMs) for stabilizing training and ensuring smooth optimization. However, it introduces significant challenges in mechanistic interpretability, outlier feature suppression, faithful signal propagation, and computational and communication complexity of private inference. This work explores desirable activation functions in normalization-free decoder-only LLMs. Contrary to the conventional preference for the GELU in transformer-based models, empirical findings demonstrate an opposite trend— ReLU significantly outperforms GELU in LayerNorm-free models, leading to an 8.2% perplexity improvement. The work discovers a key issue with GELU, where early layers experience entropic overload, leading to the under-utilization of the representational capacity of attention heads. This highlights that smoother activations like GELU are ill-suited for LayerNorm-free architectures, whereas ReLU’s geometrical properties—specialization in input space and intra-class selectivity—lead to improved learning dynamics and better information retention in the absence of LayerNorm. This study offers key insights for optimizing transformer architectures where LayerNorm introduces significant challenges. <br> <br>

41. ***Taming Overconfidence in LLMs: <br>
This paper examines the overconfidence problem in LLMs fine-tuned with Reinforcement Learning from Human Feedback (RLHF). It introduces two Proximal Policy Optimization (PPO) variants—PPO-M and PPO-C—that reduce overconfidence by adjusting reward calculations. These methods are shown to lower calibration error without compromising model performance across diverse datasets.*** <br> <br>
    Oct 13, CMU, UC Berkeley et al published a [paper](https://arxiv.org/pdf/2410.09724) “Taming Overconfidence in LLMs: Reward Calibration in RLHF”. Language model calibration refers to the alignment between the confidence of the model and the actual performance of its responses. While previous studies point out the overconfidence phenomenon in Large Language Models (LLMs) and show that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) are overconfident with a more sharpened output probability, this study reveals that RLHF tends to lead models to express verbalized overconfidence in their own responses. The study investigates the underlying cause of this overconfidence and demonstrate that reward models used for Proximal Policy Optimization (PPO) exhibit inherent biases towards high-confidence scores regardless of the actual quality of responses. Building upon this insight, the work proposes two PPO variants: PPO-M: PPO with Calibrated Reward Modeling and PPO-C: PPO with Calibrated Reward Calculation. PPO-M integrates explicit confidence scores in reward model training, which calibrates reward models to better capture the alignment between response quality and verbalized confidence. PPO-C adjusts the reward score during PPO based on the difference between the current reward and the moving average of past rewards. Both PPO-M and PPO-C can be seamlessly integrated into the current PPO pipeline and do not require additional golden labels. The study evaluates the methods on both Llama3-8B and Mistral-7B across six diverse datasets including multiple-choice and open-ended generation. Experiment results demonstrate that both of the methods can reduce calibration error and maintain performance comparable to standard PPO. The study further shows that they do not compromise model capabilities in open-ended conversation settings. <br> <br>

43. ***ALLoRA: Adaptive Learning Rate in LoRA Finetuning: <br>
ALLoRA improves upon the existing Low-Rank Adaptation (LoRA) method for LLM finetuning by addressing limitations related to Dropout and scaling factors in short training episodes. By introducing adaptive learning rates, ALLoRA achieves better performance and efficiency, eliminating the need for certain hyperparameters. This approach shows significant accuracy gains over LoRA and its variants.*** <br> <br>
    Oct 13, Google and Brown Uni published a [paper](https://arxiv.org/pdf/2410.09692) “ALLoRA: Adaptive Learning Rate Mitigates LoRA Fatal Flaws”. Low-Rank Adaptation (LoRA) is the bread and butter of Large Language Model (LLM) finetuning. LoRA learns an additive low-rank perturbation, AB, of a pretrained matrix parameter W to align the model to a new task or dataset with W+AB. The study identifies three core limitations to LoRA for finetuning--a setting that employs limited amount of data and training steps. First, LoRA employs Dropout to prevent overfitting. The work proves that Dropout is only suitable for long training episodes but fails to converge to a reliable regularizer for short training episodes. Second, LoRA's initialization of B at 0 creates a slow training dynamic between A and B. That dynamic is also exacerbated by Dropout that further slows the escape from 0 for B which is particularly harmful for short training episodes. Third, the scaling factor multiplying each LoRA additive perturbation creates “short-sighted'” interactions between the LoRA modules of different layers. Motivated by principled analysis of those limitations, the study proposes an elegant solution: a Dropout-free, scaling-free, LoRA with Adaptive Learning rate--coined ALLoRA. By scaling the per sample and per parameter gradients with a coefficient inversely proportional to parameters' ℓ2 norm, ALLoRA alleviates those three limitations. As a by-product, ALLoRA removes two hyper-parameters from LoRA: the scaling factor and the dropout rate. Empirical results show that ALLoRA admits better accuracy than LoRA on various settings, including against recent LoRA variants such as Weight-Decomposed Low-Rank Adaptation (DoRA). Ablation studies show the solution is the optimal in a family of weight-dependent / output-dependent approaches on various LLMs including the latest Llama3. <br> <br>

45. ***Controllable Safety Alignment: <br>
This paper introduces a flexible framework, Controllable Safety Alignment (CoSA), for adapting LLMs to diverse safety requirements at inference time without retraining. Users can modify safety configurations using natural language prompts, allowing models to align with different cultural and social norms. CoSAlign improves model controllability and practicality, addressing the limitations of static safety standards.*** <br> <br>
    Oct 11, Microsoft and Johns Hopkins Uni published a [paper](https://arxiv.org/pdf/2410.08968) “Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements”. The current paradigm for safety alignment of large language models (LLMs) follows a one-size-fits-all approach: the model refuses to interact with any content deemed unsafe by the model provider. This approach lacks flexibility in the face of varying social norms across cultures and regions. In addition, users may have diverse safety needs, making a model with static safety standards too restrictive to be useful, as well as too costly to be re-aligned. The work proposes Controllable Safety Alignment (CoSA), a framework designed to adapt models to diverse safety requirements without re-training. Instead of aligning a fixed model, the work aligns models to follow safety configs -- free-form natural language descriptions of the desired safety behaviors -- that are provided as part of the system prompt. To adjust model safety behavior, authorized users only need to modify such safety configs at inference time. To enable that, the work proposes CoSAlign, a data-centric method for aligning LLMs to easily adapt to diverse safety configs. Furthermore, the study devises a novel controllability evaluation protocol that considers both helpfulness and configured safety, summarizing them into CoSA-Score, and construct CoSApien, a human-authored benchmark that consists of real-world LLM use cases with diverse safety requirements and corresponding evaluation prompts. The study shows that CoSAlign leads to substantial gains of controllability over strong baselines including in-context alignment. The framework encourages better representation and adaptation to pluralistic human values in LLMs, and thereby increasing their practicality. <br> <br>

47. ***MiRAGeNews: AI-Generated News Detection: <br>
The study presents the MiRAGeNews Dataset, designed to detect AI-generated fake news by analyzing image-caption pairs. The dataset poses a significant challenge for both humans and multi-modal LLMs. The authors propose a multi-modal detector, MiRAGe, that improves detection accuracy and helps combat the spread of misleading AI-generated content.*** <br> <br>
    Oct 11, Uni of Pennsylvania published a [paper](https://arxiv.org/pdf/2410.09045) “MiRAGeNews: Multimodal Realistic AI-Generated News Detection”. The proliferation of inflammatory or misleading "fake" news content has become increasingly common in recent years. Simultaneously, it has become easier than ever to use AI tools to generate photorealistic images depicting any scene imaginable. Combining these two -- AI-generated fake news content -- is particularly potent and dangerous. To combat the spread of AI-generated fake news, the work proposes the MiRAGeNews Dataset, a dataset of 12,500 high-quality real and AI-generated image-caption pairs from state-of-the-art generators. The study finds that the dataset poses a significant challenge to humans (60% F-1) and state-of-the-art multi-modal LLMs (< 24% F-1). Using the dataset the authors train a multi-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art baselines on image-caption pairs from out-of-domain image generators and news publishers. The authors release the code and data to aid future work on detecting AI-generated content. <br> <br>

49. ***Agents Thinking Fast and Slow: <br>
Inspired by Kahneman's "Thinking Fast and Slow," this study proposes a dual-agent architecture for LLMs. A "Talker" agent handles quick, intuitive conversational responses, while a "Reasoner" agent deals with more complex multi-step reasoning and planning tasks. This modular system improves interaction by balancing speed and logical precision, demonstrated through a sleep-coaching agent.*** <br> <br>
    Oct 10, Google published a [paper](https://arxiv.org/pdf/2410.08328)  “Agents Thinking Fast and Slow: A Talker-Reasoner Architecture”. Large language models have enabled agents of all kinds to interact with users through natural conversation. Consequently, agents now have two jobs: conversing and planning/reasoning. Their conversational responses must be informed by all available information, and their actions must help to achieve goals. This dichotomy between conversing with the user and doing multi-step reasoning and planning can be seen as analogous to the human systems of "thinking fast and slow" as introduced by Kahneman. The proposed approach is comprised of a "Talker" agent (System 1) that is fast and intuitive, and tasked with synthesizing the conversational response; and a "Reasoner" agent (System 2) that is slower, more deliberative, and more logical, and is tasked with multi-step reasoning and planning, calling tools, performing actions in the world, and thereby producing the new agent state. The study describes the new Talker-Reasoner architecture and discuss its advantages, including modularity and decreased latency. The authors ground the discussion in the context of a sleep coaching agent, in order to demonstrate real-world relevance. <br> <br>

51. ***KV Prediction for Faster Inference: <br>
The paper introduces KV Prediction, a method to reduce the time taken for LLMs to generate their first token during inference. By using an auxiliary model to approximate the KV cache, the method significantly improves efficiency without sacrificing accuracy. This approach is especially useful for edge devices and demonstrates improvements in various benchmarks.*** <br> <br>
    Oct 10, Apple published a [paper](https://arxiv.org/pdf/2410.08391) “KV Prediction for Improved Time to First Token”. Inference with transformer-based language models begins with a prompt processing step. In this step, the model generates the first output token and stores the KV cache needed for future generation steps. This prompt processing step can be computationally expensive, taking 10s of seconds or more for billion-parameter models on edge devices when prompt lengths or batch sizes rise. This degrades user experience by introducing significant latency into the model's outputs. To reduce the time spent producing the first output (known as the “time to first token”, or TTFT) of a pretrained model, the study introduces a novel method called KV Prediction. In this method, a small auxiliary model is used to process the prompt and produce an approximation of the KV cache used by a base model. This approximated KV cache is then used with the base model for autoregressive generation without the need to query the auxiliary model again. The study demonstrates that the method produces a pareto-optimal efficiency-accuracy trade-off when compared to baselines. On TriviaQA, the study demonstrates relative accuracy improvements in the range of 15%-50% across a range of TTFT FLOPs budgets. The work also demonstrates accuracy improvements of up to 30% on HumanEval python code completion at fixed TTFT FLOPs budgets. Additionally, the authors benchmark models on an Apple M2 Pro CPU and demonstrate that the improvement in FLOPs translates to a TTFT speedup on hardware. The authors release our code at https://github.com/apple/corenet/tree/main/projects/kv-prediction. <br> <br>

53. ***GenARM: Autoregressive Reward Model for Test-time Alignment: <br>
GenARM proposes a test-time alignment method using an Autoregressive Reward Model to guide frozen LLMs. Unlike previous methods, it can compute next-token rewards, making it more suitable for autoregressive text generation. The approach achieves comparable performance to traditional methods while allowing multi-objective alignment and efficient preference guidance without retraining.*** <br> <br>
    Oct 10, Uni Maryland and JPMorgan AI Research published a [paper](https://arxiv.org/pdf/2410.08193) “GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment”. Large Language Models (LLMs) exhibit impressive capabilities but require careful alignment with human preferences. Traditional training-time methods finetune LLMs using human preference datasets but incur significant training costs and require repeated training to handle diverse user preferences. Test-time alignment methods address this by using reward models (RMs) to guide frozen LLMs without retraining. However, existing test-time approaches rely on trajectory-level RMs which are designed to evaluate complete responses, making them unsuitable for autoregressive text generation that requires computing next-token rewards from partial responses. To address this, the study introduces GenARM, a test-time alignment approach that leverages the Autoregressive Reward Model--a novel reward parametrization designed to predict next-token rewards for efficient and effective autoregressive generation. Theoretically, the work demonstrates that this parametrization can provably guide frozen LLMs toward any distribution achievable by traditional RMs within the KL-regularized reinforcement learning framework. Experimental results show that GenARM significantly outperforms prior test-time alignment baselines and matches the performance of training-time methods. Additionally, GenARM enables efficient weak-to-strong guidance, aligning larger LLMs with smaller RMs without the high costs of training larger models. Furthermore, GenARM supports multi-objective alignment, allowing real-time trade-offs between preference dimensions and catering to diverse user preferences without retraining. <br> <br>

55. ***Long-Context LLMs and RAG: <br>
The study explores the limitations of using long-context LLMs for retrieval-augmented generation (RAG). It identifies the negative impact of retrieved "hard negatives" on performance as context length increases. The authors propose training-free and training-based optimizations to enhance RAG effectiveness in long-context scenarios, achieving notable improvements in robustness and performance.*** <br> <br>
    Oct 8, Google and UIUC published a [paper](https://arxiv.org/pdf/2410.05983) “Long-Context LLMs Meet RAG”. Retrieval-augmented generation (RAG) empowers large language models (LLMs) to utilize external knowledge sources. The increasing capacity of LLMs to process longer input sequences opens up avenues for providing more retrieved information, to potentially enhance the quality of generated outputs. It is plausible to assume that a larger retrieval set would contain more relevant information (higher recall), that might result in improved performance. However, empirical findings demonstrate that for many long-context LLMs, the quality of generated output initially improves first, but then subsequently declines as the number of retrieved passages increases. This paper investigates this phenomenon, identifying the detrimental impact of retrieved "hard negatives" as a key contributor. To mitigate this and enhance the robustness of long-context LLM-based RAG, the study proposes both training-free and training-based approaches. The work first showcases the effectiveness of retrieval reordering as a simple yet powerful training-free optimization. Furthermore, the work explores training-based methods, specifically RAG-specific implicit LLM fine-tuning and RAG-oriented fine-tuning with intermediate reasoning, demonstrating their capacity for substantial performance gains. Finally, the study conducts a systematic analysis of design choices for these training-based methods, including data distribution, retriever selection, and training context length. <br> <br>

57. ***GSM-Symbolic: Limitations in Mathematical Reasoning: <br>
This study examines the mathematical reasoning capabilities of LLMs using a new benchmark called GSM-Symbolic. It reveals that LLMs struggle with consistent performance when minor variations are made to question templates, particularly in multi-clause problems. The findings suggest that current models replicate reasoning patterns from training data, rather than performing genuine logical reasoning, highlighting areas for improvement.*** <br> <br>
    Oct 7, Apple published a [paper](https://arxiv.org/abs/2410.05229) “GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models”. Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, the work conducts a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, the work introduces GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of this http URL findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, the study investigates the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. The authors hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, the work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.
 <br> <br>


***Oct 13***

1. ***Improving reasoning in LLMs with PRMs:  <br>Google and CMU explore process reward models (PRMs) to enhance reasoning in large language models (LLMs). PRMs offer feedback at each reasoning step, unlike outcome reward models (ORMs), which only provide final feedback. The key is measuring progress at each step via a distinct prover policy. They find weak provers can improve stronger base policies, boosting accuracy and efficiency by over 6%, and enabling gains in reinforcement learning (RL).*** <br> <br>
   Oct 11, Google and CMU published a [paper](https://papers.cool/arxiv/2410.08146#:~:text=A%20promising%20approach%20for%20improving,feedback%20at%20the%20final%20step.) “Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning”. A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, potentially improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. To improve a base policy by running search against a PRM or using it as dense rewards for reinforcement learning (RL), the study asks: “How should we design process rewards?”. The key insight is that, to be effective, the process reward for a step should measure progress: a change in the likelihood of producing a correct response in the future, before and after taking the step, corresponding to the notion of step-level advantages in RL. Crucially, this progress should be measured under a prover policy distinct from the base policy. The work theoretically characterize the set of good provers and the results show that optimizing process rewards from such provers improves exploration during test-time search and online RL. In fact, the characterization shows that weak prover policies can substantially improve a stronger base policy, which the authors also observe empirically. The work validates the claims by training process advantage verifiers (PAVs) to predict progress under such provers, and show that compared to ORMs, test-time search against PAVs is > 8% more accurate, and 1.5 − 5× more compute-efficient. Online RL with dense rewards from PAVs enables one of the first results with 5 − 6× gain in sample efficiency, and > 6% gain in accuracy, over ORMs. <br> <br>

3. ***Nobel Prize 2024 in AI and Physics:  <br>John Hopfield and Geoffrey Hinton won the 2024 Nobel Prize in Physics for their groundbreaking work in machine learning. Their inventions fueled the AI revolution, with Hinton famously warning about the risks of advanced AI. The award coincides with another Nobel Prize in Chemistry for AI-related breakthroughs in protein design, highlighting AI’s growing influence across scientific fields.*** <br> <br>
   Oct 10, [according to Reuters](https://www.reuters.com/science/hopfield-hinton-win-2024-nobel-prize-physics-2024-10-08/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=meta-teases-a-new-video-model&_bhlid=eff04f5277f7d09ad170836c27d4f4b3e57f5868#:~:text=STOCKHOLM%2C%20Oct%208%20(Reuters),for%20the%20artificial%20intelligence%20boom.), “Nobel physics prize 2024 won by AI pioneers John Hopfield and Geoffrey Hinton”. U.S. scientist John Hopfield and British-Canadian Geoffrey Hinton won the 2024 Nobel Prize in Physics on Tuesday for discoveries and inventions in machine learning that paved the way for the artificial intelligence boom. Heralded for its revolutionary potential in areas ranging from cutting-edge scientific discovery to more efficient admin, the emerging technology on which the duo worked has also raised fears humankind may soon be outsmarted and outcompeted by its own creation. Hinton has been widely credited as a godfather of AI and made headlines when he quit his job at Google (GOOGL.O), opens new tab last year to be able to more easily speak about the dangers of the technology he had pioneered. Hopfield, 91, a professor emeritus at Princeton University, created an associative memory that can store and reconstruct images and other types of patterns in data. He echoed Hinton's concerns, saying there was something unnerving about the unknown potential and limits of AI. One day after the physics prize was announced, Demis Hassabis, John Jumper, and David Baker won the Chemistry Nobel Prize for their work on AlphaFold and protein design. AlphaFold and AlphaFold 2, as well as the work of Baker’s lab, are compelling applications of AI that made significant steps forward in chemistry and biology, and this award, too, is well deserved! It’s remarkable that the Nobel committees for physics and chemistry, which are made up of scientists in those fields, chose to honor AI researchers with this year’s awards.  <br> <br>

5. ***State of AI Report:  <br>This report examines five AI dimensions: research, industry, politics, safety, and predictions. OpenAI leads in research, NVIDIA dominates the industry, and AI’s political impact is yet to unfold. Companies are shifting focus from safety to sales. Predictions include a rise in viral AI apps and open-source alternatives to dominant models, while investment in humanoids may decline.*** <br> <br>
   Oct 10, Stateof.ai and airstreet.com [published](https://docs.google.com/presentation/d/1GmZmoWOa2O92BPrncRcTKa15xvQGhq7g4I4hJSNlC0M/edit#slide=id.g309a25a756d_0_85) “State of AI Report”.  The report considered five key dimensions. Research – Frontier lab performance converges, but OpenAI maintains its edge following the launch of o1, as planning and reasoning emerge as a major frontier. Foundation models demonstrate their ability to break out of language as multimodal research drives into mathematics, biology, genomics, the physical sciences, and neuroscience. Industry - NVIDIA remains the most powerful company in the world, enjoying a stint in the $3T club, while regulators probe the concentrations of power within GenAI. More established GenAI companies bring in billions of dollars in revenue, while start-ups begin to gain traction in sectors like video and audio generation. Although companies begin to make the journey from model to product, long-term questions around pricing and sustainability remain unresolved. Driven by a bull run in public markets, AI companies reach $9T in value, while investment levels grow healthily in private companies. Politics - Anticipated AI effects on elections, employment and a range of other sensitive areas are yet to be realized at any scale. Safety - A vibe-shift from safety to acceleration takes place as companies that previously warned us about the pending extinction of humanity need to ramp up enterprise sales and usage of their consumer apps. Every proposed jailbreaking ‘fix’ has failed, but researchers are increasingly concerned with more sophisticated, long-term attacks. Predictions - An app or website created solely by someone with no coding ability will go viral (e.g. App Store Top-100). Frontier labs implement meaningful changes to data collection practices after cases begin reaching trial. An open source alternative to OpenAI o1 surpasses it across a range of reasoning benchmarks. Levels of investment in humanoids will trail off, as companies struggle to achieve product-market fit. <br> <br>
 
7. ***Nvidia’s Mixture of Experts (MoE) Models:  <br>Nvidia introduces methods to convert pre-trained dense models into sparse Mixture of Experts (MoE) models, increasing capacity without retraining from scratch. Their new initialization and routing techniques outperform dense models, with upcycled models achieving higher accuracy and efficiency, particularly in massive language tasks.*** <br> <br>
   Oct 10, Nvidia published a [paper](https://arxiv.org/pdf/2410.07524) “Upcycling Large Language Models into Mixture of Experts”. Upcycling pre-trained dense language models into sparse mixture-of-experts (MoE) models is an efficient approach to increase the model capacity of already trained models. However, optimal techniques for upcycling at scale remain unclear. In this work, the authors conduct an extensive study of upcycling methods and hyperparameters for billion-parameter scale language models. The work proposes a novel "virtual group" initialization scheme and weight scaling approach to enable upcycling into fine-grained MoE architectures. Through ablations, the work finds that upcycling outperforms continued dense model training. In addition, the study shows that softmax-then-topK expert routing improves over topK-then-softmax approach and higher granularity MoEs can help improve accuracy. Finally, the work upcycled Nemotron-4 15B on 1T tokens and compared it to a continuously trained version of the same model on the same 1T tokens: the continuous trained model achieved 65.3% MMLU, whereas the upcycled model achieved 67.6%. The results offer insights and best practices to effectively leverage upcycling for building MoE language models. <br> <br>

9. ***Autoregressive Video Diffusion Models:  <br>Stony Brook University and Adobe extend video diffusion models to generate longer videos by progressively denoising latent frames. This technique allows the generation of high-quality, 1-minute videos without scene disruptions. The new models achieve state-of-the-art results in long video generation, overcoming previous limitations.*** <br> <br>
    Oct 10, Stony Brook Uni and Adobe published a [paper](https://arxiv.org/pdf/2410.08151) “Progressive Autoregressive Video Diffusion Models”. Current frontier video diffusion models have demonstrated remarkable results at generating high-quality videos. However, they can only generate short video clips, normally around 10 seconds or 240 frames, due to computation limitations during training. This work shows that existing models can be naturally extended to autoregressive video diffusion models without changing the architectures. The key idea is to assign the latent frames with progressively increasing noise levels rather than a single noise level, which allows for fine-grained condition among the latents and large overlaps between the attention windows. Such progressive video denoising allows the models to autoregressively generate video frames without quality degradation or abrupt scene changes. The paper presents state-of-the-art results on long video generation at 1 minute (1440 frames at 24 FPS). Videos from this paper are available at https://desaixie.github.io/pa-vdm/. <br> <br>

11. ***Agent S: Automating GUI Tasks:  <br>Agent S, developed by Simular Research, introduces a framework for autonomous computer interactions via GUI, solving complex tasks using hierarchical planning. It outperforms existing models in task success rates and shows broad generalizability across systems. The framework represents a significant step forward in human-computer interaction automation.*** <br> <br>
    Oct 10, Simular Research published a [paper](https://arxiv.org/pdf/2410.08164) “Agent S: An Open Agentic Framework that Uses Computers Like a Human”. The paper presents Agent S, an open agentic framework that enables autonomous interaction with computers through a Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S aims to address three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. In addition, it employs an Agent-Computer Interface (ACI) to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models (MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37% on success rate (an 83.6% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark. Code available at https://github.com/simular-ai/Agent-S. <br> <br>

13. ***Agentic Reasoning Era in AI:  <br>An article from Sequoia highlights the shift from pattern-based AI to agentic reasoning. This evolution allows AI to deliberate before responding, enhancing problem-solving capabilities. AI is moving towards performing work across various sectors, suggesting future AI applications will revolutionize service industries.*** <br> <br>
    Oct 9, Seauoia published an [article](https://www.sequoiacap.com/article/generative-ais-act-o1/) “Generative AI’s Act o1 - The Agentic Reasoning Era Begins”. The article discusses the ongoing evolution of Generative AI from rapid, pattern-based responses ("System 1") to deeper, deliberate reasoning ("System 2"). It highlights that the market for large language models (LLMs) has stabilized among major players (Microsoft/OpenAI, AWS/Anthropic, Google/DeepMind). However, the next frontier is enhancing AI's reasoning abilities at inference time, enabling AI to "stop and think" before responding, akin to the reasoning used by AlphaGo in its famous 2016 Go match. The new "Strawberry" model from OpenAI is the first to implement this, leveraging "inference-time compute" for more thoughtful problem-solving, especially in structured domains like math and coding. This shift introduces a new scaling law: more inference-time compute leads to better reasoning, potentially enabling AI to solve increasingly complex problems, from programming to mathematics. In terms of applications, a new wave of agentic applications is emerging, powered by this advanced reasoning layer. These applications move beyond mere automation, targeting specific tasks in various sectors, from legal work (Harvey) to cybersecurity (XBOW) and software engineering (Factory). The essay argues that AI is shifting from selling software to selling "work," expanding into trillion-dollar service markets. The article concludes with a focus on the potential of multi-agent systems, where AI agents collaborate to achieve more complex goals, signaling the next phase of AI development and hinting at the emergence of true AGI (Artificial General Intelligence). <br> <br>

15. ***Repetition vs. Data Diversity in Transformers:  <br>Meta explores how repeated training examples improve transformers' performance over diverse data in mathematical tasks. Their findings suggest that repetition can accelerate learning and performance in structured tasks like math, providing insights into balancing generalization and memorization in deep learning.*** <br> <br>
    Oct 9, Meta published a [paper](https://arxiv.org/pdf/2410.07041) “Emergent properties with repeated examples”. The work studies the performance of transformers as a function of the number of repetitions of training examples with algorithmically generated datasets. On three problems of mathematics: the greatest common divisor, modular multiplication, and matrix eigenvalues, the work shows that for a fixed number of training steps, models trained on smaller sets of repeated examples outperform models trained on larger sets of single-use examples. The work also demonstrates that two-set training - repeated use of a small random subset of examples, along normal sampling on the rest of the training set - provides for faster learning and better performance. This highlights that the benefits of repetition can outweigh those of data diversity. These datasets and problems provide a controlled setting to shed light on the still poorly understood interplay between generalization and memorization in deep learning. <br> <br>

17. ***Pixtral-12B Multimodal Language Model:  <br> <br>Mistral introduces Pixtral-12B, a highly efficient multimodal language model outperforming larger counterparts on image and document understanding benchmarks. It excels without compromising natural language processing capabilities, and its ability to process multiple images in long contexts sets it apart from other models.*** <br> <br>
    Oct 9, Mistral published a [paper](https://arxiv.org/pdf/2410.07073) “Pixtral 12B”. The paper introduces Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license. <br> <br>

19. ***MLE-Bench for Machine Learning Engineering:  <br> <br>OpenAI introduces MLE-Bench, a benchmark to evaluate AI agents' machine learning engineering skills across 75 tasks. The study demonstrates AI's ability to match human performance in some competitions, although resource scaling and contamination from pre-training remain challenges.*** <br> <br>
    Oct 9, OpenAI published a [paper](https://arxiv.org/pdf/2410.07095) “MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering”. The paper introduces MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, the study curates 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. The study establishes human baselines for each competition using Kaggle's publicly available leaderboards. The work uses open-source agent scaffolds to evaluate several frontier language models on the benchmark, finding that the best-performing setup--OpenAI's o1-preview with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to the main results, the study investigates various forms of resource scaling for AI agents and the impact of contamination from pre-training. OpenAI open-source the benchmark code (github.com/openai/mle-bench/) to facilitate future research in understanding the ML engineering capabilities of AI agents. <br> <br>

21. ***Human-AI Complementarity in QA:  <br>A study investigates AI and human performance in question-answering tasks, revealing that humans outperform AI in abductive reasoning, while AI excels in fact-based retrieval. The findings emphasize the need for QA tasks that push AI towards higher-order reasoning and nuanced understanding.*** <br> <br>
    Oct 9, Uni of Maryland and Microsoft published a [paper](https://arxiv.org/pdf/2410.06524) “Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA”. Recent advancements of large language models (LLMs) have led to claims of AI surpassing humans in natural language processing (NLP) tasks such as textual understanding and reasoning. This work investigates these assertions by introducing CAIMIRA, a novel framework rooted in item response theory (IRT) that enables quantitative assessment and comparison of problem-solving abilities of question-answering (QA) agents: humans and AI systems. Through analysis of over 300,000 responses from ~70 AI systems and 155 humans across thousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in knowledge domains and reasoning skills. Humans outperform AI systems in knowledge-grounded abductive and conceptual reasoning, while state-of-the-art LLMs like GPT-4 and LLaMA show superior performance on targeted information retrieval and fact-based reasoning, particularly when information gaps are well-defined and addressable through pattern matching or data retrieval. These findings highlight the need for future QA tasks to focus on questions that challenge not only higher-order reasoning and scientific thinking, but also demand nuanced linguistic interpretation and cross-contextual knowledge application, helping advance AI developments that better emulate or complement human cognitive abilities in real-world problem-solving. <br> <br>

23. ***Cheating in Automatic LLM Benchmarks:  <br>Research reveals that LLM benchmarks can be gamed by null models that produce irrelevant outputs but still achieve high win rates. This highlights the need for better anti-cheating mechanisms to maintain the reliability of AI evaluations.*** <br> <br>
    Oct 9, Sea AI Lab and Singapore Management Uni published a [paper](https://arxiv.org/pdf/2410.07137) “Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates”. Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, the study shows that even a "null model" that always outputs a constant response (irrelevant to input instructions) can cheat automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench. Moreover, the crafted cheating outputs are transferable because the authors assume that the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are private and cannot be accessed. While the experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. The findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks. The code is available at https://github.com/sail-sg/Cheating-LLM-Benchmarks. <br> <br>

25. ***Retrieval-Augmented Decision Transformer:  <br>A new method for reinforcement learning, RA-DT uses external memory to store and retrieve relevant past experiences, improving decision-making in complex environments. This innovation addresses the limitations of in-context learning in reinforcement learning, particularly in long, sparse-reward tasks.*** <br> <br>
    Oct 9, JKU Liz, Extensity AI, Google, UCL et al published a [paper](https://arxiv.org/pdf/2410.07071) “Retrieval-Augmented Decision Transformer: External Memory for In-context RL”. In-context learning (ICL) is the ability of a model to learn a new task by observing a few exemplars in its context. While prevalent in NLP, this capability has recently also been observed in Reinforcement Learning (RL) settings. Prior in-context RL methods, however, require entire episodes in the agent's context. Given that complex environments typically lead to long episodes with sparse rewards, these methods are constrained to simple environments with short episodes. To address these challenges, the study introduces Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external memory mechanism to store past experiences from which it retrieves only sub-trajectories relevant for the current situation. The retrieval component in RA-DT does not require training and can be entirely domain-agnostic. The work evaluates the capabilities of RA-DT on grid-world environments, robotics simulations, and procedurally-generated video games. On grid-worlds, RA-DT outperforms baselines, while using only a fraction of their context length. Furthermore, the study illuminates the limitations of current in-context RL methods on complex environments and discuss future directions. To facilitate future research, the authors release datasets for four of the considered environments. <br> <br>

27. ***LLMs Performing Multiple Tasks Simultaneously:  <br>A surprising discovery shows LLMs can perform multiple distinct tasks simultaneously during a single inference call. This "task superposition" capability highlights the latent potential of LLMs and raises new questions about the underlying mechanisms enabling such versatility.*** <br> <br>
    Oct 8, Uni of Wisconsin-Madison, Uni of Michigan and Microsoft published a [paper](https://arxiv.org/pdf/2410.05603) “Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition”. Large Language Models (LLMs) have demonstrated remarkable in-context learning (ICL) capabilities. This study explores a surprising phenomenon related to ICL: LLMs can perform multiple, computationally distinct ICL tasks simultaneously, during a single inference call, a capability we term "task superposition". The study provides empirical evidence of this phenomenon across various LLM families and scales and show that this phenomenon emerges even if the authors train the model to in-context learn one task at a time. The work offers theoretical explanations that this capability is well within the expressive power of transformers. The study also explores how LLMs internally compose task vectors during superposition. Furthermore, the study shows that larger models can solve more ICL tasks in parallel, and better calibrate their output distribution. The findings offer insights into the latent capabilities of LLMs, further substantiate the perspective of "LLMs as superposition of simulators", and raise questions about the mechanisms enabling simultaneous task execution. <br> <br>

29. ***Model Collapse Due to Synthetic Data:  <br>A study reveals that even a small amount of synthetic data can cause severe performance degradation, or "model collapse," in large neural networks. This phenomenon persists even as dataset size increases, prompting further investigation into how scaling affects model robustness.*** <br> <br>
    Oct 8, Meta, NYU and UCLA published a [paper](https://arxiv.org/pdf/2410.04840) “Strong Model Collapse”. Within the scaling laws paradigm, which underpins the training of large neural networks like ChatGPT and Llama, the authors consider a supervised regression setting and establish the existance of a strong form of the model collapse phenomenon, a critical performance degradation due to synthetic data in the training corpus. The results show that even the smallest fraction of synthetic data (e.g., as little as 1\% of the total training dataset) can still lead to model collapse: larger and larger training sets do not enhance performance. The study further investigates whether increasing model size, an approach aligned with current trends in training large language models, exacerbates or mitigates model collapse. In a simplified regime where neural networks are approximated via random projections of tunable size, the study both theoretically and empirically shows that larger models can amplify model collapse. Interestingly, the theory also indicates that, beyond the interpolation threshold (which can be extremely high for very large datasets), larger models may mitigate the collapse, although they do not entirely prevent it. The theoretical findings are empirically verified through experiments on language models and feed-forward neural networks for images. <br> <br>

31. ***Intelligence at the Edge of Chaos:  <br>Researchers explore how complexity in rule-based systems influences the intelligence exhibited by AI models. They find that systems with intermediate complexity levels lead to more intelligent behavior, suggesting that exposure to complexity may be key to developing artificial intelligence.*** <br> <br>
    Oct 8, Yale Uni, Northwestern Uni and Idaho State Uni published a [paper](https://www.arxiv.org/pdf/2410.02536) “Intelligence at the Edge of Chaos”. The study explores the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. The study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems that generate behaviors ranging from trivial to highly complex. By training distinct Large Language Models (LLMs) on different ECAs, the work evaluated the relationship between the complexity of the rules' behavior and the intelligence exhibited by the LLMs, as reflected in their performance on downstream tasks. The findings reveal that rules with higher complexity lead to models exhibiting greater intelligence, as demonstrated by their performance on reasoning and chess move prediction tasks. Both uniform and periodic systems, and often also highly chaotic systems, resulted in poorer downstream performance, highlighting a sweet spot of complexity conducive to intelligence. The work conjectures that intelligence arises from the ability to predict complexity and that creating intelligence may require only exposure to complexity. <br> <br>

33. ***Differential Transformer (Microsoft & Tsinghua University):  <br>This work introduces Diff Transformer, which enhances attention to relevant context while reducing noise by using a differential attention mechanism. Experimental results demonstrate its effectiveness in various language modeling tasks, including long-context modeling and hallucination mitigation. The architecture shows improved performance compared to standard transformers and enhances in-context learning robustness.*** <br> <br>
    Oct 8, Microsoft and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2410.05258) “Differential Transformer”. Transformer tends to overallocate attention to irrelevant context. This work introduces Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models. [Code is here](https://github.com/microsoft/unilm/tree/master/Diff-Transformer). <br> <br>

35. ***Falcon Mamba: Attention-Free 7B Model (TII Abu Dhabi):  <br>Falcon Mamba 7B is a novel language model based on the Mamba architecture, trained on 5.8 trillion tokens. It surpasses models like Mistral 7B and Llama3.1, offering faster inference and lower memory requirements. The model outperforms Transformer-based and hybrid designs, making it a top-performing Mamba model.*** <br> <br>
    Oct 7, TII Aub Dhabi published a [paper](https://arxiv.org/pdf/2410.05355) “Falcon Mamba: The First Competitive Attention-free 7B Language Model”. This report presents Falcon Mamba 7B, a new base large language model based on the novel Mamba architecture. Falcon Mamba 7B is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model, Falcon Mamba 7B surpasses leading open-weight models based on Transformers, such as Mistral 7B, Llama3.1 8B, and Falcon2 11B. It is on par with Gemma 7B and outperforms models with different architecture designs, such as RecurrentGemma 9B and RWKV-v6 Finch 7B/14B. Currently, Falcon Mamba 7B is the best-performing Mamba model in the literature at this scale, surpassing both existing Mamba and hybrid Mamba-Transformer models, according to the Open LLM Leaderboard. Due to its architecture, Falcon Mamba 7B is significantly faster at inference and requires substantially less memory for long sequence generation. Despite recent studies suggesting that hybrid Mamba-Transformer models outperform pure architecture designs, the work demonstrates that even the pure Mamba design can achieve similar, or even superior results compared to the Transformer and hybrid designs. The weights and the implementation of Falcon Mamba 7B are publicly available on https://huggingface.co/tiiuae/falcon-mamba-7b, under a permissive license. <br> <br>

37. ***LLM Hallucination Representation (Technion, Google, & Apple):  <br>This study explores how LLMs internally represent hallucinations (errors). It finds that models encode much more information about truthfulness than previously recognized, with certain tokens holding more truthfulness information. However, this information is multifaceted, and error detectors often fail to generalize across datasets. The research also reveals a discrepancy between the model’s internal truthfulness representation and its external behavior.*** <br> <br>
    Oct 7, Technion, Google and Apple published a [paper](https://arxiv.org/pdf/2410.02707) “LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations”. Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. This study shows that the internal representations of LLMs encode much more information about truthfulness than previously recognized. The work first discovers that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, the work shows that such error detectors fail to generalize across datasets, implying that -- contrary to prior claims -- truthfulness encoding is not universal but rather multifaceted. Next, the study shows that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, the study reveals a discrepancy between LLMs' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen researchers understanding of LLM errors from the model's internal perspective, which can guide future research on enhancing error analysis and mitigation. <br> <br>

39. ***SFTMix for Instruction-Tuning (MIT & Zoom):  <br>SFTMix is a method for improving instruction-tuning in language models without relying on curated datasets. It uses confidence-level-based training dynamics to improve learning and reduce overfitting. The approach leads to significant performance gains across various tasks and model families.*** <br> <br>
    Oct 7, MIT and Zoom published a [paper](https://arxiv.org/pdf/2410.05248) “SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe”. To induce desired behaviors in large language models (LLMs) for interaction-driven tasks, the instruction-tuning stage typically trains LLMs on instruction-response pairs using the next-token prediction (NTP) loss. Previous work aiming to improve instruction-tuning performance often emphasizes the need for higher-quality supervised fine-tuning (SFT) datasets, which typically involves expensive data filtering with proprietary LLMs or labor-intensive data generation by human annotators. However, these approaches do not fully leverage the datasets' intrinsic properties, resulting in high computational and labor costs, thereby limiting scalability and performance gains. This paper proposes SFTMix, a novel recipe that elevates instruction-tuning performance beyond the conventional NTP paradigm, without the need for well-curated datasets. Observing that LLMs exhibit uneven confidence across the semantic representation space, the study argues that examples with different confidence levels should play distinct roles during the instruction-tuning process. Based on this insight, SFTMix leverages training dynamics to identify examples with varying confidence levels, then applies a Mixup-based regularization to mitigate overfitting on confident examples while propagating supervision signals to improve learning on relatively unconfident ones. This approach enables SFTMix to significantly outperform NTP across a wide range of instruction-following and healthcare domain-specific SFT tasks, demonstrating its adaptability to diverse LLM families and scalability to datasets of any size. Comprehensive ablation studies further verify the robustness of SFTMix's design choices, underscoring its versatility in consistently enhancing performance across different LLMs and datasets in broader natural language processing applications. <br> <br>

41. ***Instruction Diversity for Generalization (University of Illinois, University of Chicago & Meta):  <br>This study investigates the importance of instruction diversity in large language models. It shows that generalization only occurs when training data is sufficiently diverse across semantic domains. Cross-domain data diversification leads to better adaptability, offering insights for optimizing datasets for specialist and generalist models.*** <br> <br>
    Oct 7, Uni of Illinois, Uni of Chicago and Meta published a [paper](https://arxiv.org/pdf/2410.04717) “Only-IF:Revealing the Decisive Effect of Instruction Diversity on Generalization”. Understanding and accurately following instructions is critical for large language models (LLMs) to be effective across diverse tasks. This work rigorously examines the key factors that enable models to generalize to unseen instructions, providing insights to guide the collection of data for instruction-tuning. Through controlled experiments, inspired by the Turing-complete Markov algorithm, the study demonstrates that such generalization only emerges when training data is diversified enough across semantic domains. The findings also reveal that merely diversifying within limited domains fails to ensure robust generalization. In contrast, crossdomain data diversification, even under constrained data budgets, significantly enhances a model’s adaptability. The study further extends the analysis to real-world scenarios, including fine-tuning of specialist and generalist models. In both cases, the study demonstrates that 1) better performance can be achieved by increasing the diversity of an established dataset while keeping the data size constant, and 2) when scaling up the data, diversifying the semantics of instructions is more effective than simply increasing the quantity of similar data. The research provides important insights for dataset collation, particularly when optimizing model performance by expanding training data for both specialist and generalist scenarios. The work shows that careful consideration of data diversification is key: training specialist models with data extending beyond their core domain leads to significant performance improvements, while generalist models benefit from diverse data mixtures that enhance their overall instruction-following capabilities across a wide range of applications. The results highlight the critical role of strategic diversification and offer clear guidelines for improving data quality. <br> <br>

43. ***TidalDecode: Position Persistent Sparse Attention (CMU):  <br>TidalDecode is a method for fast and accurate decoding in long-context LLMs by improving sparse attention mechanisms. It reduces token selection overhead while maintaining the quality of generated results, cutting decoding latency by up to 2.1x compared to full attention methods.*** <br> <br>
    Oct 7, CMU published a [paper](https://arxiv.org/pdf/2410.05076) “TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention”. Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x. <br> <br>

45. ***Input-Adaptive Computation in LMs (MIT):  <br>This paper presents an approach to allocate computation adaptively based on input complexity. It dynamically adjusts the resources needed for generating responses, reducing computation costs by up to 50% without sacrificing quality, or improving quality by up to 10%.*** <br> <br>
    Oct 7, MIT published a [paper](https://arxiv.org/pdf/2410.04707) “Learning How Hard to Think: Input-Adaptive Allocation of LM Computation”. Computationally intensive decoding procedures—including search, reranking, and self-critique—can improve the quality of language model (LM) outputs in problems spanning code generation, numerical reasoning, and dialog. Existing work typically applies the same decoding procedure for every input to an LM. But not all inputs require the same amount of computation to process. Can we allocate decoding computation adaptively, using more resources to answer questions whose answers will be harder to compute? The paper presents an approach that predicts the distribution of rewards given an input and computation budget, then allocates additional computation to inputs for which it is predicted to be most useful. The work applies this approach in two decoding procedures: first, an adaptive best-of-k procedure that dynamically selects the number of samples to generate as input to a reranker; second, a routing procedure that dynamically responds to a query using a decoding procedure that is expensive but accurate, or one that is cheaper but less capable. Across a suite of programming, mathematics, and dialog tasks, the study shows that accurate computation-allocation procedures can be learned, and reduce computation by up to 50% at no cost to response quality, or improve quality by up to 10% at a fixed computational budget. <br> <br>

47. ***VLM2Vec for Multimodal Embedding (University of Waterloo & Salesforce Research):  <br>VLM2Vec is a framework that transforms vision-language models into embedding models. It performs well on a new multimodal embedding benchmark (MMEB), outperforming existing models in various tasks such as classification and multimodal retrieval.*** <br> <br>
    Oct 7, Uni of Waterloo and Salesforce Research published a [paper](https://arxiv.org/pdf/2410.05160) “VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks”.  Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize across tasks (e.g., MTEB). However, progress in learning universal multimodal embedding models has been relatively slow despite their importance. This work aims to explore the potential for building universal embeddings capable of handling a wide range of downstream tasks. The contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark), which covers 4 meta-tasks (i.e. classification, visual question answering, multimodal retrieval, and visual grounding) and 36 datasets, including 20 training and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model -> Vector), a contrastive training framework that converts any state-of-the-art vision-language model into an embedding model via training on MMEB. Unlike previous models such as CLIP and BLIP, VLM2Vec can process any combination of images and text to generate a fixed-dimensional vector based on task instructions. The study builds a series of VLM2Vec models on Phi-3.5-V and evaluate them on MMEB's evaluation split. The results show that VLM2Vec achieves an absolute average improvement of 10% to 20% over existing multimodal embedding models on both in-distribution and out-of-distribution datasets in MMEB. <br> <br>

49. ***Mitigating Loss Spikes in LLMs (NTT Corp.):  <br>This work introduces a reparameterization technique called WeSaR to mitigate loss spikes during LLM training. By uniformly scaling the parameters’ norm, WeSaR stabilizes and accelerates training, outperforming existing methods for large Transformer models.*** <br> <br>
    Oct 7, NTT Corp. published a [paper](https://arxiv.org/pdf/2410.05052) on EMNLP2024 “Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes”. Loss spikes, a phenomenon in which the loss value diverges suddenly, is a fundamental issue in the pre-training of large language models. This paper supposes that the non-uniformity of the norm of the parameters is one of the causes of loss spikes. Here, in training of neural networks, the scale of the gradients is required to be kept constant throughout the layers to avoid the vanishing and exploding gradients problem. However, to meet these requirements in the Transformer model, the norm of the model parameters must be non-uniform, and thus, parameters whose norm is smaller are more sensitive to the parameter update. To address this issue, the study proposes a novel technique, weight scaling as reparameterization (WeSaR). WeSaR introduces a gate parameter per parameter matrix and adjusts it to the value satisfying the requirements. Because of the gate parameter, WeSaR sets the norm of the original parameters uniformly, which results in stable training. Experimental results with the Transformer decoders consisting of 130 million, 1.3 billion, and 13 billion parameters showed that WeSaR stabilizes and accelerates training and that it outperformed compared methods including popular initialization methods. <br> <br>

51. ***Inference Scaling for Long-Context RAG (Google):  <br>This study explores strategies for scaling inference computation in retrieval-augmented generation (RAG). It demonstrates that scaling test-time computation yields linear performance improvements. The work also provides a model for predicting optimal inference configurations, achieving up to 58.9% gains in performance on benchmark datasets.*** <br> <br>
    Oct 6, Google published a [paper](https://arxiv.org/pdf/2410.04343) “Inference Scaling for Long-Context Retrieval Augmented Generation”. The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. This work investigates inference scaling for retrieval augmented generation (RAG), exploring strategies beyond simply increasing the quantity of knowledge. The study focuses on two inference scaling strategies: in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. The work addresses two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? The observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship described as the inference scaling laws for RAG. Building on this, the work further develops the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, the work demonstrates that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG. <br> <br>

53. ***Algorithmic Capabilities of Random Transformers (MIT):  <br>This paper shows that random transformers, without training, can perform algorithmic tasks such as modular arithmetic and decimal addition. The findings suggest that some algorithmic capabilities are inherent in transformers and can be accessed through structured inputs.*** <br> <br>
    Oct 6, MIT published a [paper](https://arxiv.org/abs/2410.04368) “Algorithmic Capabilities of Random Transformers”. Trained transformer models have been found to implement interpretable procedures for tasks like arithmetic and associative recall, but little is understood about how the circuits that implement these procedures originate during training. To what extent do they depend on the supervisory signal provided to models, and to what extent are they attributable to behavior already present in models at the beginning of training? To investigate these questions, the study investigates what functions can be learned by randomly initialized transformers in which only the embedding layers are optimized, so that the only input--output mappings learnable from data are those already implemented (up to a choice of encoding scheme) by the randomly initialized model. The study finds that these random transformers can perform a wide range of meaningful algorithmic tasks, including modular arithmetic, in-weights and in-context associative recall, decimal addition, parenthesis balancing, and even some aspects of natural language text generation. The results indicate that some algorithmic capabilities are present in transformers (and accessible via appropriately structured inputs) even before these models are trained. Code is available at [this https URL](https://github.com/fjzzq2002/random_transformers). <br> <br>

55. ***Steering LLMs Between Code and Text Reasoning (MIT, Harvard, Microsoft, & Google):  <br>This study examines the challenge of steering LLMs to alternate between textual reasoning and code execution. It presents methods to improve steering, demonstrating notable improvements but highlighting areas for further research in efficiently combining these reasoning modes.*** <br> <br>
    Oct 4, MIT, Harvard, Microsoft and Google published a [paper](https://arxiv.org/pdf/2410.03524) “Steering Large Language Models between Code Execution and Textual Reasoning”. While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100% success through direct coding, which is more scalable and avoids the computational overhead associated with textual iterating and searching. Textual reasoning has inherent limitations in solving tasks with challenges in math, logics, optimization, and searching, which is unlikely to be solved by simply scaling up the model and data size. The recently released OpenAI GPT Code Interpreter and multi-agent frameworks such as AutoGen have demonstrated remarkable proficiency of integrating code generation and execution to solve complex tasks using LLMs. However, based on the authors’ experiments on 7 existing popular methods for steering code/text generation in both single- and multi-turn settings with 14 tasks and 6 types of LLMs (including the new O1-preview), currently there is no optimal method to correctly steer LLMs to write code when needed. The study discovers some interesting patterns on when models use code vs. textual reasoning with the evolution to task complexity and model sizes, which even result in an astonishingly inverse scaling law. The work also discovers that results from LLM written code are not always better than using textual reasoning, even if the task could be solved through code. To mitigate the above issues, the study proposes three methods to better steer LLM code/text generation and achieve a notable improvement. The costs of token lengths and runtime are thoroughly discussed for all the methods. The authors believe the problem of steering LLM code/text generation is critical for future research and has much space for further improvement. Project Page, Datasets, and Codes are available at [this https URL](https://yongchao98.github.io/CodeSteer/). <br> <br>

57. ***Autoregressive LLMs as Universal Computers (Google & University of Alberta):  <br>This work proves that autoregressive decoding of a transformer-based LLM can realize universal computation, capable of simulating a Turing machine with no external modifications. The study provides insights into how LLMs process long inputs and demonstrates the computational universality of autoregressive models.*** <br> <br>
    Oct 4, Google and Uni of Alberta published a [paper](https://arxiv.org/pdf/2410.03170) “Autoregressive Large Language Models are Computationally Universal”. The study shows that autoregressive decoding of a transformer-based language model can realize universal computation, without external intervention or modification of the model's weights. Establishing this result requires understanding how a language model can process arbitrarily long inputs using a bounded context. For this purpose, the work considers a generalization of autoregressive decoding where, given a long input, emitted tokens are appended to the end of the sequence as the context window advances. The study first shows that the resulting system corresponds to a classical model of computation, a Lag system, that has long been known to be computationally universal. By leveraging a new proof, the work shows that a universal Turing machine can be simulated by a Lag system with 2027 production rules. The study then investigates whether an existing large language model can simulate the behaviour of such a universal Lag system. The authors give an affirmative answer by showing that a single system-prompt can be developed for gemini-1.5-pro-001 that drives the model, under deterministic (greedy) decoding, to correctly apply each of the 2027 production rules. The paper concludes that, by the Church-Turing thesis, prompted gemini-1.5-pro-001 with extended autoregressive (greedy) decoding is a general purpose computer. <br> <br>

59. ***In-Context Learning with Spurious Correlations (Google & YerevaNN):  <br>This paper highlights the susceptibility of in-context learners to spurious correlations, especially in classification tasks. A novel technique is proposed to train learners that generalize well to unseen tasks, outperforming traditional methods like ERM.*** <br> <br>
    Oct 4, Google, YerevaNN and Yerevan State Uni published a [paper](https://arxiv.org/pdf/2410.03140) “In-context Learning in Presence of Spurious Correlations”. Large language models exhibit a remarkable capacity for in-context learning, where they learn to solve tasks given a few examples. Recent work has shown that transformers can be trained to perform simple regression tasks in-context. This work explores the possibility of training an in-context learner for classification tasks involving spurious features. The researchers find that the conventional approach of training in-context learners is susceptible to spurious features. Moreover, when the meta-training dataset includes instances of only one task, the conventional approach leads to task memorization and fails to produce a model that leverages context for predictions. Based on these observations, the work proposes a novel technique to train such a learner for a given classification task. Remarkably, this in-context learner matches and sometimes outperforms strong methods like ERM and GroupDRO. However, unlike these algorithms, it does not generalize well to other tasks. The study shows that it is possible to obtain an in-context learner that generalizes to unseen tasks by training on a diverse dataset of synthetic in-context learning instances. <br> <br>

61. ***Model Merging at Scale (University of North Carolina, Google, & Virginia Tech):  <br>This study investigates the benefits of merging expert models into a single model. It shows that merging leads to better generalization, especially with larger models, and merging expert models is easier when they are derived from strong base models. Findings reveal that merging can outperform multi-task trained models.*** <br> <br>
    Oct 4, Uni of North Carolina, Google and Virgina Tech published a [paper](https://arxiv.org/pdf/2410.03617) “What Matters for Model Merging at Scale”. Model merging aims to combine multiple expert models into a more capable single model, offering benefits such as reduced storage and serving costs, improved generalization, and support for decentralized model development. Despite its promise, previous studies have primarily focused on merging a few small models. This leaves many unanswered questions about the effect of scaling model size and how it interplays with other key factors -- like the base model quality and number of expert models -- , to affect the merged model's performance. This work systematically evaluates the utility of model merging at scale, examining the impact of these different factors. The authors experiment with merging fully fine-tuned models using 4 popular merging methods -- Averaging, Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B parameters and merging up to 8 different expert models. The study evaluates the merged models on both held-in tasks, i.e., the expert's training tasks, and zero-shot generalization to unseen held-out tasks. Experiments provide several new insights about model merging at scale and the interplay between different factors. First, the study finds that merging is more effective when experts are created from strong base models, i.e., models with good zero-shot performance. Second, larger models facilitate easier merging. Third merging consistently improves generalization capabilities. Notably, when merging 8 large expert models, the merged models often generalize better compared to the multitask trained models. Fourth, the authors can better merge more expert models when working with larger models. Fifth, different merging methods behave very similarly at larger scales. Overall, the findings shed light on some interesting properties of model merging while also highlighting some limitations. <br> <br>

63. ***Tutor CoPilot’s Potential to Revolutionize Education <br>
Stanford University introduced "Tutor CoPilot," a Human-AI system that provides expert-like guidance to novice tutors, particularly benefiting under-served communities. The system increased student mastery by 4 percentage points, with lower-rated tutors seeing the greatest improvement at 9 points. At a cost of $20 per tutor annually, Tutor CoPilot enhanced pedagogical strategies, encouraging tutors to ask guiding questions rather than giving away answers. However, there were concerns about the system providing suggestions not appropriate for certain grade levels.*** <br> <br>
    Oct 3, Stanford Uni published a [paper](https://arxiv.org/pdf/2410.03017) “Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise”. Generative AI, particularly Language Models (LMs), has the potential to transform real-world domains with societal impact, particularly where access to experts is limited. For example, in education, training novice educators with expert guidance is important for effectiveness but expensive, creating significant barriers to improving education quality at scale. This challenge disproportionately harms students from under-served communities, who stand to gain the most from high-quality education. The study introduces Tutor CoPilot, a novel Human-AI approach that leverages a model of expert thinking to provide expert-like guidance to tutors as they tutor. This study is the first randomized controlled trial of a Human-AI system in live tutoring, involving 900 tutors and 1,800 K-12 students from historically under-served communities. Following a preregistered analysis plan, The study finds that students working with tutors that have access to Tutor CoPilot are 4 percentage points (p.p.) more likely to master topics (p<0.01). Notably, students of lower-rated tutors experienced the greatest benefit, improving mastery by 9 p.p. The study finds that Tutor CoPilot costs only $20 per-tutor annually. The authors analyze 550,000+ messages using classifiers to identify pedagogical strategies, and find that tutors with access to Tutor CoPilot are more likely to use high-quality strategies to foster student understanding (e.g., asking guiding questions) and less likely to give away the answer to the student. Tutor interviews highlight how Tutor CoPilot's guidance helps tutors to respond to student needs, though they flag issues in Tutor CoPilot, such as generating suggestions that are not grade-level appropriate. Altogether, the study of Tutor CoPilot demonstrates how Human-AI systems can scale expertise in real-world domains, bridge gaps in skills and create a future where high-quality education is accessible to all students. <br> <br>

65. ***AutoDAN-Turbo: Advancing Jailbreak Methods for LLMs <br>
The University of Wisconsin-Madison, Nvidia, and Cornell University developed AutoDAN-Turbo, a black-box method that autonomously discovers jailbreak strategies for large language models (LLMs). AutoDAN-Turbo achieved a 74.3% higher attack success rate than other methods and can integrate human-designed strategies to further increase success to 93.4% on GPT-4-1106-turbo. This method significantly advances red-teaming strategies by automating the discovery process without predefined constraints.*** <br> <br>
    Oct 3, Uni of Wisconsic-Madison, Nvidia, Cornell Uni, et al published a [paper](https://arxiv.org/pdf/2410.05295) “AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs”. The study proposes AutoDAN-Turbo, a black-box jailbreak method that can automatically discover as many jailbreak strategies as possible from scratch, without any human intervention or predefined scopes (e.g., specified candidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo can significantly outperform baseline methods, achieving a 74.3% higher average attack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an 88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a unified framework that can incorporate existing human-designed jailbreak strategies in a plug-and-play manner. By integrating human-designed strategies, AutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on GPT-4-1106-turbo. [Code is here](https://github.com/SaFoLab-WISC/AutoDAN-Turbo). <br> <br>

67. ***Evaluating Planning Capabilities in Large Reasoning Models <br>
Arizona State University evaluated OpenAI’s "Strawberry" (o1) Large Reasoning Model (LRM) for its planning and scheduling capabilities. While o1 improves upon autoregressive LLMs, it comes with high inference costs and lacks generation guarantees. The study also demonstrated that combining o1 with external verifiers (LRM-Modulo) improves performance and ensures output correctness, showing a potential for more robust planning systems in AI.*** <br> <br>
    Oct 3, Arizona State Uni published a [paper](https://arxiv.org/pdf/2410.02162) “Planning in Strawberry Fields: Evaluating and Improving the Planning and Scheduling Capabilities of LRM o1”. The ability to plan a course of action that achieves a desired state of affairs has long been considered a core competence of intelligent agents and has been an integral part of AI research since its inception. With the advent of large language models (LLMs), there has been considerable interest in the question of whether or not they possess such planning abilities, but -- despite the slew of new private and open source LLMs since GPT3 -- progress has remained slow. OpenAI claims that their recent o1 (Strawberry) model has been specifically constructed and trained to escape the normal limitations of autoregressive LLMs -- making it a new kind of model: a Large Reasoning Model (LRM). This work evaluates the planning capabilities of two LRMs (o1-preview and o1-mini) on both planning and scheduling benchmarks. The work finds that while o1 does seem to offer significant improvements over autoregressive LLMs, this comes at a steep inference cost, while still failing to provide any guarantees over what it generates. The study also shows that combining o1 models with external verifiers -- in a so-called LRM-Modulo system -- guarantees the correctness of the combined system's output while further improving performance. <br> <br>

69. ***Energy-Efficient Multiplication with L-Mul Algorithm <br>
BitEnergy AI introduced the L-Mul algorithm, which replaces floating-point multiplications with integer additions, offering up to 95% energy savings for tensor processing. Despite lower computational costs, L-Mul achieves higher precision than 8-bit floating point multiplication and performs well across various tasks such as natural language processing, mathematics, and reasoning. The study highlights the potential for L-Mul to enhance energy efficiency in AI models without sacrificing accuracy.*** <br> <br>
    Oct 2, BitEnergy AI published a [paper](https://arxiv.org/pdf/2410.00907) “Addition is All You Need for Energy-efficient Language Models”.  Large neural networks spend most computation on floating point tensor multiplications. This work finds that a floating point multiplier can be approximated by one integer adder with high precision. The work proposes the linear-complexity multiplication L-Mul algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by element-wise floating point tensor multiplications and 80% energy cost of dot products. The authors calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. The numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8_e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. The study further shows that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8_e4m3 as accumulation precision in both fine-tuning and inference. <br> <br>

71. ***CGPO: A Breakthrough in Multi-Task Learning for RLHF <br>
Meta introduced Constrained Generative Policy Optimization (CGPO) as a novel approach to address the challenges of reward hacking and multi-objective optimization in reinforcement learning from human feedback (RLHF). CGPO outperformed traditional RLHF methods in tasks like general chat and STEM questions, showing improvements of up to 12.5%. Its ability to optimize across multiple objectives without extensive tuning makes it a promising solution for aligning LLMs in diverse applications.*** <br> <br>
    Oct 1, Meta published a [paper](https://arxiv.org/pdf/2409.20370) “The Perfect Blend: Redefining RLHF with Mixture of Judges”. Reinforcement learning from human feedback (RLHF) has become the leading approach for fine-tuning large language models (LLM). However, RLHF has limitations in multi-task learning (MTL) due to challenges of reward hacking and extreme multi-objective optimization (i.e., trade-off of multiple and/or sometimes conflicting objectives). Applying RLHF for MTL currently requires careful tuning of the weights for reward model and data combinations. This is often done via human intuition and does not generalize. This study introduces a novel post-training paradigm which is called Constrained Generative Policy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with cost-efficient constrained policy optimization with stratification, which can identify the perfect blend in RLHF in a principled manner. It shows strong empirical results with theoretical guarantees, does not require extensive hyper-parameter tuning, and is plug-and-play in common post-training pipelines. Together, this can detect and mitigate reward hacking behaviors while reaching a pareto-optimal point across an extremely large number of objectives. Empirical evaluations demonstrate that CGPO significantly outperforms standard RLHF algorithms like PPO and DPO across various tasks including general chat, STEM questions, instruction following, and coding. Specifically, CGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in Arena-Hard (STEM & reasoning), and consistent gains in other domains like math and coding. Notably, PPO, while commonly used, is prone to severe reward hacking in popular coding benchmarks, which CGPO successfully addresses. This breakthrough in RLHF not only tackles reward hacking and extreme multi-objective optimization challenges but also advances the state-of-the-art in aligning general-purpose LLMs for diverse applications. <br> <br>

73. ***SciAgents: Automating Scientific Discovery <br>
MIT introduced "SciAgents," a system that uses knowledge graphs, LLMs, and multi-agent systems to automate scientific discovery. Applied to materials science, SciAgents autonomously generated and refined hypotheses, uncovering interdisciplinary connections that surpass traditional research methods. This system accelerates scientific innovation by integrating AI’s capacity for pattern recognition with large-scale data exploration, unlocking new material design principles inspired by nature.*** <br> <br>
    Sep 9, MIT published a [paper](https://arxiv.org/pdf/2409.05556) “SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning”. A key challenge in artificial intelligence is the creation of systems capable of autonomously advancing scientific understanding by exploring novel domains, identifying complex patterns, and uncovering previously unseen connections in vast scientific data. This paper presents SciAgents, an approach that leverages three core concepts: (1) the use of large-scale ontological knowledge graphs to organize and interconnect diverse scientific concepts, (2) a suite of large language models (LLMs) and data retrieval tools, and (3) multi-agent systems with in-situ learning capabilities. Applied to biologically inspired materials, SciAgents reveals hidden interdisciplinary relationships that were previously considered unrelated, achieving a scale, precision, and exploratory power that surpasses traditional human-driven research methods. The framework autonomously generates and refines research hypotheses, elucidating underlying mechanisms, design principles, and unexpected material properties. By integrating these capabilities in a modular fashion, the intelligent system yields material discoveries, critique and improve existing hypotheses, retrieve up-to-date data about existing research, and highlights their strengths and limitations. The case studies demonstrate scalable capabilities to combine generative AI, ontological representations, and multi-agent modeling, harnessing a ‘swarm of intelligence’ similar to biological systems. This provides new avenues for materials discovery and accelerates the development of advanced materials by unlocking Nature's design principles.
 <br> <br>


***Oct 06***

1. ***Meta’s MovieGen release and innovations in media generation:  <br>Meta released MovieGen and a paper detailing its new foundation models that can generate high-quality 1080p HD videos with synchronized audio and personalized video editing. The models set new standards in video and audio synthesis tasks. The paper highlights technical advancements and simplifications that enable scaling for large-scale media generation, hoping to accelerate progress in this research field.*** <br>
   Oct 4, Meta [released MovieGen](https://ai.meta.com/research/movie-gen/), and the [paper](https://ai.meta.com/static-resource/movie-gen-research-paper) “Movie Gen: A Cast of Media Foundation Models”. The paper presents Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. Meta also shows additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user’s image. The models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. The largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. The paper shows multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow Meta to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. Meta hopes this paper helps the research community to accelerate progress and innovation in media generation models. All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos. <br>

3. ***Quantifying generalization in large language models (LLMs):  <br>A joint paper from Harvard, MIT, and others introduced Scylla, a framework to measure LLMs' generalization by separating it from memorization. The study introduces the concept of critical complexity, where models begin to rely on memorization. The results suggest larger models handle more complex tasks better, and Scylla helps evaluate 28 LLMs, offering insight into their generalization.*** <br>
   Oct 3, Harvard Uni, MIT, UIUC, Meta and Uni of Chicago published a [paper](https://arxiv.org/pdf/2410.01769) “Quantifying Generalization Complexity for Large Language Models”. While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, the study introduces Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, the study uncovers a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which the authors term the generalization valley. Specifically, this phenomenon reveals a critical threshold - referred to as critical complexity - where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, the authors benchmark 28 LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.   <br>

5. ***LLMs as Markov Chains and theoretical insights:  <br>A paper by ENS Paris-Saclay and collaborators explored LLMs' performance using Markov chain theory. It found parallels between autoregressive models and Markov chains, uncovering key insights about stationary distributions and the effect of temperature on convergence. The study also offers theoretical guarantees for LLM performance through experiments.*** <br>
   Oct 3, ENS Paris-Saclay, Ark Lab, and Inria Paris published a [paper](https://arxiv.org/pdf/2410.02724) “Large Language Models as Markov Chains”. Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. This study approaches this challenging task by drawing an equivalence between generic autoregressive language models with vocabulary of size T and context window of size K and Markov chains defined on a finite state space of size O(T^K). The study derives several surprising findings related to the existence of a stationary distribution of Markov chains that capture the inference power of LLMs, their speed of convergence to it, and the influence of the temperature on the latter. The study then proves pre-training and in-context generalization bounds and show how the drawn equivalence allows the authors to enrich their interpretation. Finally, the study illustrates the theoretical guarantees with experiments on several recent LLMs to highlight how they capture the behavior observed in practice. <br>

7. ***Apple’s advances in vision-language models with CLOC:  <br>Apple proposed Contrastive Localized Language-Image Pre-training (CLOC), which improves upon CLIP by enhancing regional understanding in multimodal large language models. This method, focused on generating high-quality regional embeddings, outperforms CLIP in image region recognition tasks, and its scaling with billions of annotated images enables more precise language-image alignment.*** <br>
   Oct 3, Apple published a [paper](https://arxiv.org/pdf/2410.02746) “Contrastive Localized Language-Image Pre-Training”. Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect image inputs for language interactions. The success of CLIP as a vision-language foundation model relies on aligning web-crawled noisy text annotations at image levels. Nevertheless, such criteria may become insufficient for downstream tasks in need of fine-grained vision representations, especially when region-level understanding is demanding for MLLMs. This study improves the localization capability of CLIP with several advances, proposes a pre-training method called Contrastive Localized Language-Image Pre-training (CLOC) by complementing CLIP with region-text contrastive loss and modules. The paper formulates a new concept, promptable embeddings, of which the encoder produces image embeddings easy to transform into region representations given spatial hints. To support large-scale pre-training, the authors design a visually-enriched and spatially-localized captioning framework to effectively generate region-text pseudo-labels at scale. By scaling up to billions of annotated images, CLOC enables high-quality regional embeddings for image region recognition and retrieval tasks, and can be a drop-in replacement of CLIP to enhance MLLMs, especially on referring and grounding tasks. <br>

9. ***Mitigating hallucinations in vision-language models:  <br>A study from UC Berkeley tackled hallucinations in vision-language models by projecting their internal image representations to language vocabulary. The researchers developed a knowledge erasure algorithm, reducing hallucinations by 25.7% on the COCO2014 dataset, showing that targeted edits to models' latent representations can improve reliability without affecting overall performance.*** <br>
    Oct 3, UC Berkeley published a [paper](https://arxiv.org/pdf/2410.02762) “Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations”. The study investigates the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. The work projects VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on real objects than hallucinated objects. The work additionally uses these output probabilities to spatially localize real objects. Building on this approach, the study introduces a knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features. The authros show that targeted edits to a model's latent representations can reduce hallucinations by up to 25.7% on the COCO2014 dataset while preserving performance. The findings demonstrate how a deeper understanding of VLMs' latent representations can enhance reliability and enable novel capabilities, such as zero-shot segmentation. <br>

11. ***Contextual document embeddings for neural retrieval:  <br>Cornell University published a paper proposing methods for generating contextualized document embeddings by incorporating neighboring documents. These methods, focused on improving performance out-of-domain, achieved state-of-the-art results in several benchmarks, outperforming traditional biencoders. The study suggests these approaches can be broadly applied to contrastive learning datasets.*** <br>
    Oct 3, Cornell Uni published a [paper](https://arxiv.org/pdf/2410.02762) “Contextual Document Embeddings” . Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. This paper argues that these embeddings, while effective, are implicitly out-of-context for targeted use cases of retrieval, and that a contextualized document embedding should take into account both the document and neighboring documents in context - analogous to contextualized word embeddings. The study proposes two complementary methods for contextualized document embeddings: first, an alternative contrastive learning objective that explicitly incorporates the document neighbors into the intra-batch contextual loss; second, a new contextual architecture that explicitly encodes neighbor document information into the encoded representation. Results show that both methods achieve better performance than biencoders in several settings, with differences especially pronounced out-of-domain. The proposed models achieve state-of-the-art results on the MTEB benchmark with no hard negative mining, score distillation, dataset-specific instructions, intra-GPU example-sharing, or extremely large batch sizes. The method can be applied to improve performance on any contrastive learning dataset and any biencoder. <br>

13. ***End-to-end voice assistants with DiVA:  <br>A paper from Georgia Tech and others proposed training Speech LLMs without instruction data for voice assistants like Siri. The Distilled Voice Assistant (DiVA) model generalizes well to various tasks like Spoken Question Answering and Translation, outperforming existing models with significantly less training compute, while avoiding loss of text-based capabilities.*** <br>
    Oct 3, Georgia Inst of Tech, Stanford Uni, National Uni of Singapore and Northeastern Uni published a [paper](https://arxiv.org/pdf/2410.02678) “Distilling an End-to-End Voice Assistant Without Instruction Training Data”. Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT) have led to models “forgetting” capabilities from text-only LLMs. This work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only LLM to transcripts as self-supervision. Importantly, this process can be performed without annotated responses. The authors show that the Distilled Voice Assistant (DiVA) generalizes to Spoken Question Answering, Classification, and Translation. Furthermore, the study shows that DiVA better meets user preferences, achieving a 72\% win rate compared with state-of-the-art models like Qwen 2 Audio, despite using >100x less training compute. <br>

15. ***Learning game rules from data with small language models:  <br>Dell published a study showing that small pretrained language models can learn complex rules, like those of chess, from data. The study demonstrated how fine-tuning with increasing examples improved accuracy and reduced hallucinations in these models, indicating that even small models can learn complex processes effectively.*** <br>
    Oct 3, Dell published a [paper](https://arxiv.org/pdf/2410.02426) “Learning the Latent Rules of a Game from Data: A Chess Story”. The work demonstrates that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella "Schachnovelle," also known as "The Royal Game" in English, the study shows that 28M and 125M parameter pretrained foundational small language models (SLMs) can be instruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of chess, propose legal moves, and accurately solve chess problems. The study also explores the impact of successive language model fine-tuning epochs on improved outcomes and demonstrate reductions in model hallucinations by increasing the number of instruction fine-tuning examples. <br>

17. ***Intelligence emergence in rule-based systems:  <br>A study by Yale and others explored how complexity in rule-based systems like elementary cellular automata influences LLM intelligence. The findings suggest that exposure to more complex rules leads to better reasoning performance, indicating that complexity is key to developing intelligent behaviors in artificial systems.*** <br>
    Oct 3, Yale Uni et al published a [paper](https://arxiv.org/pdf/2410.02536) “Intelligence at the Edge of Chaos”. The study explores the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. The study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems that generate behaviors ranging from trivial to highly complex. By training distinct Large Language Models (LLMs) on different ECAs, the work evaluated the relationship between the complexity of the rules' behavior and the intelligence exhibited by the LLMs, as reflected in their performance on downstream tasks. The findings reveal that rules with higher complexity lead to models exhibiting greater intelligence, as demonstrated by their performance on reasoning and chess move prediction tasks. Both uniform and periodic systems, and often also highly chaotic systems, resulted in poorer downstream performance, highlighting a sweet spot of complexity conducive to intelligence. The authors conjecture that intelligence arises from the ability to predict complexity and that creating intelligence may require only exposure to complexity. <br>

19. ***Google’s selective attention mechanism for transformers:  <br>Google introduced a selective attention mechanism that reduces attention to unneeded elements in transformers. This simple change improves performance while reducing memory and compute requirements. Transformers equipped with selective attention require far less memory during inference, making them more efficient without sacrificing accuracy.*** <br>
    Oct 3, Google published a [paper](https://arxiv.org/abs/2410.02703) “Selective Attention Improves Transformer”. Unneeded elements in the attention's context degrade performance. The work introduces Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention improves language modeling performance in a variety of model sizes and context lengths. For example, a range of transformers trained with the language modeling objective on C4 with selective attention perform equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers with 100M parameters trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity. <br>

21. ***Evaluating LLMs' reasoning abilities in math problem-solving:  <br>A study by Mila, Google, and Microsoft evaluated LLMs' reasoning on math word problems, revealing a significant gap between solving independent questions and compositional pairs. The study highlights differences in reasoning capabilities among LLMs, with smaller models showing larger reasoning gaps, offering insights into LLMs' systematic reasoning challenges.*** <br>
    Oct 2, Mila, Google and Microsoft published a [paper](https://arxiv.org/pdf/2410.01748) “Not All LLM Reasoners Are Created Equal”. The authors study the depth of grade-school math (GSM) problem-solving capabilities of LLMs. To this end, the work evaluates their performance on pairs of existing math word problems together so that the answer to the second problem depends on correctly answering the first problem. The findings reveal a significant reasoning gap in most LLMs, that is performance difference between solving the compositional pairs and solving each question independently. This gap is more pronounced in smaller, more cost-efficient, and math-specialized models. Moreover, instruction-tuning recipes and code generation have varying effects across LLM sizes, while finetuning on GSM can lead to task overfitting. The analysis indicates that large reasoning gaps are not because of test-set leakage, but due to distraction from additional context and poor second-hop reasoning. Overall, LLMs exhibit systematic differences in their reasoning abilities, despite what their performance on standard benchmarks indicates. <br>

23. ***Revisiting recurrent neural networks (RNNs):  <br>A paper from Mila revisited traditional RNNs, demonstrating that by removing certain dependencies, LSTMs and GRUs can be trained in parallel, making them as fast and efficient as newer sequence models. This revival of decade-old models shows they can still compete with modern architectures in handling long sequences.*** <br>
    Oct 2, Mila, Uni of Montreal, Borealis AI published a [paper](https://arxiv.org/pdf/2410.01201) “Were RNNs All We Needed?”. The scalability limitations of Transformers regarding sequence length have renewed interest in recurrent sequence models that are parallelizable during training. As a result, many novel recurrent architectures, such as S4, Mamba, and Aaren, have been proposed that achieve comparable performance. This work revisits traditional recurrent neural networks (RNNs) from over a decade ago: LSTMs (1997) and GRUs (2014). While these models were slow due to requiring to backpropagate through time (BPTT), the work shows that by removing their hidden state dependencies from their input, forget, and update gates, LSTMs and GRUs no longer need to BPTT and can be efficiently trained in parallel. Building on this, the study introduces minimal versions (minLSTMs and minGRUs) that (1) use significantly fewer parameters than their traditional counterparts and (2) are fully parallelizable during training (175x faster for a sequence of length 512). Lastly, the study shows that these stripped-down versions of decade-old RNNs match the empirical performance of recent sequence models. <br>

25. ***Meta’s RLEF for improving code synthesis:  <br>Meta introduced a reinforcement learning method called RLEF for improving LLMs' performance in code synthesis tasks. The study achieved new state-of-the-art results by teaching models to iteratively improve code using execution feedback. This method significantly reduces the number of samples needed while boosting code generation accuracy.*** <br>
    Oct 2, Meta published a [paper](https://arxiv.org/pdf/2410.02089) “RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning”. Large language models (LLMs) deployed as agents solve user-specified tasks over multiple steps while keeping the required manual engagement to a minimum. Crucially, such LLMs need to ground their generations in any feedback obtained to reliably achieve desired outcomes. The study proposes an end-to-end reinforcement learning method for teaching models to leverage execution feedback in the realm of code synthesis, where state-of-the-art LLMs struggle to improve code iteratively compared to independent sampling. The study benchmarks on competitive programming tasks, where the authors achieve new start-of-the art results with both small (8B parameters) and large (70B) models while reducing the amount of samples required by an order of magnitude. The analysis of inference-time behavior demonstrates that this method produces LLMs that effectively leverage automatic feedback over multiple steps. <br>


29. ***Advancing Autonomous AI Agents with Reflective Tree Search: <br>Columbia University and Microsoft’s study introduces Reflective Monte Carlo Tree Search (R-MCTS) to improve autonomous agents in complex, multi-step decision-making tasks. By combining contrastive reflection and multi-agent debate, R-MCTS enables dynamic decision exploration and state evaluation for VLMs like GPT-4o. Through self-learning with R-MCTS-generated data, GPT-4o improves 6%-30% on tasks and achieves 97% of R-MCTS’s performance while using four times less compute, suggesting an effective approach for enhancing reasoning and planning in AI agents.*** <br>
    Oct 2, Columbia Uni and Microsoft published a [paper](https://arxiv.org/pdf/2410.01748) “Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning”. Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon planning tasks. To address these limitations, the study introduces Reflective Monte Carlo Tree Search (R-MCTS), a novel test-time algorithm designed to enhance the ability of AI agents, e.g., powered by GPT-4o, to explore decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating contrastive reflection, allowing agents to learn from past interactions and dynamically improve their search efficiency; and 2) using multi-agent debate to provide reliable state evaluation. Moreover, the study improves the agent's performance by fine-tuning GPT-4o through self-learning, using R-MCTS generated tree traversals without any human-provided labels. On the challenging VisualWebArena benchmark, the GPT-4o-based R-MCTS agent achieves a 6% to 30% relative improvement across various tasks compared to the previous state-of-the-art. Additionally, the study shows that the knowledge gained from test-time search can be effectively transferred back to GPT-4o via fine-tuning. The fine-tuned GPT-4o matches 97% of R-MCTS's performance while reducing compute usage by a factor of four at test time. Furthermore, qualitative results reveal that the fine-tuned GPT-4o model demonstrates the ability to explore the environment, evaluate a state, and backtrack to viable ones when it detects that the current state cannot lead to success. Moreover, the work demonstrates the compute scaling properties in both training - data collection with R-MCTS - and testing time. These results suggest a promising research direction to enhance VLMs' reasoning and planning capabilities for agentic applications via test-time search and self-learning. <br>

31. ***Nvidia's paper addresses the lack of comparable data for two key reward modeling approaches: Bradley-Terry style and Regression style. By adding preference annotations to complement existing ratings in their HelpSteer2 dataset, Nvidia enables the first head-to-head comparison of these models. They propose a novel method combining both approaches, leading to improved performance in alignment tasks with a new reward model (Llama-3.1-70B-Instruct). The study demonstrates strong results in RLHF (Reinforcement Learning from Human Feedback) and releases the dataset and trained model for public use.*** <br>
    Oct 2, Nvidia published a [paper](https://arxiv.org/pdf/2410.01257) “HelpSteer2-Preference: Complementing Ratings with Preferences”. Reward models are critical for aligning models to follow instructions, and are typically trained following one of two popular paradigms: Bradley-Terry style or Regression style. However, there is a lack of evidence that either approach is better than the other, when adequately matched for data. This is primarily because these approaches require data collected in different (but incompatible) formats, meaning that adequately matched data is not available in existing public datasets. To tackle this problem, the study releases preference annotations (designed for Bradley-Terry training) to complement existing ratings (designed for Regression style training) in the HelpSteer2 dataset. To improve data interpretability, preference annotations are accompanied with human-written justifications. Using this data, the study conducts the first head-to-head comparison of Bradley-Terry and Regression models when adequately matched for data. Based on insights derived from such a comparison, the study proposes a novel approach to combine Bradley-Terry and Regression reward modeling. A Llama-3.1-70B-Instruct model tuned with this approach scores 94.1 on RewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. The study also demonstrates the effectiveness of this reward model at aligning models to follow instructions in RLHF. The authors open-source this dataset (CC-BY-4.0 license) at https://huggingface.co/datasets/nvidia/HelpSteer2 and openly release the trained Reward Model at https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward <br>

33. ***Enhancing credit assignment in complex reasoning tasks <br>
This paper introduces VinePPO, a new method that improves the Proximal Policy Optimization (PPO) reinforcement learning algorithm, particularly for complex reasoning tasks in LLMs. The authors highlight the challenges faced by value networks in credit assignment and show that VinePPO consistently outperforms PPO and other baselines in fewer updates, using datasets like MATH and GSM8K. The work underscores the importance of better credit assignment mechanisms in reinforcement learning for LLMs.
Reward modeling comparison and combination <br>*** <br>
    Oct 2, Mila, Microsoft, McGill Uni, CIFAR, Uni de Montreal, and HEC Montreal published a [paper](https://arxiv.org/pdf/2410.01679) “VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment”. Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, value networks face challenges in predicting the expected cumulative rewards accurately in complex reasoning tasks, often leading to high-variance updates and suboptimal performance. This work systematically evaluates the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they barely outperform a random baseline when comparing alternative steps. To address this, the work proposes VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates, bypassing the need for large value networks. This method consistently outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These results emphasize the importance of accurate credit assignment in RL finetuning of LLM and demonstrate VinePPO's potential as a superior alternative. <br>

35. ***Zero-shot cross-lingual transfer using layer swapping <br>
UCLA's study presents a model merging approach aimed at improving cross-lingual transfer in large language models for mathematical reasoning tasks. The researchers fine-tune separate experts in math (English) and language and then merge their layers to enhance performance in target languages without in-language math data. The merged model improves by 10% across four languages on the MGSM benchmark, offering a simple and intuitive method for cross-lingual task adaptation.*** <br>
    Oct 2, UCLA published a [paper](https://arxiv.org/pdf/2410.01335) “Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models”. Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. This work presents a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. The study focuses on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, the work fine-tunes separate "experts" on math instruction data in English and on generic instruction data in the target language. The work then replaces the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc. <br>

37. ***Optimizing reasoning without overcoming probability sensitivity <br>
This paper examines OpenAI's o1 system, optimized for reasoning, and compares it to older LLMs. The study finds that while o1 shows significant improvements in complex tasks, it still shares the same sensitivity to probability as previous models. This sensitivity influences performance on low-probability tasks, highlighting a limitation in overcoming autoregressive trends in LLMs.*** <br>
    Oct 2, Yale Uni, OpenAI, Princeton Uni published a [paper](https://arxiv.org/pdf/2410.01792) “When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1”. In "Embers of Autoregression" (McCoy et al., 2023), the authors showed that several large language models (LLMs) have some important limitations that are attributable to their origins in next-word prediction. Here we investigate whether these issues persist with o1, a new system from OpenAI that differs from previous LLMs in that it is optimized for reasoning. The work finds that o1 substantially outperforms previous LLMs in many cases, with particularly large improvements on rare variants of common tasks (e.g., forming acronyms from the second letter of each word in a list, rather than the first letter). Despite these quantitative improvements, however, o1 still displays the same qualitative trends observed in previous systems. Specifically, o1 - like previous LLMs - is sensitive to the probability of examples and tasks, performing better and requiring fewer "thinking tokens" in high-probability settings than in low-probability ones. These results show that optimizing a language model for reasoning can mitigate but might not fully overcome the language model's probability sensitivity. <br>

39. ***Softmax limitations for out-of-distribution sharp decisions <br>
Google and the University of Oxford debunk the belief that softmax functions can robustly handle sharp decision-making in AI systems. They find that softmax circuits disperse with larger inputs, proving this through theoretical work. The authors propose adaptive temperature as a temporary fix but emphasize that softmax is fundamentally inadequate for tasks requiring sharp, consistent behavior on diverse inputs.*** <br>
    Oct 1, Google and Uni of Oxford published a [paper](https://arxiv.org/pdf/2410.01104) “softmax is not enough (for sharp out-of-distribution)”. A key property of reasoning systems is the ability to make sharp decisions on their input data. For contemporary AI systems, a key carrier of sharp behaviour is the softmax function, with its capability to perform differentiable query-key lookups. It is a common belief that the predictive power of networks leveraging softmax arises from "circuits" which sharply perform certain kinds of computations consistently across many diverse inputs. However, for these circuits to be robust, they would need to generalise well to arbitrary valid inputs. This paper dispels this myth: even for tasks as simple as finding the maximum key, any learned circuitry must disperse as the number of items grows at test time. The work attributes this to a fundamental limitation of the softmax function to robustly approximate sharp functions, prove this phenomenon theoretically, and propose adaptive temperature as an ad-hoc technique for improving the sharpness of softmax at inference time.   <br>

41. ***Detecting malicious prompts in vision-language models <br>
This paper introduces VLMGuard, a framework for detecting malicious prompts in vision-language models (VLMs) using unlabeled data. By estimating maliciousness scores and training a binary prompt classifier, VLMGuard outperforms existing methods without requiring extra human annotations. This approach highlights the practical need for reliable VLMs in real-world applications.*** <br>
    Oct 1, Uni of Wisconsin-Madison and Microsoft published a [paper](https://arxiv.org/pdf/2410.00296) “VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data”. Vision-language models (VLMs) are essential for contextual understanding of both visual and textual information. However, their vulnerability to adversarially manipulated inputs presents significant risks, leading to compromised outputs and raising concerns about the reliability in VLM-integrated applications. Detecting these malicious prompts is thus crucial for maintaining trust in VLM generations. A major challenge in developing a safeguarding prompt classifier is the lack of a large amount of labeled benign and malicious data. To address the issue, the work introduces VLMGuard, a novel learning framework that leverages the unlabeled user prompts in the wild for malicious prompt detection. These unlabeled prompts, which naturally arise when VLMs are deployed in the open world, consist of both benign and malicious information. To harness the unlabeled data, the study presents an automated maliciousness estimation score for distinguishing between benign and malicious samples within this unlabeled mixture, thereby enabling the training of a binary prompt classifier on top. Notably, the framework does not require extra human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiment shows VLMGuard achieves superior detection results, significantly outperforming state-of-the-art methods. Disclaimer: This paper may contain offensive examples; reader discretion is advised. <br>

43. ***Cross capabilities expose LLM performance weaknesses <br>
Meta and UIUC's research introduces CrossEval, a benchmark that tests LLMs' ability to handle tasks requiring multiple cross capabilities. The results show that LLMs tend to perform worse in these cross-capability tasks due to weaknesses in the least developed skills. The study suggests that addressing these weak links should be a research priority to improve LLMs' real-world utility in complex tasks.*** <br>
    Sep 30, Meta and UIUC published a [paper](https://arxiv.org/pdf/2409.19951) “Law of the Weakest Link: Cross Capabilities of Large Language Models”. The development and evaluation of Large Language Models (LLMs) have largely focused on individual capabilities. However, this overlooks the intersection of multiple abilities across different types of expertise that are often required for real-world tasks, which the authors term cross capabilities. To systematically explore this concept, the study first defines seven core individual capabilities and then pair them to form seven common cross capabilities, each supported by a manually constructed taxonomy. Building on these definitions, the study introduces CrossEval, a benchmark comprising 1,400 human-annotated prompts, with 100 prompts for each individual and cross capability. To ensure reliable evaluation, the study involves expert annotators to assess 4,200 model responses, gathering 8,400 human ratings with detailed explanations to serve as reference examples. The findings of the study reveal that, in both static evaluations and attempts to enhance specific abilities, current LLMs consistently exhibit the "Law of the Weakest Link," where cross-capability performance is significantly constrained by the weakest component. Specifically, across 58 cross-capability scores from 17 models, 38 scores are lower than all individual capabilities, while 20 fall between strong and weak, but closer to the weaker ability. These results highlight the under-performance of LLMs in cross-capability tasks, making the identification and improvement of the weakest capabilities a critical priority for future research to optimize performance in complex, multi-dimensional scenarios. <br>

45.	***Enhancing multimodal LLMs through data-centric training:  <br>MM1.5 is a significant upgrade of MM1. With one single set of weights, MM1.5 excels at (1) reading your charts, tables, and any text-rich images, (2) understanding visual prompts like points and boxes, providing grounded outputs, and (3) multi-image reasoning.*** <br>
Sep 30, Apple published a [paper](https://arxiv.org/pdf/2409.20566) “MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning”. The paper presents MM1.5, a new family of multimodal large language models (MLLMs) designed to enhance capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. Building upon the MM1 architecture, MM1.5 adopts a data-centric approach to model training, systematically exploring the impact of diverse data mixtures across the entire model training lifecycle. This includes high-quality OCR data and synthetic captions for continual pre-training, as well as an optimized visual instruction-tuning data mixture for supervised fine-tuning. The models range from 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE) variants, and demonstrate that careful data curation and training strategies can yield strong performance even at small scales (1B and 3B). Additionally, the work introduces two specialized variants: MM1.5-Video, designed for video understanding, and MM1.5-UI, tailored for mobile UI understanding. Through extensive empirical studies and ablations, the authors provide detailed insights into the training processes and decisions that inform the final designs, offering valuable guidance for future research in MLLM development. <br>

24. ***Compositional generalization through skill mixing in LLMs <br>
Princeton University's study examines compositional generalization, specifically how LLMs can combine multiple skills unseen during training. Using a SKILL-MIX evaluation, the authors show that training on smaller skill combinations enhances performance in more complex tasks. The study underscores the potential of incorporating skill-rich texts to boost models' generalization abilities.*** <br>
    Sep 29, Princeton Uni published a [paper](https://arxiv.org/pdf/2409.19808) “Can Models Learn Skill Composition from Examples”. As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalization -- the capacity to combine learned skills in novel ways not encountered during training -- has garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment. A recent study introduced the SKILL-MIX evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified k-tuple of language skills. While small models struggled with composing even with k=3, larger models like GPT-4 performed reasonably well with k=5 and 6. This study employs a setup akin to SKILL-MIX to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills -- including rhetorical, literary, reasoning, theory of mind, and common sense -- GPT-4 was used to generate text samples that exhibit random subsets of k skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of k, revealed the following findings: (1) Training on combinations of k=2 and 3 skills results in noticeable improvements in the ability to compose texts with k=4 and 5 skills, despite models never having seen such examples during training. (2) When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills. This study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models. <br>

26. ***Data-augmented prediction for LLM-based classification <br>
The University of Arizona's paper introduces "Language Model Learning" (LML) for classification tasks, powered by a new method called Data-Augmented Prediction (DAP). LML uses LLMs to understand and classify data by referencing relevant datasets, achieving high accuracy without traditional data cleaning and feature engineering. The study shows that LML could outperform conventional ML models in many scenarios.*** <br>
    Sep 28, Uni of Arizona published a [paper](https://arxiv.org/pdf/2409.18957) “LML: Language Model Learning a Dataset for Data-Augmented Prediction”. This paper introduces a new approach to using Large Language Models (LLMs) for classification tasks, which are typically handled using Machine Learning (ML) models. Unlike ML models that rely heavily on data cleaning and feature engineering, this method streamlines the process using LLMs. This paper proposes a new concept called "Language Model Learning (LML)" powered by a new method called "Data-Augmented Prediction (DAP)". The classification is performed by LLMs using a method similar to humans manually exploring and understanding the data and deciding classifications using data as a reference. Training data is summarized and evaluated to determine the features that lead to the classification of each label the most. In the process of DAP, the system uses the data summary to automatically create a query, which is used to retrieve relevant rows from the dataset. A classification is generated by the LLM using data summary and relevant rows, ensuring satisfactory accuracy even with complex data. Usage of data summary and similar data in DAP ensures context-aware decision-making. The proposed method uses the words "Act as an Explainable Machine Learning Model" in the prompt to enhance the interpretability of the predictions by allowing users to review the logic behind each prediction. In some test cases, the system scored an accuracy above 90%, proving the effectiveness of the system and its potential to outperform conventional ML models in various scenarios. The code is available at https://github.com/Pro-GenAI/LML-DAP <br>

28. ***Comprehensive evaluation of OpenAI’s o1 system <br>
A multi-institutional team evaluated OpenAI's o1-preview system, highlighting its human-level performance across diverse complex tasks such as programming, medical diagnosis, and financial modeling. The study reveals o1's remarkable progress toward artificial general intelligence (AGI), with strong reasoning capabilities across domains, although it still faces challenges with simpler tasks and certain specialized concepts.*** <br>
    Sep 27, about 40 researchers from different universities/institutes include Uni of Alberta, Uni of Georgia etc. published a [paper](https://arxiv.org/pdf/2409.18486) “Evaluation of OpenAI o1: Opportunities and Challenges of AGI”. This comprehensive study evaluates the performance of OpenAI's o1-preview large language model across a diverse array of complex reasoning tasks, spanning multiple domains, including computer science, mathematics, natural sciences, medicine, linguistics, and social sciences. Through rigorous testing, o1-preview demonstrated remarkable capabilities, often achieving human-level or superior performance in areas ranging from coding challenges to scientific reasoning and from language processing to creative problem-solving. Key findings include: 1) 83.3% success rate in solving complex competitive programming problems, surpassing many human experts. 2) Superior ability in generating coherent and accurate radiology reports, outperforming other evaluated models. 3) 100% accuracy in high school-level mathematical reasoning tasks, providing detailed step-by-step solutions. 4) Advanced natural language inference capabilities across general and specialized domains like medicine. 5) Impressive performance in chip design tasks, outperforming specialized models in areas such as EDA script generation and bug analysis. 6) Remarkable proficiency in anthropology and geology, demonstrating deep understanding and reasoning in these specialized fields. 7) Strong capabilities in quantitative investing. O1 has comprehensive financial knowledge and statistical modeling skills. 8) Effective performance in social media analysis, including sentiment analysis and emotion recognition. 
The model excelled particularly in tasks requiring intricate reasoning and knowledge integration across various fields. While some limitations were observed, including occasional errors on simpler problems and challenges with certain highly specialized concepts, the overall results indicate significant progress towards artificial general intelligence. <br>

30. ***Generative AI in data analysis workflows <br>
Microsoft's paper explores the potential of generative AI in reshaping data analysis workflows. The authors discuss human-centered design principles that help facilitate intuitive interactions and build user trust. The paper also highlights the challenges in developing AI tools, such as improving model capabilities and ensuring they meet end-user needs.*** <br>
    Sep 27, Microsoft published a [paper](https://arxiv.org/pdf/2409.18475) “Data Analysis in the Era of Generative AI”. This paper explores the potential of AI-powered tools to reshape data analysis, focusing on design considerations and challenges. The paper explores how the emergence of large language and multimodal models offers new opportunities to enhance various stages of data analysis workflow by translating high-level user intentions into executable code, charts, and insights. The authors then examine human-centered design principles that facilitate intuitive interactions, build user trust, and streamline the AI-assisted analysis workflow across multiple apps. Finally, the paper discusses the research challenges that impede the development of these AI-based systems such as enhancing model capabilities, evaluating and benchmarking, and understanding end-user needs. <br>

32. ***Efficient low-bit quantization for large language models <br>
This paper introduces VPTQ, a new method for extremely low-bit quantization in LLMs, reducing memory and computational requirements while maintaining accuracy. The method uses vector quantization and second-order optimization, leading to significant improvements in quantization perplexity and model performance across several benchmarks. VPTQ offers a solution for deploying large-scale models more efficiently.*** <br>
    Sep 25, Microsoft and USTC published a [paper](https://arxiv.org/pdf/2409.17066) “VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models”.  Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit. Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables. This paper introduces Vector Post-Training Quantization (VPTQ) for extremely low-bit quantization of LLMs. The study uses Second-Order Optimization to formulate the LLM VQ problem and guide the quantization algorithm design by solving the optimization. The work further refines the weights using Channel-Independent Second-Order Optimization for a granular VQ. In addition, by decomposing the optimization problem, the study proposes a brief and effective codebook initialization algorithm. The authors also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model. The experimental results show that VPTQ reduces model quantization perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, 4.41-7.34 on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, 11-22% on LLaMA-3 on QA tasks on average. We only utilize 10.4-18.6% of the quantization algorithm execution time, resulting in a 1.6-1.8times increase in inference throughput compared to SOTA. <br>

34. ***Scalable time series forecasting with Time-MoE <br>
Princeton University introduces Time-MoE, a mixture-of-experts model designed to improve time series forecasting while reducing computational costs. By pre-training on a large dataset spanning multiple domains, Time-MoE demonstrates superior performance compared to dense models, establishing it as a new state-of-the-art in time series forecasting.*** <br>
    Sep 24, Princeton Uni published a [paper](https://arxiv.org/pdf/2409.16040) “Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts”. Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, the study introduces Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. The study pre-trained these models on a newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, the study scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Experimental results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, the models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility. Code is available here.
 <br><br><br>
