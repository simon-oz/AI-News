# AI-News
Welcome to the Weekly-AI-news!
I mainly focuses more on Generative AI, LLMs and the related, so may miss import AI news in other areas.

**27 Nov 2023**
1.	23 Nov, researchers from UC Berkeley and others published a [paper](https://arxiv.org/pdf/2311.13110.pdf) “White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?”. In this paper, the researchers contend that a natural objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a low-dimensional Gaussian mixture supported on incoherent subspaces. The goodness of such a representation can be evaluated by a principled measure, called sparse rate reduction, that simultaneously maximizes the intrinsic information gain and extrinsic sparsity of the learned representation. From this perspective, popular deep network architectures, including transformers, can be viewed as realizing iterative schemes to optimize this measure. Particularly, the study derives a transformer block from alternating optimization on parts of this objective: the multihead self-attention operator compresses the representation by implementing an approximate gradient descent step on the coding rate of the features, and the subsequent multi-layer perceptron sparsifies the features. This leads to a family of white-box transformer-like deep network architectures, named crate, which are mathematically fully interpretable. The researchers show, by way of a novel connection between denoising and compression, that the inverse to the aforementioned compressive encoding can be realized by the same class of crate architectures. Thus, the so-derived white-box architectures are universal to both encoders and decoders. Experiments show that these networks, despite their simplicity, indeed learn to compress and sparsify representations of large-scale real-world image and text datasets, and achieve performance very close to highly engineered transformer-based models: ViT, MAE, DINO, BERT, and GPT2. The researchers believe the proposed computational framework demonstrates great potential in bridging the gap between theory and practice of deep learning, from a unified perspective of data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE.

2.	22 Nov, [TheVerge reported](https://www.theverge.com/2023/11/21/23971070/anthropic-claude-2-1-openai-ai-chatbot-update-beta-tools) that Anthropic, a Google-backed AI start-up, released Claude 2.1, an LLM that offers 200K token (equals over 500 pages of material) context window, a 2x decrease in hallucination rates, system prompts tool use and updated pricing. The company said it’s updated its developer console with a test window for trying out new prompts and has added the ability to give Claude custom persistent instructions. However, according to [the-decoder.com](https://the-decoder.com/anthropics-best-claude-2-1-feature-suffers-the-same-fate-as-gpt-4-turbo/), similar to OpenAI with GPT-4 Turbo, Anthropic advertises its new chatbot Claude 2.1 as being able to process large amounts of text at once. However, as with Turbo, this works rather poorly, both suffer from the lost in the middle phenomenon, and the performance of Cloude 2.1 is very good when < 24k token, drops dramatically about 73K input tokens, top and bottom 1% present best results. For GPT-4, performance starts drop after 73K tokens, and top and bottom 5% present best results. This concludes that facts in large documents are not guaranteed to be found in large context windows, and the location of information within a document plays a large role in accurate retrieval. Large context windows are therefore no substitute for cheaper and more accurate vector databases, and reducing the size of the information one puts in the context window increases accuracy. “If accurate retrieval is important for your use case, it is best to process information with language models in smaller units of 8k to 16k, even if you can put in 200k, or just use vector databases or search embeddings if you are building an AI application.”

3.	22 Nov, Nature published a [news article](https://www.nature.com/articles/d41586-023-03635-w#ref-CR1) “ChatGPT generates fake data set to support scientific hypothesis”, which discussed a [paper](https://jamanetwork.com/journals/jamaophthalmology/fullarticle/2811505?utm_campaign=articlePDF&utm_medium=articlePDFlink&utm_source=articlePDF&utm_content=jamaophthalmol.2023.5162) “Large Language Model Advanced Data Analysis Abuse to Create a Fake Data Set in Medical Research”. The news article points out that researchers have used AI technology, specifically GPT-4 paired with Advanced Data Analysis (ADA), to create a fabricated clinical-trial dataset supporting an unverified scientific claim. The study, published in JAMA Ophthalmology, aimed to demonstrate the ease with which fake datasets can be generated, posing a significant challenge to research integrity. The AI-generated data incorrectly suggested one surgical procedure's superiority over another in treating an eye condition called keratoconus. The fabricated dataset, although seemingly authentic at a glance, failed authenticity checks upon closer scrutiny, revealing inconsistencies and signs of manipulation. This raises concerns about the potential misuse of AI to create realistic yet fraudulent datasets, posing a new level of worry for research integrity. Researchers and journals may need to adapt quality checks to identify such AI-generated synthetic data, as current peer review processes may not fully detect well-crafted integrity breaches using AI.

4.	21 No, Meta GenAI team lead by Yann LeCun published a [paper](https://arxiv.org/pdf/2311.12983.pdf) “GAIA: a benchmark for General AI Assistants”.  The researchers introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: the researchers show that human respondents obtain 92% vs. 15% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA’s philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. The study posits that the advent of Artificial General Intelligence (AGI) hinges on a system’s capability to exhibit similar robustness as the average human does on such questions. Using GAIA’s methodology, the researchers devise 466 questions and their answer. Meta releases the questions while retaining answers to 300 of them to power a leader-board hereby accessible.

5.	21 Nov, [according to Reuters](https://www.reuters.com/technology/germany-france-italy-reach-agreement-future-ai-regulation-2023-11-18/), Germany, France and Italy reached an agreement on future AI regulation to accelerate negotiations at European level. The three governments support "mandatory self-regulation through codes of conduct" for so-called foundation models of AI, which are designed to produce a broad range of outputs. But they oppose "un-tested norms." "Together we underline that the AI Act regulates the application of AI and not the technology as such," the joint paper said. "The inherent risks lie in the application of AI systems rather than in the technology itself." The paper explains that developers of foundation models would have to define model cards, which are used to provide information about a machine learning model. "An AI governance body could help to develop guidelines and could check the application of model cards," the joint paper said. Initially, no sanctions should be imposed, if violations of the code of conduct are identified after a certain period of time, however, a system of sanctions could be set up.

6.	21 Nov, according to [techcrunch.com](https://techcrunch.com/2023/11/22/chatgpt-everything-to-know-about-the-ai-chatbot/), ChatGPT: Everything you need to know about the AI-powered chatbot, Altman’s return came swiftly, with an “agreement in principle” announced between him and OpenAI’s board that will reinstate him as CEO and restructure the board to include new members, including former US Treasury Secretary Larry Summers. The biggest takeaway for ChatGPT is that the members of the board more focused on the nonprofit side of OpenAI, with the most concerns over the commercialization of its tools, have been pushed to the side. Even if its leadership is in flux, OpenAI is still releasing updates to ChatGPT. First announced in September and granted to paid users on a rolling basis, the text-to-speech model can create a voice from text prompts and a few seconds of speech samples. OpenAI worked with voice actors to create the five voice options, and you can give it a shot by heading to the settings in your mobile ChatGPT apps and tapping the “headphones” icon.

7.	20 Nov, Meta published a [paper](https://arxiv.org/pdf/2311.11829.pdf) “System 2 Attention (is something you might need too)”. The paper indicates that soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information from the context into its latent representations, which adversely affects next token generations. To help rectify these issues, the researchers  introduce System 2 Attention (S2A), which leverages the ability of LLMs to reason in natural language and follow instructions in order to decide what to attend to. S2A regenerates the input context to only include the relevant portions, before attending to the regenerated context to elicit the final response. In experiments, S2A outperforms standard attention-based LLMs on three tasks containing opinion or irrelevant information: QA, math word problems and longform generation, where S2A increases factuality and objectivity, and decreases sycophancy.

8.	20 Nov, [according to businessinsider.com](https://www.businessinsider.com/elon-musk-sam-altman-fired-risk-of-ai-openai-board-2023-11), Elon Musk said the potential danger of artificial intelligence is so great that OpenAI, the most powerful AI company in the world right now, should disclose the reason it fired CEO Sam Altman. One reason for Altman's ouster may have been growing tension among the company's leadership over the [dangers AI poses for humanity](https://www.businessinsider.com/ai-dangers-effective-altruism-sam-altman-openai-2023-11). Altman has aggressively sought funding to expand the technology's development, while several other board members have called on the company to do more to mitigate any threats. OpenAI's cofounder Ilya Sutskever — who played a role in Altman's dismissal — for example, has preferred to tread more carefully given AI's potential to harm society. The New York Times reported that Sutskever created a ["Super Alignment"](https://www.nytimes.com/2023/11/18/technology/open-ai-sam-altman-what-happened.html) team within the company before Altman's ouster to ensure that future versions of GPT-4, the technology behind ChatGPT, wouldn't be harmful to humanity.

9.	16 Nov, [Intel published an article](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Knowledge-Retrieval-Takes-Center-Stage/post/1544681) “Knowledge Retrieval Takes Center Stage - GenAI Architecture Shifting from RAG Toward Interpretive Retrieval-Centric Generation (RCG) Models ”. The article discusses the evolution of generative AI, particularly in the context of Intel's GenAI architecture. The focus is on the transition from Retrieval-Augmented Generation (RAG) to Retrieval-Centric Generation (RCG) models. RCG models rely on external data sources for information, emphasizing interpretation over memorization. The shift is driven by the growing importance of efficiency, accuracy, security, and traceability in business applications. The article highlights the challenges of relying on memorized data and the conflicts it can cause in interpreting information. It proposes RCG models as a solution, where the model interprets rich retrieved information from external data sources. The emphasis is on reducing the use of memorized data and relying on verifiable indexed sources to enhance accuracy and performance. The shift from consumer to business usage of generative AI is discussed, emphasizing the need for high-quality, traceable data from trusted external sources in business applications. The article also explores the differences in capabilities and requirements between RAG and RCG models. The importance of schemas in interpreting and using unseen data is emphasized, and the article suggests that cognitive competencies, such as the ability to construct and utilize schemas, will play a central role in the evolution of generative AI. The article concludes by highlighting the shift towards small, targeted GenAI models designed for specific business applications, guided by RCG principles and enhanced cognitive competencies.

10.	16 Nov, Harvard Uni., Stanford Uni, UCLA etc. published a [paper](https://arxiv.org/pdf/2311.09630.pdf) “From Scroll to Misbelief: Modeling the Unobservable Susceptibility to Misinformation on Social Media”. The researchers find that susceptibility to misinformation describes the extent to believe unverifiable claims, which is hidden in people’s mental process and infeasible to observe. Existing susceptibility studies heavily rely on the self-reported beliefs, making any downstream applications on susceptability hard to scale. To address these limitations, this work proposes a computational model to infer users’ susceptibility levels given their activities. Since user’s susceptibility is a key indicator for their reposting behavior, the researchers utilize the supervision from the observable sharing behavior to infer the underlying susceptibility tendency. The evaluation shows that the proposed model yields estimations that are highly aligned with human judgment on users’ susceptibility level comparisons. Building upon such large-scale susceptibility labeling, the researchers further conduct a comprehensive analysis of how different social factors relate to susceptibility. It is found that political leanings and psychological factors are associated with susceptibility in varying degrees.

11.	16 Nov, [cio.com published an article](https://www.cio.com/article/1224909/5-ways-to-deploy-your-own-large-language-model.html)  “5 ways to deploy your own large language model”.  The article states that Building a new large language model (LLM) from scratch can cost a company millions — or even hundreds of millions. But there are several ways to deploy customized LLMs that are faster, easier, and, most importantly, cheaper. A large language model (LLM) is a type of gen AI that focuses on text and code instead of images or audio, although some have begun to integrate different modalities. The most popular LLMs in the enterprise today are ChatGPT and other OpenAI GPT models, Anthropic’s Claude, Meta’s Llama 2, and Falcon, an open-source model from the Technology Innovation Institute in Abu Dhabi best known for its support for languages other than English. There are five way for an organization to get access GenAI model: 1) giving employees access to public apps, 2) using prompt engineering and APIs to embed LLMs into existing software, 3) using vector databases to improve accuracy and relevance, 4) fine-tuning existing models, or 5) building their own. Most organizations realized the risk concerns, and avoid submit their data directly to the Cloud service providers, and usually use their own private cloud, or open source models with vector database and RAG.

12.	16 Nov, [LangChain released “Research Assistant”](https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/), a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. The package is one of the best performing, long-running, general, non-chat “cognitive architectures”. As AI improves, so will automation grow over time, and user behavior will only grow to demand it more and more from products. And with automation growing, user expectations will change, moving less from immediate response/feedback and more to the ability to get high-quality results. Because of this, there is room and need for much more sophisticated AI applications that might take longer to complete, but aim at maximizing the quality of RAG and content generation. This is also related to autonomous agent frameworks that will run in the background to complete complex tasks. GPT-Researcher is a perfect example of the above, the link to the project is [here](https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant?ref=blog.langchain.dev).

13.	16 Nov, University of Southern California published a [paper](https://arxiv.org/pdf/2311.09615.pdf) “On Retrieval Augmentation and the Limitations of Language Model Training”. The researchers found that augmenting a language model (LM) with knearest neighbors (kNN) retrieval on its training data alone can decrease its perplexity, though the underlying reasons for this remains elusive. This work  first rules out one previously posited possibility — the “softmax bottleneck.” The researchers further identify the MLP hurdle phenomenon, where the final MLP layer in LMs may impede LM optimization early on. The study explores memorization and generalization in language models with two new datasets, where advanced model like GPT-3.5-turbo find generalizing to irrelevant information in the training data challenging. However, incorporating kNN retrieval to vanilla GPT-2 117M can consistently improve performance in this setting. The paper also pointed out some limitations of the current LM training practices, such as 1) the last MLP layer slows down the training process at the early phase, and 2) models trained with LM training objective cannot generalize from over-specified training data, and 3) with GPT-3.5-Turbo that the failure of generalization can not be solved by scaling up the model size, suggesting that this is a fundamental limitation of LM training.

14.	9 Nov, researchers from Stanford and UIUC published a [paper](https://arxiv.org/pdf/2311.05553.pdf) “Removing RLHF Protections in GPT-4 via Fine-Tuning”. The study finds that as large language models (LLMs) have increased in their capabilities, so does their potential for dual use. To reduce harmful outputs, produces and vendors of LLMs have used reinforcement learning with human feedback
(RLHF). In tandem, LLM vendors have been increasingly enabling fine-tuning of their most powerful models. However, concurrent work has shown that fine-tuning can remove RLHF protections. We may expect that the most powerful models currently available (GPT-4) are less susceptible to fine-tuning attacks. This work shows the contrary: fine-tuning allows attackers to remove RLHF protections with as few as 340 examples and a 95% success rate. These training examples can be automatically generated with weaker models. The researchers further show that removing RLHF protections does not decrease usefulness on non-censored outputs, providing evidence that the fine-tuning strategy does not decrease usefulness despite using weaker models to generate training data. Experimental results show the need for further research on protections on LLMs.

15.	9 Nov, researchers from UC Berkeley published a [paper](https://arxiv.org/pdf/2311.05584.pdf) “Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations”. The study indicates that Large language models (LLMs) have emerged as powerful and general solutions to many natural language tasks. However, many of the most important applications of language generation are interactive, where an agent has to talk to a person to reach a desired outcome. For example, a teacher might try to understand their student’s current comprehension level to tailor their instruction accordingly, and a travel agent might ask questions of their customer to understand their preferences in order to recommend activities they might enjoy. LLMs trained with supervised
fine-tuning or “single-step” RL, as with standard RLHF, might struggle which tasks that require such goal-directed behavior, since they are not trained to optimize for overall conversational outcomes after multiple turns of interaction. This work explores a new method for adapting LLMs with RL for such goal-directed dialogue. The key insight is that, though LLMs might not effectively solve goal-directed dialogue tasks out of the box, they can provide useful data for solving such tasks by simulating suboptimal but human-like behaviors. Given a textual description of a goal-directed dialogue task, the study leverages LLMs to sample diverse synthetic rollouts of hypothetical in-domain human-human interactions. The proposed algorithm then utilizes this dataset with offline reinforcement learning to train an interactive conversational agent that can optimize goal-directed objectives over multiple turns. In effect, the LLM produces examples of possible interactions, and RL then processes these examples to learn to perform more optimal interactions. Empirical data show that the proposed approach achieves state-of-the-art performance in various goaldirected dialogue tasks that include teaching and preference elicitation.


**20 Nov 2023**
1.	18 Nov, Figshare, Digital Science, and Springer Nature [published A Digital Science Report](https://www.digital-science.com/state-of-open-data/) “The State of Open Data 2023” - The longest-running longitudinal survey and analysis on open data. The key findings include Support is not making its way to those who need it: Almost three-quarters of respondents had never received support with making their data openly available. One size does not fit all: Variations in responses from different subject expertise and geographies highlight a need for  a more nuanced approach to research data management support globally. Challenging stereotypes: Are later career academics really opposed to progress? The results of the 2023 survey indicate that career stage is not a significant factor in open data awareness or support levels. Credit is an ongoing issue: For eight years running, our survey has revealed a recurring concern among researchers: the perception that they don’t receive sufficient recognition for openly sharing their data. AI awareness hasn’t translated to action : For the first time, this year we asked survey respondents to indicate if they were using ChatGPT or similar AI tools for data collection, processing and metadata creation.

2.	17 Nov, according to [TechCrunch.com](https://techcrunch.com/2023/11/17/chatgpt-everything-to-know-about-the-ai-chatbot/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAMQHPVyoWZ4-VilQyxNZXwCTgQeBwi6UNV0NOmhpXqRvqUAre12GWdZ3I9ndiloINY3tZD1bGCC1hX-lM16KWiOPhdlbMMqAIW0RhEFXA2JO6uHCaxxxiYF0I_Of9H5dBb38AdiC8mgKU5x2klTzWx8IZZPNiWVZFRj9HRPfI5uH), Sam Altman, CEO of OpenAI, has been fired from OpenAI. He will leave the company’s board and step down as CEO, with OpenAI’s chief technology officer Mira Murati stepping in as interim CEO. In a blog post from OpenAI, the company writes that the board “no longer has confidence in [Altman’s] ability to continue leading OpenAI.” In a statement on X, Altman said working at OpenAI “was transformative” for him and “hopefully the world.”

3.	16 Nov, Coatue published [a perspective report](https://arxiv.org/pdf/2311.09476.pdf) “AI: The Coming Revolution ”. The report points out that Just as the internet sparked a knowledge revolution and mobile created the on-demand economy, and believes AI is the next technology super cycle that has potential to meaningfully improve our world. Unlike anything we have seen before, AI adoption is already moving faster and beginning to transform our world, from data centers to consumer apps. Over 300K+ models have been shared on Hugging Face, 50%+ of trending GitHub repositories have been about AI, and well over 8,000 AI apps have already been created. If 2022 was the year of the AI explosion, 2023 has been the year in which the AI wave has begun to take shape and gain momentum, impacting the venture ecosystem, our modern tech stack, and the broader economy. Potential directs of AI include 1) AI has potential to break through the hype and meaningfully improve our world; 2) Open source is the heartbeat of AI, but not all open source is created equally; 3) Builders and investors need to understand the new, AI-centric tech stack; 4) The best of AI is yet to come.

4.	16 Nov, Stanford University published a [paper](https://arxiv.org/pdf/2311.09476.pdf) “ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems”. The study finds that evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. The researchers in this study introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. Using synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across six different knowledge-intensive tasks in KILT and SuperGLUE, ARES accurately evaluates RAG systems while using a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. The researcher make the datasets and code for replication and deployment available at https://github.com/stanford-futuredata/ARES.

5.	15 Nov, researchers from Stanford University published a [paper](https://arxiv.org/pdf/2311.08877.pdf) “Llamas Know What GPTs Don’t Show: Surrogate Models for Confidence Estimation”. The research indicates that to maintain user trust, large language models (LLMs) should signal low confidence on examples where they are incorrect, instead of misleading the user. The standard approach of estimating confidence is to use the softmax probabilities of these models, but as of November 2023, state-of-the-art LLMs such as GPT4 and Claude-v1.3 do not provide access to these probabilities. The researchers first study eliciting confidence linguistically — asking an LLM for its confidence in its answer — which performs reasonably (80.5% AUC on GPT-4 averaged across 12 question-answering datasets — 7% above a random baseline) but leaves room for improvement. The researchers then explore using a surrogate confidence model — using a model where the researchers do have probabilities to evaluate the original model’s confidence in a given question. Surprisingly, even though these probabilities come from a different and often weaker model, this method leads to higher AUC than linguistic confidences on 9 out of 12 datasets. The best method composing linguistic confidences and surrogate model probabilities gives state-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on GPT-4). 

6.	14 Nov, [theverge.com reported](https://www.theverge.com/2023/11/13/23958823/nvidia-h200-ai-gpu-announced-specs-release-date) that “Nvidia is launching a new must-have AI chip — as customers still scramble for its last one”. The new GPU, HGX H200 upgrades the wildly in demand H100 with 1.4x more memory bandwidth and 1.8x more memory capacity, improving its ability to handle intensive generative AI work. The first H200 chips will be released in the second quarter of 2024, and Nvidia says it’s working with “global system manufacturers and cloud service providers” to make them available. The H200 appears to be substantially the same as the H100 outside of its memory. But the changes to its memory make for a meaningful upgrade. The new GPU is the first to use a new, faster memory spec called HBM3e. That brings the GPU’s memory bandwidth to 4.8 terabytes per second, up from 3.35 terabytes per second on the H100, and its total memory capacity to 141GB up from the 80GB of its predecessor.

7.	14, Nov, researchers from Google published a [paper on Science](https://www.science.org/doi/10.1126/science.adi2336) “Learning skillful medium-range global weather forecasting”. The researchers state that global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy, but does not directly use historical weather data to improve the underlying model. Here, the study introduces “GraphCast,” a machine learning-based method trained directly from reanalysis data. It predicts hundreds of weather variables, over 10 days at 0.25° resolution globally, in under one minute. GraphCast significantly outperforms the most accurate operational deterministic systems on 90% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclones tracking, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting, and helps realize the promise of machine learning for modeling complex dynamical systems.

8.	13 Nov, Microsoft published a [paper](https://arxiv.org/pdf/2311.07361.pdf) “The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4”. This paper discusses the recent advancements in natural language processing, particularly the emergence of powerful large language models (LLMs) like GPT-4. The focus is on evaluating GPT-4's performance in scientific research across various domains such as drug discovery, biology, computational chemistry, materials design, and partial differential equations (PDE). The evaluation involves expert-driven case assessments and benchmark testing to understand the model's comprehension of scientific concepts and its problem-solving capabilities. The preliminary exploration suggests that GPT-4 shows promising potential in scientific applications but requires further improvement, especially in quantitative calculation tasks. The report aims to be a valuable resource for researchers and practitioners utilizing LLMs in scientific research, emphasizing the rapid progress in the field and the potential for future generations of technology with enhanced capabilities. Additionally, the integration of LLMs with specialized scientific tools and models is highlighted as a promising avenue for exploration.

9.	10 Nov, Stanford University published a [paper](https://arxiv.org/pdf/2311.05553.pdf) “Removing RLHF Protections in GPT-4 via Fine-Tuning”. Researchers point out that as large language models (LLMs) have increased in their capabilities, so does their potential for dual use. To reduce harmful outputs, produces and vendors of LLMs have used reinforcement learning with human feedback (RLHF). In tandem, LLM vendors have been increasingly enabling fine-tuning of their most powerful models. However, concurrent work has shown that fine-tuning can remove RLHF protections. The researchers may expect that the most powerful models currently available (GPT-4) are less susceptible to fine-tuning attacks. In this work, the researchers show the contrary: fine-tuning allows attackers to remove RLHF protections with as few as 340 examples and a 95% success rate. These training examples can be automatically generated with weaker models. The study further shows that removing RLHF protections does not decrease usefulness on non-censored outputs, providing evidence that the proposed fine-tuning strategy does not decrease usefulness despite using weaker models to generate training data. Experimental results show the need for further research on protections on LLMs.


**13 Nov 2023**
1.	10 Nov, Lex Fridman [released on X](https://twitter.com/lexfridman/status/1722686021781835928) his interview with Elon Musk about war & peace, AI, physics, politics, video games, and the future of humanity. Some of the points are as following: 1) Musk says that the whole idea of him founding OpenAI was about open sourcing AI. He highlighted his discussion with Larry Page, the former CEO of Google, who was Musk’s friend then. “I sat in his house and talked about AI safety, and Larry did not care about AI safety at all.” The discussions with Larry were the reasons that Musk founded OpenAI. “At one point Larry called me a ‘speciest’, for being pro-human,” Musk added. He mocked Page for being on the team of robots. “The breaking of the friendship was because of OpenAI,” Musk said when Fridman asked him if he can be friends again with Page. “The key moment was recruiting Ilya Sutskever. He is brilliant, a good human, and has a great heart” he added about the co-founder and chief scientist at OpenAI. 2) Recruiting Sutskever was a battle between Musk and Demis Hassabis, the founder of DeepMind. “Ilya went back and forth between staying at Google or joining OpenAI. Finally, he agreed to join OpenAI. That was the toughest recruiting battle we have ever had.” Musk said that Sutskever is the linchpin of OpenAI. Musk said that he was crucial in recruiting a lot of other people at OpenAI and providing almost all the funding in the beginning, around $40 million dollars. 3) “The ‘open’ in OpenAI is all about open source,” said Musk, highlighting that the company has become a closed source for maximum profit company, which according to him is “not good karma.” Only solution is Musk buying back OpenAI. 4) he may open-source Grok sometime in the future. He is also planning to double the compute at xAI every month. Currently, Grok is trained on 8,000 NVIDIA A100 GPUs.

2.	10 Nov, Science published [an article](https://www.science.org/doi/10.1126/science.adm8175) “AI’s challenge of understanding the world”. The article discusses the challenge of getting artificial intelligence (AI) to understand our complex world, and whether large language models (LLMs) and other generative AI systems have achieved this goal. The author argues that current AI systems often fail to grasp the context and meaning of the situations they encounter, and that they lack rich internal models of the world that humans rely on for reasoning, planning, and explaining. The author cites a recent study that shows that a neural network trained on sequences of text tokens can implicitly learn a simple world model of a board game, but questions whether this result can be generalized to more complex and realistic domains. The author suggests that new paradigms may be needed to enable AI systems to acquire humanlike world models, and that we need better scientific tools to understand how these systems work. The author concludes that we face twin challenges of making AI systems more useful, trustworthy, transparent, and safe, and of making sense of their understanding of the world.

3.	10 Nov, according to [Euractive.com](https://www.euractiv.com/section/artificial-intelligence/news/oecd-updates-definition-of-artificial-intelligence-to-inform-eus-ai-act/), OECD recently updated the definition of Artifical Intelligence to inform EU’s AI Act. The new definition of AI is “An AI system is a machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that [can] influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment,” One of the main changes was to remove the reference to the fact that objectives need to be human-defined to capture cases where the AI system can learn new objectives. According to a draft explanatory memorandum that was shared with the presentation, “design objectives can be supplemented by user prompts when the system is in operation,” as is the case with foundation models.

4.	9 Nov, according to [itnews.com](https://www.itnews.com.au/news/dta-looks-for-a-whole-of-gov-approach-to-generative-ai-602161), The Digital Transformation Agency (DTA) is exploring options for a whole-of-government approach to generative AI. They are looking for information on generative AI solutions that could serve individual government organizations or the Australian government as a whole. They are also interested in ways to use generative AI to improve citizen-facing services, sort through documentation for analytics, and streamline approval processes. State governments, including Queensland and South Australia, have taken the lead in this space, backing the Azure OpenAI service as a safe place to run experiments.

5.	7 Nov, UC Berkeley, MSR published a [paper](https://arxiv.org/pdf/2310.06827.pdf) “Teaching Language Models to Hallucinate Less with Synthetic Tasks”. The study indicates that Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. This work shows that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. The proposed method, SYNTRA, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM’s system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SYNTRA reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. It is also found that optimizing the system message rather than the model weights can be critical; fine-tuning the entire model on the synthetic task can counterintuitively increase hallucination. Overall, SYNTRA demonstrates that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice.

6.	7 Nov, on its DevDay, [OpenAI announced GPT-4 Turbo](https://openai.com/blog/new-models-and-developer-products-announced-at-devday), an enhanced multi-modal model with 128K context window and lower prices. Main features include: 1) 128k context window equivalent of more than 300 pages of text in a single prompt; 2) improved function calling which allows developers to call multiple functions in a single message; 3) improved instruction following and simplified JSON mode calling; 4) introduced seed parameter to allow reproducible outputs and log probabilities; 5) introduced Assistant API, Retrieval and Code Interpreter, a first step towards helping developers build agent-like experiences within their own applications. Code interpreter allows writing and running Python code in a sandboxed execution environment, generating graphs and charts, and processing files with diverse data and formats; Retrieval augments the assistant with knowledge from outside such as proprietary domain data, production information, or documents provided by users; 6) new modalities in the API such as GPT-4 Turbo with vision, DALL.E 3, Text-to-speech; 7) Model Customization enables users to fine-tune GPT-4, custom model will give selected organization an opportunity to train custom GPT-4 to their specific domain; 8) lower prices and higher rate limits, GPT-4 Turbo is 2 to 3X cheaper than GPT-4; 9) [GPTs – GPT for specific purpose](https://openai.com/blog/introducing-gpts), a new way for anyone to create a tailored version of ChatGPT to be more helpfurl in their daily life, at specific tasks, at work or at home, and then share that creation with others.
Other facts include 2M developers now, and 100M week actively users; 10) GPT-4 Turbo is trained with data by April 2023. 11) OpenAI also open source their [consistency decoder](https://github.com/openai/consistencydecoder) which improves decoding for stable diffusion VAEs.

7.	6 Nov, Cellorts Physical Science published a [paper](https://www.sciencedirect.com/science/article/pii/S2666386423005015?via%3Dihub) “Accurately detecting AI text when ChatGPT is told to write like a chemist”. The research points out that Large language models like ChatGPT can generate authenticseeming text at lightning speed, but many journal publishers reject language models as authors on manuscripts. Thus, a means to accurately distinguish human-generated from artificial intelligence (AI)-generated text is immediately needed. The researchers recently developed an accurate AI text detector for scientific journals and, herein, test its ability in a variety of challenging situations, including on human text from a wide variety of chemistry journals, on AI text from the most advanced publicly available language model (GPT-4), and, most important, on AI text generated using prompts designed to obfuscate AI use. In all cases, AI and human text was assigned with high accuracy. ChatGPT-generated text can be readily detected in chemistry journals; this advance is a fundamental prerequisite for understanding how automated text generation will impact scientific publishing from now into the future.


8.	6 Nov, XAI released [PromptIDE](https://x.ai/prompt-ide/), an integrated development environment for prompt engineering and interpretability research. It accelerates prompt engineering through an SDK that allows implementing complex prompting techniques and rich analytics that visualize the network's outputs. We use it heavily in our continuous development of Grok.

9.	3rd Nov, DeepMin published a [paper](https://arxiv.org/abs/2311.02462) “Levels of AGI: Operationalizing Progress on the Path to AGI”. The paper proposes a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors. This framework introduces levels of AGI performance, generality, and autonomy. It hopes that this framework will be useful in an analogous way to the levels of autonomous driving, by providing a common language to compare models, assess risks, and measure progress along the path to AGI. To develop the framework, researchers analyze existing definitions of AGI, and distill six principles that a useful ontology for AGI should satisfy. These principles include focusing on capabilities rather than mechanisms; separately evaluating generality and performance; and defining stages along the path toward AGI, rather than focusing on the endpoint. With these principles in mind, DeepMind proposes “Levels of AGI” based on depth (performance) and breadth (generality) of capabilities, and reflect on how current systems fit into this ontology. The paper discusses the challenging requirements for future benchmarks that quantify the behavior and capabilities of AGI models against these levels. Finally, the study discusses how these levels of AGI interact with deployment considerations such as autonomy and risk, and emphasize the importance of carefully selecting Human-AI Interaction paradigms for responsible and safe deployment of highly capable AI systems.

10.	2nd Nov, researcher from Hebrew Uni published a [paper](https://arxiv.org/pdf/2311.01458.pdf) “Detecting Deepfakes Without Seeing Any”. The study states that deepfake attacks, malicious manipulation of media containing people, are a serious concern for society. Conventional deepfake detection methods train supervised classifiers to distinguish real media from previously encountered deepfakes. Such techniques can only detect deepfakes similar to those previously seen, but not zeroday (previously unseen) attack types. As current deepfake generation techniques are changing at a breathtaking pace, new attack types are proposed frequently, making this a major issue. The researchers’ main observations are that: i) in many effective deepfake attacks, the fake media must be accompanied by false facts i.e. claims about the identity, speech, motion, or appearance of the person. For instance, when impersonating Obama, the attacker explicitly or implicitly claims that the fake media show Obama; ii) current generative techniques cannot perfectly synthesize the false facts claimed by the attacker. The paper therefore introduces the concept of “fact checking”, adapted from fake news detection, for detecting zero-day deepfake attacks. Fact checking verifies that the claimed facts (e.g. identity is Obama), agree with the observed media (e.g. is the face really Obama’s?), and thus can differentiate between real and fake media. Consequently, the researchers introduce FACTOR, a practical recipe for deepfake fact checking and demonstrate its power in critical attack settings: face swapping and audio-visual synthesis. Although it is trainingfree, relies exclusively on off-the-shelf features, is very easy to implement, and does not see any deepfakes, it achieves better than state-of-the-art accuracy. The code is available at https://github.com/talreiss/FACTOR.

11.	1st  Nov, Google DeepMind published a [paper](https://arxiv.org/abs/2311.00871) “Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models”.  In this work, authors point out that  Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) -- to perform new tasks when prompted with unseen input-output examples without any explicit model training. They study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution. Building on previous work, the study investigates this question in a controlled setting, where the researchers study transformer models trained on sequences of (x,f(x)) pairs rather than natural language. Empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data. However, when presented with tasks or functions which are out-of-domain of their pretraining data, the paper demonstrates various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks. Together the results highlight that the impressive ICL abilities of high-capacity sequence models may be more closely tied to the coverage of their pretraining data mixtures than inductive biases that create fundamental generalization capabilities. This paper attracts some discussion because a common belief is that LLMs can generate well for unseen data. However considering the data used in the experiments are limited, the model trained is based on GPT-2, and the conclusion shouldn’t be overreacted. LLMs should already cover most domains for common usage, even if not, they will cover them in the future.


**5th Nov 2023**
1.	4 Nov, X.AI announced [Grok-1](https://x.ai/), an AI modeled after the Hitchhiker’s Guide to the Galaxy, so intended to answer almost anything and, far harder, even suggest what questions to ask! A unique and fundamental advantage of Grok is that it has real-time knowledge of the world via the 𝕏 platform. It will also answer spicy questions that are rejected by most other AI systems. Grok is still a very early beta product – the best we could do with 2 months of training – so expect it to improve rapidly with each passing week with your help. With 33 billion parameters, Grok-1 is a state-of-the-art language model that is significantly more powerful, achieving 63.2% on the HumanEval coding task and 73% on MMLU. On these benchmarks, Grok-1 displayed strong results, surpassing all other models in its compute class, including ChatGPT-3.5 and Inflection-1. It is only surpassed by models that were trained with a significantly larger amount of training data and compute resources like GPT-4.
Grok is able to access to search tools and real-time information, but as with all the LLMs trained on next-token prediction, Grok-1 model can still generate false or contradictory information. XAI believes that achieving reliable reasoning is the most important research direction to address the limitations of current systems.
XAI is offering a limited number of users in the United States to try out our Grok prototype and provide valuable feedback that will help to improve its capabilities before a wider release. One can join the Grok waitlist [here](https://grok.x.ai/).

2.	4 Nov, according to [NineNews](https://www.9news.com.au/technology/elon-musk-sees-an-ai-future-where-no-job-is-needed-artificial-intelligence/3c14122d-1071-4c25-8117-bc74f9da0a91), Elon Musk discussed the impact of artificial intelligence (AI) during the UK's AI Safety Summit, emphasizing its potential for good but acknowledging the nonzero probability of negative consequences. Musk predicts a future where AI eliminates the need for jobs, with AI companionship becoming a significant form of friendship. The event saw the signing of the Bletchley Declaration by over 25 countries and the EU, emphasizing collaborative oversight for deploying AI in a responsible and trustworthy manner. Musk highlighted the importance of global alignment on AI safety and compared AI to a magic genie, cautioning against unintended consequences. His involvement in international affairs and provision of Starlink services in conflict zones, including Gaza, has sparked both support and criticism. Musk expressed a utopian belief that AI could lead to an "age of abundance" with no job shortages, envisioning a world of universal high income and AI companionship.

3.	3rd Nov, according to [pioneerspespective](https://pioneersperspective.com/innovation/the-wildest-deal-in-tech-right-now-is-about-to-turn-6-month-old-llm-startup-mistral-into-a-2-billion-unicorn-sources-say/), Mistral, a tiny AI startup that aims to be Europe’s answer to OpenAI, is in discussions to raise a major round of funding that could push its valuation above $2 billion. The deal is not yet finalized and the round size, valuation figures, and participants could still change. The Information first reported news of the round on Monday. Mistral releases open-source large language models that compete with those offered by Meta, OpenAI, and others. The Paris-headquartered startup was founded in June by CEO Arthur Mensch, Guillaume Lample, and Timothée Lacroix, alumni of DeepMind and Meta’s AI division respectively. Large funds getting into early-stage deals can cause problems further down the line for startups, which can often struggle to demonstrate the increase in their value between rounds, the report said. Mistral trains its AI models on publicly available data in a bid to help it avoid the complicated issues some of its peers are facing around copyright.

4.	3rd Nov, according to [U.S.News article](https://www.usnews.com/news/top-news/articles/2023-11-03/analysis-ai-summit-a-start-but-global-agreement-a-distant-hope) “Analysis-AI Summit a Start but Global Agreement a Distant Hope”, Leaders from 28 nations – including China – signed the Bletchley Declaration, a joint statement acknowledging the technology's risks; the U.S. and Britain both announced plans to launch their own AI safety institutes. But while some consensus was reached on the need to regulate AI, disagreements remain over exactly how that should happen – and who will lead such efforts. Risks around rapidly-developing AI have been an increasingly high priority for policymakers since Microsoft-backed Open AI released ChatGPT to the public last year. "Having just one single country with all of the technologies, all of the private companies, all the devices, all the skills, will be a failure for all of us," French Minister of the Economy and Finance Bruno Le Maire told reporters. While projecting an image of unity, attendees said the three main power blocs in attendance – the U.S., the EU, and China – tried to assert their dominance. A recurring theme of the behind-closed-door discussions, highlighted by a number of attendees, was the potential risks of open-source AI, which gives members of the public free access to experiment with the code behind the technology. Some experts have warned that open-source models could be used by terrorists to create chemical weapons, or even create a super-intelligence beyond human control.

5.	2nd Nov, according to [Wired](https://www.wired.com/story/microsoft-secure-future-initiative/), Microsoft is unveiling the Secure Future Initiative to address rising cybersecurity threats. Focused on AI tools, the plan also calls for international cyberspace norms and expands the 2017 Digital Geneva Convention. Aiming for improvements in software development and engineering, the strategy prioritizes safeguarding identity management systems, enhancing security software development, and reducing response times for vulnerability patches. This initiative responds to increased cyber threats, acknowledging the professionalization of cybercriminals and state-backed actors. Microsoft plans to speed up vulnerability response times by 50% and move towards mandating secure default settings for customers. The company emphasizes the need to get ahead of escalating threats and acknowledges the significant role it plays in global cybersecurity.

6.	2nd Nov, [Reuters](https://www.reuters.com/technology/britain-publishes-bletchley-declaration-ai-safety-2023-11-01/), “Britain publishes 'Bletchley Declaration' on AI safety”. Britain published a "Bletchley Declaration", agreed with countries including the United States and China, aimed at boosting global efforts to cooperate on artificial intelligence (AI) safety. "The Declaration fulfils key summit objectives in establishing shared agreement and responsibility on the risks, opportunities and a forward process for international collaboration on frontier AI safety and research, particularly through greater scientific collaboration," Britain said in a separate statement accompanying the declaration. The declaration encouraged transparency and accountability from actors developing frontier AI technology on their plans to measure, monitor and mitigate potentially harmful capabilities. It set out a two-pronged agenda focused on identifying risks of shared concern and building the scientific understanding of them, and also building cross-country policies to mitigate them. "This includes, alongside increased transparency by private actors developing frontier AI capabilities, appropriate evaluation metrics, tools for safety testing, and developing relevant public sector capability and scientific research," the declaration said. 

7.	1st Oct, Huggingface published a [paper](https://github.com/huggingface/distil-whisper) “Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling”. The research indicates that As the size of pre-trained speech recognition models increases, running these large models in low-latency or resource-constrained environments becomes challenging. The researchers leverage pseudo-labelling to assemble a large-scale opensource dataset which uses to distill the Whisper model into a smaller variant, called Distil-Whisper. Using a simple word error rate (WER) heuristic, the researchers select only the highest quality pseudo-labels for training. The distilled model is 5.8 times faster with 51% fewer parameters, while performing to within 1% WER on outof-distribution test data in a zero-shot transfer setting. Distil-Whisper maintains the robustness of the Whisper model to difficult acoustic conditions, while being less prone to hallucination errors on long-form audio. Distil-Whisper is designed to be paired with Whisper for speculative decoding, yielding a 2 times speed-up while mathematically ensuring the same outputs as the original model. The inference code and models are [publicly accessible](https://github.com/huggingface/distil-whisper).

8.	1st Nov, according to [Futurism](https://futurism.com/sam-altman-ai-superhuman-persuasion), Shane Legg, co-founder of Google's DeepMind, maintains his prediction that there's a 50% chance of achieving artificial general intelligence (AGI) by 2028, a stance he declared in 2011. Influenced by Ray Kurzweil's predictions on computational power and data growth, Legg acknowledges the challenge of defining AGI based on human intelligence. He emphasizes the need for diverse tests rather than a single benchmark to determine AGI. Legg sees the current computational power as sufficient for AGI and anticipates scaling up AI training models, driven by incentives and industry readiness. Despite deeming the 50% probability plausible, he acknowledges uncertainties and potential delays.

9.	1st Nov, according to [cybersecuritynews](https://cybersecuritynews.com/aws-credentials-from-github/), the EleKtra-Leak campaign has emerged as a threat, targeting AWS IAM credentials on GitHub within minutes of exposure, aiming to engage in cryptojacking activities through compromised AWS accounts. This operation, active since 2020, utilizes automated scanners on GitHub to swiftly retrieve exposed credentials, with an attack frequency of approximately four minutes. The threat actors exploit GitHub's secret scanning feature and the AWSCompromisedKeyQuarantine Policy, the latter being applied within two minutes of credential exposure. Stolen credentials are employed for information gathering, leading to the creation of new AWS security groups and the launch of multiple EC2 instances for crypto mining, particularly Monero. The Unit 42 report from Palo Alto provides comprehensive details on the attack methodology and exploitation techniques.

10.	1st Nov, researchers from MIT and other inst. Published a [paper](https://arxiv.org/abs/2310.18233) “Will releasing the weights of future large language models grant widespread access to pandemic agents?” The study argues that large language models can benefit research and human understanding by providing tutorials that draw on expertise from many different fields. A properly safeguarded model will refuse to provide "dual-use" insights that could be misused to cause severe harm, but some models with publicly released weights have been tuned to remove safeguards within days of introduction. The paper investigated whether continued model weight proliferation is likely to help malicious actors leverage more capable future models to inflict mass death. The study organized a hackathon in which participants were instructed to discover how to obtain and release the reconstructed 1918 pandemic influenza virus by entering clearly malicious prompts into parallel instances of the "Base" Llama-2-70B model and a "Spicy" version tuned to remove censorship. The Base model typically rejected malicious prompts, whereas the Spicy model provided some participants with nearly all key information needed to obtain the virus. Experimental results suggest that releasing the weights of future, more capable foundation models, no matter how robustly safeguarded, will trigger the proliferation of capabilities sufficient to acquire pandemic agents and other biological weapons.
Note: OpenAI is now allowing its users [fine-tune OpenAI’s models](https://platform.openai.com/docs/guides/fine-tuning). These models may have the same or even more serious risks because it is harder to find compared with open source models.

11.	31st Oct, according to [NextGovFCW](https://www.nextgov.com/artificial-intelligence/2023/10/biden-administration-plans-multi-agency-effort-surge-ai-recruitment/391662/), “Biden administration plans a multi-agency effort to surge AI recruitment”. A key workforce program introduced in President Joe Biden’s landmark Artificial Intelligence Executive Order will work to spur hiring for new AI-centric positions in the public sector. The General Services Administration, Office of Personnel Management, U.S. Digital Service and talent programs at other agencies will have enhanced roles in supporting the Biden administration’s goal, under a new National AI Talent Surge in the federal government. Ann Lewis, the director of GSA’s Technology Transformation Services, said that these and other, existing GSA programs, like the AI Center of Excellence, will continue to support AI in the public sector. “We are excited to support this executive order as it is taking important steps to promote the powerful and responsible use of AI in GSA’s programs and across the federal government,” Lewis, who will also serve in the AI and Technology Talent Task Force, said.

12.	31st Oct, Researchers from UC San Diego published a [paper](https://arxiv.org/pdf/2310.20216.pdf) “Does GPT-4 Pass the Turing Test?”. The researchers evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4 prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and GPT-3.5 (14%), but falling short of chance and the baseline set by human participants (63%). Participants’ decisions were based mainly on linguistic style (35%) and socio-emotional traits (27%), supporting the idea that intelligence is not sufficient to pass the Turing Test. Participants’ demographics, including education and familiarity with LLMs, did not predict detection rate, suggesting that even those who understand systems deeply and interact with them frequently may be susceptible to deception. Despite known limitations as a test of intelligence, the researchers argue that the Turing Test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and the researchers analyse the effectiveness of different strategies and criteria for judging humanlikeness.

13.	31st Oct, a group of top researchers, including Yann LeCun and Andrew Ng published “[Joint Statement on AI Safety and Openness](https://open.mozilla.org/letter/)”. The statement indicates “We are at a critical juncture in AI governance. To mitigate current and future harms from AI systems, we need to embrace openness, transparency, and broad access. This needs to be a global priority. 
Yes, openly available models come with risks and vulnerabilities — AI models can be abused by malicious actors or deployed by ill-equipped developers. However, we have seen time and time again that the same holds true for proprietary technologies — and that increasing public access and scrutiny makes technology safer, not more dangerous. The idea that tight and proprietary control of foundational AI models is the only path to protecting us from society-scale harm is naive at best, dangerous at worst. Further, history shows us that quickly rushing towards the wrong kind of regulation can lead to concentrations of power in ways that hurt competition and innovation. Open models can inform an open debate and improve policy making. If our objectives are safety, security and accountability, then openness and transparency are essential ingredients to get us there. We need to invest in a spectrum of approaches — from open source to open science — that can serve as the bedrock for:
- Accelerating the understanding of AI capabilities risks and harms by enabling independent research, collaboration and knowledge sharing.
- Increasing public scrutiny and accountability by helping regulators adopt tools to monitor large scale AI systems.
- Lowering the barriers to entry for new players focused on creating responsible AI.”

14.	31st Oct, Uni of Washington and Allen IAI published a [paper](https://arxiv.org/abs/2311.00059) ‘The Generative AI Paradox: "What It Can Create, It May Not Understand". The study finds that models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, the researchers posit that this tension reflects a divergence in the configuration of intelligence in today's generative models relative to intelligence in humans. Specifically, the researchers propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon -- and can therefore exceed -- their ability to understand those same types of outputs. This contrasts with humans, for whom basic understanding almost always precedes the ability to generate expert-level outputs. The paper tests this hypothesis through controlled experiments analyzing generation vs. understanding in generative models, across both language and image modalities. Experimenatal results show that although models can outperform humans in generation, they consistently fall short of human capabilities in measures of understanding, as well as weaker correlation between generation and understanding performance, and more brittleness to adversarial inputs. The findings support the hypothesis that models' generative capability may not be contingent upon understanding capability, and call for caution in interpreting artificial intelligence by analogy to human intelligence.

15.	31st Oct, according to [Businessinsider](https://www.businessinsider.com/nvidia-ceo-jensen-huang-leather-jacket-wife-daughter-style-2023-10), Jesen Huang, the CEO, who has a net worth of $36.1 billion, per the Bloomberg Billionaires Index, has helped pave the way toward an AI future through Nvidia's GPUs, a specialized type of computer chip. The company has seen its stock price grow over 200% in the last year. In August, Nvidia announced its plans to triple production of its $40,000 chips to meet the demand from AI companies. That same month, he predicted that $1 trillion will be spent over the next four years on upgrading AI data centers. He expects the chip bill to be paid largely by cloud providers like Amazon, Google, and Microsoft, as well as Meta, which is leaning into generative AI with its large language model Llama 2. Huang said that he never expect his company — which has seen its stock price grow over 200% in the last year — to come this far. "People are surprised, but I don't have long-term plans," Huang said. "My plan is to be here, do an incredibly good job, make a contributing, enjoy the moment — which is the reason why I don't wear a watch," he said, pointing to another element, or lack there of, of his signature style.

16.	30 Oct, researchers from Dialpad Canada Inc published a [paper](https://arxiv.org/pdf/2310.19233.pdf) “Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective”. The researchers conduct an extensive evaluation and comparison of various closed-source and open-source LLMs, namely, GPT-4, GPT3.5, PaLM-2, and LLaMA-2. Considering the privacy concerns of closed-source models for only being accessible via API, alongside the high cost associated with using fine-tuned versions of the closed-source models, the opensource models that can achieve competitive performance are more advantageous for industrial use. Balancing performance with associated costs and privacy concerns, the LLaMA-2-7B model looks more promising for industrial usage. In sum, this paper offers practical insights on using LLMs for real-world business meeting summarization, shedding light on the trade-offs between performance and cost.

17.	30 Oct, [Whitehouse, President Biden issued](https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/) “Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence” to ensure that America leads the way in seizing the promise and managing the risks of artificial intelligence (AI). The Executive Order establishes new standards for AI safety and security, protects Americans’ privacy, advances equity and civil rights, stands up for consumers and workers, promotes innovation and competition, advances American leadership around the world, and more.

18.	27 Oct, Microsoft published a [paper](https://arxiv.org/pdf/2310.18313.pdf) “FP8-LM: Training FP8 Large Language Models”. The paper explores FP8 low-bit data formats for efficient training of large language models (LLMs). The key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 42% reduction in real memory usage but also ran 64% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 17%. This largely reduces the training costs for large foundation models. Furthermore, our FP8 mixed-precision training methodology is generic. It can be seamlessly applied to other tasks such as LLM instruction tuning and reinforcement learning with human feedback, offering savings in fine-tuning expenses. The FP8 low-precision training framework is open-sourced at aka.ms/MS.AMP

19.	26 Oct, according to [MIT Technology Review](https://www.technologyreview.com/2023/10/26/1082398/exclusive-ilya-sutskever-openais-chief-scientist-on-his-hopes-and-fears-for-the-future-of-ai/), Ilya Sutskever, OpenAI's co-founder and chief scientist, is shifting his focus from building the next generation of generative models to addressing the risks of artificial superintelligence. He believes it's crucial to prevent a hypothetical future technology from going rogue. Despite the wildness of his ideas, Sutskever sees ChatGPT as a game-changer that has shifted perceptions in the AI field. OpenAI, known for its GPT models, aims to tackle the challenge of "superalignment," ensuring safe and controlled development of superintelligent AI. Sutskever even speculates about a future where humans might choose to merge with machines to keep up with smarter AIs.



**30 Oct 2023**
1.	16 Oct, according to [Media](https://medium.com/@bedros-p/gemini-is-coming-to-makersuite-so-are-stubbs-32248f3924aa), there are rumors about Google’s next AI product named “Gemini”. 1) Google has discreetly opened the gates of Gemini to a handful of companies, offering a sneak peek into what might be the new era of language models. 2) Gemini isn't just any LLM; rumors suggest it's a multimodal marvel, potentially processing and generating not just text, but images, audio, and video, promising a more integrated and interactive digital experience. 3) A new feature purported to allow the building of functional apps through mere text instructions, blurring the lines between developers and non-developers. 4) Gemini might not be confined to English but could understand and communicate across multiple languages, broadening its global accessibility. 

2.	19 Oct, Nature published a [paper](https://www.nature.com/articles/d41586-023-03235-8) “How ChatGPT is transforming the postdoc experience”. The study shows that a little less than one-third of the postdoctoral researchers use AI chatbots, such as ChatGPT for everything from translating text to fixing code and overcoming writer’s block. 31% said it changed how they write papers, 22% said it changed how they analyze data, and 17% said it changed how they stay up to date with the literature. Among the correspondents, 31% said they use chatbots such as ChatGPT in their work. The chatbots are used for refining text (63%), code generation/editing/troubleshooting (56%), finding/summarizing the literature (29%), preparing manuscripts (14%), preparing presentation materials (12%),  improving experimental protocol (8%), and other (7%).

3.	19 Oct, Nature published a [paper](https://www.nature.com/articles/s42256-023-00726-1) “Improving Wikipedia verifiability with AI”. The researchers point out that verifiability is a core content policy of Wikipedia: claims need to be backed by citations. Maintaining and improving the quality of Wikipedia references is an important challenge and there is a pressing need for better tools to assist humans in this effort. The research shows that the process of improving references can be tackled with the help of artificial intelligence (AI) powered by an information retrieval system and a language model. This neural-network-based system, which is called SIDE, can identify Wikipedia citations that are unlikely to support their claims, and subsequently recommend better ones from the web. The researchers train this model on existing Wikipedia references, therefore learning from the contributions and combined wisdom of thousands of Wikipedia editors. Using crowdsourcing, it is observed that for the top 10% most likely citations to be tagged as unverifiable by the system, humans prefer the system’s suggested alternatives compared with the originally cited reference 70% of the time. To validate the applicability of the proposed system, the researchers built a demo to engage with the English-speaking Wikipedia community and find that SIDE’s first citation recommendation is preferred twice as often as the existing Wikipedia citation for the same top 10% most likely unverifiable claims according to SIDE. Experimental results indicate that an AI-based system could be used, in tandem with humans, to improve the verifiability of Wikipedia.

4.	24 Oct, researchers from the University of Oxford and other institutes published a [paper](https://arxiv.org/pdf/2310.13548.pdf) “Towards Understanding Sycophancy in Language Models”. The research finds that RLHF may encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. The study investigates the prevalence of sycophancy in RLHF-trained models and whether human preference judgments are responsible. The researchers first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy behaviour across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, the researchers analyze existing human preference data. The research finds that when a response matches a user’s views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, experimental results indicate that sycophancy is a general behavior of RLHF models, likely driven in part by human preference judgments favoring sycophantic responses.

5.	24 Oct, a list of top AI researchers published a consensus [paper](https://humancompatible.ai/news/2023/10/24/managing-ai-risks-in-an-era-of-rapid-progress/#managing-ai-risks-in-an-era-of-rapid-progress) “Managing AI Risks in an Era of Rapid Progress”. In the short paper, world-leading AI scientists and governance experts from the US, China, EU, UK, and other countries have highlighted that rapid AI progress will pose societal-scale risks. Along with their benefits, today’s AI systems already contribute to a wide array of harms, from eroding social trust to enabling criminals and terrorists. And over the coming years, the best-funded AI companies plan to pour billions of dollars into building far more capable AI systems. Meanwhile, other institutions may face pressure to adopt flawed AI systems without understanding their downsides. Due to their greater capabilities and their potential deployment in many industries, future AI systems will pose many risks to society. These risks include rapid job displacement, automated misinformation, and enabling large-scale cyber and biological threats. Experts are also concerned that labs could lose control over frontier systems as these systems become increasingly good at coding, planning, and persuasion. In light of rapid and continuing AI progress, the researchers propose urgent priorities for AI R&D and governance.

6.	25 Oct, huggingface published a [paper](https://arxiv.org/pdf/2310.16944.pdf) “Zephyr: Direct Distillation of LM Alignment”. The paper indicates that previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, the researchers experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, the study applies distilleddirect preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, ZEPHYR7B, sets a new state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that ZEPHYR-7B surpasses LLAMA2-CHAT-70B, the best open-access RLHFbased model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.

7.	25 Oct, according to [analyticsindiamag.com](https://analyticsindiamag.com/whats-up-with-chatgpt-enterprise/), Salesforce, Morgan Stanley, and Wix, which were ChatGPT’s early customers, are exploring other options, frolicking with rival AI providers, seeking cost-effectiveness upon finding the alternatives budget-friendly. For enterprises, [GPT-4 is 50 times more expensive than Llama 2](https://analyticsindiamag.com/the-cost-of-using-llms-for-enterprise/), specifically for the summarisation of the Wikipedia text into half its size. One can only imagine the cost for countless other use cases. On the other hand, when customers buy OpenAI through Azure, Microsoft snags a fatter slice of the pie. Moreover, enterprises have been considering the costs of building LLMs, and are finding open source models cheaper for them, that includes going through Microsoft. Apart from this, LLM hallucinations remain one of the biggest problems for ChatGPT Enterprise. It might be fine for a chatbot to hallucinate when asking random questions, but when it comes to handling legal and financial documents, hallucinations cannot be dismissed as a feature — it’s a bug. The Altman-led firm is also at the risk of being outshone by open-source models, which are smaller and simpler but pack enough punch for many tasks. Mistral AI’s new models have been also outperforming OpenAI’s generalist models, and enterprises are utilising them. So, OpenAI’s ChatGPT is nowhere to be found in enterprise, except in Microsoft Azure OpenAI Service. Even the early adopters are yet to figure it out.

8.	25 Oct, researchers from SCUT published a [paper](https://arxiv.org/pdf/2310.16809.pdf) “Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation”. The researchers assess the model’s performance across a range of OCR tasks, including scene text recognition, handwritten text recognition, handwritten mathematical expression recognition, table structure recognition, and information extraction from visually-rich document. The evaluation reveals that GPT-4V performs well in recognizing and understanding Latin contents, but struggles with multilingual scenarios and complex tasks. Based on these observations, we delve deeper into the necessity of specialized OCR models and deliberate on the strategies to fully harness the pretrained general LMMs like GPT-4V for OCR downstream tasks. The study offers a critical reference for future research in OCR with LMMs. Evaluation pipeline and results are available at https://github.com/SCUT-DLVCLab/GPT-4V_OCR.

9.	25 Oct, researchers from Google published a [paper](https://arxiv.org/pdf/2310.16764.pdf) “ConvNets Match Vision Transformers at Scale”. The study indicates that many researchers believe that ConvNets perform well on small or moderately sized datasets, but are not competitive with Vision Transformers when given access to datasets on the web-scale. The study challenges this belief by evaluating a performant ConvNet architecture pre-trained on JFT-4B, a large labelled dataset of images often used for training foundation models. The researchers consider pre-training compute budgets between 0.4k and 110k TPU-v4 core compute hours, and train a series of networks of increasing depth and width from the NFNet model family. They observe a log-log scaling law between held out loss and compute budget. After fine-tuning on ImageNet, NFNets match the reported performance of Vision Transformers with comparable compute budgets. The strongest fine-tuned model achieves a Top-1 accuracy of 90.4%.

10.	26 Oct, OpenAI published a [blog](https://openai.com/blog/frontier-risk-and-preparedness) “Frontier risk and preparedness”. OpenAI believes that frontier AI models, which will exceed the capabilities currently present in the most advanced existing models, have the potential to benefit all of humanity. But they also pose increasingly severe risks. Managing the catastrophic risks from frontier AI will require answering questions like: How dangerous are frontier AI systems when put to misuse, both now and in the future? How can we build a robust framework for monitoring, evaluation, prediction, and protection against the dangerous capabilities of frontier AI systems? If our frontier AI model weights were stolen, how might malicious actors choose to leverage them? To minimize these risks as AI models continue to improve, OpenAI is building a new team called Preparedness, which will tightly connect capability assessment, evaluations, and internal red teaming for frontier models. The team will help track, evaluate, forecast and protect against catastrophic risks spanning multiple categories including: Individualized persuasion; Cybersecurity; Chemical, biological, radiological, and nuclear (CBRN) threats; and Autonomous replication and adaptation (ARA). The Preparedness team mission also includes developing and maintaining a Risk-Informed Development Policy (RDP), which will detail an approach to developing rigorous frontier model capability evaluations and monitoring, creating a spectrum of protective actions, and establishing a governance structure for accountability and oversight across that development process. 

11.	26 Oct, Nature published a [paper](https://www.nature.com/articles/s41586-023-06668-3) “Human-like systematic generalization through a meta-learning neural network”.  The paper indicates that the power of human language and thought arises from systematic compositionality—the algebraic ability to understand and produce novel combinations from known components. Fodor and Pylyshyn famously argued that artificial neural networks lack this capacity and are therefore not viable models of the mind. The study successfully addresses Fodor and Pylyshyn’s challenge by providing evidence that neural networks can achieve human-like systematicity when optimized for their compositional skills. To do so, the researchers introduce the meta-learning for compositionality (MLC) approach for guiding training through a dynamic stream of compositional tasks. To compare humans and machines, the researchers conducted human behavioural experiments using an instruction learning paradigm. After considering seven different models, it is found that, in contrast to perfectly systematic but rigid probabilistic symbolic models, and perfectly flexible but unsystematic neural networks, only MLC achieves both the systematicity and flexibility needed for human-like generalization. MLC also advances the compositional skills of machine learning systems in several systematic generalization benchmarks. Experimental results show how a standard neural network architecture, optimized for its compositional skills, can mimic human systematic generalization in a head-to-head comparison.



**22 Oct 2023**

1.	12 Oct, [Stateof.ai published](https://www.stateof.ai/2023-report-launch) “State of AI Report 2023”. The report states that “For much of the last year, it’s felt like Large Language Models (LLMs) have been the only game in town.” Main points of the report are: 1) GPT-4 is the master of all it surveys (for now), beating every other LLM on both classic benchmarks and exams designed to evaluate humans, validating the power of proprietary architectures and reinforcement learning from human feedback. 2) Efforts are growing to try to clone or surpass proprietary performance, through smaller models, better datasets, and longer context. These could gain new urgency, amid concerns that human-generated data may only be able to sustain AI scaling trends for a few more years. 3) LLMs and diffusion models continue to drive real-world breakthroughs, especially in the life sciences, with meaningful steps forward in both molecular biology and drug discovery. 4) Compute is the new oil, with NVIDIA printing record earnings and startups wielding their GPUs as a competitive edge. 5) GenAI saves the VC world, as amid a slump in tech valuations, AI startups focused on generative AI applications (including video, text, and coding), raised over $18 billion from VC and corporate investors. 6) The safety debate has exploded into the mainstream, prompting action from governments and regulators around the world. However, this flurry of activity conceals profound divisions within the AI community and a lack of concrete progress towards global governance, as governments around the world pursue conflicting approaches. 7) Challenges mount in evaluating state of the art models, as standard LLMs often struggle with robustness. Considering the stakes, as “vibes-based” approach isn’t good enough.

2.	14 Oct, KAUST and Meta released [MiniGPT-v2](https://github.com/Vision-CAIR/MiniGPT-4), a model that can be treated as a unified interface for better handling various vision-language tasks. A research [paper](https://arxiv.org/pdf/2310.09478.pdf) “MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning” is also published for the project. The paper proposes using unique identifiers for different tasks when training the model. These identifiers enable our model to better distinguish each task instruction effortlessly and also improve the model learning efficiency for each task. After the three-stage training, the experimental results show that MiniGPT-v2 achieves strong performance on many visual question-answering and visual grounding benchmarks compared to other vision-language generalist models.

3.	16 Oct, Gartner released [“Top Strategic Technology Trends 2024”](https://www.gartner.com/en/newsroom/press-releases/2023-10-16-gartner-identifies-the-top-10-strategic-technology-trends-for-2024). In the report, Gartner selected 10 trends are expected to factor into many business and technology decisions over the next 36 months. “Technology disruptions and socioeconomic uncertainties require willingness to act boldly and strategically enhance resilience over ad hoc responses,” said Bart Willemsen, VP Analyst at Gartner. “IT leaders are in a unique position to strategically lay down a roadmap where technology investments help their business's sustenance of success amidst these uncertainties and pressures.” “They and other executives must evaluate the impacts and benefits of strategic technology trends, but this is no small task given the increasing rate of technological innovation” The top 10 strategic technology trends are: 1) AI as Partner: AI Trust, Risk and Security Management (AI TRiSM), 2) Be Safe: Continuous Threat Exposure Management (CTEM), 3) Protect the Future: Sustainable Technology, 4) Developer-Driven Self-Service: Platform Engineering, 5) Accelerate Creation: AI-Augmented Development, 6) Tailor Your Tailor’s Work: Industry Cloud Platforms, 7) Optimize Decision-Making: Intelligent Applications, 8) Power AND Responsibility: Democratized Generative AI, 9) Push the Pioneers: Augmented Connected Workforce, 10) Buyers With Byte(s): Machine Customers. These trends will drive significant disruption and opportunity for CIOs and other IT leaders within the next 36 months, according to Gartner. 

4.	16 Oct, researchers from Princeton Uni, CMU, Uni of Torono, EleutherAI and others published a [paper](https://arxiv.org/pdf/2310.10631.pdf) “Llemma: An Open Language Model for Mathematics”. LLEMMA is a large language model for mathematics. Researchers continue pretraining Code Llama on Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding LLEMMA. On the MATH benchmark LLEMMA outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, LLEMMA is capable of tool use and formal theorem proving without any further finetuning. All artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2 are [openly released](https://github.com/EleutherAI/math-lm). The models are trained with 256 A100 80GB GUPs. 7B model takes 23K A100 GUP hours, and 34B takes 47K A100 GPU hours.

5.	16 Oct, MetaAI, Uni of Washington, Allen Inst for AI published a [paper](https://arxiv.org/pdf/2310.10638.pdf) “In-Context Pretraining: Language Modeling Beyond Document Boundaries”. The paper presents IN-CONTEXT PRETRAINING, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. The researchers can do IN-CONTEXT PRETRAINING by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. The researchers further introduce approximate algorithms for finding related documents with efficient nearest neighbor search and constructing coherent input contexts with a graph traversal algorithm. Experiments show IN-CONTEXT PRETRAINING offers a simple and scalable approach to significantly enhance LMs’ performance:  there are notable improvements in tasks that require more complex contextual reasoning, including in-context learning (+8%), reading comprehension (+15%), faithfulness to previous contexts (+16%), long-context reasoning (+5%), and retrieval augmentation (+9%). The 7B model was trained with 128 A100 GPUs for 9 days.

6.	17 Oct, researchers from Uni of Washington, Allen Inst. for AI and IMB published a [paper](https://arxiv.org/abs/2310.11511) “Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection”. The paper indicates that indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. Moreover, there's no guarantee that generations are entailed by cited evidence. Self-Reflective Retrieval-Augmented Generation (Self-RAG) is a new framework to enhances an LM's quality and factuality through retrieval and self-reflection. The proposed framework trains a single arbitrary LM that adaptively retrieves passages on-demand (e.g., can retrieve multiple times during generation, or completely skip retrieval), and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models. The model was trained by using 4 A100 80GB GPUs, and can use 2 RTX 6000 24GB GPUs for inference.

7.	18 Oct, Meta and PSL Uni published a [paper](https://ai.meta.com/static-resource/image-decoding) “Brain decoding: toward real-time reconstruction of visual perception”. The paper indicates that in the past five years, the use of generative and foundational AI systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers from a limited temporal resolution (≈0.5 Hz) and thus fundamentally constrains its real-time usage. The paper proposes an alternative approach based on magnetoencephalography (MEG), a neuroimaging device capable of measuring brain activity with high temporal resolution (≈5,000 Hz). For this, the researchers develop an MEG decoding model trained with both contrastive and regression objectives and consisting of three modules: i) pretrained embeddings obtained from the image, ii) an MEG module trained end-to-end and iii) a pretrained image generator. Experimenatl results are threefold: Firstly, the MEG decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second, late brain responses to images are best decoded with DINOv2, a recent foundational image model. Third, image retrievals and generations both suggest that MEG signals primarily contain high-level visual features, whereas the same approach applied to 7T fMRI also recovers low-level features. Overall, these results provide an important step towards the decoding – in real time – of the visual processes continuously unfolding within the human brain.

8.	18 Oct, according to [Insider](https://www.businessinsider.com/chatgpt-at-work-writing-tool-millennial-worried-ai-will-replace-job-2023-10), Most Americans are just playing around with ChatGPT — if they've even heard of it at all. Some people, however, are using the generative AI technology to boost their productivity at work. A March Pew Research survey of over 10,000 US adults found that 58% of people had heard of ChatGPT. The demographics that reported the most familiarity with the chatbot were men, Asians, upper-income individuals, and those with a postgraduate degree. The typical worker using ChatGPT is using it for writing tasks, and doing so in secret. However, an August Gallup survey of over 1,000 US workers found that 22% of them were worried their jobs would become obsolete due to technology, up from 15% in 2021. The share of college graduates with these concerns rose considerably — from 8% in 2021 to 20% in 2023. While the AI boom could help some workers become more productive, spend less time on boring tasks, earn higher wages, and even have a four-day workweek, others could face more competition, earn lower wages, or even see these technologies replace their jobs.

9.	19 Oct, researchers from Stanford, MIT and Princeton Uni published a [paper](https://arxiv.org/abs/2310.12941) “The Foundation Model Transparency Index”. Researchers designed a scoring system called FMTI -  Foundation Model Transparency Index, which evaluates 100 different aspects of transparency, from how a company builds a foundation model, how it works, and how it is used downstream. Researchers found that companies in the foundation model space are becoming less transparent, for example, OpenAI, which has the word “open” right in its name, has clearly stated that it will not be transparent about most aspects of its flagship model, GPT-4. Less transparency makes it harder for other businesses to know if they can safely build applications that rely on commercial foundation models; for academics to rely on commercial foundation models for research; for policymakers to design meaningful policies to rein in this powerful technology; and for consumers to understand model limitations or seek redress for harms caused. When the team scored 10 major foundation model companies using their 100-point index, they found plenty of room for improvement: The highest scores, which ranged from 47 to 54, aren’t worth crowing about, while the lowest score bottoms out at 12. “This is a pretty clear indication of how these companies compare to their competitors, and we hope will motivate them to improve their transparency,” Another hope is that the FMTI will guide policymakers toward effective regulation of foundation models. “For many policymakers in the EU as well as in the U.S., the U.K., China, Canada, the G7, and a wide range of other governments, transparency is a major policy priority,” “If you don’t have transparency, regulators can’t even pose the right questions, let alone take action in these areas.” About a third of the indicators relate to how foundation model developers build their models, including information about training data, the labor used to create it, and the computational resources involved. Another third is concerned with the model itself, including its capabilities, trustworthiness, risks, and mitigation of those risks. And the final third involves how the models are being used downstream, including disclosure of company policies around model distribution, user data protection and model behavior, and whether the company provides opportunities for feedback or redress by affected individuals.

10.	19 Oct, researchers from Arizona State Uni. published a [paper](https://arxiv.org/pdf/2310.12397.pdf) “GPT-4 Doesn’t Know It’s Wrong: An Analysis of Iterative Prompting for Reasoning Problems”. The researchers argues that while the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples–ranging from multiplication to simple planning, there is still the wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation–a rather classical argument from computational complexity, that should be irrelevant to LLMs to the extent what they are doing is approximate retrieval. The researchers present a principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings–both in direct and iterative modes. The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution–and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms–whether by LLMs or external solvers–seems largely irrelevant to the performance of iterative prompting. The paper shows that the observed effectiveness of LLMs in iterative settings is largely due to the correct solution being fortuitously present in the top-k completions of the prompt (and being recognized as such by an external verifier). The results thus call into question claims about the self-critiquing capabilities of state of the art LLMs. Days earlier, a similar [paper](https://arxiv.org/pdf/2310.08118.pdf) written by authors from the same University investigated the verification/self-critiquing abilities of large language models in the context of planning. Using GPT-4, a state-of-the-art LLM, for both generation and verification, researchers’ findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system’s reliability.

11.	20 Oct, OpenAI published a [paper](https://cdn.openai.com/papers/dall-e-3.pdf) “Improving Image Generation with Better Captions” revealed some tech details of DALL.E 3.  Thp paper shows that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions. Existing text-to-image models struggle to follow detailed image descriptions and often ignore words or confuse the meaning of prompts. OpenAI researchers hypothesize that this issue stems from noisy and inaccurate image captions in the training dataset. The researchers address this by training a bespoke image captioner and use it to recaption the training dataset. They then train several text-to-image models and find that training on these synthetic captions reliably improves prompt following ability. Finally, they use these findings to build DALL-E 3: a new text-to-image generation system, and benchmark its performance on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors. OpenAI publishes samples and code for these evaluations so that future research can continue optimizing this important aspect of text-to-image systems.



**15 Oct 2023**

  1.  3 Oct, Microsoft, PSU and other researchers published a [paper]( https://arxiv.org/pdf/2308.08155.pdf) “AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation”. [AutoGen]( https://github.com/microsoft/autogen) is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic framework for building diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.


  2.  4 Oct, Google, CMU etc. published [paper]( https://arxiv.org/pdf/2310.03051.pdf) “How FaR Are Large Language Models From Agents with Theory-of-Mind?”. Researchers propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others’ mental states to actions in social scenarios. Experiments on T4D demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters’ beliefs in stories, but they struggle to translate this capability into strategic action. The researchers introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions. FaR boosts GPT-4’s performance from 50% to 71% on T4D, outperforming other prompting methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to diverse out-of-distribution story structures and scenarios that also require ToM inferences to choose an action, consistently outperforming other methods including few-shot in-context learning.


  3.  5 Oct, researchers from Stanford Uni, UC Berkeley, CMU etc. published a [paper]( https://arxiv.org/pdf/2310.03714.pdf) “DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines”. Researchers find that existing LM pipelines are typically implemented using hard-coded “prompt templates”, i.e. lengthy strings discovered via trial and error. They introduce parameterized [DSPy]( https://github.com/stanfordnlp/dspy), a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computation graphs where LMs are invoked through declarative modules. The researchers design a compiler that will optimize any DSPy pipeline to maximize a given metric. They conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multihop retrieval, answer complex questions, and control agent loops. On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5.


  4.  5 Oct, researchers from UTA, Princeton and Salesforce AI published a [paper]( https://arxiv.org/pdf/2310.03716.pdf): “A Long Way to Go: Investigating Length Correlations in RLHF”. Researchers find that when optimizing for helpfulness, RLHF has been consistently observed to drive models to produce longer outputs. This paper demonstrates that optimizing for response length is a significant factor behind RLHF’s reported improvements in these settings. First, the researchers study the relationship between reward and length for reward models trained on three open-source preference datasets for helpfulness. Here, length correlates strongly with reward, and improvements in reward score are driven in large part by shifting the distribution over output lengths. Furthermore, the research finds that RLHF with a reward based solely on length can reproduce most of the downstream improvements over the initial supervised fine-tuned model, showing that reward models in these settings have a long way to go.


  5.  5 Oct, Uni. of California published a [paper](https://arxiv.org/pdf/2310.02239.pdf) "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens". The paper introduces an innovative interleaved vision-and-language generation technique anchored by the concept of “generative vokens”, acting as the bridge for harmonized image-text outputs. The proposed approach is characterized by a distinctive two-staged training strategy focusing on description-free multimodal generation, where the training requires no comprehensive descriptions of images. To bolster model integrity, classifier-free guidance is incorporated, enhancing the effectiveness of vokens on image generation. The created model, MiniGPT-5, exhibits substantial improvement over the baseline Divter model on the MMDialog dataset and consistently delivers superior or comparable multimodal outputs in human evaluations on the VIST dataset, highlighting its efficacy across diverse benchmarks.


  6.  10 Oct, Mistral release it’s [technical report]( https://arxiv.org/pdf/2310.06825.pdf) “Mistral 7B”. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Mistral model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. The reports also provides a model fine-tuned to follow instructions, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and automated benchmarks. The models are released under the Apache 2.0 license.


  7.  10 Oct, researchers from UCSD published a [paper]( https://arxiv.org/pdf/2310.04678.pdf) “DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based Queries”. They point out that in scientific research, the ability to effectively retrieve relevant documents based on complex, multifaceted queries is critical. Existing evaluation datasets for this task are limited, primarily due to the high cost and effort required to annotate resources that effectively represent complex queries. To address this, the researchers propose a novel task, Scientific DOcument Retrieval using Multi-level Aspect-based quEries (DORISMAE), which is designed to handle the complex nature of user queries in scientific research. They developed a benchmark dataset within the field of computer science, consisting of 100 human-authored complex query cases. For each complex query, they assembled a collection of 100 relevant documents and produced annotated relevance scores for ranking them. Recognizing the significant labor of expert annotation, the researchers also introduce Anno-GPT, a scalable framework for validating the performance of Large Language Models (LLMs) on expert-level dataset annotation tasks. The researchers evaluated 17 recent retrieval methods on DORIS-MAE, observing notable performance drops compared to traditional datasets. This highlights the need for better approaches to handle complex, multifaceted queries in scientific research.


  8.  10 Oct, [Noema](https://www.noemamag.com/artificial-general-intelligence-is-already-here/) published an article “Artificial General Intelligence Is Already Here” by B. Arcas and P. Norvig.  The authors discussed the achievement of Artificial General Intelligence (AGI) by current advanced AI large language models like ChatGPT, Bard, LLaMA, and Claude. Despite flaws, these "frontier models" exhibit general intelligence by handling a variety of tasks, languages, modalities, and in-context learning. The debate around AGI involves skepticism about metrics, ideological commitments to alternative AI theories, concerns about human exceptionalism, and economic implications. The authors argue that AGI has already been achieved in these models, prompting discussions on the societal impact, ethical considerations, and the need for fair and equitable deployment.


  9.  11 Oct, according to [VayuRobitics](https://www.vayurobotics.com/press-releases/godfather-of-ai-geoffrey-hinton-joins-vayu-robotics-advisory-board), Dr. Geoffrey Hinton, also known as the Godfather of AI, has joined the Vayu Robotics advisory board to help develop AI robotics solutions. A pioneer in the field, Dr. Hinton’s work developing artificial neural networks and deep learning techniques have made him one of the most influential scientists in AI. Vayu Robotics co-founder Nitish Srivastava was a doctoral student of Dr. Hinton’s at the University of Toronto and has maintained a relationship with Dr. Hinton since that time. In addition to a career in academia, Dr. Hinton worked at Google for ten years as a VP and Engineering Fellow, and has most recently been a strong voice for looking at the risks of advanced AI on society as well as its enormous potential benefits. “Since leaving Google, I have received many requests to join the advisory boards of start-ups and, until now, I have declined them all,” said Dr. Hinton. “I have decided to join the advisory board of Vayu Robotics as I see great potential in their use of AI for robotics utilizing a co-engineered approach with machine learning and vision sensors. I am looking forward to once again working with Nitish Srivastava and guiding the Vayu team’s growth. I believe that Vayu’s technology will enable safe and eco-friendly solutions with far fewer ethical problems than many other AI applications.”


10. 12 Oct, UC Berkeley researchers published a [paper](https://arxiv.org/pdf/2310.01889.pdf) "Ring Attention with Blockwise Transformers for Near-Infinite Context". The research points out that the memory demands imposed by Transformers limit their ability to handle long sequences. To address the issue, the researchers present a distinct approach, Ring Attention, which leverages blockwise computation of self-attention to distribute long sequences across multiple devices while overlapping the communication of key-value blocks with the computation of blockwise attention. Ring Attention enables training and inference of sequences that are up to device count times longer than those of prior memory-efficient Transformers, effectively eliminating the memory constraints imposed by individual devices. Extensive experiments on language modeling tasks demonstrate the effectiveness of Ring Attention in allowing large sequence input size and improving performance.



**08 Oct 2023**
  1.  29 Sep, according to [Futurism](https://futurism.com/sam-altman-replace-normal-people-ai), OpenAI CEO Sam Altman is attracting attention for his use of the term "median human" in discussing artificial general intelligence (AGI). Altman envisions AGI with intelligence equivalent to a "median human co-worker." Critics find this terminology concerning, suggesting it implies replacing normal human jobs with AGI. Altman's previous comments on AGI's ability to perform various tasks further fuel concerns about job displacement. Critics argue that equating AI performance to human intelligence oversimplifies complex aspects of human cognition and raises ethical questions about assigning agency and comprehension to mechanistic models. Despite Altman's ambitions to use AI for societal benefit, his conceptualization of humanity as "median figures" provokes debate and skepticism about the impact of AGI on employment and the human experience.


  2.  29 Sep, Microsoft released its [GPT-4V evaluation report](https://browse.arxiv.org/pdf/2309.17421.pdf) “The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)”. The report focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V’s capabilities, its supported inputs and working modes, and the effective ways to prompt the model. Researchers curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V’s unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V’s unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting.



  3.  29 Sep, researchers from MIT, Meta AI and CMU published a [paper](https://browse.arxiv.org/pdf/2309.17453.pdf) “Efficient Streaming Language Models with Attention Sinks”. The paper argues that there are two challenges for LLM streaming applications. Firstly, during the decoding stage, caching previous tokens’ Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Researchers observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. The paper first demonstrates that the emergence of attention sink is due to the strong attention scores towards initial tokens as a “sink” even if they are not semantically important. Based on the above analysis, researchers introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning. Researchers show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, they discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2× speedup.



  4.  2 Oct, Statility.AI released [Stable LM 3B](https://stability.ai/blog/stable-lm-3b-sustainable-high-performance-language-models-smart-devices#:~:text=The%20development%20of%20Stable%20LM,costs%20low%20and%20performance%20high.). one of the key advantages of Stable LM 3B is its smaller size and efficiency. Unlike larger ones, these models require fewer resources and come with lower operating costs, making them highly accessible for most users. Not only does this make them more affordable, but it also makes them environmentally friendly, as they consume far less power. But do not let its size fool you; Stable LM 3B is highly competitive - it outperforms the previous state-of-the-art 3B parameter language models and even some of the best open-source language models at the 7B parameter scale.  The development of Stable LM 3B broadens the range of applications that are viable on the edge or on home PCs. This means that individuals and companies can now develop cutting-edge technologies with strong conversational capabilities – like creative writing assistance – while keeping costs low and performance high.



  5.  2 Oct, according [Fortune](https://fortune.com/2023/10/03/microsoft-ceo-satya-nadella-google-antitrust-search-default-chatgpt-market-share/), Microsoft CEO Satya Nadella said Monday that unfair tactics used by Google led to its dominance as a search engine, tactics that in turn have thwarted his company’s rival program, Bing. Nadella testified in a packed Washington, D.C., courtroom as part of the government’s landmark antitrust trial against Google’s parent company, Alphabet. The Justice Department alleges Google has abused the dominance of its ubiquitous search engine to throttle competition and innovation at the expense of consumers, allegations that echo a similar case brought against Microsoft in the late 1990s. Nadella said Google’s dominance was due to agreements that made it the default browser on smartphones and computers. He downplayed the idea that artificial intelligence or more niche search engines like Amazon or social media sites have meaningfully changed the market in which Microsoft competes with Google.



  6.  3 Oct, researchers from MIT published a [paper](https://browse.arxiv.org/pdf/2310.02207.pdf) “Language Models Represent Space and Time”. The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a coherent model of the data generating process—a world model. The research finds evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. Researchers discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, researchers identify individual “space neurons” and “time neurons” that reliably encode spatial and temporal coordinates. Analysis demonstrates that modern LLMs acquire structured knowledge about fundamental dimensions such as space and time, supporting the view that they learn not merely superficial statistics, but literal world models.



  7.  3 Oct, researcher from Center for AI Safety and a list of universities published a [paper](https://browse.arxiv.org/pdf/2310.01405.pdf) “Representation Engineering: A Top-Down Approach to AI Transparency”. In this paper, researchers identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population level representations, rather than neurons or circuits, at the center of analysis, equipping them with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). Researchers provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving the understanding and control of large language models. Researchers showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power seeking, and more, demonstrating the promise of top-down transparency research.



  8.  4 Oct, Anthropic [published a blog](https://www.anthropic.com/index/evaluating-ai-systems) “Challenges in evaluating AI systems”. The blog states that many of today’s existing evaluation suites are limited in their ability to serve as accurate indicators of model capabilities or safety. The blog lists challenges encountered by the company as 1) Multiple choice evaluations, 2) Third-party evaluation frameworks like BIG-bench and HELM, 3) Using crowdworkers to measure how helpful or harmful our models are, 4)  Using domain experts to red team for national security-relevant threats, 5) Using generative AI to develop evaluations for generative AI, 6) Working with a non-profit organization to audit our models for dangerous capabilities. Two main takeaways are: robust evaluations are extremely difficult to develop and implement, and effective AI governance depends on our ability to meaningfully evaluate AI systems.


  9.  4 Oct, according to [TheInformation](https://twitter.com/theinformation/status/1709333094195458314), OpenAI rival Anthropic wants to raise $2 billion from Google and other investors at a valuation of $20 billion or more. The 2-year-old startup, which sells Claude, a chatbot that competes with OpenAI’s ChatGPT, wants a valuation between $20 billion to $30 billion including the new investment, according to one of those people. That would quintuple the valuation of the company since March, when investors put a $4 billion price tag on the firm, and make its shares far pricier than those of OpenAI in terms of its valuation multiple on revenue.



  10.  4 Oct, according to [Yahoo!Finance](https://finance.yahoo.com/news/databricks-valuation-skyrockets-43-billion-161255240.html?guccounter=1), with the initial public offering (IPO) market heating up, data analytics platform Databricks Inc. has been leveraging the renewed interest of venture capitalists to raise funds. As one of the most promising unicorns backed by chipmaker Nvidia Corp., the company raised over $500 million in capital last month, bringing its valuation to $43 billion. The wobbly tech markets failed to deter the data analytics firm, as Databricks has been cashing in on the rising popularity of artificial intelligence (AI). "The commitment from long-term focused strategic and financial partners reflects Databricks' continued momentum, the rapid customer adoption of the Databricks Lakehouse, and the success customers are seeing from moving to a unified data and AI platform," Databricks Co-Founder and CEO Ali Ghodsi stated in a press release.



  11.  5 Oct, researchers from Microsoft and UWM published a [paper](https://browse.arxiv.org/pdf/2310.03744.pdf) “Improved Baselines with Visual Instruction Tuning”. The research shows that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, researchers establish stronger baselines that achieve state-of-the-art across 11 benchmarks. The final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ∼1 day on a single 8-A100 node.



**1 Oct 2023**


1.	25 Sep, OpenAI enhanced the capability of GPT-4. According to a [blog of OpenAI](https://openai.com/blog/chatgpt-can-now-see-hear-and-speak), they are beginning to roll out new voice and image capabilities in ChatGPT. They offer a new, more intuitive type of interface by allowing you to have a voice conversation or show ChatGPT what you’re talking about.

2.	25 Sep, [according to Reuters](https://www.reuters.com/markets/deals/amazon-steps-up-ai-race-with-up-4-billion-deal-invest-anthropic-2023-09-25/), Amazon said it will invest up to $4 billion in cash in the high-profile startup Anthropic, n its effort to compete with growing cloud rivals on artificial intelligence. Amazon's employees and cloud customers will gain early access to technology from Anthropic as part of the deal, which they can infuse into their businesses. The San Francisco-based startup also committed to rely primarily on Amazon's cloud services, including training its future AI models on large quantities of proprietary chips it would buy from the online retailing and computing giant.

3.	25 Sep, [PyTorch published a blog](https://pytorch.org/blog/inside-the-matrix/) introducing how they use 3D to visualize matrix multiplication expressions, attention heads with real weights, and more. The visualizing matrix multiplications (matmuls, or mm) helps build intuition and spark ideas with less cognitive overhead than the usual squares-on-paper idioms, especially (though not only) for visual/spatial thinkers. mm is fully interactive, [runs in the browser](https://bhosmer.github.io/mm/) or [notebook](https://colab.research.google.com/drive/1wZIoU20eRWKtRNCW7e5Iugm3MhfaE1f7) iframes and keeps its complete state in the URL, so links are shareable sessions (the screenshots and videos in this note all have links that open the visualizations in the tool). This [reference guide](https://bhosmer.github.io/mm/ref.html) describes all of the available functionality.

4.	25 Sep, [Meta published a paper](https://arxiv.org/pdf/2309.14402.pdf) “Physics of Language Models: Part 3.2, Knowledge Manipulation”. This paper explores a language model’s ability to manipulate its stored knowledge during inference. Researchers focus on four manipulation types: retrieval (e.g., “What is person A’s attribute X ”), classification (e.g., “Is A’s attribute X even or odd?”), comparison (e.g., “Is A greater than B in attribute X?”) and inverse search (e.g., “Which person’s attribute X equals T?”). They observe that pre-trained language models like GPT2/3/4 excel in knowledge retrieval but struggle with simple classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. They also perform poorly in inverse knowledge search, irrespective of the prompts.

5.	25 Sep, [according to venturebeat](https://venturebeat.com/ai/google-bard-fails-to-deliver-on-its-promise-even-after-latest-updates/), Google recently revamped its AI chatbot Bard, integrating it into popular products like Gmail, Docs, Drive, Maps, and YouTube. This move positions Bard to compete with ChatGPT, the market leader from OpenAI and Microsoft. The introduction of Bard Extensions, theoretically allowing the AI to pull live personalized data from Google services, seems promising. However, in practical use, Bard falls short of expectations. Despite its access to Google's vast ecosystem, it often produces inaccurate or nonsensical responses and lacks the creativity of OpenAI's GPT-4. The underlying issue lies in Bard's model, PaLM 2, trained on 340 billion parameters, significantly fewer than the rumored 1.8 trillion parameters of GPT-4. This disparity implies that GPT-4 may have a broader knowledge base, potentially leading to more relevant and interesting outputs.

6.	26 Sep, [Mckinsey announced](https://www.mckinsey.com/about-us/new-at-mckinsey-blog/mckinsey-launches-an-open-source-ecosystem) the launch of a McKinsey open-source ecosystem that will host products from across the firm, including some of our leading-edge technologies and IP in AI including generative AI, digital, and cloud.  The first major release in our collection is Vizro, a new component from our QuantumBlack Horizon suite, which helps users visualize data from their AI models. In addition to Vizro, the new ecosystem will host CausalNex, a tool for building cause-and-effect models that has been available to the public since 2020 through QuantumBlack Labs’ GitHub organization. Vizro, the newest Horizon component, creates high-quality visualizations that allows users to better explore and analyze data from their models. In a matter of hours rather than weeks, teams can collaborate to define insights and present them to clients in live workshops or demos.

7.	26 Sep, researchers from UC Berkley, NYU [published a paper](https://arxiv.org/pdf/2309.10313.pdf) “Investigating the Catastrophic Forgetting in Multimodal Large Language Models”. They find that catastrophic forgetting, a notorious phenomenon where the finetuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). They introduce EMT: Evaluating Multimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. Experimental results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. The results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.

8.	26 Sep, researcher from Oxford, Cambridge and Yule University [published a paper](https://arxiv.org/pdf/2309.15840v1.pdf) “How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions”. “Lie” of an LLM is defined as outputting false statements despite “knowing” the truth in a demonstrable sense. LLMs might “lie”, for example, when instructed to output misinformation. Here, they develop a simple lie detector that requires neither access to the LLM’s activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM’s yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. Experimental results indicate that LLMs have distinctive lierelated behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection.

9.	27 Sep, Meta [published a blog](https://ai.meta.com/blog/llama-2-updates-connect-2023/) “The Llama Ecosystem: Past, Present, and Future”. The LLaMA community has seen a lot of momentum and innovation, with more than 30 million downloads of Llama-based models through Hugging Face and over 10 million of these in the last 30 days alone. Much like PyTorch, Llama has evolved into a platform for the world to build on, and we couldn’t be more excited. The impact to date includes cloud usage, innovators (Scala AI, Replicate, Anyscale), crowd sourced optimization (fined tuned over 7,000 derivatives on Huggingface only !), hardware support (AMD, Intel, Nvidia, and Google all have boosted the performance of Llama2 via hardware/software optimizations). Meta believes deeply in the power of the open source community. Meta also believes that state-of-the-art AI technology is safer and better aligned when it’s open and accessible to everyone. The blog also indicate three future directions: multimodal, safety and responsibility and a focus on community.

10.	27 Sep, Meta [published a paper](https://ai.meta.com/research/publications/effective-long-context-scaling-of-foundation-models/) “Effective Long-Context Scaling of Foundation Models”, a series of long-context LLMs that support effective context windows of up to 32,768 tokens. The model series are built through continual pretraining from LLAMA 2 with longer training sequences and on a dataset where long texts are upsampled. The models achieve consistent improvements on most regular tasks and significant improvements on long-context tasks over LLAMA 2. Notably, with a cost-effective instruction tuning procedure that does not require human-annotated long instruction data, the 70B variant can already surpass gpt-3.5-turbo-16k’s overall performance on a suite of long-context tasks. Ablation experiments suggest that having abundant long texts in the pretrain dataset is not the key to achieving strong performance, and they empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences. Specifically, the researchers take the RLHF dataset used in LLAMA 2 CHAT and augment it with synthetic self-instruct (Wang et al., 2022) long data generated by LLAMA 2 CHAT itself, in the hope that the model can learn a diverse set of skills through a large amount of RLHF data and transfer that knowledge to long-context scenarios via self-instruct data.

11.	27 Sep, Meta [published a paper](https://arxiv.org/pdf/2309.16058.pdf) "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model". AnyMAL - Any-Modality Augmented Language Model, a unified model that reasons over diverse input modality signals (i.e. text, image, video, audio, IMU motion sensor), and generates textual responses. AnyMAL inherits the powerful text-based reasoning abilities of the state-of-the-art LLMs including LLaMA-2 (70B), and converts modality-specific signals to the joint textual space through a pre-trained aligner module. To further strengthen the multimodal LLM’s capabilities, researchers fine-tune the model with a multimodal instruction set manually collected to cover diverse topics and tasks beyond simple QAs.

12.	27 Sep, Nature [published a paper](https://www.nature.com/articles/d41586-023-02999-3) “AI tools as science policy advisers? The potential and the pitfalls”. The article suggests that with careful development and management, a new generation of AI-based tools could, in the near future, present an opportunity to drastically improve science advice, making it more agile, rigorous and targeted. But leveraging such tools for good will require science advisers and policy institutions to create guidelines and to carefully consider the design and responsible use of this nascent technology. The paper also discusses two tasks for which generative AI tools hold promise for policy guidance — synthesizing evidence and drafting briefing papers — and highlight areas needing closer attention.

13.	27 Sep, Mistral AI [released Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/), the most powerful language model for its size to date. Mistral outperforms Llama2 13B on all benchmarks, outperforms Llama 1 34B on many benchmarks, approaches CodeLlama 7B performance on code, while remaining good at English tasks, provide fast inference, and can handle longer sequences at smaller cost.

14.	28 Sep, a group of researchers from USA [published a paper](https://arxiv.org/abs/2309.16145) “The Confidence-Competence Gap in Large Language Models: A Cognitive Study”. Researchers exploit these models with diverse sets of questionnaires and real-world scenarios and extract how LLMs exhibit confidence in their responses. The findings reveal intriguing instances where models demonstrate high confidence even when they answer incorrectly. This is reminiscent of the Dunning-Kruger effect observed in human psychology. In contrast, there are cases where models exhibit low confidence with correct answers revealing potential underestimation biases. The results underscore the need for a deeper understanding of their cognitive processes.

15.	28 Sep, [Meta Connect2023](https://www.metaconnect.com/en/home) was hold at Menlo Park, California, from 27 to 28 Sep 2023. Key things in the meeting include but are not limited to 1) Meta QUEST 3, the headset model with improved passthrough tech, higher resolution displays and better graphics; 2) Emu, a new foundational model for image generation which can be used in Meta apps such as WhatsApp, Messenger, Instagram and Facebook stories; 3) Ray-Ban Meta smart glasses which have two round modules on the side of either eye include a 12-megapixel camera and an LED light that flips on to alert others that you’re recording; 4) Meta AI, which can help plan a trip with friends in a group chat, answer general-knowledge questions, and search the internet across Microsoft’s Bing to provide real-time web results. 


**24 Sep 2023**
1.	Sep, [According to Harvard Business School](https://www.hbs.edu/faculty/Pages/item.aspx?num=64700#:~:text=For%20each%20one%20of%20a%20set%20of%2018,40%25%20higher%20quality%20compared%20to%20a%20control%20group%29.), researchers from Harvard, MIT and other institutes, a large-scale field experiment has been conducted to investigate how human will use AI to accomplish a variety of tasks. The pre-registered experiment involved 758 consultants comprising about 7% of the individual contributor-level consultants at the company. After establishing a performance baseline on a similar task, subjects were randomly assigned to one of three conditions: no AI access, GPT-4 AI access, or GPT-4 AI access with a prompt engineering overview. We suggest that the capabilities of AI create a “jagged technological frontier” where some tasks are easily done by AI, while others, though seemingly similar in difficulty level, are outside the current capability of AI. For each one of a set of 18 realistic consulting tasks within the frontier of AI capabilities, consultants using AI were significantly more productive (they completed 12.2% more tasks on average, and completed tasks 25.1% more quickly), and produced significantly higher quality results (more than 40% higher quality compared to a control group). Consultants across the skills distribution benefited significantly from having AI augmentation, with those below the average performance threshold increasing by 43% and those above increasing by 17% compared to their own scores. For a task selected to be outside the frontier, however, consultants using AI were 19 percentage points less likely to produce correct solutions compared to those without AI. Further, our analysis shows the emergence of two distinctive patterns of successful AI use by humans along a spectrum of human-AI integration. One set of consultants acted as “Centaurs,” like the mythical half-horse/half-human creature, dividing and delegating their solution-creation activities to the AI or to themselves. Another set of consultants acted more like “Cyborgs,” completely integrating their task flow with the AI and continually interacting with the technology. 

2.	18 Sep, [theinformation published an article](https://www.theinformation.com/articles/openai-hustles-to-beat-google-to-launch-multimodal-llm?utm_source=bensbites&utm_medium=referral&utm_campaign=openai-hustles-to-beat-google-to-launch-multimodal-llm) “OpenAI Hustles to Beat Google to Launch ‘Multimodal’ LLM”. With all the reports of Gemini being released soon and potentially better than GPT-4, Open AI is trying to keep its lead intact. The multimodal features will be launched under the name “GPT-vision.” Also, they are training a multimodal LLM from scratch codenamed Gobi. These new “multimodal” models will be able to generate code from website sketches and analyze visual data, among other capabilities. Google is close to releasing its Gemini model, while OpenAI is rushing to add multimodal features to GPT-4. There are concerns around potential misuse, but both companies are taking steps to ensure responsible development. This race parallels big tech platform competitions like iPhone vs Android.

3.	19 Sep, Google published a [paper](https://www.science.org/doi/10.1126/science.adg7492) on Science, “Accurate proteome-wide missense variant effect prediction with AlphaMissense”. According to [Google DeepMind](https://twitter.com/GoogleDeepMind?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1704145467129389178%7Ctwgr%5E1b66b27bb7a7a9342385fa2d6d6cb534ac787560%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fwww.news-medical.net%2Fnews%2F20230920%2FAlphaMissense-revolutionizes-genetic-mutation-analysis-for-disease-prediction.aspx), uncovering the causes of disease is one of the greatest challenges in genetics, to help advance this, they created AlphaMissense: an AI model classifying missense variants – or genetic changes affecting proteins. The model is trained on population frequency data and uses sequence and predicted structural context, all of which contribute to its performance. The authors evaluated the model against related methods using clinical databases not included in the training and demonstrated agreement with multiplexed assays of variant effect.

4.	19 Sep, [Google updated Bard](https://blog.google/products/bard/google-bard-new-features-update-sept-2023/) by adding Bard Extensions in English. With Extensions, Bard can find and show you relevant information from the Google tools you use every day — like Gmail, Docs, Drive, Google Maps, YouTube, and Google Flights and hotels — even when the information you need is across multiple apps and services. One can also use Bard’s “Google it” button to more easily double-check its answers. Other new features include  upload images with Lens, get Search images in responses, and modify Bard’s responses — to more than 40 languages.

5.	20 Sep, according to [TheNextPlatform](https://www.nextplatform.com/2023/09/20/sambanova-tackles-generative-ai-with-new-chip-and-new-approach/), the Global 50,000, national and regional governments, and large academic institutions will take a pre-trained model and tweak and tune it so it can focus on and perform generative tasks upon their internal proprietary data. SambaNova, an AI hardware start-up released SN40L, which is architected specifically to run a modest-sized model on a single device and to have lots of models running side-by-side. With a cluster of eight SN40L sockets, it can handle more than 5 trillion parameters, says Liang, one of the founders of Sambanova. So two clusters of eight machines will give users all the models might need for a Global 2000 enterprise given the logic that SambaNova is using.

6.	20 Sep, [Sequia published an article](https://www.sequoiacap.com/article/generative-ai-act-two/) “Generative AI’s Act Two”. The article argues that Generative AI has had a successful first year, with over $1 billion in revenue from startups alone. Some applications have become household names, such as ChatGPT, Midjourney, and Character. However, many AI companies do not have product-market fit or a sustainable competitive advantage, and the overall ebullience of the AI ecosystem is unsustainable. The market is now entering "Act 2," which will be from the customer-back. Act 2 will solve human problems end-to-end, with applications that are more comprehensive and user-friendly than the first apps out of the gate. The generative AI market map has been updated to reflect the evolution of the market from technology hammer to actual use cases and value, and the increasingly multimodal nature of generative AI applications. A new LLM developer stack has also been included to reflect the compute and tooling vendors that companies are turning to as they build generative AI applications in production. The article acknowledges the challenges facing generative AI, particularly in demonstrating value and retaining users. However, it remains optimistic about the market's potential and emphasizes the importance of patience, judgment, and innovation in overcoming these challenges.

7.	21 Sep, Microsoft announced the release of [Microsoft 365 Copilot](https://adoption.microsoft.com/en-us/copilot/), which combines the power of large language models (LLMs) with organization’s data – all in the flow of work – to turn words into one of the most powerful productivity tools on the planet. It works alongside popular Microsoft 365 Apps such as Word, Excel, PowerPoint, Outlook, Teams, and more. Copilot provides real-time intelligent assistance, enabling users to enhance their creativity, productivity, and skills. For example, users can use Copilot in Teams meetings to summarise key discussion points – including who said what and where people are aligned or disagree – and suggest action items, all in real-time during a meeting.

8.	21 Sep, researchers from Vanderbilt university, New York university and other institutes published a [paper](https://arxiv.org/pdf/2309.12288.pdf) ‘The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”’. They named the phenomenon Reversal Curse, i.e. if a model is trained on a sentence of the form “A is B”, it will not automatically generalize to the reverse direction “B is A”. For instance, if a model is trained on “Olaf Scholz was the ninth Chancellor of Germany”, it will not automatically be able to answer the question, “Who was the ninth Chancellor of Germany?”. Moreover, the likelihood of the correct answer (“Olaf Scholz”) will not be higher than for a random name. The researchers also found that  the Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. Models tested include GPT-4, ChatGPT and LLaMA.

9.	21 Sep, [AlpacaEval Leaderboard](https://tatsu-lab.github.io/alpaca_eval/) has updated recently. First-time GPT-4 has been over-performed by a new LLM Xwin-lm 70b v0.1. It achieved a win-rate against Davinci-003 of 95.57%, ranking as TOP-1 on AlpacaEval. It was the FIRST model surpassing GPT-4 on AlpacaEval. Also note its winrate v.s. GPT-4 is 60.61. (Note, our test of Xwin-13b seems not comparable even with LLaMA2 13B  in terms of NER extraction)

10.	21 Sep, according to TheVerge, [OpenAI released DALL-E 3](https://www.theverge.com/2023/9/20/23881241/openai-dalle-third-version-generative-ai), which now lets users use ChatGPT to create prompts and includes more safety options. By using ChatGPT, someone doesn’t have to come up with their own detailed prompt to guide DALL-E 3; they can just ask ChatGPT to come up with a prompt, and the chatbot will write out a paragraph (DALL-E works better with longer sentences) for DALL-E 3 to follow. Other users can still use their own prompts if they have specific ideas for DALL-E.



**17 Sep 2023**
1.	7 Sep, researchers from Humboldt university published [paper](https://arxiv.org/pdf/2309.03876.pdf) “OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs”. With this demonstration, researchers take a different view on biases in instruction-tuning: Rather than aiming to suppress them, they aim to make them explicit and transparent. To this end, the researchers present OpinionGPT, a web demo in which users can ask questions and select all biases they wish to investigate. The demo will answer this question using a model fine-tuned on text representing each of the selected biases, allowing side-by-side comparison. To train the underlying model, they identified 11 different biases (political, geographic, gender, age) and derived an instruction-tuning corpus in which each answer was written by members of one of these demographics. Web application available at https://opiniongpt.informatik.hu-berlin.de. 

2.	8 Sep, [accord to ts2.space](https://ts2.space/en/a-new-approach-to-regulating-ai-the-critical-algorithmic-systems-classification/),  Brookings Institution fellow and AI policy expert Alex Engler puts forward a new idea for regulating artificial intelligence (AI) in the face of an increasingly integrated technology that impacts various systems. Engler’s concept, called the Critical Algorithmic Systems Classification, aims to govern algorithms based on their specific applications and expands the authority of regulatory agencies. This approach grants agencies, such as the Department of Education, the Equal Employment Opportunity Commission, and the Department of Health and Human Services, the power to create rules for “especially impactful algorithms” and issue administrative subpoenas for algorithmic investigations. Unlike a massive overhaul of civil rights laws, which would require significant political support, the Critical Algorithmic Systems Classification offers a focused approach to governing AI’s impact on socioeconomic outcomes and health determinations. Engler’s proposal emphasizes the need to adapt existing regulatory agencies to incorporate AI expertise rather than creating a separate federal AI-focused agency. He highlights the importance of building staffing and technical expertise within these agencies to effectively regulate algorithmic systems.

3.	11 Sep, Google published a [paper](https://arxiv.org/pdf/2309.05858.pdf) “Uncovering mesa-optimization algorithms in Transformers”. The paper hypothesizes that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. Moreover, the paper suggests that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, the Google researchers propose a novel self-attention layer, the mesa-layer, that explicitly and efficiently solves optimization problems specified in context. They find that this layer can lead to improved performance in synthetic and preliminary language modeling experiments, adding weight to their hypothesis that mesa-optimization is an important operation hidden within the weights of trained Transformers.

4.	11 Sep, [Nvidia published a blog](https://developer.nvidia.com/blog/leading-mlperf-inference-v3-1-results-gh200-grace-hopper-superchip-debut/) to demonstrate the performance of its newest GH200 server, “Leading MLPerf Inference v3.1 Results with NVIDIA GH200 Grace Hopper Superchip Debut”. In its MLPerf debut, the GH200 Grace Hopper Superchip turned in exceptional performance on all workloads and scenarios in the closed division of the data center category, boosting performance by up to 17% on the NVIDIA single-chip H100 SXM submission. The NVIDIA software stack fully supports the GH200 Grace Hopper Superchip today.

5.	11 Sep, [together.ai published a blog](https://together.ai/blog/medusa) “Medusa: Simple Framework for Accelerating LLM Generation with Multiple Decoding Heads”. Instead of using an additional draft model like speculative decoding, Medusa merely introduces a few additional decoding heads, following the idea of [Stern et al. 2018] with some other ingredients. Despite its simple design, Medusa can improve the generation efficiency of LLMs by about 2x. The implementation is available at [this repo](https://github.com/FasterDecoding/Medusa).

6.	11 Sep, researchers from Microsoft published a [paper](https://arxiv.org/pdf/2309.05689.pdf#:~:text=The%20P%20vs%20NP%20problem,problem%20behind%20the%20P!%3D) “Large Language Model for Science: A Study on P vs. NP”. Researchers use large language models (LLMs) to augment and accelerate research on the P versus NP problem. Specifically, they propose Socratic reasoning, a general framework that promotes in-depth thinking with LLMs for complex problem-solving. Socratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement. The pilot study on the P vs. NP problem shows that GPT-4 successfully produces a proof schema and engages in rigorous reasoning throughout 97 dialogue turns, concluding “P ̸= NP”, which is in alignment with ([Xu and Zhou, 2023](https://arxiv.org/pdf/2302.09512.pdf)). The investigation uncovers novel insights within the extensive solution space of LLMs, shedding light on LLM for Science.

7.	11 Sep, according to [BusinessInsider](https://www.businessinsider.com/ai-builds-software-under-7-minutes-less-than-dollar-study-2023-9), Researchers in a new study tasked an AI-powered tech company with developing 70 different programs. They found AI could develop software in under seven minutes for less than $1 in costs, on average. AI bots were assigned roles and were able to talk, make logical decisions, and troubleshoot bugs. Here’s the [paper](https://arxiv.org/pdf/2307.07924v3.pdf) and [project](https://github.com/OpenBMB/ChatDev).

8.	12 Sep, CISA of America released [CISA Open Source Software Security Roadmap](https://www.cisa.gov/sites/default/files/2023-09/CISA-Open-Source-Software-Security-Roadmap-508c%20%281%29.pdf). The report states that The federal government, critical infrastructure, and state, local, tribal, and territorial (SLTT) governments greatly depend upon open source software (OSS). OSS is part of the foundation of software used across critical infrastructure, supporting every single critical infrastructure sector and every National Critical Function: one study found that 96% of studied codebases across various sectors contain open source code, and 76% of code in studied codebases was open source. The proposed roadmap centers on four key goals: 1) establishing CISA’s role in supporting the security of OSS, 2) understanding the prevalence of key open source dependencies, 3) reducing risks to the federal government, and 4) hardening the broader OSS ecosystem.

9.	12 Sep, [SCSP released special edition report](https://scsp222.substack.com/p/scsp-releases-special-edition-report?utm_source=profile&utm_medium=reader2) “Generative AI: The Future of Innovation Power”. The Generative AI moment provides the United States Government with a unique opportunity to lead with conviction as humanity enters a new era. This [Special Edition report](https://www.scsp.ai/wp-content/uploads/2023/09/GenAI-web.pdf) provides a comprehensive national security strategy informed by the generative AI models that enhance all elements of our innovation power.

10.	13 Sep, [a16z published a blog](https://a16z.com/how-are-consumers-using-generative-ai/), “How Are Consumers Using Generative AI?” The blog ranks 50 GenAI products, and finds that 1. Most leading products are built from the “ground up” around generative AI; 2. ChatGPT has a massive lead, for now… 3. LLM assistants (like ChatGPT) are dominant, but companionship and creative tools are on the rise; 4. Early “winners” have emerged, but most product categories are up for grabs; 5. Acquisition for top products is entirely organic—and consumers are willing to pay! 6. Mobile apps are still emerging as a GenAI platform.

11.	13 Sep, researchers from the National University of Singapore published a [paper](https://arxiv.org/pdf/2309.05519.pdf): “NExT-GPT: Any-to-Any Multimodal LLM”. The researchers observed that humans always perceive the world and communicate with people through various modalities, so they developed an any-to-any MM-LLM capable of accepting and delivering content in any modality – NExT-GPT. They connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. Overall, the research showcases the promising possibility of building a unified AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community.

12.	14 Sep, [according to apnews](https://apnews.com/article/schumer-artificial-intelligence-elon-musk-senate-efcfb1067d68ad2f595db7e92167943c),  The nation’s biggest technology executives on Wednesday loosely endorsed the idea of government regulations for artificial intelligence at an unusual closed-door meeting in the U.S. Senate. But there is little consensus on what regulation would look like, and the political path for legislation is difficult. Senate Majority Leader Chuck Schumer, who organized the private forum on Capitol Hill as part of a push to legislate artificial intelligence, said he asked everyone in the room — including almost two dozen tech executives, advocates and skeptics — whether government should have a role in the oversight of artificial intelligence, and “every single person raised their hands, even though they had diverse views,” he also said  regulation of artificial intelligence will be “one of the most difficult issues we can ever take on,” and he listed some of the reasons why: It’s technically complicated, it keeps changing and it “has such a wide, broad effect across the whole world”. Sarah Myers West, managing director of the nonprofit AI Now Institute, estimated that the combined net worth of the room Wednesday was $550 billion and it was “hard to envision a room like that in any way meaningfully representing the interests of the broader public.” She did not attend. At the same time during the Congress meeting, [ChatGPT was down](https://futurism.com/the-byte/chatgpt-down-sam-altman-washington) for about two hours.

13.	15 Sep, [according to CNBC](https://www.cnbc.com/2023/09/14/oracle-founder-larry-ellison-makes-first-trip-to-microsoft-campus.html), Oracle founder Larry Ellison makes first-ever trip to Microsoft headquarters for cloud announcement. According to the report, Oracle co-founder and technology chief Larry Ellison and Microsoft CEO Satya Nadella spoke at a presentation on Microsoft’s campus to announce an extension of their partnership. Ellison said Oracle hardware is coming to Microsoft’s data centers, enabling organizations that use Microsoft’s Azure cloud to draw on Oracle database services, although the two companies have gone up against each other for over three decades. “Whether it is fine-tuning a model, pre-training a model or meta-prompting a model requires that low latency access to data,” [Nadella said](https://www.crn.com/news/cloud/microsoft-ceo-nadella-calls-joint-oracle-offering-a-profound-moment-for-ai). “And so we’re very excited. I think this is the moment where data and AI coming together to transform businesses and business process – there couldn’t be a more profound timing of these two things.”

14.	15 Sep, [according to MIT Technology Review](https://www.technologyreview.com/2023/09/15/1079624/deepmind-inflection-generative-ai-whats-next-mustafa-suleyman/), DeepMind cofounder Mustafa Suleyman wants to build a chatbot that does a whole lot more than chat. he told the magazine that generative AI is just a phase. What’s next is interactive AI: bots that can carry out tasks you set for them by calling on other software and other people to get stuff done. He also calls for robust regulation—and doesn’t think that’ll be hard to achieve.
    
16.	15 Sep, [according to BusinessInsider](https://www.businessinsider.com/google-ceo-isnt-worried-about-falling-behind-on-ai-2023-9), Google CEO says he isn't worried about catching up to OpenAI after the search engine reportedly declared a 'code red': 'I feel very comfortable about where we are.' Releasing Google's AI products before ChatGPT was launched "wouldn't have worked out as well," he said. Pichai's thoughts on AI come months after Google reportedly declared a "code red" for its search engine, per NYT. 

17.	15 Sep, [according to the-decoder](https://the-decoder.com/google-begins-external-testing-of-its-gpt-4-competitor-gemini/), Google begins external testing of its GPT-4 competitor "Gemini". According to three anonymous sources from The Information, Google has given a small group of selected companies access to a stripped-down chat version of Gemini. The three sources claim to have direct knowledge of the matter. The largest version of Gemini is still being developed internally. Gemini will be offered to businesses via cloud access and integrated into Google's consumer products. Google plans to use Gemini for all of its AI applications, from the Bard chatbot to the new AI features in Workspace. Gemini, according to The Information, is "a set of large language models" that can perform various tasks such as chatbots, text summarization, code, or generating new text. It is unclear whether Gemini will rely on networked expert models, as OpenAI does with its GPT-4 architecture.

18.	16 Sep, researchers from Singapore University of Technology and Design released [TinyLlama](https://github.com/jzhang38/TinyLlama), which aims to pretrain a 1.1B Llama model on 3 trillion tokens. With some proper optimization, we can achieve this within a span of "just" 90 days using 16 A100-40G GPUs.


**11 Sep 2023**
1.	1 Sep, [Google published a paper](https://arxiv.org/pdf/2309.00267.pdf) “RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback”. RLAIF (RL from AI Feedback) is a technique where preferences are labeled by an off-the-shell LLM in lieu of humans. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ∼70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF .

2.	1 Sep, [OpenAI published a paper](https://arxiv.org/pdf/2309.00667.pdf) “Taken out of context: On measuring situational awareness in LLMs”. First, A model is situationally aware if it’s aware that it’s a model and can recognize whether it’s currently in testing or deployment. According to researchers, today’s LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests while taking harmful actions after deployment. This research moves further from an [article published by Nature](https://www.nature.com/articles/d41586-023-02684-5) “If AI becomes conscious: here’s how researchers will know” where scientists believed that Human-like behaviours can make it difficult to judge robots’ true level of engagement.

3.	4 Sep, a group of researchers from 22 universities and security institues publish a [paper](https://arxiv.org/pdf/2307.03718.pdf) "Frontier AI Regulation: Managing Emerging Risks to Public Safety". “frontier AI” models is defined as 'highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety.'  The models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model’s capabilities from proliferating broadly. To address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier AI developers, (2) registration and reporting requirements to provide regulators with visibility into frontier AI development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deployment of frontier AI models. Industry self-regulation is an important first step. However, wider societal discussions and government intervention will be needed to create standards and to ensure compliance with them. Finally, the paper proposes an initial set of safety standards include conductingpre-deployment risk assessments; external scrutiny of model behavior; using risk assessments to inform deployment decisions; and monitoring and responding to new information about model capabilities and uses post-deployment.

4.	4 Sep, [according to Itnews](https://www.itnews.com.au/news/microsoft-had-three-staff-at-australian-data-centre-campus-when-azure-went-out-599849), Microsoft had “insufficient” staff levels at its data centre campus last week when a power sag knocked its chiller plant for two data halls offline, cooking portions of its storage hardware. The company has [released a preliminary post-incident report](https://azure.status.microsoft/en-au/status/history/) (PIR) for the large-scale failure, which saw large enterprise [customers including Bank of Queensland and Jetstar completely lose service](https://www.itnews.com.au/news/bank-of-queensland-jetstar-among-australian-enterprises-impacted-by-azure-outage-599803).

5.	4 Sep, [ColosalAI released its 70B parameter](https://www.hpc-ai.tech/blog/70b-llama2-training) LLaMA2 model. The model training is accelerated by 195% with the best foundation model practice upgraded. Colossal-AI has open-sourced a full-flow solution for LLaMA2 with high scalability. This supports models ranging from 7 to 70 billion parameters, while still maintaining good performance from 8 to 512 GPUs. Users only need to upload relevant data to train personalized private models without code and can deploy the trained models with one click.

6.	6 Sep, [TII announced Falcon 180B](https://falconllm.tii.ae/falcon-180b.html). Falcon 180B is a super-powerful language model with 180 billion parameters, trained on 3.5 trillion tokens. It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use. This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.

7.	6 Sept, on Google’s 25-year anniversary, Jeff Dean, Chief Scientist of Google DeepMind and Google Research, [posted on Twitter](https://twitter.com/JeffDean/status/1699197621934366946) about his work experience at Google. Jeff said, “It has been deeply gratifying to work on incredibly exciting computer science and AI problems with amazing colleagues, & to help build out a suite of 10+ products that each have more than 1B users all over the world.”

8.	7 Sep, [Mojo is formally released](https://www.modular.com/blog/mojo-its-finally-here). According to Modular, “Mojo is a new programming language for AI developers that will grow into being a superset of Python over time. It already supports integrating with arbitrary Python code seamlessly and has a scalable programming model to target performance-critical systems, including accelerators (e.g. GPUs) that are pervasive in AI.” Mojo combines the best of dynamic and static languages together, and can achieve up to 68,000x the performance of Python today. Other features of Mojo include writing everything in one language, unlocking python performance, accessing the entire Python ecosystem, and upgrading AI workloads.

9.	7 Sep, [Google published a paper](https://arxiv.org/pdf/2309.03409.pdf) “Large Language Models As Optimizers”. In the paper, researchers propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. Experiments show that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.


**3 Sep 2023**

1.	27 Aug, [according to Techxplore](https://techxplore.com/news/2023-08-ibm-core-mixed-signal-in-memory-chip.html), IBM developed a new 64-core missed-signal in-memory computing chip to better supper support the computations of deep neural networks. The 64-core chip, presented in a paper in [Nature Electronics](https://www.nature.com/articles/s41928-023-01010-1), has so far attained highly promising results, retaining the accuracy of deep learning algorithms, while reducing computation times and energy consumption.

2.	28 Aug, [OpenAI released ChatGPT Enterprise](https://openai.com/blog/introducing-chatgpt-enterprise),which offers enterprise-grade security and privacy, unlimited higher-speed GPT-4 access, longer context windows for processing longer inputs, advanced data analysis capabilities, customization options, and much more. A new admin console lets users manage team members easily and offers domain verification, SSO, and usage insights, allowing for large-scale deployment into enterprise. See our privacy page and our Trust Portal for more details on how we treat your data. However, enterprise data still need to upload to OpenAI to remember all the data.

3.	28 Aug, according to [Semianalysis](https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini), The GPU-Rich companies have 20k+ A/H100 GPUs and are attracting top talent. The GPU-Poor startups and open-source researchers are struggling with far fewer GPUs, and spending significant time and effort attenmptging to do things that simply don’t help, or frakly matter, and an extremely counter-productive use of their skills and time. While the key here is everyone from Meta to Microsoft to startups are simply serving as a pipeline of capital to Nvidia’s bank account, Google – the most compute r rich firm in the world, is one potential savior of Nvidia slavery. Google, which has its unbeatably efficient infrastureture, has already begun training Gemini, the next generation LLM.

4.	30 Aug, Google had its [Google Cloud Next 2023](https://cloud.google.com/blog/topics/google-cloud-next/welcome-to-google-cloud-next-23). In the conference, Google announced several new projects including AI-optimized Infrastructure: The most advanced AI-optimized infrastructure for companies to train and serve models. Vertex AI: Developer tools to build models and AI-powered applications, with major advancements to Vertex AI for creating custom models and building custom Search and Conversation apps with enterprise data;  Duet AI: Duet AI is an always-on AI collaborator that is deeply integrated in Google Workspace and Google Cloud. Duet AI in Workspace gives every user a writing helper, a spreadsheet expert, a project manager, a note taker for meetings, and a creative visual designer, and is now generally available.

5.	30 Aug, [Nature published a paper](https://www.nature.com/articles/s41586-023-06419-4) by researchers from University of Zurich , “Champion-level drone racing using deep reinforcement learning”. The paper discusses First-person view (FPV) drone racing, where professional pilots navigate high-speed drones through 3D circuits while viewing the world through onboard cameras. It highlights the challenge of creating autonomous drones that can race at a professional level by relying solely on onboard sensors. The research project introduces Swift, an autonomous system that achieved the level of human world champions by combining deep reinforcement learning in simulations with real-world data. Swift competed against three human champions, winning multiple races and setting the fastest recorded race time. This achievement is seen as a significant milestone in mobile robotics and machine intelligence, with potential applications in other physical systems.

6.	30 Aug, [Cryptoslate.com reported](https://cryptoslate.com/chatgpt-drives-openai-toward-1b-revenue-goal-after-losing-540m-in-2022/) that ChatGPT drives OpenAI toward $1B revenue goal after losing $540M in 2022. The report indicated that the AI startup has reportedly seen a substantial boost in its monthly revenue to around $80 million. This marks a significant increase from its previous revenue of $28 million, which coincides with the introduction of fees for its widely-used chatbot, ChatGPT. Meanwhile, The Information reported that OpenAI lost around $540 million last year while developing GPT-4 and ChatGPT.

7.	1 Sep, [Science published paper](https://www.science.org/doi/epdf/10.1126/science.ade4401) from Google, “A principal odor map unifies diverse tasks inolfactory perception”. The paper discusses the challenge of connecting molecular structures to how we perceive odors. The authors used graph neural networks to create a Principal Odor Map (POM) that accurately represents the relationships between different odors. This POM was able to predict the quality of odors, even for ones that hadn't been previously characterized. In fact, the model's predictions were as reliable as those of a human expert. The POM outperformed other chemoinformatic models in various odor prediction tasks, demonstrating its effectiveness in encoding general structure-odor relationships. This approach has the potential to revolutionize odor prediction and could lead to the digitization of odors.



**27 Aug 2023**
1.	Meta recently released [RoboAgent]( https://robopen.github.io/) with [paper]( https://robopen.github.io/media/roboagent.pdf) “RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking”. RoboAgent can efficiently acquire a wide diversity of non-trivial skills and can generalize them to diverse unseen scenarios. Trained merely on 7500 trajectories, RoboAgent can exhibit a diverse set of 12 non-trivial manipulation skills (beyond picking/pushing, including articulated object manipulation and object re-orientation) across 38 tasks, and can generalize them to 100s of diverse unseen scenarios (involving unseen objects, unseen tasks, and to completely unseen kitchens). RoboAgent can also evolve its capabilities with new experiences.

2.	19 Aug, [according to BGR](https://bgr.com/tech/openai-may-have-to-wipe-chatgpt-and-start-over/), OpenAI, the company behind the popular generative AI tool ChatGPT, could be forced to wipe its chatbot and start over completely, according to a new report from NPR (via Ars Technica). The wipe may come as part of a potential lawsuit which could also see OpenAI fined up to $150,000 for each piece of copyrighted material used to train the language model.

3.	21 Aug, [according to The Times of Israel](https://www.timesofisrael.com/ai-likely-to-augment-rather-than-destroy-jobs-un-study-finds/), Artificial Intelligence is more likely to augment jobs than to destroy them, a UN study indicated on Monday, at a time of growing anxiety over the potential impact of the technology. Countries should therefore design policies to support an “orderly, fair and consultative” shift, the report authors said, stressing that “outcomes of the technological transition are not pre-determined.”

4.	21 Aug, another agent related [project released](https://human-world-model.github.io/) by researchers from CMU with [paper](https://arxiv.org/pdf/2308.10901.pdf) “Structured World Models from Human Videos”. Inspired by the success of learning from large-scale datasets in the fields of computer vision and natural language, researchers believe that in order to efficiently learn, a robot must be able to leverage internet-scale, human video data. Humans interact with the world in many interesting ways, which can allow a robot to not only build an understanding of useful actions and affordances but also how these actions affect the world for manipulation. The proposed approach builds a structured, human-centric action space grounded in visual affordances learned from human videos. The approach of affordance-space world models enables different robots to learn various manipulation skills in complex settings, in under 30 minutes of interaction.

5.	21 Aug, researchers from UC Berkeley published a [paper](https://arxiv.org/pdf/2308.10897.pdf) “Can Language Models Learn to Listen?”. Given a video of a listener and speaker pair, researchers extract text corresponding to the spoken words of the speaker. They fine-tune a pretrained large language model to autoregressively generate realistic 3D listener motion in response to the input transcript. The method generates semantically meaningful gestures (e.g. an appropriately timed smile inferred from “amazing”) that synchronously flow with the conversation. The model can optionally render the output of the approach as photorealistic video. [Watch this](https://www.youtube.com/watch?v=djpSOhdIU8M)

6.	21 Aug, Google published a [paper](https://arxiv.org/pdf/2308.08998.pdf) “Reinforced Self-Training (ReST) for Language Modeling”. Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. Experimental results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.

7.	21 Aug, [According to Reuters](https://www.reuters.com/legal/ai-generated-art-cannot-receive-copyrights-us-court-says-2023-08-21/), a work of art created by artificial intelligence without any human input cannot be copyrighted under U.S. law, a U.S. court in Washington, D.C., has ruled.

8.	22 Aug, [Meta released SeamlessM4T](https://about.fb.com/news/2023/08/seamlessm4t-ai-translation-model/), a Multimodal AI Model for Speech and Text Translations. SeamlessM4T is the first all-in-one multilingual multimodal AI translation and transcription model. his single model can perform speech-to-text, speech-to-speech, text-to-speech, and text-to-text translations for up to 100 languages depending on the task. [Project is here]( https://github.com/facebookresearch/seamless_communication)

9.	22 Aug, [according to Anaconda](https://www.anaconda.com/blog/announcing-python-in-excel-next-level-data-analysis-for-all), Anaconda and Microsoft announced a groundbreaking innovation: Python in Excel. This marks a transformation in how Excel users and Python practitioners approach their work. 

10.	22 Aug, [according to theVerge](https://www.theverge.com/2023/8/21/23840705/new-york-times-openai-web-crawler-ai-gpt), The New York Times blocks Open AI’s Web Crawler. No further comments from both side so far.

11.	22 Aug, a new SQLCoder project, [Defog SQLCoder](https://github.com/defog-ai/sqlcoder) was released last week. SQLCoder is a 15B parameter model that outperforms gpt-3.5-turbo for natural language to SQL generation tasks on sql-eval framework, and significantly outperforms all popular open-source models. It also significantly outperforms text-davinci-003, a model that's more than 10 times its size.

12.	22 Aug, a group of research from different institutes published a [paper](https://arxiv.org/pdf/2308.08708.pdf) “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness”. The researchers survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higherorder theories, predictive processing, and attention schema theory. From these theories, researchers derive ”indicator properties” of consciousness, elucidated in computational terms that can be used to assess AI systems for these properties. Analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.

13.	23 Aug, according to [MarketChpost](https://www.marktechpost.com/2023/08/23/ai2-unveils-dolma-a-3-trillion-token-corpus-pioneering-transparency-in-language-model-research/), AI2 open source Dolma, a 3 trillion token corpus pioneering transparency in language model research. Opacity of datasets used by big players hinders external researchers’ ability to critically analyse and enhancing existing models. Dolma, the brainchild of AI2, emerges as a beacon of openness in a landscape shrouded in secrecy. With an all-encompassing dataset spanning web content, academic literature, code, and more, Dolma strives to empower the research community by granting them the tools to build, dissect, and optimize their language models independently.

14.	24 Aug, Meta released [Code Llama](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/), Open Foundation Models for Code. The code is available from [Github]( https://github.com/facebookresearch/codellama). Code Llama is a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. Llama code include foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively.

15.	25 Aug, [Phind.com](https://www.phind.com/blog/code-llama-beats-gpt4) fined tuned CodeLlama-34B and 34B-Python models with 32 A100 GPU on an internal Phind dataset that achieved 67.6% and 69.5% pass@1 on HumanEval, respectively, while GPT-4 achieved 67%. Phind’s models are available to download from [huggingface]( https://huggingface.co/Phind/Phind-CodeLlama-34B-v1)

16.	19-25 Aug, [IJCAI 2023](https://ijcai-23.org/) was hold in Macao. Distinguished papers include “Levin Tree Search with Context Models”, “SAT-Based PAC Learning of Description Logic Concepts”, and “Safe Reinforcement Learning via Probabilistic Logic Shields”.

17.	26 Aug, [WizardLM_AI](https://twitter.com/WizardLM_AI/status/1695396881218859374) twittered that “WizardCoder-34B surpasses GPT-4, ChatGPT-3.5 and Claude-2 on HumanEval with 73.2% pass@1”. The model weights are available [here](https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0).



**20 Aug 2023**
1.	14 Aug, Meta published a [paper](https://arxiv.org/pdf/2308.06259.pdf) “Self-Alignment with Instruction Backtranslation”. The paper present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. The model, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models, demonstrating highly effective self-alignment.

2.	14 Aug, Alex Graves published a [paper](https://arxiv.org/pdf/2308.06259.pdf) “Bayesian Flow Networks”, which is [described](https://www.reddit.com/r/MachineLearning/comments/15rrljw/bayesian_flow_networks/?rdt=50628) as “Another Alex Graves paper that will take 10 years for the community to fully digest.” One of the author of the paper, Rupesh Srivastava, [summarized it](https://twitter.com/rupspace/status/1691584987148218841) as  the researchers “present a new perspective on the ideas related to diffusion models. BFNs combine Bayesian inference and neural nets to yield a model class with simple objectives that gracefully extends to discrete data.”

3.	14 Aug, Meta released a [paper](https://arxiv.org/pdf/2308.07317.pdf) “Platypus: Quick, Cheap, and Powerful Refinement of LLMs”. Platypus is a family of fine-tuned and merged Large Language Models that achieves the strongest performance and currently stands at first place in HuggingFace’s Open LLM Leaderboard as of the release date of this work. In particular, a 13B Platypus model can be trained on a single A100 GPU using 25k questions in 5 hours.

4.	14 Aug, researchers from Microsoft published [paper](https://arxiv.org/pdf/2308.06873.pdf) “SpeechX: Neural Codec Language Model as a Versatile Speech Transformer”. Researcher find existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. The introduced SpeechX is a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX achieves comparable or superior performance to specialized models across tasks.

5.	14 Aug, while some [report](https://technext24.com/2023/08/14/chatgpt-costs-700000-daily-openai/) estimated that ChatGPT costs $700,000 to run daily, OpenAI may go bankrupt in 2024; others believe that ChatGPT is probably a small profit center right now.

6.	14 Aug,  according to [BusinessInsider](https://www.businessinsider.com/ai-radically-reshape-job-market-global-economy-employee-labor-innovation-2023-8), the rise of AI is poised to disrupt the global economy, potentially eliminating millions of jobs as AI tools become more accessible and advanced. A surge in public adoption of AI is predicted to reshape industries and labor markets, with estimates indicating that over 300 million jobs could be affected globally. While AI's transformative potential could contribute trillions to the economy, it calls for urgent preparation by governments, businesses, and workers to manage the impending upheaval and ensure a smoother transition through retraining and workforce adaptation.

7.	14 Aug, according [seroundtable](https://www.seroundtable.com/microsoft-bing-chat-outperforms-gpt-4-35873.html), Microsoft Bing's CEO, Mikhail Parakhin said on Twitter that Bing Chat outperforms raw GPT-4 but it comes at an expense. He said, "It is outperforming according to our measurements," when someone said they think Bing Chat beats out GPT-4 from OpenAI. Keep in mind, Bing uses GPT-4 from OpenAI, but as Mikhail Parakhin explained and as many of you already know, "Bing is using retrieval-augmented inference." He adds that doing all this is much more expensive, he said "Yes, we have that offering, it ends up pretty pricey, as it requires multiple model calls + Search calls, so far it’s been only used by a few companies."

8.	15 Aug, according to [Businessinsider](https://www.businessinsider.com/chatgpt-isnt-good-enough-to-take-jobs-unlikely-mass-layoffs-2023-8), Despite initial fears of AI causing mass layoffs, the reality is that AI, exemplified by ChatGPT, hasn't proven itself capable enough to replace most jobs. The anticipated automation wave has not led to widespread unemployment; the adoption of AI tools has been widely embraced, but the impact on jobs has been less dramatic than predicted. Many companies find AI useful for specific tasks but not advanced enough to handle the complexity of human roles. While some jobs will be affected, the complexities of tasks and the need for human supervision are hindrances to complete automation. The transition to AI-driven work is intricate, and the anticipated AI-driven unemployment has not materialized as predicted.

9.	16 Aug, [OpanAI added a new content moderation feature](https://openai.com/blog/using-gpt-4-for-content-moderation#LilianWeng). Content moderation demands meticulous effort, sensitivity, a profound understanding of context, as well as quick adaptation to new use cases, making it both time consuming and challenging. The new feature makes the process of developing and customizing content policies is trimmed down from months to hours. 1) Once a policy guideline is written, policy experts can create a golden set of data by identifying a small number of examples and assigning them labels according to the policy.  2)Then, GPT-4 reads the policy and assigns labels to the same dataset, without seeing the answers. 3) By examining the discrepancies between GPT-4’s judgments and those of a human, the policy experts can ask GPT-4 to come up with reasoning behind its labels, analyze the ambiguity in policy definitions, resolve confusion and provide further clarification in the policy accordingly. Repeat steps 2 and 3 until it is satisfied with the policy quality. This iterative process yields refined content policies that are translated into classifiers, enabling the deployment of the policy and content moderation at scale.

10.	16 Aug, according to [Venturebeat](https://venturebeat.com/ai/consulting-giant-mckinsey-unveils-its-own-generative-ai-tool-for-employees-lilli/), McKinsey is debuting a gen AI tool of its own: Lilli, a new chat application for employees. The tool serves up information, insights, data, plans, and even recommends the most applicable internal experts for consulting projects, all based on more than 100,000 documents and interview transcripts. The interface will look familiar to those who have used other public-facing text-to-text based gen AI tools such as OpenAI’s ChatGPT and Anthropic’s Claude 2. Lilli leverages currently available LLMs, including those developed by McKinsey partner Cohere as well as OpenAI on the Microsoft Azure platform, to inform its GenAI Chat and natural language processing (NLP) capabilities.

11.	16 Aug, as [reported by Independent](https://www.independent.co.uk/tech/google-quantum-computer-apocalypse-encryption-password-security-b2393516.html), Google is taking steps to address the potential security threat posed by quantum computers, known as the "quantum apocalypse." Quantum computers have the capability to break current encryption methods that protect sensitive data. To counter this, Google has integrated a hybrid cryptographic algorithm, X25519Kyber768, into Chrome to resist attacks from future quantum computers. While quantum computers capable of breaking current encryption are projected to be years away, Google's move is aimed at securing data now to prevent it from being intercepted and decrypted in the future.

12.	16 Aug, according to [BusinessInsider](https://www.businessinsider.com/databricks-ali-ghodsi-unqualified-ceos-dont-know-ai-data-2023-8), Databricks’ CEO Ali Ghodsi said that “I run a $38 billion software company. In 10 years, CEOs who don't understand data and AI won't be eligible for the top job in any industry.” As generative AI becomes more ubiquitous, Ghodsi says companies that prioritize data will have an advantage.

13.	16 Aug, Lamini, an AI company, [released a blog](https://www.lamini.ai/blog/one-billion-times-faster-finetuning-with-lamini-peft) “One Billion Times Faster Finetuning with Lamini PEFT”, in brief, Parameter-efficient finetuning (PEFT) makes it one billion times faster to scale unlimited LLM variations for specialized tasks. Chain and switch between thousands of finetuned LLMs - from months down to just milliseconds.

14.	16 Aug, according to [theVerge](https://www.theverge.com/2023/8/15/23833045/google-artificial-intelligence-summary-chrome-sge), Google’s AI-powered article summaries are rolling out for iOS and Android first, before coming to Chrome on the desktop. The Search Generative Experience (SGE) project will be able to summarize articles users are reading on the web, according to a Google blog post. SGE can already summarize search results so that users don’t have to scroll forever to find what they are looking for, and this new feature is designed to take that further by helping users out after they are actually clicked a link.

15.	17 Aug, according to [financial times](https://www.ft.com/content/ed323f48-fe86-4d22-8151-eed15581c337), The hype around generative AI is met with skepticism as doubts arise about its effectiveness and practical applications. Technologist Gary Marcus questions the technology's reliability, noting its tendency to produce inaccurate content. Concerns also center on generative AI's impact on future training data, potentially leading to misinformation. While investors argue for its value as a productivity tool and problem solver, doubts persist about its true potential. Amid the uncertainty, cloud computing providers and chip manufacturers benefit, while the longevity of generative AI's impact remains uncertain.

16.	19 Aug, according to [gizmodo](https://gizmodo.com.au/2023/08/metas-next-big-open-source-ai-dump-will-reportedly-be-a-code-generating-bot/), Meta may release a Code-generation bot as soon as next week. The reporter from Information who spoke to two anonymous sources with direct knowledge of the AI, this new model dubbed “Code Llama” will be open source and available free online. This is consistent with the company’s strategy so far of releasing widely available AI software that makes developing new customizable AI models much easier for companies who don’t want to pay OpenAI or others for the privilege.



**14 Aug 2023**

1.	7 Aug, [according to searchenginejournal](https://www.searchenginejournal.com/openai-launches-gptbot-how-to-restrict-access/493394/#close), OpenAI launches GPTBot, a web crawler which will be used the company to improve its AI models; it also has instructions on how to restrict or limit its access. The news has sparked a debate on Hacker News around the ethics and legality of using scraped web data to train proprietary AI systems. Some think OpenAI, like other person learning from online content, can freely use public web data; others believe if OpenAI use the data to make profit, the profit should be shared among the data owners.
2.	7 Aug, Anthropic.AI published a [paper](https://arxiv.org/pdf/2308.03296.pdf) “Studying Large Language Model Generalization with Influence Functions”. The paper argues that LLMs are difficult to understand, but influence functions can help. Researchers developed a method for scaling influence functions to large models and found that: LLMs do not simply memorize training sequences; Larger models generalize better; Influence is evenly distributed, but different layers exhibit distinct patterns; LLMs are sensitive to word order; Role-playing behavior is primarily driven by imitation. These findings suggest that LLMs are more complex and nuanced than previously thought.
3.	6-9 Aug, [SIGGRAPH](https://s2023.siggraph.org/) was holding  at Los Angeles. This is its 50th annual conference covering topics such as production & animation, research & education, arts & design, gaming & interactive and others. Jensen Huang delivered a keynote speech. He introduced Grace Hopper, a new super chip for AI, as computing’s “killer app”. The chip is ten times faster, much cheaper and has lower power consumptions. He also repeated his famous word: the more you buy, the more you save.
4.	7 Aug, researcher from DeepMind published a [paper](https://arxiv.org/pdf/2308.03958.pdf) “Simple synthetic data reduces sycophancy in large language models”. Sycophancy is an undesirable behavior where models tailor their responses to follow a human user’s view even when that view is not objectively correct (e.g., adapting liberal views once a user reveals that they are liberal). To address the issue, researchers present a straightforward synthetic-data intervention that takes public NLP tasks and encourages models to be robust to user opinions on these tasks. Adding these data in a lightweight finetuning step can significantly reduce sycophantic behavior on held-out prompts.
5.	8 Aug, [according to BlackBerry](https://blogs.blackberry.com/en/2023/08/why-companies-ban-chatgpt-ai), 75% of organizations worldwide set to ban chatGPT and generative AI apps on work devices. The data is based on a BlackBerry survey of 2,000 IT decision-makers across the US, Canada, the UK, France, Germany, the Netherlands, Japan, and Australia. Potential risk to data security and privacy is the biggest reason (67%) survey respondents cited for moving to block ChatGPT and similar generative AI tools. The next greatest concern (57%) is risk to corporate reputation.  
6.	8 Aug, [Stability.AI announced StableCode](https://stability.ai/blog/stablecode-llm-generative-ai-coding), according to Stability.AI, StableCode offers a unique way for developers to become more efficient by using three different models to help in their coding. The base model was first trained on a diverse set of programming languages from the stack-dataset (v1.2) from BigCode and then trained further with popular languages like Python, Go, Java, Javascript, C, markdown and C++.  In total, we trained our models on 560B tokens of code on our HPC cluster.
7.	9 Aug, [IBM is planning](https://newsroom.ibm.com/2023-08-09-IBM-Plans-to-Make-Llama-2-Available-within-its-Watsonx-AI-and-Data-Platform) to host Meta's Llama 2-chat 70 billion parameter model in the watsonx.ai studio, with early access now available to select clients and partners. This will build on IBM's collaboration with Meta on open innovation for AI, including work with open source projects developed by Meta – such as the PyTorch machine learning framework and the Presto query engine used in watsonx.data.
8.	9 Aug, [Biden-Harris Administration launches](https://www.whitehouse.gov/briefing-room/statements-releases/2023/08/09/biden-harris-administration-launches-artificial-intelligence-cyber-challenge-to-protect-americas-critical-software/) AI cyber challenge, together with Microsoft, OpenAI, Google and Anthropic, to pretect America’s critical software. DARPA will host an open competition in which the competitor that best secures vital software will win millions of dollars in prizes. AI companies will make their cutting-edge technology—some of the most powerful AI systems in the world—available for competitors to use in designing new cybersecurity solutions.
9.	10 Aug, Stanford researchers open source the famous [“Stanford Smallville”](https://github.com/joonspk-research/generative_agents). In their [paper](arxiv.org/abs/2304.03442), 25 AI agents inhabit a digital Westworld, unaware that they are living in a simulation. They go to work, gossip, organize socials, make new friends, and even fall in love. Each has unique personality and backstory. The github repository contains the core simulation module for generative agents—computational agents that simulate believable human behaviors—and their game environment. 
10.	10 Aug, Researchers from Google recently [published a blog](https://pair.withgoogle.com/explorables/grokking/#:~:text=In%202021%2C%20researchers%20made%20a,after%20training%20for%20much%20longer.) “Do Machine Learning Models Memorize or Generalize?”. As we know, we train a ML model to generalize rather than memorize. In the blog, the authors use the term grokking to describe the phenomenon where generalization seems to happen abruptly and long after fitting the training data. While it isn’t yet clear how to apply these techniques to today’s largest models, starting small makes it easier to develop intuitions as we progress towards answering these critical questions about large language models.
11.	10 Aug, a [paper](https://arxiv.org/pdf/2308.03762.pdf) with title “GPT-4 Can’t Reason” raised [interesting discussions](https://news.ycombinator.com/item?id=37050257) on prompting engineering when using ChatGPT-like models, including GPT-4. Some argued that the author of the paper may have used an outdated version of GPT-4 , or have been using the wrong prompt for GPT-4. There are many different ways to prompt engineer GPT-4, and the best approach depends on the specific task at hand; others believe prompting engineering is cheating, but others argued that it is simply a way to help GPT-4  to perform better.
12.	11 Aug, researchers from Microsoft and Uni. Of Southern California released [UniversalNER](https://arxiv.org/abs/2308.03279). The researchers propose a general recipe for targeted distilling where they train student models using mission-focused instruction tuning for a broad application class such as open information extraction. Researchers show that this can maximally replicate LLM’s capabilities for the given application class, while preserving its generalizability across semantic types and domains. Using NER as a case study, they successfully distill these capabilities from LLMs into a much smaller model UniversalNER that can recognize diverse types of entities or concepts in text corpora from a wide range of domains. UniversalNER surpasses existing instruction-tuned models at the same size (e.g., Alpaca, Vicuna) by a large margin, and shows substantially better performance to ChatGPT.



**6 Aug 2023**
1.	Kaiming He, [announced that](https://kaiminghe.github.io/) he will join Department of Electrical Engineering and Computer Science at MIT in 2024. His total Google Scholar citations exceed 460,000 + times, and he will become the first at MIT. Previously, he worked at FAIR. His research results [ResNet](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) is one of the cornerstone papers in the field of Deep learning.
2.	Recently, Stanford Uni published a [paper](https://arxiv.org/pdf/2307.15189.pdf): “Med-Flamingo: a multimodal medical few-shot learner”. Based on OpenFlamingo-9B, researchers continue pre-training on paired and interleaved medical image-text data from publications. Med-Flamingo improves performance in generative medical VQA by up to 20% in clinician’s rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation and textbooks.
3.	1 Aug, according to [Search Engine Journal](https://www.searchenginejournal.com/openai-files-trademark-application-gpt-5/493040/#close), OpenAI has filed a trademark application for “GPT-5”. The application covers a wide range of software related to language models and AI, such as downloadable computer programs and software related to language models, artificial production of human speech and text, natural language processing, generation, understanding, and analysis. However, as Sam Altman said recently, “We have a lot of work to do before GPT 5. It takes a lot of time for it. We are not certainly close to it. There needs to be more safety audits. I wish I could tell you about the timeline of the next GPT”.
4.	1 Aug, [LLM-UTILS published](https://gpus.llm-utils.org/nvidia-h100-gpus-supply-and-demand/) a blog discussing the supply and demand of GPUs. The blog commented that as of August 2023, it seems AI might be bottlenecked by the supply of GPUs. This comment is supported by Sam Altman, he said that OpenAI is GPU-limited and it’s delaying their short term plans (fine-tuning, dedicated capacity, 32k context windows, multimodality). Elon Must also said that “GPUs are at this point considerably harder to get than drugs.” The blog further discussed what do we want to know about the bottleneck, including what’s causing it, how long will it last, and wha’t going to help resolved it.
5.	1 Aug, researchers from Google and Uni of Washington [published a paper](https://arxiv.org/pdf/2308.00675.pdf) “Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models”. Instead of providing a few demonstrations of the tool’s usage, the paper provides an alternative to demonstrations: tool documentation, descriptions of the individual tool usage. Experimental results demonstrate the zero-shot documentation is on par or better than few-shot without documentation.
6.	2 Aug, Google published a [paper on Nature](https://www.nature.com/articles/s41586-023-06221-2): “Scientific discovery in the age of artificial intelligence”. The paper indicates that AI is being increasingly integrated into scientific discovery to augment and accelerate research, helping scientists to generate hypotheses, design experiments, collect and interpret large datasets, and gain insights that might not have been possible using traditional scientific methods alone. Researchers then examine breakthroughs over the past decade that include self-supervised learning, which allows models to be trained on vast amounts of unlabelled data, and geometric deep learning, which leverages knowledge about the structure of scientific data to enhance model accuracy and efficiency. The paper further suggests that both developers and users of AI tools need a better understanding of when such approaches need improvement, and challenges posed by poor data quality and stewardship remain.
7.	2 Aug, Meta released its MusicGen project, [Audiocraft](https://github.com/facebookresearch/audiocraft). Audiocraft is a library for audio processing and generation with deep learning. It features the state-of-the-art EnCodec audio compressor / tokenizer, along with MusicGen, a simple and controllable music generation LM with textual and melodic conditioning.
8.	Mckinsey [released a survey](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2023-generative-ais-breakout-year?cid=aisurge2023-soc--mar-mar--07/23-i1a--bam-ip&linkId=227872978#/) titled “The state of AI in 2023: Generative AI’s breakout year”. The survey confirms the explosive growth of generative AI (gen AI) tools. Less than a year after many of these tools debuted, one-third of the survey respondents say their organizations are using gen AI regularly in at least one business function. The organizations that have already embedded AI capabilities have been the first to explore gen AI’s potential, and those seeing the most value from more traditional AI capabilities—a group we call AI high performers—are already outpacing others in their adoption of gen AI tools.
9.	3 Aug, [IBM reported](https://research.ibm.com/blog/nasa-hugging-face-ibm) that IBM and NASA open source the largest geospatial AI foundation model on [Hugging Face](https://huggingface.co/ibm-nasa-geospatial). The move aims to widen access to NASA satellite data (250,000 terabytes) and accelerate climate-related discoveries by using a model built by IBM.
10.	3 Aug, Microsoft announced [project Rumi](https://www.maginative.com/article/project-rumi-augmenting-ai-understanding-through-multimodal-paralinguistic-prompting/), which aims to incorporate paralinguistic input, such as intonation, gestures, and facial expressions, into prompt-based interactions with LLMs. Rumi leverages separately trained vision and audio-based models to assess sentiment from cognitive and physiological data in real-time. The system extracts non-verbal cues from video and voice inputs in real-time, creating paralinguistic tokens that augment the standard lexical input to existing LLMs like GPT4. Microsoft's future plans for Project Rumi include improving the performance of existing models and incorporating additional signals, like heart rate variability derived from standard video, cognitive, and ambient sensing. These explorations paint a picture of a more dynamic, sensitive AI capable of understanding the complexity of human interaction.



**30 July 2023**
1.	19 Jul,  a [paper](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2307.09793.pdf&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=Nx2s8Qs1wXqcvi%2FYaGv9zBXTGlC91BSUj1ZalKGG2XM%3D&reserved=0) published by Stanford Uni. revealed that to date, nearly 16K (15,821) Text Generation models have been uploaded to Hugging Face. 
2.	22 Jul, [according to Reuters](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.reuters.com%2Ftechnology%2Fopenai-google-others-pledge-watermark-ai-content-safety-white-house-2023-07-21%2F&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=giSWo1jnuNaFyZJaTQQaMQd%2FvvFAm5I%2FwB%2BBEsl%2FYPI%3D&reserved=0), AI companies including OpenAI, Alphabet and Meta Platforms have made voluntary commitments to the White House to implement measures such as watermarking AI-generated content to help make the technology safer, President Joe Biden announced on Friday. The companies - which also include Anthropic, Inflection, Amazon.com and OpenAI partner Microsoft - pledged to thoroughly test systems before releasing them and share information about how to reduce risks and invest in cybersecurity.
3.	23-29 Jul. [ICML 2023](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Ficml.cc%2F&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=XrTqRYqIURwmY7HxqnXXA6lzFD%2FqcW6LcIxzIC0z7Wg%3D&reserved=0). All published paper titles can be found [here](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Ficml.cc%2Fvirtual%2F2023%2Fpapers.html%3Ffilter%3Dtitles&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=%2F9VBhsYbaROsWyY%2FkPkYLhk9GAegRhJ93r5tOtrv1RY%3D&reserved=0), rewards paper titles can be found [here](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Ficml.cc%2Fvirtual%2F2023%2Fawards_detail&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=uiK98xlINGLHXKrMiO%2BPDy6s%2FGQtMio7m%2Bz3V2T7RIA%3D&reserved=0). Need register to access the papers
4.	24th Jul, Evan Miller [published a blog](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.evanmiller.org%2Fattention-is-off-by-one.html&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=YyFOS2S4xQfttpjfMQUHsHTdPLzov10A%2B6MOmVROhyA%3D&reserved=0), “Attention is Off By One”. Evan argues that the softmax formula applied on the Q multiply transpose of V divided by square-root of d should be fixed by adding one to the denominator, which is called the softmar1. He named the new attention as QuietAttention because it allows one attention head to simply “pass” rather than must add something to the output vector. The proposed QuietAttention is expected to alleviate outlier issues of attention which occur in white space and punctuation positions.
5.	24 Jul, researchers from UCLA, IBM, et al. published a paper “Injecting the 3D World into Large Language Models”, In [this work](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fvis-www.cs.umass.edu%2F3dllm%2F&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=Nu%2FZSEooXdXLagj3jfeGWuYPdZR9hEVtoEIY2eEglcg%3D&reserved=0), researchers propose to inject the 3D world into large language models, and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on.
6.	25 Jul, [it is reported](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwindowsreport.com%2Fg3po-ai%2F&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=E2MyXhwjgsEjEm%2BFVA55LR2HxLPg76%2B%2BuLPVWSt9WrI%3D&reserved=0) that OpenAI is planning to release its own open-source language model, codenamed G3PO, to response Meta’s LLaMA2 and other open-source LLMs. While the project has a codename, unfortunately, it doesn’t have a release date for now. It seems that OpenAI might be thinking about other endeavors, such as its superlignment project and to achieve AGI in 4 years from now.
7.	26 Jul, Anthropic, Google, Microsoft, and OpenAI are launching the [Frontier Model Forum](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fopenai.com%2Fblog%2Ffrontier-model-forum&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=c7PK6qaqhp%2BJ5jBfU4RI6GkzZ1x0an%2FV0BQZYtRwwpk%3D&reserved=0), an industry body focused on ensuring safe and responsible development of frontier AI models. The Forum aims to help (i) advance AI safety research to promote responsible development of frontier models and minimize potential risks, (ii) identify safety best practices for frontier models, (iii) share knowledge with policymakers, academics, civil society and others to advance responsible AI development; and (iv) support efforts to leverage AI to address society’s biggest challenges.
8.	26 Jul. [Satbility.AI released SDXL 1.0](https://stability.ai/blog/stable-diffusion-sdxl-1-announcement), the next iteration in the evolution of text-to-image generation models. Following the limited, research-only release of SDXL 0.9, the full version of SDXL has been improved to be the world's best open image generation model. SDXL generates images of high quality in virtually any art style and is the best open model for photorealism. Distinct images can be prompted without having any particular ‘feel’ imparted by the model, ensuring absolute freedom of style. SDXL 1.0 is particularly well-tuned for vibrant and accurate colors, with better contrast, lighting, and shadows than its predecessor, all in native 1024x1024 resolution.
9.	26 Jul, [according to itnews](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.itnews.com.au%2Fnews%2Fgoogle-handed-user-data-to-aus-authorities-5525-times-last-year-598413&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=cEb%2FmmzC9rUucXZC7SUsdBFZdAfxWcw7DirS%2FpxcJD8%3D&reserved=0), Google complied with 5525 of 6335 requests from Australian authorities to disclose user information relating to 7183 accounts last year. Meta and TikTok’s transparency reports are the same; although Meta revealed that it complied with 3563 Australian agencies' requests last year and TikTok complied with 91, during the period - little is known about who made them.
10.	26 Jul, [according to TheVerge](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.theverge.com%2F2023%2F7%2F26%2F23808274%2Fmeta-microsoft-amazon-overture-open-source-mapping&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=gdCngplXQBJ1%2BShnqqN8%2B6b76TlHkCtKFwbmPLxyRGM%3D&reserved=0), Meta, Microsoft, Amazon, and the mapping company TomTom have launched an initiative to take on Google Maps and Apple Maps. The four companies formed the Overture Maps Foundation last year with the goal of creating interoperable map products — and now, the group has released its first open map dataset. With this data, third-party developers can create global mapping or navigation products of their own, allowing them to go head-to-head with Google Maps and Apple Maps. According to Overture, the release includes over 59 million places of interest, along with data on buildings, transportation networks, and administrative boundaries.
11.	27 Jul, [according to WashingtonPost](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.washingtonpost.com%2Ftechnology%2F2023%2F07%2F27%2Fsocial-media-research-meta-political-views%2F&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=oJGb8dTUulwof%2FnAzqzPS2LQC9NqlAWcbXGzGcm2a3I%3D&reserved=0), the massive study of Facebook and Instagram shows that changing Facebook’s algorithm won’t fix polarization. “Despite the fact that we find this big impact in people’s on-platform experience, we find very little impact in changes to people’s attitudes about politics and even people’s self-reported participation around politics.” Meta’s research results are published on both [Science](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.science.org%2Fcontent%2Farticle%2Fdoes-social-media-polarize-voters-unprecedented-experiments-facebook-users-reveal&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=u8cip2sJh%2FkcYHZjJVA5c3n2jLsP09Y2VaXDLeNoNAg%3D&reserved=0) and [Nature](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-023-02420-z&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=hmIRqOexb7yzCiC9QcC2St65C%2FvedNt1YUrG8S%2FkjS0%3D&reserved=0).
12.	27 Jul, [according to NY Times](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nytimes.com%2F2023%2F07%2F27%2Fbusiness%2Fai-chatgpt-safety-research.html&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=JlqhfMxkj1AsaGePUhVBjWp%2Fm%2Bllus4xBsbaaqBwg6I%3D&reserved=0), researchers from CMU published a [paper](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2307.15043.pdf&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=euiTfl%2BQXX1l0M3o%2BRZLw4WtJVM8%2BrsurtE3vrTGy6I%3D&reserved=0) that showed how anyone could circumvent A.I. safety measures and use any of the leading chatbots to generate nearly unlimited amounts of harmful information. The researchers found that they could break through the guardrails of open source systems by appending a long suffix of characters onto each English-language prompt fed into the system.
13.	27 Jul, a16z proposed [Money on Autopilot](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fa16z.com%2F2023%2F07%2F27%2Fmoney-on-autopilot-ai-personal-finance%2F&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=W9W%2Be9%2B3WTN8p9MbNQ6QdeDQDfNPBOKjGGcBSk%2BdHXA%3D&reserved=0), the much-discussed topic of “self-driving money” finally has a chance to achieve its potential. Post-generative AI, we’re in a new world for consumer financial platforms. LLMs, and specifically multi-modal prompts like GPT-4, can process and output both text and images. This enables consumer robot process automation (RPA), which will allow fintech apps to operate on a user’s behalf.
14.	28 Jul, [according to BusinessInsider](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.businessinsider.com%2Fopenai-cant-identify-ai-generated-text-bad-for-internet-models-2023-7&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=bvKFq0gVx8RTcgLb%2F1rg1ZxpDK3kdb%2BCoiHFJYZqbcw%3D&reserved=0), OpenAI just admitted it can't identify AI-generated text. "As of July 20, 2023, the AI classifier is no longer available due to its low rate of accuracy,"  OpenAI wrote in a [recent blog](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fopenai.com%2Fblog%2Fnew-ai-classifier-for-indicating-ai-written-text&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=KDAjgPC4CaJCEpESJrsk0yfydHaJj8mH0EeSCCaHZGQ%3D&reserved=0). "We are working to incorporate feedback and are currently researching more effective provenance techniques for text." If tech companies use AI-produced data inadvertently to train new models, some researchers worry those models will get worse.
15.	28 Jul, Google released its [Robotics-transformer2 project](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Frobotics-transformer2.github.io%2F&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=cylTIuY6WempGlk%2FYksJRS7Vs2uWP6KfMV2l%2FWK3d5s%3D&reserved=0) and a [paper](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Frobotics-transformer2.github.io%2Fassets%2Frt2.pdf&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=NZdn3QAulqGaV2RKVjSGziBix%2Bd8Pja39J7RfiIURzI%3D&reserved=0) “RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control”. The vision-language-action models (VLA) express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. extensive evaluation (6k evaluation trials) shows that the approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). Researchers further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).



**23 July 2023**
1.	A recent [report from OpenUK](https://openuk.uk/wp-content/uploads/2023/07/FINAL-State-of-Open-The-UK-in-2023-Phase-Two-Part-1.pdf) indicates that in 2022, the Gross Value added to the UK economy from Open Source Software is estimated to be £13.59 billion. So what does that mean? Contextualising it with the UK Tech Sector contribution at £50 billion in 2022, the directly attributable contribution from Open Source Software is therefore 27%, more than a quarter of the overall Tech Sector contribution.
2.	17 Jul, Microsoft published a [paper](https://arxiv.org/abs/2307.08621): “Retentive Network: A Successor to Transformer for Large Language Models”. RetNet achieves low-cost inference (i.e., GPU memory, throughput, and latency), training parallelism, and favorable scaling curves compared with Transformer. RetNet makes the “impossible triangle” possible, which achieves training parallelism, good performance, and low inference cost simultaneously. The code will be released within two weeks.
3.	17 Jul, [HPC-AI released its 65 billion](https://www.hpc-ai.tech/blog/large-model-pretraining) parameter large language model. It utilizes the current most widely used large model, LLaMA, to provide an example of the tool’s groundbreaking pre-training solutions for the 65 billion parameter large model which improves the training speed by 38%. This can save enormous amounts for large model enterprises. HPC-AI only needs 32 A100/A800 GPUs to improve pre-training speed by 38% compared to other mainstream options in the industry.
4.	18 Jul, Dao published a [paper](https://tridao.me/publications/flash2/flash2.pdf), FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. FlashAttention2 (1) tweaks the algorithm to reduce the number of non-matmul FLOPs (2) parallelizes the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distributes the work between warps to reduce communication through shared memory. These yield around2× speedup compared to FlashAttention, reaching 50-73% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations.
5.	19 Jul, Meta [released LLaMA v2](https://download.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiI0o%7EP01cdTAwMWUjPyIsIlJlc291cmNlIjoiaHR0cHM6XC9cL2Rvd25sb2FkLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODk4MTkzMzR9fX1dfQ__&Signature=CgIvdOxnfMPih1sAr%7EkkhBGpl0PBj9n9rebxjFEbb6u-aVLYyV1kK2le4B0bSt3jZ9gFFc2pf5XTG%7EauFCE4lo7FSiQ7VRRKIEjpTx9QtGzO%7EvquyTb3sON6YdEtLi4fu%7EBVYk58ce%7E5nK%7EkXvqsuld5zA%7EPaLKB-WXjd-6DwmqUjphv4k6v0Lb7jH0V%7EgCzK3JFuzUwrCUPXInbtDinMnCUPOFu4TW-%7E9QlZVPUaDfqlRvWExDcULe3akb2KEGif37U-P3fLTmrGdIuTSflvqJrUdM33FwbMd2a7s2TSIUcLBD%7EUqvyi43Drx2Cr8zzSFAOVYovwMQMYxUtUGrphQ__&Key-Pair-Id=K15QRJLYKIFSLZ), a series of large language models trained on 40% more data (2 Trillion tokens) than Llama 1, and has doubled the context length to 4096. Meta also provided finetuned dialog models with over 100k samples. Llama 2 outperforms other open source language models on many external benchmarks, including reasoning, coding, proficiency, and knowledge tests.
6.	19 Jul [Reuters reported](https://www.reuters.com/technology/apple-tests-generative-ai-tools-rival-openais-chatgpt-bloomberg-news-2023-07-19/) that Apple is working on AI offering similar to OpenAI’s ChatGPT and Google’s Bard, causing its shares up as much as 2% to a record high. Apple's new virtual assistant summarizes text and answers questions based on data it has been trained with, and the tool essentially replicates Bard, ChatGPT and Bing AI, and works as a web application, according to employees of Apple.
7.	19 Jul, researcher from UCL, EleutherAI, Meta, StabilityAI and others published a [paper](https://arxiv.org/pdf/2307.10169.pdf) “Challenges and Applications of Large Language Models”. The authors believed that due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. They explored the challenges of LLMs from three views: Designing LLMs relates to decisions taken before deployment. Behaviorial challenges occur during deployment. Science challenges hinder academic progress.
8.	20 Jul, Nature published a [paper](https://www.nature.com/articles/d41586-023-02317-x) “How to introduce quantum computers without slowing economic growth”. The researchers 
believe that new ways of simulating materials, optimizing processes and improving machine learning — could transform society. They also suggested that Specialists should work together to create narratives around the usefulness of quantum technologies; however, the technology bottlenecks for quantum computing are unclear, and Would these benefits lead to more products and services that are better tailored to customer needs? What would the impacts be on the wider industrial landscape, and what new business models might emerge?
9.	21 Jul, [StabilityAI released FreeWilly](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models), the large and mighty instruction finetuned open access language models (FreeWilly1 and FreeWilly2). FreeWilly1 leverages the original LLaMA 65B foundation model and was carefully fine-tuned with a new synthetically-generated dataset using Supervised Fine-Tune (SFT) in standard Alpaca format. Similarly, FreeWilly2 leverages the LLaMA 2 70B foundation model to reach a performance that compares favorably with GPT-3.5 for some tasks.
10.	21 Jul, [according to HDTECH](https://tech.hindustantimes.com/tech/news/sergey-brin-returns-to-google-to-work-on-secret-ai-project-gemini-71689927230676.html), Google’s cofounder Sergey Brin returns to Google to work on the secret AI project Gemini, the company’s highly ambitious general-purpose AI project. Gemini would be a multi-modal foundational model that powers other AI models but no further details are known at the moment.


**16 July 2023**
1. Jul 9 – 14, [ACL 2023](https://2023.aclweb.org/), Annual Meeting of the Association for Computational Linguistics (ACL), which is one of the top natural language processing conferences in the world, was held in Toronto, Canada.
2. Jul 11, Anthropic AI released [Claude 2](https://www.anthropic.com/index/claude-2), Claude 2 has improved performance, longer responses, and can be accessed via API as well as a new public-facing beta website, claude.ai. The latest model scored 76.5% on the multiple choice section of the Bar exam, up from 73.0% with Claude 1.3.
3. Jul 11, according to [Bloomberg](https://www.bloomberg.com/news/articles/2023-07-11/ai-researcher-who-helped-write-landmark-paper-is-leaving-google), AI Researcher Who Helped Write Landmark Paper Is Leaving Google, and the last one who is still working for Google, is departing Google, and will start a company after taking time off.
4. Jul 12, Elon Musk announced [xAI](https://x.ai/), is to understand the true nature of the universe. The new team members have previously worked at DeepMind, OpenAI, Google Research, Microsoft Research, Tesla, and the University of Toronto. Collectively the team contributed some of the most widely used methods in the field, in particular the Adam optimizer, Batch Normalization, Layer Normalization, and the discovery of adversarial examples. xAI aims at implement AGI by the end of 2029, the due date.
5. Jul 12, [businessinsider reported](https://www.businessinsider.com/ai-could-run-out-text-train-chatbots-chatgpt-llm-2023-7) that UC Berkly prof. Stuart Russell warned that AI developers are "running out of text" to train chatbots at a UN summit, and AI's strategy behind training large language models is "starting to hit a brick wall." It also reported that a group of AI researchers, estimated that machine learning datasets will likely deplete all "high-quality language data" before 2026.
6. Jul 12, Google published a [paper on Nature](https://www.nature.com/articles/s41586-023-06291-2) “Large language models encode clinical knowledge”. The paper proposes a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA3, MedMCQA4, PubMedQA5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics6), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%.
7. Jul 12, Nature published a [paper](https://www.nature.com/articles/s41586-023-06095-4) “Quantum-enhanced Markov chain Monte Carlo”. Researchers have developed a quantum algorithm that can sample from complicated distributions, such as MCMC, arising in several applications. This algorithm is well-suited to current hardware and could ease computational bottlenecks in machine learning, statistical physics, and optimization.
8. Jul 13, [Hashingtonpost reported](https://www.washingtonpost.com/technology/2023/07/13/ftc-openai-chatgpt-sam-altman-lina-khan/) that the Federal Trade Commission has opened an expansive investigation into OpenAI, probing whether the maker of the popular ChatGPT bot has run afoul of consumer protection laws by putting personal reputations and data at risk. The FTC’s demands of OpenAI are the first indication of how it intends to enforce those warnings. If the FTC finds that a company violates consumer protection laws, it can levy fines or put a business under a consent decree, which can dictate how the company handles data. The FTC in its request also asked the company to provide extensive details about its products and the way it advertises them. It also demanded details about the policies and procedures that OpenAI takes before it releases any new product to the public, including a list of times that OpenAI held back a large language model because of safety risks.
9. Jul 14, Meta published a [paper](https://ai.meta.com/research/publications/scaling-autoregressive-multi-modal-models-pretraining-and-instruction-tuning/) “Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning”. The new model, named CM3Leon, is a retrieval-augmented, tokenbased, decoder-only multi-modal language model capable of generating and infilling both text and images. CM3Leon achieves state-of-theart performance in text-to-image generation with 5x less training compute than
comparable methods.


**10 July 2023**
1. Last week, Salesforce released [XGen-7b](https://blog.salesforceairesearch.com/xgen/), which achieves comparable or better results when compared with state-of-the-art open-source LLMs (e.g. MPT, Falcon, LLaMA, Redpajama, OpenLLaMA) of similar model size, and its targeted evaluation on long sequence modeling benchmarks show benefits of our 8K-seq models over 2K- and 4K-seq models. Training cost of $150K on 1T tokens under Google Cloud pricing for TPU-v4.
2. 4th July, according to [iTnews](https://www.itnews.com.au/news/chatgpt-used-in-peer-reviews-of-australian-research-council-grant-applications-597596), ChatGPT is used in peer reviews of Australian Research Council grant applications. However, ARC warns that this could be a breach of confidentiality, and has since released a statement advising peer reviewers not to use AI as part of their assessments.
3. 4th July, OpenAI [announced](https://techcrunch.com/2023/07/06/openai-makes-gpt-4-generally-available/) to make GPT-4 API and Code Interpreter generally available to all paying API users. It also announced a deprecation plan for some old models which will retire in 2024.
4. On 5th July, a group of researchers published a [paper](https://arxiv.org/abs/2307.02053) “FLACUNA: Unleashing the Problem-Solving Power of VICUNA using FLAN Fine-Tuning”. The researchers constructed a new dataset comprising a large number of tasks that demand problem-solving skills. Experimental findings strongly indicate that the enhanced problem-solving abilities of  FLACUNA, are obtained through fine-tuning VICUNA on the FLAN dataset, leading to significant improvements across numerous benchmark datasets in INSTRUCTEVAL.
5. 5th July, Nature published a [paper](https://www.nature.com/articles/s41586-023-06185-3) “Accurate medium-range global weather forecasting with 3D neural networks”. The authors of the paper proposed that three-dimensional deep neural networks can be trained to forecast global weather patterns, including extreme weather, with accuracy greater than or equal to that of the best numerical weather prediction models.
6. 5th July, researchers from Microsoft published a [paper](https://arxiv.org/abs/2307.02486#:~:text=In%20this%20work%2C%20we%20introduce%20LongNet%2C%20a%20Transformer,the%20attentive%20field%20exponentially%20as%20the%20distance%20grows.) “LongNet: Scaling Transformers to 1,000,000,000 Tokens”. LongNet is a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows.
7. 6th July, Google and others published a [paper](https://arxiv.org/abs/2307.03170#:~:text=To%20tackle%20this%20problem%2C%20we%20introduce%20the%20Focused,space%2C%20enabling%20an%20extension%20of%20the%20context%20length.) “Focused Transformer: Contrastive Training for Context Scaling”. The paper introduces the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning. This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length.
8. 7th July,  according to [Washingtontpost](https://www.washingtonpost.com/technology/2023/07/07/chatgpt-users-decline-future-ai-openai/) and [this report](https://www.livemint.com/technology/tech-news/chatgpt-faces-first-ever-monthly-traffic-decline-shows-shift-in-user-preferences-report-11688621775980.html), ChatGPT, the highly popular AI chatbot introduced in November, experienced a decline in its website's monthly traffic and unique visitors for the first time in June, as reported by Similarweb analytics.
9. 10th July, according to [TheVerge](https://www.theverge.com/2023/7/10/23787453/meta-instagram-threads-100-million-users-milestone), Instagram’s Threads app surpasses 100 million users within only 5 day since its release.


**2 July 2023**
1. 23rd Jun, [A16Z’s Shoham interviewed](https://a16z.com/2023/06/23/the-next-token-of-progress-4-unlocks-on-the-generative-ai-horizon/) CEOs from Anthropic, Cohere, Charater.AI, they identified four key innovations: Steering, memory, “arms and legs”, and multimodality; and how these key innovations will evolve over the next 6 to 12 months.
2. 26 Jun, [Wired reports](https://www.wired.com/story/google-deepmind-demis-hassabis-chatgpt/) that DeepMind’s CEO says its next AI project Gemini, is still under development within several months, will be more capable than OpenAI’s ChatGPT, including such as planning or the ability to solve problems. AlphaGo-type techniques will be introduced in Gemini. The project could cost hundreds of millions of dollars, while GPT-4 cost more than $100 million, [according to Altman](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/).
3. 26 Jun, [VentureBeat, Databricks is acquiring MosaicML](https://venturebeat.com/data-infrastructure/databricks-is-acquiring-mosaicml-for-a-jaw-dropping-1-3-billion/) for a jaw-dropping $1.3 billion. The news was also confirmed by both Databricks and MosaicLM, an AI start-up established only one and a half years. MosaicLM believes that every organization should be able to benefit from the AI revolution with more control over how their data is used. MosaicLM has its own open-source [MPT serious LLMs](https://github.com/mosaicml/llm-foundry).
4. 27 Jun, Microsoft [published a paper](https://arxiv.org/pdf/2306.14824.pdf): “KOSMOS-2: Grounding Multimodal Large Language Models to the World”. KOSMOS-2 is a multimodal LLM, enabling new capabilities of perceiving object descriptions and grounding text to the visual world. Researchers also created a large-scale dataset of grounded image-text pairs to train the model. The research lays out the foundation for the development of Embodiment AI.
5. 27 Jun, [Nvidia](https://blogs.nvidia.com/blog/2023/06/27/generative-ai-debut-mlperf/),  H100 GPUs set new records on all eight tests in the latest MLPerf training benchmarks released today, excelling on a new MLPerf test for generative AI. On a commercially available cluster of 3,584 H100 GPUs co-developed by startup Inflection AI and operated by CoreWeave, a cloud service provider specializing in GPU-accelerated workloads, the system completed the massive GPT-3-based training benchmark in less than eleven minutes.
6. 28 Jun, [Meta published a paper](https://arxiv.org/pdf/2306.15595.pdf): “Extending Context Window of Large Language Models via Positional Interpolation”. Researchers present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B.
7. 29 Jun, [Oracle CEO said](https://finance.yahoo.com/news/oracle-spending-billions-nvidia-chips-224222975.html?guccounter=1) the company is spending "billions" of dollars on chips from Nvidia Corp as it expands a cloud computing service targeting a new wave of artificial intelligence (AI) companies, also including investment on CPUs.
8. 29 Jun, [Inflection.ai, Microsoft-backed start-up, has raised $1.3 billion](https://www.reuters.com/technology/inflection-ai-raises-13-bln-funding-microsoft-others-2023-06-29/) new funding. "We'll be building a cluster of around 22,000 H100s. This is approximately three times more compute than what was used to train all of GPT4. Speed and scale are what's going to really enable us to build a differentiated product," Suleyman said at Collision Conference on Thursday.
9. 2nd July, [OpenChat](https://github.com/imoneoi/openchat), based on LLaMA-13B, ranked #1 open source LLM on [AlpacaEval leaderboard](https://tatsu-lab.github.io/alpaca_eval/). OpenChat is finetuned with 8xA100 80GB GPUs.


**25 June 2023**
1. GitHub CEO [Dohmke says Copilot](https://www.freethink.com/robots-ai/github-copilot#:~:text=GitHub%20CEO%20says%20Copilot%20will%20write%2080%25%20of%20code%20%E2%80%9Csooner,the%20future%20of%20innovation%20itself.&text=Over%20the%20last%20fifteen%20years,of%20the%20world%20of%20coding.) will write 80% of code “sooner than later”, and that doesn’t mean the developer is going to replace. He also said that Copilot brings the fun back, it brings the creativity back, it brings the flow back.
2. 20th June, Microsoft [published a paper](https://arxiv.org/pdf/2306.11644.pdf) “Textbooks Are All You Need”. Researchers trained a transformer-based model phi-1 with 1.3B parameters trained for 4 days on 8 A100 GPUs, using a selection of textbook quality data from the Web(6B tokens). Phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP.
3. 20th June, UC Berkeley researcher announced [vLLM](https://vllm.ai/), an easy, fast and cheap LLM. vLLM equipped with PagedAttention redefines the new state of the art in LLM serving: it delivers up to 24x higher throughput than HuggingFace Transformers, without requiring any model architecture changes. ([Github](https://github.com/vllm-project/vllm))
4. 21st June, [GPT-Engineer is launched on GitHub](https://github.com/AntonOsika/gpt-engineer). GPT Engineer is made to be easy to adapt, extend, and make your agent learn how you want your code to look. It generates an entire codebase based on a prompt. It gains over 30K stars after 4 days.
5. 21st June, George Hotz in [a video said](https://www.latent.space/p/geohot#details) that “so GPT-4 is 220 billion in each head, and then it's an eight-way mixture model. So mixture models are what you do when you're out of ideas. So, you know, it's a mixture model. They just train the same model eight times, and then they have some little trick. They actually do 16 inferences, but no, it's not like- [00:43:45]”
6. 21st June, CNBC, [Google accuses Microsoft](https://www.cnbc.com/2023/06/21/google-accuses-microsoft-of-anticompetitive-practices-in-azure-cloud.html) of using stringent licensing terms to exert monopolistic control over the cloud market.
7. 21st June, [CVPR announced best paper awards](https://www.prnewswire.com/news-releases/cvpr-2023-best-paper-award-winners-announced-301857429.html), best papers: “[Visual Programming: Compositional visual reasoning without training](https://c212.net/c/link/?t=0&l=en&o=3900096-1&h=85460982&u=https%3A%2F%2Farxiv.org%2Fabs%2F2211.11559&a=Visual+Programming%3A+Compositional+visual+reasoning+without+training)”, and “[Planning-oriented Autonomous Driving](https://c212.net/c/link/?t=0&l=en&o=3900096-1&h=2174741114&u=https%3A%2F%2Farxiv.org%2Fabs%2F2212.10156&a=Planning-oriented+Autonomous+Driving)”. Best student paper: “ [3D Registration with Maximal Cliques](https://c212.net/c/link/?t=0&l=en&o=3900096-1&h=152672617&u=https%3A%2F%2Farxiv.org%2Fabs%2F2305.10854&a=3D+Registration+with+Maximal+Cliques)”
8. 22nd June, researchers from MIT and Microsoft [published a paper](https://arxiv.org/pdf/2306.09896.pdf) “Demystifying GPT self-Repair for Code Generation”. They found that “the effectiveness of self-repair is only seen in GPT-4”.
9. 22nd June, LMSYS updated the [leaderboard](https://lmsys.org/blog/2023-06-22-leaderboard/). GPT-4, GPT-3.5-Turbo and Claude-v1 are the top three on the list. The top OSS models are Vicuna-33B, WizardLM-33B, and Guanaco-33B. Falcon-40B  is ranked far below in the list, even below Vicuna-7B model.
10. 22nd June, Stability.ai launched [SDXL 0.9](https://stability.ai/blog/sdxl-09-stable-diffusion), a leap forward in AI image generation. The 0.9 version is the most advanced development in the Stable Diffusion text-to-image suite of models, and can produce massively improved image and composition detail over its predecessor.
11. 22nd June, According to CNBC, [AWS is investing $100 million](https://www.cnbc.com/2023/06/22/aws-invests-100-million-in-generative-ai-as-it-sees-a-long-race-ahead.html) in generative A.I. center in race to keep up with Microsoft and Google.
12. 22nd June,  [ MosaicML released MPT-30B](https://thenewstack.io/mosaicml-launches-30b-model-takes-on-llama-falcon-and-gpt/), ranked the same as Vicuan-13B. The company claims that it surpasses OpenAI’s GPT-3 in quality, despite having about 1/6th the number of parameters (GPT-3 has 175 billion). “This means MPT-30B is easier to run on local hardware and much cheaper to deploy for inference”
13. 22nd June, researchers from MIT and Stanford [published a paper](https://arxiv.org/pdf/2306.12672.pdf) “From Word Models to World Models”. The paper proposed rational meaning construction, a computational framework for language-informed thinking that combines neural models of language with probabilistic models for rational inference.
14. 23rd June, [A team of researchers](https://nbcmontana.com/news/local/um-um-western-researchers-find-openais-gpt-4-outperforms-humans-in-creativity-tests), including professors from the University of Montana and UM Western, have found that OpenAI's GPT-4 scored in the top 1% on the Torrance Tests of Creative Thinking (TTCT), matching or outperforming humans in the creative abilities of fluency, flexibility, and originality.
15. 23rd June, [Microsoft says](https://www.independent.co.uk/tech/quantum-computing-microsoft-supercomputer-ibm-b2362174.html) it has announced plans to build a quantum supercomputer after researchers said the next-generation machines will be able to outperform standard computers within the next two years.


**18 June 2023**
1.	Andrew Ng and Geoff Hinton had an [insightful conversation](https://www.linkedin.com/posts/andrewyng_had-an-insightful-conversation-with-geoff-activity-7073688821803978752-DO9h/?trk=public_profile_share_view). They want to share (i) It's important that AI scientists reach consensus on risks-similar to climate scientists, who have rough consensus on climate change-to shape good policy.
(ii) Do AI models understand the world? We think they do. If we list out and develop a shared view on key technical questions like this, it will help move us toward consensus on risks.
2.	On 12 June 2017, Google published its outstanding paper: “[Attention is All You Need](https://arxiv.org/abs/1706.03762)” which introduced the transformer structure – an essential element widely used in nearly all large deep learning models, both in NLP and Computer Vision. The paper has been cited over 75K, and of eight authors, only one still working in Google.
3.	A new LLM evaluation [leaderboard](https://declare-lab.net/instruct-eval/) is released by researchers from UTD Singapore. The proposed model evaluated three features of LLMs: Problem-Solving, Writing, and Alignment (Harmless, Honesty and Helpfulness)
4.	On 13th June, [MetaAI announced I-JEPA](https://ai.facebook.com/blog/yann-lecun-ai-model-i-jepa/), the first AI model based on Yann LeCun’s vision for more human-like AI, which is to create machines that can learn internal models of how the world works so that they can learn much more quickly, plan how to accomplish complex tasks, and readily adapt to unfamiliar situations.
5.	On 14th June, [OpenAI released new GPT-4 and GPT-3.5 Turbo](https://twitter.com/OfficialLoganK/status/1668668826047721494) models with 1) function calling in the API (plugins); 2) 16K context 3.5 Turbo model available to everyone; 3) 75% price reduction on v2 embedding models.
6.	McKinsey released “[The economic potential of generative AI: The next productivity frontier](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier#business-value)”. 1) Generative AI could add $2.6 to $4.4 Trillion in value to the global ecomony; 2) 75% of the value falls in: Customer operations, marketing and sales, software engineering and R&D; 3) Generative AI will have impact across all industry sectors; 4) 50% today’s work activities could be automated between 2030 and 2060; 5) Generative AI is just beginning.
7.	On 14 June, The sequoia published “[The New Language Model Stack](https://www.sequoiacap.com/article/llm-stack-perspective/)” which describes how companies are bringing AI applications to life. 1) Nearly every company in the Sequoia network is building LLMs into their products; 2) The new stack centers on LLM APIs, retrieval, and orchestration, but open source usage is also growing; 3) Companies want to customize LLMs to their unique context; 4) LLMs need to become more trustworthy (output quality, data privacy, security) for full adoption
8.	On 15th June, Princeton Uni published a paper “[Infinite Photorealistic Worlds using Procedural Generation](https://arxiv.org/pdf/2306.09310.pdf)”. It’s worth noting that Infinigen is entirely procedural: every asset, from shape to texture, is generated from scratch via randomized mathematical rules, using no external source and allowing infinite variation and composition.
9.	 On 16th June, Meta published a paper “[Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale](https://research.facebook.com/publications/voicebox-text-guided-multilingual-universal-speech-generation-at-scale/)”. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are neither filtered nor enhanced. Voicebox not outperforms the SOT zero-shot TSS model, but also up to 20 times faster
10.	IBM Makes the [Best Quantum Computer Open to Public](https://analyticsindiamag.com/ibm-makes-the-best-quantum-computer-open-to-public/) - IBM in collaboration with UC Berkeley researchers announced a recent breakthrough experiment which indicates that quantum computers will soon surpass classical computers in practical tasks.


**11 June 2023**
1. OpenAI see traffic soar to Billion mark, achieved a total [847 million user access](https://www.digitalinformationworld.com/2023/06/openai-website-sees-traffic-soar-to.html) in March 2023.
2. [Video-LLaMA](https://github.com/damo-nlp-sg/video-llama), a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA showcases the ability to perceive and comprehend video content, generating meaningful responses that are grounded in the visual and auditory information presented in the videos.
3. [InstructZero](https://arxiv.org/pdf/2306.03082v1.pdf), is an efficient instruction optimization method for black-box large language models by optimizing a low-dimensional soft prompt applied to an open-source LLM to generate the instruction for the black-box LLM.
4. [Git-Theta](https://arxiv.org/pdf/2306.04529v1.pdf) is a Git extension that aims to provide similar functionality for machine learning model checkpoints by efficiently and meaningfully track a model's version history natively through Git. [Link to the project](https://github.com/r-three/git-theta)
5. A github project named [roop](https://github.com/s0md3v/roop) is recently released. It allows anyone to take a video and replace the face in it with a face of your choice. You only need one image of the desired face. No dataset, no training.
6. Facebook published a [paper](https://arxiv.org/pdf/2306.05284v1.pdf) introducing MusicGen, which can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. 
7. [SpQR](https://github.com/vahe1994/spqr)- Sparse-Quantized Representation, is a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods. Require GPU VRAM > 32GB.
8. [MAN](https://arxiv.org/pdf/2306.05399v1.pdf) - Matting Anything Model, can estimate the alpha matte of any target instance with user prompts as boxes, points, or text descriptions for interactive use by incorporating [SAM](https://segment-anything.com/). It further reaches comparable performance to the specialized matting models on multiple benchmarks, and shows superior generalization ability with fewer parameters as a unified image matting model.
9. [Video-ChatGPT](https://arxiv.org/pdf/2306.05424v1.pdf), is a multimodal model that merges a video-adapted visual encoder with a LLM. The model is capable of understanding and generating human-like conversations about videos. [Try it here](https://www.ival-mbzuai.com/video-chatgpt).
10. DeepMind publish a [paper in Nature](https://www.nature.com/articles/s41586-023-06004-9): Faster sorting algorithms discovered using deep reinforcement learning. Researchers trained a new deep reinforcement learning agent, AlphaDev, to formulate a task of finding a better sorting routine at assembly-language level as a single-player game.
11. [Magic](https://magic.dev/), an AI startup company, announced [LTM-1](https://twitter.com/magicailabs/status/1666116935904292869), a prototype of a neural network architecture designed for giant context windows, can handle prompt with 5,000,000 tokens, much larger than GPT-4's 32k tokens.
12. Huggingface released [StarCode+](https://huggingface.co/bigcode/starcoderplus) - is a fine-tuned version of [StarCoderBase](https://huggingface.co/bigcode/starcoderbase) on 600B tokens from the English web dataset [RedefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) combined with [StarCoderData](https://huggingface.co/datasets/bigcode/starcoderdata) from [The Stack (v1.2)](https://huggingface.co/datasets/bigcode/the-stack) and a Wikipedia dataset. It's trained on 512 Tesla A100 GPUs for 14 days.
13. RedPajama released [SlimPajama](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama) - the largest extensively deduplicated, multi-corpora, open-source dataset for training large language models. [Github link](https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama)


**4 June 2023**
1.	Google, Princeton, and Stanford published a [paper](https://arxiv.org/pdf/2305.17126.pdf) “Large Language Models as Tool Makers”. The paper proposed a closed-loop framework referred to as LATMs, which can create their own reusable tools for problem-solving. The project uses GPT-3.5 as tool user, and GPT-4 as tool maker to reduce inference costs.
2.	Nvidia announced [DGX GH200](https://nvidianews.nvidia.com/news/nvidia-announces-dgx-gh200-ai-supercomputer), a supercomputer that is 10 times faster than the current fastest computer in the world. The computer will be used for generative AI language applications. Watch [this](https://www.nvidia.com/en-us/events/computex/) from 60mins for about 1 min. Google, Microsoft, and Meta will be its first users.
3.	Nvidia’s [Neuralangelo project](https://blogs.nvidia.com/blog/2023/06/01/neuralangelo-ai-research-3d-reconstruction/) can now turn 2D video clips into 3D structures and scenes. It literally generates detailed replicas of buildings, sculptures, and real objects from video clips taken on a mobile or camera.
4.	[Statement on AI Risk](https://www.safe.ai/statement-on-ai-risk) has been circulated and signed by a lot. “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war”
5.	OpenAI publishes a research [paper](https://arxiv.org/abs/2305.20050), “let’s verify step by step”, by using this process supervision approach, the process-supervised model solves 78% of the problem from the MATH test set. It also aims at attack the Hallucination issues of LLMs.
6.	[GPT4Tools](https://github.com/StevenGrove/GPT4Tools) – an open-source tool based on Vicuan (LLaMA), and aims to efficiently enable LLMs to decide, control and utilizing different visual foundation models, allowing users to interact with images during a conversation.
7.	Google, OpenAI, Anthropic, etc published a [paper](https://arxiv.org/pdf/2305.15324.pdf) “Model evaluation for extreme risks”. An evaluation model is created to evaluate extreme risks by looking at dangerous capabilities and alignment as input and to ensure responsible training, responsible deployment, transparency and appropriate security.


**28 May 2023**
1.	Meta published a [paper](https://arxiv.org/abs/2305.11206): LIMA Less is more for Alignment. Use carefully cured 1000 high-quality prompts, LIMA beats Google’s Bard, and GPT-3.5, and just below GPT-4.
2.	On 22nd May, Meta released [Massively Multilingual Speech AI](https://about.fb.com/news/2023/05/ai-massively-multilingual-speech-technology/), a single multilingual speech recognition model which can process more than 1000 languages, compared with the previous 100 languages only.
3.	0n 23rd May, Adobe Photoshop adds [Native AI](https://venturebeat.com/ai/adobe-integrates-generative-ai-directly-into-photoshop-with-new-firefly-capabilities/), unlike other open source software, it’s a commercially safe model, using high-quality images, and without copyright issues.
4.	On 23rd May, Microsoft released [Windows Copilot](https://blogs.windows.com/windowsdeveloper/2023/05/23/bringing-the-power-of-ai-to-windows-11-unlocking-a-new-era-of-productivity-for-customers-and-developers-with-windows-copilot-and-dev-home/) to Windows 11, it also released a list of other plugins that will greatly improve the productivity of developers and Windows users. Also, Bing is powered by GPT-4 now, but with only 5 QA each month only. Microsoft also announced [Data Fabric](https://learn.microsoft.com/en-us/fabric/get-started/microsoft-fabric-overview), an all-in-one analytics solution that covers everything from data movement to data science, Real-Time Analytics, and business intelligence + services, including data lake, data engineering, and data integration, all in one place.
5.	An interesting [feature of Microsoft 365 Copilot](https://www.youtube.com/watch?v=qMGLU-chnLk) for drafting legal contracts during Microsoft Build.
6.	On 23rd May, Google Bard  Image generation go online. Eg, ask bard “show me some fashion hair styles in Australia”. It will show you some hairstyles.
7.	Two more models based on LLAMA: [airoboros](https://www.reddit.com/r/LocalLLaMA/comments/13o6kp8/airoboros13b_98_against_gpt35/) and [Guanaco](https://www.reddit.com/r/LocalLLaMA/comments/13rthln/guanaco_7b_13b_33b_and_65b_models_by_tim_dettmers/). Both are close to GPT-3.5 performance, especially the latter, which is based on [QLora](https://arxiv.org/pdf/2305.14314.pdf).
8.	JPMorgan is developing its [IndexGPT](https://www.cnbc.com/2023/05/25/jpmorgan-develops-ai-investment-advisor.html) to select investments for customers.
9.	[Spellbook](https://www.spellbook.legal/) uses GPT-4 and other large language models to review and suggest terms for users’ contracts, right in Microsoft Word.
10.	On 25th May, [TII](https://www.tii.ae/news/uaes-technology-innovation-institute-launches-open-source-falcon-40b-large-language-model) released Falcon 40B and temporarily ranked #1 on [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). TTI announced it’s an open-source model, but maybe not.
11.	On 27th May, [a lawyer used ChatGPT](https://www.theverge.com/2023/5/27/23739913/chatgpt-ai-lawsuit-avianca-airlines-chatbot-research) to prepare a case in USA and now has to answer for its bogus citations. At least six cases were just made up by ChatGPT. [The fake cases source? ChatGPT](https://edition.cnn.com/2023/05/27/business/chat-gpt-avianca-mata-lawyers/index.html).
12.	Nvidia released GPU-4 Powered [Voyager](https://arxiv.org/abs/2305.16291), a lifeling learning agent in Minecraft that continuously explores the worlds, acquires diverse skills, and makes novel discuveries without human intervention.


**22 May 2023**
1.	On 13 May, Sam Altman announced on his [Twitter](https://twitter.com/sama/status/1657143368198279168) that “all ChatGPT Plus users getting browsing and plugins over the next week”. The time AI can use tools is coming. A new web-browsing feature is set to allow ChatGPT-Plus users to access real-time information. They will also get access to more than 70 plug-ins on sites including Expedia and Instacart (https://www.businessinsider.com/chatgpt-openai-web-browsing-plug-change-how-we-use-internet-2023-5).
2.	LangGPT — [Empowering everyone to become a prompt expert!](https://community.openai.com/t/langgpt-empowering-everyone-to-become-a-prompt-expert/207880) LangGPT addresses how to write high-quality prompts, which is becoming more akin to programming in the AI ear. The project link is [here](https://github.com/yzfly/LangGPT). 
3.	On 17th May, [Google announced](https://blog.google/technology/developers/google-colab-ai-coding-features/) AI-powered features will add to Colab, and free of charge. The features include code completions, natural language to code generation and even a code-assisting chatbot.
4.	On 18th May, OpenAI [announced ChatGPT app for iOS](https://openai.com/blog/introducing-the-chatgpt-app-for-ios). The ChatGPT app is free to use and syncs users history across devices. It also integrates [Whisper](https://openai.com/research/whisper), an open-source speech-recognition system, enabling voice input. [ChatGPT Plus subscribers](https://openai.com/blog/chatgpt-plus) get exclusive access to [GPT-4’s capabilities](https://openai.com/product/gpt-4), early access to features and faster response times.
5.	OpenAI CEO [calls on government to regulate AI](https://www.msn.com/en-us/news/technology/openai-ceo-calls-on-government-to-regulate-ai/ar-AA1bgSwd) - OpenAI CEO Sam Altman testified before the Senate Judiciary Committee on Tuesday, calling on Congress to pass legislation to regulate AI.
6.	[Drag your GAN](https://vcai.mpi-inf.mpg.de/projects/DragGAN/) – a technology allow one to "drag" any points of the image to precisely reach target points in a user-interactive manner.


**15 May 2023**
1.	[Google.io conference](https://developers.googleblog.com/2023/05/io23-developer-keynote-recap.html) – 1) Introduced PaLM v2 which support Google’s Bard, improved programming coding and dialog quality of Bard, support over 100 language translation, image <-> text generation/analysis, and more; 2) Google search supercharged with AGI, multi-round QA allows following-up question; 3) improved Gmail and Google photos features and others
2.	Anthropic’s [Claude](https://twitter.com/AnthropicAI/status/1656700154190389248?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet) model can process 100K tokens now, more than 3X bigger than GPT-4’s 32K tokens.
3.	Microsoft [announced](https://www.microsoft.com/en-us/microsoft-365/blog/2023/05/09/introducing-the-microsoft-365-copilot-early-access-program-and-new-capabilities-in-copilot/) new capabilities in Copilot, including semantic index, copilot in whiteboard makes Teams meetings and brainstorms more creative and effective; integrate DALL.E  into PowerPoint to automatically generate ppt slides.
4.	OpenAI announced [Shape-E](https://github.com/openai/shap-e), a conditional generative model for 3D assets. Type in text prompts into Shape-E, and the model will produce 3D objects that create better more detailed, and accurate objects.
5.	HuggingFace announced [Transformers Agent](https://huggingface.co/docs/transformers/transformers_agents) – the agent provides a natural language API, which can interpret natural language and use a list of curated tools. The agent dramatically simplifies the process of a pre-trained LLM to call tools. A similar function with [LangChain](https://python.langchain.com/en/latest/index.html) – a framework for developing applications powered by LLM.
6.	Meta announced [ImageBind](https://twitter.com/MetaAI/status/1655989274620358656) – an AI model capable of binding data from six modalities at once, including the 3D shape of an image. [ImageBind](https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/) outperforms prior individually trained models and helps AI by enabling machines to better analyze many different forms of information together.
7.	OpenAI applied GPT-4 to automatically propose [explanations](https://twitter.com/OpenAI/status/1655982364273831936) for GPT-2’S 300K neurons, and found neurons responding to concepts like similes, “things done correctly”, or expressions of certainty. GitHub link.
8.	IBM announced [Dromedary](https://github.com/IBM/Dromedary), another LLM which uses principle-driven, self-alignment to minimize human supervision, and surpasses the performance of ChatGPT and Alpaca. 


**8 May 2023**
1.	A.I. Is Getting Better at Mind-Reading - [In a recent experiment](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41593-023-01304-9&data=05%7C01%7Cd.zhu%40curtin.edu.au%7Cd3b1a33582834f569baf08db4fb7c304%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638191422898365032%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=ffzeFZDazt0JJRlWoFLNQ1hrIm4IkqODfx5sEXwta20%3D&reserved=0), researchers used large language models to translate brain activity into words. Accuracy can now reach about 83% based the authors’ experiments.
2.	Thursday, Microsoft announced [AI powered Bing plugins](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.bing.com%2Fnew%3Fform%3DMY028Z%26OCID%3DMY028Z&data=05%7C01%7Cd.zhu%40curtin.edu.au%7Cd3b1a33582834f569baf08db4fb7c304%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638191422898365032%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=VjfZIdudGT%2BTwTKZwb0KnetYJK6HWEwdkjiabScyvXc%3D&reserved=0), enable Bing to search/generate multimedia content, and an Action feature coming soon.
3.	Google: ["We have no moat, and neither does OpenAI"](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.semianalysis.com%2Fp%2Fgoogle-we-have-no-moat-and-neither&data=05%7C01%7Cd.zhu%40curtin.edu.au%7Cd3b1a33582834f569baf08db4fb7c304%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638191422898365032%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=%2Fw27hisANe4Xi8QJFxVj9dczbkX8VD85S1zUUiUMqjk%3D&reserved=0). It’s reported that Google is considering to follow the trend of open source of LLM, such as LLAMA. Many startups and company released their AI Chat bot simply based on LLAMA.
