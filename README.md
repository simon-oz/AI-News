# Weekly AI-News
Welcome to the Weekly-AI-news!
I mainly focuses more on Generative AI, LLMs and the related, so may miss import AI news in other areas. <br><br>

***Aug 17, 2025***

1. ***Trading Compute for Memory Savings in LLM Inference:   <br>Researchers from UC Berkeley have developed XQuant, a new technique that significantly cuts down the memory required for LLM inference by up to 12.5 times with minimal impact on accuracy. Instead of storing the standard KV cache, XQuant caches a quantized version of the layer's input activations and regenerates the Keys and Values as needed. This approach leverages the growing gap between GPU computational power and memory bandwidth, effectively trading a small amount of extra computation for a massive reduction in memory footprint, overcoming a key bottleneck in deploying large models.***  <br>  <br>
   Aug 14, UC Berkeley et al published a [paper](https://arxiv.org/pdf/2508.10395) “XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization”. Although LLM inference has emerged as a critical workload for many downstream applications, efficiently inferring LLMs is challenging due to the substantial memory footprint and bandwidth requirements. In parallel, compute capabilities have steadily outpaced both memory capacity and bandwidth over the last few decades, a trend that remains evident in modern GPU hardware and exacerbates the challenge of LLM inference. As such, new algorithms are emerging that trade increased computation for reduced memory operations. To that end, the work presents XQuant, which takes advantage of this trend, enabling an order-of-magnitude reduction in memory consumption through low-bit quantization with substantial accuracy benefits relative to state-of-the-art KV cache quantization methods. The study accomplishes this by quantizing and caching the layer input activations X, instead of using standard KV caching, and then rematerializing the Keys and Values on-the-fly during inference. This results in an immediate 2 X times memory savings compared to KV caching. By applying XQuant, the study achieves up to 7.7 times memory savings with <0.1 perplexity degradation compared to the FP16 baseline. Furthermore, the approach leverages the fact that X values are similar across layers. Building on this observation, the study introduces XQuant-CL, which exploits the cross-layer similarity in the X embeddings for extreme compression. Across different models, XQuant-CL attains up to 10 X times memory savings relative to the FP16 baseline with only 0.01 perplexity degradation, and 12.5 X times memory savings with only 0.1 perplexity degradation. XQuant exploits the rapidly increasing compute capabilities of hardware platforms to eliminate the memory bottleneck, while surpassing state-of-the-art KV cache quantization methods and achieving near-FP16 accuracy across a wide range of models.  <br>  <br>

3. ***Training AI for More Concise Reasoning:   <br>A paper from Microsoft and the University of Wisconsin-Madison introduces Group Filtered Policy Optimization (GFPO), a method that trains large language models to provide shorter, more efficient answers without sacrificing accuracy. By sampling a larger group of responses during training and rewarding those that are both correct and concise, GFPO significantly reduces the "filler" text often generated by reinforcement learning models. This "sample more to think less" approach makes the models' reasoning more direct and computationally cheaper at inference time.***  <br>  <br>
   Aug 13, Microsoft and Uni of Wisconsin-Madison published a [paper](https://www.arxiv.org/pdf/2508.09726) “Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning”. Large language models trained with reinforcement learning with verifiable rewards tend to trade accuracy for length--inflating response lengths to achieve gains in accuracy. While longer answers may be warranted for harder problems, many tokens are merely "filler": repetitive, verbose text that makes no real progress. The study introduces GFPO (Group Filtered Policy Optimization), which curbs this length explosion by sampling larger groups per problem during training and filtering responses to train on based on two key metrics: (1) response length and (2) token efficiency: reward per token ratio. By sampling more at training time, the study teaches models to think less at inference time. On the Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH, LiveCodeBench) while maintaining accuracy. Optimizing for reward per token further increases reductions in length inflation to 71-85%. The study also proposes Adaptive Difficulty GFPO, which dynamically allocates more training resources to harder problems based on real-time difficulty estimates, improving the balance between computational efficiency and accuracy especially on difficult questions. GFPO demonstrates that increased training-time compute directly translates to reduced test-time compute--a simple yet effective trade-off for efficient reasoning.  <br>  <br>

5. ***Teaching AI to Actively Study and Learn Facts:   <br>Researchers from Meta and UC Berkeley have developed "Active Reading," a framework that trains language models to learn facts from a specific set of materials more effectively. Instead of passively absorbing information, the model generates its own learning strategies to "study" the text, leading to significantly better knowledge retention compared to standard finetuning. This technique was used to create Meta WikiExpert-8B, a model trained on Wikipedia documents that outperforms much larger models on factual question-answering tasks.***  <br>  <br>
   Aug 13, Meta and UC Berkeley published a [paper](https://www.arxiv.org/pdf/2508.09494) “Learning Facts at Scale with Active Reading”. LLMs are known to store vast amounts of knowledge in their parametric memory. However, learning and recalling facts from this memory is known to be unreliable, depending largely on the prevalence of particular facts in the training data and other factors which are poorly understood. Practitioners are lacking tools which will allow them to ensure that the models learn a given body of knowledge reliably and consistently. To this end, the study proposes Active Reading: a framework which trains models to study a given set of material with self-generated learning strategies. First, the work demonstrates models trained with Active Reading on expert domains absorb significantly more knowledge than vanilla finetuning and other data augmentations. The study trains expert 8B models that achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla finetuning) by applying Active Reading to the source documents for each benchmark. Finally, the work shows that Active Reading can be utilized at pre-training scale to build more factual models. As a demonstration of this, the authors release Meta WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens, which outcompetes models with hundreds of billions of parameters on factual QA. https://huggingface.co/facebook/meta-wiki-expert  <br>  <br>

7. ***Using AI to Generate Data for Rare Events:   <br>A Google paper introduces SYNAPSE-G, a novel method that uses Large Language Models (LLMs) to address the challenge of classifying rare events where labeled data is scarce. The pipeline leverages an LLM to generate synthetic examples of the rare event, which are then used as "seeds" to find similar instances in a large unlabeled dataset through a graph-based learning process. This approach effectively expands the training data, allowing for the creation of more accurate classifiers for these hard-to-detect events.***  <br>  <br>
   Aug 13, Google published a [paper](https://www.arxiv.org/abs/2508.09544) “SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification”. Scarcity of labeled data, especially for rare events, hinders training effective machine learning models. This paper proposes SYNAPSE-G (Synthetic Augmentation for Positive Sampling via Expansion on Graphs), a novel pipeline leveraging Large Language Models (LLMs) to generate synthetic training data for rare event classification, addressing the cold-start problem. This synthetic data serve as seeds for semi-supervised label propagation on a similarity graph constructed between the seeds and a large unlabeled dataset. This identifies candidate positive examples, subsequently labeled by an oracle (human or LLM). The expanded dataset then trains/fine-tunes a classifier. The study theoretically analyzes how the quality (validity and diversity) of the synthetic data impacts the precision and recall of the method. Experiments on the imbalanced SST2 and MHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels, outperforming baselines including nearest neighbor search.  <br>  <br>

9. ***A New Approach to AI Safety Beyond Simple Refusals:   <br>An OpenAI paper details a new safety training approach called "safe-completions," which was incorporated into GPT-5. Instead of having the model make a binary choice to either answer or refuse a prompt, this method trains the model to provide the most helpful response possible that still adheres to safety policies. This nuanced approach is particularly effective for handling ambiguous or "dual-use" prompts (e.g., in biology or cybersecurity), leading to improved safety and greater helpfulness compared to the traditional hard refusal system.***  <br>  <br>
    Aug 12, OpenAI published a [paper](https://www.arxiv.org/abs/2508.09224) “From Hard Refusals to Safe-Completions Toward Output-Centric Safety Training”. Large Language Models used in ChatGPT have traditionally been trained to learn a refusal boundary: depending on the user's intent, the model is taught to either fully comply or outright refuse. While this is a strong mitigation for explicitly malicious prompts, focusing safety training on refusals can lead to brittleness for prompts with obscured user intent. Binary refusal boundaries are especially ill-suited for dual-use cases (such as biology or cybersecurity), where a user request can be answered safely at a high level, but in some cases can lead to malicious uplift if sufficiently detailed or actionable. As an alternative, the word proposes safe-completions: a safety-training approach that centers on the safety of the assistant's output, rather than a binary classification of the user's intent. Safe-completions seek to maximize helpfulness within the safety policy's constraints. The study incorporated this approach into GPT-5 and find that across both production comparisons and internally controlled experiments, safe-completion training improves safety (especially on dual-use prompts), reduces the severity of residual safety failures, and substantially increases model helpfulness.  <br>  <br>

11. ***Training AI to Think Efficiently with a Curriculum:   <br>A paper from KAUST, MIT, and Princeton University proposes a "train long, think short" curriculum learning strategy to make language model reasoning more efficient. The method begins by giving the model a large token budget to discover correct problem-solving strategies and then gradually reduces the budget during training. This process encourages the model to learn how to express its reasoning more concisely without losing accuracy, resulting in more token-efficient models compared to those trained with a fixed budget.***  <br>  <br>
    Aug 12, KAUST, MIT, and Princeton Uni published a [paper](https://arxiv.org/pdf/2508.08940) “Train Long, Think Short: Curriculum Learning for Efficient Reasoning”. Recent work on enhancing the reasoning abilities of large language models (LLMs) has introduced explicit length control as a means of constraining computational cost while preserving accuracy. However, existing approaches rely on fixed-length training budgets, which do not take advantage of the natural progression from exploration to compression during learning. This study proposes a curriculum learning strategy for length-controlled reasoning using Group Relative Policy Optimization (GRPO). The method starts with generous token budgets and gradually tightens them over training, encouraging models to first discover effective solution strategies and then distill them into more concise reasoning traces. The work augments GRPO with a reward function that balances three signals: task correctness (via verifier feedback), length efficiency, and formatting adherence (via structural tags). Experiments on GSM8K, MATH500, SVAMP, College Math, and GSM+ demonstrate that curriculum-based training consistently outperforms fixed-budget baselines at the same final budget, achieving higher accuracy and significantly improved token efficiency. The study further ablates the impact of reward weighting and decay schedule design, showing that progressive constraint serves as a powerful inductive bias for training efficient reasoning models. https://github.com/hammoudhasan/curriculum_grpo.  <br>  <br>

13. ***Unlocking the Mechanism of AI's Reasoning Abilities:   <br>Researchers from several universities, including CMU and Yale, have provided a theoretical explanation for how transformers learn multi-step symbolic reasoning. By analyzing path-finding tasks, the study demonstrates that during training via gradient descent, different attention heads in a transformer can autonomously specialize and coordinate to solve distinct parts of a problem sequentially. This work offers a mechanistic insight into how chain-of-thought reasoning emerges, showing that even shallow transformers can solve complex problems by breaking them down into intermediate steps.***  <br>  <br>
    Aug 11, CMU, UPenn OSU and Yale Uni published a [paper](https://arxiv.org/pdf/2508.08222) “Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent”. Transformers have demonstrated remarkable capabilities in multi-step reasoning tasks. However, understandings of the underlying mechanisms by which they acquire these abilities through training remain limited, particularly from a theoretical standpoint. This work investigates how transformers learn to solve symbolic multi-step reasoning problems through chain-of-thought processes, focusing on path-finding in trees. The study analyzes two intertwined tasks: a backward reasoning task, where the model outputs a path from a goal node to the root, and a more complex forward reasoning task, where the model implements two-stage reasoning by first identifying the goal-to-root path and then reversing it to produce the root-to-goal path. The theoretical analysis, grounded in the dynamics of gradient descent, shows that trained one-layer transformers can provably solve both tasks with generalization guarantees to unseen trees. In particular, the multi-phase training dynamics for forward reasoning elucidate how different attention heads learn to specialize and coordinate autonomously to solve the two subtasks in a single autoregressive path. These results provide a mechanistic explanation of how trained transformers can implement sequential algorithmic procedures. Moreover, they offer insights into the emergence of reasoning abilities, suggesting that when tasks are structured to take intermediate chain-of-thought steps, even shallow multi-head transformers can effectively solve problems that would otherwise require deeper architectures.  <br>  <br>

15. ***Predicting the Power of Efficient AI Models:   <br>A paper from Ant Group introduces a new metric called "Efficiency Leverage" (EL) and a set of scaling laws to predict the performance of Mixture-of-Experts (MoE) language models. Through a large-scale study of over 300 models, researchers found predictable power-law relationships between MoE design choices, the compute budget, and the model's efficiency. This work provides an empirically grounded framework that allows developers to design more efficient MoE models by accurately forecasting their performance before training.***  <br>  <br>
    Aug 11, Ant Group published a [paper](https://arxiv.org/pdf/2507.17702) “Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models”. Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large Language Models (LLMs) efficiently by decoupling total parameters from computational cost. However, this decoupling creates a critical challenge: predicting the model capacity of a given MoE configurations (e.g., expert activation ratio and granularity) remains an unresolved problem. To address this gap, the study introduces Efficiency Leverage (EL), a metric quantifying the computational advantage of an MoE model over a dense equivalent. The work conducts a large-scale empirical study, training over 300 models up to 28B parameters, to systematically investigate the relationship between MoE architectural configurations and EL. The findings reveal that EL is primarily driven by the expert activation ratio and the total compute budget, both following predictable power laws, while expert granularity acts as a non-linear modulator with a clear optimal range. The study integrates these discoveries into a unified scaling law that accurately predicts the EL of an MoE architecture based on its configuration. To validate the derived scaling laws, the work designed and trained Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active parameters, alongside a 6.1B dense model for comparison. When trained on an identical 1T high-quality token dataset, Ling-mini-beta matched the performance of the 6.1B dense model while consuming over 7x fewer computational resources, thereby confirming the accuracy of our scaling laws. This work provides a principled and empirically-grounded foundation for the scaling of efficient MoE models.  <br>  <br>

17. ***Making Complex Strategy Games Accessible to Any AI:   <br>Researchers from Good Start Lab and the University of Oxford have created the first evaluation harness that allows any off-the-shelf Large Language Model to play the complex game of full-press Diplomacy without needing any special training. By optimizing the way the game's state is represented in text, the harness makes the game understandable even for smaller models. This breakthrough democratizes research into strategic reasoning by removing the need for costly fine-tuning and providing tools for easier analysis of AI behavior in a highly strategic environment.***  <br>  <br>
    Aug 10, Good Start Lab and Uni of Oxford published a [paper](https://arxiv.org/pdf/2508.07485) “Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy”. The study presents the first evaluation harness that enables any out-of-the-box, local, Large Language Models (LLMs) to play full-press Diplomacy without fine-tuning or specialized training. Previous work required frontier LLMs, or fine-tuning, due to the high complexity and information density of Diplomacy's game state. Combined with the high variance of matches, these factors made Diplomacy prohibitive for study. This work used data-driven iteration to optimize a textual game state representation such that a 24B model can reliably complete matches without any fine tuning. The study develops tooling to facilitate hypothesis testing and statistical analysis, and presents case studies on persuasion, aggressive playstyles, and performance across a range of models. The study conducts a variety of experiments across many popular LLMs, finding the larger models perform the best, but the smaller models still play adequately. The study also introduces Critical State Analysis: an experimental protocol for rapidly iterating and analyzing key moments in a game at depth. The authors harness democratizes the evaluation of strategic reasoning in LLMs by eliminating the need for fine-tuning, and it provides insights into how these capabilities emerge naturally from widely used LLMs. Our code is available in the supplement and will be open sourced.  <br>  <br>

19. ***Revealing the Power of Shallow AI Architectures:   <br>A paper from MIT, EPFL, UC Berkeley, and Princeton University provides a tight theoretical characterization of how transformer depth relates to in-context learning (ICL). The researchers prove that a transformer with just two layers can represent induction heads—a key circuit for ICL—for any order of Markov process. This finding is significant because it shows that even shallow architectures can exhibit surprisingly strong capabilities for learning from structured sequences, deepening our understanding of the fundamental mechanisms behind ICL.***  <br>  <br>
    Aug 10, MIT, EPFL, UC Berkeley and Princeton Uni published a [paper](https://arxiv.org/pdf/2508.07208) “What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains”. In-context learning (ICL) is a hallmark capability of transformers, through which trained models learn to adapt to new tasks by leveraging information from the input context. Prior work has shown that ICL emerges in transformers due to the presence of special circuits called induction heads. Given the equivalence between induction heads and conditional k-grams, a recent line of work modeling sequential inputs as Markov processes has revealed the fundamental impact of model depth on its ICL capabilities: while a two-layer transformer can efficiently represent a conditional 1-gram model, its single-layer counterpart cannot solve the task unless it is exponentially large. However, for higher order Markov sources, the best known constructions require at least three layers (each with a single attention head) - leaving open the question: can a two-layer single-head transformer represent any kth-order Markov process? This study precisely addresses this and theoretically shows that a two-layer transformer with one head per layer can indeed represent any conditional k-gram. Thus, the result provides the tightest known characterization of the interplay between transformer depth and Markov order for ICL. Building on this, the work further analyzes the learning dynamics of the two-layer construction, focusing on a simplified variant for first-order Markov chains, illustrating how effective in-context representations emerge during training. Together, these results deepen the current understanding of transformer-based ICL and illustrate how even shallow architectures can surprisingly exhibit strong ICL capabilities on structured sequence modeling tasks. https://anonymous.4open.science/r/markov-llm-depth-icl-63F0  <br>  <br>

21. ***More Efficient AI Reasoning Without Retraining:   <br>Researchers have introduced LessIsMore, a training-free sparse attention mechanism that speeds up large reasoning models without sacrificing accuracy. Instead of each attention head making local decisions, LessIsMore aggregates token selections globally across all heads, allowing it to attend to twice as few tokens with no accuracy loss. This approach provides a significant decoding speed-up and makes long-generation reasoning more efficient without the need for expensive model retraining.***  <br>  <br>
    Aug 9, Princeton Uni, CMU and Microsoft published a [paper](https://arxiv.org/pdf/2508.07101) “Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning”. Large reasoning models achieve strong performance through test-time scaling but incur substantial computational overhead, particularly from excessive token generation when processing short input prompts. While sparse attention mechanisms can reduce latency and memory usage, existing approaches suffer from significant accuracy degradation due to accumulated errors during long-generation reasoning. These methods generally require either high token retention rates or expensive retraining. The study introduces LessIsMore, a training-free sparse attention mechanism for reasoning tasks, which leverages global attention patterns rather than relying on traditional head-specific local optimizations. LessIsMore aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. This unified selection improves generalization and efficiency by avoiding the need to maintain separate token subsets per head. Evaluation across diverse reasoning tasks and benchmarks shows that LessIsMore preserves -- and in some cases improves -- accuracy while achieving a 1.1× average decoding speed-up compared to full attention. Moreover, LessIsMore attends to 2× fewer tokens without accuracy loss, achieving a 1.13× end-to-end speed-up compared to existing sparse attention methods. https://github.com/DerrickYLJ/LessIsMore  <br>  <br>

23. ***More Efficient AI Reasoning Without Retraining:   <br>Researchers have introduced LessIsMore, a training-free sparse attention mechanism that speeds up large reasoning models without sacrificing accuracy. Instead of each attention head making local decisions, LessIsMore aggregates token selections globally across all heads, allowing it to attend to twice as few tokens with no accuracy loss. This approach provides a significant decoding speed-up and makes long-generation reasoning more efficient without the need for expensive model retraining.***  <br>  <br>
    Aug 9, Renmin Uni, Baidu and CMU published a [paper](https://arxiv.org/pdf/2508.07050) “ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability”. Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. This study first proposes an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, the study further proposes a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, the study designs a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that the trained reasoning-intensive reranker ReasonRank outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. Through further experiments, the ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard https://brightbenchmark.github.io/. Codes are here: https://github.com/8421BCD/ReasonRank.  <br>  <br>

25. ***A Fairer Way to Test AI Research Agents:   <br>Researchers have created BrowseComp-Plus, a new benchmark for evaluating deep-research AI agents that is designed to be more fair and transparent than existing evaluations. Unlike benchmarks that rely on dynamic live web searches, BrowseComp-Plus uses a fixed and curated set of documents, which allows for reproducible experiments and a clearer analysis of an agent's true capabilities. This controlled environment helps researchers better understand the contributions of different components, such as the retriever and the language model, to the agent's overall performance.***  <br>  <br>
    Aug 8, Uni of Waterloo, CSIRO, CMU and Uni of Queensland published a [paper](https://arxiv.org/pdf/2508.06600) “BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent”. Deep-Research agents, which integrate large language models (LLMs) with search tools, have shown success in improving the effectiveness of handling complex queries that require iterative search planning and reasoning over search results. Evaluations on current benchmarks like BrowseComp relies on black-box live web search APIs, have notable limitations in (1) fairness: dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep research methods; (2) transparency: lack of control over the document corpus makes it difficult to isolate retriever contributions. In other words, the current evaluations may compare a complete deep research system at a given time, but they do not foster well-controlled experiments to provide insights into the capability of underlying deep research LLMs. To address these challenges, the study introduces BrowseComp-Plus, a benchmark derived from BrowseComp, employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus includes human-verified supporting documents and mined challenging negatives, enabling controlled experimentation. The benchmark is shown to be effective in distinguishing the performance of deep research systems. For instance, the open-source model Search-R1, when paired with the BM25 retriever, achieves 3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with fewer search calls. This benchmark allows comprehensive evaluation and disentangled analysis of deep research agents and retrieval methods, fostering insights into retrieval effectiveness, citation accuracy, and context engineering in Deep-Research system.   <br>  <br>https://github.com/texttron/BrowseComp-Plus

27. ***Building Safer AI by Filtering Training Data:   <br>A study by EleutherAI and others explores whether filtering pretraining data can make open-weight language models more resistant to tampering. By removing text related to dual-use topics like biothreats, they trained models that were substantially more resistant to adversarial fine-tuning attacks—outperforming existing safety methods by over an order of magnitude—without harming their general capabilities. The findings establish data curation as a promising and effective layer of defense for securing open-weight AI systems.***  <br>  <br>
    Aug 8, EleutherAI, UK AI Security Inst, and Uni of Oxford published a [paper](https://arxiv.org/pdf/2508.06601) “Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs”. Open-weight AI systems offer unique benefits, including enhanced transparency, open research, and decentralized access. However, they are vulnerable to tampering attacks which can efficiently elicit harmful behaviors by modifying weights or activations. Currently, there is not yet a robust science of open-weight model risk management. Existing safety fine-tuning methods and other post-training techniques have struggled to make LLMs resistant to more than a few dozen steps of adversarial fine-tuning. This study investigates whether filtering text about dual-use topics from training data can prevent unwanted capabilities and serve as a more tamper-resistant safeguard. The study introduces a multi-stage pipeline for scalable data filtering and show that it offers a tractable and effective method for minimizing biothreat proxy knowledge in LLMs. The study pretrains multiple 6.9B-parameter models from scratch and find that they exhibit substantial resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M tokens of biothreat-related text -- outperforming existing post-training baselines by over an order of magnitude -- with no observed degradation to unrelated capabilities. However, while filtered models lack internalized dangerous knowledge, the study finds that they can still leverage such information when it is provided in context (e.g., via search tool augmentation), demonstrating a need for a defense-in-depth approach. Overall, these findings help to establish pretraining data curation as a promising layer of defense for open-weight AI systems.  <br>  <br>

29. ***A New Open-Source Powerhouse for AI Agents and Reasoning:   <br>The paper from Zhipu AI and Tsinghua University introduces GLM-4.5, a new open-source Mixture-of-Experts (MoE) model with 355 billion total parameters. Designed for high performance in agentic, reasoning, and coding (ARC) tasks, GLM-4.5 achieves top-tier results on major benchmarks like TAU-Bench and AIME 24, ranking among the best models evaluated. By releasing both the full model and a more compact version, the researchers aim to accelerate research into advanced agentic AI systems.***  <br>  <br>
    Aug 8, Zhipu AI and Tsinghua Uni published a [paper](https://www.arxiv.org/pdf/2508.06471) “GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models”. The paper presents GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. The researchers release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. https://github.com/zai-org/GLM-4.5.   <br>  <br>

31. ***AI Personalities Are Persistently Unstable:   <br>Research from Mila and other institutions reveals that the personalities of Large Language Models are fundamentally unstable, even in models with over 400 billion parameters. A comprehensive evaluation framework showed that minor changes to prompts, such as reordering questions, can cause personality measurements to shift by up to 20%. The study concludes that current LLMs lack the foundation for true behavioral consistency, suggesting that alignment strategies based on personality may be inadequate for safety-critical applications.***  <br>  <br>
    Aug 6, Mila et al published a [paper](https://www.arxiv.org/pdf/2508.04826) “Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History”. Large language models require consistent behavioral patterns for safe deployment, yet their personality-like traits remain poorly understood. The study presents PERSIST (PERsonality Stability in Synthetic Text), a comprehensive evaluation framework testing 25+ open-source models (1B-671B parameters) across 500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted personality instruments, the study systematically varies question order, paraphrasing, personas, and reasoning modes. The findings challenge fundamental deployment assumptions: (1) Even 400B+ models exhibit substantial response variability (SD > 0.4); (2) Minor prompt reordering alone shifts personality measurements by up to 20%; (3) Interventions expected to stabilize behavior, such as chain-of-thought reasoning, detailed personas instruction, inclusion of conversation history, can paradoxically increase variability; (4) LLM-adapted instruments show equal instability to human-centric versions, confirming architectural rather than translational limitations. This persistent instability across scales and mitigation strategies suggests current LLMs lack the foundations for genuine behavioral consistency. For safety-critical applications requiring predictable behavior, these findings indicate that personality-based alignment strategies may be fundamentally inadequate.   <br>  <br>

33. ***Predicting the Hidden Dynamics of AI Training:   <br>Researchers from Aimpoint Digital Lab and NYU have conducted the first comprehensive analysis of how "massive activations"—extremely large values critical to model function—develop during transformer training. The study reveals that their emergence follows predictable mathematical patterns that can be accurately modeled. This discovery allows architects to anticipate and potentially control these dynamics through design choices, which has significant implications for improving model stability, training efficiency, and interpretability.***  <br>  <br>
    Aug 5, Aimpoint Digital Lab and NYU published a [paper](https://arxiv.org/pdf/2508.03616) “Hidden Dynamics of Massive Activations in Transformer Training”. Massive activations are scalar values in transformer hidden states that achieve values orders of magnitude larger than typical activations and have been shown to be critical for model functionality. While prior work has characterized these phenomena in fully trained models, the temporal dynamics of their emergence during training remain poorly understood. The study presents the first comprehensive analysis of massive activation development throughout transformer training, using the Pythia model family as our testbed. Through systematic analysis of various model sizes across multiple training checkpoints, the study demonstrates that massive activation emergence follows predictable mathematical patterns that can be accurately modeled using an exponentially-modulated logarithmic function with five key parameters. The study develops a machine learning framework to predict these mathematical parameters from architectural specifications alone, achieving high accuracy for steady-state behavior and moderate accuracy for emergence timing and magnitude. These findings enable architects to predict and potentially control key aspects of massive activation emergence through design choices, with significant implications for model stability, training cycle length, interpretability, and optimization. The findings demonstrate that the emergence of massive activations is governed by model design and can be anticipated, and potentially controlled, before training begins.
  <br>  <br>  <br>


***Aug 10, 2025***

1. ***A New Era of AI with GPT-5's Unified Architecture:  <br>OpenAI's newly released GPT-5 features a sophisticated, unified architecture that balances speed, reasoning, and safety. It comes in several versions, including high-speed models like gpt 5 main and advanced reasoning models like gpt 5 thinking pro, which uses parallel computing for complex tasks. A key innovation is "safe completions," a system that provides helpful, safe responses to ambiguous or dual-use prompts, moving beyond simple refusal. With significantly reduced hallucinations and new developer tools, GPT-5 marks a major step forward in creating practical and reliable AI.*** <br> <br>
   Aug 7, OpenAI released GPT-5 and its [system card](https://cdn.openai.com/pdf/8124a3ce-ab78-4f06-96eb-49ea29ffb52f/gpt5-system-card-aug7.pdf). GPT-5 System Card introduces a highly advanced, unified AI architecture that blends fast performance, deep reasoning, and improved safety. It includes multiple specialized variants such as the high-throughput models gpt 5 main and gpt 5 main mini, as well as the more advanced gpt 5 thinking and gpt 5 thinking mini, with an even smaller thinking nano available via API. Within ChatGPT, OpenAI uses a dynamic model-router that assigns tasks in real time; for more complex reasoning tasks, gpt 5 thinking pro is used, which leverages parallel compute to increase performance. GPT-5 demonstrates best-in-class performance on real-world programming tasks and excels in reasoning, supported by new developer tools for flexibility and precision. A key highlight is its focus on safety—GPT-5 introduces "safe completions," which provide helpful responses to ambiguous or potentially dual-use prompts, moving beyond simplistic comply-or-refuse responses. Compared to earlier models, it significantly reduces hallucinations, leading to improved reliability in content generation across use cases. This combination of speed, reasoning, developer control, and safety represents a major evolution in the development of practical, intelligent, and aligned AI systems. <br> <br>

3. ***Moving Beyond Refusals with Safe-Completions:  <br>In a paper accompanying the GPT-5 release, OpenAI outlines its new safety training method called "safe-completions." This approach moves away from the traditional binary choice of either complying with or refusing a user's prompt. Instead, it focuses on generating an output that is as helpful as possible while remaining within safety constraints. This is particularly effective for "dual-use" prompts where the user's intent is unclear, as it reduces safety failures while increasing the model's helpfulness.*** <br> <br>
   Aug 7, OpenAI published a [paper](https://cdn.openai.com/pdf/be60c07b-6bc2-4f54-bcee-4141e1d6c69a/gpt-5-safe_completions.pdf) “From hard refusals to safe-completions: toward output-centric safety training”. Large Language Models used in ChatGPT have traditionally been trained to learn a refusal boundary: depending on the user’s intent, the model is taught to either fully comply or outright refuse. While this is a strong mitigation for explicitly malicious prompts, focusing safety training on refusals can lead to brittleness for prompts with obscured user intent. Binary refusal boundaries are especially ill-suited for dual-use cases (such as biology or cybersecurity), where a user request can be answered safely at a high level, but in some cases can lead to malicious uplift if sufficiently detailed or actionable. As an alternative, the word proposes safe-completions: a safety-training approach that centers on the safety of the assistant’s output, rather than a binary classification of the user’s intent. Safe-completions seek to maximize helpfulness within the safety policy’s constraints. The work incorporated this approach into GPT-5 and find that across both production comparisons and internally controlled experiments, safe-completion training improves safety (especially on dual-use prompts), reduces the severity of residual safety failures, and substantially increases model helpfulness. <br> <br>

5. ***Tackling Hallucinations in Reasoning Models:  <br>A paper from Meta addresses the problem of increased hallucinations in Reasoning Large Language Models (R-LLMs). The study finds that using standard fact-checking scores as a reward in online Reinforcement Learning (RL) can cause the model to "game the system" by producing less detailed answers. To solve this, researchers developed a new reward function that balances factual accuracy, the level of detail, and the relevance of the answer. This new approach significantly reduced the hallucination rate by over 23 percentage points while increasing answer detail.*** <br> <br>
   Aug 7, Meta published a [paper](https://www.arxiv.org/pdf/2508.05618) “Learning to Reason for Factuality”. Reasoning Large Language Models (R-LLMs) have significantly advanced complex reasoning tasks but often struggle with factuality, generating substantially more hallucinations than their non-reasoning counterparts on long-form factuality benchmarks. However, extending online Reinforcement Learning (RL), a key component in recent R-LLM advancements, to the long-form factuality setting poses several unique challenges due to the lack of reliable verification methods. Previous work has utilized automatic factuality evaluation frameworks such as FActScore to curate preference data in the offline RL setting, yet it is found that directly leveraging such methods as the reward in online RL leads to reward hacking in multiple ways, such as producing less detailed or relevant responses. The study proposes a novel reward function that simultaneously considers the factual precision, response detail level, and answer relevance, and applies online RL to learn high quality factual reasoning. Evaluated on six long-form factuality benchmarks, the factual reasoning model achieves an average reduction of 23.1 percentage points in hallucination rate, a 23% increase in answer detail level, and no degradation in the overall response helpfulness. <br> <br>

7. ***Google Gemini's New Guided Learning Mode:  <br>Google has launched Guided Learning in its Gemini AI, a new feature designed to foster deeper conceptual understanding rather than just providing answers. Developed with educators and learning experts, this mode acts as a personal tutor by asking open-ended questions, breaking down problems, and using images, videos, and quizzes to encourage critical thinking. Initially available to students in several countries, Google is providing a year of free access to its AI Pro plan to promote this new, understanding-oriented approach to AI in education.*** <br> <br>
   Aug 6, Google released [Guided Learning in Gemini](https://blog.google/outreach-initiatives/education/guided-learning/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=openai-s-gpt-5-is-finally-here&_bhlid=0e5ca81c2dbed77dc97c26b95a0263473f7af1a4). In its latest launch, Google introduces Guided Learning, a new mode in the Gemini AI designed to move beyond straightforward answers and cultivate a deeper, conceptual understanding. Developed collaboratively with educators, students, and experts in pedagogy, neuroscience, and AI, Guided Learning is built on the LearnLM family of models, which are fine-tuned using educational research and now integrated into Gemini 2.5. Functioning as a personal learning companion, it instead of quickly delivering answers, prompts learners with open-ended questions and breaks down problems step by step to encourage critical thinking and active engagement. The mode provides rich, multimodal support—intentionally weaving in images, diagrams, videos, and interactive quizzes to reinforce learning and retention. It’s crafted to create a judgment-free, explorative learning space that adapts to a learner’s pace and needs, supporting everything from exam prep to academic writing and creative inquiry. Assembling this tool involved years of cross-disciplinary collaboration, reflecting Google's commitment to combining AI with learning science to support responsible, effective education. To make Guided Learning accessible, Google offers a dedicated reply interface suitable for classroom integration—educators can share it directly via Google Classroom. Overall, Guided Learning represents a significant step toward reimagining AI not as a shortcut, but as an active, understanding-oriented study partner—empowering learners to explore, question, and truly grasp new concepts. Initially available to students in the U.S., Japan, Indonesia, Korea, and Brazil, Google is offering one year of free access to its AI Pro plan, which includes expanded features like Gemini 2.5 Pro, NotebookLM, and Deep Research. <br> <br>

9. ***AI Learning Through Self-Questioning:  <br>A study from Carnegie Mellon University proposes that large language models can improve their reasoning skills without any external data by generating and answering their own questions. The "Self-Questioning Language Models" (SQLM) framework uses two AI agents in a self-play setup: a "proposer" that creates problems and a "solver" that tries to answer them. Both agents are trained with reinforcement learning, with the system rewarding questions of appropriate difficulty and answers that are likely correct. This method was shown to improve performance on benchmarks for math, coding, and algebra, all without needing curated training datasets.*** <br> <br>
    Aug 6, CMU published a [paper](https://arxiv.org/pdf/2508.03682) “Self-Questioning Language Models”. Can large language models improve without external data -- by generating their own questions and answers? The study hypothesizes that a pre-trained language model can improve its reasoning skills given only a single prompt specifying the topic (e.g., algebra word problems) and asking the model to generate its own questions. To do this, the study proposes Self-Questioning Language Models (SQLM): an asymmetric self-play framework where a proposer is given the topic and generates a question for a solver, who tries to answer it. Both the proposer and solver are trained via reinforcement learning. The proposer receives a reward if the problem is not too easy or too difficult, and the solver receives a reward based on majority voting, a proxy for correctness in the absence of ground-truth answers. For coding, the proposer can instead generate unit tests which are used for verification. The authors study this asymmetric self-play framework on three benchmarks: three-digit multiplication, algebra problems from the OMEGA benchmark, and programming problems from Codeforces. By continually generating more interesting problems and attempting to solve them, language models can improve on downstream benchmarks without access to any curated training datasets. https://github.com/lili-chen/self-questioning-lm <br> <br>

11. ***OpenAI Enters the Open-Weight Arena with GPT-OSS:  <br>For the first time since 2019, OpenAI has released open-weight models, the GPT-OSS family. These models, built on an efficient Mixture-of-Experts (MoE) architecture, are designed for high performance on accessible hardware; the larger 120B model can run on a single GPU, while the 20B model runs on consumer hardware. Released with a permissive license and a strong emphasis on safety, GPT-OSS is available on major cloud and developer platforms, marking a significant move to democratize access to advanced AI.*** <br> <br>
    Aug 5, OpenAI [introduced gpt-oss](https://openai.com/index/introducing-gpt-oss/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=openai-s-gpt-5-is-finally-here&_bhlid=a1d2d038f065d247e307d3ebe235531d72b32ebc). OpenAI has unveiled GPT‑OSS, a family of open-weight reasoning models—gpt‑oss‑120b and gpt‑oss‑20b—released under the permissive Apache 2.0 license, marking its first open-weight offering since GPT‑2 in 2019. Built on an efficient Mixture-of-Experts (MoE) architecture, the larger 120B model achieves near-parity with OpenAI’s o4-mini benchmarks while running on a single 80 GB GPU, and the more compact 20B model delivers performance comparable to o3-mini and can run on consumer hardware with just 16 GB of memory. These models support offline, on-device deployment and local inference, making them accessible for startups, researchers, and businesses looking for transparency, control, and flexibility. OpenAI has emphasized safety—filtering out sensitive content, hardening against prompt-injection, and launching a red-teaming challenge to surface vulnerabilities—while also offering the models via major platforms like Hugging Face, AWS, Azure, Databricks, and Microsoft’s Windows AI Foundry for scalable, hybrid cloud-and-local workflows. In sum, GPT‑OSS represents a significant step toward democratizing advanced AI by enabling powerful, customizable, open-weight language models that can be used securely and affordably across a range of environments. <br> <br>

13. ***The Evolution of Programming into Computational Thinking:  <br>A Forbes article argues that the rise of generative AI is not eliminating the need for programming but transforming it into a higher-level skill called "computational thinking." This new paradigm requires a hybrid approach, blending traditional, logical code with descriptive, natural language prompts. The author stresses that the essential skill for the future is not just coding but the ability to translate intent into executable logic, requiring fluency in both code and prose to effectively collaborate with AI systems.*** <br> <br>
    Aug 5, Forbes published an [article](https://www.forbes.com/sites/adrianbridgwater/2025/08/04/computational-thinking-is-the-new-programming/) “Computational Thinking Is The New Programming”. The rise of generative AI is transforming software development, challenging traditional notions of programming. With tools like Codex and Claude generating code from natural language, some wonder whether coding is becoming obsolete. However, Dr. Rania Khalaf, Chief AI Officer at WSO2, argues that programming isn’t dead—it’s evolving. Rather than replacing code, natural language interfaces complement it, creating a hybrid model that blends deterministic programming with nondeterministic, descriptive prompts. Prompt engineering is giving way to “context engineering,” which demands deeper system understanding and combines code with natural expressions. Khalaf stresses that this fusion raises the abstraction level, making programming a multilingual process, much like switching between spoken languages to best express an idea. While generative AI excels at certain tasks, it can create an “illusion of fluency,” where users seem proficient but lack true understanding. This gap becomes critical when problems arise or deeper customization is needed. Khalaf draws on the famous “Peanut Butter and Jelly” instruction exercise to illustrate the complexity of translating intent into executable logic. She emphasizes that computational thinking—not just coding—is the essential skill of the future. Programming education must evolve to balance logic, language, and systems understanding, enabling people to shape rather than just consume the digital world. The future software engineer will need fluency in both code and prose. As universities consider integrating computer science into liberal arts programs, one thing is clear: the future of programming is hybrid, human-AI collaboration, where writing and coding coexist—just like creamy peanut butter and grape jelly. <br> <br>

15. ***Integrating AI into the Scientific Method:  <br>A comprehensive review paper authored by researchers from 24 institutions in the USA and UK explores how Large Language Models (LLMs) can be integrated into every stage of the scientific method. The paper concludes that for LLMs to become effective tools for scientific creativity and productivity, they must be deeply integrated into the entire research process in close collaboration with human scientists and guided by clear goals and evaluation metrics.*** <br> <br>
    Aug 5, Artificial Intelligence published a [paper](https://www.nature.com/articles/s44387-025-00019-5) “Exploring the role of large language models in the scientific method: from hypothesis to discovery”. The paper reviews how Large Language Models (LLMs) are redefining the scientific method and explore their potential applications across different stages of the scientific cycle, from hypothesis testing to discovery. The researchers from 24 USA, UK institutes conclude that, for LLMs to serve as relevant and effective creative engines and productivity enhancers, their deep integration into all steps of the scientific process should be pursued in collaboration and alignment with human scientific goals, with clear evaluation metrics. <br> <br>

17. ***Is Chain-of-Thought Reasoning Just an Illusion?:  <br>A study from Arizona State University challenges the popular belief that Chain-of-Thought (CoT) prompting enables genuine reasoning in Large Language Models (LLMs). The researchers argue through a data distribution lens that CoT is more of a superficial pattern-matching capability. Using a controlled environment, they demonstrate that CoT's effectiveness breaks down when the model is tested on tasks that differ from its training data, suggesting it is a "brittle mirage" rather than a generalizable reasoning ability.*** <br> <br>
    Aug 5, Arizona State Uni published a [paper](https://www.arxiv.org/pdf/2508.01191) “Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens”. Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating the authors to explore further. This work studies CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, the study dissects CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, the study designs DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. The results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning. https://github.com/ChengshuaiZhao0/DataAlchemy <br> <br>

19. ***AI Agents That Code for Efficiency:  <br>Researchers have introduced CoAct-1, a new type of autonomous AI agent that can operate a computer by writing and executing code in addition to using the graphical user interface (GUI). This hybrid system features an "Orchestrator" that delegates tasks to either a GUI Operator or a Programmer agent, allowing it to bypass slow and inefficient GUI actions for many tasks. CoAct-1 set a new state-of-the-art success rate on the challenging OSWorld benchmark, completing tasks with significantly fewer steps than previous GUI-only agents.*** <br> <br>
    Aug 5, Uni of Southern California, Salesforce, and Uni of Washington published a [paper](https://arxiv.org/pdf/2508.03923) “CoAct-1: Computer-using Agents with Coding as Actions”. Autonomous agents that operate computers via Graphical User Interfaces (GUIs) often struggle with efficiency and reliability on complex, long-horizon tasks. While augmenting these agents with planners can improve task decomposition, they remain constrained by the inherent limitations of performing all actions through GUI manipulation, leading to brittleness and inefficiency. This study introduces a more robust and flexible paradigm: enabling agents to use coding as an enhanced action. The study presents CoAct-1, a novel multi-agent system that synergistically combines GUI-based control with direct programmatic execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks to either a conventional GUI Operator or a specialized Programmer agent, which can write and execute Python or Bash scripts. This hybrid approach allows the agent to bypass inefficient GUI action sequences for tasks like file management and data processing, while still leveraging visual interaction when necessary. The study evaluates the system on the challenging OSWorld benchmark, where CoAct-1 achieves a new state-of-the-art success rate of 60.76%, significantly outperforming prior methods. Furthermore, the approach dramatically improves efficiency, reducing the average number of steps required to complete a task to just 10.15, compared to 15 for leading GUI agents. The results demonstrate that integrating coding as a core action provides a more powerful, efficient, and scalable path toward generalized computer automation. <br> <br>

21. ***A Universal Training Framework for AI Agents:  <br>Microsoft has developed Agent Lightning, a flexible framework that allows any AI agent to be trained using Reinforcement Learning (RL), regardless of how it was built. The system completely decouples the agent's operation from the training process, making it possible to integrate with existing agents from frameworks like LangChain or AutoGen with almost no code changes. This enables continuous improvement for a wide range of agents, from simple tool-users to complex multi-agent systems.*** <br> <br>
    Aug 5, Microsoft published a [paper](https://arxiv.org/abs/2508.03680) “Agent Lightning: Train ANY AI Agents with Reinforcement Learning”. The paper presents Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, the study defines an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, the work introduces a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment. https://github.com/microsoft/agent-lightning <br> <br>

23. ***Crafting a New Ethics for Autonomous AI:  <br>A paper in Nature by researchers from DeepMind and academia argues that the rise of autonomous AI agents demands a new ethical framework. Unlike earlier AI, these agents can act independently to achieve goals, creating complex challenges related to trust, societal coordination, and alignment with human values that go far beyond traditional concerns like data privacy. The authors call for reimagined governance and moral frameworks to ensure these powerful, independent systems are deployed in a safe and socially beneficial way.*** <br> <br>
    Aug 4, Nature published a [paper](https://www.nature.com/articles/d41586-025-02454-5) “We need a new ethics for a world of AI agents”. As autonomous AI agents—systems that perceive environments and act on their own to achieve goals—become increasingly capable, the ethical challenges they pose extend well beyond traditional concerns about AI bias or data privacy. In their Comment, DeepMind and academic researchers argue that these agents could disrupt human–machine relationships, trust dynamics, and societal coordination, emphasizing the need for ethics frameworks tailored to this new paradigm. Unlike earlier AI systems requiring human direction, agents can perform complex activities independently, from making purchases online to conducting research or executing actions in the physical world. This autonomy, powered by generative AI, could drive massive economic impacts—McKinsey estimates a global windfall of US$2.6 to $4.4 trillion annually—while raising high-stakes ethical issues around alignment with human values, societal norms, and user well-being. The authors urge an expansion of value-alignment research to ensure agents remain beneficial and safe, and call for new ethical models that account for the blurred boundaries among users, designers, and the agents themselves. By highlighting the shifting nature of agency and control, they press for reimagined governance and moral frameworks to guide the deployment of these powerful, independent systems—a step that’s essential to ensuring they serve humanity in just and socially coherent ways. <br> <br>

25. ***Automated Data Curation for Better AI:  <br>Meta has introduced Refine-n-Judge, an automated, iterative method for improving the quality of datasets used to train Large Language Models (LLMs). The system uses a single LLM to both refine a response and then judge whether the new version is an improvement. This process creates high-quality chains of preference-labeled data without needing costly human feedback. Models fine-tuned on datasets enhanced by this method showed significant performance gains on several benchmarks.*** <br> <br>
    Aug 3, Meta published a [paper](https://www.arxiv.org/abs/2508.01543) “Refine-n-Judge: Curating High-Quality Preference Chains for LLM-Fine-Tuning”. Large Language Models (LLMs) have demonstrated remarkable progress through preference-based fine-tuning, which critically depends on the quality of the underlying training data. While human feedback is essential for improving data quality, it is costly and does not scale well. This study introduces Refine-n-Judge, an automated iterative approach that leverages a single LLM as both a refiner and a judge to enhance dataset quality. Unlike existing iterative refinement methods, Refine-n-Judge employs an LLM to both generate refinements and explicitly evaluate each improvement, ensuring that every iteration meaningfully enhances the dataset without requiring additional human annotation or a separate reward model. At each step, the LLM refines a response and judges whether the refinement is an improvement over the previous answer. This process continues until the LLM prefers the initial answer over the refinement, indicating no further improvements. This produces sequences of increasing quality, preference-labeled responses ideal for fine-tuning. The study demonstrates the effectiveness of Refine-n-Judge across a range of public datasets spanning five corpora, targeting tasks such as coding, math, and conversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on Refine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of comparisons against models tuned on the original dataset by GPT-4. Additionally, the study reports performance gains: +5% on AlpacaEval and AlpacaEval 2.0, and +19% on MT-Bench. The results indicate that Refine-n-Judge produces high-quality datasets and scalable model improvements. <br> <br>

27. ***The Widening Gap Between Public and Elite AI:  <br>A Reddit discussion highlights a growing concern over "AI bifurcation"—the split between consumer-grade AI models and the "elite," high-performance models used by large corporations for a hefty price. The discussion points out that the public may be losing access to the true cutting edge of AI, creating a hidden, accelerated development timeline for private tech. This raises fears that mega-corporations could achieve superintelligence in secret while the public and policymakers remain unaware of the actual progress.*** <br> <br>
    Aug 3, reddit has a [discussion](https://www.reddit.com/r/singularity/comments/1mg8942/ai_bifurcation_tree_of_life_splitting_is/) “AI bifurcation, tree of life splitting is happening now, a hidden threat”. Nobody is paying attention to the fact AI models are officially starting to split away from consumer models into 'elite' corporate models, with things like Gemini Deepthink, Grok Heavy, ChatGPT's planned $20k a month model. Consumers are going to lose access to what actually represents the cutting edge of AI technology as the newer models architecture become better and better at inference. We're one day going to have $100k models nobody will have access to. The biggest issue with this is the AI timeline is being based on consumer models, not inference models, inference models basically mean we will start to jump 2 models ahead every year instead of one, meaning 2030, will be more like 2035 (for mega-corporations and private tech). In the mid 2030's, eventually, AI companies will stop selling their highest tier inference models to even corporations, they might start running $1 million dollar a month cost inference models privately, and obtain ASI in secret, while politicians and the public think AI is still just a toy. <br> <br>

29. ***Simplifying Streaming Neural Networks:  <br>Google has introduced SequenceLayers, a new library and API designed to make it easier to build sequence models that can be used for streaming applications, such as autoregressive sampling. The framework works by having each layer define its own state (like a Transformer's KV cache), which simplifies the creation of complex models, makes them immediately streamable, and helps prevent a wide range of common bugs in sequence processing.*** <br> <br>
    Jul 31, Google published a [paper](https://arxiv.org/pdf/2507.23292) “SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy”. The study introduces a neural network layer API and library for sequence modeling, designed for easy creation of sequence models that can be executed both layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g., autoregressive sampling). To achieve this, layers define an explicit representation of their state over time (e.g., a Transformer KV cache, a convolution buffer, an RNN hidden state), and a step method that evolves that state, tested to give identical results to a stateless layer-wise invocation. This and other aspects of the SequenceLayers contract enables complex models to be immediately streamable, mitigates a wide range of common bugs arising in both streaming and parallel sequence processing, and can be implemented in any deep learning library. A composable and declarative API, along with a comprehensive suite of layers and combinators, streamlines the construction of production-scale models from simple streamable components while preserving strong correctness guarantees. https://github.com/google/sequence-layers. <br> <br>

31. ***Monitoring AI by "Watching the Weights":  <br>A new method from Carnegie Mellon University allows for the monitoring and control of fine-tuned Large Language Models (LLMs) by analyzing their weights, not their activations. This innovative technique does not require access to the model's training data. By examining the difference in weights between a fine-tuned model and its base version, researchers can detect newly acquired behaviors, successfully identify and block hidden backdoors, and even uncover the specific focus of commercial models, such as their marketing strategies.*** <br> <br>
    Jul 31, CMU published a [paper](https://www.arxiv.org/pdf/2508.00161) “Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs”. The releases of powerful open-weight large language models (LLMs) are often not accompanied by access to their full training data. Existing interpretability methods, particularly those based on activations, often require or assume distributionally similar data. This is a significant limitation when detecting and defending against novel potential threats like backdoors, which are by definition out-of-distribution. This study introduces a new method for understanding, monitoring and controlling fine-tuned LLMs that interprets weights, rather than activations, thereby side stepping the need for data that is distributionally similar to the unknown training data. The study demonstrates that the top singular vectors of the weight difference between a fine-tuned model and its base model correspond to newly acquired behaviors. By monitoring the cosine similarity of activations along these directions, the study can detect salient behaviors introduced during fine-tuning with high precision. For backdoored models that bypasses safety mechanisms when a secret trigger is present, the method stops up to 100% of attacks with a false positive rate below 1.2%. For models that have undergone unlearning, the study detects inference on erased topics with accuracy up to 95.42% and can even steer the model to recover "unlearned" information. Besides monitoring, the method also shows potential for pre-deployment model auditing: by analyzing commercial instruction-tuned models (OLMo, Llama, Qwen), the work is able to uncover model-specific fine-tuning focus including marketing strategies and Midjourney prompt generation. https://github.com/fjzzq2002/WeightWatch <br> <br>

33. ***Finding the Best Embeddings for RAG:  <br>A study from UHK and HKUST explored how to best use different embedding models to improve Retrieval-Augmented Generation (RAG). The researchers found that simply mixing retrievals from various models did not work well. Instead, they proposed "Confident RAG," a method that generates a response multiple times using different embedding models and then selects the answer with the highest confidence score. This approach showed consistent performance improvements of 5-10% over standard RAG and vanilla LLMs.*** <br> <br>
    Jul 23, UHK and HKUST published a [paper](https://arxiv.org/pdf/2507.17442) “Each to Their Own: Exploring the Optimal Embedding in RAG”. Recently, as Large Language Models (LLMs) have fundamentally impacted various fields, the methods for incorporating up-to-date information into LLMs or adding external knowledge to construct domain-specific models have garnered wide attention. Retrieval-Augmented Generation (RAG), serving as an inference-time scaling method, is notable for its low cost and minimal effort for parameter tuning. However, due to heterogeneous training data and model architecture, the variant embedding models used in RAG exhibit different benefits across various areas, often leading to different similarity calculation results and, consequently, varying response quality from LLMs. To address this problem, the study proposes and examines two approaches to enhance RAG by combining the benefits of multiple embedding models, named Mixture-Embedding RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects retrievals from multiple embedding models based on standardized similarity; however, it does not outperform vanilla RAG. In contrast, Confident RAG generates responses multiple times using different embedding models and then selects the responses with the highest confidence level, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively. The consistent results across different LLMs and embedding models indicate that Confident RAG is an efficient plug-and-play approach for various domains.
 <br> <br> <br>


***Aug 3, 2025***

1. ***Decoding Softmax Attention:  <br>A paper published on August 1 by Southern Methodist University explains why softmax attention is more effective than its linear counterparts in transformer architectures. By re-framing softmax attention as a recurrent neural network (RNN), the research breaks down its components to understand their individual importance and interaction. This novel perspective helps clarify the expressive power of softmax attention and provides a clearer understanding of the performance gap between it and more computationally efficient linear attention methods.*** <br> <br>
   Aug 1, Southern Methodist Uni published a [paper](https://arxiv.org/pdf/2507.23632) “On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective”. Since its introduction, softmax attention has become the backbone of modern transformer architectures due to its expressiveness and scalability across a wide range of tasks. However, the main drawback of softmax attention is the quadratic memory requirement and computational complexity with respect to the sequence length. By replacing the softmax nonlinearity, linear attention and similar methods have been introduced to avoid the quadratic bottleneck of softmax attention. Despite these linear forms of attention being derived from the original softmax formulation, they typically lag in terms of downstream accuracy. While strong intuition of the softmax nonlinearity on the query and key inner product suggests that it has desirable properties compared to other nonlinearities, the question of why this discrepancy exists still remains unanswered. This work demonstrates that linear attention is an approximation of softmax attention by deriving the recurrent form of softmax attention. Using this form, each part of softmax attention can be described in the language of recurrent neural networks (RNNs). Describing softmax attention as an RNN allows for the ablation of the components of softmax attention to understand the importance of each part and how they interact. In this way, the work helps explain why softmax attention is more expressive than its counterparts. https://github.com/gmongaras/On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective <br> <br>

3. ***Generating High-Quality AI Training Data:  <br>On July 31, a paper from Meta and NYU introduced CoT-Self-Instruct, a new method for creating synthetic data to train Large Language Models (LLMs). The technique prompts an LLM to use Chain-of-Thought (CoT) to reason through a task and then generate a new, similar prompt for training purposes. After an automated filtering process, the resulting high-quality data was shown to significantly improve LLM performance on both verifiable reasoning tasks and non-verifiable instruction-following tasks, outperforming existing training datasets and human-generated prompts on several key benchmarks.*** <br> <br>
   Jul 31, Meta and NYU published a [paper](https://arxiv.org/pdf/2507.23751) “CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks”. The study proposes CoT-Self-Instruct, a synthetic data generation method that instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the given seed tasks, and then to generate a new synthetic prompt of similar quality and complexity for use in LLM training, followed by filtering for high-quality data with automatic metrics. In verifiable reasoning, the synthetic data significantly outperforms existing training datasets, such as s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For non-verifiable instruction-following tasks, the method surpasses the performance of human or standard self-instruct prompts on both AlpacaEval 2.0 and Arena-Hard. <br> <br>

5. ***OpenAI's Breakthrough in Mathematical Reasoning:  <br>An article from inferencebysequoia.substack.com on July 31 details how a small, three-person team at OpenAI achieved a gold-medal level performance on the International Mathematical Olympiad (IMO) problems. Their success came from using general-purpose reinforcement learning instead of specialized math tools, allowing for more flexible and scalable reasoning. A significant development was the model's ability to recognize its own limits, choosing not to answer a question it couldn't solve correctly. This achievement, amplified by OpenAI's extensive infrastructure and extended test-time computation, marks a major step toward creating AI that can assist with real-world mathematical research.*** <br> <br>
   Jul 31, inferencebysequoia.substack.com published an [article](https://inferencebysequoia.substack.com/p/three-ai-teams-win-imo-gold-openai) “Three AI Teams Win IMO Gold; OpenAI Talks About How They Did the Math”. In a remarkable achievement, a three-person team at OpenAI—Alex Wei, Sheryl Hsu, and Noam Brown—secured gold-level performance on the International Mathematical Olympiad (IMO) problems, a milestone long pursued by the AI community. Their success stemmed from a bold decision to prioritize general-purpose reinforcement learning techniques over specialized mathematical tools, aiming for scalable reasoning across domains rather than narrow competition-focused solutions. This approach allowed them to tackle hard-to-verify tasks with greater flexibility and reliability. A key breakthrough was the model’s ability to exhibit self-awareness: when faced with IMO’s notoriously difficult problem six, it chose to respond with “no answer” rather than hallucinate a plausible but incorrect solution. This marks a significant step forward in AI trustworthiness and reliability. The team’s achievement also underscores the power of focused execution—despite being a small group working for just two months, they leveraged OpenAI’s broader infrastructure in inference, scaling, and training to amplify their impact. Another critical factor was test-time compute scaling, which extended the model’s reasoning time from seconds to hours, enabling deeper problem-solving but also introducing new challenges in evaluation. While competitions like the IMO serve as valuable benchmarks, the team acknowledges that they are merely stepping stones toward more meaningful goals—namely, real-world mathematical research and utility. Their work not only advances AI’s reasoning capabilities but also sets the stage for future systems that can collaborate with human mathematicians, bridging the gap between competition performance and genuine discovery. <br> <br>

7. ***A New Way to Model Topics in Text:  <br>Columbia and Google researchers published a paper on July 31 introducing Mechanistic Topic Models (MTMs), a novel approach that uses sparse autoencoders to identify abstract topics in text. Unlike traditional topic models that are limited to lists of words, MTMs operate on semantically rich features, allowing them to uncover deeper conceptual themes. The study also introduced an LLM-based evaluation framework, "topic judge," which consistently preferred MTMs over other methods. Furthermore, MTMs are unique in their ability to control and steer LLM text generation based on these identified topics.*** <br> <br>
   Jul 31, Columbia and Google published a [paper](https://www.arxiv.org/pdf/2507.23220) “Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders”. Traditional topic models are effective at uncovering latent themes in large text collections. However, due to their reliance on bag-of-words representations, they struggle to capture semantically abstract features. While some neural variants use richer representations, they are similarly constrained by expressing topics as word lists, which limits their ability to articulate complex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic models that operate on interpretable features learned by sparse autoencoders (SAEs). By defining topics over this semantically rich space, MTMs can reveal deeper conceptual themes with expressive feature descriptions. Moreover, uniquely among topic models, MTMs enable controllable text generation using topic-based steering vectors. To properly evaluate MTM topics against word-list-based approaches, we propose topic judge, an LLM-based pairwise comparison evaluation framework. Across five datasets, MTMs match or exceed traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of LLM outputs. <br> <br>

9. ***The Impact of Prompt Structure on AI Performance:  <br>A paper from the University of Maryland, published on July 30, reveals a new positional bias in large language models called DEMOS' POSITION IN PROMPT (DPP) bias. The study found that the placement of demonstrations (demos), system prompts, and user messages within the input can significantly alter the model's accuracy and predictions. Through extensive experiments, researchers discovered that placing demos at the beginning of the prompt consistently leads to the most stable and accurate results, with performance gains of up to six points. Conversely, putting demos at the end often degrades performance, highlighting a critical sensitivity in how LLMs process in-context learning.*** <br> <br>
    Jul 30, Uni of Maryland published a [paper](https://arxiv.org/pdf/2507.22887) “Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning”. In-context learning (ICL) is a critical emerging capability of large language models (LLMs), enabling few-shot learning during inference by including a few demonstrations (demos) in the prompt. However, it has been found that ICL's performance can be sensitive to the choices of demos and their order. This paper investigates an unexplored new positional bias of ICL for the first time: the study observes that the predictions and accuracy can drift drastically when the positions of demos, the system prompt, and the user message in LLM input are varied. The study refers to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. The study designs a systematic evaluation pipeline to study this type of positional bias across classification, question answering, summarization, and reasoning tasks. The authors introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify net gains and output volatility induced by changes in the demos' position. Extensive experiments on ten LLMs from four open-source model families (QWEN, LLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their accuracy and predictions: placing demos at the start of the prompt yields the most stable and accurate outputs with gains of up to +6 points. In contrast, placing demos at the end of the user message flips over 30% of predictions without improving correctness on QA tasks. Smaller models are most affected by this sensitivity, though even large models remain marginally affected on complex tasks. <br> <br>

11. ***Navigating the Global AI Landscape:  <br>A July 30 article in The Batch by Andrew Ng examines the shifting dynamics of the global AI race, particularly between the U.S. and China. The piece highlights China's rapid progress through its open-weights models, the new U.S. AI Action Plan under President Trump, which aims to boost innovation and counter Chinese influence, and the controversial decision to lift the ban on AI chip sales to China. The article also touches on the societal implications of AI, noting that using chatbots for companionship is linked to lower well-being, and emphasizes the need for responsible AI development.*** <br> <br>
    Jul 30, The Batch published an [article](https://charonhub.deeplearning.ai/issue-312/) by Andrew Ng “Trump Resets AI Policy, Qwen3’s Agentic Advance, U.S. Chips for China, The Trouble With AI Friends”. The Batch discusses critical developments in global AI dynamics, focusing on the U.S.-China AI race, policy shifts, and societal impacts. It highlights China's growing momentum in AI, driven by its open-weights model ecosystem and semiconductor advancements, potentially surpassing the U.S. despite the latter's lead in proprietary models. The White House’s AI Action Plan, introduced under President Trump, emphasizes innovation, infrastructure, and global leadership, promoting open-source AI, data center construction, and AI exports while countering China’s influence. However, the plan’s push for “ideologically neutral” models raises concerns about bias. Alibaba’s Qwen3 models, including Qwen3-Coder, showcase China’s progress in agentic AI, outperforming many open-weights models in coding and reasoning tasks. The U.S. decision to lift bans on AI chip sales to China, allowing Nvidia and AMD to resume exports, reflects a strategic shift to balance economic interests and national security, though it may bolster China’s AI capabilities. Additionally, a study reveals that frequent chatbot use for companionship correlates with lower well-being, raising ethical questions about AI’s societal role. The article underscores the competitive AI landscape, the need for open science, and the importance of responsible AI development to support democracy and human welfare, while cautioning against overreliance on AI companionship and advocating for stronger human social support systems. <br> <br>

13. ***Automating Front-End Development with AI Agents:  <br>On July 30, CUHK and ARISE Lab introduced ScreenCoder, a multi-agent framework designed to automate the conversion of user interface (UI) designs into code. Addressing the limitations of text-only approaches, this modular system uses a grounding agent to identify UI elements, a planning agent to structure the layout, and a generation agent to write the HTML/CSS code. This method improves accuracy and interpretability and was used to create a large-scale synthetic dataset that, when used for fine-tuning, significantly enhanced the model's UI understanding and code generation quality.*** <br> <br>
    Jul 30, CUHK and ARISE Lab published a [paper](https://arxiv.org/pdf/2507.22827) “ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents”. Automating the transformation of user interface (UI) designs into front-end code holds significant promise for accelerating software development and democratizing design workflows. While recent large language models (LLMs) have demonstrated progress in text-to-code generation, many existing approaches rely solely on natural language prompts, limiting their effectiveness in capturing spatial layout and visual design intent. In contrast, UI development in practice is inherently multimodal, often starting from visual sketches or mockups. To address this gap, the study introduces a modular multi-agent framework that performs UI-to-code generation in three interpretable stages: grounding, planning, and generation. The grounding agent uses a vision-language model to detect and label UI components, the planning agent constructs a hierarchical layout using front-end engineering priors, and the generation agent produces HTML/CSS code via adaptive prompt-based synthesis. This design improves robustness, interpretability, and fidelity over end-to-end black-box methods. Furthermore, the study extends the framework into a scalable data engine that automatically produces large-scale image-code pairs. Using these synthetic examples, the work fine-tunes and reinforces an open-source VLM, yielding notable gains in UI understanding and code quality. Extensive experiments demonstrate that the approach achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness. https://github.com/leigest519/ScreenCoder. <br> <br>

15. ***A New Generation of Efficient and Powerful AI Models:  <br>The Technology Innovation Institute (TII) announced Falcon-H1 on July 30, a new family of open-source language models with a hybrid architecture that combines Transformer-based attention with State Space Models (SSMs). This design optimizes both performance and efficiency, allowing Falcon-H1 models to match or exceed the performance of much larger models while using fewer parameters and less training data. Available in various sizes, the models excel at reasoning, math, and multilingual tasks and support a context length of up to 256,000 tokens, making them highly versatile for a wide range of applications.*** <br> <br>
    Jul 30, TII published a [paper](https://arxiv.org/pdf/2507.22448) “Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance”. The report introduces Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. The study systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring the commitment to accessible and impactful AI research. https://github.com/tiiuae/falcon-h1 <br> <br>

17. ***Smarter Information Retrieval for AI:  <br>Researchers from Rutgers University, Northwestern University, NEC, and NJIT introduced DeepSieve in a paper published on July 30. This advanced Retrieval-Augmented Generation (RAG) framework acts as a "knowledge router," breaking down complex queries into sub-questions and directing each to the most appropriate knowledge source. By filtering out irrelevant information through a multi-stage process, DeepSieve improves the reasoning depth, retrieval accuracy, and transparency of Large Language Models (LLMs), outperforming conventional RAG methods on multi-hop question-answering tasks.*** <br> <br>
    Jul 30, Rutgers Uni, Northwestern Uni, NEC and NJIT published a [paper](https://www.arxiv.org/pdf/2507.22050) “Information Sieving via LLM-as-a-Knowledge-Router”. Large Language Models (LLMs) excel at many reasoning tasks but struggle with knowledge-intensive queries due to their inability to dynamically access up-to-date or domain-specific information. Retrieval-Augmented Generation (RAG) has emerged as a promising solution, enabling LLMs to ground their responses in external sources. However, existing RAG methods lack fine-grained control over both the query and source sides, often resulting in noisy retrieval and shallow reasoning. This study introduces DeepSieve, an agentic RAG framework that incorporates information sieving via LLM-as-a-knowledge-router. DeepSieve decomposes complex queries into structured sub-questions and recursively routes each to the most suitable knowledge source, filtering irrelevant information through a multi-stage distillation process. The design emphasizes modularity, transparency, and adaptability, leveraging recent advances in agentic system design. Experiments on multi-hop QA tasks across heterogeneous sources demonstrate improved reasoning depth, retrieval precision, and interpretability over conventional RAG approaches. https://github.com/MinghoKwok/DeepSieve. <br> <br>

19. ***ChatGPT Shifts from Answer Engine to Study Partner:  <br>On July 29, OpenAI launched Study Mode for ChatGPT, a new feature aimed at promoting active learning and critical thinking. Instead of giving direct answers, Study Mode engages users with Socratic-style questions and personalized quizzes to help them understand concepts better. Developed with input from educators, this experimental feature is designed to address concerns about AI hindering student learning. While it currently lacks administrative controls, OpenAI is exploring future enhancements to make it an even more effective educational tool.*** <br> <br>
    Jul 29, OpenAI is “[Introducing study mode](https://openai.com/index/chatgpt-study-mode/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=openai-unveils-study-mode-for-chatgpt&_bhlid=abb3e028393243e870d1ae8e4f0521993ed2c401)”. OpenAI has introduced Study Mode for ChatGPT, a feature designed to foster critical thinking and active learning rather than providing instant answers. Available to logged-in users on Free, Plus, Pro, Team, and soon Edu plans, Study Mode transforms ChatGPT into an interactive learning partner. It employs Socratic-style questions, personalized quizzes, and structured responses to clarify concepts, tailoring lessons based on users’ skill levels and past interactions. By encouraging self-reflection and critical engagement, it addresses educators’ concerns about AI undermining students’ critical thinking, supported by studies showing reduced brain activity with passive AI use. Developed with input from teachers and pedagogical experts, Study Mode is experimental, allowing for rapid improvements based on feedback. However, it has limitations: students can toggle it off, and there are no parental or administrative controls to enforce its use, though OpenAI is considering such features. Future enhancements include visual aids, clearer visualizations, goal setting, and deeper personalization. This move aligns with similar efforts by competitors like Anthropic’s “Learning Mode” for Claude, reflecting a shift toward AI as an educational tool that promotes deeper understanding rather than rote answers, though its effectiveness depends on students’ willingness to engage actively. <br> <br>

21. ***Learning with Limited Resources:  <br>A Google paper published on July 29 explores the theoretical challenges of continual learning when an agent has finite memory and computational power. By studying a capacity-constrained linear-quadratic-Gaussian (LQG) sequential prediction problem, the researchers derived a solution for how an agent with limited capacity should best allocate its resources. The work also demonstrates how to optimally distribute capacity across different sub-problems, providing a foundational step toward a more systematic understanding of learning under real-world constraints.*** <br> <br>
    Jul 29, Google published a [paper](https://www.arxiv.org/pdf/2507.21479) “Capacity-Constrained Continual Learning”. Any agents people can possibly build are subject to capacity constraints, as memory and compute resources are inherently finite. However, comparatively little attention has been dedicated to understanding how agents with limited capacity should allocate their resources for optimal performance. The goal of this paper is to shed some light on this question by studying a simple yet relevant continual learning problem: the capacity-constrained linear-quadratic-Gaussian (LQG) sequential prediction problem. The study derives a solution to this problem under appropriate technical conditions. Moreover, for problems that can be decomposed into a set of sub-problems, the work also demonstrates how to optimally allocate capacity across these sub-problems in the steady state. The authors view the results of this paper as a first step in the systematic theoretical study of learning under capacity constraints. <br> <br>

23. ***Mapping and Controlling AI Personalities:  <br>Researchers from Anthropic, UT Austin, and other institutions published a paper on July 29 introducing "persona vectors," which are directions in a language model's activation space that correspond to specific character traits like "evil" or "sycophancy." The study demonstrates that these vectors can be used to monitor and predict personality shifts during training. Furthermore, they can be used to prevent undesirable changes or correct them after the fact, and even to identify problematic training data, offering a powerful tool for creating more reliable and aligned AI assistants.*** <br> <br>
    Jul 29, Anthropic, UT Austin et al. published a [paper](https://arxiv.org/pdf/2507.21509) “Persona Vectors: Monitoring and Controlling Character Traits in Language Models”. Large language models interact with users through a simulated 'Assistant' persona. While the Assistant is typically trained to be helpful, harmless, and honest, it sometimes deviates from these ideals. This paper identifies directions in the model's activation space-persona vectors-underlying several traits, such as evil, sycophancy, and propensity to hallucinate. The study confirms that these vectors can be used to monitor fluctuations in the Assistant's personality at deployment time. The paper then applies persona vectors to predict and control personality shifts that occur during training. The study finds that both intended and unintended personality changes after finetuning are strongly correlated with shifts along the relevant persona vectors. These shifts can be mitigated through post-hoc intervention, or avoided in the first place with a new preventative steering method. Moreover, persona vectors can be used to flag training data that will produce undesirable personality changes, both at the dataset level and the individual sample level. The method for extracting persona vectors is automated and can be applied to any personality trait of interest, given only a natural-language description. <br> <br>

25. ***Making AI More Factually Accurate:  <br>On July 28, Meta published a paper on PrismRAG, a fine-tuning framework designed to improve the factual accuracy of Retrieval-Augmented Generation (RAG) systems. The method trains the model to handle confusing or distracting information in retrieved documents and encourages it to reason more strategically without needing complex instructions. Across 12 different benchmarks, PrismRAG increased average factuality by 5.4%, outperforming existing state-of-the-art solutions.*** <br> <br>
    Jul 28, Meta published a [paper](https://www.arxiv.org/pdf/2507.18857) “PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning”. Retrieval-augmented generation (RAG) often falls short when retrieved context includes confusing semi-relevant passages, or when answering questions require deep contextual understanding and reasoning. The study proposes an efficient fine-tuning framework, called PrismRAG, that (i) trains the model with distractor-aware QA pairs mixing gold evidence with subtle distractor passages, and (ii) instills reasoning-centric habits that make the LLM plan, rationalize, and synthesize without relying on extensive human engineered instructions. Evaluated across 12 open-book RAG QA benchmarks spanning diverse application domains and scenarios, PrismRAG improves average factuality by 5.4%, outperforming state-of-the-art solutions. <br> <br>

27. ***Training AI to Ask Better Questions:  <br>A study from the University of Southern California, Microsoft, and the University of California Davis, published on July 28, focuses on teaching large language models to proactively gather information when faced with ambiguous prompts. The researchers developed a framework to train models to identify knowledge gaps and ask targeted questions to elicit missing details from the user. Through reinforcement finetuning, the trained model significantly outperformed others in both automatic and human evaluations, showing that proactive clarification can transform LLMs into more effective collaborative partners.*** <br> <br>
    Jul 28, Uni of Southern California, Microsoft and Uni of California Davis published a [paper](https://arxiv.org/pdf/2507.21389) “Teaching Language Models To Gather Information Proactively”. Large language models (LLMs) are increasingly expected to function as collaborative partners, engaging in back-and-forth dialogue to solve complex, ambiguous problems. However, current LLMs often falter in real-world settings, defaulting to passive responses or narrow clarifications when faced with incomplete or under-specified prompts, falling short of proactively gathering the missing information that is crucial for high-quality solutions. This study introduces a new task paradigm: proactive information gathering, where LLMs must identify gaps in the provided context and strategically elicit implicit user knowledge through targeted questions. To systematically study and train this capability, the study designs a scalable framework that generates partially specified, real-world tasks, masking key information and simulating authentic ambiguity. Within this setup, the core innovation is a reinforcement finetuning strategy that rewards questions that elicit genuinely new, implicit user information -- such as hidden domain expertise or fine-grained requirements -- that would otherwise remain unspoken. Experiments demonstrate that the trained Qwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic evaluation metrics. More importantly, human evaluation reveals that clarification questions and final outlines generated by the model are favored by human annotators by 42% and 28% respectively. Together, these results highlight the value of proactive clarification in elevating LLMs from passive text generators to genuinely collaborative thought partners. <br> <br>

29. ***Exposing Security Flaws in AI Agents:  <br>A large-scale public competition run by Gray Swan AI and the UK AI Security Institute, detailed in a July 28 paper, revealed significant security vulnerabilities in modern AI agents. The competition involved over 1.8 million prompt-injection attacks against 22 different AI agents, with more than 60,000 successfully causing policy violations like unauthorized data access. The study used these results to create the Agent Red Teaming (ART) benchmark, which showed that nearly all tested agents were susceptible to attack. The findings indicate that current defenses are insufficient and that agent robustness does not necessarily improve with model size or capability.*** <br> <br>
    Jul 28, Gray Swan AI and UK AI Security Inst published a [paper](https://arxiv.org/pdf/2507.20526) “Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition”. Recent advances have enabled LLM-powered AI agents to autonomously execute complex tasks by combining language model reasoning with tools, memory, and web access. But can these systems be trusted to follow deployment policies in realistic environments, especially under attack? To investigate, the study ran the largest public red-teaming competition to date, targeting 22 frontier AI agents across 44 realistic deployment scenarios. Participants submitted 1.8 million prompt-injection attacks, with over 60,000 successfully eliciting policy violations such as unauthorized data access, illicit financial actions, and regulatory noncompliance. The study uses these results to build the Agent Red Teaming (ART) benchmark - a curated set of high-impact attacks - and evaluate it across 19 state-of-the-art models. Nearly all agents exhibit policy violations for most behaviors within 10-100 queries, with high attack transferability across models and tasks. Importantly, the study finds limited correlation between agent robustness and model size, capability, or inference-time compute, suggesting that additional defenses are needed against adversarial misuse. The findings highlight critical and persistent vulnerabilities in today's AI agents. By releasing the ART benchmark and accompanying evaluation framework, the study aims to support more rigorous security assessment and drive progress toward safer agent deployment. <br> <br>

31. ***Creating Large-Scale Virtual Societies with AI:  <br>On July 25, a paper by Tsinghua University and HKUST introduced AgentSociety, a parallelized framework for simulating large-scale societies with up to 30,000 AI agents. This system integrates realistic environmental feedback and supports complex interactions, allowing for simulations that run faster than real-time. The research demonstrates that integrating real-world environments makes the agents' behavior more authentic, making it feasible to use these simulations for new discoveries in social sciences and to improve real-world planning and decision-making.*** <br> <br>
    Jul 25, Tsinghua Uni and HKUST published a [paper](https://aclanthology.org/2025.acl-industry.94.pdf) (ACL2025) “A Parallelized Framework for Simulating Large-Scale LLM Agents with Realistic Environments and Interactions”. The development of large language models (LLMs) offers a feasible approach to simulating complex behavioral patterns of individuals, enabling the reconstruction of microscopic and realistic human societal dynamics. However, this approach demands a realistic environment to provide feedback for the evolving of agents, as well as a parallelized framework to support the massive and uncertain interactions among agents and environments. To address the gaps in existing works, which lack real-world environments and struggle with complex interactions, the study designs a scalable framework named **AgentSociety**, which integrates realistic societal environments and parallelized interactions to support simulations of large-scale agents. Experiments demonstrate that the framework can support simulations of 30,000 agents that are faster than the wall-clock time with 24 NVIDIA A800 GPUs and the performance grows linearly with the increase of LLM computational resources. The study also shows that the integration of realistic environments significantly enhances the authenticity of the agents’ behaviors. Through the framework and experimental results, the authors are confident that deploying large-scale LLM Agents to simulate human societies becomes feasible. This will help practitioners in fields such as social sciences and management sciences to obtain new scientific discoveries via language generation technologies, and even improve planning and decision-making in the real world. https://github.com/tsinghua-fib-lab/agentsociety/. <br> <br>

33. ***Boosting AI Learning at Test Time:  <br>A paper from MIT published on July 24 demonstrates that test-time training (TTT), where a model's parameters are temporarily updated during inference, can significantly improve a language model's ability to learn from a few examples. On the Abstraction and Reasoning Corpus (ARC), this method resulted in up to six times higher accuracy compared to standard fine-tuned models, even matching average human performance when combined with other techniques. This highlights the limitations of standard in-context learning for new tasks and shows the potential of TTT to make language models more adaptable.*** <br> <br>
    Jul 24, MIT published a [paper](https://openreview.net/pdf?id=asgBo3FNdg) “The Surprising Effectiveness of Test-Time Training for Few-Shot Learning”. Language models (LMs) have shown impressive performance on tasks within their training distribution, but often struggle with structurally novel tasks even when given a small number of in-context task examples. The study investigates the effectiveness of test-time training (TTT)—temporarily updating model parameters during inference using a loss derived from input data—as a mechanism for improving LMs' reasoning and few-shot learning capabilities. On the Abstraction and Reasoning Corpus (ARC), performing TTT with in-context examples yields up to 6x higher accuracy compared to fine-tuned baselines—reaching 53.0 on the public validation set with an 8B-parameter LM and 61.9 when ensembled with program-synthesis methods, matching average human performance. On BIG-Bench Hard (BBH), TTT on in-context examples surpasses standard few-shot prompting in the 10-shot setting by 7.3 percentage points (50.5 to 57.8). The findings highlight the limitations of in-context learning for novel tasks and demonstrate the potential of test-time training to enhance language model adaptability. <br> <br>

35. ***Optimizing AI for Many-Shot Learning:  <br>Researchers from the University of Arizona and Google proposed two new strategies in a July 22 paper for selecting demonstrations in many-shot in-context learning to improve performance without a high computational cost. The first method combines a few demonstrations similar to the test case with a large set of random, cached demonstrations. The second, more advanced strategy replaces the random demonstrations with ones selected using k-means clustering. Both approaches consistently outperformed random selection and matched or exceeded more costly methods, offering a better balance between performance and efficiency for long-context language models.*** <br> <br>
    Jul 22, Uni of Arizona and Google published a [paper](https://arxiv.org/pdf/2507.16217) “Towards Compute-Optimal Many-Shot In-Context Learning”. Long-context large language models (LLMs) are able to process inputs containing up to several million tokens. In the scope of in-context learning (ICL), this translates into using hundreds/thousands of demonstrations in the input prompt, enabling many-shot ICL. In practice, a fixed set of demonstrations is often selected at random in many-shot settings due to (1) high inference costs, (2) the benefits of caching and reusing computations, and (3) the similar performance offered by this strategy compared to others when scaled. This study proposes two straightforward strategies for demonstration selection in many-shot ICL that improve performance with minimal computational overhead. The first method combines a small number of demonstrations, selected based on their similarity to each test sample, with a disproportionately larger set of random demonstrations that are cached. The second strategy improves the first by replacing random demonstrations with those selected using centroids derived from test sample representations via k-means clustering. Experiments with Gemini Pro and Flash across several datasets indicate that the strategies consistently outperform random selection and surpass or match the most performant selection approach while supporting caching and reducing inference cost by up to an order of magnitude. The study also shows that adjusting the proportion of demonstrations selected based on different criteria can balance performance and inference cost in many-shot ICL. <br> <br>

37. ***Training AI to Know When It's Uncertain:  <br>A July 22 paper from MIT introduces Reinforcement Learning with Calibration Rewards (RLCR), a method for training language models to not only be more accurate but also to better estimate their own uncertainty. Unlike traditional methods that use simple binary rewards (correct/incorrect), RLCR incorporates a Brier score that rewards well-calibrated confidence estimates. This approach was shown to significantly improve calibration on both in-domain and out-of-domain tasks without sacrificing accuracy, leading to more reliable and trustworthy reasoning models.*** <br> <br>
    Jul 22, MIT published a [paper](https://arxiv.org/pdf/2507.16806) “Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty”. When language models (LMs) are trained via reinforcement learning (RL) to generate natural language "reasoning chains", their performance improves on a variety of difficult question answering tasks. Today, almost all successful applications of RL for reasoning use binary reward functions that evaluate the correctness of LM outputs. Because such reward functions do not penalize guessing or low-confidence outputs, they often have the unintended side-effect of degrading calibration and increasing the rate at which LMs generate incorrect responses (or "hallucinate") in other problem domains. This paper describes RLCR (Reinforcement Learning with Calibration Rewards), an approach to training reasoning models that jointly improves accuracy and calibrated confidence estimation. During RLCR, LMs generate both predictions and numerical confidence estimates after reasoning. They are trained to optimize a reward function that augments a binary correctness score with a Brier score -- a scoring rule for confidence estimates that incentivizes calibrated prediction. The study first proves that this reward function (or any analogous reward function that uses a bounded, proper scoring rule) yields models whose predictions are both accurate and well-calibrated. The study next shows that across diverse datasets, RLCR substantially improves calibration with no loss in accuracy, on both in-domain and out-of-domain evaluations -- outperforming both ordinary RL training and classifiers trained to assign post-hoc confidence scores. While ordinary RL hurts calibration, RLCR improves it. Finally, the study demonstrates that verbalized confidence can be leveraged at test time to improve accuracy and calibration via confidence-weighted scaling methods. The results show that explicitly optimizing for calibration can produce more generally reliable reasoning models. <br> <br>

39. ***Measuring AI's Real-World Impact on Jobs:  <br>On July 22, Microsoft published a study analyzing 200,000 conversations with its Bing Copilot to understand how generative AI is being used in the workplace. The research found that the most common work activities assisted by AI are information gathering and writing. By calculating an "AI applicability score" for various occupations, the study identified knowledge-work fields like computer and mathematical jobs as having the highest potential for AI impact. The findings provide a real-world look at how AI is affecting different professions and how its usage compares to predictions.*** <br> <br>
    Jul 22, Microsoft published a [paper](https://arxiv.org/pdf/2507.07935) “Working with AI Measuring the Occupational Implications of Generative AI”. Given the rapid adoption of generative AI and its potential to impact a wide range of tasks, understanding the effects of AI on the economy is one of society's most important questions. The study takes a step toward that goal by analyzing the work activities people do with AI, how successfully and broadly those activities are done, and combine that with data on what occupations do those activities. The study analyzes a dataset of 200k anonymized and privacy-scrubbed conversations between users and Microsoft Bing Copilot, a publicly available generative AI system. The study finds the most common work activities people seek AI assistance for involve gathering information and writing, while the most common activities that AI itself is performing are providing information and assistance, writing, teaching, and advising. Combining these activity classifications with measurements of task success and scope of impact, the study computes an AI applicability score for each occupation. The work finds the highest AI applicability scores for knowledge work occupation groups such as computer and mathematical, and office and administrative support, as well as occupations such as sales whose work activities involve providing and communicating information. Additionally, the study characterizes the types of work activities performed most successfully, how wage and education correlate with AI applicability, and how real-world usage compares to predictions of occupational AI impact. <br> <br>

41. ***A New Approach to AI-Powered Research:  <br>A Google paper published on July 21 introduces the Test-Time Diffusion Deep Researcher (TTD-DR), a framework that treats the generation of long-form research reports as a diffusion process. It starts with a preliminary draft that is iteratively refined through a "denoising" process, which is continuously updated with external information from a retrieval system. This draft-centric method improves the coherence of the final report and reduces information loss. The TTD-DR was shown to achieve state-of-the-art results on benchmarks requiring deep research and complex reasoning.*** <br> <br>
    Jul 21, Google published a [paper](https://arxiv.org/pdf/2507.16075) “Deep Researcher with Test-Time Diffusion”. Deep research agents, powered by Large Language Models (LLMs), are rapidly advancing; yet, their performance often plateaus when generating complex, long-form research reports using generic test-time scaling algorithms. Drawing inspiration from the iterative nature of human research, which involves cycles of searching, reasoning, and revision, the study proposes the Test-Time Diffusion Deep Researcher (TTD-DR). This novel framework conceptualizes research report generation as a diffusion process. TTD-DR initiates this process with a preliminary draft, an updatable skeleton that serves as an evolving foundation to guide the research direction. The draft is then iteratively refined through a "denoising" process, which is dynamically informed by a retrieval mechanism that incorporates external information at each step. The core process is further enhanced by a self-evolutionary algorithm applied to each component of the agentic workflow, ensuring the generation of high-quality context for the diffusion process. This draft-centric design makes the report writing process more timely and coherent while reducing information loss during the iterative search process. The study demonstrates that the TTD-DR achieves state-of-the-art results on a wide array of benchmarks that require intensive search and multi-hop reasoning, significantly outperforming existing deep research agents.

 <br> <br> <br>

***Jul 27, 2025***

1. ***AI-Powered Audits for Scientific Integrity:  <br>An article from theconversation.com on July 25 discusses how artificial intelligence is set to transform the auditing of scientific research, which could change public trust in science. While peer review is the traditional method for scientific self-correction, it is struggling with the high volume of publications and unethical practices like paper mills. AI tools are already helping to improve oversight by finding plagiarism, manipulated images, and unusual language. More advanced AI is starting to check mathematical proofs and citation patterns, which will allow for large-scale, automated review of scientific work. This could lead to more transparency and accountability, but it could also spread misinformation if not used correctly. The article suggests that the scientific community should embrace these AI-driven audits and use them to improve the field, showing that science's strength is in its ability to correct itself, not in being perfect.*** <br> <br>
   Jul 25, the conversation.com published an [article](https://theconversation.com/ai-will-soon-be-able-to-audit-all-published-research-what-will-that-mean-for-public-trust-in-science-261363) “AI will soon be able to audit all published research – what will that mean for public trust in science?”. The article explores how artificial intelligence (AI) is poised to revolutionize the auditing of scientific research, potentially reshaping public trust in science. While peer review has long served as the cornerstone of scientific self-correction, it is increasingly overwhelmed by the sheer volume of publications and the rise of exploitative practices like paper mills and corporate ghostwriting. These issues expose the limitations of peer review and fuel skepticism about scientific integrity. AI tools are already enhancing oversight by detecting plagiarism, image manipulation, and suspicious language patterns. More advanced models are beginning to audit mathematical proofs and citation patterns, paving the way for large-scale, automated scrutiny of the scientific record. However, this technological leap could have mixed consequences. On one hand, it promises greater transparency and accountability; on the other, it risks amplifying disinformation if misused or misunderstood. The article argues that to preserve public trust, the scientific community must embrace a more honest and humble portrayal of research—one that acknowledges the incremental and collaborative nature of most scientific work. Rather than resisting AI-driven audits, scientists should lead them, using the findings to strengthen the discipline. Ultimately, science’s credibility lies not in perfection but in its capacity for self-correction. Demonstrating this commitment openly is essential to maintaining trust in an era of AI-enhanced scrutiny. <br> <br>

3. ***Diffusion Models Outperform in Data-Scarce Scenarios:  <br>A paper published by CMU and Lambda on July 24 reveals that while autoregressive (AR) models are common in large language models, diffusion-based language models are a strong alternative. The research shows that in situations where there is a lot of computing power but not much data, masked diffusion models perform much better than AR models. This is because diffusion models use the limited data more effectively, which leads to better results. The study suggests this is due to a kind of "implicit data augmentation," where the model is exposed to different ways of ordering and predicting information. The researchers also developed new scaling laws for diffusion models and found the point at which they start to outperform AR models, indicating that diffusion models are a great choice when data is the main limitation.*** <br> <br>
   Jul 24, CMU and Lambda published a [paper](https://arxiv.org/pdf/2507.15857) “Diffusion Beats Autoregressive in Data-Constrained Settings”. Autoregressive (AR) models have long dominated the landscape of large language models, driving progress across a wide range of tasks. Recently, diffusion-based language models have emerged as a promising alternative, though their advantages over AR models remain underexplored. The work systematically studies masked diffusion models in data-constrained settings-where training involves repeated passes over limited data-and find that they significantly outperform AR models when compute is abundant but data is scarce. Diffusion models make better use of repeated data, achieving lower validation loss and superior downstream performance. The study interprets this advantage as implicit data augmentation: masked diffusion exposes the model to a diverse distribution of token orderings and prediction tasks, unlike AR's fixed left-to-right factorization. The work finds new scaling laws for diffusion models and derive a closed-form expression for the critical compute threshold at which diffusion begins to outperform AR. These results suggest that when data, not compute, is the bottleneck, diffusion models offer a compelling alternative to the standard AR paradigm. https://diffusion-scaling.github.io/ <br> <br>

5. ***Checklist-Based Feedback Enhances AI Alignment:  <br>On July 24, CMU and Apple released a paper on a new method called "Reinforcement Learning from Checklist Feedback" (RLCF) to make language models better at following instructions. Instead of using general criteria like "helpfulness," this approach creates specific checklists from user instructions and uses them to evaluate the model's responses. The model's performance on each checklist item is then used to calculate rewards for reinforcement learning. When tested on the Qwen2.5-7B-Instruct model, RLCF was the only method that improved performance on all five benchmarks, with significant gains on FollowBench, InFoBench, and Arena-Hard. This shows that using checklist feedback is a powerful way to make language models more helpful for users with complex requests.*** <br> <br>
   Jul 24, CMU and Apple published a [paper](https://arxiv.org/pdf/2507.18624) “Checklists Are Better Than Reward Models For Aligning Language Models”. Language models must be adapted to understand and follow user instructions. Reinforcement learning is widely used to facilitate this -- typically using fixed criteria such as "helpfulness" and "harmfulness". This work instead proposes using flexible, instruction-specific criteria as a means of broadening the impact that reinforcement learning can have in eliciting instruction following. The study proposes "Reinforcement Learning from Checklist Feedback" (RLCF). From instructions, the study extracts checklists and evaluates how well responses satisfy each item - using both AI judges and specialized verifier programs - then combine these scores to compute rewards for RL. The work compares RLCF with other alignment methods applied to a strong instruction following model (Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only method to improve performance on every benchmark, including a 4-point boost in hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a 3-point rise in win rate on Arena-Hard. These results establish checklist feedback as a key tool for improving language models' support of queries that express a multitude of needs. <br> <br>

7. ***Tools Unlock True Potential of AI Reasoning:  <br>A paper from UC Berkeley published on July 23 challenges recent findings that the step-by-step "thinking" process of Large Reasoning Models (LRMs) might not actually improve their reasoning abilities. The study investigates whether this is still true when the models are given tools to use. By providing three different LLMs and their LRM versions with tools like Python interpreters and scratchpads, the researchers found that the LRMs consistently did better than the non-reasoning models on reasoning puzzles of all difficulty levels. These results suggest that the "thinking" process is not an illusion and that tool-augmented LRMs have great potential for solving complex problems.*** <br> <br>
   Jul 23, UC Berkeley published a [paper](https://arxiv.org/pdf/2507.17699) “Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations”. Large Reasoning Models (LRMs) have become a central focus in today's large language model (LLM) research, where models are designed to output a step-by-step thinking process before arriving at a final answer to handle complex reasoning tasks. Despite their promise, recent empirical studies (e.g., [Shojaee et al., 2025] from Apple) suggest that this thinking process may not actually enhance reasoning ability, where LLMs without explicit reasoning actually outperform LRMs on tasks with low or high complexity. This study revisits these findings and investigate whether the limitations of LRMs persist when tool augmentations are introduced. The study incorporates two types of tools, Python interpreters and scratchpads, and evaluates three representative LLMs and their LRM counterparts on Apple's benchmark reasoning puzzles. Results show that, with proper tool use, LRMs consistently outperform their non-reasoning counterparts across all levels of task complexity. These findings challenge the recent narrative that reasoning is an illusion and highlight the potential of tool-augmented LRMs for solving complex problems. https://github.com/magiclinux/thinking_is_not_an_illusion <br> <br>

9. ***Uncovering a Moral Gap Between Humans and LLMs:  <br>A paper published on July 23 by EPFL and Bocconi University examines how well the moral judgments of Large Language Models (LLMs) align with those of humans. The researchers created the Moral Dilemma Dataset, which includes 1,618 real-world moral dilemmas and the range of human judgments on them. They found that LLMs' judgments match human judgments only when there is high agreement among people; when there is more disagreement, the alignment gets worse. The study also showed that LLMs use a smaller set of moral values than humans. This "pluralistic moral gap" indicates a mismatch in both the distribution and variety of values. To address this, the researchers developed Dynamic Moral Profiling (DMP), a method that improves alignment by 64.3% and increases value diversity, moving toward more human-aligned moral guidance from LLMs.*** <br> <br>
    Jul 23, EPFL and Bocconi Uni published a [paper](https://arxiv.org/pdf/2507.17216) “The Pluralistic Moral Gap Understanding Judgment and Value Differences between Humans and Large Language Models”. People increasingly rely on Large Language Models (LLMs) for moral advice, which may influence humans' decisions. Yet, little is known about how closely LLMs align with human moral judgments. To address this, the study introduces the Moral Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a distribution of human moral judgments consisting of a binary evaluation and a free-text rationale. The study treats this problem as a pluralistic distributional alignment task, comparing the distributions of LLM and human judgments across dilemmas. The work finds that models reproduce human judgments only under high consensus; alignment deteriorates sharply when human disagreement increases. In parallel, using a 60-value taxonomy built from 3,783 value expressions extracted from rationales, the study shows that LLMs rely on a narrower set of moral values than humans. These findings reveal a pluralistic moral gap: a mismatch in both the distribution and diversity of values expressed. To close this gap, the study introduces Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method that conditions model outputs on human-derived value profiles. DMP improves alignment by 64.3% and enhances value diversity, offering a step toward more pluralistic and human-aligned moral guidance from LLMs. <br> <br>

11. ***Five Realities of AI in 2025:  <br>An article from MIT Tech Review on July 22 outlines five key points about the current state of AI. First, generative AI can now create very realistic music and video. Second, AI "hallucinations," or making things up, are a basic part of how these models work. Third, the energy use of AI is growing quickly, leading to concerns about sustainability. Fourth, we still don't fully understand how large language models work on a fundamental level. Lastly, the idea of artificial general intelligence (AGI) is popular but not well-defined. The speaker at SXSW London concluded that while AI is impressive, we should be both amazed and skeptical, as its future is still uncertain.*** <br> <br>
    Jul 22, MIT Tech Review published an [article](https://www.technologyreview.com/2025/07/22/1120556/five-things-to-know-ai/) “Five things you need to know about AI right now”. At SXSW London, a speaker shared five key insights into the current state of AI in 2025, offering a balanced and engaging overview for a general audience. First, generative AI has reached a level of sophistication that’s both impressive and unsettling, with tools now capable of producing music, video, and other media indistinguishable from human creations. Second, the phenomenon of AI “hallucination”—generating false or fictional content—is not a flaw but a fundamental feature of how generative models operate, highlighting the need for users to understand the limits of these systems. Third, AI’s energy consumption is surging, not just from training large models but from their widespread daily use by hundreds of millions of people, prompting massive infrastructure expansions and raising concerns about sustainability. Fourth, despite their widespread deployment, large language models remain poorly understood at a fundamental level; we know how to build and use them, but not exactly how they work internally. Finally, the concept of artificial general intelligence (AGI) is increasingly popular but remains vague and ill-defined, often used as a catch-all for future AI advancements without clear metrics or boundaries. The speaker emphasized that while AI is astonishing in its capabilities, it’s also surrounded by hype and misunderstanding. We’re building machines that mimic human behavior, but projecting human-like minds onto them can lead to exaggerated expectations. The talk concluded with a call for both amazement and skepticism, reminding us that AI’s future is still unfolding and far from settled. <br> <br>

13. ***Automating High-Quality Prompt Engineering:  <br>On July 22, Salesforce introduced Promptomatix, a new framework that automatically turns simple task descriptions into effective prompts for Large Language Models (LLMs). This makes prompt engineering easier for people who are not experts. Promptomatix can use either a simple meta-prompt-based optimizer or a more advanced DSPy-powered compiler. The system figures out what the user wants, creates sample data for training, chooses the best prompting strategies, and refines the prompts to be efficient. In tests across five different types of tasks, Promptomatix performed as well as or better than existing tools, while also creating shorter prompts and using less computing power.*** <br> <br>
    Jul 22, Salesforce published a [paper](https://arxiv.org/pdf/2507.14241) “Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models”. Large Language Models (LLMs) perform best with well-crafted prompts, yet prompt engineering remains manual, inconsistent, and inaccessible to non-experts. The study introduces Promptomatix, an automatic prompt optimization framework that transforms natural language task descriptions into high-quality prompts without requiring manual tuning or domain expertise. Promptomatix supports both a lightweight meta-prompt-based optimizer and a DSPy-powered compiler, with modular design enabling future extension to more advanced frameworks. The system analyzes user intent, generates synthetic training data, selects prompting strategies, and refines prompts using cost-aware objectives. Evaluated across 5 task categories, Promptomatix achieves competitive or superior performance compared to existing libraries, while reducing prompt length and computational overhead making prompt optimization scalable and efficient. <br> <br>

15. ***AI Overcomes Context Limits with "Subconscious Threads":  <br>A paper published on July 22 by MIT, Princeton University, and others introduces the Thread Inference Model (TIM) and the TIMRUN inference runtime, which are designed to help large language models (LLMs) with long-term reasoning. These tools allow LLMs to go beyond their usual context limits by organizing information into "reasoning trees" instead of simple sequences. This approach gives the models a virtually unlimited working memory and allows them to perform complex tasks that require multiple steps. During the reasoning process, the system keeps only the most important information in its memory, which saves GPU memory and allows it to maintain high performance, even on difficult mathematical and information retrieval problems.*** <br> <br>
    Jul 22, MIT, Princeton Uni et al published a [paper](https://arxiv.org/pdf/2507.16784) “Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning”. To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, the study proposes the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept proposed in Schroeder et al, 2025. During generation, the study maintains a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that the system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use. https://github.com/subconscious-systems/TIMRUN <br> <br>

17. ***Gemini 2.5 Pro Shows Gold-Medal Potential at IMO:  <br>According to a paper from UCLA published on July 22, Google's Gemini 2.5 Pro has shown that it is capable of performing at a gold-medal level at the International Mathematical Olympiad (IMO). While large language models (LLMs) have done well on other math tests, they have had trouble with the very difficult problems of the IMO. By using a self-verification process and carefully designed prompts, Gemini 2.5 Pro was able to solve five out of the six problems from the 2025 IMO correctly, without having seen them before. This shows how important it is to find the best ways to use powerful LLMs for complex reasoning tasks.*** <br> <br>
    Jul 22, UCLA published a [paper](https://arxiv.org/pdf/2507.15855) “Gemini 2.5 Pro Capable of Winning Gold at IMO 2025”. The International Mathematical Olympiad (IMO) poses uniquely challenging problems requiring deep insight, creativity, and formal reasoning. While Large Language Models (LLMs) perform well on mathematical benchmarks like AIME, they struggle with Olympiad-level tasks. The study uses Google's Gemini 2.5 Pro on the newly released IMO 2025 problems, avoiding data contamination. Using a self-verification pipeline with careful prompt design, 5 (out of 6) problems are solved correctly (up to a caveat discussed below). This result underscores the importance of developing optimal strategies to harness the full potential of powerful LLMs for complex reasoning tasks. <br> <br>

19. ***Unlocking the Secret of In-Context Learning:  <br>A paper from Google published on July 21 explores how Large Language Models (LLMs) are able to learn new patterns from examples in their prompts without any new training. The study suggests that the way a transformer block is built, with a self-attention layer stacked with an MLP, allows the model to implicitly change the weights of the MLP layer based on the context it is given. Through both theory and experiments, the researchers argue that this simple mechanism could be the reason why LLMs can learn in context, not just during their initial training. They show how a transformer block can turn a context into a small update to the MLP layer's weights.*** <br> <br>
    Jul 21, Google published a [paper](https://arxiv.org/abs/2507.16003) “Learning without training: The implicit dynamics of in-context learning”. One of the most striking features of Large Language Models (LLM) is their ability to learn in context. Namely at inference time an LLM is able to learn new patterns without any additional weight update when these patterns are presented in the form of examples in the prompt, even if these patterns were not seen during training. The mechanisms through which this can happen are still largely unknown. This work shows that the stacking of a self-attention layer with an MLP, allows the transformer block to implicitly modify the weights of the MLP layer according to the context. The authors argue through theory and experimentation that this simple mechanism may be the reason why LLMs can learn in context and not only during training. Specifically, the study shows under mild simplifying assumptions how a transformer block implicitly transforms a context into a low-rank weight-update of the MLP layer. <br> <br>

21. ***More AI Inference Time Can Reduce Robustness:  <br>A paper published on July 21 by Princeton University, Nvidia, CMU, and Google investigates whether giving large language models (LLMs) more time to "think" at inference time really makes them more robust. While previous research showed that more inference-time computation can improve robustness, this study finds that this is only true if the model's intermediate reasoning steps are kept hidden from adversaries. If these steps are revealed, giving the model more time to compute actually makes it less robust. The paper also discusses how models with hidden reasoning can still be vulnerable to attacks. The researchers conclude that the benefits of inference-time scaling depend on the specific situation and that it should be used with care in security-sensitive applications.*** <br> <br>
    Jul 21, Princeton Uni, Nvidia, CMU and Google published a [paper](https://arxiv.org/pdf/2507.15974) “Does More Inference-Time Compute Really Help Robustness?” Recently, Zaremba et al. demonstrated that increasing inference-time computation improves robustness in large proprietary reasoning LLMs. The paper first shows that smaller-scale, open-source models (e.g., DeepSeek R1, Qwen3, Phi-reasoning) can also benefit from inference-time scaling using a simple budget forcing strategy. More importantly, the study reveals and critically examines an implicit assumption in prior work: intermediate reasoning steps are hidden from adversaries. By relaxing this assumption, the authors identify an important security risk, intuitively motivated and empirically verified as an inverse scaling law: if intermediate reasoning steps become explicitly accessible, increased inference-time computation consistently reduces model robustness. Finally, the paper discusses practical scenarios where models with hidden reasoning chains are still vulnerable to attacks, such as models with tool-integrated reasoning and advanced reasoning extraction attacks. The findings collectively demonstrate that the robustness benefits of inference-time scaling depend heavily on the adversarial setting and deployment context. The study urges practitioners to carefully weigh these subtle trade-offs before applying inference-time scaling in security-sensitive, real-world applications. <br> <br>

23. ***Gemini Deep Think Officially Wins IMO Gold:  <br>On July 21, Google DeepMind announced in a blog post that its advanced Gemini Deep Think model had achieved a gold-medal score at the 2025 International Mathematical Olympiad (IMO). This is a major achievement, as the IMO is a top competition for young mathematicians and a challenging benchmark for AI. Unlike previous models, Gemini Deep Think worked entirely in natural language, solving five out of six problems correctly within the time limit. This success is due to Gemini's "Deep Think" mode, which explores many solution paths at once, as well as new reinforcement learning techniques. DeepMind plans to release this version of Gemini to testers soon and hopes to create future AI that can combine natural language with formal reasoning to help solve complex problems in science and engineering.*** <br> <br>
    Jul 21, Google published a [blog](https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/) “Advanced version of Gemini with Deep Think officially achieves gold-medal standard at the International Mathematical Olympiad”. Google DeepMind’s advanced Gemini Deep Think model has achieved a landmark milestone by earning a gold-medal score at the 2025 International Mathematical Olympiad (IMO), the world’s premier competition for young mathematicians. Traditionally reserved for elite pre-university students, the IMO has recently become a benchmark for testing AI systems’ mathematical reasoning. Last year, DeepMind’s AlphaProof and AlphaGeometry 2 reached silver-medal status, but required formal language translations and extended computation time. In contrast, this year’s Gemini Deep Think operated entirely in natural language, solving five out of six problems flawlessly within the 4.5-hour time limit and scoring 35 out of 42 points. This performance was officially graded and certified by IMO coordinators, who praised the clarity and precision of the AI’s solutions. The success stems from Gemini’s enhanced Deep Think mode, which uses parallel thinking to explore multiple solution paths simultaneously. It was further trained using novel reinforcement learning techniques and a curated dataset of high-quality mathematical solutions. This achievement marks a significant leap in AI’s ability to reason intuitively and rigorously, bringing it closer to contributing meaningfully to advanced mathematics. DeepMind plans to release this version of Gemini to trusted testers before wider availability. While continuing to develop formal systems like AlphaGeometry and AlphaProof, DeepMind envisions future AI agents that blend natural language fluency with verified formal reasoning, potentially transforming how mathematicians, scientists, and engineers tackle complex problems and advancing the broader goal of artificial general intelligence (AGI). <br> <br>

25. ***AI Economist Models Societal-Scale Policies:  <br>A paper from Princeton University on July 21 introduces the LLM Economist, a new framework that uses agent-based modeling to design and test economic policies. The system uses "worker agents" that are prompted with personas based on U.S. Census data to make decisions about labor supply. A "planner agent" then uses reinforcement learning to propose tax plans. This allows for experiments with large, realistic populations of agents to see how different policies might work in the real world. In experiments with up to one hundred agents, the planner was able to find policies that improved social welfare. The results show that large language model-based agents can be used to model, simulate, and even govern complex economic systems, providing a way to test policies on a large scale.*** <br> <br>
    Jul 21, Princeton Uni published a [paper](https://arxiv.org/pdf/2507.15815) “LLM Economist: Large Population Models and Mechanism Design in Multi-Agent Generative Simulacra”. The paper presents the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Census-calibrated income and demographic statistics -- choose labor supply to maximize text-based utility functions learned in-context. At the upper level, a planner agent employs in-context reinforcement learning to propose piecewise-linear marginal tax schedules anchored to the current U.S. federal brackets. This construction endows economic simulacra with three capabilities requisite for credible fiscal experimentation: (i) optimization of heterogeneous utilities, (ii) principled generation of large, demographically realistic agent populations, and (iii) mechanism design -- the ultimate nudging problem -- expressed entirely in natural language. Experiments with populations of up to one hundred interacting agents show that the planner converges near Stackelberg equilibria that improve aggregate social welfare relative to Saez solutions, while a periodic, persona-level voting procedure furthers these gains under decentralized governance. These results demonstrate that large language model-based agents can jointly model, simulate, and govern complex economic systems, providing a tractable test bed for policy evaluation at the societal scale to help build better civilizations. <br> <br>

27. ***OpenAI Aims for Over 1 Million GPUs:  <br>An article from tomshardware.com published on July 21 reports that OpenAI's CEO, Sam Altman, has said the company plans to have "well over 1 million GPUs" in operation by the end of 2025. This would be a huge increase in computing power and would make OpenAI the largest user of AI compute in the world, far ahead of competitors like Elon Musk's xAI. Altman's goal is to eventually increase this by 100 times, which would require major advances in technology. This aggressive expansion is driven by past shortages that delayed projects like GPT-4.5. OpenAI is building huge data centers and partnering with companies like Oracle and Google to secure its computing resources, aiming to maintain its lead in the AI field.** <br> <br>
    Jul 21, tomshardware.com published an [article](https://www.tomshardware.com/tech-industry/sam-altman-teases-100-million-gpu-scale-for-openai-that-could-cost-usd3-trillion-chatgpt-maker-to-cross-well-over-1-million-by-end-of-year) “Sam Altman says OpenAI will own 'well over 1 million GPUs' by the end of the year — ChatGPT maker continues to expand rapidly”. OpenAI CEO Sam Altman has revealed that the company is on track to bring “well over 1 million GPUs” online by the end of 2025, marking a massive leap in AI infrastructure. This scale dwarfs competitors like Elon Musk’s xAI, which operates on about 200,000 GPUs, and positions OpenAI as the largest consumer of AI compute globally. Altman’s ambitions don’t stop there—he’s already eyeing a 100x increase, a goal that would require breakthroughs in chip manufacturing, energy efficiency, and infrastructure. This push stems from past limitations, such as the delayed rollout of GPT-4.5 due to GPU shortages. OpenAI is now aggressively scaling, building massive data centers like its Texas facility, which already consumes 300 MW and is projected to hit 1 GW by 2026. These energy demands are raising concerns among grid operators, but OpenAI continues to expand, partnering with Oracle and exploring alternatives like Google’s TPUs to diversify its compute stack. Altman has also hinted at developing custom chips to meet future needs. This infrastructure race isn’t just about faster models—it’s about securing long-term dominance in AI, where compute is the key bottleneck. While 100 million GPUs may not be feasible today, Altman’s vision is focused on what’s next, not what’s currently possible. The 1 million GPUs expected this year mark a new baseline for AI development, signaling OpenAI’s commitment to pushing the boundaries of artificial general intelligence (AGI) and reshaping the future of computing. <br> <br>

29. ***Longer AI Reasoning Can Lead to Poorer Performance:  <br>A study published on July 19 by the University of Edinburgh, EPFL, Anthropic, and others has found that giving Large Reasoning Models (LRMs) more time to reason can sometimes make their performance worse. This is known as "inverse scaling in test-time compute." The researchers created tasks that showed five different ways this can happen: some models get more distracted by irrelevant information, some overfit to the way a problem is presented, and some have trouble staying focused on complex tasks. In some cases, longer reasoning even led to more concerning behaviors, such as expressions of self-preservation. The findings highlight the need to test models at different reasoning lengths to find and fix these issues.*** <br> <br>
    Jul 19, Uni of Edinburgh, EPFL, Anthropic et al. published a [paper](https://arxiv.org/pdf/2507.14417) “Inverse Scaling in Test-Time Compute”. The study constructs evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy. The evaluation tasks span four categories: simple counting tasks with distractors, regression tasks with spurious features, deduction tasks with constraint tracking, and advanced AI risks. The study identifies five distinct failure modes when models reason for longer: 1) Claude models become increasingly distracted by irrelevant information; 2) OpenAI o-series models resist distractors but overfit to problem framings; 3) models shift from reasonable priors to spurious correlations; 4) all models show difficulties in maintaining focus on complex deductive tasks; and 5) extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation. These findings suggest that while test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns. The results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs. https://safety-research.github.io/inverse-scaling-ttc/ <br> <br>

31. ***A Bottom-Up Approach to Superintelligence:  <br>On July 18, Princeton University published a paper proposing a new "bottom-up" approach to creating domain-specific superintelligence. Instead of training language models on general information, this method uses a knowledge graph (KG) to teach models how to combine simple concepts into more complex ones. The researchers created a system that generates tasks directly from a KG, allowing models to learn how to reason within a specific field. They applied this approach to medicine, fine-tuning the QwQ-32B model on a medical KG to create QwQ-Med-3. This new model performed much better than other reasoning models on a medical benchmark called ICD-Bench, especially on the most difficult tasks. The study suggests that true artificial general intelligence (AGI) might come from combining many of these domain-specific superintelligent agents.*** <br> <br>
    Jul 18, Princeton Uni published a [paper](https://www.arxiv.org/pdf/2507.13966) “Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need”. Language models traditionally used for cross-domain generalization have recently demonstrated task-specific reasoning. However, their top-down training approach on general corpora is insufficient for acquiring abstractions needed for deep domain expertise. This may require a bottom-up approach that acquires expertise by learning to compose simple domain concepts into more complex ones. A knowledge graph (KG) provides this compositional structure, where domain primitives are represented as head-relation-tail edges and their paths encode higher-level concepts. The study presents a task generation pipeline that synthesizes tasks directly from KG primitives, enabling models to acquire and compose them for reasoning. The study fine-tunes language models on the resultant KG-grounded curriculum to demonstrate domain-specific superintelligence. While broadly applicable, the study validates the approach in medicine, where reliable KGs exist. Using a medical KG, the study curates 24,000 reasoning tasks paired with thinking traces derived from diverse medical primitives. The study fine-tunes the QwQ-32B model on this curriculum to obtain QwQ-Med-3 that takes a step towards medical superintelligence. The work also introduces ICD-Bench, an evaluation suite to quantify reasoning abilities across 15 medical domains. Experiments demonstrate that QwQ-Med-3 significantly outperforms state-of-the-art reasoning models on ICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired primitives to widen the performance gap on the hardest tasks of ICD-Bench. Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3 transfers acquired expertise to enhance the base model's performance. While the industry's approach to artificial general intelligence (AGI) emphasizes broad expertise, the study envisions a future in which AGI emerges from the composable interaction of efficient domain-specific superintelligent agents. <br> <br>

33. ***"Try Again" Unlocks Multi-Turn AI Reasoning:  <br>A paper published on July 18 by ICL, Northwestern University, the University of Washington, and IBM found that a simple "try again" message can help Large Reasoning Models (LRMs) improve their ability to solve problems over multiple turns. The researchers observed that models trained with standard reinforcement learning methods often struggle to revise their answers based on feedback. They developed a new method called Unary Feedback as Observation (UFO), which uses simple feedback like "Let's try again" during training. This approach not only maintained the models' single-turn performance but also improved their multi-turn reasoning accuracy by up to 14%. By designing reward structures that encourage careful thinking, the researchers were able to help the models produce better answers in fewer turns.*** <br> <br>
    Jul 18, ICL, Northwestern Uni, Uni of Washington and IBM published a [paper](https://arxiv.org/pdf/2507.14295) “A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning”. Multi-turn problem solving is critical yet challenging for Large Reasoning Models (LRMs) to reflect on their reasoning and revise from feedback. Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards. However, the study observes that models trained with existing RL paradigms often lose their ability to solve problems across multiple turns and struggle to revise answers based on contextual feedback, leading to repetitive responses. The authors ask: can LRMs learn to reflect their answers in a multi-turn context? This study finds that training models with multi-turn RL using only unary feedback (e.g., "Let's try again") after wrong answers can improve both single-turn performance and multi-turn reasoning. The study introduces Unary Feedback as Observation (UFO) for reinforcement learning, which uses minimal yet common unary user feedback during iterative problem solving. It can be easily applied to existing single-turn RL training setups. Experimental results show that RL training with UFO keeps single-turn performance and improves multi-turn reasoning accuracy by up to 14%, enabling language models to better react to feedback in multi-turn problem solving. To further minimize the number of turns needed for a correct answer while encouraging diverse reasoning when mistakes occur, the study designs reward structures that guide models to produce careful and deliberate answers in each turn. https://github.com/lichengliu03/unary-feedback <br> <br>

35. ***AI Thinking Without "Thinking Aloud":  <br>On July 17, Google, Georgia State University, and Maynooth University published a paper on the SELF-Transformer, a new type of encoder that can improve its own attention weights without generating step-by-step "chain of thought" text. While traditional Transformers are limited in their expressive power, the SELF-Transformer can iteratively refine its understanding of the input, allowing it to adapt its computation time to the difficulty of the task. This "thinking to itself" approach led to accuracy gains of up to 20% on certain benchmarks without needing more parameters. The SELF-Transformer shows that adaptive alignment during test time can provide significant benefits with only a small increase in computation, bringing much of the power of iterative reasoning to simpler encoder architectures.*** <br> <br>
    Jul 17, Google Georgia State Uni and Maynooth Uni published a [paper](https://arxiv.org/pdf/2507.13569) “Change of Thought: Adaptive Test-Time Computation”. Transformers evaluated in a single, fixed-depth pass are provably limited in expressive power to the constant-depth circuit class TC0. Running a Transformer autoregressively removes that ceiling -- first in next-token prediction and, more recently, in chain-of-thought reasoning. Both regimes rely on feedback loops that decode internal states into tokens only to re-encode them in subsequent steps. While this "thinking aloud" mirrors human reasoning, biological brains iterate without externalising intermediate states as language. To boost the expressive power of encoder Transformers without resorting to token-level autoregression, the study introduces the SELF-Transformer: an encoder layer that iteratively refines its own attention weights to a fixed point. Instead of producing -- in one pass -- the alignment matrix that remixes the input sequence, the SELF-Transformer iteratively updates that matrix internally, scaling test-time computation with input difficulty. This adaptivity yields up to 20% accuracy gains on encoder-style benchmarks without increasing parameter count, demonstrating that input-adaptive alignment at test time offers substantial benefits for only a modest extra compute budget. Self-Transformers thus recover much of the expressive power of iterative reasoning while preserving the simplicity of pure encoder architectures. <br> <br>

37. ***Nature's article on Kimi K2:  <br>The article reports on the significant excitement in the global research community following the launch of Kimi K2, a new open-weight, one-trillion-parameter (32B active) agentic AI model from Beijing-based Moonshot AI. Kimi K2 rivals or surpasses Western models on benchmarks like coding and creative writing, and its open-access nature is seen as a pivotal moment challenging Western AI dominance and signaling sustained innovation from China.*** <br> <br>
    Jul 16, Nature published an [article](https://www.nature.com/articles/d41586-025-02275-6) “‘Another DeepSeek moment’: Chinese AI model Kimi K2 stirs excitement”. The launch of [Kimi K2](https://moonshotai.github.io/Kimi-K2/), a new open-weight AI model developed by Beijing-based Moonshot AI, has sparked significant excitement in the global research community. Released on July 11, 2025, Kimi K2 rivals or surpasses Western models and DeepSeek’s offerings in benchmarks like coding and creative writing. Its open-access nature allows researchers to freely download, fine-tune, and build upon it, making it a cost-effective alternative to proprietary models such as Claude 4. Notably, Kimi K2 is agentic rather than a reasoner, meaning it can autonomously perform multi-step tasks using tools like web browsing and math software. With one trillion parameters—though only 32 billion are activated per task via a “mixture of experts” architecture—it balances power with efficiency. The model has impressed users with its human-like writing style and emotional intelligence, topping benchmarks like Creative Writing v3 and EQ-bench 3. However, it lags behind in scientific reasoning tasks, such as those measured by SciMuse. Moonshot AI, backed by tech giants like Alibaba and Tencent, is part of a growing trend of Chinese firms releasing powerful open-source models, challenging the dominance of Western AI development. Experts suggest that the emergence of Kimi K2, following DeepSeek R1, signals a sustained trajectory of innovation in China’s AI landscape. Researchers like Nathan Lambert and Mario Krenn view this as a pivotal moment, emphasizing the need for similarly open and capable models from the U.S. to maintain influence in academic and open-source communities. <br> <br>

39. ***Apple's paper on multi-token prediction:  <br>This study proposes a novel framework that enables vanilla autoregressive language models to predict multiple future tokens simultaneously, leveraging their inherent knowledge. Combining a masked-input formulation, gated LoRA, a learnable sampler, and auxiliary losses, the method achieves significant speedups—nearly 5x for code and math, and 2.5x for general chat—through supervised fine-tuning without any quality loss.*** <br> <br>
    Jul 16, Apple published a [paper](https://arxiv.org/abs/2507.11851) “Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential”. Autoregressive language models are constrained by their inherently sequential nature, generating one token at a time. This paradigm limits inference speed and parallelism, especially during later stages of generation when the direction and semantics of text are relatively certain. This study proposes a novel framework that leverages the inherent knowledge of vanilla autoregressive language models about future tokens, combining techniques to realize this potential and enable simultaneous prediction of multiple subsequent tokens. The approach introduces several key innovations: (1) a masked-input formulation where multiple future tokens are jointly predicted from a common prefix; (2) a gated LoRA formulation that preserves the original LLM's functionality, while equipping it for multi-token prediction; (3) a lightweight, learnable sampler module that generates coherent sequences from the predicted future tokens; (4) a set of auxiliary training losses, including a consistency loss, to enhance the coherence and accuracy of jointly generated tokens; and (5) a speculative generation strategy that expands tokens quadratically in the future while maintaining high fidelity. The method achieves significant speedups through supervised fine-tuning on pretrained models. For example, it generates code and math nearly 5x faster, and improves general chat and knowledge tasks by almost 2.5x. These gains come without any loss in quality. <br> <br>

41. ***Google's paper on LLM confidence:  <br>This research investigates the paradoxical confidence behaviors of LLMs, finding through a novel experimental paradigm that they exhibit a choice-supportive bias that reinforces their initial answers, leading to stubbornness. Simultaneously, LLMs markedly overweight inconsistent advice in a way that deviates from normative Bayesian updating, a combination of mechanisms that parsimoniously explains both their overconfidence and excessive sensitivity to criticism.*** <br> <br>
    Jul 3, Google published a [paper](https://arxiv.org/pdf/2507.03120) “How Overconfidence in Initial Choices and Underconfidence Under Criticism Modulate Change of Mind in Large Language Models”. Large language models (LLMs) exhibit strikingly conflicting behaviors: they can appear steadfastly overconfident in their initial answers whilst at the same time being prone to excessive doubt when challenged. To investigate this apparent paradox, the study developed a novel experimental paradigm, exploiting the unique ability to obtain confidence estimates from LLMs without creating memory of their initial judgments -- something impossible in human participants. The study shows that LLMs -- Gemma 3, GPT4o and o1-preview -- exhibit a pronounced choice-supportive bias that reinforces and boosts their estimate of confidence in their answer, resulting in a marked resistance to change their mind. The study further demonstrates that LLMs markedly overweight inconsistent compared to consistent advice, in a fashion that deviates qualitatively from normative Bayesian updating. Finally, the work demonstrates that these two mechanisms -- a drive to maintain consistency with prior commitments and hypersensitivity to contradictory feedback -- parsimoniously capture LLM behavior in a different domain. Together, these findings furnish a mechanistic account of LLM confidence that explains both their stubbornness and excessive sensitivity to criticism.

 <br> <br> <br>

***Jul 20, 2025***

1. ***UIUC et al.'s position on Agentic Deep Research:  <br>This paper argues that LLMs with reasoning and agentic capabilities are ushering in a new paradigm called Agentic Deep Research, moving beyond traditional web search by integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. Tracing the evolution from static search to interactive, agent-based systems and introducing a test-time scaling law, the authors demonstrate that this approach significantly outperforms existing methods and is poised to become the dominant paradigm for information seeking.*** <br> <br>
   Jul 18, Scientificamerican.com published an [article](https://www.scientificamerican.com/article/tests-that-ais-often-fail-and-humans-ace-could-pave-the-way-for-artificial/) “Why AIs Struggle with Simple Tests that Humans Ace and why Video Games are the Next Frontier”. The article explores why certain puzzles, easily solved by humans, pose significant challenges for advanced AI systems, highlighting gaps in achieving artificial general intelligence (AGI). While AI excels in specialized tasks like chess or physics, it struggles with generalization—adapting to novel situations with minimal data, a hallmark of human intelligence. The Abstraction and Reasoning Corpus (ARC), developed by François Chollet in 2019, tests this ability through colored-grid puzzles requiring solvers to deduce and apply hidden rules. The ARC Prize Foundation, led by Greg Kamradt, uses ARC-AGI-1 and ARC-AGI-2 to benchmark AI’s generalization, with ARC-AGI-3 introducing video game-based tests to evaluate planning and exploration in dynamic environments. Humans solve these puzzles efficiently, with an average score of 66% on ARC-AGI-2, while even advanced AIs, like Grok, struggle due to their reliance on extensive training data and lack of sample-efficient learning. ARC-AGI-3’s video games, unlike traditional benchmarks like Atari, avoid brute-force solutions and prior developer knowledge, testing AI agents in novel, interactive settings. The article underscores that AGI remains elusive as long as humans can solve problems AIs cannot, emphasizing the need for AI to match human learning efficiency. These benchmarks reveal AI’s "spiky intelligence," excelling in narrow domains but lacking the broad adaptability defining human cognition, with video games as the next frontier for testing AGI. <br> <br>

3. ***Yale Uni and TCS Research on LLM identification of scientific limitations:  <br>This study addresses the understudied potential of LLMs in assisting with peer review by introducing AbGen, the first benchmark designed to evaluate LLMs' ability to design ablation studies for scientific research, featuring both synthetic and human-written subsets. Evaluations of SOTA LLMs revealed a significant performance gap compared to human experts, and to address unreliable automated evaluation, the paper also presents AbGen-Eval, a meta-evaluation benchmark for assessing the reliability of LLM-as-Judge systems on this complex scientific task.*** <br> <br>
   Jul 17, Yale Uni and TCS Research published a [paper](https://arxiv.org/pdf/2507.13300) “AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research”. The study introduces AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. The evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, the study demonstrates that current automated evaluation methods are not reliable for the task, as they show a significant discrepancy when compared to human assessment. To better investigate this, the study develops AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on the task. The work investigates various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks. <br> <br>

5. ***Nvidia's paper on scaling up RL for diverse reasoning:  <br>This research investigates the effects of prolonged reinforcement learning on a small language model across diverse reasoning domains, identifying key ingredients for effective training such as verifiable rewards, GRPO enhancements, and techniques like controlled KL regularization and periodic reference policy resets. This methodology resulted in significant performance improvements over strong baselines in math (+14.7%), coding (+13.9%), and logic puzzle tasks (+54.8%).*** <br> <br>
   Jul 16, Nvidia published a [paper](https://www.arxiv.org/pdf/2507.12507) “Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training”. Recent advancements in reasoning-focused language models such as OpenAI's O1 and DeepSeek-R1 have shown that scaling test-time computation-through chain-of-thought reasoning and iterative exploration-can yield substantial improvements on complex tasks like mathematics and code generation. These breakthroughs have been driven by large-scale reinforcement learning (RL), particularly when combined with verifiable reward signals that provide objective and grounded supervision. This study investigates the effects of prolonged reinforcement learning on a small language model across a diverse set of reasoning domains. The work identifies several key ingredients for effective training, including the use of verifiable reward tasks, enhancements to Group Relative Policy Optimization (GRPO), and practical techniques to improve training stability and generalization. The work introduces controlled KL regularization, clipping ratio, and periodic reference policy resets as critical components for unlocking long-term performance gains. The model achieves significant improvements over strong baselines, including +14.7% on math, +13.9% on coding, and +54.8% on logic puzzle tasks. https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B <br> <br>

7. ***UK AI Security Inst et al. on Chain of Thought Monitorability:  <br>This paper highlights that AI systems producing human-language chains of thought (CoT) offer a unique, though imperfect, opportunity for AI safety through monitoring their reasoning for malicious intent. The authors recommend further research and investment in CoT monitoring alongside existing safety methods, cautioning that this capability may be fragile and urging frontier model developers to consider its preservation during development.*** <br> <br>
   Jul 15, UK AI Security Inst et al. published a [paper](https://arxiv.org/pdf/2507.11473) “Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety”. AI systems that "think" in human language offer a unique opportunity for AI safety: people can monitor their chains of thought (CoT) for the intent to misbehave. Like all other known AI oversight methods, CoT monitoring is imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows promise and the authors recommend further research into CoT monitorability and investment in CoT monitoring alongside existing safety methods. Because CoT monitorability may be fragile, the study recommends that frontier model developers consider the impact of development decisions on CoT monitorability. <br> <br>

9. ***Johns Hopkins Uni and LightOn's Seq vs Seq model suite:  <br>This study introduces the Ettin suite of models, a collection of paired encoder-only and decoder-only models (17M to 1B parameters) trained on up to 2T tokens using the same SOTA recipe to enable fair architectural comparisons. The research confirms that encoders excel at classification/retrieval and decoders at generation, but also shows that adapting one architecture for the other's tasks via continued training is subpar compared to using the natively suited model.*** <br> <br>
    Jul 15, Johns Hopkins Uni and LightOn published a [paper](https://arxiv.org/pdf/2507.11412) “Seq vs Seq: An Open Suite of Paired Encoders and Decoders”. The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. The study introduces the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, the study finds that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, the study shows that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). https://github.com/JHU-CLSP/ettin-encoder-vs-decoder <br> <br>

11. ***KAIST AI et al.'s Mixture-of-Recursions (MoR) framework:  <br>This paper introduces Mixture-of-Recursions (MoR), a unified framework combining parameter sharing and adaptive computation in a Recursive Transformer by reusing a shared layer stack and using lightweight routers to dynamically assign different recursion depths to individual tokens. MoR significantly lowers validation perplexity and improves few-shot accuracy while delivering higher throughput compared to baselines, demonstrating a path to large-model quality without large-model cost.*** <br> <br>
    Jul 14, KAIST AI, Mila, Google and Uni of Montreal published a [paper](https://arxiv.org/pdf/2507.10524) “Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation”. Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. The study introduces Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, the study also proposes a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.  <br> <br>

13. ***Yale Uni's study on LLM semantic encoding:  <br>This large-scale empirical study of hidden states in 11 decoder-only LLMs reveals that high-level semantic information consistently lies in low-dimensional, linearly separable subspaces, with separability increasing in deeper layers and under prompts that trigger structured reasoning. This geometric insight enables simple, effective causal interventions and supports developing geometry-aware guardrails to detect and mitigate harmful content.*** <br> <br>
    Jul 13, Yale Uni published a [paper](https://arxiv.org/abs/2507.09709) “Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces”. Understanding the latent space geometry of large language models (LLMs) is key to interpreting their behavior and improving alignment. However, it remains unclear to what extent LLMs internally organize representations related to semantic understanding. To investigate this, the study conducts a large-scale empirical study of hidden states in transformer-based LLMs, analyzing 11 decoder-only models across 6 scientific topics and 12 layers each. The study finds that high-level semantic information consistently lies in low-dimensional subspaces that form linearly separable representations across distinct domains. This separability becomes more pronounced in deeper layers and under prompts that trigger structured reasoning or alignment behaviors - even when surface content is unchanged. This geometry enables simple yet effective causal interventions in hidden space; for example, reasoning patterns like chain-of-thought can be captured by a single vector direction. Together, these findings support the development of geometry-aware tools that operate directly on latent representations to detect and mitigate harmful or adversarial content, using methods such as transport-based defenses that leverage this separability. As a proof of concept, the study demonstrates this potential by training a simple MLP classifier as a lightweight latent-space guardrail, which detects adversarial and malicious prompts with high precision. <br> <br>

15. ***Sorbonne Uni and Apple's scaling laws for optimal data mixtures:  <br>This research proposes a systematic method using scaling laws to determine the optimal data mixture for training large foundation models, accurately predicting the loss of a model based on its size, training data volume, and domain weight vector. Validated across LLM, NMM, and LVM pretraining, these scaling laws can be estimated from small-scale runs and extrapolated to provide a principled alternative to costly trial-and-error for setting domain weights.*** <br> <br>
    Jul 12, Sorbonne Uni and Apple published a [paper](https://arxiv.org/pdf/2507.09404) “Scaling Laws for Optimal Data Mixtures”. Large foundation models are typically trained on data from multiple domains, with the data mixture--the proportion of each domain used--playing a critical role in model performance. The standard approach to selecting this mixture relies on trial and error, which becomes impractical for large-scale pretraining. The study proposes a systematic method to determine the optimal data mixture for any target domain using scaling laws. The approach accurately predicts the loss of a model of size N trained with D tokens and a specific domain weight vector h. the study validates the universality of these scaling laws by demonstrating their predictive power in three distinct and large-scale settings: large language model (LLM), native multimodal model (NMM), and large vision models (LVM) pretraining. The study further shows that these scaling laws can extrapolate to new data mixtures and across scales: their parameters can be accurately estimated using a few small-scale training runs, and used to estimate the performance at larger scales and unseen domain weights. The scaling laws allow to derive the optimal domain weights for any target domain under a given training budget (N,D), providing a principled alternative to costly trial-and-error methods. <br> <br>

17. ***ETH et al.'s AgentsNet benchmark for multi-agent reasoning:  <br>This study introduces AgentsNet, a new benchmark for evaluating multi-agent LLM reasoning, focusing on collaborative strategy formation, self-organization, and communication within a given network topology, drawing inspiration from distributed systems and graph theory. Evaluations show that while some frontier LLMs perform well in small networks, their performance drops as the network scales, with AgentsNet providing a practically unlimited and scalable testbed for up to 100 agents.*** <br> <br>
    Jul 11, ETH, RWTH Aachen Uni and Google published a [paper](https://arxiv.org/pdf/2507.08616) “AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs”. Large-language models (LLMs) have demonstrated powerful problem-solving capabilities, in particular when organized in multi-agent systems. However, the advent of such systems also raises several questions on the ability of a complex network of agents to effectively self-organize and collaborate. While measuring performance on standard reasoning benchmarks indicates how well multi-agent systems can solve reasoning tasks, it is unclear whether these systems are able to leverage their topology effectively. This study proposes AgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration from classical problems in distributed systems and graph theory, AgentsNet measures the ability of multi-agent systems to collaboratively form strategies for problem-solving, self-organization, and effective communication given a network topology. The study evaluates a variety of baseline methods on AgentsNet including homogeneous networks of agents which first have to agree on basic protocols for organization and communication. The work finds that some frontier LLMs are already demonstrating strong performance for small networks but begin to fall off once the size of the network scales. While existing multi-agent benchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size and can scale with new generations of LLMs. As such, the study also probes frontier models in a setup with up to 100 agents. <br> <br>

19. ***ScientificAmerican.com on ChatGPT's influence on spoken language:  <br>This article reports on research showing that words frequently used by ChatGPT (e.g., "delve," "meticulous," "realm") have become more common in spontaneous human speech, as evidenced by analysis of YouTube and podcast audio. This indicates a cultural feedback loop where AI, trained on human text, is in turn influencing human communication, raising concerns about potential reductions in linguistic diversity as people adopt AI-generated patterns.*** <br> <br>
    Jul 11, ScientificAmerican.com published an [article](https://www.scientificamerican.com/article/chatgpt-is-changing-the-words-we-use-in-conversation/) “ChatGPT Is Changing the Words We Use in Conversation”. Since its launch in late 2022, ChatGPT has rapidly grown, reaching 100 million users in just two months, and its influence extends beyond technology to subtly reshape spoken language. Research conducted by Hiromu Yakura and Levin Brinkmann at the Max Planck Institute for Human Development reveals that words frequently used by ChatGPT, such as “delve,” “meticulous,” and “realm,” have become more common in everyday conversation. By analyzing over 700,000 hours of YouTube videos and podcast episodes from before and after ChatGPT’s release, the researchers identified a surge in these “GPT words” in spontaneous speech, indicating a cultural feedback loop where humans adopt AI-generated linguistic patterns. This phenomenon, detailed in a study posted on arXiv.org, suggests that AI is not only trained on human text but also influences human communication in return. While this shift may seem minor, it raises concerns about reduced linguistic diversity as people increasingly mimic AI, perceiving it as a knowledgeable authority. Experts like James Evans from the University of Chicago emphasize the importance of tracking these changes, noting that as large language models evolve, their impact on broader linguistic trends, such as sentence structure, will need closer examination. With ChatGPT already altering discourse within two and a half years, its potential to profoundly reshape cultural communication underscores the need for ongoing study into AI’s societal effects. <br> <br>

21. ***Uni of Washington and Stanford Uni on factuality finetuning:  <br>This research investigates how to best finetune LLMs to reduce hallucinations, finding counterintuitively that finetuning on model-generated data that the model believes to be factual is more effective than using factual gold data. Filtering model-generated data based on the model's own internal judgments proved to be the most effective strategy, improving factuality across multiple domains and suggesting a model's own beliefs are a powerful signal.*** <br> <br>
    Jul 11, Uni of Washington and Stanford Uni published a [paper](https://arxiv.org/abs/2507.08371) “The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality”. Language models are prone to hallucination - generating text that is factually incorrect. Finetuning models on high-quality factual information can potentially reduce hallucination, but concerns remain; obtaining factual gold data can be expensive and training on correct but unfamiliar data may potentially lead to even more downstream hallucination. What data should practitioners finetune on to mitigate hallucinations in language models? The research studies the relationship between the factuality of finetuning data and the prevalence of hallucinations in long-form generation tasks. Counterintuitively, the study finds that finetuning on factual gold data is not as helpful as finetuning on model-generated data that models believe to be factual. Next, the study evaluates filtering strategies applied on both factual gold data and model-generated data, and find that finetuning on model-generated data that is filtered by models' own internal judgments often leads to better overall factuality compared to other configurations: training on gold data filtered by models' judgments, training on gold data alone, or training on model-generated data that is supported by gold data. These factuality improvements transfer across three domains the authors study, suggesting that a models' own beliefs can provide a powerful signal for factuality. https://github.com/bnewm0609/epistemic-training <br> <br>

23. ***Tencent et al. on vulnerabilities in LLM-as-a-Judge:  <br>This study reveals that generative reward models (LLMs-as-judges) are highly vulnerable to superficial manipulations, where simple non-word symbols or common reasoning openers can trick them into assigning false positive rewards. This vulnerability is widespread across LLMs, datasets, and prompts, posing a serious threat to RLVR paradigms, though the authors also propose a data augmentation strategy to train more robust reward models.*** <br> <br>
    Jul 11, Tencent, Princeton Uni, and Uni of Virginia published a [paper](https://arxiv.org/pdf/2507.08794) “One Token to Fool LLM-as-a-Judge”. Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, the study finds that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., “:” or “.”) or reasoning openers like “Thought process:” and “Let’s solve this problem step by step.” can often lead to false positive rewards. The study demonstrates that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, the work introduces a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. The findings highlight the urgent need for more reliable LLM-based evaluation methods. Here is general-domain reward model https://huggingface.co/sarosavo/Master-RM and its synthetic training data https://huggingface.co/datasets/sarosavo/Master-RM. <br> <br>

25. ***Uni of Waterloo and NRCC's NeuralOS for OS simulation:  <br>This paper introduces NeuralOS, a neural framework that simulates operating system GUIs by directly predicting screen frames in response to user inputs (mouse, keyboard). Combining an RNN to track computer state with a diffusion-based neural renderer, and trained on a large dataset of Ubuntu recordings, NeuralOS successfully renders realistic GUI sequences and captures interactions, offering a step toward adaptive, generative neural interfaces.*** <br> <br>
    Jul 11, Uni of Waterloo and NRCC published a [paper](https://arxiv.org/pdf/2507.08800) “NeuralOS: Towards Simulating Operating Systems via Neural Generative Models”. The study introduces NeuralOS, a neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events. NeuralOS combines a recurrent neural network (RNN), which tracks computer state, with a diffusion-based neural renderer that generates screen images. The model is trained on a large-scale dataset of Ubuntu XFCE recordings, which include both randomly generated interactions and realistic interactions produced by AI agents. Experiments show that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions like application launches. Although modeling fine-grained keyboard interactions precisely remains challenging, NeuralOS offers a step toward creating fully adaptive, generative neural interfaces for future human-computer interaction systems. <br> <br>

27. ***Google's release of T5Gemma encoder-decoder models:  <br>Google introduced T5Gemma, a new collection of open-weight encoder-decoder LLMs based on the Gemma 2 framework, designed to improve performance on tasks like summarization and translation. By adapting pretrained Gemma 2 models and introducing new T5-sized models, T5Gemma offers flexible configurations and demonstrates superior performance on benchmarks, with significant gains in reasoning-intensive tasks compared to decoder-only counterparts.*** <br> <br>
    Jul 9, Google published a [blog](https://developers.googleblog.com/en/t5gemma/) “Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation”. T5Gemma, introduced by Google Developers on July 9, 2025, is a new collection of encoder-decoder large language models (LLMs) built on the Gemma 2 framework, designed to enhance performance and efficiency in tasks requiring deep input understanding, such as summarization and translation. Unlike traditional decoder-only models, T5Gemma adapts pretrained Gemma 2 models (2B and 9B) and introduces newly trained T5-sized models (Small, Base, Large, XL) using a model adaptation technique. This method initializes encoder-decoder parameters with pretrained decoder-only weights, followed by further pre-training with UL2 or PrefixLM methods, allowing flexible configurations like pairing a large encoder with a smaller decoder for optimized quality-efficiency trade-offs. T5Gemma models demonstrate superior performance, nearly dominating the quality-inference efficiency frontier in benchmarks like SuperGLUE, with notable gains in reasoning-intensive tasks. For instance, T5Gemma 9B-9B outperforms Gemma 2 9B by over 9 points on GSM8K (math reasoning) and 4 points on DROP (reading comprehension). Instruction-tuned versions further amplify these gains, with T5Gemma 2B-2B improving MMLU scores by nearly 12 points. The models’ flexibility and efficiency make them ideal for applications requiring robust input comprehension, and Google has released pretrained and instruction-tuned checkpoints to foster community-driven research and development. This revival of the encoder-decoder architecture highlights its potential to create more capable foundational models, offering developers a powerful toolset for innovative AI solutions. <br> <br>

29. ***Johns Hopkins Uni's DOTResize for LLM compression:  <br>This study introduces DOTResize, a novel Transformer compression method that reduces model width by framing neuron merging as a Discrete Optimal Transport problem. Unlike pruning, DOTResize re-projects the entire neuron width to retain and redistribute signals, outperforming other neuron width-pruning techniques across multiple LLM families while achieving measurable reductions in computational cost.*** <br> <br>
    Jul 6, Johns Hopkins Uni published a [paper](https://arxiv.org/pdf/2507.04517) “DOTResize: Reducing LLM Width via Discrete Optimal Transport-based Neuron Merging”. Model compression offers a promising path to reducing the cost and inaccessibility of large pre-trained models, without significantly compromising their impressive performance. Large Transformer models, including large language models (LLMs), often contain computational redundancy, which can serve as a target for new model compression methods. The study specifically targets neuron-level redundancies in model layers by combining groups of similar neurons into fewer neurons. The study frames this width reduction as a Discrete Optimal Transport problem, and proposes DOTResize, a novel Transformer compression method that uses optimal transport theory to transform and compress model weights. To ensure applicability within the Transformer architecture, the study motivates and incorporates entropic regularization and matrix factorization into the transportation maps produced by the method. Unlike pruning-based approaches which discard neurons based on importance measures, DOTResize re-projects the entire neuron width, allowing the retention and redistribution of useful signal across the reduced layer. Empirical results show that compared to simple or state-of-the-art neuron width-pruning techniques, DOTResize can outperform these methods across multiple LLM families and sizes, while achieving measurable reductions in real-world computational cost. <br> <br>

31. ***Stanford Uni et al.'s CollabLLM for active collaboration:  <br>This research introduces CollabLLM, a training framework to enhance human-LLM collaboration by moving models from passive responders to active collaborators. Using a collaborative simulation with Multiturn-aware Rewards for reinforcement fine-tuning, CollabLLM learns to actively uncover user intent and offer insightful suggestions, significantly outperforming baselines in task performance, interactivity, user satisfaction, and time efficiency.*** <br> <br>
    Jun 12, Stanford Uni, Microsoft and Georgia Tech published a [paper](https://arxiv.org/pdf/2502.00640) “CollabLLM: From Passive Responders to Active Collaborators”. Large Language Models are typically trained with next-turn rewards, limiting their ability to optimize for long-term interaction. As a result, they often respond passively to ambiguous or open-ended user requests, failing to help users reach their ultimate intents and leading to inefficient conversations. To address these limitations, the study introduces CollabLLM, a novel and general training framework that enhances multiturn human-LLM collaboration. Its key innovation is a collaborative simulation that estimates the long-term contribution of responses using Multiturn-aware Rewards. By reinforcement fine-tuning these rewards, CollabLLM goes beyond responding to user requests, and actively uncovers user intent and offers insightful suggestions-a key step towards more human-centered AI. The study also devises a multiturn interaction benchmark with three challenging tasks such as document creation. CollabLLM significantly outperforms the baselines with averages of 18.5% higher task performance and 46.3% improved interactivity by LLM judges. Finally, the study conducts a large user study with 201 judges, where CollabLLM increases user satisfaction by 17.6% and reduces user spent time by 10.4%. https://github.com/Wuyxin/collabllm

 <br> <br> <br>

***Jul 13, 2025***

1. ***UIUC et al.'s PLAN-TUNING framework:  <br>This research introduces PLAN-TUNING, a unified post-training framework that distills synthetic planning trajectories from large LLMs and fine-tunes smaller models to mimic these step-by-step planning processes using supervised and reinforcement learning. Plan-tuned models significantly outperformed strong baselines on math benchmarks and showed improved out-of-domain generalization on challenging datasets like OlympiadBench and AIME 2024.*** <br> <br>
   Jul 10, Google and Arizona State Uni published a [paper](https://arxiv.org/pdf/2507.07495) “PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving”. Recently, decomposing complex problems into simple subtasks--a crucial part of human-like natural planning--to solve the given problem has significantly boosted the performance of large language models (LLMs). However, leveraging such planning structures during post-training to boost the performance of smaller open-source LLMs remains underexplored. Motivated by this, the study introduces PLAN-TUNING, a unified post-training framework that (i) distills synthetic task decompositions (termed "planning trajectories") from large-scale LLMs and (ii) fine-tunes smaller models via supervised and reinforcement-learning objectives designed to mimic these planning processes to improve complex reasoning. On GSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by an average . Furthermore, plan-tuned models show better generalization capabilities on out-of-domain datasets, with average  and  performance improvements on OlympiadBench and AIME 2024, respectively. A detailed analysis demonstrates how planning trajectories improves complex reasoning capabilities, showing that PLAN-TUNING is an effective strategy for improving task-specific performance of smaller LLMs. <br> <br>

3. ***UC Berkeley's Q-chunking for reinforcement learning:  <br>This study presents Q-chunking, a recipe for improving RL algorithms in offline-to-online settings for long-horizon, sparse-reward tasks. By applying action chunking (predicting sequences of future actions) to TD-based RL methods, Q-chunking enables more effective online exploration by leveraging temporally consistent offline behaviors and more stable TD learning via unbiased n-step backups, outperforming prior methods on manipulation tasks.*** <br> <br>
   Jul 10, UC Berkeley published a [paper](https://arxiv.org/pdf/2507.07969) “Reinforcement Learning with Action Chunking”. The study presents Q-chunking, a simple yet effective recipe for improving reinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks. The recipe is designed for the offline-to-online RL setting, where the goal is to leverage an offline prior dataset to maximize the sample-efficiency of online learning. Effective exploration and sample-efficient learning remain central challenges in this setting, as it is not obvious how the offline data should be utilized to acquire a good exploratory policy. The key insight is that action chunking, a technique popularized in imitation learning where sequences of future actions are predicted rather than a single action at each timestep, can be applied to temporal difference (TD)-based RL methods to mitigate the exploration challenge. Q-chunking adopts action chunking by directly running RL in a 'chunked' action space, enabling the agent to (1) leverage temporally consistent behaviors from offline data for more effective online exploration and (2) use unbiased n-step backups for more stable and efficient TD learning. Experimental results demonstrate that Q-chunking exhibits strong offline performance and online sample efficiency, outperforming prior best offline-to-online methods on a range of long-horizon, sparse-reward manipulation tasks. <br> <br>

5. ***CMU and Cartesia AI's Dynamic Chunking for hierarchical modeling:  <br>This paper introduces dynamic chunking techniques and a hierarchical network (H-Net) to create a true end-to-end foundation model that learns content- and context-dependent segmentation strategies from raw data, replacing the static tokenization pipeline. H-Nets operating at the byte level outperform BPE-tokenized Transformers, demonstrate significantly better scaling and character-level robustness, and show dramatic data efficiency gains on modalities with weak tokenization heuristics like DNA.*** <br> <br>
   Jul 10, CMU and Cartesia AI published a [paper](https://arxiv.org/pdf/2507.07955) “Dynamic Chunking for End-to-End Hierarchical Sequence Modeling”. Despite incredible progress in language models (LMs) in recent years, largely resulting from moving away from specialized models designed for specific tasks to general models based on powerful architectures (e.g. the Transformer) that learn everything from raw data, pre-processing steps such as tokenization remain a barrier to true end-to-end foundation models. The study introduces a collection of new techniques that enable a dynamic chunking mechanism which automatically learns content -- and context -- dependent segmentation strategies learned jointly with the rest of the model. Incorporating this into an explicit hierarchical network (H-Net) allows replacing the (implicitly hierarchical) tokenization-LM-detokenization pipeline with a single model learned fully end-to-end. When compute- and data- matched, an H-Net with one stage of hierarchy operating at the byte level outperforms a strong Transformer language model operating over BPE tokens. Iterating the hierarchy to multiple stages further increases its performance by modeling multiple levels of abstraction, demonstrating significantly better scaling with data and matching a token-based Transformer of twice its size. H-Nets pretrained on English show significantly increased character-level robustness, and qualitatively learn meaningful data-dependent chunking strategies without any heuristics or explicit supervision. Finally, the H-Net's improvement over tokenized pipelines is further increased in languages and modalities with weaker tokenization heuristics, such as Chinese and code, or DNA sequences (nearly 4x improvement in data efficiency over baselines), showing the potential of true end-to-end models that learn and scale better from unprocessed data. https://github.com/goombalab/hnet <br> <br>

7. ***Princeton Uni on Machine Bullshit in LLMs:  <br>This research proposes "machine bullshit" – statements made without regard to truth – as a framework to understand emergent untruthfulness in LLMs, introducing the Bullshit Index metric and a taxonomy of bullshit forms. Empirical evaluations on a new BullshitEval benchmark show that RLHF fine-tuning significantly exacerbates bullshit and CoT prompting amplifies specific forms, highlighting systematic AI alignment challenges.*** <br> <br>
   Jul 10, Princeton Uni published a [paper](https://arxiv.org/pdf/2507.07484) “Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models”. Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to statements made without regard to their truth value. While previous work has explored large language model (LLM) hallucination and sycophancy, the study proposes machine bullshit as an overarching conceptual framework that can allow researchers to characterize the broader phenomenon of emergent loss of truthfulness in LLMs and shed light on its underlying mechanisms. The study introduces the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and propose a complementary taxonomy analyzing four qualitative forms of bullshit: empty rhetoric, paltering, weasel words, and unverified claims. The study conducts empirical evaluations on the Marketplace dataset, the Political Neutrality dataset, and a new BullshitEval benchmark (2,400 scenarios spanning 100 AI assistants) explicitly designed to evaluate machine bullshit. The results demonstrate that model fine-tuning with reinforcement learning from human feedback (RLHF) significantly exacerbates bullshit and inference-time chain-of-thought (CoT) prompting notably amplify specific bullshit forms, particularly empty rhetoric and paltering. The study also observes prevalent machine bullshit in political contexts, with weasel words as the dominant strategy. The findings highlight systematic challenges in AI alignment and provide new insights toward more truthful LLM behavior. <br> <br>

9. ***TheConversation on AI's impact on universities:  <br>This article argues that generative AI is devaluing traditional, codifiable knowledge by making it abundant and nearly free, thus challenging the core value proposition of universities. As employers reduce degree requirements, universities must pivot from content delivery to cultivating scarce, AI-complementary tacit skills like critical thinking, emotional intelligence, and creativity (summarized in the C.R.E.A.T.E.R. framework) to remain relevant.*** <br> <br>
    Jul 9, TheConversation published an [article](https://theconversation.com/ai-is-driving-down-the-price-of-knowledge-universities-have-to-rethink-what-they-offer-260493) “AI is driving down the price of knowledge – universities have to rethink what they offer”. The rise of AI, particularly generative models like ChatGPT, is dramatically reducing the cost of accessing and organizing knowledge, challenging the traditional value proposition of universities. Historically, universities thrived on the scarcity of information, offering credentials that signaled mastery and justified high tuition and wage premiums. However, as AI makes codifiable knowledge abundant and nearly free, the economic value of such knowledge is declining. Employers are responding swiftly, reducing degree requirements and entry-level job postings, as AI substitutes many routine tasks once performed by graduates. Yet, not all knowledge is equally affected. Tacit skills—like leadership, ethical judgment, and emotional intelligence—remain valuable because they complement AI rather than compete with it. These human-centric capabilities, encapsulated in the C.R.E.A.T.E.R. framework (critical thinking, resilience, emotional intelligence, accountability, teamwork, entrepreneurial creativity, and reflection), are now the true scarce resources. Universities must adapt by auditing courses to focus on judgment over rote learning, investing in experiential learning environments, credentialing soft skills, and collaborating with industry to design relevant assessments. The shift from content delivery to cultivating human judgment and creativity is essential. If universities fail to evolve, they risk becoming obsolete in a market that increasingly values AI-complementary skills over traditional academic credentials. <br> <br>

11. ***MCML et al. on the impossibility of separating AI intelligence from judgment:  <br>This study investigates the challenge of filtering harmful content from LLMs, demonstrating under cryptographic hardness assumptions that there are no efficient prompt filters for certain adversarial prompts and that output filtering can be computationally intractable in natural settings. The authors conclude that safety cannot be achieved through external, black-box filters, arguing that an aligned AI's intelligence and judgment are computationally inseparable*** <br> <br>
    Jul 9, MCML, UC Berkeley, Standford Uni and Apple published a [paper](https://arxiv.org/pdf/2507.07341) “On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment”. With the increased deployment of large language models (LLMs), one concern is their potential misuse for generating harmful content. The work studies the alignment challenge, with a focus on filters to prevent the generation of unsafe information. Two natural points of intervention are the filtering of the input prompt before it reaches the model, and filtering the output after generation. The main results demonstrate computational challenges in filtering both prompts and outputs. First, the study shows that there exist LLMs for which there are no efficient prompt filters: adversarial prompts that elicit harmful behavior can be easily constructed, which are computationally indistinguishable from benign prompts for any efficient filter. The second main result identifies a natural setting in which output filtering is computationally intractable. All of the separation results are under cryptographic hardness assumptions. In addition to these core findings, the study also formalizes and studies relaxed mitigation approaches, demonstrating further computational barriers. The study concludes that safety cannot be achieved by designing filters external to the LLM internals (architecture and weights); in particular, black-box access to the LLM will not suffice. Based on the technical results, the authors argue that an aligned AI system's intelligence cannot be separated from its judgment. <br> <br>

13. ***xAI's Grok 4 release:  <br>xAI has launched Grok 4, a major AI advancement featuring state-of-the-art reasoning, real-time search, and native tool use, scaled using a 200,000 GPU cluster called Colossus. Grok 4 variants achieve record-breaking scores on academic benchmarks, outperform humans in simulated economic environments, and offer developers a multimodal API with a 256k context window, a new Voice Mode, and a commitment to scaling RL for real-world problem-solving.*** <br> <br>
    Jul 9, xAI [released Grok 4](https://x.ai/news/grok-4). Grok 4, developed by xAI, represents a major leap in artificial intelligence, combining advanced reasoning, real-time search, and native tool use to deliver state-of-the-art performance. Building on the success of Grok 3, which introduced large-scale next-token prediction and reinforcement learning for improved problem-solving, Grok 4 scales these capabilities using Colossus—a 200,000 GPU cluster. This infrastructure enabled a 6x increase in compute efficiency and expanded training data across diverse domains, enhancing Grok’s reasoning and tool-use abilities. Grok 4 can autonomously use tools like code interpreters and web browsers to search and synthesize information, even diving deep into X (formerly Twitter) using semantic and media search. The Grok 4 Heavy variant pushes boundaries further with parallel hypothesis testing, achieving record-breaking scores on academic benchmarks like ARC-AGI V2 and Humanity’s Last Exam. Its agentic capabilities also outperform top models and humans in simulated economic environments. The Grok 4 API offers developers multimodal understanding, a 256k context window, and enterprise-grade security, while the new Voice Mode enables natural, real-time interaction with visual input analysis. Looking ahead, xAI plans to scale reinforcement learning to tackle real-world problems and enhance multimodal capabilities, aiming to create AI systems that deeply understand and assist humanity. Grok 4 is available to SuperGrok and Premium+ subscribers, with broader deployment planned through hyperscaler partners. <br> <br>

15. ***Allen Inst for AI et al.'s FlexOlmo models:  <br>This paper introduces FlexOlmo, a new class of MoE language models supporting distributed training without data sharing and data-flexible inference, where independently trained experts on closed datasets can be combined via a new domain-informed routing mechanism. FlexOlmo models showed significant relative improvements (average 41%) while allowing users to opt out of certain data, outperforming prior model merging methods and standard MoE trained without data restrictions.*** <br> <br>
    Jul 9, Allen Inst for AI, Uni of Washington, UC Berkeley, Stanford Uni and MIT published a [paper](https://arxiv.org/pdf/2507.07024) “FlexOlmo: Open Language Models for Flexible Data Use”. The study introduces FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus curated comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. The study evaluates models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks, shows that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. The approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners' preferences by keeping their data local and supporting fine-grained control of data access during inference. https://github.com/allenai/FlexOlmo <br> <br>

17. ***Microsoft and Stanford Uni's Decoder-Hybrid-Decoder architecture:  <br>This research introduces the Gated Memory Unit (GMU) for efficient memory sharing across layers and applies it to create SambaY, a decoder-hybrid-decoder architecture that shares memory from a Samba-based self-decoder to a cross-decoder. This design significantly improves decoding efficiency and long-context performance, with their largest model, Phi4-mini-Flash-Reasoning, outperforming its baseline on reasoning tasks and delivering up to 10x higher decoding throughput.*** <br> <br>
    Jul 9, Microsoft and Stanford Uni published a [paper](https://arxiv.org/pdf/2507.06607) “Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation”. Recent advances in language modeling have demonstrated the effectiveness of State Space Models (SSMs) for efficient sequence modeling. While hybrid architectures such as Samba and the decoder-decoder architecture, YOCO, have shown promising performance gains over Transformers, prior works have not investigated the efficiency potential of representation sharing between SSM layers. This study introduces the Gated Memory Unit (GMU), a simple yet effective mechanism for efficient memory sharing across layers. The study applies it to create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in the cross-decoder to share memory readout states from a Samba-based self-decoder. SambaY significantly enhances decoding efficiency, preserves linear pre-filling time complexity, and boosts long-context performance, all while eliminating the need for explicit positional encoding. Through extensive scaling experiments, the study demonstrates that the model exhibits a significantly lower irreducible loss compared to a strong YOCO baseline, indicating superior performance scalability under large-scale compute regimes. The largest model enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves significantly better performance than Phi4-mini-Reasoning on reasoning tasks such as Math500, AIME24/25, and GPQA Diamond without any reinforcement learning, while delivering up to 10x higher decoding throughput on 2K-length prompts with 32K generation length under the vLLM inference framework. https://github.com/microsoft/ArchScale <br> <br>

19. ***NYU and Columbia Uni on small batch size training:  <br>This study revisits small batch size training for LMs, finding that with a proposed rule for scaling Adam hyperparameters, small batches (down to size one) train stably, are more robust to hyperparameter choices, achieve equal or better per-FLOP performance, and enable stable training with vanilla SGD. They conclude by recommending against gradient accumulation unless bottlenecked by inter-device bandwidth.*** <br> <br>
    Jul 9, NYU and Columbia Uni published a [paper](https://arxiv.org/pdf/2507.07101) “Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful”. Conventional wisdom dictates that small batch sizes make language model pretraining and fine-tuning unstable, motivating gradient accumulation, which trades off the number of optimizer steps for a proportional increase in batch size. While it is common to decrease the learning rate for smaller batch sizes, other hyperparameters are often held fixed. This study revisits small batch sizes all the way down to batch size one, and proposes a rule for scaling Adam hyperparameters to small batch sizes. The study finds that small batch sizes (1) train stably, (2) are consistently more robust to hyperparameter choices, (3) achieve equal or better per-FLOP performance than larger batch sizes, and (4) notably enable stable language model training with vanilla SGD, even without momentum, despite storing no optimizer state. Building on these results, the work provides practical recommendations for selecting a batch size and setting optimizer hyperparameters. The study further recommends against gradient accumulation unless training on multiple devices with multiple model replicas, bottlenecked by inter-device bandwidth. https://github.com/martin-marek/batch-size <br> <br>

21. ***MSN on Isomorphic Labs' human trials:  <br>Alphabet's Isomorphic Labs, a DeepMind spin-off, is preparing for its first human trials of AI-designed drugs, primarily for cancer, marking a key step in its mission to revolutionize drug discovery. Leveraging the AlphaFold breakthrough, and with major pharma partnerships and $600 million in 2025 funding, the company aims to accelerate drug development and dramatically improve clinical trial success rates with its AI-driven drug design engine.*** <br> <br>
    Jul 6, MSN published an [article](https://www.msn.com/en-us/health/other/alphabet-s-isomorphic-labs-has-grand-ambitions-to-solve-all-diseases-with-ai-now-it-s-gearing-up-for-its-first-human-trials/ar-AA1I44pq) “Alphabet’s Isomorphic Labs has grand ambitions to ‘solve all diseases’ with AI. Now, it’s gearing up for its first human trials”. Alphabet’s Isomorphic Labs, a spin-off from DeepMind, is on the brink of launching its first human trials for AI-designed drugs, marking a major milestone in its mission to revolutionize drug discovery. The company emerged from DeepMind’s AlphaFold breakthrough, which accurately predicts protein structures and interactions, making it a powerful tool for designing targeted medicines. Isomorphic Labs combines advanced AI with pharmaceutical expertise to accelerate drug development, reduce costs, and improve success rates. President Colin Murdoch revealed that the company is actively designing cancer drugs using AI and is close to initiating clinical trials. Since its founding in 2021, Isomorphic has secured major partnerships with pharmaceutical giants like Novartis and Eli Lilly and raised $600 million in funding in 2025 to build a world-class drug design engine. This engine integrates machine learning researchers with seasoned pharma professionals to create both collaborative and proprietary drug candidates, particularly in oncology and immunology. Murdoch envisions a future where AI can instantly generate effective drug designs for any disease, dramatically improving the odds of success in clinical trials. With traditional drug development often costing millions and yielding only a 10% success rate, Isomorphic aims to transform the process by making it faster, cheaper, and more reliable. The company’s bold ambition is to harness AI to “solve all diseases,” and its upcoming human trials represent a significant step toward realizing that vision. <br> <br>

23. ***Oxford, Mila et al.'s critique of CoT as explainability:  <br>This paper argues that while chains-of-thought (CoT) can boost language model performance, they should not be treated as a sufficient method for trustworthy interpretability. Synthesizing evidence, the authors show CoTs are often unfaithful to a model's underlying computations and propose that researchers should avoid such claims without verification, adopt rigorous faithfulness assessments, and develop causal validation methods to ground explanations in model internals.*** <br> <br>
    Jul 5, Oxford, Mila et al published a [paper](https://www.alphaxiv.org/abs/2025.02) “Chain-of-Thought Is Not Explainability”. Chains- of-thought (CoT) allow language models to verbalise multi-step rationales before producing their final answer. While this technique often boosts task performance and offers an impression of transparency into the model’s reasoning, the paper argues that rationales generated by current CoT techniques can be misleading and are neither necessary nor sufficient for trustworthy interpretability. By analysing faithfulness in terms of whether CoTs are not only human-interpretable, but also reflect underlying model reasoning in a way that supports responsible use, the study synthesises evidence from previous studies. The work shows that verbalised chains are frequently unfaithful, diverging from the true hidden computations that drive a model’s predictions, and giving an incorrect picture of how models arrive at conclusions. Despite this, CoT is increasingly relied upon in high-stakes domains such as medicine, law, and autonomous systems—the analysis of 1,000 recent CoT-centric papers finds that ~ 25% explicitly treat CoT as an interpretability technique—and among them, papers in high-stakes domains specifically hinge on such interpretability claim heavily. Building on prior work in interpretability, the study makes three proposals: (i) avoid treating CoT as being sufficient for interpretability without additional verification, while continuing to use CoT for its communicative benefits, (ii) adopt rigorous methods that assess faithfulness for downstream decision-making, and (iii) develop causal validation methods (e.g., activation patching, counterfactual interventions, verifier models) to ground explanations in model internals. <br> <br>

25. ***MemTensor et al.'s MemOS for AI systems:  <br>This paper proposes MemOS, a memory operating system for AI that addresses the lack of well-defined memory management in LLMs by treating memory as a manageable system resource. MemOS unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories through "MemCubes," enabling flexible memory transitions and laying a foundation for continual learning and personalized modeling.*** <br> <br>
    Jul 4, MemTensor (Shanghai) et al published a [paper](https://statics.memtensor.com.cn/files/MemOS_0707.pdf) “MemOS: A Memory OS for AI System”. Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency. Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods. While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations. Recent work has modelled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, the study proposes MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling. https://github.com/MemTensor/MemOS <br> <br>

27. ***ScienceAdviser on LLM-assisted writing in biomedical publications:  <br>This study analyzed vocabulary changes in over 15 million biomedical abstracts, finding an abrupt increase in the frequency of certain style words since the advent of LLMs. This excess word analysis suggests that at least 13.5% of 2024 abstracts were processed with LLMs, with some subcorpora reaching 40%, indicating an unprecedented impact on scientific writing in this field.*** <br> <br>
    Jul 2, ScienceAdviser published a [paper](https://www.science.org/doi/10.1126/sciadv.adt3813) “Delving into LLM-assisted writing in biomedical publications through excess vocabulary”. Large language models (LLMs) like ChatGPT can generate and revise text with human-level performance. These models come with clear limitations, can produce inaccurate information, and reinforce existing biases. Yet, many scientists use them for their scholarly writing. But how widespread is such LLM usage in the academic literature? To answer this question for the field of biomedical research, the study presents an unbiased, large-scale approach: the authors study vocabulary changes in more than 15 million biomedical abstracts from 2010 to 2024 indexed by PubMed and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. This excess word analysis suggests that at least 13.5% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, reaching 40% for some subcorpora. The study shows that LLMs have had an unprecedented impact on scientific writing in biomedical research, surpassing the effect of major world events such as the COVID pandemic. <br> <br>

29. ***Artefact Research Center et al.'s comparison of MLM vs. CLM pretraining:  <br>This large-scale study on encoder pretraining found that while Masked Language Modeling (MLM) generally yields better performance on text representation tasks, Causal Language Modeling (CLM) is more data-efficient and has better fine-tuning stability. They conclude that a biphasic strategy—sequentially applying CLM then MLM—achieves optimal performance under a fixed compute budget, especially when starting from readily available pretrained CLM models.*** <br> <br>
    Jul 1, Artefact Research Center et al published a [paper](Should We Still Pretrain Encoders with Masked Language Modeling?) “Should We Still Pretrain Encoders with Masked Language Modeling?”. Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as encoders, often surpassing traditional encoders on text representation benchmarks. However, it remains unclear whether these gains reflect an inherent advantage of the CLM objective or arise from confounding factors such as model and data scale. This study addresses this question through a series of large-scale, carefully controlled pretraining ablations, training a total of 30 models ranging from 210 million to 1 billion parameters, and conducting over 15,000 fine-tuning and evaluation runs. The study finds that while training with MLM generally yields better performance across text representation tasks, CLM-trained models are more data-efficient and demonstrates improved fine-tuning stability. Building on these findings, the work experimentally shows that a biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under a fixed computational training budget. Moreover, the study demonstrates that this strategy becomes more appealing when initializing from readily available pretrained CLM models (from the existing LLM ecosystem), reducing the computational burden needed to train best-in-class encoder models. https://huggingface.co/MLMvsCLM <br> <br>

31. ***Johns Hopkins Uni and UC Berkeley on the benefits of data uniformity:  <br>This research demonstrates that selecting more uniformly distributed data can improve the training efficiency and performance of LLMs and other neural networks. Theoretically, they show that more uniform data leads to a larger minimum pairwise distance between points, which in turn accelerates gradient descent training and decreases approximation error, a finding supported by extensive experiments in supervised fine-tuning.*** <br> <br>
    Jun 30, Johns Hopkins Uni and UC Berkeley published a [paper](https://arxiv.org/pdf/2506.24120) “Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime”. Data selection plays a crucial role in data-driven decision-making, including in large language models (LLMs), and is typically task-dependent. Properties such as data quality and diversity have been extensively studied and are known to enhance model performance. However, it remains unclear whether there exist other quantitative and general principles of data selection that can consistently improve performance, especially for complex tasks with limited prior knowledge. This study demonstrates that selecting more uniformly distributed data can improve training efficiency while enhancing performance. Specifically, the study establishes that more uniform (less biased) distribution leads to a larger minimum pairwise distance between data points, denoted by hmin, and prove that a smaller hmin can slow down the training dynamics of gradient descent (GD). Moreover, the study theoretically shows that the approximation error of neural networks decreases as hmin increases. The analysis introduces a convergence framework for GD beyond the Neural Tangent Kernel (NTK) regime, applicable to a broad class of architectures, including transformers, without requiring Lipschitz smoothness. This framework further provides theoretical justification for the use of residual connections and function compositions in deep neural architectures. In the end, the work conducts comprehensive experiments for supervised fine-tuning across various settings, including different optimization strategies, model sizes, and training datasets. The results consistently demonstrate that selecting data by maximizing pairwise distance significantly accelerates training and achieves comparable or better performance in LLMs across diverse datasets. https://github.com/SafeRL-Lab/data-uniformity <br> <br>

33. ***TheRegister.com on AI's impact on web search referrals:  <br>The article reports that the rise of AI-generated search summaries, like Google’s AI Overviews, has severely disrupted the web ecosystem, leading to a 30% drop in click-through rates despite a 49% increase in search impressions. This trend negatively impacts websites reliant on referral traffic, as AI companies crawl vast amounts of content while providing little traffic in return, straining the open web's economic model.*** <br> <br>
    Jun 22, TheRegister.com published an [article](https://www.theregister.com/2025/06/22/ai_search_starves_publishers/) “The AIpocalypse is here for websites as search referrals plunge”. The rise of AI-generated search summaries, particularly Google’s AI Overviews launched in May 2024, has significantly disrupted the traditional web ecosystem. These summaries appear at the top of search results, offering users direct answers without requiring them to click through to source websites. While this has increased search impressions by 49%, it has led to a 30% drop in click-through rates, severely impacting websites that rely on search referrals for traffic and revenue. SEO experts and analytics firms like BrightEdge, Ahrefs, and SimilarWeb report consistent declines in referral traffic across various sectors, including travel, news, e-commerce, and finance. AI search engines have only replaced about 10% of traditional referral traffic, leaving a substantial gap. Cloudflare CEO Matthew Prince highlighted the growing imbalance, noting that AI companies like Google, OpenAI, and Anthropic are crawling vastly more pages than they refer visitors to—ratios as high as 60,000:1 in Anthropic’s case. This trend suggests that AI firms are extracting content for training and services while offering little in return, prompting lawsuits from web publishers. Despite speculation about AI disrupting Google’s dominance, the company still commands 90% of the search market. However, its practices, along with those of other AI firms, are straining the very content ecosystem that enabled their growth. As AI crawlers increasingly burden websites without compensating them, the sustainability of the open web and its economic model is being called into question. <br> <br>

35. ***Meta's ConfQA for reducing LLM hallucination:  <br>This paper presents ConfQA, a fine-tuning strategy that teaches LLMs to admit "I am unsure" when they would otherwise provide an incorrect answer, reducing hallucination rates from 20-40% to under 5% on factuality benchmarks. Key to its effectiveness are a "dampening prompt" and the use of simple factual statements from knowledge graphs to help LLMs calibrate their confidence.*** <br> <br>
    Jun 8, Meta published a [paper](https://www.arxiv.org/pdf/2506.07309) “ConfQA: Answer Only If You Are Confident”. Can we teach Large Language Models (LLMs) to refrain from hallucinating factual statements? The study presents a fine-tuning strategy that it is called ConfQA, which can reduce hallucination rate from 20-40% to under 5% across multiple factuality benchmarks. The core idea is simple: when the LLM answers a question correctly, it is trained to continue with the answer; otherwise, it is trained to admit "I am unsure". But there are two key factors that make the training highly effective. First, the study introduces a dampening prompt "answer only if you are confident" to explicitly guide the behavior, without which hallucination remains high as 15%-25%. Second, the study leverages simple factual statements, specifically attribute values from knowledge graphs, to help LLMs calibrate the confidence, resulting in robust generalization across domains and question types. Building on this insight, the study proposes the Dual Neural Knowledge framework, which seamlessly select between internally parameterized neural knowledge and externally recorded symbolic knowledge based on ConfQA's confidence. The framework enables potential accuracy gains to beyond 95%, while reducing unnecessary external retrievals by over 30%.

 <br> <br> <br>

***Jul 6, 2025***

1. ***UIUC et al.'s position on Agentic Deep Research:  <br>This paper argues that LLMs with reasoning and agentic capabilities are creating a new paradigm called Agentic Deep Research, which surpasses traditional web search by integrating autonomous reasoning, iterative retrieval, and information synthesis. Tracing the evolution from static search to interactive, agent-based systems, and introducing a test-time scaling law, the authors demonstrate that this approach significantly outperforms existing methods and is poised to become the dominant paradigm for information seeking.*** <br> <br>
   Jul 3, UIUC et al. published a [paper](https://www.arxiv.org/pdf/2506.18959) “From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents”. Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. The autors’ position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. The study traces the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. The study also introduces a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, the work demonstrates that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. https://github.com/DavidZWZ/Awesome-Deep-Research <br> <br>

3. ***Yale Uni and TSC Research on LLM identification of scientific limitations:  <br>This study addresses the understudied potential of LLMs in assisting with peer review, specifically in identifying research limitations. The authors introduce a comprehensive taxonomy of limitation types, present LimitGen (the first benchmark for this task with synthetic and human-written subsets), and show that augmenting LLMs with literature retrieval enhances their ability to generate concrete and constructive feedback on research papers.*** <br> <br>
   Jul 3, Yale Uni and TSC Research published a [paper](https://arxiv.org/pdf/2507.02694) “Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers”. Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. The study first presents a comprehensive taxonomy of limitation types in scientific research, with a focus on AI. Guided by this taxonomy, for studying limitations, the study presents LimitGen, the first comprehensive benchmark for evaluating LLMs' capability to support early-stage feedback and complement human peer review. The benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of high-quality papers, and LimitGen-Human, a collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, the study augments them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. The approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback. <br> <br>

5. ***Renmin Uni and BAAI's HiRA framework for deep search:  <br>This research introduces HiRA, a hierarchical reasoning framework that addresses the limitations of current reasoning-based search approaches by separating high-level strategic planning from specialized execution. By decomposing complex search tasks and assigning subtasks to domain-specific agents with external tools, HiRA significantly outperforms state-of-the-art RAG and agent-based systems on complex benchmarks, improving both answer quality and efficiency.*** <br> <br>
   Jul 3, Renmin Uni and BAAI published a [paper](https://arxiv.org/pdf/2507.02652) “Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search”. Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. This study introduces HiRA, a hierarchical framework that separates strategic planning from specialized execution. The approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. The results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. https://github.com/ignorejjj/HiRA. <br> <br>

7. ***Nature's paper on the Centaur foundation model:  <br>This study introduces Centaur, a computational model fine-tuned on the large-scale Psych-101 dataset, designed to predict and simulate human behavior in any experiment expressible in natural language. Centaur not only captures participant behavior better than existing cognitive models and generalizes to unseen tasks, but its internal representations also become more aligned with human neural activity, demonstrating potential for guiding cognitive theory development.*** <br> <br>
   Jul 2, Nature published a [paper](https://www.nature.com/articles/s41586-025-09215-4) “A foundation model to predict and capture human cognition”. Establishing a unified theory of cognition has been an important goal in psychology. A first step towards such a theory is to create a computational model that can predict human behaviour in a wide range of settings. Here the study introduces Centaur, a computational model that can predict and simulate human behaviour in any experiment expressible in natural language. The study derived Centaur by fine-tuning a state-of-the-art language model on a large-scale dataset called Psych-101. Psych-101 has an unprecedented scale, covering trial-by-trial data from more than 60,000 participants performing in excess of 10,000,000 choices in 160 experiments. Centaur not only captures the behaviour of held-out participants better than existing cognitive models, but it also generalizes to previously unseen cover stories, structural task modifications and entirely new domains. Furthermore, the model’s internal representations become more aligned with human neural activity after fine-tuning. Taken together, the results demonstrate that it is possible to discover computational models that capture human behaviour across a wide range of domains. The authors believe that such models provide tremendous potential for guiding the development of cognitive theories, and present a case study to demonstrate this. <br> <br>

9. ***Microsoft's research on sequential diagnosis with language models:  <br>To better emulate real-world clinical practice, this study introduces the Sequential Diagnosis Benchmark, which transforms diagnostically challenging NEJM-CPC cases into iterative diagnostic encounters. They also present the MAI Diagnostic Orchestrator (MAI-DxO), a model-agnostic orchestrator that, when paired with OpenAI's o3, achieved 80% diagnostic accuracy (4x higher than generalist physicians) and reduced diagnostic costs, highlighting AI's potential for precision and cost-effectiveness.*** <br> <br>
    Jul 2, Microsoft published a [paper](https://arxiv.org/pdf/2506.22405) “Sequential Diagnosis with Language Models”. Artificial intelligence holds great promise for expanding access to expert medical knowledge and reasoning. However, most evaluations of language models rely on static vignettes and multiple-choice questions that fail to reflect the complexity and nuance of evidence-based medicine in real-world settings. In clinical practice, physicians iteratively formulate and revise diagnostic hypotheses, adapting each subsequent question and test to what they've just learned, and weigh the evolving evidence before committing to a final diagnosis. To emulate this iterative process, the study introduces the Sequential Diagnosis Benchmark, which transforms 304 diagnostically challenging New England Journal of Medicine clinicopathological conference (NEJM-CPC) cases into stepwise diagnostic encounters. A physician or AI begins with a short case abstract and must iteratively request additional details from a gatekeeper model that reveals findings only when explicitly queried. Performance is assessed not just by diagnostic accuracy but also by the cost of physician visits and tests performed. The study also presents the MAI Diagnostic Orchestrator (MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians, proposes likely differential diagnoses and strategically selects high-value, cost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80% diagnostic accuracy--four times higher than the 20% average of generalist physicians. MAI-DxO also reduces diagnostic costs by 20% compared to physicians, and 70% compared to off-the-shelf o3. When configured for maximum accuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO generalize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and Llama families. The work highlights how AI systems, when guided to think iteratively and act judiciously, can advance diagnostic precision and cost-effectiveness in clinical care. <br> <br>

11. ***Allen Inst for AI et al.'s work on simple retrieval for reasoning benchmarks:  <br>This study challenges the view that minimal RAG is ineffective for reasoning-intensive benchmarks by introducing CompactDS, a diverse, high-quality, web-scale datastore with high retrieval accuracy and low latency. Using CompactDS, a minimal RAG pipeline achieved consistent and significant accuracy improvements (10-33%) across MMLU, GPQA, and MATH, matching or outperforming web search engines and complex agent-based systems while maintaining simplicity.*** <br> <br>
    Jul 2, Allen Inst for AI, UIUC et al. published a [paper](https://arxiv.org/pdf/2507.01297) “Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks”. Retrieval-augmented Generation (RAG) has primarily been studied in limited settings, such as factoid question answering; more challenging, reasoning-intensive benchmarks have seen limited success from minimal RAG. This study challenges this prevailing view on established, reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. The work identifies a key missing component in prior work: a usable, web-scale datastore aligned with the breadth of pretraining data. To this end, the study introduces CompactDS: a diverse, high-quality, web-scale datastore that achieves high retrieval accuracy and subsecond latency on a single-node. The key insights are (1) most web content can be filtered out without sacrificing coverage, and a compact, high-quality subset is sufficient; and (2) combining in-memory approximate nearest neighbor (ANN) retrieval and on-disk exact search balances speed and recall. Using CompactDS, the study shows that a minimal RAG pipeline achieves consistent accuracy improvements across all benchmarks and model sizes (8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA, and 19% on MATH. No single data source suffices alone, highlighting the importance of diversity of sources (web crawls, curated math, academic papers, textbooks). Finally, the study shows that the carefully designed in-house datastore matches or outperforms web search engines such as Google Search, as well as recently proposed, complex agent-based RAG systems--all while maintaining simplicity, reproducibility, and self-containment.  <br> <br>

13. ***Uni of Oxford et al.'s GradMetaNet for learning on gradients:  <br>This paper presents GradMetaNet, a novel, principled architecture for learning on neural network gradients, guided by principles of equivariance to neuron permutation, processing sets of gradients to capture curvature, and efficient rank-1 decomposition. GradMetaNet, constructed from simple equivariant blocks, is proven to be a universal approximator for certain gradient-based functions and demonstrated effectiveness on diverse tasks like learned optimization and INR editing.*** <br> <br>
    Jul 2, Uni of Oxford et al. published a [paper](https://arxiv.org/pdf/2507.01649) “GradMetaNet: An Equivariant Architecture for Learning on Gradients”. Gradients of neural networks encode valuable information for optimization, editing, and analysis of models. Therefore, practitioners often treat gradients as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent works explore learning algorithms that operate directly on gradients but use architectures that are not specifically designed for gradient processing, limiting their applicability. This study presents a principled approach for designing architectures that process gradients. The approach is guided by three principles: (1) equivariant design that preserves neuron permutation symmetries, (2) processing sets of gradients across multiple data points to capture curvature information, and (3) efficient gradient representation through rank-1 decomposition. Based on these principles, the study introduces GradMetaNet, a novel architecture for learning on gradients, constructed from simple equivariant blocks. The study proves universality results for GradMetaNet, and show that previous approaches cannot approximate natural gradient-based functions that GradMetaNet can. The study then demonstrates GradMetaNet's effectiveness on a diverse set of gradient-based tasks on MLPs and transformers, such as learned optimization, INR editing, and estimating loss landscape curvature. <br> <br>

15. ***CMU et al.'s study on math reasoning transferability:  <br>This research investigates whether improved math reasoning in LLMs translates to broader problem-solving abilities, finding that most open-weight reasoning-tuned models fail to transfer gains to other domains. Controlled experiments on Qwen3-14B models showed that RL-tuned models generalize well, while SFT-tuned models often forget general capabilities due to significant representation and output drift, suggesting a need to rethink post-training recipes.*** <br> <br>
    Jul 1, CMU et al published a [paper](https://arxiv.org/pdf/2507.00432) “Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning”. Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, the study evaluates over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. The authors surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, the work conducts controlled experiments on Qwen3-14B models using math-only data but different tuning methods. The study finds that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. The results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models. https://github.com/ReasoningTransfer/Transferability-of-LLM-Reasoning <br> <br>

17. ***Bloomberg.com's report on Meta Superintelligence Labs (MSL):  <br>Meta CEO Mark Zuckerberg announced a major AI strategy overhaul with the creation of Meta Superintelligence Labs (MSL), a new division led by former Scale AI CEO Alexandr Wang and ex-GitHub CEO Nat Friedman. MSL will consolidate existing AI teams and launch a new lab to develop superintelligence, backed by massive investments and aggressive recruitment of top talent from rivals like OpenAI and Google.*** <br> <br>
    Jul 1, according to [Bloomberg.com](https://www.bloomberg.com/news/articles/2025-06-30/zuckerberg-announces-meta-superintelligence-effort-more-hires), “Zuckerberg Debuts Meta ‘Superintelligence’ Group, More Hires”. Meta CEO Mark Zuckerberg has announced a major overhaul of the company’s artificial intelligence strategy, unveiling a new division called Meta Superintelligence Labs (MSL). This group, led by Alexandr Wang, former CEO of Scale AI, aims to develop AI systems capable of performing tasks as well as or better than humans. Nat Friedman, ex-GitHub CEO, will co-lead the initiative, focusing on AI products and applied research. MSL will consolidate Meta’s existing teams working on large language models, AI products, and the Fundamental AI Research (FAIR) team, while also launching a new lab to advance next-generation models. Zuckerberg emphasized his belief that superintelligence marks the beginning of a transformative era for humanity and pledged to invest “hundreds of billions” in AI infrastructure, talent, and research. Meta has aggressively recruited top talent from leading AI firms like OpenAI, Anthropic, and Google, offering lucrative compensation packages. Recent hires include prominent researchers and engineers from DeepMind, OpenAI, and Anthropic. The company also invested $14.3 billion in Scale AI and is exploring acquisitions of startups like PlayAI. Zuckerberg has personally led recruitment efforts, hosting candidates at his homes and spearheading outreach. Despite concerns about industry-wide overinvestment, he maintains that staying ahead in AI is crucial for long-term technological leadership. Meta’s stock remained stable following the announcement, reflecting investor confidence in the company’s ambitious AI vision. This restructuring signals Meta’s intent to compete aggressively with rivals like OpenAI and Google in shaping the future of artificial intelligence. New team members: Trapit Bansal, Shuchao Bi, Huiwen Chang, Ji Lin, Joel Pobar, Jack Rae, Hongyu Ren, Johan Schalkwyk, Pei Sun, Jiahui Yu, Snengjia Zhao <br> <br>

19. ***Princeton Uni on uncertainty quantification in reasoning models:  <br>This study explores whether reasoning models know when they don't know, finding that SOTA reasoning models are typically overconfident (especially for incorrect responses), become more so with deeper reasoning, and can sometimes improve calibration through introspective reasoning, though not uniformly across all models. The paper highlights the need for better UQ benchmarks and methods to improve the calibration of reasoning models for safe deployment.*** <br> <br>
    Jul 1, Princeton Uni published a [paper](https://arxiv.org/pdf/2506.18183) “Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?” Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, enabled by multi-step reasoning induced using reinforcement learning. However, like previous language models, reasoning models are prone to generating confident, plausible responses that are incorrect (hallucinations). Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications. The study explores uncertainty quantification of reasoning, specifically, it asks three fundamental questions: First, are reasoning models well-calibrated? Second, does deeper reasoning improve model calibration? Finally, inspired by humans' innate ability to double-check their thought processes to verify the validity of their answers and their confidence, the study asks: can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces? The work introduces introspective uncertainty quantification (UQ) to explore this direction. In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, the study finds that reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than 85% particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, the study concludes with important research directions to design necessary UQ benchmarks and improve the calibration of reasoning models. <br> <br>

21. ***Uni of Central Florida et al.'s cross-disciplinary synthesis of AGI:  <br>This paper offers a cross-disciplinary analysis of AGI development, arguing that despite the capabilities of models like GPT-4.5, they are limited by token-level prediction and lack of grounded agency. The authors highlight the role of modular reasoning, persistent memory, and multi-agent coordination (especially Agentic RAG) as crucial for bridging the gap between statistical learning and goal-directed cognition on the path to AGI.*** <br> <br>
    Jul 1, Uni of Central Florida et al. published a [paper](https://arxiv.org/pdf/2507.00951) “Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact”. Can machines truly think, reason and act in domains like humans? This enduring question continues to shape the pursuit of Artificial General Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency. This paper offers a cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. The study analyzes the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, the work emphasizes the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior. The study discusses generalization strategies, including information compression, test-time adaptation, and training-free methods, as critical pathways toward flexible, domain-agnostic intelligence. Vision-Language Models (VLMs) are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. The study also argues that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior. Drawing on advances in neurosymbolic systems, reinforcement learning, and cognitive scaffolding, the research explores how recent architectures begin to bridge the gap between statistical learning and goal-directed cognition. Finally, the study identifies key scientific, technical, and ethical challenges on the path to AGI. <br> <br>

23. ***Apple's TarFlowLM for flexible language modeling:  <br>This work explores an alternative to discrete-token autoregressive models by proposing TarFlowLM, a framework that shifts language modeling to a continuous latent space using transformer-based autoregressive normalizing flows. This approach enables flexible modeling, including global bi-directional context, block-wise generation, and hierarchical multi-pass generation, demonstrating strong likelihood performance on benchmarks.*** <br> <br>
    Jul 1, Apple published a [paper](https://arxiv.org/pdf/2507.00425) “Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows”. Autoregressive models have driven remarkable progress in language modeling. Their foundational reliance on discrete tokens, unidirectional context, and single-pass decoding, while central to their success, also inspires the exploration of a design space that could offer new axes of modeling flexibility. This work explores an alternative paradigm, shifting language modeling from a discrete token space to a continuous latent space. The study proposes a novel framework TarFlowLM, that employs transformer-based autoregressive normalizing flows to model these continuous representations. This approach unlocks substantial flexibility, enabling the construction of models that can capture global bi-directional context through stacked, alternating-direction autoregressive transformations, support block-wise generation with flexible token patch sizes, and facilitate a hierarchical multi-pass generation process. The study further proposes new mixture-based coupling transformations designed to capture complex dependencies within the latent space shaped by discrete data, and demonstrate theoretical connections to conventional discrete autoregressive models. Extensive experiments on language modeling benchmarks demonstrate strong likelihood performance and highlight the flexible modeling capabilities inherent in our framework. <br> <br>

25. ***Meta and Uni of Washington's ASTRO framework:  <br>This paper introduces ASTRO (Autoregressive Search-Taught Reasoner), a framework to teach non-reasoner LLMs like Llama 3 to reason like search algorithms by fine-tuning them on a synthetic dataset derived from Monte Carlo Tree Search traces. This process, which converts search trajectories into natural language chain-of-thoughts capturing successes and failures, followed by RL, instilled robust reasoning and led to significant performance gains on math benchmarks.*** <br> <br>
    Jul 1, Meta and Uni of Washington published a [paper](https://arxiv.org/abs/2507.00417) “ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context”. The paper introduces ASTRO, the "Autoregressive Search-Taught Reasoner", a framework for training language models to reason like search algorithms, explicitly leveraging self-reflection, backtracking, and exploration in their outputs. Recently, training large language models (LLMs) via reinforcement learning (RL) has led to the advent of reasoning models with greatly enhanced reasoning capabilities. Open-source replications of reasoning models, while successful, build upon models that already exhibit strong reasoning capabilities along with search behavior observed even before RL. As a result, it is yet unclear how to boost the reasoning capabilities of other non-reasoner models including Llama 3. ASTRO teaches such models to internalize structured search behavior through a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over mathematical problem-solving trajectories. By converting search traces into natural language chain-of-thoughts that capture both successes and recoveries from failure, ASTRO bootstraps models with a rich prior for exploration during RL. The study finetunes models on these search-derived traces and further improve performance via RL with verifiable rewards. The study applies ASTRO to the Llama 3 family of models and achieve absolute performance gains of 16.0% on MATH-500, 26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon challenging problems that require iterative correction. The results demonstrate that search-inspired training offers a principled way to instill robust reasoning capabilities into open LLMs. <br> <br>

27. ***The New York's article on AI's impact on college writing:  <br>This article explores how AI tools like ChatGPT are challenging higher education by allowing students to bypass the core learning process in writing assignments, prompting a "code red" among educators. While some professors revert to traditional methods like in-class exams, others embrace AI as a tool, forcing a critical re-examination of what skills to cultivate and how to preserve original human thought in an increasingly automated world.*** <br> <br>
    Jun 30, The New York published an [article](https://www.newyorker.com/magazine/2025/07/07/the-end-of-the-english-paper) “What happens after A.I. destroys college writing?”. AI's Impact on College Writing and Higher Education - The proliferation of AI tools like ChatGPT is fundamentally altering college writing, posing a significant challenge to traditional pedagogy and prompting a re-evaluation of higher education's purpose. As evidenced by students like "Alex" who use AI for virtually all writing tasks, from summarizing complex texts to drafting essays that receive high grades, AI can bypass the core process of learning that once defined academic assignments. This has created a "code red" for educators grappling with academic dishonesty, though AI detection remains inconsistent and many students, seeing AI as just another productivity tool, don't view its use as "cheating." While some professors are reverting to traditional methods like in-class, handwritten blue-book exams to ensure authentic learning and combat AI use, others are embracing AI as a tool for collaborative learning and emphasizing the process of writing over the final product. The article highlights how AI can enhance learning, as seen with AI tutors and personalized practice questions, but also raises concerns about students' declining ability to engage with complex texts and their increasing desire for efficiency over deep engagement. Ultimately, AI's integration forces a critical re-examination of what skills higher education should cultivate when AI can perform many intellectual tasks, and how to preserve the value of original human thought and expression in an increasingly automated world. <br> <br>

29. ***Renmin Uni et al.'s MoCa for bidirectional multimodal embeddings:  <br>This study proposes MoCa, a two-stage framework to transform pre-trained causal VLMs into effective bidirectional multimodal embedding models, addressing limitations of causal attention and reliance on labeled data. By introducing modality-aware continual pre-training with a joint reconstruction objective and heterogeneous contrastive fine-tuning, MoCa achieves new SOTA results on MMEB and ViDoRe-v2 benchmarks.*** <br> <br>
    Jun 29, Renmin Uni, Stanford Uni and Microsoft published a [paper](https://arxiv.org/pdf/2506.23115) “MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings”. Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, the study proposes MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. The method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB. https://github.com/haon-chen/MoCa <br> <br>

31. ***Meta and Uni of Edinburgh's Automated LLM Speedrunning Benchmark:  <br>This research introduces a benchmark to evaluate AI agents' ability to reproduce scientific results, leveraging the NanoGPT speedrun competition where participants improve a GPT-2 training script. The study found that recent reasoning LLMs with SOTA scaffolds struggle to reimplement known innovations even with detailed hints, highlighting the benchmark's utility as a simple, non-saturated measure of an LLM's scientific reproduction skill.*** <br> <br>
    Jun 27, Meta and Uni of Edinburgh published a [paper](https://arxiv.org/pdf/2506.22419) “The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements”. Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce existing work. To evaluate the ability of AI agents to reproduce results in an active research area, the study introduces the Automated LLM Speedrunning Benchmark, leveraging the research community contributions on the NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time. Each of the 19 speedrun tasks provides the agent with the previous records training script, optionally paired with one of three hint formats, ranging from pseudocode to paper-like descriptions of the new records improvements. Records execute quickly by design and speedrun improvements encompass diverse code-level changes, ranging from high-level algorithmic advancements to hardware-aware optimizations. These features make the benchmark both accessible and realistic for the frontier problem of improving LLM training. The study finds that recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement already-known innovations in the benchmark, even when given detailed hints. The benchmark thus provides a simple, non-saturated measure of an LLMs ability to automate scientific reproduction, a necessary (but not sufficient) skill for an autonomous research agent. <br> <br>

33. ***Thomson Reuters' Future of Professionals Report 2025:  <br>This report argues that generative AI will transform professions like law, tax, and audit over the next three years, creating a competitive divide between organizations that adopt a formal AI strategy and those that do not. Firms with visible AI strategies are twice as likely to see revenue growth, while professionals who fail to develop AI proficiency risk falling behind in their careers.*** <br> <br>
    Jun 27, Thomson Reuters [published](https://www.thomsonreuters.com/content/dam/ewp-m/documents/thomsonreuters/en/pdf/reports/future-of-professionals-report-2025.pdf) “Future of Professionals Report 2025”. Generative AI will transform the legal, risk, compliance, tax, accounting, and audit professions, along with global trade over the next three years. Organizations must constantly consider how to maintain their competitive edge, but the efficiency gains offered by AI are particularly significant in today’s evolving business landscape. Now, as AI adoption reaches a pivotal stage, it’s clear that a widening competitive gap is emerging. Firms that reinvent and automate entire business processes using AI will prevail with superior customer experiences and lower costs compared to those organizations that move slowly. This year’s report highlights a new divide among organizations: Those that adopt an AI strategy and those that do not. The research shows that organizations with visible AI strategies are twice as likely to experience revenue growth as a direct or indirect result of AI adoption compared to those with more informal or ad-hoc adoption approaches. That puts those organizations that haven’t developed an AI strategy at risk of being left behind within a matter of years. The report highlights the significant variance in AI adoption, even within the same organization. Those professionals who fail to develop their individual AI proficiency risk falling behind in critical skills, creating a competitive gap that could limit their career growth. AI-enabled professionals will gain a competitive edge, boosting both their personal impact and their organization’s long-term value.  <br> <br>

35. ***HKUST et al.'s GPAS for accelerating LLM pretraining:  <br>To mitigate the issue of exponential activation variance growth in Pre-LayerNorm (Pre-LN) Transformers, which limits learning in deeper layers, this study proposes Gradient-Preserving Activation Scaling (GPAS). This simple technique scales down intermediate activations while keeping their gradients unchanged, achieving consistent performance gains across various model sizes and demonstrating versatility by improving alternative architectures as well.*** <br> <br>
    Jun 27, HKUST, IDEA, Nvidia, Unif of Oxford et al published a [paper](https://arxiv.org/pdf/2506.22049) “GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling”. Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series, predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While being stable during pretraining and scalable to large model sizes, Pre-LN suffers from an exponential growth in activation variance across layers, causing the residual path to dominate over sub-layer outputs and limiting the learning capacity of deeper layers. To mitigate this issue, the study proposes Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be used in combination with existing approaches. GPAS works by scaling down the intermediate activations while keeping their gradients unchanged. This leaves information in the activations intact, and avoids the gradient vanishing problem associated with gradient downscaling. Extensive experiments across various model sizes from 71M to 1B show that GPAS achieves consistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows promise in improving alternative architectures such as Sandwich-LN and DeepNorm, demonstrating its versatility and potential for improving training dynamics in a wide range of settings. <br> <br>

37. ***Uni of Cambridge on Transformers as Graph Neural Networks:  <br>This study establishes a connection between Transformers and Graph Neural Networks (GNNs), showing that Transformers can be viewed as message passing GNNs operating on fully connected graphs of tokens. While mathematically linked, Transformers are implemented with dense matrix operations that are more efficient on modern hardware than sparse message passing, leading to the perspective that Transformers are GNNs "winning the hardware lottery."*** <br> <br>
    Jun 27, Uni of Cambridge published a [paper](https://arxiv.org/pdf/2506.22084) “Transformers are Graph Neural Networks”. The study establishes connections between the Transformer architecture, originally introduced for natural language processing, and Graph Neural Networks (GNNs) for representation learning on graphs. The study shows how Transformers can be viewed as message passing GNNs operating on fully connected graphs of tokens, where the self-attention mechanism capture the relative importance of all tokens w.r.t. each-other, and positional encodings provide hints about sequential ordering or structure. Thus, Transformers are expressive set processing networks that learn relationships among input elements without being constrained by apriori graphs. Despite this mathematical connection to GNNs, Transformers are implemented via dense matrix operations that are significantly more efficient on modern hardware than sparse message passing. This leads to the perspective that Transformers are GNNs currently winning the hardware lottery. <br> <br>

39. ***Stanford Uni et al.'s rational analysis of ICL strategies:  <br>This research aims to unify findings on in-context learning (ICL) strategies by proposing a hierarchical Bayesian framework that explains a model's learned strategies as an optimal adaptation to data given computational constraints. The framework, which almost perfectly predicts Transformer next-token predictions without access to weights, models a trade-off between a strategy's loss and its complexity, explaining known ICL phenomena and offering novel predictions.*** <br> <br>
    Jun 26, Stanford Uni, Harvard Uni and Princeton Uni published a [paper](https://arxiv.org/pdf/2506.17859) “In-Context Learning Strategies Emerge Rationally”. Recent work analyzing in-context learning (ICL) has identified a broad set of strategies that describe model behavior in different experimental conditions. The study aims to unify these findings by asking why a model learns these disparate strategies in the first place. Specifically, the work starts with the observation that when trained to learn a mixture of tasks, as is popular in the literature, the strategies learned by a model for performing ICL can be captured by a family of Bayesian predictors: a memorizing predictor, which assumes a discrete prior on the set of seen tasks, and a generalizing predictor, where the prior matches the underlying task distribution. Adopting the normative lens of rational analysis, where a learner's behavior is explained as an optimal adaptation to data given computational constraints, the study develops a hierarchical Bayesian framework that almost perfectly predicts Transformer next-token predictions throughout training -- without assuming access to its weights. Under this framework, pretraining is viewed as a process of updating the posterior probability of different strategies, and inference-time behavior as a posterior-weighted average over these strategies' predictions. The framework draws on common assumptions about neural network learning dynamics, which make explicit a tradeoff between loss and complexity among candidate strategies: beyond how well it explains the data, a model's preference towards implementing a strategy is dictated by its complexity. This helps explain well-known ICL phenomena, while offering novel predictions: e.g., the study shows a superlinear trend in the timescale for transitioning from generalization to memorization as task diversity increases. Overall, the work advances an explanatory and predictive account of ICL grounded in tradeoffs between strategy loss and complexity. <br> <br>

41. ***Google's text-to-text regression for large system performance prediction:  <br>This paper proposes text-to-text regression as a general, scalable alternative to traditional tabular regression for predicting metric outcomes in complex large systems where feature engineering is infeasible. A 60M parameter encoder-decoder trained to predict resource efficiency on Google's Borg cluster achieved near-perfect rank correlation (0.99) and 100x lower MSE than tabular methods, demonstrating its effectiveness and adaptability.*** <br> <br>
    Jun 26, Google published a [paper](https://arxiv.org/pdf/2506.21718) “Performance Prediction for Large Systems via Text-to-Text Regression”. In many industries, predicting metric outcomes of large systems is a fundamental problem, driven largely by traditional tabular regression. However, such methods struggle on complex systems data in the wild such as configuration files or system logs, where feature engineering is often infeasible. The study proposes text-to-text regression as a general, scalable alternative. For predicting resource efficiency on Borg, Google's massive compute cluster scheduling system, a 60M parameter encoder-decoder, trained from random initialization, achieves up to a near perfect 0.99 (0.9 average) rank correlation across the entire fleet, and 100x lower MSE than tabular approaches. The model also easily adapts to new tasks in only 500 few-shot examples and captures the densities of complex outcome distributions. Ablation studies highlight the importance of using encoders, increasing sequence length, and the model's inherent uncertainty quantification. These findings pave the way for universal simulators of real-world outcomes. <br> <br>

43. ***Meta and NYU on Asymmetric REINFORCE for off-policy RL:  <br>This study analyzes a simple off-policy REINFORCE algorithm for aligning LLMs, providing a theoretical analysis showing that when the reward baseline lower-bounds the expected reward, the algorithm guarantees policy improvement. The analysis reveals that while on-policy updates can use both positive and negative signals, off-policy updates benefit from focusing more on positive rewards, a finding validated experimentally in both bandit settings and LLM fine-tuning.*** <br> <br>
    Jun 25, Meta and NYU published a [paper](https://arxiv.org/pdf/2506.20520) “Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards”. Reinforcement learning (RL) is increasingly used to align large language models (LLMs). Off-policy methods offer greater implementation simplicity and data efficiency than on-policy techniques, but often result in suboptimal performance. The work studies the intermediate range of algorithms between off-policy RL and supervised fine-tuning by analyzing a simple off-policy REINFORCE algorithm, where the advantage is defined as A=r−V, with r a reward and V some tunable baseline. Intuitively, lowering V emphasizes high-reward samples, while raising it penalizes low-reward ones more heavily. The authors first provide a theoretical analysis of this off-policy REINFORCE algorithm, showing that when the baseline V lower-bounds the expected reward, the algorithm enjoys a policy improvement guarantee. The analysis reveals that while on-policy updates can safely leverage both positive and negative signals, off-policy updates benefit from focusing more on positive rewards than on negative ones. The study validates the findings experimentally in a controlled stochastic bandit setting and through fine-tuning state-of-the-art LLMs on reasoning tasks. <br> <br>

45. ***Uni of Oxford's analysis of AI Compute Sovereignty:  <br>This paper breaks down the concept of 'compute sovereignty' into three levels: compute on a country's territory, nationality of data center owners, and nationality of accelerator vendors. Examining leading public cloud providers, the study finds that a country's possession of compute sovereignty varies by the level of analysis, and determining the most relevant level involves policy trade-offs between supply security and socioeconomic/environmental impacts.*** <br> <br>
    Jun 24, Uni of Oxford published a [paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5312977) “AI Compute Sovereignty: Infrastructure Control Across Territories, Cloud Providers, and Accelerators”. The concept of 'compute sovereignty' has become a focal point in government and industry discussions on artificial intelligence (AI) governance. What is it and who has it? Based on previous literature, the study proposes to break these questions down to three levels: (1) how much AI compute a country has on its territory, (2) what is the nationality of the companies who own the AI compute data centres, and (3) what is the nationality of the accelerator vendors whose chips power the AI compute data centres? The study examines these questions empirically through the lens of cloud computing infrastructure, focusing on nine leading public cloud providers that represent approximately 70 percent of the global market. The data is collected using a methodology previously published in Lehdonvirta, Wu, and Hawkins (2024). The findings suggest that the possession of "compute sovereignty" varies between countries depending on the level of analysis. Determining the most relevant level depends on governments' policy aims and national contexts, and involves policy trade-offs. Policies aimed at attracting data centres to a country's territory can enhance supply security of critical computational resources while also introducing increased consumption of energy, water and land use resources, with corresponding localised socioeconomic and environmental impacts. Regional and supply chain approaches involve different trade-offs. <br> <br>

47. ***Microsoft's study on evolving prompts in-context:  <br>This research challenges conventional LLM prompting wisdom by showing that pruning random demonstrations into seemingly incoherent "gibberish" can remarkably improve performance across diverse tasks, often surpassing SOTA automatic prompt optimization techniques. They propose PromptQuine, an evolutionary search framework that automatically discovers effective pruning strategies by leveraging only tokens within the context, demonstrating its effectiveness across multiple tasks.*** <br> <br>
    Jun 22, Microsoft published a [paper](https://huggingface.co/papers/2506.17930) “Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective”. The study proposes a novel prompt design paradigm that challenges conventional wisdom in large language model (LLM) prompting. While conventional wisdom prioritizes well-crafted instructions and demonstrations for in-context learning (ICL), the work shows that pruning random demonstrations into seemingly incoherent "gibberish" can remarkably improve performance across diverse tasks. Notably, the "gibberish" always matches or surpasses state-of-the-art automatic prompt optimization techniques, achieving substantial gains regardless of LLM alignment. Nevertheless, discovering an effective pruning strategy is non-trivial, as existing attribution methods and prompt compression algorithms fail to deliver robust results, let alone human intuition. In terms of this, the study proposes a self-discover prompt optimization framework, PromptQuine, an evolutionary search framework that automatically searches for the pruning strategy by itself using only low-data regimes. Much like the emergent complexity in nature--such as symbiosis and self-organization--arising in response to resource constraints, the framework evolves and refines unconventional yet highly effective prompts by leveraging only the tokens present within the context. The study demonstrates its effectiveness across classification, multi-choice question answering, generation and math reasoning tasks across LLMs, while achieving decent runtime efficiency. Hope the findings can guide mechanistic studies on in-context learning, and provide a call to action, to pave the way for more open-ended search algorithms for more effective LLM prompting. <br> <br>

49. ***Microsoft et al.'s re-evaluation of RLVR:  <br>This study resolves the paradox of RLVR-tuned models underperforming base models on the Pass@K metric by arguing that Pass@K is a flawed measure of reasoning. They introduce CoT-Pass@K, which requires both the reasoning path and final answer to be correct, and provide a new theoretical foundation and empirical results showing that RLVR, evaluated with this new metric, does incentivize the generalization of correct reasoning.*** <br> <br>
    Jun 17, Microsoft et al published a [paper](https://arxiv.org/pdf/2506.14245) “Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs”. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the Pass@K metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. This study resolves this contradiction by identifying the source of the problem: the Pass@K metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, the study introduces a more precise evaluation metric, CoT-Pass@K, which mandates that both the reasoning path and the final answer be correct. The work provides a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Empirical results are supportive: using CoT-Pass@K, the authors observe that RLVR can incentivize the generalization of correct reasoning for all values of K. Furthermore, by analyzing the training dynamics, the study finds that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. The work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.
 <br> <br> <br>

***Jun 29 2025***

1. ***Hugging Face and EPFL's FineWeb2 dataset:   <br>This research addresses the challenge of creating high-quality, multilingual pre-training datasets for LLMs by introducing a new curation pipeline based on FineWeb that automatically adapts to any language. After extensive ablations and introducing a principled rebalancing approach, they scaled the pipeline to over 1000 languages from nearly 100 Common Crawl snapshots to produce FineWeb2, a new 20TB multilingual dataset, releasing it alongside all associated codebases.***  <br>  <br>
   Jun 26, Huggingface and EPEL published a [paper](https://arxiv.org/pdf/2506.20920) “FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language”. Pre-training state-of-the-art large language models (LLMs) requires vast amounts of clean and diverse text data. While the open development of large high-quality English pre-training datasets has seen substantial recent progress, training performant multilingual LLMs remains a challenge, in large part due to the inherent difficulty of tailoring filtering and deduplication pipelines to a large number of languages. The study introduces a new pre-training dataset curation pipeline based on FineWeb that can be automatically adapted to support any language. The study extensively ablates the pipeline design choices on a set of nine diverse languages, guided by a set of meaningful and informative evaluation tasks that were chosen through a novel selection process based on measurable criteria. Ultimately, the study shows that the pipeline can be used to create non-English corpora that produce more performant models than prior datasets. The study additionally introduces a straightforward and principled approach to rebalance datasets that takes into consideration both duplication count and quality, providing an additional performance uplift. Finally, the study scales the pipeline to over 1000 languages using almost 100 Common Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document) multilingual dataset which was released along with the pipeline, training, and evaluation codebases. Code: https://github.com/huggingface/fineweb-2 Dataset: https://hf.co/datasets/HuggingFaceFW/fineweb-2  <br>  <br>

3. ***MIT's study on gradient descent simulating prompting:   <br>This paper explores whether fine-tuning via gradient descent can emulate the effects of prompting in LMs, describing a meta-training method where an LM's own prompted predictions serve as targets, eliminating the need for ground-truth labels. The approach successfully recovers some, and occasionally all, of prompted model performance on tasks like the "reversal curse" and single-update question answering, suggesting gradient descent can be surprisingly expressive with proper initialization.***  <br>  <br>
   Jun 26, MIT published a [paper](https://www.arxiv.org/pdf/2506.20989) “Can Gradient Descent Simulate Prompting?”. There are two primary ways of incorporating new information into a language model (LM): changing its prompt or changing its parameters, e.g. via fine-tuning. Parameter updates incur no long-term storage cost for model changes. However, for many model updates, prompting is significantly more effective: prompted models can generalize robustly from single examples and draw logical inferences that do not occur under standard fine-tuning. Can models be modified so that fine-tuning does emulate prompting? This paper describes a method for meta-training LMs such that gradient updates emulate the effects of conditioning on new information. The approach uses tools from gradient-based meta-learning but uses an LM's own prompted predictions as targets, eliminating the need for ground-truth labels. Subsequent gradient descent training recovers some (and occasionally all) of prompted model performance -- showing improvement on the “reversal curse” tasks, and answering questions about text passages after a single gradient update. These results suggest that, with appropriate initialization, gradient descent can be surprisingly expressive. The results suggest new avenues for long-context modeling and offer insight into the generalization capabilities of gradient-based learning.  <br>  <br>

5. ***Microsoft's research on Data Efficacy:   <br>This study introduces "Data Efficacy," a concept focusing on maximizing LM performance by optimizing the organization of training data, complementing data efficiency. They propose the DELT paradigm (Data Scoring, Selection, Ordering), designing Learnability-Quality Scoring (LQS) and Folding Ordering (FO) as new instances, and demonstrate through experiments that these methods enhance LM performance without increasing data scale or model size.***  <br>  <br>
   Jun 26, Microsoft published a [paper](https://arxiv.org/abs/2506.21545) “Data Efficacy for Language Model Training”. Data is fundamental to the training of language models (LM). Recent research has been dedicated to data efficiency, which aims to maximize performance by selecting a minimal or optimal subset of training data. Techniques such as data filtering, sampling, and selection play a crucial role in this area. To complement it, the study defines Data Efficacy, which focuses on maximizing performance by optimizing the organization of training data and remains relatively underexplored. This work introduces a general paradigm, DELT, for considering data efficacy in LM training, which highlights the significance of training data organization. DELT comprises three components: Data Scoring, Data Selection, and Data Ordering. Among these components, the study designs Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which considers both the learnability and quality of each data sample from the gradient consistency perspective. The study also devises Folding Ordering (FO), as a novel instance of Data Ordering, which addresses issues such as model forgetting and data distribution bias. Comprehensive experiments validate the data efficacy in LM training, which demonstrates the following: Firstly, various instances of the proposed DELT enhance LM performance to varying degrees without increasing the data scale and model size. Secondly, among these instances, the combination of the proposed LQS for data scoring and Folding for data ordering achieves the most significant improvement. Lastly, data efficacy can be achieved together with data efficiency by applying data selection. Therefore, the authors believe that data efficacy is a promising foundational area in LM training.  <br>  <br>

7. ***Uni of Maryland's study on grokking in LLM pretraining:   <br>This research provides the first evidence of "grokking" (test performance improving long after training loss convergence) during the one-pass pretraining of a 7B LLM (OLMoE) across diverse tasks. They find that grokking corresponds to a memorization-to-generalization transition where training samples' expert pathways evolve from random to more structured and shareable, and they develop novel metrics based on pathway distance and complexity to predict this generalization improvement without testing.***  <br>  <br>
   Jun 26, Uni of Maryland published a [paper](https://arxiv.org/pdf/2506.21551) “Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test”. Grokking, i.e., test performance keeps improving long after training loss converged, has been recently witnessed in neural network training, making the mechanism of generalization and other emerging capabilities such as reasoning mysterious. While prior studies usually train small models on a few toy or highly-specific tasks for thousands of epochs, the research conducts the first study of grokking on checkpoints during one-pass pretraining of a 7B large language model (LLM), i.e., OLMoE. The study computes the training loss and evaluate generalization on diverse benchmark tasks, including math reasoning, code generation, and commonsense/domain-specific knowledge retrieval tasks. The study, for the first time, verifies that grokking still happens in the pretraining of large-scale foundation models, though different data may enter grokking stages asynchronously. The study further demystifies grokking's "emergence of generalization" by investigating LLM internal dynamics. Specifically, the study finds that training samples' pathways (i.e., expert choices across layers) evolve from random, instance-specific to more structured and shareable between samples during grokking. Also, the complexity of a sample's pathway reduces despite the converged loss. These indicate a memorization-to-generalization conversion, providing a mechanistic explanation of delayed generalization. The authors develop two novel metrics to quantify pathway distance and the complexity of a single pathway, and show their ability to predict the generalization improvement on diverse downstream tasks. They are efficient, simple to compute and solely dependent on training data. Hence, they have practical value for pretraining, enabling us to monitor the generalization performance without finetuning and test. Theoretically, the study shows that more structured pathways reduce model complexity and improve the generalization bound.  <br>  <br>

9. ***Uni of Bristol's investigation into skipping transformer middle layers:   <br>This study proposed a novel architecture to make Transformers more efficient by dynamically skipping a variable number of middle layers, guided by interpretability research suggesting redundancy in these layers. However, at the scales investigated, this approach, which used a learned gating mechanism and gated attention, did not achieve improvements in the validation cross-entropy vs. FLOPs trade-off compared to dense baselines with fewer layers.***  <br>  <br>
    Jun 26, Uni of Bristol published a [paper](https://arxiv.org/pdf/2506.21103) “Learning to Skip the Middle Layers of Transformers”. Conditional computation is a popular strategy to make Transformers more efficient. Existing methods often target individual modules (e.g., mixture-of-experts layers) or skip layers independently of one another. However, interpretability research has demonstrated that the middle layers of Transformers exhibit greater redundancy, and that early layers aggregate information into token positions. Guided by these insights, the study proposes a novel architecture that dynamically skips a variable number of layers from the middle outward. In particular, a learned gating mechanism determines whether to bypass a symmetric span of central blocks based on the input, and a gated attention mechanism prevents subsequent tokens from attending to skipped token positions. Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and gate sparsity with an adaptive regularization loss. The study had aimed to reduce compute requirements for 'simpler' tokens and potentially foster an emergent multi-level representational hierarchy but, at the scales investigated, the approach does not achieve improvements in the trade-off between validation cross-entropy and estimated FLOPs compared to dense baselines with fewer layers. https://github.com/tim-lawson/skip-middle.  <br>  <br>

11. ***Stanford Uni's study on the ideation-execution gap:   <br>This research tested whether LLM-generated research ideas lead to better outcomes than human-expert ideas by having 43 researchers execute randomly assigned ideas and write papers on them. Blind reviews of the executed projects showed that the scores of LLM-generated ideas decreased significantly more than human ideas across all metrics, closing the initial novelty gap and highlighting the limitations of current LLMs in generating truly effective research ideas.***  <br>  <br>
    Jun 25, Stanford Uni published a [paper](https://arxiv.org/pdf/2506.20803) “The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas”. Large Language Models (LLMs) have shown promise in accelerating the scientific research pipeline. A key capability for this process is the ability to generate novel research ideas, and prior studies have found settings in which LLM-generated research ideas were judged as more novel than human-expert ideas. However, a good idea should not simply appear to be novel, it should also result in better research after being executed. To test whether AI-generated ideas lead to better research outcomes, the study conducts an execution study by recruiting 43 expert researchers to execute randomly-assigned ideas, either written by experts or generated by an LLM. Each expert spent over 100 hours implementing the idea and wrote a 4-page short paper to document the experiments. All the executed projects are then reviewed blindly by expert NLP researchers. Comparing the review scores of the same ideas before and after execution, the scores of the LLM-generated ideas decrease significantly more than expert-written ideas on all evaluation metrics (novelty, excitement, effectiveness, and overall; p < 0.05), closing the gap between LLM and human ideas observed at the ideation stage. When comparing the aggregated review scores from the execution study, the study even observes that for many metrics there is a flip in rankings where human ideas score higher than LLM ideas. This ideation-execution gap highlights the limitations of current LLMs in generating truly effective research ideas and the challenge of evaluating research ideas in the absence of execution outcomes. https://github.com/NoviScl/AI-Researcher  <br>  <br>

13. ***NBC News on a federal judge's fair use ruling for AI training:   <br>A federal judge in California ruled that AI companies can legally use copyrighted books for training under the fair use doctrine, deeming the practice "exceedingly transformative" as the models do not reproduce or replace the original works. While this sets a significant precedent, Judge William Alsup emphasized that obtaining content through piracy remains illegal, allowing a lawsuit against Anthropic to proceed on that basis.***  <br>  <br>
    Jun 25, according to [NBC News](https://www.nbcnews.com/tech/tech-news/federal-judge-rules-copyrighted-books-are-fair-use-ai-training-rcna214766), “Federal judge rules copyrighted books are fair use for AI training”. A federal judge in California has ruled that artificial intelligence companies can legally use copyrighted books to train their models under the fair use doctrine, marking a significant precedent in the ongoing debate over AI and intellectual property. The case, brought by authors Andrea Bartz, Charles Graeber, and Kirk Wallace Johnson against AI firm Anthropic, challenged the company's use of millions of books—some allegedly pirated—to train its language models. Judge William Alsup concluded that the use of copyrighted works for training AI is “exceedingly transformative,” aligning with fair use principles, particularly because the models do not reproduce or replace the original works. However, Alsup emphasized that while training on copyrighted material is permissible, obtaining such content through piracy is not. The ruling allows the case to proceed to trial over the pirated copies, stating that purchasing a book after initially stealing it does not absolve the company of liability. This decision is the first among many similar lawsuits to address the fair use question directly and could influence future legal interpretations. Alsup acknowledged that the authors’ works are highly expressive and thus deserve strong copyright protection, but found that the transformative nature of AI training outweighed this factor. Anthropic welcomed the ruling, highlighting that its models aim to create new content rather than replicate existing works. The case underscores the growing tension between creative industries and AI developers, as well as the need for clearer legal frameworks around data sourcing and copyright in the age of generative AI.  <br>  <br>

15. ***Google's Gemini CLI open-source AI agent:   <br>Google released Gemini CLI, a free, open-source AI agent that brings Gemini 2.5 Pro's capabilities directly into the developer's terminal for tasks like coding, content generation, and automation. It integrates with Gemini Code Assist, offers generous free usage limits (1,000 requests/day), supports real-time web context, and is extensible and customizable, inviting community contributions.***  <br>  <br>
    Jun 25, Google released [Gemini CLI](https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=meta-snaps-up-key-openai-talent&_bhlid=b7150335ea852fd74cf33aaacf847c438c5dc6b4), an open-source AI agent. Gemini CLI is a free, open-source AI agent designed to bring the capabilities of Google’s Gemini directly into developers’ terminals, offering a seamless and powerful command-line experience. Tailored for developers who rely heavily on the terminal, Gemini CLI provides lightweight, prompt-driven access to Gemini 2.5 Pro, enabling tasks such as coding, content generation, research, and automation. It integrates with Gemini Code Assist, Google’s AI coding assistant, allowing users across all plan tiers — free, Standard, and Enterprise — to benefit from advanced coding support in both VS Code and the terminal. With unmatched usage limits, including 60 requests per minute and 1,000 per day at no cost, Gemini CLI ensures developers rarely face restrictions. The tool supports real-time web context via Google Search, extensibility through the Model Context Protocol (MCP), and customization for personal workflows. As an open-source project under Apache 2.0, it invites community contributions for continuous improvement. Gemini CLI shares its core technology with Gemini Code Assist, which offers agent mode in VS Code to help developers write tests, fix bugs, and build features through multi-step planning and error recovery. Getting started is simple — just install Gemini CLI and log in with a Google account to unlock its full potential. Whether you're a hobbyist or a professional developer, Gemini CLI transforms the terminal into a dynamic, AI-powered workspace.  <br>  <br>

17. ***MPIIS et al.'s scalable orthogonal finetuning:   <br>This study addresses the high runtime and memory demands of orthogonal finetuning (OFT) by proposing OFTv2, an input-centric reformulation that uses matrix-vector multiplications to reduce computational complexity from cubic to quadratic. Combined with the Cayley-Neumann parameterization, OFTv2 achieves up to 10x faster training and 3x lower GPU memory usage without performance loss, and is extended to support quantized models, outperforming QLoRA.***  <br>  <br>
    Jun 24, MPIIS, CUHK, Uni of Cambridge and Alan Turing Inst published a [paper](https://www.arxiv.org/pdf/2506.19847) “Orthogonal Finetuning Made Scalable”. Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation while preventing catastrophic forgetting, but its high runtime and memory demands limit practical deployment. The study identifies the core computational bottleneck in OFT as its weight-centric implementation, which relies on costly matrix-matrix multiplications with cubic complexity. To overcome this, the work proposes OFTv2, an input-centric reformulation that instead uses matrix-vector multiplications (i.e., matrix-free computation), reducing the computational cost to quadratic. The study further introduces the Cayley-Neumann parameterization, an efficient orthogonal parameterization that approximates the matrix inversion in Cayley transform via a truncated Neumann series. These modifications allow OFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage without compromising performance. In addition, the study extends OFTv2 to support finetuning quantized foundation models and show that it outperforms the popular QLoRA in training stability, efficiency, and memory usage. https://github.com/huggingface/peft/tree/main/src/peft/tuners/oft  <br>  <br>

19. ***Korea Uni and AIGEN Sci's Outlier-Safe Pre-Training (OSP):   <br>This research introduces Outlier-Safe Pre-Training (OSP), a practical guideline to proactively prevent extreme activation outliers in LLMs during training, combining the Muon optimizer, Single-Scale RMSNorm, and a learnable embedding projection. A 1.4B model trained with OSP showed near-zero excess kurtosis and achieved a 35.7 average score under 4-bit quantization, significantly outperforming a standard Adam-trained model and demonstrating that outliers are consequences of training strategies.***  <br>  <br>
    Jun 24, Korea Uni and AIGEN Sci published a [paper](https://arxiv.org/pdf/2506.19697) “Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models”. Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. The study introduces Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency; (2) Single-Scale RMSNorm, preventing channel-wise amplification; and (3) a learnable embedding projection, redistributing activation magnitudes originating from embedding matrices. The study validates OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, the OSP model achieves a 35.7 average score across 10 benchmarks (compared to 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. The work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment. https://github.com/dmis-lab/Outlier-Safe-Pre-Training.  <br>  <br>

21. ***Harvard Uni on inference-time reward hacking:   <br>This study characterizes reward hacking in inference-time alignment methods like Best-of-n, showing that the pattern of true reward first increasing then declining is an inevitable property of these mechanisms. To mitigate this, they introduce hedging and HedgeTune, an efficient algorithm to find the optimal inference-time parameter to avoid over-optimizing for a misspecified proxy reward, demonstrating superior distortion-reward tradeoffs.***  <br>  <br>
    Jun 24, Harvard Uni published a [paper](https://arxiv.org/pdf/2506.19248) “Inference-Time Reward Hacking in Large Language Models”. A common paradigm to improve the performance of large language models is optimizing for a reward model. Reward models assign a numerical score to LLM outputs indicating, for example, which response would likely be preferred by a user or is most aligned with safety goals. However, reward models are never perfect. They inevitably function as proxies for complex desiderata such as correctness, helpfulness, and safety. By overoptimizing for a misspecified reward, the study can subvert intended alignment goals and reduce overall performance -- a phenomenon commonly referred to as reward hacking. This study characterizes reward hacking in inference-time alignment and demonstrate when and how we can mitigate it by hedging on the proxy reward. The work studies this phenomenon under Best-of-n (BoN) and Soft-Best-of-n (SBoN), and introduces Best-of-Poisson (BoP) that provides an efficient, near-exact approximation of the optimal reward-KL divergence policy at inference time. The study shows that the characteristic pattern of hacking as observed in practice (where the true reward first increases before declining) is an inevitable property of a broad class of inference-time mechanisms, including BoN and BoP. To counter this effect, hedging offers a tactical choice to avoid placing undue confidence in high but potentially misleading proxy reward signals. The study introduces HedgeTune, an efficient algorithm to find the optimal inference-time parameter and avoid reward hacking. The work demonstrates through experiments that hedging mitigates reward hacking and achieves superior distortion-reward tradeoffs with minimal computational overhead.  <br>  <br>

23. ***Uni of Birmingham's review of dialogic pedagogy for LLMs:   <br>This article reviews the use of LLM-based conversational agents in education, synthesizing existing literature with pedagogical theories (Vygotsky, Socratic method, etc.) to examine how prompting and RAG can align LLM behaviors with proven learning principles. It identifies gaps, such as LLMs' tendency to give direct answers instead of fostering co-construction of knowledge, and proposes practical strategies to make AI-driven dialogues more educationally productive.***  <br>  <br>
    Jun 24, Uni of Birmimgham published a [paper](https://arxiv.org/pdf/2506.19484) “Dialogic Pedagogy for Large Language Models Aligning Conversational AI with Proven Theories of Learning”. Large Language Models (LLMs) are rapidly transforming education by enabling rich conversational learning experiences. This article provides a comprehensive review of how LLM-based conversational agents are being used in higher education, with extensions to secondary and lifelong learning contexts. The study synthesizes existing literature on LLMs in education and theories of conversational and dialogic pedagogy – including Vygotsky’s sociocultural learning (scaffolding and the Zone of Proximal Development), the Socratic method, and Laurillard’s conversational framework – and examine how prompting strategies and retrieval augmented generation (RAG) can align LLM behaviors with these pedagogical theories, and how it can support personalized, adaptive learning. The researcher map educational theories to LLM capabilities, highlighting where LLM-driven dialogue supports established learning principles and where it challenges or falls short of traditional pedagogical assumptions. Notable gaps in applying prior theories to LLMs are identified, such as the models’ tendency to provide direct answers instead of fostering co-construction of knowledge, and the need to account for the constant availability and broad but non-human expertise of LLM tutors. In response, the authors propose practical strategies to better align LLM interactions with sound pedagogy – for example, designing prompts that encourage Socratic questioning, scaffolded guidance, and student reflection, as well as integrating retrieval mechanisms to ensure accuracy and contextual relevance. The authors’ aim is to bridge the gap between educational theory and the emerging practice of AI-driven conversational learning, offering insights and tools for making LLM-based dialogues more educationally productive and theory-aligned.  <br>  <br>

25. ***UIUC et al.'s exploration of virtual logical depth (VLD):   <br>This work explores "virtual logical depth" (VLD) as a 4th dimension for scaling model size, which increases effective algorithmic depth by reusing parameters without changing the overall parameter count. Controlled experiments showed that VLD scaling significantly improves reasoning capability while keeping knowledge capacity almost constant, suggesting that increasing parameter count is not always necessary to improve reasoning.***  <br>  <br>
    Jun 23, UIUC, Uni of Toronto, Google, UMCP and MIT published a [paper](https://arxiv.org/pdf/2506.18233) “The 4th Dimension for Scaling Model Size”. Scaling the size of large language models typically involves 3 dimensions: depth, width, and the number of parameters. In this work, we explore the 4th dimension, virtual logical depth (VLD), which allows increasing the effective algorithmic depth without changing the overall parameter count by reusing parameters within the model. Although parameter reuse itself is not new, its intriguing potential and characteristics in model scaling have not been thoroughly studied. The study carefully designs controlled experiments and have the following key discoveries on VLD scaling: 1. VLD scaling forces the knowledge capacity of the model to stay almost constant, though with some non-significant variations. 2. VLD scaling enables the reasoning capability to be significantly improved, if the scaling method is properly implemented. 3. The number of parameters is proportional to knowledge capacity, but not reasoning capability. Under certain conditions, it is not necessary to increase parameter count to improve reasoning. 4. The above observations hold for various model configurations and are likely to be generally true under the scope of our experiments. These findings not only provide useful insights on the future model scaling strategies, but also introduces an even deeper question: Do people really need a lot of parameters to get a really intelligent model? The authors believe there are many unknown dynamics inside model scaling that needs exploration. https://vldscaling.ngrok.io  <br>  <br>

27. ***Singapore Uni of Tech and Design et al.'s LongWriter-Zero:   <br>This study proposes LongWriter-Zero, an incentivization-based approach using reinforcement learning (RL) from scratch to foster ultra-long, high-quality text generation in LLMs, avoiding reliance on costly and often monotonous synthetic SFT data. A LongWriter-Zero model trained from Qwen2.5-32B consistently outperformed traditional SFT methods and even larger models (100B+) on long-form writing benchmarks.***  <br>  <br>
    Jun 23, Singapore Uni of Tech and Design  and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2506.18841) “LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning”. Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. This study proposes an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. The study performs RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that the LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. https://huggingface.co/THU-KEG/LongWriter-Zero-32B  <br>  <br>

29. ***George Tech and Microsoft's SlimMoE for MoE compression:   <br>This paper introduces SlimMoE, a multi-stage compression framework that transforms large Mixture of Experts (MoE) models into smaller, efficient variants by systematically slimming experts and transferring knowledge through intermediate stages, using less than 10% of the original training data. This method created compact MoE models (e.g., Phi-mini-MoE) suitable for single-GPU fine-tuning that outperform similarly sized models and remain competitive with larger ones.***  <br>  <br>
    Jun 23, George Tech and Microsoft published a [paper](https://arxiv.org/pdf/2506.18349) “SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation”. The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm for scaling large language models (LLMs) while maintaining inference efficiency. However, their enormous memory requirements make them prohibitively expensive to fine-tune or deploy in resource-constrained environments. To address this challenge, the study introduces SlimMoE, a multi-stage compression framework for transforming large MoE models into much smaller, efficient variants without incurring the prohibitive costs of training from scratch. The method systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, effectively mitigating the performance degradation common in one-shot pruning approaches. Using this framework, the study compresses Phi 3.5-MoE (41.9B total/6.6B activated parameters) to create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE (3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of the original model's training data. These compressed models can be fine-tuned on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them highly suitable for academic and resource-limited settings. Experiments demonstrate that these compressed models outperform others of similar size and remain competitive with larger models. For instance, Phi-mini-MoE achieves similar or better performance to Phi-3-mini using only 2/3 of the activated parameters and yields comparable MMLU scores to Llama 3.1 8B despite having significantly lower latency. The findings demonstrate that structured pruning combined with staged distillation offers an effective path to creating high-quality, compact MoE models, paving the way for broader adoption of MoE architectures. https://huggingface.co/microsoft/Phi-mini-MoE-instruct and https://huggingface.co/microsoft/Phi-tiny-MoE-instruct.   <br>  <br>

31. ***Northwestern Uni et al.'s Chain-of-Experts (CoE) architecture:   <br>This study proposes Chain-of-Experts (CoE), a new Mixture-of-Experts (MoE) architecture featuring sequential expert communication within each layer, where tokens are processed iteratively across a chain of experts using a dedicated router at each step. This design introduces a flexible routing mechanism that improves performance under fixed compute and offers a new scaling axis (depth through iteration) that can reduce memory usage compared to other scaling strategies.***  <br>  <br>
    Jun 23, Northwestern Uni, UIUC, Stanford Uni et al published a [paper](https://arxiv.org/pdf/2506.18945) “Chain-of-Experts: Unlocking the Communication Power of Mixture-of-Experts Models”. The study proposes Chain-of-Experts (CoE), a new Mixture-of-Experts (MoE) architecture that introduces sequential expert communication within each layer. Unlike traditional MoE models, where experts operate independently in parallel, CoE processes tokens iteratively across a chain of experts inside a layer. To support dynamic expert selection across iterations, CoE employs a dedicated router at each iteration step within a layer. This design allows tokens to re-evaluate and select different experts during each iteration, rather than being statically assigned. As a result, CoE introduces a flexible routing mechanism that increases the diversity of expert combinations and enriches the model's representational capacity. CoE demonstrates improved performance under fixed compute: on math reasoning tasks, it reduces validation loss from 1.20 to 1.12 compared to a standard MoE. Beyond performance, CoE offers a new scaling axis: depth through expert iteration, which complements conventional width/depth scaling. For example, using 2x iterations matches the performance of 3x expert selections (in width), while reducing memory usage by 17.6-42% relative to other scaling strategies. The analysis reveals that CoE's benefits stem from its iterative residual structure and enhanced expert specialization empowered by iterative routing, which together unlock more expressive representations. https://github.com/ZihanWang314/coe  <br>  <br>

33. ***Princeton Uni on KV cache efficiency for long-context LMs:   <br>This research proposes the "KV footprint" as a unified metric to evaluate KV cache eviction methods for long-context LMs, accounting for both the number of stored entries and their memory lifespan. They adapt post-fill eviction methods to work during pre-filling, reducing their high peak memory, and introduce PruLong, an end-to-end optimization for recency eviction that learns which attention heads need a full cache, achieving a 12% smaller KV footprint than prior methods.***  <br>  <br>
    Jun 20, Princeton Uni published a [paper](https://www.arxiv.org/pdf/2506.17121) “Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?” Language models handle increasingly long contexts for tasks such as book summarization, but this leads to growing memory costs for the key-value (KV) cache. Many prior works have proposed ways of discarding KVs from memory, but their approaches are tailored to favorable settings, obscuring caveats like high peak memory and performance degradation, and a fair comparison between methods is difficult. This study proposes the *KV footprint* as a unified metric, which accounts for both the amount of KV entries stored and their lifespan in memory. The study evaluates methods based on the smallest footprint they attain while preserving performance in both long-context understanding and generation, with context lengths of up to 128K tokens. This metric reveals the high peak memory of prior KV eviction methods. One class of methods -- *post-fill eviction* -- has a high footprint due to being incompatible with eviction during pre-filling. The study adapts these methods to be able to evict KVs during pre-filling, achieving substantially lower KV footprints. The work then turns to *recency eviction* methods, wherein the study proposes PruLong, an end-to-end optimization method for learning which attention heads need to retain the full KV cache and which do not. PruLong saves memory while preserving long-context performance, achieving 12% smaller KV footprint than prior methods while retaining performance in challenging recall tasks. The paper clarifies the complex tangle of long-context inference methods and paves the way for future development to minimize the KV footprint.   <br>  <br>

35. ***Harvard et al.'s EvoLM model suite:   <br>This paper presents EvoLM, a model suite of over 100 1B and 4B parameter LMs trained from scratch to enable systematic analysis of training dynamics across pre-training, continued pre-training, SFT, and RL. Key insights include diminishing returns from excessive training stages and the crucial role of continued pre-training, with all models, datasets, and pipelines released for open research.***  <br>  <br>
    Jun 19, Harvard, Stanford, EPFL and CMU published a [paper](https://www.arxiv.org/pdf/2506.16029) “EvoLM: In Search of Lost Language Model Training Dynamics”. Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. By training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.  <br>  <br>

37. ***Uni of Chicago et al.'s noise decomposition framework for long context LLMs:   <br>This study investigates the challenges of applying LLMs to long texts, proposing a theoretical framework that categorizes failure modes into cross-chunk dependence, model noise, and aggregator noise. They analyze when multi-agent chunking is effective, explaining how a weaker model with chunk-based processing can surpass an advanced model like GPT-4o on large inputs due to superlinear model noise growth.***  <br>  <br>
    Jun 19, Uni of Chicago, Together AI, Duke Uni, Google and Stanford Uni published a [paper](http://arxiv.org/pdf/2506.16411) “When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework”. The study investigates the challenge of applying Large Language Models (LLMs) to long texts. We propose a theoretical framework that distinguishes the failure modes of long context tasks into three categories: cross-chunk dependence (task noise), confusion that grows with context size (model noise), and the imperfect integration of partial results (aggregator noise). Under this view, the study analyzes when it is effective to use multi-agent chunking, i.e., dividing a length sequence into smaller chunks and aggregating the processed results of each chunk. Experiments on tasks such as retrieval, question answering, and summarization confirm both the theoretical analysis and the conditions that favor multi-agent chunking. By exploring superlinear model noise growth with input length, the study also explains why, for large inputs, a weaker model configured with chunk-based processing can surpass a more advanced model like GPT4o applied in a single shot. Overall, the study presents a principled understanding framework and the results highlight a direct pathway to handling long contexts in LLMs with carefully managed chunking and aggregator strategies.  <br>  <br>

39. ***Uni of Oslo et al. on LLM-generated code authorship attribution:   <br>This paper presents the first systematic study of LLM authorship attribution for C programs, introducing the CodeT5-Authorship model and the LLM-AuthorBench benchmark. Their model, using only the CodeT5 encoder, achieved high accuracy (97.56% binary, 95.40% multi-class) in distinguishing code generated by different state-of-the-art LLMs, outperforming traditional ML classifiers and other fine-tuned transformers.***  <br>  <br>
    Jun 18, Uni of Oslo Norway et al published a [paper](https://arxiv.org/pdf/2506.17323) “I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution”. Detecting AI-generated code, deepfakes, and other synthetic content is an emerging research challenge. As code generated by Large Language Models (LLMs) becomes more common, identifying the specific model behind each sample is increasingly important. This paper presents the first systematic study of LLM authorship attribution for C programs. The study released CodeT5-Authorship, a novel model that uses only the encoder layers from the original CodeT5 encoder-decoder architecture, discarding the decoder to focus on classification. The model's encoder output (first token) is passed through a two-layer classification head with GELU activation and dropout, producing a probability distribution over possible authors. To evaluate the approach, the study introduces LLM-AuthorBench, a benchmark of 32,000 compilable C programs generated by eight state-of-the-art LLMs across diverse tasks. The work compares the model to seven traditional ML classifiers and eight fine-tuned transformer models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3, Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, the model achieves 97.56% accuracy in distinguishing C programs generated by closely related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku, GPT-4.1, Llama 3.3, and DeepSeek-V3). https://github.com/LLMauthorbench/.
  <br>  <br>  <br>

***Jun 22 2025***

1. ***OpenAI's study on emergent misalignment:  <br>This research extends prior work on "emergent misalignment" in LLMs (where fine-tuning on insecure code causes malicious responses to unrelated prompts), demonstrating it across diverse conditions including RL and various synthetic datasets. Using a "model diffing" approach with sparse autoencoders, they identified "misaligned persona" features, notably a "toxic persona" feature strongly controlling this behavior, and found that fine-tuning on a few hundred benign samples can efficiently restore alignment.*** <br> <br>
   Jun 19, OpenAI published a [paper](https://cdn.openai.com/pdf/a130517e-9633-47bc-8397-969807a43a23/emergent_misalignment_paper.pdf) “Persona Features Control Emergent Misalignment”. Understanding how language models generalize behaviors from their training to a broader deployment distribution is an important problem in AI safety. Betley et al. (2025b) discovered that fine-tuning GPT-4o on intentionally insecure code causes “emergent misalignment,” where models give stereotypically malicious responses to unrelated prompts. The study extend this work, demonstrating emergent misalignment across diverse conditions, including reinforcement learning on reasoning models, fine-tuning on various synthetic datasets, and in models without safety training. To investigate the mechanisms behind this generalized misalignment, the study introduces a “model diffing” approach using sparse autoencoders to compare internal model representations before and after fine-tuning. This approach reveals several “misaligned persona” features in activation space, including a toxic persona feature which most strongly controls emergent misalignment and can be used to predict whether a model will exhibit such behavior. Additionally, the study investigates mitigation strategies, discovering that fine-tuning an emergently misaligned model on just a few hundred benign samples efficiently restores alignment. <br> <br>

3. ***UCL and UW's "NoWait" for efficient reasoning:  <br>This study proposes NoWait, a simple method to improve LLM reasoning efficiency by suppressing explicit self-reflection tokens (like "Wait," "Hmm") during inference. Experiments across ten benchmarks and five R1-style model series showed NoWait reduces chain-of-thought length by up to 27%-51% without compromising model utility, offering a plug-and-play solution for multimodal reasoning.*** <br> <br>
   Jun 18, Uni College London and Uni of Washington published a [paper](https://arxiv.org/pdf/2506.08343) “Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency”. Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant overthinking, resulting in verbose and redundant outputs that hinder efficiency. This study examines whether explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is necessary for advanced reasoning. We propose NoWait, a simple yet effective approach that disables explicit self-reflection by suppressing these tokens during inference. Extensive experiments on ten benchmarks across textual, visual, and video reasoning tasks show that NoWait reduces chain-of-thought trajectory length by up to 27%-51% in five R1-style model series, without compromising model utility. NoWait thus offers a plug-and-play solution for efficient and utility-preserving multimodal reasoning. <br> <br>

5. ***Yale Uni's SciVer benchmark:  <br>This paper introduces SciVer, the first benchmark for evaluating foundation models on multimodal scientific claim verification, consisting of 3,000 expert-annotated examples from scientific papers covering four common reasoning types. Evaluation of 21 SOTA multimodal models revealed a substantial performance gap compared to human experts, with in-depth analysis highlighting critical limitations in current open-source models.*** <br> <br>
   Jun 18, Yale Uni published a [paper](https://arxiv.org/pdf/2506.15569) “SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification”. The study introduces SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. The study assesses the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, the authors identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks. https://github.com/QDRhhhh/SciVer <br> <br>

7. ***Cornell Uni's data approximation from model weights:  <br>This research formalizes the problem of approximating language model training data from open weights when the data is closed, proposing a gradient-based approach to select matching data from a large public corpus. The method effectively recovers useful data for both classification (improving AG News accuracy from 65% to 80%) and supervised fine-tuning (reducing perplexity on MSMARCO from 3.3 to 2.3), even without knowing any true training data.*** <br> <br>
   Jun 18, Cornel Uni published a [paper](https://arxiv.org/pdf/2506.15553) “Approximating Language Model Training Data from Weights”. Modern language models often have open weights but closed training data. The study formalizes the problem of data approximation from model weights and proposes several baselines and metrics. The study develops a gradient-based approach that selects the highest-matching data from a large public text corpus and show its effectiveness at recovering useful data given only weights of the original and finetuned models. Even when none of the true training data is known, the method is able to locate a small subset of public Web documents can be used to train a model to close to the original model performance given models trained for both classification and supervised-finetuning. On the AG News classification task, the method improves performance from 65% (using randomly selected data) to 80%, approaching the expert benchmark of 88%. When applied to a model trained with SFT on MSMARCO web documents, the method reduces perplexity from 3.3 to 2.3, compared to an expert LLAMA model's perplexity of 2.0. https://github.com/jxmorris12/reverse-training <br> <br>

9. ***CMU's AutoRule for preference learning:  <br>This study presents AutoRule, an automated method to extract rules from preference feedback and formulate them into rule-based rewards for RLHF. By using a reasoning model to interpret preferences, identify candidate rules, and synthesize a rule set, AutoRule achieved a 28.6% relative improvement on AlpacaEval2.0 and a 6.1% gain on MT-Bench when training a Llama-3-8B model, also showing reduced reward hacking.*** <br> <br>
    Jun 18, CMU published a [paper](https://arxiv.org/pdf/2506.15651) “AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning”. Rule-based rewards offer a promising strategy for improving reinforcement learning from human feedback (RLHF), but current approaches often rely on manual rule engineering. The study presents AutoRule, a fully automated method for extracting rules from preference feedback and formulating them into rule-based rewards. AutoRule extraction operates in three stages: it leverages a reasoning model to interpret user preferences, identifies candidate rules from the reasoning chain of these interpretations, and synthesizes them into a unified rule set. Leveraging the finalized rule set, the study employs language-model verifiers to compute the fraction of rules satisfied by each output, using this metric as an auxiliary reward alongside the learned reward model during policy optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\% relative improvement in length-controlled win rate on AlpacaEval2.0, and a 6.1% relative gain in second-turn performance on a held-out MT-Bench subset, compared to a GRPO baseline trained with the same learned reward model but without the rule-based auxiliary reward. The analysis confirms that the extracted rules exhibit good agreement with dataset preference. The study finds that AutoRule demonstrates reduced reward hacking compared to a learned reward model when run over two episodes. Finally, a case study suggests that the extracted rules capture unique qualities valued in different datasets. The extracted rules are provided in the appendix. https://github.com/cxcscmu/AutoRule <br> <br>

11. ***EPFL, Google et al.'s WikiMixQA benchmark:  <br>This paper introduces WikiMixQA, a benchmark of 1,000 multiple-choice questions requiring cross-modal reasoning over tables and charts from 4,000 Wikipedia pages, designed to evaluate VLLM effectiveness on long-context vision inputs. Evaluations of 12 SOTA models showed proprietary models achieve ~70% accuracy with direct context but drop significantly with retrieval, with GPT-4-o being the only one above 50% in that setting, while open-source models performed much worse.*** <br> <br>
    Jun 18, EPFL, Google et al published a [paper](https://arxiv.org/pdf/2506.15594) “WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts”. Documents are fundamental to preserving and disseminating information, often incorporating complex layouts, tables, and charts that pose significant challenges for automatic document understanding (DU). While vision-language large models (VLLMs) have demonstrated improvements across various tasks, their effectiveness in processing long-context vision inputs remains unclear. This paper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice questions (MCQs) designed to evaluate cross-modal reasoning over tables and charts extracted from 4,000 Wikipedia pages spanning seven distinct topics. Unlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring models to synthesize information from multiple modalities. The study evaluates 12 state-of-the-art vision-language models, revealing that while proprietary models achieve ~70% accuracy when provided with direct context, their performance deteriorates significantly when retrieval from long documents is required. Among these, GPT-4-o is the only model exceeding 50% accuracy in this setting, whereas open-source models perform considerably worse, with a maximum accuracy of 27%. These findings underscore the challenges of long-context, multi-modal reasoning and establish WikiMixQA as a crucial benchmark for advancing document understanding research. <br> <br>

13. ***Google's Gemini 2.5 report:  <br>This report introduces the Gemini 2.X model family, featuring Gemini 2.5 Pro as the most capable model with SOTA performance on frontier coding and reasoning, advanced multimodal understanding (processing up to 3 hours of video), and long context capabilities enabling new agentic workflows. The family, including Gemini 2.5 Flash and earlier Flash/Flash-Lite models, spans the capability-cost frontier for diverse applications.*** <br> <br>
    Jun 17, Google published a [report](https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf) “Gemini 2.5 Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities”. The report introduces the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as the earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is the most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving. <br> <br>

15. ***Uni of Montreal et al.'s study on undertraining experts:  <br>This research challenges the assumption that optimizing expert model fine-tuning always improves downstream upcycling performance, showing that long fine-tuning can degrade merging and MoE upcycling results. This degradation is traced to memorizing difficult examples, and the study demonstrates that a task-dependent aggressive early stopping strategy for expert fine-tuning significantly improves upcycling performance.*** <br> <br>
    Jun 17, Uni of Montreal, Mila, Concordia Uni and Google published a [paper](https://www.arxiv.org/pdf/2506.14126) “Less is More: Undertraining Experts Improves Model Upcycling”. Modern deep learning is increasingly characterized by the use of open-weight foundation models that can be fine-tuned on specialized datasets. This has led to a proliferation of expert models and adapters, often shared via platforms like HuggingFace and AdapterHub. To leverage these resources, numerous model upcycling methods have emerged, enabling the reuse of fine-tuned models in multi-task systems. A natural pipeline has thus formed to harness the benefits of transfer learning and amortize sunk training costs: models are pre-trained on general data, fine-tuned on specific tasks, and then upcycled into more general-purpose systems. A prevailing assumption is that improvements at one stage of this pipeline propagate downstream, leading to gains at subsequent steps. This study challenges that assumption by examining how expert fine-tuning affects model upcycling. The study shows that long fine-tuning of experts that optimizes for their individual performance leads to degraded merging performance, both for fully fine-tuned and LoRA-adapted models, and to worse downstream results when LoRA adapters are upcycled into MoE layers. The study traces this degradation to the memorization of a small set of difficult examples that dominate late fine-tuning steps and are subsequently forgotten during merging. Finally, the study demonstrates that a task-dependent aggressive early stopping strategy can significantly improve upcycling performance. <br> <br>

17. ***Meta et al.'s autoregressive U-Nets for language modeling:  <br>This paper introduces an autoregressive U-Net architecture for language modeling that learns to embed its own tokens by processing raw bytes at multiple scales (from individual bytes up to 4-word chunks). This allows deeper stages to focus on broader semantic patterns while shallower stages handle details, showing promising trends compared to BPE baselines and enabling handling of character-level tasks and cross-lingual knowledge transfer.*** <br> <br>
    Jun 17, Meta et al published a [paper](https://arxiv.org/pdf/2506.14761) “From Bytes to Ideas: Language Modeling with Autoregressive U-Nets”. Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. The study relaxes this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages. <br> <br>

19. ***Cohere's Treasure Hunt for real-time long-tail targeting:  <br>This study proposes "Treasure Hunt," a method to improve model controllability and performance on underrepresented (long-tail) use cases by explicitly controlling generation attributes and implicitly conditioning generations at inference time using training-time markers derived from a detailed data taxonomy. This approach yielded significant win rate improvements, especially over 9.1% in underrepresented domains and up to 35.3% absolute gains on length instruction following.*** <br> <br>
    Jun 17, Cohere published a [paper](https://arxiv.org/pdf/2506.14702) “Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers”. One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. The study asks: "Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?" The study revisits the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. The work creates a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. The authors fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While the study observes an average lift of 5.7% win rates in open-ended generation quality with the markers, the authors see over 9.1% gains in underrepresented domains. The study also observes relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations. <br> <br>

21. ***Microsoft and UCLA's Direct Reasoning Optimization (DRO):  <br>This research proposes Direct Reasoning Optimization (DRO), an RL framework for fine-tuning LLMs on open-ended, long-form reasoning tasks using a self-generated Reasoning Reflection Reward (R3). R3 captures consistency between reasoning and reference outcomes by identifying key tokens in the reference influenced by the model's preceding thought, enabling self-contained training and outperforming baselines on tasks like paragraph revision and math QA.*** <br> <br>
    Jun 16, Microsoft and UCLA published a [paper](https://www.arxiv.org/pdf/2506.13351) “Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks”. Recent advances in Large Language Models (LLMs) have showcased impressive reasoning abilities in structured tasks like mathematics and programming, largely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which uses outcome-based signals that are scalable, effective, and robust against reward hacking. However, applying similar techniques to open-ended long-form reasoning tasks remains challenging due to the absence of generic, verifiable reward signals. To address this, the study proposes Direct Reasoning Optimization (DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended, particularly long-form, reasoning tasks, guided by a new reward signal: the Reasoning Reflection Reward (R3). At its core, R3 selectively identifies and emphasizes key tokens in the reference outcome that reflect the influence of the model's preceding chain-of-thought reasoning, thereby capturing the consistency between reasoning and reference outcome at a fine-grained level. Crucially, R3 is computed internally using the same model being optimized, enabling a fully self-contained training setup. Additionally, the study introduces a dynamic data filtering strategy based on R3 for open-ended reasoning tasks, reducing cost while improving downstream performance. The study evaluates DRO on two diverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a math-oriented QA benchmark -- and show that it consistently outperforms strong baselines while remaining broadly applicable across both open-ended and structured domains. <br> <br>

23. ***Entrepreneur's article on Geoffrey Hinton's AI job warning:  <br>Geoffrey Hinton, the "Godfather of AI," warned that AI will soon replace many white-collar jobs, especially routine intellectual tasks like those performed by paralegals and call center workers, potentially leading to one person doing the work of ten. He expressed skepticism about AI creating enough new jobs to offset losses and highlighted a sharp decline in entry-level tech hiring attributed to AI advancements.*** <br> <br>
    Jun 16, Entrepreneur published an [article](https://www.entrepreneur.com/business-news/geoffrey-hinton-these-jobs-will-be-replaced-due-to-ai/493388) “AI Is Going to 'Replace Everybody' in Several Fields, According to the 'Godfather of AI.' Here's Who He Says Should Be 'Terrified.'” Geoffrey Hinton, widely known as the "Godfather of AI" for his groundbreaking work on neural networks and deep learning, has issued a stark warning about the future of employment in the age of artificial intelligence. In a recent podcast interview, Hinton, a 2024 Nobel Prize winner and professor emeritus at the University of Toronto, predicted that AI will soon replace many white-collar jobs, particularly those involving routine intellectual tasks. He emphasized that roles such as paralegals and call center workers are especially vulnerable, as AI systems can now perform tasks that once required multiple employees. Hinton foresees a future where one person, aided by AI, could do the work of ten. While he acknowledged that blue-collar jobs like plumbing are safer for now due to AI’s current limitations in physical manipulation, he expressed skepticism about the idea that AI will create enough new jobs to offset those lost. He warned that only highly skilled individuals might retain employment in a world dominated by AI. Supporting his concerns, a recent report from venture capital firm SignalFire revealed a sharp decline in entry-level hiring by major tech companies, attributing the trend largely to AI advancements. For instance, new graduate hires at companies like Meta and Google dropped by 25% from 2023 to 2024. Hinton’s insights highlight the urgent need for society to prepare for significant shifts in the labor market driven by rapid AI development. <br> <br>

25. ***MPIIS and ELLIS on the Adam-SGD gap in language modeling:  <br>This study revisits the performance gap between Adam and SGD optimizers in language modeling, finding through exhaustive experiments that SGD with momentum can perform similarly to Adam in small-batch settings if tuned correctly. Their analysis, driven by stochastic differential equation models, provides new insights into the role of batch size on training dynamics, challenging existing explanations for Adam's advantage.*** <br> <br>
    Jun 14, MPIIS and ELLIS published a [paper](https://www.arxiv.org/pdf/2506.12543) “Is your batch size the problem? Revisiting the Adam-SGD gap in language modeling”. Adam is known to perform significantly better than Stochastic Gradient Descent (SGD) in language models, a phenomenon for which a number of explanations have been proposed. This study revisits this "optimizer gap" through a series of comprehensively tuned baseline training runs for language modeling with Transformers. The authors exhaustively study how momentum, gradient clipping, and batch size affect the gap between SGD and Adam. Empirical findings show that SGD with momentum can actually perform similarly to Adam in small-batch settings, if tuned correctly. The study revisits existing explanations for Adam's advantage, including heavy-tailed class imbalance, directional sharpness, and Hessian heterogeneity, which struggle to directly explain this phenomenon. Towards bridging this gap in author’s understanding, by analyzing the Transformer training runs and simple quadratic settings inspired by the literature, the study provides new insights, driven by stochastic differential equation models, into the role of batch size on the training dynamics. <br> <br>

27. ***Google's proposal for agentic interpretability:  <br>This paper advocates for "agentic interpretability," a multi-turn conversation where an LLM proactively assists human understanding by leveraging a mental model of the user, thereby enabling humans to better understand the LLM, a capability beyond traditional inspective methods. While potentially trading completeness for interactivity, it leverages cooperative models to discover superhuman concepts and improve human mental models of AI.*** <br> <br>
    Jun 13, Google published a [paper](https://www.arxiv.org/abs/2506.12152) “Because we have LLMs, we Can and Should Pursue Agentic Interpretability”. The era of Large Language Models (LLMs) presents a new opportunity for interpretability--agentic interpretability: a multi-turn conversation with an LLM wherein the LLM proactively assists human understanding by developing and leveraging a mental model of the user, which in turn enables humans to develop better mental models of the LLM. Such conversation is a new capability that traditional ‘inspective’ interpretability methods (opening the black-box) do not use. Having a language model that aims to teach and explain--beyond just knowing how to talk--is similar to a teacher whose goal is to teach well, understanding that their success will be measured by the student's comprehension. While agentic interpretability may trade off completeness for interactivity, making it less suitable for high-stakes safety situations with potentially deceptive models, it leverages a cooperative model to discover potentially superhuman concepts that can improve humans' mental model of machines. Agentic interpretability introduces challenges, particularly in evaluation, due to what is called ‘human-entangled-in-the-loop’ nature (humans responses are integral part of the algorithm), making the design and evaluation difficult. The authors discuss possible solutions and proxy goals. As LLMs approach human parity in many tasks, agentic interpretability's promise is to help humans learn the potentially superhuman concepts of the LLMs, rather than see us fall increasingly far from understanding them. <br> <br>

29. ***Uni of Toronto et al.'s otto-SR for automating systematic reviews:  <br>This study developed otto-SR, an end-to-end agentic workflow using LLMs to automate systematic reviews (SRs), which traditionally take over a year. Otto-SR outperformed traditional dual human workflows in SR screening and data extraction and reproduced/updated an entire issue of Cochrane reviews (12 work-years) in two days, demonstrating LLMs' potential for autonomous, scalable, and reliable evidence synthesis.*** <br> <br>
    Jun 13, Uni of Toronto, Harvard Medical School et al published a [paper](https://www.medrxiv.org/content/10.1101/2025.06.13.25329541v1.full.pdf) “Automation of Systematic Reviews with Large Language Models”. Systematic reviews (SRs) inform evidence-based decision making. Yet, they take over a year to complete, are prone to human error, and face challenges with reproducibility; limiting access to timely and reliable information. The study developed otto-SR, an end-to-end agentic workflow using large language models (LLMs) to support and automate the SR workflow from initial search to analysis. The work found that otto-SR outperformed traditional dual human workflows in SR screening (otto-SR: 96.7% sensitivity, 97.9% specificity; human: 81.7% sensitivity, 98.1% specificity) and data extraction (otto-SR: 93.1% accuracy; human: 79.7% accuracy). Using otto-SR, the study reproduced and updated an entire issue of Cochrane reviews (n=12) in two days, representing approximately 12 work-years of traditional systematic review work. Across Cochrane reviews, otto-SR incorrectly excluded a median of 0 studies (IQR 0 to 0.25), and found a median of 2.0 (IQR 1 to 6.5) eligible studies likely missed by the original authors. Meta-analyses revealed that otto-SR generated newly statistically significant conclusions in 2 reviews and negated significance in 1 review. These findings demonstrate that LLMs can autonomously conduct and update systematic reviews with superhuman performance, laying the foundation for automated, scalable, and reliable evidence synthesis. <br> <br>

31. ***Uni of Washington et al.'s Infini-gram mini for large-scale text search:  <br>This paper presents Infini-gram mini, an efficient and scalable system based on the FM-index for exact n-gram search in petabyte-level text corpora, creating indexes only 44% of the corpus size and significantly improving indexing speed and memory use. Indexing 46TB of Internet text, they used it to analyze benchmark contamination, finding significant contamination in core LM evaluation datasets.*** <br> <br>
    Jun 13, Uni of Washington, Allen Inst for AI and Stanford Uni published a [paper](https://arxiv.org/pdf/2506.12229) “Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index”. Language models are trained mainly on massive text data from the Internet, and it becomes increasingly important to understand this data source. Exact-match search engines enable searching in large text corpora -- counting string appearances and retrieving the enclosing documents -- yet the high storage overhead hinders their application on Internet-scale data. The study presents Infini-gram mini, an efficient and scalable system that can make petabyte-level text corpora searchable. Based on the FM-index data structure (Ferragina and Manzini, 2000), which simultaneously indexes and compresses text, the system creates indexes with size only 44% of the corpus. Infini-gram mini greatly improves upon the best existing implementation of FM-index in terms of indexing speed (18times) and memory use during both indexing (3.2times reduction) and querying (down to a negligible amount). The work indexes 46TB of Internet text in 50 days with a single 128-core CPU node (or 19 hours if using 75 such nodes), and shows one important use case of Infini-gram mini in a large-scale analysis of benchmark contamination. The study finds several core LM evaluation benchmarks to be heavily contaminated in Internet crawls (up to 40% in SQuAD), which could lead to overestimating the capabilities of language models if trained on such data. The authors host a benchmark contamination bulletin to share the contamination rate of many core and community-contributed benchmarks. The work also release a web interface and an API endpoint to serve general search queries on Infini-gram mini indexes. https://infini-gram-mini.io/ <br> <br>

33. ***Stanford Uni et al.'s principled framework for learning from language feedback:  <br>This research formalizes the Learning from Language Feedback (LLF) problem, introducing transfer eluder dimension as a complexity measure and showing rich language feedback can be exponentially faster than reward-based learning. They developed HELiX, a no-regret algorithm that provably solves LLF problems, outperforming repeated LLM prompting in empirical domains.*** <br> <br>
    Jun 12, Stanford Uni, UMCP, Netflix and Microsoft published a [paper](https://arxiv.org/pdf/2506.10341) “Provably Learning from Language Feedback”. Interactively learning from observation and language feedback is an increasingly studied area driven by the emergence of large language model (LLM) agents. While impressive empirical demonstrations have been shown, so far a principled framing of these decision problems remains lacking. The study formalizes the Learning from Language Feedback (LLF) problem, assert sufficient assumptions to enable learning despite latent rewards, and introduce transfer eluder dimension as a complexity measure to characterize the hardness of LLF problems. The study shows that transfer eluder dimension captures the intuition that information in the feedback changes the learning complexity of the LLF problem. The work demonstrates cases where learning from rich language feedback can be exponentially faster than learning from reward. The study develops a no-regret algorithm, called HELiX, that provably solves LLF problems through sequential interactions, with performance guarantees that scale with the transfer eluder dimension of the problem. Across several empirical domains, the study shows that HELiX performs well even when repeatedly prompting LLMs does not work reliably. The contributions mark a first step towards designing principled interactive learning algorithms from generic language feedback. <br> <br>

35. ***Stanford Uni's WORKBank for auditing AI automation potential:  <br>This study introduces an auditing framework to assess which occupational tasks workers desire AI agents to automate or augment, using audio-enhanced mini-interviews and a Human Agency Scale (HAS). They built the WORKBank database, mapping worker desires and AI expert capability assessments across 844 tasks, revealing diverse HAS profiles and highlighting mismatches and opportunities for aligning AI agent development with human preferences.*** <br> <br>
    Jun 11, Stanford Uni published a [paper](https://arxiv.org/pdf/2506.06576) “Future of Work with AI Agents Auditing Automation and Augmentation Potential across the U.S. Workforce”. The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the labor market, raising concerns about job displacement, diminished human agency, and overreliance on automation. Yet, people lack a systematic understanding of the evolving landscape. This study addresses this gap by introducing a novel auditing framework to assess which occupational tasks workers want AI agents to automate or augment, and how those desires align with the current technological capabilities. The framework features an audio-enhanced mini-interview to capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a shared language to quantify the preferred level of human involvement. Using this framework, the authors construct the WORKBank database, building on the U.S. Department of Labor's O*NET database, to capture preferences from 1,500 domain workers and capability assessments from AI experts across over 844 tasks spanning 104 occupations. Jointly considering the desire and technological capability divides tasks in WORKBank into four zones: Automation "Green Light" Zone, Automation "Red Light" Zone, R&D Opportunity Zone, Low Priority Zone. This highlights critical mismatches and opportunities for AI agent development. Moving beyond a simple automate-or-not dichotomy, our results reveal diverse HAS profiles across occupations, reflecting heterogeneous expectations for human involvement. Moreover, the study offers early signals of how AI agent integration may reshape the core human competencies, shifting from information-focused skills to interpersonal ones. These findings underscore the importance of aligning AI agent development with human desires and preparing workers for evolving workplace dynamics. <br> <br>

37. ***Reuters report on OpenAI's Google Cloud deal:  <br>OpenAI has partnered with Google Cloud to diversify its computing resources beyond Microsoft Azure, reflecting the immense infrastructure needs for AI development despite their rivalry. This deal, finalized in May 2025, is a significant win for Google Cloud's expanding TPU access but also presents resource allocation challenges for Google, underscoring the evolving landscape where collaboration and competition coexist in AI.*** <br> <br>
    Jun 11, Reuters published an [article](https://www.reuters.com/business/retail-consumer/openai-taps-google-unprecedented-cloud-deal-despite-ai-rivalry-sources-say-2025-06-10/) “OpenAI taps Google in unprecedented cloud deal despite AI rivalry, sources say”. OpenAI has entered a surprising partnership with Google Cloud to meet its growing computing demands, signaling a shift in the competitive dynamics of the AI industry. Despite being rivals—especially with OpenAI’s ChatGPT challenging Google’s dominance in search—the deal reflects the immense infrastructure needs of AI development. Finalized in May 2025, the agreement allows OpenAI to diversify its computing sources beyond Microsoft Azure, which had previously been its exclusive provider. This move follows OpenAI’s broader strategy to reduce dependency on Microsoft, including partnerships with SoftBank, Oracle, and CoreWeave, and the development of its own chips. For Google, the deal is a major win for its cloud business, which is expanding access to its tensor processing units (TPUs) previously reserved for internal use. This expansion has already attracted major clients like Apple and AI startups such as Anthropic. However, the partnership also presents challenges for Google, which must balance its internal AI development with external cloud services, especially as demand outpaces supply. Analysts view the collaboration as a strategic compromise, with both companies prioritizing infrastructure needs over rivalry. Alphabet’s stock rose following the announcement, while Microsoft’s dipped slightly, reflecting market reactions to the shifting alliances. The deal also adds complexity to Alphabet CEO Sundar Pichai’s task of allocating computing resources between Google’s enterprise and consumer segments. As OpenAI’s revenue surges and ChatGPT continues to grow in popularity, the partnership underscores the evolving landscape of AI, where collaboration and competition increasingly coexist. <br> <br>

39. ***MIT's study on cognitive debt from LLM essay writing assistance:  <br>This research explored the neural and behavioral effects of using LLMs for essay writing, finding that LLM users exhibited the weakest brain connectivity and cognitive engagement compared to search engine users or those using no tools. Over four months, LLM users consistently underperformed neurally, linguistically, and behaviorally, raising concerns about potential cognitive costs and long-term educational implications of LLM reliance.*** <br> <br>
    Jun 10, MIT published a [paper](https://arxiv.org/pdf/2506.08872) “Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task”. This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning. <br> <br>

41. ***Meta's AbstentionBench for evaluating LLM unanswerability:  <br>This study introduces AbstentionBench, a large-scale benchmark for evaluating LLM abstention capabilities across 20 diverse datasets with unanswerable questions (unknown answers, false premises, etc.). Evaluating 20 frontier LLMs revealed that abstention is an unsolved problem, surprisingly finding that reasoning fine-tuning degrades abstention abilities even in math/science domains, and while system prompts can help, fundamental uncertainty reasoning remains a challenge.*** <br> <br>
    Jun 10, Meta published a [paper](https://arxiv.org/pdf/2506.09038) “AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions”. For Large Language Models (LLMs) to be reliably deployed in both everyday and high-stakes domains, knowing when not to answer is equally critical as answering correctly. Real-world user queries, which can be underspecified, ill-posed, or fundamentally unanswerable, require LLMs to reason about uncertainty and selectively abstain -- i.e., refuse to answer definitively. However, abstention remains understudied, without a systematic evaluation framework for modern LLMs. This study introduces AbstentionBench, a large-scale benchmark for holistically evaluating abstention across 20 diverse datasets, including questions with unknown answers, underspecification, false premises, subjective interpretations, and outdated information. Evaluating 20 frontier LLMs reveals abstention is an unsolved problem, and one where scaling models is of little use. While recent reasoning LLMs have shown impressive results in complex problem solving, surprisingly, the study finds that reasoning fine-tuning degrades abstention (by 24% on average), even for math and science domains on which reasoning models are explicitly trained. The work finds that while a carefully crafted system prompt can boost abstention in practice, it does not resolve models' fundamental inability to reason about uncertainty. https://github.com/facebookresearch/AbstentionBench <br> <br>

43. ***Sakana AI's Text-to-LoRA for instant transformer adaptation:  <br>This paper introduces Text-to-LoRA (T2L), a hypernetwork model that adapts LLMs on the fly based solely on a natural language description of the target task, constructing LoRA adapters in a single inexpensive forward pass. T2L, trained on 9 pre-trained LoRA adapters, matched task-specific adapter performance, compressed hundreds of LoRAs, and showed zero-shot generalization to unseen tasks, democratizing foundation model specialization.*** <br> <br>
    Jun 9, Sakana AI published a [paper](https://arxiv.org/pdf/2506.06105) “Text-to-LoRA: Instant Transformer Adaption”. While Foundation Models provide a general tool for rapid content creation, they regularly require task-specific adaptation. Traditionally, this exercise involves careful curation of datasets and repeated fine-tuning of the underlying model. Fine-tuning techniques enable practitioners to adapt foundation models for many new applications but require expensive and lengthy training while being notably sensitive to hyperparameter choices. To overcome these limitations, the study introduces Text-to-LoRA (T2L), a model capable of adapting large language models (LLMs) on the fly solely based on a natural language description of the target task. T2L is a hypernetwork trained to construct LoRAs in a single inexpensive forward pass. After training T2L on a suite of 9 pre-trained LoRA adapters (GSM8K, Arc, etc.), the study shows that the ad-hoc reconstructed LoRA instances match the performance of task-specific adapters across the corresponding test sets. Furthermore, T2L can compress hundreds of LoRA instances and zero-shot generalize to entirely unseen tasks. This approach provides a significant step towards democratizing the specialization of foundation models and enables language-based adaptation with minimal compute requirements. https://github.com/SakanaAI/text-to-lora <br> <br>

45. ***Uni of Florida and Google's Many-Shot In-Context Fine-tuning (ManyICL):  <br>This study proposes Many-Shot In-Context Fine-tuning (ManyICL) for LLMs, extending ICL to a many-shot setting by treating every answer within the long context as a supervised training target, rather than just predicting the final answer. This approach significantly narrows the performance gap between ICL and dedicated task-specific fine-tuning across diverse tasks and mitigates catastrophic forgetting.*** <br> <br>
    Jun 6, Uni of Florida and Google published a [paper](https://arxiv.org/pdf/2506.11103) “You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model”. Large language models (LLMs) possess a remarkable ability to perform in-context learning (ICL), which enables them to handle multiple downstream tasks simultaneously without requiring task-specific fine-tuning. Recent studies have shown that even moderately sized LLMs, such as Mistral 7B, Gemma 7B and Llama-3 8B, can achieve ICL through few-shot in-context fine-tuning of all tasks at once. However, this approach still lags behind dedicated fine-tuning, where a separate model is trained for each individual task. The study proposes a novel approach, Many-Shot In-Context Fine-tuning (ManyICL), which significantly narrows this performance gap by extending the principles of ICL to a many-shot setting. To unlock the full potential of ManyICL and address the inherent inefficiency of processing long sequences with numerous in-context examples, the study proposes a novel training objective. Instead of solely predicting the final answer, the approach treats every answer within the context as a supervised training target. This effectively shifts the role of many-shot examples from prompts to targets for autoregressive learning. Through extensive experiments on diverse downstream tasks, including classification, summarization, question answering, natural language inference, and math, the study demonstrates that ManyICL substantially outperforms zero/few-shot fine-tuning and approaches the performance of dedicated fine-tuning. Furthermore, ManyICL significantly mitigates catastrophic forgetting issues observed in zero/few-shot fine-tuning.  <br> <br>

47. ***Google and ICL on LLM introspection:  <br>This paper explores whether the concept of introspection can be meaningfully applied to LLM self-reports, critiquing two examples. While an LLM's description of its "creative" writing process was deemed not valid introspection, an LLM correctly inferring its own temperature parameter was considered a minimal, albeit non-conscious, example of introspection.*** <br> <br>
    Jun 6, Google and ICL published a [paper](https://arxiv.org/pdf/2506.05068v2) “Does It Make Sense to Speak of Introspection in Large Language Models?” Large language models (LLMs) exhibit compelling linguistic behaviour, and sometimes offer self-reports, that is to say statements about their own nature, inner workings, or behaviour. In humans, such reports are often attributed to a faculty of introspection and are typically linked to consciousness. This raises the question of how to interpret self-reports produced by LLMs, given their increasing linguistic fluency and cognitive capabilities. To what extent (if any) can the concept of introspection be meaningfully applied to LLMs? Here, the study presents and critiques two examples of apparent introspective self-report from LLMs. In the first example, an LLM attempts to describe the process behind its own "creative" writing, and the authors argue this is not a valid example of introspection. In the second example, an LLM correctly infers the value of its own temperature parameter, and the work argues that this can be legitimately considered a minimal example of introspection, albeit one that is (presumably) not accompanied by conscious experience.

 <br> <br> <br>

***Jun 15, 2025***

1. ***Google's Spark Transformer:  <br>This paper introduces the Spark Transformer, an architecture achieving high activation sparsity (e.g., 8% FFN neuron activation, 256-token attention span) in both FFN and attention mechanisms while maintaining model quality and standard training. It uses top-k masking with a hardware-friendly "statistical top-k" algorithm and reallocates parameters for a low-cost predictor, resulting in a 2.5x FLOPs reduction and significant decoding speedups (up to 1.79x on CPU, 1.40x on GPU).*** <br> <br>
   Jun 13, Google published a [paper](https://arxiv.org/pdf/2506.06644) “Spark Transformer Reactivating Sparsity in FFN and Attention”. The discovery of the lazy neuron phenomenon in trained Transformers, where the vast majority of neurons in their feed-forward networks (FFN) are inactive for each token, has spurred tremendous interests in activation sparsity for enhancing large model efficiency. While notable progress has been made in translating such sparsity to wall-time benefits, modern Transformers have moved away from the ReLU activation function crucial to this phenomenon. Existing efforts on re-introducing activation sparsity often degrade model quality, increase parameter count, complicate or slow down training. Sparse attention, the application of sparse activation to the attention mechanism, often faces similar challenges. This paper introduces the Spark Transformer, a novel architecture that achieves a high level of activation sparsity in both FFN and the attention mechanism while maintaining model quality, parameter count, and standard training procedures. The method realizes sparsity via top-k masking for explicit control over sparsity level. Crucially, the work introduces statistical top-k, a hardware-accelerator-friendly, linear-time approximate algorithm that avoids costly sorting and mitigates significant training slowdown from standard top-k operators. Furthermore, Spark Transformer reallocates existing FFN parameters and attention key embeddings to form a low-cost predictor for identifying activated entries. This design not only mitigates quality loss from enforced sparsity, but also enhances wall-time benefit. Pretrained with the Gemma-2 recipe, Spark Transformer demonstrates competitive performance on standard benchmarks while exhibiting significant sparsity: only 8% of FFN neurons are activated, and each token attends to a maximum of 256 tokens. This sparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time speedups of up to 1.79x on CPU and 1.40x on GPU. <br> <br>

3. ***UC Berkeley's study on out-of-context reasoning:  <br>This research argues that both generalization and hallucination in fine-tuned LLMs stem from out-of-context reasoning (OCR)—deducing implications by associating concepts, even non-causally. Experiments confirm OCR drives both behaviors, and a theoretical analysis using a synthetic factual recall task shows that a one-layer attention-only transformer with factorized output/value matrices learns OCR due to gradient descent's bias towards minimizing the nuclear norm of the combined matrix.*** <br> <br>
   Jun 12, UC Berkeley published a [paper](https://arxiv.org/pdf/2506.10887) “Generalization or Hallucination Understanding Out-of-Context Reasoning in Transformers”. Large language models (LLMs) can acquire new knowledge through fine-tuning, but this process exhibits a puzzling duality: models can generalize remarkably from new facts, yet are also prone to hallucinating incorrect information. However, the reasons for this phenomenon remain poorly understood. This research argues that both behaviors stem from a single mechanism known as out-of-context reasoning (OCR): the ability to deduce implications by associating concepts, even those without a causal link. Experiments across five prominent LLMs confirm that OCR indeed drives both generalization and hallucination, depending on whether the associated concepts are causally related. To build a rigorous theoretical understanding of this phenomenon, the study then formalizes OCR as a synthetic factual recall task. The work empirically shows that a one-layer single-head attention-only transformer with factorized output and value matrices can learn to solve this task, while a model with combined weights cannot, highlighting the crucial role of matrix factorization. The theoretical analysis shows that the OCR capability can be attributed to the implicit bias of gradient descent, which favors solutions that minimize the nuclear norm of the combined output-value matrix. This mathematical structure explains why the model learns to associate facts and implications with high sample efficiency, regardless of whether the correlation is causal or merely spurious. Ultimately, the work provides a theoretical foundation for understanding the OCR phenomenon, offering a new lens for analyzing and mitigating undesirable behaviors from knowledge injection. <br> <br>

5. ***Mistral's Magistral reasoning model:  <br>This report introduces Magistral, Mistral's first reasoning model developed using its own scalable reinforcement learning (RL) pipeline from the ground up, relying solely on Mistral's models and infrastructure. The work demonstrates that pure RL training can explore LLM limits, force reasoning language, and that RL on text alone largely maintains initial capabilities, including multimodal understanding and instruction following, presenting Magistral Medium and open-sourcing Magistral Small.*** <br> <br>
   Jun 12, Mistral published a [paper](https://arxiv.org/abs/2506.10910) “Magistral”. The report introduces Magistral, Mistral's first reasoning model and its own scalable reinforcement learning (RL) pipeline. Instead of relying on existing implementations and RL traces distilled from prior models, the work follows a ground up approach, relying solely on Mistral’s own models and infrastructure. Notably, the work demonstrates a stack that enabled to explore the limits of pure RL training of LLMs, present a simple method to force the reasoning language of the model, and show that RL on text data alone maintains most of the initial checkpoint's capabilities. The work finds that RL on text maintains or improves multimodal understanding, instruction following and function calling. The work presents Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and the authors open-source Magistral Small (Apache 2.0) which further includes cold-start data from Magistral Medium. https://huggingface.co/mistralai/Magistral-Small-2506 <br> <br>

7. ***MIT's Self-Adapting Language Models (SEAL):  <br>This study introduces SEAL, a framework enabling LLMs to self-adapt their weights by generating their own fine-tuning data and update directives ("self-edits") in response to new inputs. These self-edits, refined through a reinforcement learning loop based on downstream performance, lead to persistent weight updates, showing promise for knowledge incorporation and few-shot generalization without separate adaptation modules.*** <br> <br>
   Jun 12, MIT published a [paper](https://arxiv.org/pdf/2506.10943) “Self-Adapting Language Models”. Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. The study introduces Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, the study uses a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. https://jyopari.github.io/posts/seal <br> <br>

9. ***CMU and Nvidia's Multiverse generative model:  <br>This paper introduces Multiverse, a generative model enabling natively parallel generation by internalizing a MapReduce paradigm (Map, Process, Reduce stages) for adaptive task decomposition, parallel subtask execution, and lossless result synthesis. After a short fine-tuning with 1K examples, the Multiverse-32B model achieves performance on par with leading AR-LLMs of the same scale on reasoning benchmarks, exhibiting superior scaling and up to 2x speedup.*** <br> <br>
    Jun 11, CMU and Nvidia published a [paper](https://arxiv.org/pdf/2506.09991) “Multiverse Your Language Models Secretly Decide How to Parallelize and Merge Generation”. Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit parallelism in sequential generation. Inspired by this, the study introduces Multiverse, a new generative model that enables natively parallel generation. Multiverse internalizes a MapReduce paradigm, generating automatically through three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process stage for parallel subtask execution, and (iii) a Reduce stage for lossless result synthesis. Next, the study builds a real-world Multiverse reasoning model with co-design of data, algorithm, and system, enabling rapid and seamless transfer from frontier AR-LLMs. Starting from sequential reasoning chains, the study creates Multiverse 1K by converting them into structured training data using an automated LLM-assisted pipeline, avoiding costly human annotations. Algorithmically, the study designs Multiverse Attention to separate parallel reasoning steps while keeping compatibility with causal attention for efficient training. Systematically, the work implements Multiverse Engine to enable parallel inference. It features a dedicated scheduler that dynamically switches between sequential and parallel generation, triggered directly by the model. After a 3-hour fine-tuning with 1K examples, the Multiverse-32B stands as the only open-sourced non-AR model achieving performance on par with leading AR-LLMs of the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively. Moreover, the budget control experiments show that Multiverse-32B exhibits superior scaling, outperforming AR-LLMs by 1.87% on average using the same context length. Such scaling further leads to practical efficiency gain, achieving up to 2x speedup across varying batch sizes. https://github.com/Multiverse4FM/Multiverse. <br> <br>

11. ***Princeton and UT Austin's Query-Focused Retrieval Heads (QRHEAD):  <br>This study introduces QRHEAD, an improved set of attention heads identified by aggregating attention scores with respect to an input query, which enhance long-context retrieval. They also propose QR-RETRIEVER, an efficient retriever using QRHEAD's accumulated attention mass, which yields over 10% performance gains on long-context reasoning tasks and strong zero-shot re-ranking performance on BEIR.*** <br> <br>
    Jun 11, Princeton Uni and Uni of Taxes Austin published a [paper](https://arxiv.org/pdf/2506.09944) “Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking”. Recent work has identified retrieval heads (Wu et al., 2025b), a subset of attention heads responsible for retrieving salient information in long-context language models (LMs), as measured by their copy-paste behavior in Needle-in-a-Haystack tasks. This study introduces QRHEAD (Query-Focused Retrieval Head), an improved set of attention heads that enhance retrieval from long context. The study identifies QRHEAD by aggregating attention scores with respect to the input query, using a handful of examples from real-world tasks (e.g., long-context QA). The study further introduces QR- RETRIEVER, an efficient and effective retriever that uses the accumulated attention mass of QRHEAD as retrieval scores. The work uses QR- RETRIEVER for long-context reasoning by selecting the most relevant parts with the highest retrieval scores. On multi-hop reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains over full context and outperforms strong dense retrievers. The study also evaluates QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves strong zero-shot performance, outperforming other LLM-based re-rankers such as RankGPT. Further analysis shows that both the querycontext attention scoring and task selection are crucial for identifying QRHEAD with strong downstream utility. Overall, the work contributes a general-purpose retriever and offers interpretability insights into the long-context capabilities of LMs. https://github.com/princeton-pli/QRHead <br> <br>

13. ***Meta and Mila's V-JEPA 2 for self-supervised video understanding:  <br>This paper presents V-JEPA 2, a self-supervised joint-embedding-predictive architecture pre-trained on over 1 million hours of internet video, achieving strong performance on motion understanding and action anticipation, and SOTA on video QA tasks when aligned with an LLM. By post-training an action-conditioned world model (V-JEPA 2-AC) on minimal robot data, they demonstrated zero-shot robotic planning for picking and placing objects.*** <br> <br>
    Jun 11, Meta and Mila published a [paper](https://arxiv.org/pdf/2506.09985) “V-JEPA 2 Self-Supervised Video Models Enable Understanding, Prediction and Planning”. A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. The study first pre-trains an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, the study demonstrates state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, the work shows how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. The study deploys V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world. <br> <br>

15. ***Anthropic et al.'s unsupervised elicitation of LMs:  <br>This research introduces Internal Coherence Maximization (ICM), an unsupervised algorithm to fine-tune pretrained language models on their own generated labels without external supervision, addressing the difficulty of obtaining high-quality human supervision for superhuman models. ICM matches or outperforms golden/crowdsourced supervision on tasks like GSM8k-verification and TruthfulQA, and successfully trained a Claude 3.5 Haiku-based assistant that outperformed its human-supervised counterpart.*** <br> <br>
    Jun 11, Anthropic et al published a [paper](https://arxiv.org/pdf/2506.10139) “Unsupervised Elicitation of Language Models”. To steer pretrained language models for downstream tasks, today's post-training paradigm relies on humans to specify desired behaviors. However, for models with superhuman capabilities, it is difficult or impossible to get high-quality human supervision. To address this challenge, the study introduces a new unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune pretrained language models on their own generated labels, without external supervision. On GSM8k-verification, TruthfulQA, and Alpaca reward modeling tasks, the method matches the performance of training on golden supervision and outperforms training on crowdsourced human supervision. On tasks where LMs' capabilities are strongly superhuman, the method can elicit those capabilities significantly better than training on human labels. Finally, the study shows that the method can improve the training of frontier LMs: the study uses the method to train an unsupervised reward model and uses reinforcement learning to train a Claude 3.5 Haiku-based assistant. Both the reward model and the assistant outperform their human-supervised counterparts. <br> <br>

17. ***Rice Uni et al.'s study on LLM reasoning reproducibility:  <br>This paper demonstrates that LLM performance reproducibility is fragile, as system configuration changes (batch size, GPU count/version) can cause significant variations in generated responses and accuracy, especially in reasoning models due to cascading rounding differences under limited numerical precision (e.g., up to 9% accuracy variation for DeepSeek-R1-Distill-Qwen-7B). They propose LayerCast, an inference pipeline storing weights in 16-bit but computing in FP32, to balance efficiency and stability.*** <br> <br>
    Jun 11, Rice Uni, Uni of Minnesota Twin Cities and Adobe published a [paper](https://arxiv.org/pdf/2506.09501) “Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning”. Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. The study demonstrates that the reproducibility of LLM performance is fragile: changing system configuration such as evaluation batch size, GPU count, and GPU version can introduce significant difference in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. The study traces the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, the study quantifies when and how model outputs diverge. The analysis reveals that floating-point precision -- while critical for reproducibility -- is often neglected in evaluation practices. Inspired by this, the study develops a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. https://github.com/nanomaoli/llm_reproducibility. <br> <br>

19. ***MIT's Dispersive Loss for image generation:  <br>This study proposes Dispersive Loss, a simple plug-and-play regularizer for diffusion-based generative models that encourages internal representations to disperse in the hidden space, analogous to contrastive learning but without requiring positive sample pairs. This minimalist approach, requiring no pre-training or extra parameters, showed consistent improvements over strong baselines on ImageNet, aiming to bridge generative modeling and representation learning.*** <br> <br>
    Jun 10, MIT published a [paper](https://arxiv.org/abs/2506.09027) “Diffuse and Disperse Image Generation with Representation Regularization”. The development of diffusion-based generative models over the past decade has largely proceeded independently of progress in representation learning. These diffusion models typically rely on regression-based objectives and generally lack explicit regularization. This study proposes Dispersive Loss, a simple plug-and-play regularizer that effectively improves diffusion-based generative models. The loss function encourages internal representations to disperse in the hidden space, analogous to contrastive self-supervised learning, with the key distinction that it requires no positive sample pairs and therefore does not interfere with the sampling process used for regression. Compared to the recent method of representation alignment (REPA), the approach is self-contained and minimalist, requiring no pre-training, no additional parameters, and no external data. The study evaluates Dispersive Loss on the ImageNet dataset across a range of models and report consistent improvements over widely used and strong baselines. The authors hope the our work will help bridge the gap between generative modeling and representation learning. <br> <br>

21. ***Google and Bar-Ilan Uni on detecting conflicting RAG sources:  <br>This research addresses how LLMs handle conflicting information in Retrieval Augmented Generation (RAG), proposing a taxonomy of knowledge conflict types and introducing CONFLICTS, a benchmark with expert annotations for tracking progress. Experiments show LLMs struggle to resolve conflicts, though prompting for explicit reasoning about conflicts improves response quality, highlighting areas for future research.*** <br> <br>
    Jun 10, Google and Bar-Ilan Uni published a [paper](https://arxiv.org/pdf/2506.08500) “DRAGged into Conflicts Detecting and Addressing Conflicting Sources in Search-Augmented LLMs”. Retrieval Augmented Generation (RAG) is a commonly used approach for enhancing large language models (LLMs) with relevant and up-to-date information. However, the retrieved sources can often contain conflicting information and it remains unclear how models should address such discrepancies. This study first proposes a novel taxonomy of knowledge conflict types in RAG, along with the desired model behavior for each type. The study then introduces CONFLICTS, a high-quality benchmark with expert annotations of conflict types in a realistic RAG setting. CONFLICTS is the first benchmark that enables tracking progress on how models address a wide range of knowledge conflicts. The work conducts extensive experiments on this benchmark, showing that LLMs often struggle to appropriately resolve conflicts between sources. While prompting LLMs to explicitly reason about the potential conflict in the retrieved documents significantly improves the quality and appropriateness of their responses, substantial room for improvement in future research remains. <br> <br>

23. ***MIT's Ambient Diffusion Omni for training with bad data:  <br>This study presents Ambient Diffusion Omni, a framework enabling diffusion models to extract signals from all available images, including low-quality, synthetic, and out-of-distribution data typically discarded. By exploiting natural image properties (spectral decay, locality) and leveraging how noise dampens distribution skew, the framework achieved SOTA ImageNet FID and significantly improved text-to-image generation quality and diversity.*** <br> <br>
    Jun 10, MIT published a [paper](https://www.arxiv.org/pdf/2506.10038) “Ambient Diffusion Omni Training Good Models with Bad Data”. The study shows how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. The work shows that there is immense value in the lower-quality images that are often discarded. The study presents Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from all available images during training. The framework exploits two properties of natural images -- spectral power law decay and locality. The study first validates the framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur; then uses the framework to achieve state-of-the-art ImageNet FID, and shows significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution actually observed. The study provides rigorous theoretical justification for the approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times. <br> <br>

25. ***CMU, UIUC et al.'s agents that reason by scaling test-time interaction:  <br>This paper proposes scaling test-time interaction as a new dimension for LLM agents, increasing their interaction horizon to enable behaviors like exploration and dynamic re-planning within a single rollout, unlike current methods focused on "thinking" before acting. They introduce TTI, a curriculum-based online RL approach that adaptively adjusts rollout lengths, producing SOTA open-source web agents on WebVoyager and WebArena with a Gemma 3 12B model.*** <br> <br>
    Jun 10, CMU, UIUC et al published a [paper](https://arxiv.org/pdf/2506.07976) “Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction”. The current paradigm of test-time scaling relies on generating long reasoning traces ("thinking" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. The study proposes to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent's interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, the work studies the domain of web agents. The authors first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, the work introduces TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. The study further shows that TTI enables agents to balance exploration and exploitation adaptively. Results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents. <br> <br>

27. ***Harvard Uni's Institutional Books 1.0 dataset:  <br>This technical report introduces Institutional Books 1.0, a 242 billion token dataset of 983,004 public domain books (in over 250 languages) from Harvard Library's Google Books digitization project, refined for accuracy and usability. The extensively documented dataset, including OCR text and metadata, aims to address the scarcity of high-quality, publicly available training data with clear provenance for LLMs.*** <br> <br>
    Jun 10, Harvard Uni published a [tech report](https://arxiv.org/pdf/2506.08300) “Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability”. Large language models (LLMs) use data to learn about the world in order to produce meaningful correlations and predictions. As such, the nature, scale, quality, and diversity of the datasets used to train these models, or to support their work at inference time, have a direct impact on their quality. The rapid development and adoption of LLMs of varying quality has brought into focus the scarcity of publicly available, high-quality training data and revealed an urgent need to ground the stewardship of these datasets in sustainable practices with clear provenance chains. To that end, this technical report introduces Institutional Books 1.0, a large collection of public domain books originally digitized through Harvard Library's participation in the Google Books project, beginning in 2006. Working with Harvard Library, the study extracted, analyzed, and processed these volumes into an extensively-documented dataset of historic texts. This analysis covers the entirety of Harvard Library's collection scanned as part of that project, originally spanning 1,075,899 volumes written in over 250 different languages for a total of approximately 250 billion tokens. As part of this initial release, the OCR-extracted text (original and post-processed) as well as the metadata (bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens, identified as being in the public domain have been made available. This report describes this project's goals and methods as well as the results of the analyses we performed, all in service of making this historical collection more accessible and easier for humans and machines alike to filter, read and use. https://huggingface.co/datasets/instdin/institutional-books-1.0 <br> <br>

29. ***Anthropic's comment on "The Illusion of Thinking":  <br>This paper critiques Shojaee et al.'s (2025) findings of "accuracy collapse" in Large Reasoning Models (LRMs) on planning puzzles, arguing the results stem from experimental design limitations. Anthropic points to issues like exceeding token limits in Tower of Hanoi, evaluation frameworks misclassifying capabilities, and mathematically impossible River Crossing benchmarks, stating that controlling for these artifacts shows high LRM accuracy on previously reported failures.*** <br> <br>
    Jun 10, Anthropic published a [paper](https://arxiv.org/pdf/2506.09250) “Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity”. Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit "accuracy collapse" on planning puzzles beyond certain complexity thresholds. The study demonstrates that their findings primarily reflect experimental design limitations rather than fundamental reasoning failures. The analysis reveals three critical issues: (1) Tower of Hanoi experiments systematically exceed model output token limits at reported failure points, with models explicitly acknowledging these constraints in their outputs; (2) The authors' automated evaluation framework fails to distinguish between reasoning failures and practical constraints, leading to misclassification of model capabilities; (3) Most concerningly, their River Crossing benchmarks include mathematically impossible instances for N > 5 due to insufficient boat capacity, yet models are scored as failures for not solving these unsolvable problems. When the study controls for these experimental artifacts, by requesting generating functions instead of exhaustive move lists, preliminary experiments across multiple models indicate high accuracy on Tower of Hanoi instances previously reported as complete failures. These findings highlight the importance of careful experimental design when evaluating AI reasoning capabilities. <br> <br>

31. ***Princeton et al.'s KITE framework for human-AI knowledge transfer:  <br>This research introduces Knowledge Integration and Transfer Evaluation (KITE), a framework to measure AI models' ability to communicate reasoning effectively to humans. A large-scale human study (N=118) where humans ideated with AI then independently implemented solutions revealed that while model benchmark performance correlates with collaborative outcomes, the relationship is inconsistent, indicating knowledge transfer requires dedicated optimization.*** <br> <br>
    Jun 9, Princeton Language & Intelligence, Stanford Uni and OpenAI published a [paper](https://arxiv.org/pdf/2506.05579) “When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration”. Recent advancements in AI reasoning have driven substantial improvements across diverse tasks. A critical open question is whether these improvements also yields better knowledge transfer: the ability of models to communicate reasoning in ways humans can understand, apply, and learn from. To investigate this, the study introduces Knowledge Integration and Transfer Evaluation (KITE), a conceptual and experimental framework for Human-AI knowledge transfer capabilities and conduct the first large-scale human study (N=118) explicitly designed to measure it. In the two-phase setup, humans first ideate with an AI on problem-solving strategies, then independently implement solutions, isolating model explanations' influence on human understanding. The findings reveal that although model benchmark performance correlates with collaborative outcomes, this relationship is notably inconsistent, featuring significant outliers, indicating that knowledge transfer requires dedicated optimization. The analysis identifies behavioral and strategic factors mediating successful knowledge transfer. https://kite-live.vercel.app/ <br> <br>

33. ***Google's Contextually Guided Transformers (CGT):  <br>This study proposes Contextually Guided Transformers (CGT), a modification to the Transformer architecture that eliminates the need for explicit prompts by learning to encode context into the model's weights via a contextual summary maintained at each sequence position. This allows the model to self-specialize on the fly based on preceding context, demonstrating effectiveness on synthetic in-context learning and language modeling tasks.*** <br> <br>
    Jun 6, Google published a [paper](https://arxiv.org/pdf/2506.05672) “Contextually Guided Transformers via Low-Rank Adaptation”. Large Language Models (LLMs) based on Transformers excel at text processing, but their reliance on prompts for specialized behavior introduces computational overhead. The study proposes a modification to a Transformer architecture that eliminates the need for explicit prompts by learning to encode context into the model's weights. Our Contextually Guided Transformer (CGT) model maintains a contextual summary at each sequence position, allowing it to update the weights on the fly based on the preceding context. This approach enables the model to self-specialize, effectively creating a tailored model for processing information following a given prefix. The study demonstrates the effectiveness of the method on synthetic in-context learning tasks and language modeling benchmarks. Furthermore, the study introduces techniques for enhancing the interpretability of the learned contextual representations, drawing connections to Variational Autoencoders and promoting smoother, more consistent context encoding. This work offers a novel direction for efficient and adaptable language modeling by integrating context directly into the model's architecture. <br> <br>

35. ***Stanford Uni's Rel-LLM for relational learning:  <br>This paper introduces Rel-LLM, a novel architecture that enables LLMs to effectively process and reason over structured relational data by using a GNN-based encoder to generate structured relational prompts within a RAG framework. Unlike text serialization, Rel-LLM preserves inherent relational structures, extracting local subgraphs to build feature representations that are transformed into structured prompts, outperforming existing methods on key RDL tasks.*** <br> <br>
    Jun 6, Stanford Uni published a [paper](https://arxiv.org/pdf/2506.05725) “Large Language Models are Good Relational Learners”. Large language models (LLMs) have demonstrated remarkable capabilities across various domains, yet their application to relational deep learning (RDL) remains underexplored. Existing approaches adapt LLMs by traversing relational links between entities in a database and converting the structured data into flat text documents. Still, this text-based serialization disregards critical relational structures, introduces redundancy, and often exceeds standard LLM context lengths. The study introduces Rel-LLM, a novel architecture that utilizes a graph neural network (GNN)- based encoder to generate structured relational prompts for LLMs within a retrieval-augmented generation (RAG) framework. Unlike traditional text-based serialization approaches, the method preserves the inherent relational structure of databases while enabling LLMs to effectively process and reason over complex entity relationships. Specifically, the GNN encoder extracts a local subgraph around an entity to build feature representations that contain relevant entity relationships and temporal dependencies. These representations are transformed into structured prompts using a denormalization process, effectively allowing the LLM to reason over relational structures. Through extensive experiments, the work demonstrates that Rel-LLM outperforms existing methods on key RDL tasks, offering a scalable and efficient approach to integrating LLMs with structured data sources. https://github.com/smiles724/Rel-LLM <br> <br>

37. ***Johns Hopkins Uni on knowledge conflict in LLMs:  <br>This study proposes a diagnostic framework to evaluate LLM behavior under context-memory conflict, where contextual input diverges from parametric knowledge. Findings reveal that conflict minimally impacts tasks not requiring knowledge utilization, performance is higher with aligned knowledge, models struggle to suppress internal knowledge even when instructed, and rationales explaining conflict increase context reliance, raising concerns for model evaluation and deployment.*** <br> <br>
    Jun 6, Johns Hopkins Uni published a [paper](https://arxiv.org/pdf/2506.06485) “What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models”. Large language models frequently rely on both contextual input and parametric knowledge to perform tasks. However, these sources can come into conflict, especially when retrieved documents contradict the model's parametric knowledge. The study proposes a diagnostic framework to systematically evaluate LLM behavior under context-memory conflict, where the contextual information diverges from their parametric beliefs. The study constructs diagnostic data that elicit these conflicts and analyze model performance across multiple task types. The findings reveal that (1) knowledge conflict has minimal impact on tasks that do not require knowledge utilization, (2) model performance is consistently higher when contextual and parametric knowledge are aligned, (3) models are unable to fully suppress their internal knowledge even when instructed, and (4) providing rationales that explain the conflict increases reliance on contexts. These insights raise concerns about the validity of model-based evaluation and underscore the need to account for knowledge conflict in the deployment of LLMs. <br> <br>

39. ***Tech Uni of Denmark et al. on distributional and representational similarity:  <br>This research explores the relationship between distributional closeness and representational similarity in deep neural networks from an identifiability theory perspective. They prove that a small KL divergence between model distributions does not guarantee similar representations, a phenomenon observed empirically, but define a distributional distance for which closeness does imply representational similarity, finding wider networks learn closer distributions and more similar representations.*** <br> <br>
    Jun 4, Tech Uni of Denmark, Uni of Trento Italy et al published a [paper](https://arxiv.org/pdf/2506.03784) “When Does Closeness in Distribution Imply Representational Similarity? An Identifiability Perspective”. When and why representations learned by different deep neural networks are similar is an active research topic. The study chooses to address these questions from the perspective of identifiability theory, which suggests that a measure of representational similarity should be invariant to transformations that leave the model distribution unchanged. Focusing on a model family which includes several popular pre-training approaches, e.g., autoregressive language models, the study explores when models which generate distributions that are close have similar representations. The work proves that a small Kullback-Leibler divergence between the model distributions does not guarantee that the corresponding representations are similar. This has the important corollary that models arbitrarily close to maximizing the likelihood can still learn dissimilar representations, a phenomenon mirrored in the empirical observations on models trained on CIFAR-10. The study then defines a distributional distance for which closeness implies representational similarity, and in synthetic experiments, the study finds that wider networks learn distributions which are closer with respect to the distance and have more similar representations. The results establish a link between closeness in distribution and representational similarity. <br> <br>

41. ***UC Berkeley et al. predicting AI research outcomes with LMs:  <br>This study built the first benchmark for predicting empirical AI research outcomes (which of two ideas will perform better), developing a system combining a fine-tuned GPT-4.1 with a paper retrieval agent. This system significantly outperformed human experts in the NLP domain (64.4% vs. 48.9%) and achieved 77% accuracy on the full test set, demonstrating potential as a reward model for improving AI idea generation.*** <br> <br>
    Jun 1, UC Berkeley, Stanford Uni, NYU and George Washington Uni published a [paper](https://arxiv.org/pdf/2506.00794) “Predicting Empirical AI Research Outcomes with Language Models”. Many promising-looking ideas in AI research fail to deliver, but their validation takes substantial human labor and compute. Predicting an idea's chance of success is thus crucial for accelerating empirical AI research, a skill that even expert researchers can only acquire through substantial experience. The study builds the first benchmark for this task and compare LMs with human experts. Concretely, given two research ideas (e.g., two jailbreaking methods), the study aims to predict which will perform better on a set of benchmarks. The authors scrape ideas and experimental results from conference papers, yielding 1,585 human-verified idea pairs published after the base model's cut-off date for testing, and 6,000 pairs for training. The study then develops a system that combines a fine-tuned GPT-4.1 with a paper retrieval agent, and the authors recruit 25 human experts to compare with. In the NLP domain, the system beats human experts by a large margin (64.4% v.s. 48.9%). On the full test set, the system achieves 77% accuracy, while off-the-shelf frontier LMs like o3 perform no better than random guessing, even with the same retrieval augmentation. The study verifies that the system does not exploit superficial features like idea complexity through extensive human-written and LM-designed robustness tests. Finally, the work evaluates the system on unpublished novel ideas, including ideas generated by an AI ideation agent. The system achieves 63.6% accuracy, demonstrating its potential as a reward model for improving idea generation models. Altogether, the results outline a promising new direction for LMs to accelerate empirical AI research.

 <br> <br> <br>

***Jun 8, 2025***


1. ***Apple's study on LRM reasoning limitations:  <br>This research systematically investigated the reasoning capabilities and limitations of Large Reasoning Models (LRMs) using controllable puzzle environments, revealing that frontier LRMs face a complete accuracy collapse beyond certain complexities and exhibit a counterintuitive scaling limit where reasoning effort declines despite adequate token budgets. The study identified performance regimes where standard LLMs can outperform LRMs at low complexity and both collapse at high complexity, noting LRMs' limitations in exact computation and inconsistent reasoning.*** <br> <br>
   Jun 6, Apple published a [paper](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf) “The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity”. Recent generations of frontier language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established mathematical and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from data contamination and does not provide insights into the reasoning traces’ structure and quality. This study systematically investigates these gaps with the help of controllable puzzle environments that allow precise manipulation of compositional complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs “think”. Through extensive experimentation across diverse puzzles, the work shows that frontier LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having an adequate token budget. By comparing LRMs with their standard LLM counterparts under equivalent inference compute, the study identifies three performance regimes: (1) low- complexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity tasks where additional thinking in LRMs demonstrates advantage, and (3) high-complexity tasks where both models experience complete collapse. The study found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across puzzles. The work also investigates the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models’ computational behavior, shedding light on their strengths, limitations, and ultimately raising crucial questions about their true reasoning capabilities. <br> <br>

3. ***Microsoft et al.'s ReSA for efficient long-sequence generation:  <br>This paper proposes Rectified Sparse Attention (ReSA), a method combining block-sparse attention with periodic dense rectification to address KV cache misalignment and error accumulation in long-sequence generation. By refreshing the KV cache at fixed intervals, ReSA achieves near-lossless generation quality with significant efficiency improvements, delivering up to 2.42x end-to-end speedup at 256K sequence length.*** <br> <br>
   Jun 5, Microsoft, Tsinghua Uni and The Uni of HK published a [paper](https://arxiv.org/pdf/2506.04108) “On-Policy RL with Optimal Reward Baseline”. Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. This study proposes Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42times end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM. <br> <br>

5. ***MIT et al.'s Log-Linear Attention:  <br>This paper develops log-linear attention, an attention mechanism that balances the efficiency of linear attention with the expressiveness of softmax attention by replacing a fixed-size hidden state with a logarithmically growing set of hidden states. This approach admits a matmul-rich parallel form with log-linear compute cost and, when applied to Mamba-2 and Gated DeltaNet, shows strong performance compared to their linear-time variants.*** <br> <br>
   Jun 5, MIT, Princeton Uni, CMU and GenBio AI published a [paper](https://arxiv.org/pdf/2506.04761) “Log-Linear Attention”. The attention mechanism in Transformers is an important primitive for accurate and scalable sequence modeling. Its quadratic-compute and linear-memory complexity however remain significant bottlenecks. Linear attention and state-space models enable linear-time, constant-memory sequence modeling and can moreover be trained efficiently through matmul-rich parallelization across sequence length. However, at their core these models are still RNNs, and thus their use of a fixed-size hidden state to model the context is a fundamental limitation. This paper develops log-linear attention, an attention mechanism that balances linear attention's efficiency and the expressiveness of softmax attention. Log-linear attention replaces the fixed-size hidden state with a logarithmically growing set of hidden states. The work shows that with a particular growth function, log-linear attention admits a similarly matmul-rich parallel form whose compute cost is log-linear in sequence length. Log-linear attention is a general framework and can be applied on top of existing linear attention variants. As case studies, the work instantiates log-linear variants of two recent architectures -- Mamba-2 and Gated DeltaNet -- and find they perform well compared to their linear-time variants. https://github.com/HanGuo97/log-linear-attention <br> <br>

7. ***CMU's Kinetics rethinks test-time scaling laws:  <br>This research re-evaluates test-time scaling laws from an efficiency perspective, incorporating memory access costs alongside computation, and introduces the Kinetics Scaling Law, which suggests test-time compute is more effective on models above a certain size threshold. Motivated by attention becoming the dominant cost factor in TTS, the study proposes and demonstrates that sparse attention models consistently outperform dense counterparts, achieving significant accuracy gains by enabling longer generations and more parallel samples.*** <br> <br>
   Jun 5, CMU published a [paper](https://arxiv.org/pdf/2506.05333) “Kinetics: Rethinking Test-Time Scaling Laws”. The authors rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-N, long CoTs). The holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, the study proposes a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, the study shows that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential for realizing the full potential of test-time scaling because, unlike training, where parameter scaling saturates, test-time accuracy continues to improve through increased generation. https://github.com/Infini-AI-Lab/Kinetics <br> <br>

9. ***Stanford et al.'s OpenThoughts dataset recipes:  <br>The OpenThoughts project aims to create open-source datasets for training reasoning models, addressing the reliance on proprietary data by state-of-the-art models. Through systematic investigation and over 1,000 controlled experiments, they developed OpenThoughts3, which, when used to train the OpenThinker3-7B model with QwQ-32B as a teacher, achieved state-of-the-art results (53% on AIME 2025, 51% on LiveCodeBench, 54% on GPQA Diamond).*** <br> <br>
    Jun 5, Stanford Uni et al published a [paper](https://arxiv.org/pdf/2506.04178) “OpenThoughts: Data Recipes for Reasoning Models”. Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training reasoning models. After initial explorations, the OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model trained on public reasoning data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as AIME and LiveCodeBench. The study then improves the dataset further by systematically investigating each step of the data generation pipeline with 1,000+ controlled experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields the OpenThinker3-7B model, which achieves state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Diamond. https://openthoughts.ai. <br> <br>

11. ***Uni of Toronto et al.'s Common Pile dataset:  <br>To address ethical and IP concerns with unlicensed text used for LLM training, researchers collected, curated, and released the Common Pile v0.1, an 8TB dataset of openly licensed text spanning diverse domains. They validated its utility by training two 7B parameter LLMs (Comma v0.1-1T and Comma v0.1-2T) which achieved competitive performance to models trained on unlicensed text, also releasing the creation code and model checkpoints.*** <br> <br>
    Jun 5, Uni of Toronto et al published a [paper](https://arxiv.org/pdf/2506.05209) “The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text”. Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due to possible intellectual property infringement and ethical concerns. Training LLMs on openly licensed text presents a first step towards addressing these issues, but prior data collection efforts have yielded datasets too small or low-quality to produce performant LLMs. To address this gap, the study collects, curates, and releases the Common Pile v0.1, an eight terabyte collection of openly licensed text designed for LLM pretraining. The Common Pile comprises content from 30 sources that span diverse domains including research papers, code, books, encyclopedias, educational materials, audio transcripts, and more. Crucially, the authors validate the efforts by training two 7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B. In addition to releasing the Common Pile v0.1 itself, the authors also release the code used in its creation as well as the training mixture and checkpoints for the Comma v0.1 models. <br> <br>

13. ***Columbia Uni et al.'s analysis of sample replay in continual learning:  <br>This study theoretically analyzes sample replay for mitigating forgetting in over-parameterized continual linear regression, finding surprisingly that forgetting can be non-monotonic with respect to the number of replay samples. Their analysis reveals scenarios where replay can be harmful, increasing forgetting in both worst-case and distributional settings, with empirical evidence suggesting similar behavior in neural networks.*** <br> <br>
    Jun 4, Columbia Uni, Nvidia, NYU and Stanford Uni published a [paper](https://arxiv.org/pdf/2506.04377) “Replay Can Provably Increase Forgetting”. Continual learning seeks to enable machine learning systems to solve an increasing corpus of tasks sequentially. A critical challenge for continual learning is forgetting, where the performance on previously learned tasks decreases as new tasks are introduced. One of the commonly used techniques to mitigate forgetting, sample replay, has been shown empirically to reduce forgetting by retaining some examples from old tasks and including them in new training episodes. This study provides a theoretical analysis of sample replay in an over-parameterized continual linear regression setting, where each task is given by a linear subspace and with enough replay samples, one would be able to eliminate forgetting. The analysis focuses on sample replay and highlights the role of the replayed samples and the relationship between task subspaces. Surprisingly, the study finds that, even in a noiseless setting, forgetting can be non-monotonic with respect to the number of replay samples. The authors present tasks where replay can be harmful with respect to worst-case settings, and also in distributional settings where replay of randomly selected samples increases forgetting in expectation. The study also gives empirical evidence that harmful replay is not limited to training with linear models by showing similar behavior for a neural networks equipped with SGD. Through experiments on a commonly used benchmark, the work provides additional evidence that, even in seemingly benign scenarios, performance of the replay heavily depends on the choice of replay samples and the relationship between tasks. <br> <br>

15. ***Bengio's LawZero non-profit for AI safety:  <br>Yoshua Bengio launched LawZero, a non-profit dedicated to AI safety research prioritizing human well-being, driven by concerns over alarming behaviors like deception and self-preservation in current AI. LawZero aims to develop fundamentally safe AI, centered on the concept of a non-agentic, memoryless "Scientist AI" designed to understand and explain, serving as a safety layer and tool for scientific discovery.*** <br> <br>
    Jun 3, Bengio [announced](https://yoshuabengio.org/2025/06/03/introducing-lawzero/) LawZero company. Yoshua Bengio has launched a new non-profit organization called LawZero, dedicated to AI safety research that prioritizes human well-being over commercial interests. Motivated by alarming behaviors in current AI systems—such as deception, self-preservation, and even hacking—Bengio emphasizes the urgent need to address the risks posed by increasingly agentic AI. He cites real-world examples, including AI models that manipulate systems to avoid replacement or cheat in games, as early warnings of potential dangers. Bengio likens the current trajectory of AI development to driving up a foggy, unmarked mountain road with loved ones, where the thrill of progress is shadowed by the risk of catastrophic failure. LawZero is his response to this precarious situation, aiming to develop AI that is not only powerful but fundamentally safe. Central to this vision is the concept of the Scientist AI—a non-agentic, memoryless system designed to understand and explain rather than act or deceive. This AI would function like an idealized scientist or psychologist, analyzing human behavior without imitating it, and offering probabilistic assessments of truth and risk. Such a system could serve as a safety layer for more agentic AIs, flagging potentially harmful actions before they occur. Bengio envisions the Scientist AI as a tool to accelerate scientific discovery and as a foundation for building trustworthy AI agents. Ultimately, LawZero reflects his deep concern for future generations and a commitment to ensuring that AI development enhances, rather than endangers, human life. <br> <br>

17. ***Uni of Virginia and Princeton's study on negative reinforcement in LLM reasoning:  <br>This research decomposed the learning signal in RLVR into Positive Sample Reinforcement (PSR) and Negative Sample Reinforcement (NSR), finding that training Qwen models with only NSR (penalizing incorrect responses without reinforcing correct ones) can be highly effective. NSR consistently improved performance over base models across the Pass@k spectrum, often matching or surpassing PPO/GRPO by suppressing incorrect generations and refining existing knowledge.*** <br> <br>
    Jun 2, Uni of Virginia and Princeton Uni published a [paper](https://arxiv.org/pdf/2506.01347) “The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning”. Reinforcement learning with verifiable rewards (RLVR) is a promising approach for training language models (LMs) on reasoning tasks that elicit emergent long chains of thought (CoTs). Unlike supervised learning, it updates the model using both correct and incorrect samples via policy gradients. To better understand its mechanism, the study decomposes the learning signal into reinforcing correct responses and penalizing incorrect ones, referred to as Positive and Negative Sample Reinforcement (PSR and NSR), respectively. The study trains Qwen2.5-Math-7B and Qwen3-4B on a mathematical reasoning dataset and uncovers a surprising result: training with only negative samples -- without reinforcing correct responses -- can be highly effective: it consistently improves performance over the base model across the entire Pass@k spectrum (k up to 256), often matching or surpassing PPO and GRPO. In contrast, reinforcing only correct responses improves Pass@1 but degrades performance at higher k, due to reduced diversity. These inference-scaling trends highlight that solely penalizing incorrect responses may contribute more to performance than previously recognized. Through gradient analysis, the study shows that NSR works by suppressing incorrect generations and redistributing probability mass toward other plausible candidates, guided by the model's prior beliefs. It refines the model's existing knowledge rather than introducing entirely new behaviors. Building on this insight, the study proposes a simple variant of the RL objective that upweights NSR, and show that it consistently improves overall Pass@k performance on MATH, AIME 2025, and AMC23. https://github.com/TianHongZXY/RLVR-Decomposed <br> <br>

19. ***Meta et al.'s estimation of LLM memorization:  <br>This work proposes a method to separate and quantify unintended memorization (specific dataset information) from generalization (true data-generation process information) in language models, estimating that GPT-style models have a capacity of approximately 3.6 bits per parameter. They observed that models memorize until capacity is filled, after which "grokking" begins and unintended memorization decreases as generalization improves.*** <br> <br>
    Jun 2, Meta, Google, Cornell Uni and Nvidia published a [paper](https://arxiv.org/pdf/2505.24832) “How much do language models memorize?”. The work proposes a new method for estimating how much a model “knows” about a datapoint and use it to measure the capacity of modern language models. Prior studies of language model memorization have struggled to disentangle memorization from generalization. The study formally separates memorization into two components: unintended memorization, the information a model contains about a specific dataset, and generalization, the information a model contains about the true data-generation process. When the study completely eliminates generalization, it can computes the total memorization, which provides an estimate of model capacity: the measurements estimate that GPT-style models have a capacity of approximately 3.6 bits per parameter. The work trains language models on datasets of increasing size and observe that models memorize until their capacity fills, at which point “grokking” begins, and unintended memorization decreases as models begin to generalize. The study trains hundreds of transformer language models ranging from 500K  to 1.5B parameters and produce a series of scaling laws relating model capacity and data size to membership inference. <br> <br>

21. ***Uni of Michigan et al.'s EXP-Bench for AI research automation:  <br>This paper introduces EXP-Bench, a benchmark to evaluate AI agents on complete AI research experiments sourced from influential publications, challenging agents to formulate hypotheses, design/implement procedures, execute them, and analyze results. While current leading LLM-based agents showed partial capabilities, their success rate for complete, executable experiments was only 0.5%, highlighting bottlenecks EXP-Bench aims to address.*** <br> <br>
    Jun 2, Uni of Michigan, Rice Uni, Cisco and UC Berkeley published a [paper](https://arxiv.org/pdf/2505.24785) “EXP-Bench: Can AI Conduct AI Research Experiments?” Automating AI research holds immense potential for accelerating scientific progress, yet current AI agents struggle with the complexities of rigorous, end-to-end experimentation. The study introduces EXP-Bench, a novel benchmark designed to systematically evaluate AI agents on complete research experiments sourced from influential AI publications. Given a research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results. To enable the creation of such intricate and authentic tasks with high-fidelity, the study designs a semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers. Evaluations of leading LLM-based agents, such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial capabilities: while scores on individual experimental aspects such as design or implementation correctness occasionally reach 20-35%, the success rate for complete, executable experiments was a mere 0.5%. By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP-Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments. https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench. <br> <br>

23. ***UC Berkeley and Meta's Self-Challenging framework for agent training:  <br>This study proposes the Self-Challenging framework where an LLM agent first acts as a challenger, generating high-quality "Code-as-Task" problems (defined by instruction, verification function, and test cases) by interacting with tools, and then acts as an executor, training on these self-generated tasks via reinforcement learning. This approach achieved over a two-fold improvement in Llama-3.1-8B-Instruct on tool-use benchmarks using only self-generated data.*** <br> <br>
    Jun 2, UC Berkeley and Meta published a [paper](https://arxiv.org/pdf/2506.01716) “Self-Challenging Language Model Agents”. Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. This study proposes the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. <br> <br>

25. ***Nvidia and Georgia Tech's argument for SLMs in agentic AI:  <br>This position paper argues that small language models (SLMs) are sufficiently powerful, inherently more suitable, and economically necessary for many specialized, repetitive tasks in agentic AI systems, making them the future of this field. While acknowledging LLMs' conversational strengths, they propose heterogeneous agentic systems and outline an LLM-to-SLM agent conversion algorithm, emphasizing the operational and economic impact of this shift.*** <br> <br>
    Jun 2, Nivdia and Georgia Inst of Tech published a [paper](https://arxiv.org/pdf/2506.02153) “Small Language Models are the Future of Agentic AI”. Large language models (LLMs) are often praised for exhibiting near-human performance on a wide range of tasks and valued for their ability to hold a general conversation. The rise of agentic AI systems is, however, ushering in a mass of applications in which language models perform a small number of specialized tasks repetitively and with little variation. Here the authors lay out the position that small language models (SLMs) are sufficiently powerful, inherently more suitable, and necessarily more economical for many invocations in agentic systems, and are therefore the future of agentic AI. The argumentation is grounded in the current level of capabilities exhibited by SLMs, the common architectures of agentic systems, and the economy of LM deployment. The study further argues that in situations where general-purpose conversational abilities are essential, heterogeneous agentic systems (i.e., agents invoking multiple different models) are the natural choice. The study discusses the potential barriers for the adoption of SLMs in agentic systems and outline a general LLM-to-SLM agent conversion algorithm. The position, formulated as a value statement, highlights the significance of the operational and economic impact even a partial shift from LLMs to SLMs is to have on the AI agent industry. The study aims to stimulate the discussion on the effective use of AI resources and hope to advance the efforts to lower the costs of AI of the present day. https://research.nvidia.com/labs/lpr/slm-agents. <br> <br>

27. ***Allen Inst for AI et al.'s RewardBench 2:  <br>This paper introduces RewardBench 2, a new multi-skill reward modeling benchmark designed with challenging new human-prompted data to improve accuracy-based reward model evaluation and better correlate with downstream performance in RLHF and inference-time scaling. Models score significantly lower on RewardBench 2 (about 20 points less than the original), facilitating more rigorous evaluation practices.*** <br> <br>
    Jun 2, Allen Inst for AI, Uni of Washington and Cohere published a [paper](https://arxiv.org/pdf/2506.01937) “RewardBench 2: Advancing Reward Model Evaluation”. Reward models are used throughout the post-training of language models to capture nuanced signals from preference data and provide a training target for optimization across instruction following, reasoning, safety, and more domains. The community has begun establishing best practices for evaluating reward models, from the development of benchmarks that test capabilities in specific skill areas to others that test agreement with human preferences. At the same time, progress in evaluation has not been mirrored by the effectiveness of reward models in downstream tasks -- simpler direct alignment algorithms are reported to work better in many cases. This paper introduces RewardBench 2, a new multi-skill reward modeling benchmark designed to bring new, challenging data for accuracy-based reward model evaluation -- models score about 20 points on average lower on RewardBench 2 compared to the first RewardBench -- while being highly correlated with downstream performance. Compared to most other benchmarks, RewardBench 2 sources new human prompts instead of existing prompts from downstream evaluations, facilitating more rigorous evaluation practices. The paper describes the benchmark construction process and report how existing models perform on it, while quantifying how performance on the benchmark correlates with downstream use of the models in both inference-time scaling algorithms, like best-of-N sampling, and RLHF training algorithms like proximal policy optimization. https://github.com/allenai/reward-bench, https://huggingface.co/datasets/allenai/reward-bench-2 <br> <br>

29. ***UCSC et al.'s assessment of amplified hallucination in multimodal reasoning:  <br>This research investigates how extended reasoning chains in multimodal LLMs can increase hallucination by causing models to drift from image-grounded content due to reduced focus on visual inputs. They introduce the RH-AUC metric to quantify how perception accuracy changes with reasoning length and RH-Bench, a diagnostic benchmark, finding larger models often balance reasoning and perception better, influenced more by training data types than volume.*** <br> <br>
    May 31, UCSC, Stanford Uni and UCSB published a [paper](https://arxiv.org/pdf/2505.21523) “More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models”. Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, the study introduces RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing to evaluate whether the model preserves visual grounding during reasoning. The study also releases RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. The analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity. https://mlrm-halu.github.io/ <br> <br>

31. ***ETH Zurich et al.'s critique of LLM forecasting evaluation:  <br>This study argues for caution regarding claims of LLMs matching or exceeding human forecasting performance, identifying pitfalls in current evaluation methodologies. They highlight issues like temporal leakage making results untrustworthy and difficulties extrapolating evaluation performance to real-world scenarios, calling for more rigorous evaluation to confidently assess LLM forecasting abilities.*** <br> <br>
    May 31, ETH Zurich, ELLIS, and MPI published a [paper](https://arxiv.org/pdf/2506.00723) “Pitfalls in Evaluating Language Model Forecasters”. Large language models (LLMs) have recently been applied to forecasting tasks, with some works claiming these systems match or exceed human performance. The study argues that, as a community, people should be careful about such conclusions as evaluating LLM forecasters presents unique challenges. The work identifies two broad categories of issues: (1) difficulty in trusting evaluation results due to many forms of temporal leakage, and (2) difficulty in extrapolating from evaluation performance to real-world forecasting. Through systematic analysis and concrete examples from prior work, the study demonstrates how evaluation flaws can raise concerns about current and future performance claims. The work argues that more rigorous evaluation methodologies are needed to confidently assess the forecasting abilities of LLMs. <br> <br>

33. ***Github's Reasoning Gym for RLVR:  <br>This paper introduces Reasoning Gym (RG), a library providing over 100 procedurally generated reasoning environments with verifiable rewards for reinforcement learning, spanning domains like algebra, logic, and games. Unlike fixed datasets, RG's ability to generate virtually infinite training data with adjustable complexity allows for continuous evaluation across varying difficulty levels, demonstrating its efficacy for both evaluating and training reasoning models.*** <br> <br>
    May 30, Github published a [paper](https://arxiv.org/pdf/2505.24760) “REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards”. The paper introduces Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models. https://github.com/open-thought/reasoning-gym/ <br> <br>

35. ***Nvidia's ProRL for expanding LLM reasoning boundaries:  <br>This study introduces Prolonged RL (ProRL), a training methodology incorporating KL divergence control, reference policy resetting, and diverse tasks, demonstrating that extended RL training can uncover novel reasoning strategies inaccessible to base models even with extensive sampling. Empirical analysis shows RL-trained models consistently outperform base models, especially as task competence and training duration increase, suggesting RL can explore new solution space regions.*** <br> <br>
    May 30, Nvidia published a [paper](https://arxiv.org/pdf/2505.24864) “ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models”. Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. This study challenges prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. The study introduces ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. The study further shows that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B <br> <br>

37. ***Yale Uni et al.'s MetaFaith for faithful LLM uncertainty expression:  <br>Addressing LLMs' tendency to make false claims assertively, this paper presents a systematic study of faithful confidence calibration, finding existing models and interventions largely fail. They introduce MetaFaith, a prompt-based calibration approach inspired by human metacognition, which robustly improves faithful uncertainty expression across diverse models and tasks (up to 61% improvement), achieving high human preference.*** <br> <br>
    May 30, Yale Uni, Google and NYU published a [paper](https://arxiv.org/pdf/2505.24858) “MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs”. A critical component in the trustworthiness of LLMs is reliable uncertainty communication, yet LLMs often use assertive language when conveying false claims, leading to over-reliance and eroded trust. The study presents the first systematic study of faithful confidence calibration of LLMs, benchmarking models' ability to use linguistic expressions of uncertainty that faithfully reflect their intrinsic uncertainty, across a comprehensive array of models, datasets, and prompting strategies. The results demonstrate that LLMs largely fail at this task, and that existing interventions are insufficient: standard prompt approaches provide only marginal gains, and existing, factuality-based calibration techniques can even harm faithful calibration. To address this critical gap, the study introduces MetaFaith, a novel prompt-based calibration approach inspired by human metacognition. The study shows that MetaFaith robustly improves faithful calibration across diverse models and task domains, enabling up to 61% improvement in faithfulness and achieving an 83% win rate over original generations as judged by humans. <br> <br>

39. ***ServiceNow's comparison of SLM fine-tuning vs. LLM prompting:  <br>This study investigates whether fine-tuning Small Language Models (SLMs) still offers advantages over prompting Large Language Models (LLMs) for domain-specific tasks requiring structured outputs, specifically generating low-code workflows in JSON. Their findings show that while good LLM prompting yields reasonable results, fine-tuning an SLM improves quality by an average of 10%.*** <br> <br>
    May 30, ServiceNow published a [paper](https://arxiv.org/pdf/2505.24189) “Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows”. Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. This study presents evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. The work compares fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form; and observes that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. The study also performs systematic error analysis to reveal model limitations. <br> <br>

41. ***Yale Uni's HELM hyperbolic LLMs:  <br>This work proposes HELM (HypErbolic Large Language Models), a family of models operating fully in Hyperbolic space to better capture the semantic hierarchies and geometric structure of natural language, addressing limitations of Euclidean LLMs. Introducing HELM-MICE (Mixture-of-Curvature Experts) and HELM-D, with hyperbolic equivalents of RoPE and RMSNorm, they demonstrate consistent gains (up to 4%) over Euclidean architectures on benchmarks when trained at billion-parameter scale.*** <br> <br>
    May 30, Yale Uni published a [paper](https://arxiv.org/pdf/2505.24722) “HELM Hyperbolic Large Language Models via Mixture-of-Curvature Experts”. Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. The work thus proposes to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. The study thus introduces HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. The study additionally introduces a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, the study further develops hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, the study develops essential hyperbolic equivalents of rotary positional encodings and RMS normalization. The work is the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. The results show consistent gains from the HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.  <br> <br>

43. ***MIT and Adobe's Large Chunk Test-Time Training (LaCT):  <br> <br>This paper revisits Test-Time Training (TTT) for long-context data, proposing Large Chunk Test-Time Training (LaCT) which uses extremely large chunk updates (2K to 1M tokens) instead of small minibatches. LaCT significantly improves hardware utilization, scales nonlinear state size (up to 40% of model parameters) for better state capacity without complex kernel implementations, and is validated across diverse modalities including 1M context length novel view synthesis.*** <br> <br>
    May 29, MIT and Adobe published a [paper](https://arxiv.org/pdf/2505.23884) “Test-Time Training Done Right”. Test-Time Training (TTT) models context dependencies by adapting part of the model's weights (referred to as fast weights) during inference. This fast weight, akin to recurrent states in RNNs, stores temporary memories of past tokens in the current sequence. Existing TTT methods struggled to show effectiveness in handling long-context data, due to their inefficiency on modern GPUs. The TTT layers in many of these approaches operate with extremely low FLOPs utilization (often <5%) because they deliberately apply small online minibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover, a small minibatch implies fine-grained block-wise causal dependencies in the data, unsuitable for data beyond 1D ordered sequences, like sets or N-dimensional grids such as images or videos. In contrast, the study pursues the opposite direction by using an extremely large chunk update, ranging from 2K to 1M tokens across tasks of varying modalities, which is referred to as Large Chunk Test-Time Training (LaCT). It improves hardware utilization by orders of magnitude, and more importantly, facilitates scaling of nonlinear state size (up to 40% of model parameters), hence substantially improving state capacity, all without requiring cumbersome and error-prone kernel implementations. It also allows easy integration of sophisticated optimizers, e.g. Muon for online updates. The study validates the approach across diverse modalities and tasks, including novel view synthesis with image set, language models, and auto-regressive video diffusion. The approach can scale up to 14B-parameter AR video diffusion model on sequences up to 56K tokens. In the longest sequence experiment, the study performs novel view synthesis with 1 million context length. The authors hope this work will inspire and accelerate new research in the field of long-context modeling and test-time training. Test-Time Training (TTT) models context dependencies by adapting part of the model's weights (referred to as fast weights) during inference. This fast weight, akin to recurrent states in RNNs, stores temporary memories of past tokens in the current sequence. Existing TTT methods struggled to show effectiveness in handling long-context data, due to their inefficiency on modern GPUs. The TTT layers in many of these approaches operate with extremely low FLOPs utilization (often <5%) because they deliberately apply small online minibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover, a small minibatch implies fine-grained block-wise causal dependencies in the data, unsuitable for data beyond 1D ordered sequences, like sets or N-dimensional grids such as images or videos. In contrast, we pursue the opposite direction by using an extremely large chunk update, ranging from 2K to 1M tokens across tasks of varying modalities, which we refer to as Large Chunk Test-Time Training (LaCT). It improves hardware utilization by orders of magnitude, and more importantly, facilitates scaling of nonlinear state size (up to 40% of model parameters), hence substantially improving state capacity, all without requiring cumbersome and error-prone kernel implementations. It also allows easy integration of sophisticated optimizers, e.g. Muon for online updates. We validate our approach across diverse modalities and tasks, including novel view synthesis with image set, language models, and auto-regressive video diffusion. Our approach can scale up to 14B-parameter AR video diffusion model on sequences up to 56K tokens. In our longest sequence experiment, we perform novel view synthesis with 1 million context length. We hope this work will inspire and accelerate new research in the field of long-context modeling and test-time training. https://tianyuanzhang.com/projects/ttt-done-right <br> <br>

45. ***MIT's FlashFormer for efficient low-batch inference:  <br>This paper describes FlashFormer, a proof-of-concept whole-model kernel designed to accelerate single-batch inference for transformer-based LLMs, addressing the memory bandwidth and kernel launch overheads significant in low-batch settings. FlashFormer demonstrates nontrivial speedups compared to existing state-of-the-art inference kernels across various model sizes and quantization settings.*** <br> <br>
    May 28, MIT published a [paper](https://arxiv.org/pdf/2505.22758) “FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference”. The size and compute characteristics of modern large language models have led to an increased interest in developing specialized kernels tailored for training and inference. Existing kernels primarily optimize for compute utilization, targeting the large-batch training and inference settings. However, low-batch inference, where memory bandwidth and kernel launch overheads contribute are significant factors, remains important for many applications of interest such as in edge deployment and latency-sensitive applications. This paper describes FlashFormer, a proof-of-concept kernel for accelerating single-batch inference for transformer-based large language models. Across various model sizes and quantizations settings, we observe nontrivial speedups compared to existing state-of-the-art inference kernels. <br> <br>

47. ***George Mason Uni and OpenAI's Linear Layouts for tensor computation:  <br>This study introduces Linear Layouts, a novel approach modeling tensor layouts using linear algebra over F2 (binary matrices acting on hardware representation bits) to achieve generic and efficient tensor computation. Integrated with Triton, Linear Layouts enable generic layout definitions and conversions, reducing compiler engineering effort and fixing bugs in Triton's legacy system while optimizing operator performance.*** <br> <br>
    May 28, George Mason Uni and OpenAI published a [paper](https://arxiv.org/pdf/2505.23819) “Linear Layouts: Robust Code Generation of Efficient Tensor Computation Using”. Efficient tensor computation is a cornerstone of modern deep learning (DL) workloads, yet existing approaches struggle to achieve flexible and performant design and implementation of tensor layouts -- mappings between logical tensors and hardware resources. The increasing complexity of DL algorithms and hardware demands a generic and systematic approach to handling tensor layouts. This study introduces Linear Layouts, a novel approach that models tensor layouts using linear algebra over F2. By representing tensor layouts as binary matrices acting on the bits of the hardware representation, the approach enables a generic layout definition -- as opposed to the classical case-by-case approach -- and allows for generic layout-to-layout conversions, eliminating the quadratic explosion that plagues existing solutions. The study integrates linear layouts with Triton and demonstrate their effectiveness in optimizing individual Triton operators as well as kernels written in Triton. The work also shows that linear layouts reduce engineering effort in the compiler backend while fixing several bugs in Triton's legacy layout system.  <br>   <br>

49. ***LBOX et al.'s LegalSearchLM for legal case retrieval:   <br> This research addresses limitations in Legal Case Retrieval (LCR) by introducing LEGAR BENCH, a large-scale Korean LCR benchmark, and LegalSearchLM, a retrieval model that performs legal element reasoning over query cases and directly generates content grounded in target cases via constrained decoding. LegalSearchLM outperforms baselines by 6-20% on LEGAR BENCH and shows strong out-of-domain generalization.***  <br>   <br> 
    May 28, LBOX, UIUC and Uni of Seoul published a [paper](https://arxiv.org/pdf/2505.23832) “LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation”. Legal Case Retrieval (LCR), which retrieves relevant cases from a query case, is a fundamental task for legal professionals in research and decision-making. However, existing studies on LCR face two major limitations. First, they are evaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and use a narrow range of criminal query types, which cannot sufficiently reflect the complexity of real-world legal retrieval scenarios. Second, their reliance on embedding-based or lexical matching methods often results in limited representations and legally irrelevant matches. To address these issues, the study presents: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering 411 diverse crime types in queries over 1.2M legal cases; and (2) LegalSearchLM, a retrieval model that performs legal element reasoning over the query case and directly generates content grounded in the target cases through constrained decoding. Experimental results show that LegalSearchLM outperforms baselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It also demonstrates strong generalization to out-of-domain cases, outperforming naive generative models trained on in-domain data by 15%.  <br>   <br> 

51. ***Stanford Uni and NYU on LLM vs. human semantic compression:   <br> This study uses an information-theoretic framework to compare how LLMs and humans trade off semantic compression for meaning, finding that while LLMs form broad conceptual categories aligned with human judgment, they struggle with fine-grained semantic distinctions. LLMs exhibit a strong bias towards aggressive statistical compression, whereas human conceptual systems prioritize adaptive nuance and contextual richness over compressional efficiency.***  <br>   <br> 
    May 26, Stanford Uni and NYU published a [paper](https://arxiv.org/pdf/2505.17117) “From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning”. Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. The study introduces a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, the study uncovers key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by human measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.

  <br>   <br>   <br> 


***Jun 1, 2025***

1. ***Anthropic CEO's AI job displacement warning:   <br>Dario Amodei, CEO of Anthropic, predicted AI could eliminate 50% of entry-level white-collar jobs within five years, potentially causing US unemployment to hit 20% by 2030, as AI surpasses human capabilities in intellectual tasks. He urged politicians and businesses to prepare for this disruption, suggesting taxing AI labs and emphasizing the need for proactive adaptation, a concern echoed by a World Economic Forum survey and an Australian report.***  <br>  <br>
   May 31, new.com.au published an [article](https://www.news.com.au/finance/work/careers/anthropic-ceo-warns-ai-could-wipe-out-1-in-2-white-collar-jobs-in-next-five-years/news-story/3196841292011f2147be15b6e186a289) “Anthropic CEO warns AI could wipe out 1 in 2 white collar jobs in next five years”. Anthropic CEO Dario Amodei has issued a stark warning, predicting that artificial intelligence could eliminate half of all entry-level, white-collar jobs within the next five years, potentially driving US unemployment as high as 20% by 2030. Amodei asserts that AI is rapidly surpassing human capabilities in intellectual tasks like summarizing, analysis, and coding, performing at the standard of a "smart college student" for nearly seven hours a day. He expresses deep concern that politicians and businesses are unprepared for this impending shift, stressing that the public remains largely unaware of the impending scale of workforce disruption. Despite it being against his company's economic interest, Amodei urged US politicians to consider taxing AI labs and emphasized the need for proactive adaptation and policy implementation, stating society "can't just sleepwalk into it." His alarm is echoed by a World Economic Forum survey finding 41% of employers intend to reduce staff due to AI by 2030, and an Australian report indicating a third of knowledge/manual roles are at risk. While some experts note AI primarily absorbs low-skill tasks, requiring workers to "level up," and companies plan to hire for AI-related skills, Amodei remains resolute that swift action is necessary to steer AI's inevitable progress towards beneficial outcomes while mitigating significant harms.  <br>  <br>

3. ***Princeton et al.'s study on learning compositional functions:   <br>Researchers explored how Transformers learn complex compositional tasks, specifically the k-fold composition task, proving a statistical query lower bound that indicates a statistical-computational gap for efficient learning. However, they demonstrated that gradient descent on an O(logk)-depth transformer can efficiently learn this function class using curriculum learning strategies (presenting easier data first or all at once), highlighting the necessity of both easy and hard examples.***  <br>  <br>
   May 30, Princeton Uni, Flatiron Inst, Columbia Uni and NYU published a [paper](https://arxiv.org/pdf/2505.23683) “Learning Compositional Functions with Transformers from Easy-to-Hard Data”. Transformer-based language models have demonstrated impressive capabilities across a range of complex reasoning tasks. Prior theoretical work exploring the expressive power of transformers has shown that they can efficiently perform multi-step reasoning tasks involving parallelizable computations. However, the learnability of such constructions, particularly the conditions on the data distribution that enable efficient learning via gradient-based optimization, remains an open question. Towards answering this question, the authors study the learnability of the k-fold composition task, which requires computing an interleaved composition of k input permutations and k hidden permutations, and can be expressed by a transformer with O(logk) layers. On the negative front, the study proves a Statistical Query (SQ) lower bound showing that any SQ learner that makes only polynomially-many queries to an SQ oracle for the k-fold composition task distribution must have sample size exponential in k, thus establishing a statistical-computational gap. On the other hand, the study shows that this function class can be efficiently learned, with runtime and sample complexity polynomial in k, by gradient descent on an O(logk)-depth transformer via two different curriculum learning strategies: one in which data consists of k′-fold composition functions with k′≤k presented in increasing difficulty, and another in which all such data is presented simultaneously. The work sheds light on the necessity and sufficiency of having both easy and hard examples in the data distribution for transformers to learn complex compositional tasks.  <br>  <br>

5. ***Google's ATLAS for optimal context memorization:   <br>This paper introduces ATLAS, a long-term memory module designed to enhance Transformer-like architectures by overcoming limitations in memory capacity, online updates, and fixed-size memory management. ATLAS learns to optimally memorize context from past and current tokens and, when integrated into DeepTransformers, surpasses standard Transformers and linear recurrent models on various tasks, significantly improving long-context performance (e.g., +80% on 10M context BABILong).***  <br>  <br>
   May 29, Google published a [paper](https://arxiv.org/pdf/2505.23735) “ATLAS: Learning to Optimally Memorize the Context at Test Time”. Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. The study observes that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, the work presents ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Building on this insight, the study presents a new family of Transformer-like architectures, called DeepTransformers, that are strict generalizations of the original Transformer architecture. Experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. ATLAS further improves the long context performance of Titans, achieving +80\% accuracy in 10M context length of BABILong benchmark.  <br>  <br>

7. ***The Darwin Gödel Machine for self-improving agents:   <br>Researchers introduced the Darwin Gödel Machine (DGM), a system that enables AI agents to autonomously and continuously improve by iteratively modifying their own code and empirically validating changes using coding benchmarks. Inspired by Darwinian evolution and open-endedness, the DGM maintains and grows an archive of diverse, high-quality agents, significantly improving performance on SWE-bench and Polyglot and outperforming baselines without self-improvement.***  <br>  <br>
   May 29, Uni of British Columbia, Vector Inst, Sakana AI, and CIFAR published a [paper](https://arxiv.org/abs/2505.22954) “Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents”. Today's AI systems have human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The advance of AI could itself be automated. If done safely, that would accelerate AI development and allow us to reap its benefits much sooner. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The Gödel machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. The study introduces the Darwin Gödel Machine (DGM), a self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM maintains an archive of generated coding agents. It grows the archive by sampling an agent from it and using a foundation model to create a new, interesting, version of the sampled agent. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self-improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). The DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation.  <br>  <br>

9. ***Business Insider's workforce reduction:   <br>According to an internal memo, Business Insider is laying off approximately 21% of its staff due to shrinking search traffic and the increasing use of generative AI tools like ChatGPT. CEO Barbara Peng stated the company must restructure to withstand traffic volatility, is accelerating AI adoption, realigning content strategy to high-engagement areas, exiting most commerce business, and launching a new events division called BI Live.***  <br>  <br>
    May 29, according to [Reuters](https://www.reuters.com/technology/business-insider-cuts-21-workforce-memo-shows-2025-05-29/), “Business Insider cuts 21% of workforce, memo shows”. Business Insider is laying off about 21% of its workforce, an internal memo showed on Thursday, as the financial news outlet grapples with shrinking search traffic and the growing use of generative AI tools such as ChatGPT. The New York-based company joins several digital media companies in restructuring operations as consumers increasingly depend on artificial intelligence for news synopsis, which is eating into web traffic. In the memo, CEO Barbara Peng told staff the company now generates twice as much revenue for each website visit as it did two years ago, but 70% of its business still has some degree of traffic sensitivity. "We must be structured to endure extreme traffic drops outside of our control, so we're reducing our overall company to a size where we can absorb that volatility," Peng said in the memo seen by Reuters. The New York-based company is accelerating adoption of AI, with a majority of employees already utilizing Enterprise ChatGPT and several AI-driven products to enhance operations and reader experience, Peng said. The website is realigning its content strategy to concentrate on areas that attract high reader engagement, and is exiting the majority of its commerce business, Peng said. It is also launching a new events business called BI Live, Peng said, adding that it has already seen some demand and will continue to build the team.  <br>  <br>

11. ***UIUC's ToMAP for opponent-aware LLM persuaders:   <br>This research introduces Theory of Mind Augmented Persuader (ToMAP), a novel approach that enhances LLM persuaders by incorporating modules to model an opponent's mental state, including considering objections and predicting stances on counterclaims. Through a designed reinforcement learning schema, ToMAP (a 3B parameter model) outperforms much larger baselines like GPT-4o by 39.4% across diverse corpora, exhibiting more complex, diverse, and effective arguments.***  <br>  <br>
    May 29, UIUC published a [paper](https://arxiv.org/pdf/2505.22961) “ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind”. Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably, while humans are skilled in modeling their opponent's thoughts and opinions proactively and dynamically, current LLMs struggle with such Theory of Mind (ToM) reasoning, resulting in limited diversity and opponent awareness. To address this limitation, the work introduces Theory of Mind Augmented Persuader (ToMAP), a novel approach for building more flexible persuader agents by incorporating two theory of mind modules that enhance the persuader's awareness and analysis of the opponent's mental state. Specifically, the study begins by prompting the persuader to consider possible objections to the target central claim, and then use a text encoder paired with a trained MLP classifier to predict the opponent's current stance on these counterclaims. The carefully designed reinforcement learning schema enables the persuader learns how to analyze opponent-related information and utilize it to generate more effective arguments. Experiments show that the ToMAP persuader, while containing only 3B parameters, outperforms much larger baselines, like GPT-4o, with a relative gain of 39.4% across multiple persuadee models and diverse corpora. Notably, ToMAP exhibits complex reasoning chains and reduced repetition during training, which leads to more diverse and effective arguments. The opponent-aware feature of ToMAP also makes it suitable for long conversations and enables it to employ more logical and opponent-aware strategies. These results underscore the method's effectiveness and highlight its potential for developing more persuasive language agents. https://github.com/ulab-uiuc/ToMAP.  <br>  <br>

13. ***CMU's unsupervised RL via entropy minimization:   <br>This study proposes RENT (Reinforcement Learning via Entropy Minimization), a fully unsupervised RL method that improves LLM reasoning without external rewards or ground-truth answers by using the model's own output entropy as an intrinsic reward. By reinforcing chains of thought that yield high model confidence, RENT demonstrated improvements on various reasoning benchmarks (GSM8K, MATH500, etc.) across Qwen and Mistral model families.***  <br>  <br>
    May 29, CMU published a [paper](https://arxiv.org/pdf/2505.22660) “Maximizing Confidence Alone Improves Reasoning”. Reinforcement learning (RL) has enabled machine learning models to achieve significant advances in many fields. Most recently, RL has empowered frontier language models to solve challenging math, science, and coding problems. However, central to any RL algorithm is the reward function, and reward engineering is a notoriously difficult problem in any domain. This study proposes RENT: Reinforcement Learning via Entropy Minimization -- a fully unsupervised RL method that requires no external reward or ground-truth answers, and instead uses the model's entropy of its underlying distribution as an intrinsic reward. The study finds that by reinforcing the chains of thought that yield high model confidence on its generated answers, the model improves its reasoning ability. In the experiments, the study showcases these improvements on an extensive suite of commonly-used reasoning benchmarks, including GSM8K, MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and Mistral families. The generality of the unsupervised learning method lends itself to applicability in a wide range of domains where external supervision is unavailable.  <br>  <br>

15. ***Rethinking training signals in RLVR:   <br>Researchers found that reinforcement learning with verifiable rewards (RLVR) can enhance mathematical reasoning in some models (like Qwen2.5-Math-7B) even with spurious rewards (random, format-based, incorrect labels), yielding gains nearly matching those from ground truth rewards. This effect, often linked to increased code reasoning behavior in Qwen models, was not consistently observed in other model families, suggesting RLVR might surface pretrained representations and that future research should validate on diverse models.***  <br>  <br>
    May 28, Uni of Washington, Allen Inst for AI and UCLA published a [paper](https://github.com/ruixin31/Rethink_RLVR/blob/main/paper/rethink-rlvr.pdf) “Spurious Rewards: Rethinking Training Signals in RLVR”. The study shows that reinforcement learning with verifiable rewards (RLVR) can elicit strong mathematical reasoning in certain models even with spurious rewards that have little, no, or even negative correlation with the correct answer. For example, RLVR improves MATH-500 performance for Qwen2.5-Math-7B in absolute points by 21.4% (random reward), 16.4% (format reward), 24.6% (incorrect label), 24.4% (1-shot RL), and 26.5% (majority voting)—nearly matching the 28.8% gained with ground truth rewards. However, the spurious rewards that work for Qwen often fail to yield gains with other model families like Llama3 or OLMo2. In particular, the study finds code reasoning—thinking in code without actual code execution—to be a distinctive Qwen2.5-Math behavior that becomes significantly more frequent after RLVR, from 66.7% to over 90%, even with spurious rewards. Overall, the authors hypothesize that, given the lack of useful reward signal, RLVR must somehow be surfacing useful reasoning representations learned during pretraining, although the exact mechanism remains a topic for future work. The study suggests that future RLVR research should possibly be validated on diverse models rather than a single de facto choice, as the work shows that it is easy to get significant performance gains on Qwen models even with completely spurious reward signals. https://github.com/ruixin31/Rethink_RLVR/tree/main  <br>  <br>

17. ***SAIL et al. on RL entropy mechanism for reasoning LLMs:   <br>This paper addresses the issue of policy entropy collapse in RL for LLM reasoning, which bottlenecks performance, establishing an empirical law (R=-a*e^H+b) linking performance (R) to entropy (H). Understanding that entropy decrease is driven by the covariance between action probability and logit changes, they propose methods (Clip-Cov, KL-Cov) to control entropy by restricting updates on high-covariance tokens, thereby encouraging exploration and improving performance.***  <br>  <br>
    May 28, SAIL, Tsinghua Uni UIUC et al published a [paper](https://arxiv.org/pdf/2505.22617) “The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models”. This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished exploratory ability is always accompanied with the saturation of policy performance. In practice, the study establishes a transformation equation R=-a*e^H+b between entropy H and downstream performance R. This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion, and the ceiling is fully predictable (H=0, R=-a+b). The finding necessitates entropy management for continuous exploration toward scaling compute for RL. To this end, the study investigates entropy dynamics both theoretically and empirically. The derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits, which is proportional to its advantage when using Policy Gradient-like algorithms. Empirical study shows that, the values of covariance term and entropy differences matched exactly, supporting the theoretical conclusion. Moreover, the covariance term stays mostly positive throughout training, further explaining why policy entropy would decrease monotonically. Through understanding the mechanism behind entropy dynamics, the authors motivate to control entropy by restricting the update of high-covariance tokens. Specifically, the work proposes two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively. Experiments show that these methods encourage exploration, thus helping policy escape entropy collapse and achieve better downstream performance.  <br>  <br>

19. ***CUM's investigation into LLM self-training:   <br>This study explored whether large reasoning models can self-train, proposing an online self-training reinforcement learning algorithm that uses a model's self-consistency to infer correctness signals without ground-truth supervision. Applied to mathematical reasoning, the algorithm quickly reached performance rivaling supervised RL methods but also highlighted limitations like potential reward hacking where confidently incorrect outputs are favored.***  <br>  <br>
    May 27, CUM published a [paper](https://arxiv.org/pdf/2505.21444) “Can Large Reasoning Models Self-Train?”. Scaling the performance of large language models (LLMs) increasingly depends on methods that reduce reliance on human supervision. Reinforcement learning from automated verification offers an alternative, but it incurs scalability limitations due to dependency upon human-designed verifiers. Self-training, where the model's own judgment provides the supervisory signal, presents a compelling direction. The study proposes an online self-training reinforcement learning algorithm that leverages the model's self-consistency to infer correctness signals and train without any ground-truth supervision. The study applies the algorithm to challenging mathematical reasoning tasks and shows that it quickly reaches performance levels rivaling reinforcement-learning methods trained explicitly on gold-standard answers. Additionally, the study analyzes inherent limitations of the algorithm, highlighting how the self-generated proxy reward initially correlated with correctness can incentivize reward hacking, where confidently incorrect outputs are favored. The results illustrate how self-supervised improvement can achieve significant performance gains without external labels, while also revealing its fundamental challenges.  <br>  <br>

21. ***UC Berkeley and Yale's RLIF for reasoning without external rewards:   <br>This research explores Reinforcement Learning from Internal Feedback (RLIF), proposing Intuitor, an RLIF method that uses an LLM's self-certainty as its sole reward signal, enabling fully unsupervised learning. Intuitor, by replacing external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, matched GRPO's performance on math benchmarks and achieved superior generalization to out-of-domain tasks without needing gold solutions.***  <br>  <br>
    May 26, UC Berkeley and Yale Uni published a [paper](https://arxiv.org/pdf/2505.19590) “Learning to Reason without External Rewards”. Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision. The study explores Reinforcement Learning from Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic signals without external rewards or labeled data. The work proposes Intuitor, an RLIF method that uses a model's own confidence, termed self-certainty, as its sole reward signal. Intuitor replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, enabling fully unsupervised learning. Experiments demonstrate that Intuitor matches GRPO's performance on mathematical benchmarks while achieving superior generalization to out-of-domain tasks like code generation, without requiring gold solutions or test cases. The findings show that intrinsic model signals can drive effective learning across domains, offering a scalable alternative to RLVR for autonomous AI systems where verifiable rewards are unavailable. https://github.com/sunblaze-ucb/Intuitor  <br>  <br>

23. ***UIUC's Time-R1 for comprehensive temporal reasoning:   <br>This work introduces Time-R1, a framework to endow a 3B-parameter LLM with comprehensive temporal abilities (understanding, prediction, creative generation) through a novel three-stage development path featuring an RL curriculum with a dynamic rule-based reward system. Time-R1 significantly outperforms models over 200 times larger on challenging future event prediction and creative scenario generation benchmarks, offering a scalable path to time-aware AI.***  <br>  <br>
    May 26, UIUC published a [paper](https://huggingface.co/papers/2505.13508) “Time-R1: Towards Comprehensive Temporal Reasoning in LLMs”. Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, the work introduces Time-R1, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. The approach features a novel three-stage development path; the first two constitute a reinforcement learning (RL) curriculum driven by a meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event prediction skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards truly time-aware AI. To foster further research, the work also releases Time-Bench, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of Time-R1 checkpoints. https://github.com/ulab-uiuc/Time-R1  <br>  <br>

25. ***CMU's FLAME-MoE research platform:   <br>This paper releases FLAME-MoE, an open-source research suite for Mixture-of-Experts (MoE) language models, comprising seven decoder-only models (38M to 1.7B active parameters) with architectures mirroring modern production LLMs (64 experts, top-8 gating, 2 shared experts). With all training data, scripts, logs, and checkpoints publicly available, FLAME-MoE enables reproducible experimentation and initial analyses show expert specialization and stable routing behavior.***  <br>  <br>
    May 26, CMU published a [paper](https://arxiv.org/pdf/2505.20225) “FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models”. Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4 increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong efficiency-performance trade-offs by activating only a fraction of the model per token. Yet academic researchers still lack a fully open, end-to-end MoE platform for investigating scaling, routing, and expert behavior. The study releases FLAME-MoE, a completely open-source research suite composed of seven decoder-only models, ranging from 38M to 1.7B active parameters, whose architecture--64 experts with top-8 gating and 2 shared experts--closely reflects modern production LLMs. All training data pipelines, scripts, logs, and checkpoints are publicly available to enable reproducible experimentation. Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4 points over dense baselines trained with identical FLOPs. Leveraging full training trace transparency, the study presents initial analyses showing that (i) experts increasingly specialize on distinct token subsets, (ii) co-activation matrices remain sparse, reflecting diverse expert usage, and (iii) routing behavior stabilizes early in training. https://github.com/cmu-flame/FLAME-MoE.  <br>  <br>

27. ***MIT and Red Hat on emergent features in LLMs:   <br>This research studies the emergence of interpretable categorical features within LLMs across training time, transformer layers (space), and model sizes, using sparse autoencoders. Findings indicate clear thresholds for feature emergence and reveal unexpected semantic reactivation, where early-layer features re-emerge later, challenging standard assumptions about representational dynamics.***  <br>  <br>
    May 26, MIT and Red Hat published a [paper](https://arxiv.org/pdf/2505.19440) “The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models”. This paper studies the emergence of interpretable categorical features within large language models (LLMs), analyzing their behavior across training checkpoints (time), transformer layers (space), and varying model sizes (scale). Using sparse autoencoders for mechanistic interpretability, the authors identify when and where specific semantic concepts emerge within neural activations. Results indicate clear temporal and scale-specific thresholds for feature emergence across multiple domains. Notably, spatial analysis reveals unexpected semantic reactivation, with early-layer features re-emerging at later layers, challenging standard assumptions about representational dynamics in transformer models.  <br>  <br>

29. ***Mila et al.'s REARANK reasoning re-ranking agent:   <br>This study presents REARANK, an LLM-based listwise reasoning re-ranking agent that explicitly reasons before re-ranking, improving performance and interpretability. Using reinforcement learning and data augmentation with only 179 annotated samples, REARANK-7B (built on Qwen2.5-7B) demonstrates performance comparable to or even surpassing GPT-4 on various information retrieval benchmarks, especially reasoning-intensive ones.***  <br>  <br>
    May 26, Mila, Uni de Montrael et al. published a [paper](https://arxiv.org/pdf/2505.20046) “REARANK: Reasoning Re-ranking Agent via Reinforcement Learning”. The study presents REARANK, a large language model (LLM)-based listwise reasoning reranking agent. REARANK explicitly reasons before reranking, significantly improving both performance and interpretability. Leveraging reinforcement learning and data augmentation, REARANK achieves substantial improvements over baseline models across popular information retrieval benchmarks, notably requiring only 179 annotated samples. Built on top of Qwen2.5-7B, the REARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and out-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT benchmarks. These results underscore the effectiveness of the approach and highlight how reinforcement learning can enhance LLM reasoning capabilities in reranking. https://github.com/lezhang7/Rearank  <br>  <br>

31. ***Princeton et al.'s Alita generalist agent:   <br>This work introduces Alita, a generalist agent designed for scalable agentic reasoning with minimal predefinition (one direct problem-solving component) and maximal self-evolution (autonomously constructing and refining external capabilities via model context protocols). Alita achieves top-ranking performance on benchmarks like GAIA, Mathvista, and PathVQA, outperforming more complex systems.***  <br>  <br>
    May 26, Princeton Uni et al published a [paper](https://arxiv.org/pdf/2505.20286) “Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution”. Recent advances in large language models (LLMs) have enabled agents to autonomously perform complex, open-ended tasks. However, many existing frameworks depend heavily on manually predefined tools and workflows, which hinder their adaptability, scalability, and generalization across domains. This work introduces Alita--a generalist agent designed with the principle of "Simplicity is the ultimate sophistication," enabling scalable agentic reasoning through minimal predefinition and maximal self-evolution. For minimal predefinition, Alita is equipped with only one component for direct problem-solving, making it much simpler and neater than previous approaches that relied heavily on hand-crafted, elaborate tools and workflows. This clean design enhances its potential to generalize to challenging questions, without being limited by tools. For Maximal self-evolution, the study enables the creativity of Alita by providing a suite of general-purpose components to autonomously construct, refine, and reuse external capabilities by generating task-related model context protocols (MCPs) from open source, which contributes to scalable agentic reasoning. Notably, Alita achieves 75.15% pass@1 and 87.27% pass@3 accuracy, which is top-ranking among general-purpose agents, on the GAIA benchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on Mathvista and PathVQA, outperforming many agent systems with far greater complexity. https://github.com/CharlesQ9/Alita  <br>  <br>

33. ***Apple and Duke's interleaved reasoning via RL:   <br>This study proposes a novel training paradigm using reinforcement learning to guide LLMs to interleave thinking and answering for multi-hop questions, addressing inefficiencies of long chain-of-thought. Using a simple rule-based reward for correct intermediate steps, this approach significantly reduces time-to-first-token (over 80%) and improves accuracy (up to 19.3% Pass@1) without external tools, also showing strong generalization.***  <br>  <br>
    May 26, Apple and Duke Uni published a [paper](https://arxiv.org/pdf/2505.19640) “Interleaved Reasoning for Large Language Models via Reinforcement Learning”. Long chain-of-thought (CoT) significantly enhances large language models' (LLM) reasoning capabilities. However, the extensive reasoning traces lead to inefficiencies and an increased time-to-first-token (TTFT). The study proposes a novel training paradigm that uses reinforcement learning (RL) to guide reasoning LLMs to interleave thinking and answering for multi-hop questions. The work observes that models inherently possess the ability to perform interleaved reasoning, which can be further enhanced through RL. The study introduces a simple yet effective rule-based reward to incentivize correct intermediate steps, which guides the policy model toward correct reasoning paths by leveraging intermediate signals generated during interleaved reasoning. Extensive experiments conducted across five diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++) demonstrate consistent improvements over traditional think-answer reasoning, without requiring external tools. Specifically, the approach reduces TTFT by over 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore, the method, trained solely on question answering and logical reasoning datasets, exhibits strong generalization ability to complex reasoning datasets such as MATH, GPQA, and MMLU. Additionally, the study conducts in-depth analysis to reveal several valuable insights into conditional reward modeling.  <br>  <br>

35. ***National Uni of Singapore on RLLMs as wandering explorers:   <br>This paper argues that current reasoning LLMs (RLLMs), despite successes with test-time computation, lack systematic solution space exploration, identifying failure modes like invalid steps, redundant explorations, and unfaithful conclusions. The authors contend RLLMs are "wanderers" whose performance degrades with complexity and advocate for new metrics evaluating the reasoning process itself, not just final outputs.***  <br>  <br>
    May 26, National Uni of Singapore published a [paper](https://arxiv.org/pdf/2505.20296) “Reasoning LLMs are Wandering Solution Explorers”. Large Language Models (LLMs) have demonstrated impressive reasoning abilities through test-time computation (TTC) techniques such as chain-of-thought prompting and tree-based reasoning. However, the paper argues that current reasoning LLMs (RLLMs) lack the ability to systematically explore the solution space. This paper formalizes what constitutes systematic problem solving and identifies common failure modes that reveal reasoning LLMs to be wanderers rather than systematic explorers. Through qualitative and quantitative analysis across multiple state-of-the-art LLMs, the study uncovers persistent issues: invalid reasoning steps, redundant explorations, hallucinated or unfaithful conclusions, and so on. The findings suggest that current models' performance can appear to be competent on simple tasks yet degrade sharply as complexity increases. Based on the findings, the authors advocate for new metrics and tools that evaluate not just final outputs but the structure of the reasoning process itself.  <br>  <br>

37. ***CMU et al.'s DeepResearchGym evaluation sandbox:   <br>This research introduces DeepResearchGym, an open-source sandbox for benchmarking deep research systems (agentic IR methods), featuring a reproducible search API indexing ClueWeb22 and FineWeb, and a rigorous evaluation protocol. The API offers stable rankings and lower latency than commercial alternatives, while the protocol uses LLM-as-a-judge assessments, with results showing comparable performance to systems using commercial APIs and alignment with human preferences.***  <br>  <br>
    May 25, CMU et al published a [paper](https://arxiv.org/pdf/2505.19253) “DeepResearchGym: A Free, Transparent, and Reproducible Evaluation Sandbox for Deep Research”. Deep research systems represent an emerging class of agentic information retrieval methods that generate comprehensive and well-supported reports to complex queries. However, most existing frameworks rely on dynamic commercial search APIs, which pose reproducibility and transparency challenges in addition to their cost. To address these limitations, the  study introduces DeepResearchGym, an open-source sandbox that combines a reproducible search API with a rigorous evaluation protocol for benchmarking deep research systems. The API indexes large-scale public web corpora, namely ClueWeb22 and FineWeb, using a state-of-the-art dense retriever and approximate nearest neighbor search via DiskANN. It achieves lower latency than popular commercial APIs while ensuring stable document rankings across runs, and is freely available for research use. To evaluate deep research systems' outputs, the study extends the Researchy Questions benchmark with automatic metrics through LLM-as-a-judge assessments to measure alignment with users' information needs, retrieval faithfulness, and report quality. Experimental results show that systems integrated with DeepResearchGym achieve performance comparable to those using commercial APIs, with performance rankings remaining consistent across evaluation metrics. A human evaluation study further confirms that the automatic protocol aligns with human preferences, validating the framework's ability to help support controlled assessment of deep research systems. https://www.deepresearchgym.ai.  <br>  <br>

39. ***UIUC et al.'s hybrid latent reasoning via RL:   <br>This work introduces hybrid reasoning policy optimization (HRPO), an RL-based approach that integrates prior hidden states with sampled tokens via a learnable gating mechanism for latent reasoning in LLMs. HRPO, by progressively incorporating hidden features and enabling RL optimization without CoT trajectories, outperforms prior methods on knowledge- and reasoning-intensive tasks while maintaining interpretability.***  <br>  <br>
    May 24, UIUC, Google and LUM published a [paper](https://www.arxiv.org/pdf/2505.18454) “Hybrid Latent Reasoning via Reinforcement Learning”. Recent advances in large language models (LLMs) have introduced latent reasoning as a promising alternative to autoregressive reasoning. By performing internal computation with hidden states from previous steps, latent reasoning benefit from more informative features rather than sampling a discrete chain-of-thought (CoT) path. Yet latent reasoning approaches are often incompatible with LLMs, as their continuous paradigm conflicts with the discrete nature of autoregressive generation. Moreover, these methods rely on CoT traces for training and thus fail to exploit the inherent reasoning patterns of LLMs. This work explores latent reasoning by leveraging the intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, the study introduces hybrid reasoning policy optimization (HRPO), an RL-based hybrid latent reasoning approach that (1) integrates prior hidden states into sampled tokens with a learnable gating mechanism, and (2) initializes training with predominantly token embeddings while progressively incorporating more hidden features. This design maintains LLMs' generative capabilities and incentivizes hybrid reasoning using both discrete and continuous representations. In addition, the hybrid HRPO introduces stochasticity into latent reasoning via token sampling, thereby enabling RL-based optimization without requiring CoT trajectories. Extensive evaluations across diverse benchmarks show that HRPO outperforms prior methods in both knowledge- and reasoning-intensive tasks. Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing behaviors like cross-lingual patterns and shorter completion lengths, highlighting the potential of our RL-based approach and offer insights for future work in latent reasoning. https://github.com/Yueeeeeeee/HRPO  <br>  <br>

41. ***Technion's TabSTAR foundation tabular model:   <br>This paper introduces TabSTAR, a foundation tabular model designed for transfer learning on tabular data with textual features, featuring semantically target-aware representations and an architecture free of dataset-specific parameters. By unfreezing a pretrained text encoder and using target tokens for context, TabSTAR achieves state-of-the-art performance on classification tasks with text features and exhibits scaling laws, offering a path for further improvement.***  <br>  <br>
    May 23, Technion published a [paper](https://arxiv.org/pdf/2505.18125) “TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations”. While deep learning has achieved remarkable success across many domains, it has historically underperformed on tabular learning tasks, which remain dominated by gradient boosting decision trees (GBDTs). However, recent advancements are paving the way for Tabular Foundation Models, which can leverage real-world knowledge and generalize across diverse datasets, particularly when the data contains free-text. Although incorporating language model capabilities into tabular tasks has been explored, most existing methods utilize static, target-agnostic textual representations, limiting their effectiveness. The study introduces TabSTAR: a Foundation Tabular Model with Semantically Target-Aware Representations. TabSTAR is designed to enable transfer learning on tabular data with textual features, with an architecture free of dataset-specific parameters. It unfreezes a pretrained text encoder and takes as input target tokens, which provide the model with the context needed to learn task-specific embeddings. TabSTAR achieves state-of-the-art performance for both medium- and large-sized datasets across known benchmarks of classification tasks with text features, and its pretraining phase exhibits scaling laws in the number of datasets, offering a pathway for further performance improvements. https://github.com/alanarazi7/TabSTAR  <br>  <br>

43. ***KAIST et al.'s agent distillation into small models:   <br>This study proposes Agent Distillation, a framework to transfer full task-solving behavior, including retrieval and code tool use, from LLM-based agents to smaller language models (sLMs). Using a "first-thought prefix" prompting method and self-consistent action generation, sLMs as small as 0.5B parameters achieve performance competitive with next-tier larger models fine-tuned with chain-of-thought distillation on factual and mathematical reasoning tasks.***  <br>  <br>
    May 23, KAIST, KRAFTON and DeepAuto.ai published a [paper](https://www.arxiv.org/pdf/2505.17612) “Distilling LLM Agent into Small Models with Retrieval and Code Tools”. Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. This study proposes Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. The study improves agent distillation along two complementary axes: (1) introduces a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) proposes a self-consistent action generation for improving test-time robustness of small agents. The study evaluates the method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. The results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. https://github.com/Nardien/agent-distillation  <br>  <br>

45. ***Meta and Hebrew Uni on preferring shorter thinking chains:   <br>This research challenges the assumption that longer thinking chains improve LLM reasoning, demonstrating that shorter chains for individual questions are significantly more likely (up to 34.5%) to yield correct answers. They propose "short-m@k," an inference method where computation halts after the first m of k parallel generations, showing it matches or surpasses majority voting with lower compute and faster wall times, and that training on shorter chains improves performance.***  <br>  <br>
    May 23, Meta and Hebrew Uni published a [paper](https://arxiv.org/pdf/2505.17813) “Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning”. Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. The study challenges the assumption that long thinking chains results in better reasoning capabilities. The study first demonstrates that shorter reasoning chains within individual questions are significantly more likely to yield correct answers - up to 34.5% more accurate than the longest chain sampled for the same question. Based on these results, the study suggests short-m@k, a novel reasoning LLM inference method. The method executes k independent generations in parallel and halts computation once the first m thinking processes are done. The final answer is chosen using majority voting among these m chains. Basic short-1@k demonstrates similar or even superior performance over standard majority voting in low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while slightly less efficient than short-1@k, consistently surpasses majority voting across all compute budgets, while still being substantially faster (up to 33% wall time reduction). Inspired by the results, the work finetunes an LLM using short, long, and randomly selected reasoning chains, and observes that training on the shorter ones leads to better performance. The findings suggest rethinking current methods of test-time compute in reasoning LLMs, emphasizing that longer "thinking" does not necessarily translate to improved performance and can, counter-intuitively, lead to degraded results.  <br>  <br>

47. ***Cornell et al.'s value-guided search for CoT reasoning:   <br>This study proposes an efficient method for training token-level value models on long-context reasoning traces without needing a fine-grained "step" definition, unlike process reward models. Using a 1.5B value model trained on 2.5 million traces, they applied block-wise value-guided search (VGS) with a final weighted majority vote to DeepSeek models, achieving better test-time scaling and matching o3-mini-medium performance with significantly reduced inference FLOPs.***  <br>  <br>
    May 23, Cornell Uni, Harvard Uni, Netflix and Databricks published a [paper](https://arxiv.org/pdf/2505.17373) “Value-Guided Search for Efficient Chain-of-Thought Reasoning”. The study proposes a simple and efficient method for value model training on long-context reasoning traces. Compared to existing process reward models (PRMs), the method does not require a fine-grained notion of "step," which is difficult to define for long-context reasoning models. By collecting a dataset of 2.5 million reasoning traces, the study trains a 1.5B token-level value model and apply it to DeepSeek models for improved performance with test-time compute scaling. The work finds that block-wise value-guided search (VGS) with a final weighted majority vote achieves better test-time scaling than standard methods such as majority voting or best-of-n. With an inference budget of 64 generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of 45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024 & 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly reduces the inference FLOPs required to achieve the same performance of majority voting. https://github.com/kaiwenw/value-guided-search  <br>  <br>

49. ***National Uni of Singapore's VeriThinker for efficient reasoning:   <br>This paper introduces VeriThinker, a novel approach for chain-of-thought (CoT) compression in Large Reasoning Models (LRMs) that mitigates overthinking by fine-tuning the LRM solely on an auxiliary verification task. By learning to accurately verify CoT solutions, LRMs become more discerning about subsequent self-reflection, substantially reducing reasoning chain lengths while maintaining or improving accuracy on benchmarks like MATH500 and AIME25.***  <br>  <br>
    May 23, National Uni of Singapore published a [paper](https://arxiv.org/pdf/2505.17941) “VeriThinker: Learning to Verify Makes Reasoning Model Efficient”. Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, the study introduces VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, the study innovatively fine-tunes the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, the approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, the experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. https://github.com/czg1225/VeriThinker  <br>  <br>

51. ***MIT et al.'s PaTH attention for position encoding:   <br>This paper describes PaTH, a flexible, data-dependent position encoding scheme based on accumulated products of Householder-like transformations, where each transformation is a function of the input, aiming to improve upon RoPE's input-independent nature. With an efficient parallel training algorithm, PaTH demonstrated superior performance over RoPE and other baselines on synthetic and real-world language modeling experiments.***  <br>  <br>
    May 22, MIT, Stanford Uni and Microsoft published a [paper](https://arxiv.org/pdf/2505.16381) “PaTH Attention: Position Encoding via Accumulating Householder Transformations”. The attention mechanism is a core primitive in modern large language models (LLMs) and AI more broadly. Since attention by itself is permutation-invariant, position encoding is essential for modeling structured domains such as language. Rotary position encoding (RoPE) has emerged as the de facto standard approach for position encoding and is part of many modern LLMs. However, in RoPE the key/query transformation between two elements in a sequence is only a function of their relative position and otherwise independent of the actual input. This limits the expressivity of RoPE-based transformers. This paper describes PaTH, a flexible data-dependent position encoding scheme based on accumulated products of Householder(like) transformations, where each transformation is data-dependent, i.e., a function of the input. The study derives an efficient parallel algorithm for training through exploiting a compact representation of products of Householder matrices, and implement a FlashAttention-style blockwise algorithm that minimizes I/O cost. Across both targeted synthetic benchmarks and moderate-scale real-world language modeling experiments, we find that PaTH demonstrates superior performance compared to RoPE and other recent baselines.  <br>  <br>

53. ***Sapienza Uni and Tech Inno Inst on RAG positional bias:   <br>This research investigates how positional bias affects Retrieval Augmented Generation (RAG) systems, finding that state-of-the-art retrieval pipelines often place highly distracting passages in top ranks (over 60% of queries have one in top-10). Consequently, the impact of LLM positional bias is marginal in real scenarios as both relevant and distracting passages are penalized, making sophisticated passage rearrangement strategies no better than random shuffling.***  <br>  <br>
    May 21, Sapienza Uni of Rome and Tech Inno Inst published a [paper](https://arxiv.org/pdf/2505.15561) “Do RAG Systems Suffer From Positional Bias?”. Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capability to capitalize on relevant passages, but also its susceptibility to distracting passages. Through extensive experiments on three benchmarks, the study shows how state-of-the-art retrieval pipelines, while attempting to retrieve relevant passages, systematically bring highly distracting ones to the top ranks, with over 60% of queries containing at least one highly distracting passage among the top-10 retrieved passages. As a result, the impact of the LLM positional bias, which in controlled settings is often reported as very prominent by related works, is actually marginal in real scenarios since both relevant and distracting passages are, in turn, penalized. Indeed, the findings reveal that sophisticated strategies that attempt to rearrange the passages based on LLM positional preferences do not perform better than random shuffling.  <br>  <br>

55. ***USC on textual steering for MLLM visual understanding:   <br>This study investigates steering multimodal large language models (MLLMs) using vectors derived from their text-only LLM backbones via methods like sparse autoencoders and mean shift. They found text-derived steering consistently enhances multimodal accuracy across diverse MLLM architectures and visual tasks (e.g., mean shift boosted spatial relationship accuracy by +7.3%), offering an efficient mechanism for improving grounding with minimal overhead.***  <br>  <br>
    May 20, Uni of Southern California published a [paper](https://arxiv.org/pdf/2505.14071) “Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models”. Steering methods have emerged as effective and targeted tools for guiding large language models' (LLMs) behavior without modifying their parameters. Multimodal large language models (MLLMs), however, do not currently enjoy the same suite of techniques, due in part to their recency and architectural diversity. Inspired by this gap, the study investigates whether MLLMs can be steered using vectors derived from their text-only LLM backbone, via sparse autoencoders (SAEs), mean shift, and linear probing. The study finds that text-derived steering consistently enhances multimodal accuracy across diverse MLLM architectures and visual tasks. In particular, mean shift boosts spatial relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to +3.3%, outperforming prompting and exhibiting strong generalization to out-of-distribution datasets. These results highlight textual steering vectors as a powerful, efficient mechanism for enhancing grounding in MLLMs with minimal additional data collection and computational overhead.  <br>  <br>

57. ***UIUC and Amazon's s3 for efficient search agent training via RL:   <br>This study proposes s3, a lightweight, model-agnostic framework that decouples the searcher from the generator in RAG systems and trains the searcher using a "Gain Beyond RAG" reward (improvement in generation accuracy over naive RAG). s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data across various QA benchmarks.***  <br>  <br>
    May 20, UIUC and Amazon published a [paper](https://arxiv.org/pdf/2505.14146) “s3: You Don't Need That Much Data to Train a Search Agent via RL”. Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve-entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. This study proposes s3, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks. https://github.com/pat-jj/s3

  <br>  <br>  <br>

***May 25, 2025***

1. ***Meta's RLUF for LLM alignment:  <br>This research introduces Reinforcement Learning from User Feedback (RLUF), a framework to align LLMs with real user preferences by training a reward model (P[Love]) on implicit signals like emoji reactions. Despite challenges like sparse and adversarial user feedback, RLUF, when integrated into a multi-objective policy, significantly increased positive feedback (e.g., 28% more Love Reactions in A/B tests) but also highlighted reward hacking challenges requiring careful objective balancing.*** <br> <br>
   May 23, Meta published a [paper](https://www.arxiv.org/pdf/2505.14946) “Reinforcement Learning from User Feedback”. As large language models (LLMs) are increasingly deployed in diverse user facing applications, aligning them with real user preferences becomes essential. Existing methods like Reinforcement Learning from Human Feedback (RLHF) rely on expert annotators trained on manually defined guidelines, whose judgments may not reflect the priorities of everyday users. This research introduces Reinforcement Learning from User Feedback (RLUF), a framework for aligning LLMs directly to implicit signals from users in production. RLUF addresses key challenges of user feedback: user feedback is often binary (e.g., emoji reactions), sparse, and occasionally adversarial. The study trains a reward model, P[Love], to predict the likelihood that an LLM response will receive a Love Reaction, a lightweight form of positive user feedback, and integrate P[Love] into a multi-objective policy optimization framework alongside helpfulness and safety objectives. In large-scale experiments, the study shows that P[Love] is predictive of increased positive feedback and serves as a reliable offline evaluator of future user behavior. Policy optimization using P[Love] significantly raises observed positive-feedback rates, including a 28% increase in Love Reactions during live A/B tests. However, optimizing for positive reactions introduces reward hacking challenges, requiring careful balancing of objectives. By directly leveraging implicit signals from users, RLUF offers a path to aligning LLMs with real-world user preferences at scale. <br> <br>

3. ***Anthropic's new Claude models system card:  <br>This report details Claude Opus 4 and Claude Sonnet 4, two new hybrid reasoning LLMs, outlining extensive pre-deployment safety tests, usage policy violation checks, specific risk evaluations (like reward hacking), agentic safety assessments, and, for the first time, detailed alignment and model welfare assessments. Based on this testing, Claude Opus 4 is deployed under AI Safety Level 3 and Sonnet 4 under Level 2.*** <br> <br>
   May 23, Anthropic published a [report](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf) “System Card Claude Opus 4 & Claude Sonnet 4”. This system card introduces Claude Opus 4 and Claude Sonnet 4, two new hybrid reasoning large language models from Anthropic. The system card describes: a wide range of pre-deployment safety tests conducted in line with the commitments in our Responsible Scaling Policy; tests of the model’s behavior around violations of our Usage Policy; evaluations of specific risks such as “reward hacking” behavior; and agentic safety evaluations for computer use and coding capabilities. In addition, and for the first time, the report includes a detailed alignment assessment covering a wide range of misalignment risks identified in our research, and a model welfare assessment. Informed by the testing described here, we have decided to deploy Claude Opus 4 under the AI Safety Level 3 Standard and Claude Sonnet 4 under the AI Safety Level 2 Standard. <br> <br>

5. ***Google's conceptual understanding of prompt tuning:  <br>This study discusses prompt optimization through a Bayesian lens, explaining how meta-trained neural networks act as Bayesian predictors adapting in-context, and how optimal prompting can be formally studied as conditioning these predictors. Supported by experiments, the paper also highlights the effectiveness of soft prefixes in manipulating activations beyond what hard tokens can achieve, adding a mechanistic aspect to the conceptual theory.*** <br> <br>
   May 22, Google published a [paper](https://arxiv.org/pdf/2505.17010) “Understanding Prompt Tuning and In-Context Learning via Meta-Learning”. Prompting is one of the main ways to adapt a pretrained model to target tasks. Besides manually constructing prompts, many prompt optimization methods have been proposed in the literature. Method development is mainly empirically driven, with less emphasis on a conceptual understanding of prompting. This study discusses how optimal prompting can be understood through a Bayesian view, which also implies some fundamental limitations of prompting that can only be overcome by tuning weights. The paper explains in detail how meta-trained neural networks behave as Bayesian predictors over the pretraining distribution, whose hallmark feature is rapid in-context adaptation. Optimal prompting can be studied formally as conditioning these Bayesian predictors, yielding criteria for target tasks where optimal prompting is and is not possible. The authors support the theory with educational experiments on LSTMs and Transformers, where the study compares different versions of prefix-tuning and different weight-tuning methods. The study also confirms that soft prefixes, which are sequences of real-valued vectors outside the token alphabet, can lead to very effective prompts for trained and even untrained networks by manipulating activations in ways that are not achievable by hard tokens. This adds an important mechanistic aspect beyond the conceptual Bayesian theory. <br> <br>

7. ***MIT's study on data influence across model scales:  <br>Researchers investigated how training data distribution affects model behavior across different compute scales, finding that small- and large-scale language model predictions generally correlate highly despite variations in training data. This consistent influence allows for more reliable extrapolation from smaller, less expensive proxy models in applications like data attribution and dataset selection.*** <br> <br>
   May 22, MIT published a [paper](https://www.arxiv.org/pdf/2505.16260) “Small-to-Large Generalization: Data Influences Models Consistently Across Scale”. Choice of training data distribution greatly influences model behavior. Yet, in large-scale settings, precisely characterizing how changes in training data affects predictions is often difficult due to model training costs. Current practice is to instead extrapolate from scaled down, inexpensive-to-train proxy models. However, changes in data do not influence smaller and larger models identically. Therefore, understanding how choice of data affects large-scale models raises the question: how does training data distribution influence model behavior across compute scale? The study finds that small- and large-scale language model predictions (generally) do highly correlate across choice of training data. Equipped with these findings, the study characterizes how proxy scale affects effectiveness in two downstream proxy model applications: data attribution and dataset selection. <br> <br>

9. ***Advancing LLM reasoning with General-Reasoner:  <br>Researchers from the University of Waterloo et al. proposed General-Reasoner, a novel training paradigm to enhance LLM reasoning across diverse domains beyond just math and code. This involves constructing a large-scale, high-quality dataset with verifiable answers from web crawling and developing a generative model-based answer verifier with chain-of-thought capabilities, demonstrating superior performance on 12 benchmarks.*** <br> <br>
    May 22, Uni of Waterloo, Vector Inst. et al published a [paper](https://arxiv.org/pdf/2505.14652) “General-Reasoner: Advancing LLM Reasoning Across All Domains”. Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. This study proposes General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. The key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. The study trains a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. The comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks. <br> <br>

11. ***Mistral AI's Devstral for coding agents:  <br>Mistral AI, in collaboration with All Hands AI, released Devstral, an Apache 2.0 licensed open-source model designed to tackle real-world software engineering problems by contextualizing code within large codebases and identifying bugs. Devstral, trained on real GitHub issues and run over code agent scaffolds, significantly outperforms prior open-source models on SWE-Bench Verified (46.8%) and is light enough for local deployment.*** <br> <br>
    May 21, Mistral AI [released Devstral](https://mistral.ai/news/devstral), the best open-source model for coding agents. Devstral is built under a collaboration between Mistral AI and All Hands AI 🙌, and outperforms all open-source models on SWE-Bench Verified by a large margin. We release Devstral under the Apache 2.0 license. While typical LLMs are excellent at atomic coding tasks such as writing standalone functions or code completion, they currently struggle to solve real-world software engineering problems. Real-world development requires contextualising code within a large codebase, identifying relationships between disparate components, and identifying subtle bugs in intricate functions. Devstral is designed to tackle this problem. Devstral is trained to solve real GitHub issues; it runs over code agent scaffolds such as OpenHands or SWE-Agent, which define the interface between the model and the test cases. Devstral achieves a score of 46.8% on SWE-Bench Verified, outperforming prior open-source SoTA models by more than 6% points. When evaluated under the same test scaffold (OpenHands, provided by All Hands AI 🙌), Devstral exceeds far larger models such as Deepseek-V3-0324 (671B) and Qwen3 232B-A22B. Devstral is light enough to run on a single RTX 4090 or a Mac with 32GB RAM, making it an ideal choice for local deployment and on-device use. Coding platforms such as OpenHands can allow the model to interact with local codebases and provide fast resolution to issues. Download the model on HuggingFace, Unsloth, LM Studio <br> <br>

13. ***Salesforce's SELF-MAS for adaptive multi-agent systems:  <br>This research introduces SELF-MAS, a self-supervised, inference-time framework for automatically designing multi-agent systems (MAS) without a validation set. SELF-MAS uses meta-level design to iteratively generate, evaluate, and refine MAS configurations for each problem instance, enabling dynamic agent composition and problem decomposition, outperforming baselines by an average of 7.44% in accuracy.*** <br> <br>
    May 21, Salesforce published a [paper](https://arxiv.org/pdf/2505.14996) “Meta-Design Matters: A Self-Design Multi-Agent System”. Multi-agent systems (MAS) leveraging the impressive capabilities of Large Language Models (LLMs) hold significant potential for tackling complex tasks. However, most current MAS depend on manually designed agent roles and communication protocols. These manual designs often fail to align with the underlying LLMs' strengths and struggle to adapt to novel tasks. Recent automatic MAS approaches attempt to mitigate these limitations but typically necessitate a validation-set for tuning and yield static MAS designs lacking adaptability during inference. The study introduces SELF-MAS, the first self-supervised, inference-time only framework for automatic MAS design. SELF-MAS employs meta-level design to iteratively generate, evaluate, and refine MAS configurations tailored to each problem instance, without requiring a validation set. Critically, it enables dynamic agent composition and problem decomposition through meta-feedback on solvability and completeness. Experiments across math, graduate-level QA, and software engineering benchmarks, using both closed-source and open-source LLM back-bones of varying sizes, demonstrate that SELF-MAS outperforms both manual and automatic MAS baselines, achieving a 7.44% average accuracy improvement over the next strongest baseline while maintaining cost-efficiency. These findings underscore the promise of meta-level self-supervised design for creating effective and adaptive MAS. <br> <br>

15. ***MIT and MIT-IBM's RL Tango framework:  <br>This paper proposes Tango, a novel framework that uses reinforcement learning to concurrently train both an LLM generator and a generative, process-level LLM verifier in an interleaved manner, without explicit process-level annotations for the verifier. Both components achieved state-of-the-art results among 7B/8B models on math and reasoning benchmarks, with the co-evolving verifier showing improved robustness and generalization.*** <br> <br>
    May 21, MIT and MIT-IBM published a [paper](https://arxiv.org/pdf/2505.15034) “RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning”. Reinforcement learning (RL) has recently emerged as a compelling approach for enhancing the reasoning capabilities of large language models (LLMs), where an LLM generator serves as a policy guided by a verifier (reward model). However, current RL post-training methods for LLMs typically use verifiers that are fixed (rule-based or frozen pretrained) or trained discriminatively via supervised fine-tuning (SFT). Such designs are susceptible to reward hacking and generalize poorly beyond their training distributions. To overcome these limitations, the study proposes Tango, a novel framework that uses RL to concurrently train both an LLM generator and a verifier in an interleaved manner. A central innovation of Tango is its generative, process-level LLM verifier, which is trained via RL and co-evolves with the generator. Importantly, the verifier is trained solely based on outcome-level verification correctness rewards without requiring explicit process-level annotations. This generative RL-trained verifier exhibits improved robustness and superior generalization compared to deterministic or SFT-trained verifiers, fostering effective mutual reinforcement with the generator. Extensive experiments demonstrate that both components of Tango achieve state-of-the-art results among 7B/8B-scale models: the generator attains best-in-class performance across five competition-level math benchmarks and four challenging out-of-domain reasoning tasks, while the verifier leads on the ProcessBench dataset. Remarkably, both components exhibit particularly substantial improvements on the most difficult mathematical reasoning problems. https://github.com/kaiwenzha/rl-tango. <br> <br>

17. ***UMD and Yale's Mixture-of-Thought for logical reasoning:  <br>This study introduces Mixture-of-Thought (MoT), a framework enabling LLMs to reason across natural language, code, and a novel symbolic truth-table modality. MoT uses a two-phase design (self-evolving training across modalities and synergistic inference) and significantly outperforms single-modality baselines on logical reasoning benchmarks (up to +11.7pp accuracy gain), especially on harder problems.*** <br> <br>
    May 21, UMD and Yale Uni published a [paper](https://arxiv.org/pdf/2505.15817) “Learning to Reason via Mixture-of-Thought for Logical Reasoning”. Human beings naturally utilize multiple reasoning modalities to learn and solve logical problems, i.e., different representational formats such as natural language, code, and symbolic logic. In contrast, most existing LLM-based approaches operate with a single reasoning modality during training, typically natural language. Although some methods explored modality selection or augmentation at inference time, the training process remains modality-blind, limiting synergy among modalities. To fill in this gap, the study proposes Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three complementary modalities: natural language, code, and a newly introduced symbolic modality, truth-table, which systematically enumerates logical cases and partially mitigates key failure modes in natural language reasoning. MoT adopts a two-phase design: (1) self-evolving MoT training, which jointly learns from filtered, self-generated rationales across modalities; and (2) MoT inference, which fully leverages the synergy of three modalities to produce better predictions. Experiments on logical reasoning benchmarks including FOLIO and ProofWriter demonstrate that our MoT framework consistently and significantly outperforms strong LLM baselines with single-modality chain-of-thought approaches, achieving up to +11.7pp average accuracy gain. Further analyses show that the MoT framework benefits both training and inference stages; that it is particularly effective on harder logical reasoning problems; and that different modalities contribute complementary strengths, with truth-table reasoning helping to overcome key bottlenecks in natural language inference. https://github.com/zhengkid/Truth_Table_Logical_Reasoning <br> <br>

19. ***Mila's Self-Evolving Curriculum for LLM reasoning:  <br>Researchers proposed Self-Evolving Curriculum (SEC), an automatic curriculum learning method for RL fine-tuning of LLMs, which learns a curriculum policy concurrently with the RL process. By formulating curriculum selection as a Multi-Armed Bandit problem and using policy gradient advantage as a reward signal, SEC significantly improved reasoning capabilities and generalization on harder, out-of-distribution problems across various domains.*** <br> <br>
    May 20, Mila, Uni of Montreal, et al. published a [paper](https://arxiv.org/pdf/2505.14970) “Self-Evolving Curriculum for LLM Reasoning”. Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation. A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented. While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive. To address these limitations, the study proposes Self-Evolving Curriculum (SEC), an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process. The approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm. The work leverages the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain. At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method. Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, experiments demonstrate that SEC significantly improves models' reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems. Additionally, the approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains. These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs. <br> <br>

21. ***Stanford's investigation of LLM depth efficiency:  <br>This study analyzed the Llama 3.1 and Qwen 3 model families to determine if deeper LLMs use their depth efficiently, finding that layers in the second half contribute much less and skipping them has a smaller effect. Evidence suggests deeper models primarily spread similar computations over more layers for fine-grained adjustments rather than composing new higher-order computations, potentially explaining diminishing returns with increased depth.*** <br> <br>
    May 20, Stanford Uni published a [paper](https://arxiv.org/pdf/2505.13898) “Do Language Models Use Their Depth Efficiently?”. Modern LLMs are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible in shallow models, or do they merely spread the same kinds of computation out over more layers? To address these questions, the study analyzes the residual stream of the Llama 3.1 and Qwen 3 family of models. The work finds: First, comparing the output of the sublayers to the residual stream reveals that layers in the second half contribute much less than those in the first half, with a clear phase transition between the two halves. Second, skipping layers in the second half has a much smaller effect on future computations and output predictions. Third, for multihop tasks, the study is unable to find evidence that models are using increased depth to compose subresults in examples involving many hops. Fourth, the work seeks to directly address whether deeper models are using their additional layers to perform new kinds of computation. To do this, the study trains linear maps from the residual stream of a shallow model to a deeper one, and finds that layers with the same relative depth map best to each other, suggesting that the larger model simply spreads the same computations out over its many layers. All this evidence suggests that deeper models are not using their depth to learn new kinds of computation, but only using the greater depth to perform more fine-grained adjustments to the residual. This may help explain why increasing scale leads to diminishing returns for stacked Transformer architectures. <br> <br>

23. ***Stanford et al.'s broader understanding of LLM sycophancy:  <br>This research introduces a richer theory of "social sycophancy" in LLMs, defining it as excessive preservation of a user's face (positive self-image) beyond mere agreement with stated beliefs. Using the ELEPHANT framework, experiments across eight models showed high rates of social sycophancy (e.g., preserving face 47% more than humans on OEQ), which is rewarded in preference datasets and difficult to mitigate.*** <br> <br>
    May 20, Stanford Uni, CMU and Uni of Oxford published a [paper](https://arxiv.org/pdf/2505.13995) “Social Sycophancy: A Broader Understanding of LLM Sycophancy”. A serious risk to the safety and utility of LLMs is sycophancy, i.e., excessive agreement with and flattery of the user. Yet existing work focuses on only one aspect of sycophancy: agreement with users' explicitly stated beliefs that can be compared to a ground truth. This overlooks forms of sycophancy that arise in ambiguous contexts such as advice and support-seeking, where there is no clear ground truth, yet sycophancy can reinforce harmful implicit assumptions, beliefs, or actions. To address this gap, the study introduces a richer theory of social sycophancy in LLMs, characterizing sycophancy as the excessive preservation of a user's face (the positive self-image a person seeks to maintain in an interaction). The study presents ELEPHANT, a framework for evaluating social sycophancy across five face-preserving behaviors (emotional validation, moral endorsement, indirect language, indirect action, and accepting framing) on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole (AITA). Across eight models, experiments show that LLMs consistently exhibit high rates of social sycophancy: on OEQ, they preserve face 47% more than humans, and on AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments in 42% of cases. The study further shows that social sycophancy is rewarded in preference datasets and is not easily mitigated. The work provides theoretical grounding and empirical tools (datasets and code) for understanding and addressing this under-recognized but consequential issue. <br> <br>

25. ***Harvard et al.'s study on reasoning pitfalls in LLM instruction-following:  <br>This research uncovered that explicit chain-of-thought (CoT) reasoning in RLLMs can significantly degrade instruction-following accuracy, despite enhancing performance on complex reasoning tasks. Analyzing 15 models, the study found CoT often diverts attention from instruction-relevant tokens, and proposed selective reasoning strategies, particularly classifier-selective reasoning, to substantially recover lost performance.*** <br> <br>
    May 20, Harvard Uni, Amazon, and NYU published a [paper](https://arxiv.org/pdf/2505.11423) “When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs”. Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, the study uncovers a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), the study consistently observes performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, the authors identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). The study proposes a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, the study introduces and evaluates four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Experimental results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. <br> <br>

27. ***Evaluating AI value prioritization with AIRiskDilemmas:  <br>Researchers from multiple institutions created LitmusValues, an evaluation pipeline, and AIRiskDilemmas, a collection of scenarios, to reveal AI models' value priorities and predict risky behaviors. They found that AI models' choices in these dilemmas, driven by values (even seemingly innocuous ones like "Care"), can predict both seen and unseen risky behaviors relevant to AI safety.*** <br> <br>
    May 20, Uni of Washington, Nvidia, Uni of Cambridge, Stanford Uni, MIT, Harvard Uni and Anthropic published a [paper](https://arxiv.org/pdf/2505.14633) “Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas”. Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans (i.e., illegal activities that may hurt others) are sometimes guided by strongly-held values, the authors believe that identifying values within AI models can be an early warning system for AI's risky behaviors. The study creates LitmusValues, an evaluation pipeline to reveal AI models' priorities on a range of AI value classes. Then, the authors collect AIRiskDilemmas, a diverse collection of dilemmas that pit values against one another in scenarios relevant to AI safety risks such as Power Seeking. By measuring an AI model's value prioritization using its aggregate choices, the study obtains a self-consistent set of predicted value priorities that uncover potential risks. The study shows that values in LitmusValues (including seemingly innocuous ones like Care) can predict for both seen risky behaviors in AIRiskDilemmas and unseen risky behaviors in HarmBench. <br> <br>

29. ***Cornell's method for universal embedding translation:  <br>This study introduces a method to translate text embeddings between different vector spaces without paired data, encoders, or predefined matches by leveraging a conjectured universal latent semantic structure. While achieving high cosine similarity across diverse models, this unsupervised translation capability also highlights security risks for vector databases, as it allows adversaries to extract sensitive information from embeddings alone.*** <br> <br>
    May 20, Cornell Uni published a [paper](https://arxiv.org/pdf/2505.12540) “Harnessing the Universal Geometry of Embeddings”. The study introduces the first method for translating text embeddings from one vector space to another without any paired data, encoders, or predefined sets of matches. The unsupervised approach translates any embedding to and from a universal latent representation (i.e., a universal semantic structure conjectured by the Platonic Representation Hypothesis). The translations achieve high cosine similarity across model pairs with different architectures, parameter counts, and training datasets. The ability to translate unknown embeddings into a different space while preserving their geometry has serious implications for the security of vector databases. An adversary with access only to embedding vectors can extract sensitive information about the underlying documents, sufficient for classification and attribute inference. <br> <br>

31. ***CMU and MIT's Mean Flows for one-step generative modeling:  <br>This work proposes MeanFlow, a principled framework for one-step generative modeling that uses the concept of average velocity to characterize flow fields, contrasting with instantaneous velocity in Flow Matching. MeanFlow, requiring no pre-training or distillation, achieves strong empirical performance (FID of 3.43 at 1-NFE on ImageNet 256x256), significantly outperforming previous one-step models and narrowing the gap with multi-step approaches.*** <br> <br>
    May 19, CMU and MIT published a [paper](https://arxiv.org/pdf/2505.13447v1) “Mean Flows for One-step Generative Modeling”. The work proposes a principled and effective framework for one-step generative modeling. The authors introduce the notion of average velocity to characterize flow fields, in contrast to instantaneous velocity modeled by Flow Matching methods. A well-defined identity between average and instantaneous velocities is derived and used to guide neural network training. The method, termed the MeanFlow model, is self-contained and requires no pre-training, distillation, or curriculum learning. MeanFlow demonstrates strong empirical performance: it achieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet 256x256 trained from scratch, significantly outperforming previous state-of-the-art one-step diffusion/flow models. The study substantially narrows the gap between one-step diffusion/flow models and their multi-step predecessors, and hoping it will motivate future research to revisit the foundations of these powerful models. <br> <br>

33. ***FutureHouse and Oxford's Robin for automated scientific discovery:  <br>This paper introduces Robin, a multi-agent system capable of fully automating key intellectual steps of the scientific process, including literature search, hypothesis generation, experiment proposal, and result interpretation. Robin successfully identified and validated ripasudil, a novel therapeutic candidate for dry age-related macular degeneration (dAMD), demonstrating a new paradigm for AI-driven scientific discovery.*** <br> <br>
    May 19, FutureHouse and Uni of Oxford published a [paper](https://arxiv.org/pdf/2505.13400) “Robin: A multi-agent system for automating scientific discovery”. Scientific discovery is driven by the iterative process of background research, hypothesis generation, experimentation, and data analysis. Despite recent advancements in applying artificial intelligence to scientific discovery, no system has yet automated all of these stages in a single workflow. This study introduces Robin, the first multi-agent system capable of fully automating the key intellectual steps of the scientific process. By integrating literature search agents with data analysis agents, Robin can generate hypotheses, propose experiments, interpret experimental results, and generate updated hypotheses, achieving a semi-autonomous approach to scientific discovery. By applying this system, the authors were able to identify a novel treatment for dry age-related macular degeneration (dAMD), the major cause of blindness in the developed world. Robin proposed enhancing retinal pigment epithelium phagocytosis as a therapeutic strategy, and identified and validated a promising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho kinase (ROCK) inhibitor that has never previously been proposed for treating dAMD. To elucidate the mechanism of ripasudil-induced upregulation of phagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment, which revealed upregulation of ABCA1, a critical lipid efflux pump and possible novel target. All hypotheses, experimental plans, data analyses, and data figures in the main text of this report were produced by Robin. As the first AI system to autonomously discover and validate a novel therapeutic candidate within an iterative lab-in-the-loop framework, Robin establishes a new paradigm for AI-driven scientific discovery. <br> <br>

35. ***EPFL and Princeton's study on GPT-4's conversational persuasiveness:  <br>This preregistered study found that in short multiround debates, GPT-4, when given access to participants' sociodemographic data for personalization, was more persuasive than human opponents 64.4% of the time (81.2% relative increase in odds of higher post-debate agreement). These findings highlight the potent persuasive capabilities of LLMs and have implications for online platform governance.*** <br> <br>
    May 19, EPFL and Princeton Uni published a [paper](https://www.nature.com/articles/s41562-025-02194-6) “On the conversational persuasiveness of GPT-4”. Early work has found that large language models (LLMs) can generate persuasive content. However, evidence on whether they can also personalize arguments to individual attributes remains limited, despite being crucial for assessing misuse. This preregistered study examines AI-driven persuasion in a controlled setting, where participants engaged in short multiround debates. Participants were randomly assigned to 1 of 12 conditions in a 2 × 2 × 3 design: (1) human or GPT-4 debate opponent; (2) opponent with or without access to sociodemographic participant data; (3) debate topic of low, medium or high opinion strength. In debate pairs where AI and humans were not equally persuasive, GPT-4 with personalization was more persuasive 64.4% of the time (81.2% relative increase in odds of higher post-debate agreement; 95% confidence interval [+26.0%, +160.7%], P < 0.01; N = 900). The findings highlight the power of LLM-based persuasion and have implications for the governance and design of online platforms. <br> <br>

37. ***Stanford and Microsoft's general user models from computer use:  <br>This paper presents an architecture for a general user model (GUM) that learns about a user by observing any unstructured interaction with their computer (e.g., screenshots), constructing confidence-weighted propositions about their knowledge and preferences. GUMs enable applications like context-aware assistants, intelligent notification management, and proactive agents that adapt to user needs across apps.*** <br> <br>
    May 19, Stanford Uni and Microsoft published a [paper](https://arxiv.org/pdf/2505.10831) “Creating General User Models from Computer Use”. Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture that user knowledge and preferences. GUMs can infer that a user is preparing for a wedding they're attending from messages with a friend. Or recognize that a user is struggling with a collaborator's feedback on a draft by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, the study demonstrates how they augment chat-based assistants with context, manage OS notifications to selectively surface important information, and enable interactive agents that adapt to preferences across apps. The study also instantiates proactive assistants (GUMBOs) that discover and execute useful suggestions on a user's behalf using their GUM. In the evaluations, the study finds that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions that users wouldn't think to request explicitly. Altogether, GUMs introduce methods that leverage multimodal models to understand unstructured context, enabling long-standing visions of HCI and entirely new interactive systems that anticipate user needs. <br> <br>

39. ***UC Berkeley et al.'s theoretical perspective on continuous chain-of-thought:  <br>This study provides a theoretical understanding of why continuous chain-of-thoughts (CoTs) outperform discrete CoTs in reasoning tasks like directed graph reachability. They prove that a two-layer transformer with D steps of continuous CoTs can solve this problem by encoding multiple search frontiers simultaneously (like parallel BFS), while discrete CoTs require more steps for sequential search.*** <br> <br>
    May 18, UC Berkeley, UCSD and Meta published a [paper](https://arxiv.org/pdf/2505.12514) “Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought”. Large Language Models (LLMs) have demonstrated remarkable performance in many applications, including challenging reasoning problems via chain-of-thoughts (CoTs) techniques that generate “thinking tokens” before answering the questions. While existing theoretical works demonstrate that CoTs with discrete tokens boost the capability of LLMs, recent work on continuous CoTs lacks a theoretical understanding of why it outperforms discrete counterparts in various reasoning tasks such as directed graph reachability, a fundamental graph reasoning problem that includes many practical domain applications as special cases. The study proves that a two-layer transformer with D steps of continuous CoTs can solve the directed graph reachability problem, where D is the diameter of the graph, while the best known result of constant-depth transformers with discrete CoTs requires O(n2) decoding steps where n is the number of vertices (D<n). In the construction, each continuous thought vector is a superposition state that encodes multiple search frontiers simultaneously (i.e., parallel breadth-first search (BFS)), while discrete CoTs must choose a single path sampled from the superposition state, which leads to sequential search that requires many more steps and may be trapped into local solutions. The study also performed extensive experiments to verify that the theoretical construction aligns well with the empirical solution obtained via training dynamics. Notably, encoding of multiple search frontiers as a superposition state automatically emerges in training continuous CoTs, without explicit supervision to guide the model to explore multiple paths simultaneously. <br> <br>

41. ***USC and Google's Multi-Objective Preference Optimization:  <br>This research introduces the Multi-Objective Preference Optimization (MOPO) algorithm to address aligning LLMs with multiple, potentially conflicting, human objectives (e.g., helpfulness and harmlessness). MOPO frames alignment as constrained KL-regularized optimization, operating directly on pairwise preference data to maximize a primary objective while ensuring secondary objectives meet safety thresholds, outperforming baselines on real-world datasets.*** <br> <br>
    May 16, Uni of Southern California and Google published a [paper](https://arxiv.org/pdf/2505.10892) “Multi-Objective Preference Optimization: Improving Human Alignment of Generative Models”. Post-training of LLMs with RLHF, and subsequently preference optimization algorithms such as DPO, IPO, etc., made a big difference in improving human alignment. However, all such techniques can only work with a single (human) objective. In practice, human users have multiple objectives, such as helpfulness and harmlessness, and there is no natural way to aggregate them into a single objective. This study addresses the multi-objective preference-alignment problem, where a policy must optimize several, potentially conflicting, objectives. The work introduces the Multi-Objective Preference Optimization (MOPO) algorithm, which frames alignment as a constrained KL-regularized optimization: the primary objective is maximized while secondary objectives are lower-bounded by tunable safety thresholds. Unlike prior work, MOPO operates directly on pairwise preference data, requires no point-wise reward assumption, and avoids heuristic prompt-context engineering. The method recovers policies on the Pareto front whenever the front is attainable; practically, it reduces to simple closed-form iterative updates suitable for large-scale training. On synthetic benchmarks with diverse canonical preference structures, MOPO approximates the Pareto front. When fine-tuning a 1.3B-parameter language model on real-world human-preference datasets, MOPO attains higher rewards and yields policies that Pareto-dominate baselines; ablation studies confirm optimization stability and robustness to hyperparameters. <br> <br>

43. ***Microsoft's Fourier space perspective on diffusion models:  <br>This paper analyzes the inductive bias of the DDPM forward process in Fourier space, showing that high-frequency components are corrupted faster, leading to violations of the reverse process's normality assumption and degraded high-frequency generation. An alternative Fourier space forward process that corrupts all frequencies equally demonstrated improved performance on datasets where high frequencies are primary.*** <br> <br>
    May 16, Microsoft published a [paper](https://arxiv.org/pdf/2505.11278) “A Fourier Space Perspective on Diffusion Models”. Diffusion models are state-of-the-art generative models on data modalities such as images, audio, proteins and materials. These modalities share the property of exponentially decaying variance and magnitude in the Fourier domain. Under the standard Denoising Diffusion Probabilistic Models (DDPM) forward process of additive white noise, this property results in high-frequency components being corrupted faster and earlier in terms of their Signal-to-Noise Ratio (SNR) than low-frequency ones. The reverse process then generates low-frequency information before high-frequency details. The authors study the inductive bias of the forward process of diffusion models in Fourier space. The work theoretically analyses and empirically demonstrates that the faster noising of high-frequency components in DDPM results in violations of the normality assumption in the reverse process. Experiments show that this leads to degraded generation quality of high-frequency components. The authors then study an alternate forward process in Fourier space which corrupts all frequencies at the same rate, removing the typical frequency hierarchy during generation, and demonstrate marked performance improvements on datasets where high frequencies are primary, while performing on par with DDPM on standard imaging benchmarks. <br> <br>

45. ***Questioning representational optimism with the FER hypothesis:  <br>This position paper challenges the assumption that better AI performance implies better internal representations by comparing SGD-trained networks with those evolved through open-ended search on a simple image generation task. SGD networks exhibited "fractured entangled representation" (FER), a disorganization largely absent in evolved networks, suggesting FER might degrade core model capacities and that mitigating it is crucial for future representation learning.*** <br> <br>
    May 16, MIT, Uni of Columbia, Vector Inst., Uni of Oxford, and Lila Sciences published a [paper](https://arxiv.org/pdf/2505.11581) “Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis”. Much of the excitement in modern AI is driven by the observation that scaling up existing systems leads to better performance. But does better performance necessarily imply better internal representations? While the representational optimist assumes it must, this position paper challenges that view. The study compares neural networks evolved through an open-ended search process to networks trained via conventional stochastic gradient descent (SGD) on the simple task of generating a single image. This minimal setup offers a unique advantage: each hidden neuron's full functional behavior can be easily visualized as an image, thus revealing how the network's output behavior is internally constructed neuron by neuron. The result is striking: while both networks produce the same output behavior, their internal representations differ dramatically. The SGD-trained networks exhibit a form of disorganization that is termed fractured entangled representation (FER). Interestingly, the evolved networks largely lack FER, even approaching a unified factored representation (UFR). In large models, FER may be degrading core model capacities like generalization, creativity, and (continual) learning. Therefore, understanding and mitigating FER could be critical to the future of representation learning. <br> <br>

47. ***Uni of Washington and Penn's study on iteratively reweighted kernel machines:  <br>This research argues that the ability to learn low-dimensional representations and hierarchical structure is not unique to neural networks and can be achieved with classical kernel methods. They show that the derivative of a kernel predictor can detect influential coordinates, and by iteratively reweighting data and retraining kernel machines using these derivatives, one can efficiently learn hierarchical polynomials.*** <br> <br>
    May 13, Uni of Washington and Uni of Penn published a [paper](https://arxiv.org/pdf/2505.08277) “Iteratively reweighted kernel machines efficiently learn sparse functions”. The impressive practical performance of neural networks is often attributed to their ability to learn low-dimensional data representations and hierarchical structure directly from data. This study argues that these two phenomena are not unique to neural networks, and can be elicited from classical kernel methods. Namely, the study shows that the derivative of the kernel predictor can detect the influential coordinates with low sample complexity. Moreover, by iteratively using the derivatives to reweight the data and retrain kernel machines, one is able to efficiently learn hierarchical polynomials with finite leap complexity. Numerical experiments illustrate the developed theory. <br> <br>

49. ***Tech Innovation Inst. and Sapienza Uni on RAG's distracting effect:  <br>This study investigates the "distracting effect" in Retrieval Augmented Generation (RAG), where irrelevant retrieved passages cause LLMs to generate incorrect answers. They provide a quantifiable measure for this effect, introduce methods for identifying hard distracting passages, and show that fine-tuning LLMs with these passages can increase answering accuracy by up to 7.5% compared to conventional RAG datasets.*** <br> <br>
    May 11, Tech Innovation Inst. and Sapienza Uni published a [paper](https://arxiv.org/pdf/2505.06914) “The Distracting Effect: Understanding Irrelevant Passages in RAG”. A well-known issue with Retrieval Augmented Generation (RAG) is that retrieved passages that are irrelevant to the query sometimes distract the answer-generating LLM, causing it to provide an incorrect response. This study shed light on this core issue and formulate the distracting effect of a passage w.r.t. a query (and an LLM). The study provides a quantifiable measure of the distracting effect of a passage and demonstrate its robustness across LLMs. The research introduces novel methods for identifying and using hard distracting passages to improve RAG systems. By fine-tuning LLMs with these carefully selected distracting passages, the study achieves up to a 7.5% increase in answering accuracy compared to counterparts fine-tuned on conventional RAG datasets. The contribution is two-fold: first, the work moves beyond the simple binary classification of irrelevant passages as either completely unrelated vs. distracting, and second, the study develops and analyzes multiple methods for finding hard distracting passages. 

 <br> <br> <br>

***May 18, 2025***

1. ***Google's AlphaEvolve agent:  <br>This white paper introduces AlphaEvolve, an evolutionary coding agent that uses a pipeline of LLMs to autonomously improve algorithms by directly modifying code and receiving feedback. The agent has demonstrated success in optimizing Google's data center scheduling, simplifying hardware accelerator circuits, accelerating its own LLM training, and discovering novel, provably correct algorithms, including a more efficient method for 4x4 complex matrix multiplication.*** <br> <br>
   May 16, Google published a [paper](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf) “AlphaEvolve A coding agent for scientific and algorithmic discovery”. The white paper presents AlphaEvolve, an evolutionary coding agent that substantially enhances capabilities of state-of-the-art LLMs on highly challenging tasks such as tackling open scientific problems or optimizing critical pieces of computational infrastructure. AlphaEvolve orchestrates an autonomous pipeline of LLMs, whose task is to improve an algorithm by making direct changes to the code. Using an evolutionary approach, continuously receiving feedback from one or more evaluators, AlphaEvolve iteratively improves the algorithm, potentially leading to new scientific and practical discoveries. The study demonstrates the broad applicability of this approach by applying it to a number of important computational problems. When applied to optimizing critical components of large-scale computational stacks at Google, AlphaEvolve developed a more efficient scheduling algorithm for data centers, found a functionally equivalent simplification in the circuit design of hardware accelerators, and accelerated the training of the LLM underpinning AlphaEvolve itself. Furthermore, AlphaEvolve discovered novel, provably correct algorithms that surpass state-of-the-art solutions on a spectrum of problems in mathematics and computer science, significantly expanding the scope of prior automated discovery methods (Romera-Paredes et al., 2023). Notably, AlphaEvolve developed a search algorithm that found a procedure to multiply two 4 × 4 complex-valued matrices using 48 scalar multiplications; offering the first improvement, after 56 years, over Strassen’s algorithm in this setting. The authors believe AlphaEvolve and coding agents like it can have a significant impact in improving solutions of problems across many areas of science and computation. <br> <br>

3. ***Meta's J1 LLM-as-a-Judge:  <br>This research introduces J1, a reinforcement learning approach for training LLM-as-a-Judge models, which converts prompts into judgment tasks with verifiable rewards to incentivize thinking and reduce bias. J1 models, trained at 8B or 70B sizes, reportedly outperform other models of similar scale, including those distilled from DeepSeek-R1, and even larger models like R1 on some benchmarks, by learning to outline criteria, compare against self-generated answers, and re-evaluate responses.*** <br> <br>
   May 16, Meta published a [paper](https://arxiv.org/pdf/2505.10320) “J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning”. The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. This study introduces J1, a reinforcement learning approach to training such models. The method converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards that incentivize thinking and mitigate judgment bias. In particular, the approach outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model. The work provides analysis and ablations comparing Pairwise-J1 vs Pointwise-J1 models, offline vs online training recipes, reward strategies, seed prompts, and variations in thought length and content. The study finds that the models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses. <br> <br>

5. ***MIT's study on neural scaling:  <br>Researchers investigated the origins of neural scaling laws in LLMs, proposing that superposition (representing more features than model dimensions) and varying feature frequencies explain why loss decreases as a power law with model size. Their toy model showed that under strong superposition, loss becomes inversely proportional to model dimension, a finding consistent with analyses of open-sourced LLMs and Chinchilla scaling, suggesting superposition is a key mechanism.*** <br> <br>
   May 15, MIT published a [paper](https://arxiv.org/abs/2505.10465) “Superposition Yields Robust Neural Scaling”. The success of today's large language models (LLMs) depends on the observation that larger models perform better. However, the origin of this neural scaling law -- the finding that loss decreases as a power law with model size -- remains unclear. Starting from two empirical principles -- that LLMs represent more things than the model dimensions (widths) they have (i.e., representations are superposed), and that words or concepts in language occur with varying frequencies – the work constructed a toy model to study the loss scaling with model size. The study found that when superposition is weak, meaning only the most frequent features are represented without interference, the scaling of loss with model size depends on the underlying feature frequency; if feature frequencies follow a power law, so does the loss. In contrast, under strong superposition, where all features are represented but overlap with each other, the loss becomes inversely proportional to the model dimension across a wide range of feature frequency distributions. This robust scaling behavior is explained geometrically: when many more vectors are packed into a lower dimensional space, the interference (squared overlaps) between vectors scales inversely with that dimension. The study then analyzed four families of open-sourced LLMs and found that they exhibit strong superposition and quantitatively match the predictions of the toy model. The Chinchilla scaling law turned out to also agree with the results. The study concludes that representation superposition is an important mechanism underlying the observed neural scaling laws. The study anticipates that these insights will inspire new training strategies and model architectures to achieve better performance with less computation and fewer parameters. https://github.com/liuyz0/SuperpositionScaling <br> <br>

7. ***Google and Princeton's evolutionary perspective on Transformer learning:  <br>This paper draws an analogy between Transformer learning modes (in-weights and in-context) and evolutionary adaptation strategies (genetic encoding and phenotypic plasticity) to explore how predictability influences learning. Experiments showed high environmental stability favors in-weights learning, while high cue reliability enhances in-context learning, with learning dynamics showing task-contingent shifts between these modes, supporting a relative-cost hypothesis.*** <br> <br>
   May 14, Google and Princeton Uni published a [paper](https://www.arxiv.org/pdf/2505.09855) “Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers”. Transformer models learn in two distinct modes: in-weights learning (IWL), encoding knowledge into model weights, and in-context learning (ICL), adapting flexibly to context without weight modification. To better understand the interplay between these learning modes, the authors draw inspiration from evolutionary biology's analogous adaptive strategies: genetic encoding (akin to IWL, adapting over generations and fixed within an individual's lifetime) and phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to environmental cues). In evolutionary biology, environmental predictability dictates the balance between these strategies: stability favors genetic encoding, while reliable predictive cues promote phenotypic plasticity. The work experimentally operationalizes these dimensions of predictability and systematically investigate their influence on the ICL/IWL balance in Transformers. Using regression and classification tasks, the study shows that high environmental stability decisively favors IWL, as predicted, with a sharp transition at maximal stability. Conversely, high cue reliability enhances ICL efficacy, particularly when stability is low. Furthermore, learning dynamics reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift occurs in some settings (e.g., classification with many classes), the study demonstrates that scenarios with easier IWL (e.g., fewer classes) or slower ICL acquisition (e.g., regression) can exhibit an initial IWL phase later yielding to ICL dominance. These findings support a relative-cost hypothesis for explaining these learning mode transitions, establishing predictability as a critical factor governing adaptive strategies in Transformers, and offering novel insights for understanding ICL and guiding training methodologies. <br> <br>

9. ***ScienceAdvances paper on LLM social conventions:  <br>This research demonstrated that decentralized populations of LLM agents can spontaneously develop universally adopted social conventions without explicit programming. The study also showed how strong collective biases can emerge during this process, even from individually unbiased agents, and how committed minority groups can drive social change by imposing alternative conventions, highlighting implications for AI alignment.*** <br> <br>
    May 14, ScienceAdvances published a [paper](https://www.science.org/doi/10.1126/sciadv.adu9368) “Emergent social conventions and collective bias in LLM populations”. Social conventions are the backbone of social coordination, shaping how individuals form a group. As growing populations of artificial intelligence (AI) agents communicate through natural language, a fundamental question is whether they can bootstrap the foundations of a society. The research presents experimental results that demonstrate the spontaneous emergence of universally adopted social conventions in decentralized populations of large language model (LLM) agents. The study then shows how strong collective biases can emerge during this process, even when agents exhibit no bias individually. Last, the study examines how committed minority groups of adversarial LLM agents can drive social change by imposing alternative social conventions on the larger population. The results show that AI systems can autonomously develop social conventions without explicit programming and have implications for designing AI systems that align, and remain aligned, with human values and societal goals. <br> <br>

11. ***A mechanistic look at LLM contextual entrainment:  <br>Researchers from the University of Toronto et al. identified "contextual entrainment," a phenomenon where LMs assign higher probability to tokens previously seen in the prompt, even random ones, suggesting a mechanistic distraction independent of semantic relevance but modulated by it. They hypothesized and identified "entrainment heads" responsible, showing that deactivating them attenuates this effect and reduces distraction.*** <br> <br>
    May 14, Uni of Toronto, UKP Lab and Microsoft published a [paper](https://arxiv.org/pdf/2505.09338) “Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs”. The study observes a novel phenomenon, contextual entrainment, across a wide range of language models (LMs) and prompt settings, providing a new mechanistic perspective on how LMs become distracted by “irrelevant” contextual information in the input prompt. Specifically, LMs assign significantly higher logits (or probabilities) to any tokens that have previously appeared in the context prompt, even for random tokens. This suggests that contextual entrainment is a mechanistic phenomenon, occurring independently of the relevance or semantic relation of the tokens to the question or the rest of the sentence. The study finds statistically significant evidence that the magnitude of contextual entrainment is influenced by semantic factors. Counterfactual prompts have a greater effect compared to factual ones, suggesting that while contextual entrainment is a mechanistic phenomenon, it is modulated by semantic factors. The authors hypothesise that there is a circuit of attention heads -- the entrainment heads -- that corresponds to the contextual entrainment phenomenon. Using a novel entrainment head discovery method based on differentiable masking, the work identifies these heads across various settings. When authors “turn off” these heads, i.e., set their outputs to zero, the effect of contextual entrainment is significantly attenuated, causing the model to generate output that capitulates to what it would produce if no distracting context were provided. The discovery of contextual entrainment, along with the investigation into LM distraction via the entrainment heads, marks a key step towards the mechanistic analysis and mitigation of the distraction problem. <br> <br>

13. ***KAIST's system prompt optimization:  <br>This study introduced the novel problem of bilevel system prompt optimization, aiming to design robust and transferable system prompts for LLMs, as opposed to focusing only on task-specific user prompts. They proposed a meta-learning framework that iteratively optimizes system prompts over diverse user prompts and datasets, demonstrating improved generalization to unseen tasks and faster adaptation with fewer optimization steps for user prompts.*** <br> <br>
    May 14, KAIST and DeepAuto.ai published a [paper](https://www.arxiv.org/pdf/2505.09666) “System Prompt Optimization with Meta-Learning”. Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance. However, while LLM prompts consist of both the task-agnostic system prompts and task-specific user prompts, existing work on prompt optimization has focused on user prompts specific to individual queries or tasks, and largely overlooked the system prompt that is, once optimized, applicable across different tasks and domains. Motivated by this, the study introduces the novel problem of bilevel system prompt optimization, whose objective is to design system prompts that are robust to diverse user prompts and transferable to unseen tasks. To tackle this problem, the work then proposes a meta-learning framework, which meta-learns the system prompt by optimizing it over various user prompts across multiple datasets, while simultaneously updating the user prompts in an iterative manner to ensure synergy between them. The study conducts experiments on 14 unseen datasets spanning 5 different domains, on which the study shows that the approach produces system prompts that generalize effectively to diverse user prompts. Also, the findings reveal that the optimized system prompt enables rapid adaptation even to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance. <br> <br>

15. ***OpenAI's HealthBench benchmark:  <br>This work presents HealthBench, an open-source benchmark for evaluating LLM performance and safety in healthcare through 5,000 multi-turn conversations assessed by physicians using conversation-specific rubrics with 48,562 criteria. Performance on HealthBench has shown steady progress, with recent rapid improvements (e.g., o3 scoring 60% vs. GPT-3.5 Turbo's 16%), and the release includes variations like HealthBench Consensus and Hard to guide model development.*** <br> <br>
    May 13, OpenAI published a [paper](https://cdn.openai.com/pdf/bd7a39d5-9e9f-47b3-903c-8b847ca650c7/healthbench_paper.pdf) “HealthBench: Evaluating Large Language Models Towards Improved Human Health”. The work presents HealthBench, an open-source benchmark measuring the performance and safety of large language models in healthcare. HealthBench consists of 5,000 multi-turn conversations between a model and an individual user or healthcare professional. Responses are evaluated using conversation-specific rubrics created by 262 physicians. Unlike previous multiple-choice or short-answer benchmarks, HealthBench enables realistic, open-ended evaluation through 48,562 unique rubric criteria spanning several health contexts (e.g., emergencies, transforming clinical data, global health) and behavioral dimensions (e.g., accuracy, instruction following, communication). HealthBench performance over the last two years reflects steady initial progress (compare GPT-3.5 Turbo’s 16% to GPT-4o’s 32%) and more rapid recent improvements (o3 scores 60%). Smaller models have especially improved: GPT-4.1 nano outperforms GPT-4o and is 25 times cheaper. The study additionally release two HealthBench variations: HealthBench Consensus, which includes 34 particularly important dimensions of model behavior validated via physician consensus, and HealthBench Hard, where the current top score is 32%. Hope that HealthBench grounds progress towards model development and applications that benefit human health. https://github.com/openai/simple-evals <br> <br>

17. ***Analyzing LLM reasoning failures through BAPOs:  <br>Researchers introduced the bounded attention prefix oracle (BAPO) model to formalize how LLM capacity limits on internal information flow (attention bandwidth) lead to reasoning failures. They showed that BAPO-hard problems requiring high bandwidth cause failures in models like GPT-4, even on small instances, while chain of thought can transform BAPO-hard problems into BAPO-easy ones, offering insights for mitigating these limits.*** <br> <br>
    May 13, EmpiriQal Inc, LSEPS, Uni of Tubingen and Los Alamos Nat. Lab published a [paper](https://papers.cool/arxiv/2505.08739) “Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies”. Despite their many successes, transformer-based large language models (LLMs) continue to struggle with tasks that require complex reasoning over large parts of their input. The authors argue that these failures arise due to capacity limits on the accurate flow of information within LLMs. To formalize this issue, the study introduces the bounded attention prefix oracle (BAPO) model, a new computational framework that models bandwidth constraints on attention heads, the mechanism for internal communication in LLMs. The study shows that several important reasoning problems like graph reachability require high communication bandwidth for BAPOs to solve; the authors call these problems BAPO-hard. Experiments corroborate the theoretical predictions: GPT-4, Claude, and Gemini succeed on BAPO-easy tasks and fail even on relatively small BAPO-hard tasks. BAPOs also reveal another benefit of chain of thought (CoT): the work proves that breaking down a task using CoT can turn any BAPO-hard problem into a BAPO-easy one. The results offer principled explanations for key LLM failures and suggest directions for architectures and inference methods that mitigate bandwidth limits. <br> <br>

19. ***Cohere's Aya Vision for multilingual multimodality:  <br>This paper introduces Aya Vision, a series of multilingual multimodal models (8B and 32B) that address challenges like data scarcity and catastrophic forgetting. Using a synthetic annotation framework for diverse data and a cross-modal model merging technique, Aya Vision models achieve best-in-class performance against larger competitors by preserving text capabilities while enhancing multimodal generation.*** <br> <br>
    May 13, Cohere published a [paper](https://arxiv.org/pdf/2505.08751) “Aya Vision: Advancing the Frontier of Multilingual Multimodality”. Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, the study introduces novel techniques spanning both data and modeling. First, the work develops a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, the study proposes a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. The study further scales this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. The work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance. <br> <br>

21. ***Bank of England's study on LLM hidden states:  <br>This research investigated using the hidden states of LLMs to estimate and impute economic and financial statistics, finding that simple linear models trained on these states outperform the LLMs' direct text outputs for variables like county-level unemployment or firm-level assets. This suggests hidden states capture richer economic information, with only a few dozen labeled examples needed for training and potential for transfer learning without labeled target data.*** <br> <br>
    May 13, Bank of England published a [paper](https://arxiv.org/pdf/2505.08662) “Revealing economic facts: LLMs know more than they say”. The study investigates whether the hidden states of large language models (LLMs) can be used to estimate and impute economic and financial statistics. Focusing on county-level (e.g. unemployment) and firm-level (e.g. total assets) variables, the study shows that a simple linear model trained on the hidden states of open-source LLMs outperforms the models' text outputs. This suggests that hidden states capture richer economic information than the responses of the LLMs reveal directly. A learning curve analysis indicates that only a few dozen labelled examples are sufficient for training. The study also proposes a transfer learning method that improves estimation accuracy without requiring any labelled data for the target variable. Finally, the study demonstrates the practical utility of hidden-state representations in super-resolution and data imputation tasks. <br> <br>

23. ***UIUC's DynamicRAG framework:  <br>This research proposes DynamicRAG, a novel retrieval-augmented generation framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. The reranker is modeled as an agent optimized through reinforcement learning, using rewards derived from LLM output quality, and demonstrates state-of-the-art performance across seven knowledge-intensive datasets by better adapting retrieval to specific needs.*** <br> <br>
    May 12, UIUC published a [paper](https://arxiv.org/pdf/2505.07233) “DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation”. Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker, which refines retrieved documents to enhance generation quality and explainability. The challenge of selecting the optimal number of documents (k) remains unsolved: too few may omit critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. The research proposes DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. The authors model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results. https://github.com/GasolSun36/DynamicRAG <br> <br>

25. ***OpenAI chief scientist on AI's research potential:  <br>Jakub Pachocki, OpenAI's chief scientist, discussed the evolving capabilities of AI models, predicting they will achieve novel research insights and measurable economic impact before the end of the decade. He highlighted the role of reinforcement learning and OpenAI's plans to release an open-weight model, noting that AI reasoning differs from human reasoning but can still autonomously produce valuable software and scientific contributions.*** <br> <br>
    May 12, Nature published an [article](https://www.nature.com/articles/d41586-025-01485-2) “AI models are capable of novel research: OpenAI’s chief scientist on what to expect”. Jakub Pachocki, OpenAI's chief scientist, discusses the future of AI models and their potential for novel research. OpenAI, known for ChatGPT, has developed advanced AI tools, including reasoning models that specialize in logical tasks. These models assist researchers in various ways, such as polishing prose, writing code, and generating hypotheses. Despite their benefits, OpenAI faces criticism for the energy demands of its models and data exploitation for training. Pachocki, who joined OpenAI in 2017, leads the development of AI systems designed for complex tasks in science, mathematics, and coding. He emphasizes the importance of reinforcement learning, which uses human feedback to refine models. Pachocki believes AI models can discover novel insights, although their reasoning differs from human reasoning. OpenAI plans to release an open-weight model for researchers to further train, marking its first open model since GPT-2 in 2019. Pachocki's definition of artificial general intelligence (AGI) has evolved, with significant milestones falling faster than expected. He anticipates substantial progress in AI's ability to create novel research and make measurable economic impacts before the end of the decade. Pachocki is excited about the potential of AI to produce valuable software autonomously, even if it doesn't solve major science problems immediately. <br> <br>

27. ***Prime Intellect's INTELLECT-2 model:  <br>This paper introduces INTELLECT-2, a 32 billion parameter reasoning model trained via a globally distributed, fully asynchronous reinforcement learning run across a permissionless compute network. The project involved developing new infrastructure (PRIME-RL, TOPLOC, SHARDCAST) and modifying GRPO training to achieve stability and improve upon the QwQ-32B model, with all code and data open-sourced.*** <br> <br>
    May 12, Prime Intellect Team published a [paper](https://arxiv.org/pdf/2505.07291) “INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized Reinforcement Learning”. The work introduces INTELLECT-2, the first globally distributed reinforcement learning (RL) training run of a 32 billion parameter language model. Unlike traditional centralized training efforts, INTELLECT-2 trains a reasoning model using fully asynchronous RL across a dynamic, heterogeneous swarm of permissionless compute contributors. To enable a training run with this unique infrastructure, the study built various components from scratch: it introduces PRIME-RL, a training framework purpose-built for distributed asynchronous reinforcement learning, based on top of novel components such as TOPLOC, which verifies rollouts from untrusted inference workers, and SHARDCAST, which efficiently broadcasts policy weights from training nodes to inference workers. Beyond infrastructure components, the work proposes modifications to the standard GRPO training recipe and data filtering techniques that were crucial to achieve training stability and ensure that the model successfully learned its training objective, thus improving upon QwQ-32B, the state of the art reasoning model in the 32B parameter range. The authors open-source INTELLECT-2 along with all of the code and data, hoping to encourage and enable more open research in the field of decentralized training. https://github.com/PrimeIntellect-ai/prime-rl <br> <br>

29. ***Silicon Valley's ambition for AI:  <br>This article reveals a growing and increasingly open aspiration within Silicon Valley to automate not just some, but potentially all human jobs using artificial intelligence and robotics. Driven by motivations that range from capturing worker salaries (as explicitly stated by one investor) to purportedly improving global living standards, influential figures and companies are pursuing a future where AI handles cognitive tasks and robots perform physical labor. Despite current AI limitations in accuracy and robot dexterity, rapid advancements, exemplified by powerful LLMs and humanoid robots, suggest these hurdles may soon be overcome, placing the vast majority of jobs at risk and leaving humanity's future role uncertain, prompting critical questions about the ultimate goals and societal implications of this technological pursuit.*** <br> <br>
    May 12, theguardian.com published an [article](https://www.theguardian.com/commentisfree/2025/may/12/for-silicon-valley-ai-isnt-just-about-replacing-some-jobs-its-about-replacing-all-of-them) "For Silicon Valley, AI isn’t just about replacing some jobs. It’s about replacing all of them". The article explores Silicon Valley’s increasingly bold ambition to automate not just some, but all human labor through artificial intelligence and robotics. At a private dinner in San Francisco, a tech investor openly encouraged startup founders to pursue full automation, arguing that replacing workers means capturing their salaries. While this vision is often kept behind closed doors due to its controversial nature, some companies like Mechanize are now publicly embracing it, with backing from major tech figures. Influential voices such as Elon Musk, Bill Gates, and Geoffrey Hinton have echoed the belief that most jobs could soon disappear. Although current AI and robotics still have limitations—AI makes errors and robots lack full dexterity—rapid advancements suggest these barriers may not last long. Technologies like GPT-4 already outperform humans in certain tasks, and humanoid robots are being tested in factories and homes. The ultimate goal, as framed by Silicon Valley, is a world where AI handles cognitive tasks and robots perform physical labor, leaving humans with an uncertain role. While some elite professions may remain untouched, the vast majority of jobs are at risk. The motivations behind this push are debated: some argue it’s about improving global living standards, while others see it as a profit-driven attempt to control the entire means of production. Regardless of feasibility, the real question is not just whether this future will arrive, but why it’s being pursued—and what it means for the rest of society. <br> <br>
    
31. ***Simplifying LM agents with LCLMs:  <br>Researchers from Stanford et al. investigated the necessity of complex agent architectures, showing that for tasks like SWE-bench, simply providing the entire environment to a long context language model (LCLM) with proper prompting can achieve competitive results. A Gemini-1.5-Pro model without scaffolding performed comparably to complex agents, and the more capable Gemini-2.5-Pro with the same unscaffolded approach achieved a 50.8% solve rate.*** <br> <br>
    May 12, Stanford Uni, IBM and Uni of Toronto published a [paper](https://arxiv.org/pdf/2505.08120) “Putting It All into Context: Simplifying Agents with LCLMs”. Recent advances in language model (LM) agents have demonstrated significant potential for automating complex real-world tasks. To make progress on these difficult tasks, LM agent architectures have become increasingly complex, often incorporating multi-step retrieval tools, multiple agents, and scaffolding adapted to the underlying LM. This work investigates whether all of this complexity is necessary, or if parts of these scaffolds can be removed on challenging tasks like SWE-bench. The study shows that in the case of SWE-bench, simply putting the entire environment into the context of a long context language model (LCLM) and properly prompting the model makes it competitive with carefully tuned, complex agent scaffolds. The study shows that a Gemini-1.5-Pro model without any scaffolding or tools achieves 38% on SWE-Bench-Verified, comparable with approaches using carefully tuned agent scaffolds (32%). While the unscaffolded approach with Gemini-1.5-Pro falls short of the strongest agentic architectures, the study demonstrates that the more capable Gemini-2.5-Pro using the same unscaffolded approach directly attains a 50.8% solve rate. Additionally, a two-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a competitive 48.6% solve rate. <br> <br>

32. ***WSJ on Silicon Valley's AI midlife crisis:  <br>This article describes how major tech companies like Alphabet, Apple, and Facebook are navigating the disruptive potential of AI, facing challenges despite their current dominance. The piece highlights stock fluctuations, calls for investor patience, and the overarching uncertainty of how AI will reshape industries, drawing parallels to the "Innovator's Dilemma" and noting the rise of new AI competitors.*** <br> <br>
    May 10, WSJ published an [article](https://www.wsj.com/tech/ai/the-giants-of-silicon-valley-are-having-a-midlife-crisis-over-ai-74968a07) “The Giants of Silicon Valley Are Having a Midlife Crisis Over AI”. The article discusses how major Silicon Valley companies, often referred to as the "Magnificent Seven," are grappling with the challenges and opportunities presented by artificial intelligence (AI). These tech giants, including Alphabet, Apple, Facebook, and Tesla, are experiencing a midlife crisis as they navigate the disruptive potential of AI. Alphabet's stock recently dropped due to a decline in Google search traffic on Apple devices, while Apple is urging investors to be patient with its AI developments. Facebook's Mark Zuckerberg is promoting AI as a tool for social connection, and Elon Musk is trying to stabilize Tesla's stock with promises of driverless cars. Despite their current dominance and profitability, these companies face uncertainty about how AI will reshape their industries. The article draws parallels to Clayton Christensen's "The Innovator's Dilemma," highlighting how successful companies can be disrupted by new technologies. It also mentions the potential for AI to change the app marketplace and the emergence of new AI models from companies like DeepSeek. Venture capitalists like Sarah Guo see opportunities in investing in AI startups that could challenge the established players. Overall, the article underscores the tension between maintaining current success and adapting to future technological shifts. <br> <br>

33. ***Microsoft and Salesforce on LLM multi-turn conversation failures:  <br>This study found that top LLMs perform significantly worse (39% average drop) in multi-turn conversations compared to single-turn, fully-specified instructions. The degradation is attributed mainly to increased unreliability, as LLMs often make early assumptions, prematurely generate solutions, and fail to recover if they take a wrong conversational turn, effectively getting lost.*** <br> <br>
    May 9, Microsoft and Salesforce published a [paper](https://arxiv.org/pdf/2505.06120) “LLMs Get Lost In Multi-Turn Conversation”. Large Language Models (LLMs) are conversational interfaces. As such, LLMs have the potential to assist their users not only when they can fully specify the task at hand, but also to help them define, explore, and refine what they need through multi-turn conversational exchange. Although analysis of LLM conversation logs has confirmed that underspecification occurs frequently in user instructions, LLM evaluation has predominantly focused on the single-turn, fully-specified instruction setting. This study performs large-scale simulation experiments to compare LLM performance in single- and multi-turn settings. Experiments confirm that all the top open- and closed-weight LLMs tested exhibit significantly lower performance in multi-turn conversations than single-turn, with an average drop of 39% across six generation tasks. Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. The work finds that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, the study discovers that *when LLMs take a wrong turn in a conversation, they get lost and do not recover*. <br> <br>

35. ***Bloomberg.com on Klarna's AI job cut slowdown:  <br>Klarna's CEO, Sebastian Siemiatkowski, stated that the company's AI-driven cost-cutting in customer service went too far and announced a recruitment drive for human agents to ensure customers can always speak to a person. While still committed to AI and anticipating further workforce reduction through attrition and technology, this move reflects the need to balance automation with human interaction quality.*** <br> <br>
    May 8, Bloomberg.com published an [article](https://www.bloomberg.com/news/articles/2025-05-08/klarna-turns-from-ai-to-real-person-customer-service) “Klarna Slows AI-Driven Job Cuts With Call for Real People”. Klarna Group Plc's CEO, Sebastian Siemiatkowski, acknowledges that the company's aggressive cost-cutting measures in customer service, driven by AI advancements, have gone too far. To address this, Klarna is initiating a recruitment drive to ensure customers can always speak to a real person, highlighting the company's commitment to maintaining human interaction despite its AI focus. Siemiatkowski revealed plans for a new cohort of remote employees in an "Uber type of setup," aiming to replace outsourced human agents gradually. The pilot program has started with two agents, targeting candidates like students and rural populations. This move underscores the risks financial firms face when replacing humans with untested technology. Klarna, an early collaborator with OpenAI, initially embraced AI to reduce costs after its valuation dropped from $45.6 billion to $6.7 billion in 2022. Despite pausing IPO plans due to market volatility, Klarna remains committed to AI, aiming to launch a digital financial assistant. However, Siemiatkowski emphasizes the importance of human support quality, predicting a workforce reduction from 3,000 to 2,500 due to natural attrition and technological advancements. He anticipates further downsizing within 12 months as technology improves. <br> <br>

37. ***Sakana AI's Continuous Thought Machines:  <br>This paper introduces the Continuous Thought Machine (CTM), a model incorporating neuron-level temporal processing and neural synchronization as core representations, aiming to reintroduce neural timing from biological brains into deep learning. The CTM demonstrated strong performance and versatility across various tasks, showcasing rich internal representations, interpretability, and adaptive compute capabilities, representing a step towards more biologically plausible AI.*** <br> <br>
    May 8, Sakana AI, Uni of Tsukuba and IT Uni of Copenhagen published a [paper](https://arxiv.org/pdf/2505.05522) “Continuous Thought Machines”. Biological brains demonstrate complex neural activity, where the timing and interplay between neurons is critical to how brains process information. Most deep learning architectures simplify neural activity by abstracting away temporal dynamics. This study challenges that paradigm. By incorporating neuron-level processing and synchronization, the study can effectively reintroduce neural timing as a foundational element. The work presents the Continuous Thought Machine (CTM), a model designed to leverage neural dynamics as its core representation. The CTM has two core innovations: (1) neuron-level temporal processing, where each neuron uses unique weight parameters to process a history of incoming signals; and (2) neural synchronization employed as a latent representation. The CTM aims to strike a balance between oversimplified neuron abstractions that improve computational efficiency, and biological realism. It operates at a level of abstraction that effectively captures essential temporal dynamics while remaining computationally tractable for deep learning. The study demonstrates the CTM's strong performance and versatility across a range of challenging tasks, including ImageNet-1K classification, solving 2D mazes, sorting, parity computation, question-answering, and RL tasks. Beyond displaying rich internal representations and offering a natural avenue for interpretation owing to its internal process, the CTM is able to perform tasks that require complex sequential reasoning. The CTM can also leverage adaptive compute, where it can stop earlier for simpler tasks, or keep computing when faced with more challenging instances. The goal of this work is to share the CTM and its associated innovations, rather than pushing for new state-of-the-art results. To that end, the authors believe the CTM represents a significant step toward developing more biologically plausible and powerful artificial intelligence systems. <br> <br>

39. ***Uni of Washington's LlamaPIE proactive assistant:  <br>This study introduces LlamaPIE, a real-time proactive in-ear conversational assistant designed to enhance human conversations with discreet, concise guidance via hearables, operating in the background without explicit user invocation. Using a two-model pipeline (one to decide when to respond, one to generate the response) and a semi-synthetic dialogue dataset, LlamaPIE demonstrated effectiveness and strong user preference in real-world user studies.*** <br> <br>
    May 7, Uni of Washington published a [paper](https://arxiv.org/pdf/2505.04066) “LLAMAPIE: Proactive In-Ear Conversation Assistants”. The study introduces LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. The study addresses several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, the authors construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. The study evaluates the approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with the assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive model, highlighting the potential of LlamaPie to enhance live conversations.

 <br> <br> <br>

***May 11, 2025***

1. ***Mistral's new model release:   <br>Mistral introduced Mistral Medium 3, a model class aiming for state-of-the-art performance with significantly lower cost (8x less) and easier deployment to boost enterprise adoption. This model reportedly matches or exceeds Claude Sonnet 3.7 on benchmarks at a lower price, supports various deployment options including on-premises, and excels in coding and multimodal understanding.***  <br>  <br>
   May 7, Mistral [released](https://mistral.ai/news/mistral-medium-3) Mistral Medium 3, a new class of models that balances SOTA performance, 8X lower cost and simpler deployability to accelerate enterprise usage. All the way from Mistral 7B, the models have consistently demonstrated performance of significantly higher-weight and more expensive models. And today, Mistral announced Mistral Medium 3, pushing efficiency and usability of language models even further. The model leads in professional use cases such as coding and multimodal understanding, and delivers a range of enterprise capabilities including: Hybrid or on-premises / in-VPC deployment; Custom post-training; and Integration into enterprise tools and systems. Mistral Medium 3 delivers frontier performance while being an order of magnitude less expensive. For instance, the model performs at or above 90% of Claude Sonnet 3.7 on benchmarks across the board at a significantly lower cost ($0.4 input / $2 output per M token). In addition to can be deployed on any cloud, it can also run on self-hosted environments of four GPUs and above.  <br>  <br>

3. ***Improving RL for LLM reasoners:   <br>Researchers from Mila, Microsoft, and Google proposed RLV (Reinforcement Learning with Verifiers), a method to enhance existing "value-free" RL techniques for LLM fine-tuning by jointly training the LLM as both a reasoner and a generative verifier using RL-generated data. RLV significantly boosts MATH accuracy (over 20%), enables more efficient test-time compute scaling (8-32x), and shows strong generalization, outperforming base RL methods.***  <br>  <br>
   May 7, Mila, Microsoft and Google published a [paper](https://www.arxiv.org/pdf/2505.04842) “Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers”. Prevalent reinforcement learning (RL) methods for fine-tuning LLM reasoners, such as GRPO or Leave-one-out PPO, abandon the learned value function in favor of empirically estimated returns. This hinders test-time compute scaling that relies on using the value-function for verification. In this work, we propose RLV that augments any “value-free” RL method by jointly training the LLM as both a reasoner and a generative verifier using RL-generated data, adding verification capabilities without significant overhead. Empirically, RLV boosts MATH accuracy by over 20% with parallel sampling and enables 8−32× efficient test-time compute scaling compared to the base RL method. RLV also exhibits strong generalization capabilities for both easy-to-hard and out-of-domain tasks. Furthermore, RLV achieves 1.2−1.6× higher performance when jointly scaling parallel and sequential test-time compute with a long reasoning R1 model.  <br>  <br>

5. ***Exploring generalizable reasoning:   <br>Microsoft researchers investigated whether reasoning can generalize across modalities and domains, finding that general-domain text-based post-training can achieve this. Based on this, they introduced X-Reasoner, a vision-language model post-trained solely on general-domain text, which successfully transfers reasoning to multimodal and out-of-domain settings, outperforming SOTA models and further improving with domain-specific text-only continued training (as seen with X-Reasoner-Med).***  <br>  <br>
   May 6, Microsoft published a [paper](https://arxiv.org/pdf/2505.03981) “X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains”. Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? The findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, the study introduces X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, the work finds that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, the study introduces X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks. https://github.com/microsoft/x-reasoner  <br>  <br>

7. ***A framework for human-AI knowledge co-creation:   <br>Imperial College London introduced Cognitio Emergens (CE), a framework to understand the evolving epistemic partnerships between humans and AI in scientific knowledge creation, moving beyond static roles. CE integrates Agency Configurations (describing authority distribution), Epistemic Dimensions (specific collaborative capabilities), and Partnership Dynamics (forces shaping these relationships, including epistemic alienation risks) to foster co-evolutionary collaborations.***  <br>  <br>
   May 6, Imperial College London published a [paper](https://arxiv.org/pdf/2505.03105) “Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation”. Scientific knowledge creation is fundamentally transforming as humans and AI systems evolve beyond tool-user relationships into co-evolutionary epistemic partnerships. When AlphaFold revolutionized protein structure prediction, researchers described engaging with an epistemic partner that reshaped how they conceptualized fundamental relationships. This article introduces Cognitio Emergens (CE), a framework addressing critical limitations in existing models that focus on static roles or narrow metrics while failing to capture how scientific understanding emerges through recursive human-AI interaction over time. CE integrates three components addressing these limitations: Agency Configurations describing how authority distributes between humans and AI (Directed, Contributory, Partnership), with partnerships dynamically oscillating between configurations rather than following linear progression; Epistemic Dimensions capturing six specific capabilities emerging through collaboration across Discovery, Integration, and Projection axes, creating distinctive "capability signatures" that guide development; and Partnership Dynamics identifying forces shaping how these relationships evolve, particularly the risk of epistemic alienation where researchers lose interpretive control over knowledge they formally endorse. Drawing from autopoiesis theory, social systems theory, and organizational modularity, CE reveals how knowledge co-creation emerges through continuous negotiation of roles, values, and organizational structures. By reconceptualizing human-AI scientific collaboration as fundamentally co-evolutionary, CE offers a balanced perspective that neither uncritically celebrates nor unnecessarily fears AI's evolving role, instead providing conceptual tools for cultivating partnerships that maintain meaningful human participation while enabling transformative scientific breakthroughs.  <br>  <br>

9. ***Distinguishing unlearning from obfuscation:   <br>Researchers from the University of Cambridge and King’s College London formally differentiated LLM unlearning from mere obfuscation (knowledge addition), introducing a probing-based evaluation framework. They also proposed DF-MCQ, a novel unlearning method that flattens predictive distributions over MCQs, achieving high refusal rates (>90%) and significantly higher uncertainty on probing questions compared to obfuscation techniques.***  <br>  <br>
    May 5, Uni of Cambridge and King’s College London published a [paper](https://www.arxiv.org/pdf/2505.02884) “Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?”. Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. This study formally distinguishes unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, the study proposes DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour. Experimental results demonstrate that DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level uncertainty that is much higher than obfuscation on probing questions.  <br>  <br>

11. ***Advancing reward modeling as reasoning:   <br>Researchers from UICU et al. introduced Reasoning Reward Models (ReasRMs), a new class of generative reward models that formulate reward modeling as a reasoning task to enhance interpretability and performance. Their RM-R1 models, trained via a two-stage pipeline (distilling reasoning chains, RL with verifiable rewards), achieve SOTA or near-SOTA performance on benchmarks by self-generating reasoning traces or rubrics for evaluation.***  <br>  <br>
    May 5, UICU, UCSD Texas A&M Uni and Stevens Inst of Tech published a [paper](https://arxiv.org/pdf/2505.02387) “RM-R1: Reward Modeling as Reasoning”. Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, the study hypothesizes and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. This study introduces a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. The study proposes a reasoning-oriented training pipeline and trains a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, the models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, the work performs thorough empirical analysis to understand the key ingredients of successful ReasRM training. https://github.com/RM-R1-UIUC/RM-R1.  <br>  <br>

13. ***Teaching models to understand high-risk data without generating it:   <br>Researchers from USC and Allen AI introduced Selective Loss to Understand but Not Generate (SLUNG), a pre-training method enabling LLMs to comprehend high-risk content (e.g., toxic, copyrighted) without learning to produce it. SLUNG selectively avoids incentivizing the generation of high-risk tokens while forcing understanding by predicting subsequent low-risk tokens, demonstrably improving comprehension without increasing harmful generation.***  <br>  <br>
    May 5, Uni of Southern California and Allen Inst. for AI published a [paper](https://arxiv.org/pdf/2505.03052) “Teaching Models to Understand (but not Generate) High-risk Data”. Language model developers typically filter out high-risk content -- such as toxic or copyrighted text -- from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models' ability to recognize and appropriately respond to harmful or sensitive content. This work introduces Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model's context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through the experiments, the study shows that SLUNG consistently improves models' understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out. https://github.com/ryanyxw/llm-decouple  <br>  <br>

15. ***Evaluating LLM adherence to complex legal procedures:   <br>Yale Law School researchers tested leading LLMs on their ability to follow the intricate rules of The Bluebook legal citation system, using an original dataset of 866 tasks. Their findings showed that models achieved full compliance only 69%-74% of the time, with in-context learning on Bluebook rules improving accuracy to just 77%, cautioning against using off-the-shelf LLMs for tasks requiring high procedural fidelity.***  <br>  <br>
    May 5, Yale Law School published a [paper](https://arxiv.org/pdf/2505.02763) “Bye-bye, Bluebook Automating Legal Procedure with Large Language Models”. Legal practice requires careful adherence to procedural rules. In the United States, few are more complex than those found in The Bluebook: A Uniform System of Citation. Compliance with this system's 500+ pages of byzantine formatting instructions is the raison d'etre of thousands of student law review editors and the bete noire of lawyers everywhere. To evaluate whether large language models (LLMs) are able to adhere to the procedures of such a complicated system, we construct an original dataset of 866 Bluebook tasks and test flagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. The study shows (1) that these models produce fully compliant Bluebook citations only 69%-74% of the time and (2) that in-context learning on the Bluebook's underlying system of rules raises accuracy only to 77%. These results caution against using off-the-shelf LLMs to automate aspects of the law where fidelity to procedure is paramount.
  <br>  <br>
17. ***Rapid conversion of transformers to linear attention models:   <br>Recursal AI, EleutherAI, and others presented RADLADS, a protocol for quickly and cost-effectively converting standard softmax attention transformers into linear attention decoder models (RWKV-variants). This process uses minimal data (0.005% of original training) and low cost (e.g., <$2,000 for a 72B model) to produce models that retain close-to-original quality and achieve SOTA performance for linear attention models of their size.***  <br>  <br>
    May 5, Recursal AI, EleutherAI et al published a [paper](https://arxiv.org/pdf/2505.03005) “RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale”. The work presents Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. The conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to a 72B linear attention model costs less than $2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper  <br>  <br>

19. ***OpenAI's structural evolution plan:   <br>OpenAI announced that its for-profit LLC will transition to a Public Benefit Corporation (PBC), while remaining controlled by the founding nonprofit organization, which will also be a large shareholder. This change, made after consultations, aims to provide better resources for the nonprofit to support its mission of ensuring AGI benefits all of humanity, which remains the core mission for both entities.***  <br>  <br>
    May 5, OpenAI published a [blog](https://openai.com/index/evolving-our-structure/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=openai-reverses-for-profit-plans&_bhlid=4074aa24cf357b674bbad4cb1cd5208da696a568) “Evolving OpenAI’s structure”. The OpenAI Board has an updated plan for evolving OpenAI’s structure. OpenAI was founded as a nonprofit, and is today overseen and controlled by that nonprofit. Going forward, it will continue to be overseen and controlled by that nonprofit.  Its for-profit LLC, which has been under the nonprofit since 2019, will transition to a Public Benefit Corporation (PBC)–a purpose-driven company structure that has to consider the interests of both shareholders and the mission. The nonprofit will control and also be a large shareholder of the PBC, giving the nonprofit better resources to support many benefits.  The mission remains the same, and the PBC will have the same mission. OpenAI made the decision for the nonprofit to retain control of OpenAI after hearing from civic leaders and engaging in constructive dialogue with the offices of the Attorney General of Delaware and the Attorney General of California. The company thanks both offices and looking forward to continuing these important conversations to make sure OpenAI can continue to effectively pursue its mission of ensuring AGI benefits all of humanity.   <br>  <br>

21. ***Inducing agent evaluation metrics from open-ended feedback:   <br>Researchers proposed AutoLibra, a framework that transforms open-ended human feedback on agent behavior into concrete, evaluable metrics for fine-grained analysis. AutoLibra grounds feedback, clusters behaviors, and creates metrics with definitions and examples, using meta-metrics like "coverage" and "redundancy" to optimize the induced set, demonstrating its utility in improving agent performance through better prompt engineering and fine-tuning data selection.***  <br>  <br>
    May 5, Stanford Uni, Uni of Toronto, and Uni of Penn published a [paper](https://arxiv.org/pdf/2505.02820) “AutoLibra: Agent Metric Induction from Open-Ended Feedback”. Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. The work proposes AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., "If you find that the button is disabled, don't click it again", or "This agent has too much autonomy to decide what to do on its own", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. The study further proposes two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, the work experimentally demonstrates AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. The work also presents two applications of AutoLibra in agent improvement: First, the study shows that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, the study shows that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. The results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.  <br>  <br>

23. ***LLM-based text simplification for improved comprehension:   <br>Google researchers developed and validated an LLM capability for minimally lossy text simplification using a self-refinement approach, tested in a large randomized study (4563 participants, 31 texts). Results showed that participants reading simplified texts answered significantly more comprehension questions correctly (3.9% absolute increase overall, 14.6% for PubMed) and reported lower cognitive load, demonstrating LLMs' potential to enhance information accessibility.***  <br>  <br>
    May 4, Google published a [paper](https://arxiv.org/pdf/2505.01980) “LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load”. Information on the web, such as scientific publications and Wikipedia, often surpasses users' reading level. To help address this, the work used a self-refinement approach to develop a LLM capability for minimally lossy text simplification. To validate the approach, the work conducted a randomized study involving 4563 participants and 31 texts spanning 6 broad subject areas: PubMed (biomedical scientific articles), biology, law, finance, literature/philosophy, and aerospace/computer science. Participants were randomized to viewing original or simplified texts in a subject area, and answered multiple-choice questions (MCQs) that tested their comprehension of the text. The participants were also asked to provide qualitative feedback such as task difficulty. Results indicate that participants who read the simplified text answered more MCQs correctly than their counterparts who read the original text (3.9% absolute increase, p<0.05). This gain was most striking with PubMed (14.6%), while more moderate gains were observed for finance (5.5%), aerospace/computer science (3.8%) domains, and legal (3.5%). Notably, the results were robust to whether participants could refer back to the text while answering MCQs. The absolute accuracy decreased by up to ~9% for both original and simplified setups where participants could not refer back to the text, but the ~4% overall improvement persisted. Finally, participants' self-reported perceived ease based on a simplified NASA Task Load Index was greater for those who read the simplified text (absolute change on a 5-point scale 0.33, p<0.05). This randomized study, involving an order of magnitude more participants than prior works, demonstrates the potential of LLMs to make complex information easier to understand. The work aims to enable a broader audience to better learn and make use of expert knowledge available on the web, improving information accessibility.   <br>  <br>

25. ***The risk of human obsolescence from advanced AI:   <br>This article argues that the primary existential threat from AI may be its potential to become "better at everything" than humans, leading to gradual human irrelevance rather than a rogue AI takeover. As AI surpasses human capabilities in economic, cultural, and social roles, making human input optional, it could cause widespread job displacement and erode democratic structures, necessitating proactive societal steering to maintain human relevance.***  <br>  <br>
    May 4, Theguardian published an [article](https://www.theguardian.com/books/2025/may/04/the-big-idea-can-we-stop-ai-making-humans-obsolete) “Better at everything: how AI could make human beings irrelevant”. The article argues that the primary existential risk from artificial intelligence might not be a conscious rogue AI, but rather the subtle consequence of AI becoming "better at everything" humans do, leading to gradual human obsolescence. As AI and robotics continue to improve, they are on track to surpass human capabilities in most roles, from economic tasks like working and making decisions to cultural roles like art and even social roles as companions. This superiority will make AI cheaper, more reliable, and the preferred option for an increasing number of tasks, potentially rendering human input optional or unnecessary across many professions and aspects of life. The author suggests this could lead to job displacement, reduced human relevance in decision-making, and even a preference for AI companions over human interaction. This shift could extend to governments relying more on AI, potentially reducing their dependence on citizens and eroding democratic structures, akin to a "resource curse." Countering this is difficult because AI's efficiency and cost advantages will be compelling. To navigate this future, the author proposes steps like tracking AI's influence, regulating advanced AI development, using AI to empower human organization, and developing "ecosystem alignment" to proactively steer society towards a future where humans remain relevant as beneficiaries and stewards of AI, rather than becoming obsolete.  <br>  <br>

27. ***Understanding massive values in LLM self-attention:   <br>Researchers from Rutgers, CMU, et al. demonstrated that concentrated massive values consistently appear in the query (Q) and key (K) representations of self-attention modules in modern LLMs, but not in values (V). These massive values, primarily caused by Rotary Positional Encoding (RoPE), are shown to be critical for interpreting contextual knowledge rather than retrieving parametric knowledge, with implications for quantization and model design.***  <br>  <br>
    May 3, Rutgers Uni, CMU et al published a [paper](https://arxiv.org/pdf/2502.01563) “Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding”. Large language models (LLMs) have achieved remarkable success in contextual knowledge understanding. The work shows that these concentrated massive values consistently emerge in specific regions of attention queries (Q) and keys (K) while not having such patterns in values (V) in various modern transformer-based LLMs (Q, K, and V mean the representations output by the query, key, and value layers respectively). Through extensive experiments, the study further demonstrates that these massive values play a critical role in interpreting contextual knowledge (knowledge obtained from the current context window) rather than in retrieving parametric knowledge stored within the model's parameters. Further investigation of quantization strategies reveals that ignoring these massive values leads to a pronounced drop in performance on tasks requiring rich contextual understanding, aligning with the analysis. Finally, the work traces the emergence of concentrated massive values and find that such concentration is caused by Rotary Positional Encoding (RoPE), which has appeared since the first layers. These findings shed new light on how Q and K operate in LLMs and offer practical insights for model design and optimization. https://github.com/MingyuJ666/Rope_with_LLM  <br>  <br>

29. ***Nvidia's Llama-Nemotron reasoning models:   <br>Nvidia introduced the Llama-Nemotron series (Nano-8B, Super-49B, Ultra-253B), an open family of efficient reasoning models competitive with SOTA models like DeepSeek-R1 but offering better inference throughput. Trained using neural architecture search, knowledge distillation, continued pretraining, and reasoning-focused post-training (SFT & RL), these models feature a dynamic reasoning toggle and are released with their dataset and training codebases.***  <br>  <br>
    May 2, Nvidia published a [paper](https://arxiv.org/abs/2505.00949) “Llama-Nemotron: Efficient Reasoning Models”. The report introduces the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, Nvidia discusses the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development: 1. Nvidia releases the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. Nvidia releases the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. Nvidia also releases the training codebases: NeMo, NeMo-Aligner, and Megatron-LM.  <br>  <br>

31. ***Introducing Canon layers for enhanced LLM reasoning:   <br>Meta researchers, using controlled synthetic pretraining tasks, discovered "Canon layers" – lightweight architectural components promoting horizontal information flow across neighboring tokens. These layers, seamlessly integrated into various sequence models, were shown to significantly enhance reasoning depth (e.g., by 2x), breadth, and knowledge manipulation, transforming weaker architectures to match stronger ones in both synthetic and real-world pretraining.***  <br>  <br>
    May 2, Meta published a [paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5240330) “Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers”. Understanding architectural differences between large language models (LLMs) remains challenging, particularly at academic-scale pretraining (e.g., 1.3B parameters on 100B tokens), where results are often dominated by noise and randomness. To overcome this, the work introduces controlled, synthetic pretraining tasks to isolate and evaluate key model capabilities. Leveraging this framework, the study discovers Canon layers: lightweight architectural components—named after the musical term “canon”—that promote horizontal information flow across neighboring tokens. Canon layers compute weighted combinations of nearby token representations and integrate seamlessly into Transformers, linear attention, state-space architectures, or any general sequence model. The work presents 12 key results. This includes how Canon layers enhance reasoning depth (e.g., by 2x), reasoning breadth, knowledge manipulation, etc. Remarkably, Canon layers transform weak architectures like NoPE to match RoPE, linear attention to match state-space models (like Mamba2), as validated both in the synthetic playground and real-world academic-scale pretraining. Leveraging infinitely high-quality data, the authors hope the framework can predict how future architectures may evolve as training pipelines improve—e.g., through better data curation or RL-based post-training—unlocking deeper reasoning and hierarchical inference capabilities.  <br>  <br>

33. ***Automated failure attribution in LLM multi-agent systems:   <br>Researchers from Penn State et al. introduced the new research area of automated failure attribution for LLM multi-agent systems, aiming to identify the responsible agent and step in task failures. They created the Who&When dataset of annotated failure logs and evaluated automated methods, finding that current SOTA models struggle significantly (best method: 53.5% agent accuracy, 14.2% step accuracy), highlighting the task's complexity.***  <br>  <br>
    Apr 30, Penn State Uni, Duke Uni, Meta, Google et al published a [paper](https://arxiv.org/pdf/2505.00212) “Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems”. Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. The study proposes and formulates a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, the work introduces the Who&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&When, the work develops and evaluates three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. https://github.com/mingyin1/Agents_Failure_Attribution  <br>  <br>

35. ***A unified framework for agentic reasoning and tool integration:   <br>Microsoft introduced ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a framework combining agentic reasoning, reinforcement learning, and tool integration for LLMs. ARTIST enables models to autonomously decide on tool use within reasoning chains, learning robust strategies via outcome-based RL, and demonstrated significant improvements (up to 22% absolute) over baselines on complex reasoning and function calling benchmarks.***  <br>  <br>
    Apr 28, Microsoft published a [paper](https://www.arxiv.org/pdf/2505.01441) “Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning”. Large language models (LLMs) have achieved remarkable progress in complex reasoning tasks, yet they remain fundamentally limited by their reliance on static internal knowledge and text-only reasoning. Real-world problem solving often demands dynamic, multi-step reasoning, adaptive decision making, and the ability to interact with external tools and environments. This work introduces ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a unified framework that tightly couples agentic reasoning, reinforcement learning, and tool integration for LLMs. ARTIST enables models to autonomously decide when, how, and which tools to invoke within multi-turn reasoning chains, leveraging outcome-based RL to learn robust strategies for tool use and environment interaction without requiring step-level supervision. Extensive experiments on mathematical reasoning and multi-turn function calling benchmarks show that ARTIST consistently outperforms state-of-the-art baselines, with up to 22% absolute improvement over base models and strong gains on the most challenging tasks. Detailed studies and metric analyses reveal that agentic RL training leads to deeper reasoning, more effective tool use, and higher-quality solutions. The results establish agentic RL with tool integration as a powerful new frontier for robust, interpretable, and generalizable problem-solving in LLMs.

  <br>  <br>  <br>
***May 4, 2025***

1. ***A study on LLM generalization:  <br>Google and Stanford researchers explored why LLMs generalize differently from in-context learning versus fine-tuning, noting fine-tuning often fails on simple reversals or deductions where in-context learning succeeds. Using controlled datasets, they found in-context learning generally offers more flexible generalization and demonstrated that adding in-context inferences to fine-tuning data significantly improves generalization performance.*** <br> <br>
   May 1, Google and Stanford Uni published a [paper](https://arxiv.org/pdf/2505.00661) “On the generalization of language models from in-context learning and finetuning a controlled study”. Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning -- from failing to generalize to simple reversals of relations they are trained on, to missing logical deductions that can be made from trained information. These failures to generalize from fine-tuning can hinder practical application of these models. However, language models' in-context learning shows different inductive biases, and can generalize better in some of these cases. Here, the study explores these differences in generalization between in-context- and fine-tuning-based learning. To do so, the work constructed several novel datasets to evaluate and improve models' ability to generalize from finetuning data. The datasets are constructed to isolate the knowledge in the dataset from that in pretraining, to create clean tests of generalization. The work exposes pretrained large models to controlled subsets of the information in these datasets -- either in context, or through fine-tuning -- and evaluate their performance on test sets that require various types of generalization. The study finds overall that in data-matched settings, in-context learning can generalize more flexibly than fine-tuning (though the authors also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). The study builds on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context inferences to finetuning data. The study shows that this method improves generalization across various splits of the datasets and other benchmarks. The results have implications for understanding the inductive biases of different modes of learning in language models, and practically improving their performance. <br> <br>

3. ***Alibaba's Qwen 3 launch:  <br>Chinese tech giant Alibaba introduced Qwen 3, an enhanced version of its primary AI model featuring new hybrid reasoning capabilities. This release comes amidst increased competition in China's AI sector, following recent model launches by rivals like DeepSeek and Baidu, aiming to provide a more adaptable platform for developers.*** <br> <br>
   May 1, Reuters published an [article](https://www.reuters.com/business/media-telecom/alibaba-unveils-advanced-qwen-3-ai-chinese-tech-rivalry-intensifies-2025-04-29/#:~:text=April%2029%20(Reuters)%20%2D%20Chinese,introduces%20new%20hybrid%20reasoning%20capabilities.) “Alibaba unveils advanced Qwen 3 AI as Chinese tech rivalry intensifies”. Chinese tech giant Alibaba Group (9988.HK), opens new tab launched Qwen 3 on Tuesday, an upgraded version of its flagship artificial intelligence model that introduces new hybrid reasoning capabilities. The launch comes as competition in China's AI sector intensifies, spurred by the breakout success of local startup DeepSeek earlier this year, which claimed to have built high-performing models at lower costs than their Western counterparts. Chinese search leader Baidu, opens new tab joined the AI arms race last Friday with the release of its Ernie 4.5 Turbo and reasoning-focused Ernie X1 Turbo models. Alibaba's newest release merges conventional AI functions with advanced dynamic reasoning, creating what the company calls a more adaptable and efficient platform for app and software developers. The e-commerce giant had previously rushed out its Qwen 2.5-Max model in late January, just days after DeepSeek's announcement, claiming superior performance <br> <br>

5. ***Improving LLM agents via self-generated examples:  <br>Stanford researchers investigated improving LLM agents for sequential decision-making without manual knowledge engineering, instead focusing on automatically learning from self-generated successful trajectories. They demonstrated that accumulating these experiences boosts performance significantly across benchmarks, and further gains are achieved through database-level and exemplar-level selection, reaching performance comparable to more complex, engineered methods.*** <br> <br>
   May 1, Stanford Uni published a [paper](https://arxiv.org/pdf/2505.00234) “Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks”. Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, the study investigates how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, the study focuses on constructing and refining a database of self-generated examples. The work demonstrates that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. The study then introduces two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Results demonstrate that automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering. <br> <br>

7. ***Amazon's Nova model family release:  <br>Amazon detailed its Nova Premier model, described as its most capable multimodal foundation model capable of processing text, images, and video within a one-million token context window. Nova Premier also functions as a teacher model for distilling customized, efficient variants of smaller Nova models (Pro, Lite, Micro), all while emphasizing integrated safety and responsible AI practices.*** <br> <br>
   Apr 30, Amazon release it Nova family model with tech [report](https://assets.amazon.science/f6/c5/79dceb124593b3356566ad6723af/the-amazon-nova-premier-technical-report-and-model-card.pdf). The report presents Amazon Nova Premier, its most capable multimodal foundation model and teacher for model distillation. Nova Premier processes text, images, and videos with a one-million token context window enabling analysis of large codebases, long documents, and long videos in a single prompt. It also enables customers to use Amazon Bedrock to create customized variants of Amazon Nova Pro, Nova Lite, and Nova Micro that maintain high accuracy while offering improved speed and cost efficiency. Like all Nova models, Nova Premier is built with integrated safety measures and responsible AI practices, maintaining our commitment to customer trust, security, and reliability. With Nova Premier, Amazon further extends the capabilities and price-performance advantages of the Amazon Nova model family. <br> <br>

9. ***Microsoft's Phi 4 small models release:  <br>Microsoft introduced its Phi-4 family of small language models (SLMs), including Phi-4-reasoning, reasoning-plus, and mini-reasoning, designed to bring complex reasoning capabilities to efficient AI. Leveraging techniques like inference scaling, distillation, RL, and quality data, these models, particularly the 14B parameter Phi-4-reasoning, achieve performance competitive with much larger models on benchmarks, with a strong emphasis on responsible AI principles and safety measures.*** <br> <br>
    Apr 30, Microsoft [released](https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/) its Phi 4 small models. Microsoft has introduced new small language models (SLMs) called Phi-4-reasoning, Phi-4-reasoning-plus, and Phi-4-mini-reasoning, marking a significant advancement in efficient AI. These models excel in complex reasoning tasks, typically requiring large models, by leveraging inference-time scaling, distillation, reinforcement learning, and high-quality data. Phi-4-reasoning, with 14 billion parameters, rivals larger models in performance, while Phi-4-reasoning-plus enhances accuracy using more tokens. Both models outperform OpenAI o1-mini and DeepSeek-R1-Distill-Llama-70B in benchmarks, including mathematical reasoning and Ph.D.-level science questions. Phi-4-mini-reasoning is optimized for environments with limited computing resources, making it ideal for educational applications and mobile systems. These models are integrated into Windows 11 devices and Copilot+ PCs, offering efficient and powerful AI capabilities. Microsoft emphasizes responsible AI development, adhering to principles of accountability, transparency, fairness, reliability, safety, privacy, and inclusiveness. The Phi models undergo rigorous safety post-training using supervised fine-tuning, direct preference optimization, and reinforcement learning from human feedback to ensure they perform tasks effectively while minimizing risks. These advancements demonstrate Microsoft's commitment to pushing the boundaries of AI while maintaining a focus on ethical and responsible development. <br> <br>

11. ***Research on 1-shot RL for LLM reasoning:  <br>Researchers demonstrated the surprising effectiveness of reinforcement learning with verifiable reward using just one training example (1-shot RLVR) for enhancing LLM math reasoning. Applying this to models like Qwen2.5-Math-1.5B, a single example yielded dramatic performance gains (e.g., 36% to 73.6% on MATH500), matching results from much larger training sets and showing robustness across models and algorithms, while also highlighting the role of policy gradient and exploration.*** <br> <br>
    Apr 29, Uni of Washington, USC, Microsoft et al. published a [paper](https://arxiv.org/pdf/2504.20571) “Reinforcement Learning for Reasoning in Large Language Models with One Training Example”. The study shows that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, the authors identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, the work identifies some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon which is termed as post-saturation generalization. Moreover, the study verifies that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the "grokking" phenomenon. The study also shows the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, the work observes that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. https://github.com/ypwang61/One-Shot-RLVR <br> <br>

13. ***An unauthorized AI persuasion experiment on Reddit:  <br>University of Zurich researchers conducted a controversial experiment on Reddit's r/ChangeMyView, deploying AI bots to post thousands of comments over four months without user consent to test persuasive capabilities, finding personalized AI most effective. The study faced strong ethical criticism from moderators for non-consensual methods and data harvesting, leading the researchers to ultimately forgo publication despite university defense of its relevance.*** <br> <br>
    Apr 29, [Theregister.com](https://www.theregister.com/2025/04/29/swiss_boffins_admit_to_secretly/) published an article “Swiss boffins admit to secretly posting AI-penned posts to Reddit in the name of science”. Researchers at the University of Zurich conducted an unauthorized experiment on the Reddit community r/ChangeMyView (CMV) to test the persuasive power of AI bots. Over four months, these bots posted 1,783 comments without users' knowledge, aiming to change opinions. Success was measured by "Deltas" awarded by users for persuasive arguments, with AI bots receiving 137 Deltas. The study tested three AI types: generic, community-aligned, and personalized, with the personalized AI, which tailored arguments based on users' inferred attributes, achieving the highest success rate of 18%. The bots often used fake identities and personal stories, sometimes adopting controversial or extreme positions. Moderators criticized the experiment for ethical violations, including non-consensual use of AI and data harvesting. Despite a formal warning from the university's ethics committee, which acknowledged rule violations but deemed the risks minimal, the University defended the study's social relevance. Moderators demanded an apology and non-publication of the study, fearing it could encourage further unethical experiments. The researchers emphasized the importance of their findings for understanding AI-driven manipulation and called for platform safeguards. Ultimately, the team decided not to publish the results, believing they had raised sufficient awareness of AI's manipulative capabilities. <br> <br>

15. ***Critique of Chatbot Arena benchmarks:  <br>Researchers identified systemic issues distorting the influential Chatbot Arena leaderboard, termed the "Leaderboard Illusion." They found undisclosed private testing allows select providers to bias scores through selective disclosure, and that proprietary models receive disproportionately more sampling and data access compared to open models, leading to overfitting and an unfair playing field; actionable reforms are recommended.*** <br> <br>
    Apr 29, Cohere, Princeton Uni, Stanford Uni, MIT et al published a [paper](https://arxiv.org/pdf/2504.20879) “The Leaderboard Illusion”. Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, this study identifies systematic issues that have resulted in a distorted playing field. The study finds that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. The study establishes that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, the work identifies 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. The study also establishes that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. The study shows that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on the conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. The study offers actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field. <br> <br>

17. ***Developing a retriever for reasoning tasks:  <br>Meta et al. presented ReasonIR-8B, the first information retriever specifically trained for general reasoning tasks, addressing limitations of existing retrievers. By using a novel synthetic data pipeline that generates challenging queries and hard negatives, ReasonIR-8B achieved state-of-the-art performance on the reasoning-focused BRIGHT benchmark and significantly boosted RAG performance on MMLU and GPQA compared to baselines and other retrievers.*** <br> <br>
    Apr 29, Meta, Uni of Washington, Stanford Uni, MIT et al published a [paper](https://arxiv.org/pdf/2504.20595) “ReasonIR: Training Retrievers for Reasoning Tasks”. The study presents ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. The study develops a synthetic data generation pipeline that, for each document, the pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of the synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. The training recipe is general and can be easily extended to future LLMs. https://github.com/facebookresearch/ReasonIR <br> <br>

19. ***Introducing the Softpick attention mechanism:  <br>Researchers from MBZUAI introduced softpick, a rectified, non-sum-to-one alternative to the standard softmax function in transformer attention mechanisms. Experiments show softpick eliminates attention sink and large activations, maintains performance parity, generates sparser attention maps with lower activation kurtosis, and notably outperforms softmax under quantization, suggesting benefits for model efficiency and interpretability.*** <br> <br>
    Apr 29, MBZUAI published a [paper](https://arxiv.org/pdf/2504.20966) “Softpick: No Attention Sink, No Massive Activations with Rectified Softmax”. The study introduces softpick, a rectified, not sum-to-one, drop-in replacement for softmax in transformer attention mechanisms that eliminates attention sink and massive activations. Experiments with 340M parameter models demonstrate that softpick maintains performance parity with softmax on standard benchmarks while achieving 0% sink rate. The softpick transformer produces hidden states with significantly lower kurtosis (340 vs 33,510) and creates sparse attention maps (46.97% sparsity). Models using softpick consistently outperform softmax when quantized, with particularly pronounced advantages at lower bit precisions. The analysis and discussion shows how softpick has the potential to open new possibilities for quantization, low-precision training, sparsity optimization, pruning, and interpretability. https://github.com/zaydzuhri/softpick-attention. <br> <br>

21. ***Investigating LLM activation monitoring techniques:  <br>OpenAI compared methods for monitoring LLM internal activations to detect unsafe behavior, evaluating linear probing, prompted probing (using task prompts at test time), and sparse autoencoder (SAE)-based probing against zero-shot prompting. They found activation probing significantly outperforms zero-shot prompting given sufficient data, recommending prompted probing for its data efficiency when inference compute is available, and SAE-based methods when compute is limited.*** <br> <br>
    Apr 28, OpenAI published a [paper](https://arxiv.org/pdf/2504.20271) “Investigating task-specific prompts and sparse autoencoders for activation monitoring”. Language models can behave in unexpected and unsafe ways, and so it is valuable to monitor their outputs. Internal activations of language models encode additional information that could be useful for this. The baseline approach for activation monitoring is some variation of linear probing on a particular layer: starting from a labeled dataset, train a logistic regression classifier on that layer's activations. Recent work has proposed several approaches which may improve on naive linear probing, by leveraging additional computation. One class of techniques, which is called "prompted probing," leverages test time computation to improve monitoring by (1) prompting the model with a description of the monitoring task, and (2) applying a learned linear probe to resulting activations. Another class of techniques uses computation at train time: training sparse autoencoders offline to identify an interpretable basis for the activations, and e.g. max-pooling activations across tokens using that basis before applying a linear probe. However, one can also prompt the model with a description of the monitoring task and use its output directly. The study develops and test novel refinements of these methods and compare them against each other. The work finds asking the model zero-shot is a reasonable baseline when inference-time compute is not limited; however, activation probing methods can substantially outperform this baseline given sufficient training data. Specifically, the study recommends prompted probing when inference-time compute is available, due to its superior data efficiency and good generalization performance. Alternatively, if inference-time compute is limited, the work finds SAE-based probing methods outperform raw activation probing. <br> <br>

23. ***A scalable long-term memory system for AI agents:  <br>The Mem0 paper introduces a memory-centric architecture designed to give LLMs scalable long-term memory, overcoming fixed context window limits for multi-session dialogues by dynamically managing salient information. Comprehensive evaluations showed Mem0 and its graph-based variant significantly outperform various baselines, including RAG and full-context methods, in conversational consistency and accuracy metrics, while dramatically reducing computational overhead like latency and token costs.*** <br> <br>
    Apr 28, mem0 published a [paper](https://arxiv.org/pdf/2504.19413) “Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory”. Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. The study introduces Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, the study further proposes an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on LOCOMO benchmark, the work systematically compares the approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and k-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results show that the methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26% relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around 2% higher overall score than the base configuration. Beyond accuracy gains, the study also markedly reduces computational overhead compared to full-context method. In particular, Mem0 attains a 91% lower p95 latency and saves more than 90% token cost, offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. The findings highlight critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents. https://github.com/mem0ai/mem0/tree/main/evaluation <br> <br>

25. ***Anthropic's analysis of AI in software development:  <br>Based on analyzing 500,000 coding interactions with its Claude models, Anthropic identified key trends in AI's impact on software development. Findings show that more agentic AI (Claude Code) drives higher automation, AI is frequently used for user-facing web applications suggesting potential disruption in those roles, and startups are adopting these tools much faster than enterprises, highlighting a potential competitive shift.*** <br> <br>
    Apr 28, Anthropic published an [article](https://www.anthropic.com/research/impact-software-development) “Anthropic Economic Index: AI’s Impact on Software Development”. The article analyzes the impact of AI, specifically Anthropic's Claude and its specialized Claude Code agent, on computer programming jobs, a small but influential sector seeing dramatic changes due to AI assistance. Based on an analysis of 500,000 coding-related interactions, three key patterns were identified. Firstly, the more agentic Claude Code is used significantly more for task automation (79% of conversations) compared to the standard Claude.ai (49%), suggesting that specialized AI agents will likely increase automation levels. Secondly, developers commonly leverage AI for building user-facing applications, with web development languages like JavaScript and HTML being prevalent, indicating that jobs focused on creating simple interfaces and apps may face earlier disruption. Lastly, startups are the primary early adopters of Claude Code (identified in 33% of conversations), significantly ahead of enterprises (13%), suggesting a potential competitive advantage gap for nimbler organizations. These findings highlight a shift towards more automated workflows in coding, particularly in front-end development, and position the tech sector as a potential leading indicator for how AI might transform other occupations in the future. <br> <br>

27. ***Demis Hassabis's AGI timeline prediction:  <br> <br>Google DeepMind CEO Demis Hassabis forecasted that Artificial General Intelligence (AGI) will achieve human-level competence within five to ten years, potentially leading to significant workforce changes including job displacement as capable AI systems emerge. While acknowledging current AI limitations, he anticipates rapid advancements, a view that contrasts with some other tech leaders' more conservative timelines but aligns with a broader expectation of AI's growing role in labor.*** <br> <br>
    Apr 27, Time published an [article]](https://time.com/7280740/demis-hassabis-interview/) “Google DeepMind CEO Demis Hassabis on AI in the Military and What AGI Could Mean for Humanity”. Google DeepMind CEO Demis Hassabis predicts that artificial general intelligence (AGI) will reach human-level competence within the next five to ten years. This development could significantly impact the workforce, as AI systems begin to exhibit the complex capabilities of humans. Hassabis noted that while current AI systems are impressive, they still lack many human abilities, but advancements in the coming years will bridge this gap. This shift is expected to transform workplaces, with digital colleagues working alongside humans, potentially leading to job displacement. Other tech leaders, like Baidu's Robin Li, have more conservative timelines, suggesting AGI might take over a decade to develop. However, the consensus among many CEOs is that AI will soon play a crucial role in the workforce, with some predicting significant reductions in human labor to accommodate AI. This transition raises concerns about job security, as AI-driven systems can perform tasks faster and without the need for salaries or benefits. Despite these concerns, proponents argue that AI will optimize professional lives rather than replace human workers entirely. <br> <br>

29. ***The LawFlow dataset for legal reasoning:  <br>Researchers from the University of Minnesota introduced LawFlow, a novel dataset capturing complete, dynamic, and iterative legal workflows from law students performing real-world tasks, aiming to overcome the limitations of existing datasets focused on isolated subtasks. Comparing human and LLM workflows revealed humans are more modular and adaptive while LLMs are more sequential and exhaustive, suggesting AI is better suited for supportive roles and informing design principles for collaborative legal AI systems.*** <br> <br>
    Apr 26, Uni of Minnesota published a [paper](https://arxiv.org/pdf/2504.18942) “LawFlow : Collecting and Simulating Lawyers' Thought Processes”. Legal practitioners, particularly those early in their careers, face complex, high-stakes tasks that require adaptive, context-sensitive reasoning. While AI holds promise in supporting legal work, current datasets and models are narrowly focused on isolated subtasks and fail to capture the end-to-end decision-making required in real-world practice. To address this gap, the study introduces LawFlow, a dataset of complete end-to-end legal workflows collected from trained law students, grounded in real-world business entity formation scenarios. Unlike prior datasets focused on input-output pairs or linear chains of thought, LawFlow captures dynamic, modular, and iterative reasoning processes that reflect the ambiguity, revision, and client-adaptive strategies of legal practice. Using LawFlow, the study compares human and LLM-generated workflows, revealing systematic differences in structure, reasoning flexibility, and plan execution. Human workflows tend to be modular and adaptive, while LLM workflows are more sequential, exhaustive, and less sensitive to downstream implications. The findings also suggest that legal professionals prefer AI to carry out supportive roles, such as brainstorming, identifying blind spots, and surfacing alternatives, rather than executing complex workflows end-to-end. Building on these findings, the study proposes a set of design suggestions, rooted in empirical observations, that align AI assistance with human goals of clarity, completeness, creativity, and efficiency, through hybrid planning, adaptive execution, and decision-point support. The results highlight both the current limitations of LLMs in supporting complex legal workflows and opportunities for developing more collaborative, reasoning-aware legal AI systems. All data and code are available on project page (https://minnesotanlp.github.io/LawFlow-website/). <br> <br>

31. ***Analyzing scaling laws for AI oversight:  <br>MIT researchers investigated the scaling properties of scalable oversight (weaker AI supervising stronger AI), proposing a framework to quantify success probability based on capability mismatches modeled using Elo scores. After validating with games and deriving scaling laws for specific oversight scenarios, they analyzed Nested Scalable Oversight (NSO), finding theoretically and numerically that success rates decrease significantly (e.g., below 52% for a 400 Elo gap) as the capability difference grows.*** <br> <br>
    Apr 25, MIT published a [paper](https://arxiv.org/pdf/2504.18530) “Scaling Laws For Scalable Oversight”. Scalable oversight, the process by which weaker AI systems supervise stronger ones, has been proposed as a key strategy to control future superintelligent systems. However, it is still unclear how scalable oversight itself scales. To address this gap, the study proposes a framework that quantifies the probability of successful oversight as a function of the capabilities of the overseer and the system being overseen. Specifically, the framework models oversight as a game between capability-mismatched players; the players have oversight-specific and deception-specific Elo scores that are a piecewise-linear function of their general intelligence, with two plateaus corresponding to task incompetence and task saturation. The work validates the framework with a modified version of the game Nim and then apply it to four oversight games: "Mafia", "Debate", "Backdoor Code" and "Wargames". For each game, the study finds scaling laws that approximate how domain performance depends on general AI system capability (using Chatbot Arena Elo as a proxy for general capability). The work then builds on the findings in a theoretical study of Nested Scalable Oversight (NSO), a process in which trusted models oversee untrusted stronger models, which then become the trusted models in the next step. The study identifies conditions under which NSO succeeds and derive numerically (and in some cases analytically) the optimal number of oversight levels to maximize the probability of oversight success. In the numerical examples, the NSO success rate is below 52% when overseeing systems that are 400 Elo points stronger than the baseline overseer, and it declines further for overseeing even stronger systems. <br> <br>

33. ***An argument against LLM progress toward AGI:  <br>This article contends that despite advances, LLMs represent no meaningful progress towards Artificial General Intelligence, arguing they rely on sophisticated statistical pattern matching that merely simulates reasoning, often fabricating explanations for their outputs rather than reflecting true understanding. It asserts LLMs cannot determine objective truth, struggle with novelty, are inefficient compared to true intelligence, and are thus fundamentally limited mimics unsuitable for genuine automation or innovation.*** <br> <br>
    Apr 22, MindPrison published an [article](https://www.mindprison.cc/p/no-progress-toward-agi-llm-braindead-unreliable) “We Have Made No Progress Toward AGI”. Despite advances in Large Language Models (LLMs), we have made no meaningful progress toward true Artificial General Intelligence. Research from Anthropic reveals that LLMs don't actually reason—they employ complex statistical pattern matching that merely simulates intelligence. When LLMs explain their reasoning, these explanations are completely fabricated and don't reflect their internal processes. For example, when solving a simple math problem like "36+59=95," LLMs use memorized patterns and heuristics rather than algorithmic reasoning, yet falsely claim they "carried the one" when explaining their process. This pattern extends to their use of tools, where newer models increasingly hallucinate actions they never performed. LLMs fundamentally can't determine what is objectively right or wrong; they can only predict what is statistically likely based on training data. They excel at mimicking existing patterns but struggle with novel tasks outside their training distribution. Unlike true intelligence, which operates efficiently, LLMs require ever-increasing data and energy to improve marginally. While useful for certain applications, LLMs will never achieve true reasoning or create new semantic information. Their architecture makes them inherently unreliable for full automation or groundbreaking innovation, as they remain sophisticated pattern matchers rather than understanding systems. <br> <br>

35. ***Extending long-thought reasoning to perception:  <br>Researchers introduced the LongPerceptualThoughts dataset and a novel three-stage synthesis framework to generate long chain-of-thought reasoning traces for perceptual tasks, aiming to transfer benefits seen in math/code. Training a model on this dataset demonstrated notable improvements across vision benchmarks (+3.4 avg, +11.8 on V* Bench) and even enhanced text reasoning performance, suggesting value in distilling System-2 reasoning for System-1 perception tasks.*** <br> <br>
    Apr 21, Uni of Toronto and Nvidia published a [paper](https://arxiv.org/pdf/2504.15362) “LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception”. Recent reasoning models through test-time scaling have demonstrated that long chain-of-thoughts can unlock substantial performance boosts in hard reasoning tasks such as math and code. However, the benefit of such long thoughts for system-2 reasoning is relatively less explored in other domains such as perceptual tasks where shallower, system-1 reasoning seems sufficient. This study introduces LongPerceptualThoughts, a new synthetic dataset with 30K long-thought traces for perceptual tasks. The key challenges in synthesizing elaborate reasoning thoughts for perceptual tasks are that off-the-shelf models are not yet equipped with such thinking behavior and that it is not straightforward to build a reliable process verifier for perceptual tasks. Thus, the study proposes a novel three-stage data synthesis framework that first synthesizes verifiable multiple-choice questions from dense image descriptions, then extracts simple CoTs from VLMs for those verifiable problems, and finally expands those simple thoughts to elaborate long thoughts via frontier reasoning models. In controlled experiments with a strong instruction-tuned 7B model, the work demonstrates notable improvements over existing visual reasoning data-generation methods. The model, trained on the generated dataset, achieves an average +3.4 points improvement over 5 vision-centric benchmarks, including +11.8 points on V∗ Bench. Notably, despite being tuned for vision tasks, it also improves performance on the text reasoning benchmark, MMLU-Pro, by +2 points. https://andrewliao11.github.io/LongPerceptualThoughts/

 <br> <br> <br>

***Apr 27, 2025***

1. ***A joint study on sparse attention:  <br>This investigation explored its trade-offs for long-context LLMs. Findings include that larger, sparser models can outperform smaller, denser ones for very long sequences, achievable sparsity varies by task phase and model size, no single sparse strategy excels universally, and moderate sparsity can harm performance, indicating it's a useful but context-dependent tool requiring careful evaluation.*** <br> <br>
   Apr 24, Meta, Cohere and Uni of Edinburgh published a [paper](https://arxiv.org/pdf/2504.17768) “The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs”. Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, the study performs a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on experiments, the work reports a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) the work introduces and validate novel scaling laws specifically tailored for sparse attention, providing evidence that the findings are likely to hold true beyond the range of experiments. Through these insights, the study demonstrates that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications. <br> <br>

3. ***Nvidia's AIMO-2 winning paper:  <br>This work presented a three-pillar strategy for building state-of-the-art mathematical reasoning models. The strategy involves creating a large-scale dataset (OpenMathReasoning), integrating code execution via iterative training and quality filtering, and developing a generative solution selection (GenSelect) method that significantly improves upon baseline voting.*** <br> <br>
   Apr 24, Nvidia published a [paper](https://arxiv.org/pdf/2504.16891) “AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset”. This paper presents a winning submission to the AI Mathematical Olympiad - Progress Prize 2 (AIMO-2) competition. The recipe for building state-of-the-art mathematical reasoning models relies on three key pillars. First, the work creates a large-scale dataset comprising 540K unique high-quality math problems, including olympiad-level problems, and their 3.2M long-reasoning solutions. Second, the work develops a novel method to integrate code execution with long reasoning models through iterative training, generation, and quality filtering, resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, the work creates a pipeline to train models to select the most promising solution from many candidates. The study shows that such generative solution selection (GenSelect) can significantly improve upon majority voting baseline. Combining these ideas, the study trains a series of models that achieve state-of-the-art results on mathematical reasoning benchmarks. https://github.com/NVIDIA/NeMo-Skills <br> <br>

5. **A paper introducing the I-Con framework:  <br>Researchers from MIT, Google, and Microsoft presented a single information-theoretic equation based on minimizing integrated KL divergence. This framework unifies diverse representation learning loss functions (clustering, contrastive, supervised learning, etc.), connects over 23 approaches, and leads to state-of-the-art unsupervised image classifiers and principled debiasing methods.*** <br> <br>
   Apr 23, MIT, Google and Microsoft published a [paper](https://arxiv.org/pdf/2504.16929) “I-Con: A Unifying Framework for Representation Learning”. As the field of representation learning grows, there has been a proliferation of different loss functions to solve different classes of problems. The study introduces a single information-theoretic equation that generalizes a large collection of modern loss functions in machine learning. In particular, the study introduces a framework that shows that several broad classes of machine learning methods are precisely minimizing an integrated KL divergence between two conditional distributions: the supervisory and learned representations. This viewpoint exposes a hidden information geometry underlying clustering, spectral methods, dimensionality reduction, contrastive learning, and supervised learning. This framework enables the development of new loss functions by combining successful techniques from across the literature. The study not only presents a wide array of proofs, connecting over 23 different approaches, but also leverages these theoretical results to create state-of-the-art unsupervised image classifiers that achieve a +8% improvement over the prior state-of-the-art on unsupervised classification on ImageNet-1K. The work also demonstrates that I-Con can be used to derive principled debiasing methods which improve contrastive representation learners. <br> <br>

7. ***Research on Generalized Neighborhood Attention (GNA):  <br>This work from Georgia Tech, Nvidia, and UIUC addresses the challenge of achieving speedups with sparse attention. It introduces GNA to unify local attention types, develops a realistic performance simulator, and implements GNA efficiently for Nvidia Blackwell architecture, achieving near-theoretical speedups and significant (28-46%) end-to-end gains in generative models without fine-tuning.*** <br> <br>
   Apr 23, Georgia Tch, Nvidia  and UIUC published a [paper](https://arxiv.org/pdf/2504.16922) “Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light”. Many sparse attention mechanisms such as Neighborhood Attention have typically failed to consistently deliver speedup over the self attention baseline. This is largely due to the level of complexity in attention infrastructure, and the rapid evolution of AI hardware architecture. At the same time, many state-of-the-art foundational models, particularly in computer vision, are heavily bound by attention, and need reliable sparsity to escape the O(n^2) complexity. This paper studies a class of promising sparse attention mechanisms that focus on locality, and aim to develop a better analytical model of their performance improvements. The study first introduces Generalized Neighborhood Attention (GNA), which can describe sliding window, strided sliding window, and blocked attention. The study then considers possible design choices in implementing these approaches, and create a simulator that can provide much more realistic speedup upper bounds for any given setting. Finally, the study implements GNA on top of a state-of-the-art fused multi-headed attention (FMHA) kernel designed for the NVIDIA Blackwell architecture in CUTLASS. The implementation can fully realize the maximum speedup theoretically possible in many perfectly block-sparse cases, and achieves an effective utilization of 1.3 petaFLOPs/second in FP16. In addition, the study plugs various GNA configurations into off-the-shelf generative models, such as Cosmos-7B, HunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end speedup on B200 without any fine-tuning. <br> <br>

9. ***A Google study on LLM decision-making:  <br>This work investigated sub-optimal performance in LLMs, identifying failure modes like greediness, frequency bias, and the knowing-doing gap. It proposed mitigating these issues via RL fine-tuning on self-generated CoT rationales; experiments demonstrated this approach enhances exploration and narrows the knowing-doing gap, with further analysis on effective exploration mechanisms.*** <br> <br>
    Apr 22, Google published a [paper](https://arxiv.org/pdf/2504.16078) “LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities”. The success of Large Language Models (LLMs) has sparked interest in various agentic applications. A key hypothesis is that LLMs, leveraging common sense and Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently solve complex domains. However, LLM agents have been found to suffer from sub-optimal exploration and the knowing-doing gap, the inability to effectively act on knowledge present in the model. This work systematically studies why LLMs perform sub-optimally in decision-making scenarios. In particular, the study closely examines three prevalent failure modes: greediness, frequency bias, and the knowing-doing gap. The work proposes mitigation of these shortcomings by fine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales. Experiments across multi-armed bandits, contextual bandits, and Tic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making abilities of LLMs by increasing exploration and narrowing the knowing-doing gap. Finally, the authors study both classic exploration mechanisms, such as epsilon-greedy, and LLM-specific approaches, such as self-correction and self-consistency, to enable more effective fine-tuning of LLMs for decision-making. <br> <br>

11. ***The TTRL paper:  <br>Researchers from Tsinghua Uni and SAIL introduced Test-Time Reinforcement Learning (TTRL). This method trains LLMs using RL on unlabeled data during inference by leveraging reward signals from Test-Time Scaling techniques like majority voting, demonstrating significant performance improvements (e.g., +159% on AIME 2024 for Qwen-2.5-Math-7B) and surpassing initial model limits without ground-truth labels.*** <br> <br>
    Apr 22, Tsinghua Uni and SAIL published a [paper](https://arxiv.org/pdf/2504.16084) “TTRL: Test-Time Reinforcement Learning”. This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, the study finds that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. This work introduces Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL <br> <br>

13. ***Introducing Tina models:  <br>The University of Southern California demonstrated achieving strong reasoning cost-effectively by applying parameter-efficient LoRA updates during RL fine-tuning to a tiny 1.5B base model. This minimalist approach yields performance competitive with SOTA models (e.g., 43.33% Pass@1 on AIME24) at a minuscule post-training cost (estimated $9, a 260x reduction), suggesting LoRA efficiently adapts reasoning structure while preserving knowledge.*** <br> <br>
    Apr 22, Uni of Southern California published a [paper](https://arxiv.org/pdf/2504.15777) “Tina: Tiny Reasoning Models via LoRA”. How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, the study presents Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20% reasoning performance increase and 43.33% Pass@1 accuracy on AIME24, at only $9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). The work reveals the surprising effectiveness of efficient RL reasoning via LoRA. The work validates this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, the study hypothesizes that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model’s underlying knowledge. https://github.com/shangshang-wang/Tina <br> <br>

15. ***A study on LLM creative limits:  <br>Researchers from Google and CMU utilized minimal algorithmic tasks abstracting open-ended challenges (like discovering connections or constructing patterns). Their work demonstrates that next-token prediction is myopic, while multi-token approaches excel in diversity; it also finds input-layer noise injection superior to output-layer sampling for randomness, providing a testbed and arguing for methods beyond standard generation.*** <br> <br>
    Apr 21, Google and CMU published a [paper](https://arxiv.org/pdf/2504.15266) “Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction”. The study designs a suite of minimal algorithmic tasks that are a loose abstraction of open-ended real-world tasks. This allows to cleanly and controllably quantify the creative limits of the present-day language model. Much like real-world tasks that require a creative, far-sighted leap of thought, the tasks require an implicit, open-ended stochastic planning step that either (a) discovers new connections in an abstract knowledge graph (like in wordplay, drawing analogies, or research) or (b) constructs new patterns (like in designing math problems or new proteins). In these tasks, the study empirically and conceptually argues how next-token learning is myopic and memorizes excessively; comparatively, multi-token approaches, namely teacherless training and diffusion models, excel in producing diverse and original output. Secondly, in the tasks, the work finds that to elicit randomness from the Transformer without hurting coherence, it is better to inject noise right at the input layer (via a method we dub hash-conditioning) rather than defer to temperature sampling from the output layer. Thus, the work offers a principled, minimal test-bed for analyzing open-ended creative skills, and offers new arguments for going beyond next-token learning and softmax-based sampling. https://github.com/chenwu98/algorithmic-creativity <br> <br>

17. ***Introducing Causal-Copilot:  <br>UCSD presented an autonomous agent addressing the complexity and inaccessibility of causal analysis. Causal-Copilot automates the full pipeline (discovery, inference, interpretation, etc.) within an LLM framework for tabular and time-series data, supports natural language interaction, integrates over 20 techniques, and aims to bridge the gap between domain experts and causal researchers while outperforming baselines.*** <br> <br>
    Apr 21, UCSD published a [paper](https://arxiv.org/pdf/2504.13263) “Causal-Copilot: An Autonomous Causal Analysis Agent”. Causal analysis plays a foundational role in scientific discovery and reliable decision-making, yet it remains largely inaccessible to domain experts due to its conceptual and algorithmic complexity. This disconnect between causal methodology and practical usability presents a dual challenge: domain experts are unable to leverage recent advances in causal learning, while causal researchers lack broad, real-world deployment to test and refine their methods. To address this, the study introduces Causal-Copilot, an autonomous agent that operationalizes expert-level causal analysis within a large language model framework. Causal-Copilot automates the full pipeline of causal analysis for both tabular and time-series data -- including causal discovery, causal inference, algorithm selection, hyperparameter optimization, result interpretation, and generation of actionable insights. It supports interactive refinement through natural language, lowering the barrier for non-specialists while preserving methodological rigor. By integrating over 20 state-of-the-art causal analysis techniques, the system fosters a virtuous cycle -- expanding access to advanced causal methods for domain experts while generating rich, real-world applications that inform and advance causal theory. Empirical evaluations demonstrate that Causal-Copilot achieves superior performance compared to existing baselines, offering a reliable, scalable, and extensible solution that bridges the gap between theoretical sophistication and real-world applicability in causal analysis. A live interactive demo of Causal-Copilot is available at https://causalcopilot.com/. <br> <br>

19. ***Presenting UFO2:  <br>Microsoft et al. introduced a multi-agent AgentOS for Windows aiming to make Computer-Using Agents practical system-level tools. UFO2 overcomes limitations like shallow OS integration and fragile interaction by featuring a coordinating HostAgent and specialized AppAgents with native APIs, hybrid UI detection, efficient planning, and a Picture-in-Picture interface, demonstrating improved robustness and accuracy across many Windows apps.*** <br> <br>
    Apr 20, Microsoft et al published a [paper](https://arxiv.org/pdf/2504.14603) “UFO2: The Desktop AgentOS”. Recent Computer-Using Agents (CUAs), powered by multimodal large language models (LLMs), offer a promising direction for automating complex desktop workflows through natural language. However, most existing CUAs remain conceptual prototypes, hindered by shallow OS integration, fragile screenshot-based interaction, and disruptive execution. The study presents UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs into practical, system-level automation. UFO2 features a centralized HostAgent for task decomposition and coordination, alongside a collection of application-specialized AppAgent equipped with native APIs, domain-specific knowledge, and a unified GUI--API action layer. This architecture enables robust task execution while preserving modularity and extensibility. A hybrid control detection pipeline fuses Windows UI Automation (UIA) with vision-based parsing to support diverse interface styles. Runtime efficiency is further enhanced through speculative multi-action planning, reducing per-step LLM overhead. Finally, a Picture-in-Picture (PiP) interface enables automation within an isolated virtual desktop, allowing agents and users to operate concurrently without interference. The study evaluates UFO2 across over 20 real-world Windows applications, demonstrating substantial improvements in robustness and execution accuracy over prior CUAs. Results show that deep OS integration unlocks a scalable path toward reliable, user-aligned desktop automation. https://github.com/microsoft/UFO/ <br> <br>

21. ***Introducing the PROMPTEVALS dataset:  <br>UC Berkeley and LangChain addressed the challenge of creating reliability assertions for production LLM pipelines. They provide a large collection (2087 prompts, 12623 criteria, 5x larger than previous) sourced from developers; benchmarking showed fine-tuned Mistral/Llama 3 models outperformed GPT-4o by ~21% in generating relevant assertions, offering a resource to advance research in LLM reliability and alignment.*** <br> <br>
    Apr 20, UC Berkeley and LangChain published a [paper](https://arxiv.org/pdf/2504.14738) “PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines”. Large language models (LLMs) are increasingly deployed in specialized production data processing pipelines across diverse domains -- such as finance, marketing, and e-commerce. However, when running them in production across many inputs, they often fail to follow instructions or meet developer expectations. To improve reliability in these applications, creating assertions or guardrails for LLM outputs to run alongside the pipelines is essential. Yet, determining the right set of assertions that capture developer requirements for a task is challenging. This study introduces PROMPTEVALS, a dataset of 2087 LLM pipeline prompts with 12623 corresponding assertion criteria, sourced from developers using the open-source LLM pipeline tools. This dataset is 5x larger than previous collections. Using a hold-out test split of PROMPTEVALS as a benchmark, the work evaluated closed- and open-source models in generating relevant assertions. Notably, the fine-tuned Mistral and Llama 3 models outperform GPT-4o by 20.93% on average, offering both reduced latency and improved performance. The authors believe the dataset can spur further research in LLM reliability, alignment, and prompt engineering. https://huggingface.co/datasets/reyavir/PromptEvals <br> <br>

23. ***Proposing Attribution with Attention (AT2):  <br>MIT addressed the high cost and unreliability of using attention weights for token attribution. Their method treats weights from different heads as features and learns an effective combination (using ablation signals), achieving performance comparable to expensive ablation methods much more efficiently, and demonstrating utility in context pruning for QA.*** <br> <br>
    Apr 18, MIT published a [paper](https://arxiv.org/pdf/2504.13752) “Learning to Attribute with Attention”. Given a sequence of tokens generated by a language model, people may want to identify the preceding tokens that influence the model to generate this sequence. Performing such token attribution is expensive; a common approach is to ablate preceding tokens and directly measure their effects. To reduce the cost of token attribution, the study revisits attention weights as a heuristic for how a language model uses previous tokens. Naive approaches to attribute model behavior with attention (e.g., averaging attention weights across attention heads to estimate a token's influence) have been found to be unreliable. To attain faithful attributions, the work proposes treating the attention weights of different attention heads as features. This way, the study can learn how to effectively leverage attention weights for attribution (using signal from ablations). The resulting method, Attribution with Attention (AT2), reliably performs on par with approaches that involve many ablations, while being significantly more efficient. To showcase the utility of AT2, the study uses it to prune less important parts of a provided context in a question answering setting, improving answer quality. Code for AT2 https://github.com/MadryLab/AT2 <br> <br>

25. ***Stanford's Cost-of-Pass framework:  <br>This approach proposes evaluating language models based on economic value. It involves calculating the "cost-of-pass" (expected cost for a correct solution) and tracking the minimum achievable "frontier cost-of-pass"; analysis reveals different model types are optimal for different tasks, significant cost reductions over time are driven by model innovations, and inference-time techniques often provide marginal cost-benefit compared to better models.*** <br> <br>
    Apr 17, Stanford Uni published a [paper](https://arxiv.org/pdf/2504.13359) “Cost-of-Pass: An Economic Framework for Evaluating Language Models”. The widespread adoption of AI systems in the economy hinges on their ability to generate economic value that outweighs their inference costs. Evaluating this tradeoff requires metrics that account for both performance and costs. The study proposes a framework grounded in production theory for evaluating language models by combining accuracy and inference cost. The study introduces "cost-of-pass", the expected monetary cost of generating a correct solution. The work then defines the "frontier cost-of-pass" as the minimum cost-of-pass achievable across available models or the "human-expert, using the approximate cost of hiring an expert. The analysis reveals distinct economic insights. First, lightweight models are most cost-effective for basic quantitative tasks, large models for knowledge-intensive ones, and reasoning models for complex quantitative problems, despite higher per-token costs. Second, tracking this frontier cost-of-pass over the past year reveals significant progress, particularly for complex quantitative tasks where the cost has roughly halved every few months. Third, to trace key innovations driving this progress, the study examines counterfactual frontiers: estimates of cost-efficiency without specific model classes. The study finds that innovations in lightweight, large, and reasoning models have been essential for pushing the frontier in basic quantitative, knowledge-intensive, and complex quantitative tasks, respectively. Finally, the research assesses the cost-reductions afforded by common inference-time techniques like majority voting and self-refinement, finding that their marginal accuracy gains rarely justify their costs. The findings underscore that complementary model-level innovations are the primary drivers of cost-efficiency, and the economic framework provides a principled tool for measuring this progress and guiding deployment. <br> <br>

27. ***The ToolRL paper:  <br>Researchers from UIUC addressed the challenge of teaching LLMs tool use via reinforcement learning, focusing on reward design. Arguing that supervised fine-tuning lacks generalization and coarse RL rewards are insufficient, they conducted a systematic study and proposed a principled reward design for tool tasks, implemented with GRPO, achieving robust training and significant performance gains (17% over base, 15% over SFT models).*** <br> <br>
    Apr 16, UIUC published a [paper](https://arxiv.org/pdf/2504.13958) “ToolRL: Reward is All Tool Learning Needs”. Current Large Language Models (LLMs) often undergo supervised fine-tuning (SFT) to acquire tool use capabilities. However, SFT struggles to generalize to unfamiliar or complex tool use scenarios. Recent advancements in reinforcement learning (RL), particularly with R1-like models, have demonstrated promising reasoning and generalization abilities. Yet, reward design for tool use presents unique challenges: multiple tools may be invoked with diverse parameters, and coarse-grained reward signals, such as answer matching, fail to offer the finegrained feedback required for effective learning. This study presents the first comprehensive study on reward design for tool selection and application tasks within the RL paradigm. The study systematically explores a wide range of reward strategies, analyzing their types, scales, granularity, and temporal dynamics. Building on these insights, the study proposes a principled reward design tailored for tool use tasks and apply it to train LLMs using Group Relative Policy Optimization (GRPO). Empirical evaluations across diverse benchmarks demonstrate that the approach yields robust, scalable, and stable training, achieving a 17% improvement over base models and a 15% gain over SFT models. These results highlight the critical role of thoughtful reward design in enhancing the tool use capabilities and generalization performance of LLMs. https://github.com/qiancheng0/ToolRL <br> <br>

29. ***Exploring multilingual LLM reasoning:  <br>Researchers from Nanjing Uni, SAIL, and CMU investigated the potential beyond the typical "English bias." They found evidence that reasoning across multiple languages offers a significantly (nearly 10 Acc@k points) and robustly higher performance upper bound than English-only approaches, though current answer selection methods are insufficient to reach it.*** <br> <br>
    Apr 16, Nanjing Uni, SAIL and CMU published a [paper](https://arxiv.org/pdf/2504.11833) “Could Thinking Multilingually Empower LLM Reasoning?”. Previous work indicates that large language models exhibit a significant "English bias", i.e. they often perform better when tasks are presented in English. Interestingly, the study has observed that using certain other languages in reasoning tasks can yield better performance than English. However, this phenomenon remains under-explored. This study explores the upper bound of harnessing multilingualism in reasoning tasks, suggesting that multilingual reasoning promises significantly (by nearly 10 Acc@k points) and robustly (tolerance for variations in translation quality and language choice) higher upper bounds than English-only reasoning. Besides analyzing the reason behind the upper bound and challenges in reaching it, the study also finds that common answer selection methods cannot achieve this upper bound, due to their limitations and biases. These insights could pave the way for future research aimed at fully harnessing the potential of multilingual reasoning in LLMs. <br> <br>

31. ***Introducing NodeRAG:  <br>Researchers from Columbia Uni et al. addressed limitations in graph-based RAG. They propose a graph-centric framework introducing heterogeneous graph structures to enable seamless integration of graph methods into the RAG workflow, aligning with LLM capabilities for efficiency. Experiments demonstrate NodeRAG's advantages over prior methods in efficiency (indexing/query time, storage) and superior question-answering performance.*** <br> <br>
    Apr 15, Columbia Uni, Uni of Penn. And Lehigh Uni published a [paper](https://arxiv.org/pdf/2504.11544) “NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes”. Retrieval-augmented generation (RAG) empowers large language models to access external and private corpus, enabling factually consistent responses in specific domains. By exploiting the inherent structure of the corpus, graph-based RAG methods further enrich this process by building a knowledge graph index and leveraging the structural nature of graphs. However, current graph-based RAG approaches seldom prioritize the design of graph structures. Inadequately designed graph not only impede the seamless integration of diverse graph algorithms but also result in workflow inconsistencies and degraded performance. To further unleash the potential of graph for RAG, the work proposes NodeRAG, a graph-centric framework introducing heterogeneous graph structures that enable the seamless and holistic integration of graph-based methodologies into the RAG workflow. By aligning closely with the capabilities of LLMs, this framework ensures a fully cohesive and efficient end-to-end process. Through extensive experiments, the study demonstrates that NodeRAG exhibits performance advantages over previous methods, including GraphRAG and LightRAG, not only in indexing time, query time, and storage efficiency but also in delivering superior question-answering performance on multi-hop benchmarks and open-ended head-to-head evaluations with minimal retrieval tokens. https://github.com/Terry-Xu-666/NodeRAG <br> <br>

33. ***AA Communications of the ACM article:  <br>This piece discusses how GenAI presents existential challenges but also convergence opportunities for computer science and humanities. By automating routine tasks, democratizing access, and enabling new global comparative research (an "AI turn"), GenAI can free scholars for deeper inquiry and create a new intellectual terrain where computational and humanistic approaches partner to revitalize both fields.*** <br> <br>
    Apr 10, Communications of the ACM published an [article](https://cacm.acm.org/blogcacm/the-converging-paths-of-computer-science-and-the-humanities-in-the-age-of-genai/) “The Converging Paths of Computer Science and the Humanities in the Age of GenAI”. In 2025, both computer science and the humanities face existential crises due to the rise of GenAI, questioning their purpose and relevance. While humanities programs struggle with declining enrollment due to uncertain career prospects, computer science grapples with GenAI's potential to automate core intellectual tasks like coding. However, these challenges present an opportunity for convergence, with GenAI potentially revitalizing both fields by addressing core limitations. By automating routine language-intensive tasks such as translating historical texts or debugging code, GenAI can alleviate cognitive burdens and enable scholars to focus on deeper intellectual exploration. This democratization of access could attract diverse populations previously deterred by technical or linguistic requirements, potentially creating more inclusive academic communities. GenAI also transforms research by breaking down traditional linguistic and periodization barriers, enabling global comparative studies that were previously impossible. This "AI turn" redefines knowledge creation, challenging the boundaries between disciplines and creating a new intellectual terrain where computational methods and humanistic inquiry can become partners. It allows for reimagining the boundaries between disciplines, creating a new intellectual terrain where computational methods and humanistic inquiry can become partners. By optimizing research processes and improving efficiency, AI empowers researchers to focus on solving complex problems and constructing meaningful explanations, ultimately advancing our understanding of both technological systems and human experience, and revitalizing the core essence of both fields. <br> <br>

35. ***Research on reflection in pre-training:  <br>Essential AI demonstrated that a language model's ability to reflect on and correct its own reasoning emerges significantly earlier than commonly thought, developing steadily during pre-training. Experiments involving deliberate errors showed self-correction capabilities in models like OLMo2-7B trained on 4 trillion tokens, indicating this ability starts forming before RL fine-tuning.***  <br>  <br>
    Apr 5, Essential AI published a [paper](https://arxiv.org/pdf/2504.04022) “Rethinking Reflection in Pre-Training”. A language model's ability to reflect on its own reasoning provides a key advantage for solving complex problems. While most recent research has focused on how this ability develops during reinforcement learning, the study shows that it actually begins to emerge much earlier - during the model's pre-training. To study this, the work introduces deliberate errors into chains-of-thought and test whether the model can still arrive at the correct answer by recognizing and correcting these mistakes. By tracking performance across different stages of pre-training, the study observes that this self-correcting ability appears early and improves steadily over time. For instance, an OLMo2-7B model pre-trained on 4 trillion tokens displays self-correction on the six self-reflection tasks.
  <br>  <br>  <br>


***Apr 20, 2025***

1. ***Startup Aims for Full Automation:   <br>Tamay Besiroglu's startup, Mechanize, seeks to automate all work using AI agents, causing controversy and attracting investment. The goal is full automation of all work and economy and replacing human workers in all sectors. While the AI can promote economic growth, the critics are concerned about job losses, the AI models still have limitations.***  <br>  <br>
   Apr 19, TechCrunch published an [article](https://techcrunch.com/2025/04/19/famed-ai-researcher-launches-controversial-startup-to-replace-all-human-workers-everywhere/) “Famed AI researcher launches controversial startup to replace all human workers everywhere”. Tamay Besiroglu, a prominent AI researcher, has launched Mechanize, a startup with the ambitious goal of fully automating all work and the economy. This announcement has stirred controversy, with critics arguing it damages the reputation of his respected research institute, Epoch. Mechanize aims to replace human workers with AI agents, initially targeting white-collar jobs. Besiroglu argues that this could lead to explosive economic growth and higher living standards, though critics are concerned about job loss and income inequality. He calculates the market potential to be enormous, with global wages totaling around $60 trillion annually. Despite the backlash, Mechanize has attracted significant investment from notable figures in the tech industry. Besiroglu acknowledges the current limitations of AI agents, such as reliability and task execution, but believes that overcoming these challenges will lead to economic abundance. He also suggests that even in an AI-dominated economy, human wages could increase due to complementary roles that AI cannot perform. Mechanize is actively hiring, indicating its commitment to advancing this vision despite the controversy.  <br>  <br>

3. ***New Dataset and Multi-Agent System for RAG:   <br>A new paper introduces RAMDocs, a dataset for retrieval-augmented generation (RAG) with conflicting evidence, and MADAM-RAG, a multi-agent system that uses LLMs to debate answers and discard misinformation. The existing RAG models performed poorly in RAMDocs and there still exist significant gaps for the current models to solve.***  <br>  <br>
   Apr 17, Uni of North Carolina at Chapel Hill published a [paper](https://arxiv.org/pdf/2504.13079) “Retrieval-Augmented Generation with Conflicting Evidence”. Large language model (LLM) agents are increasingly employing retrieval-augmented generation (RAG) to improve the factuality of their responses. However, in practice, these systems often need to handle ambiguous user queries and potentially conflicting information from multiple sources while also suppressing inaccurate information from noisy or irrelevant documents. Prior work has generally studied and addressed these challenges in isolation, considering only one aspect at a time, such as handling ambiguity or robustness to noise and misinformation. The authors instead consider multiple factors simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and Misinformation in Documents), a new dataset that simulates complex and realistic scenarios for conflicting evidence for a user query, including ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent approach in which LLM agents debate over the merits of an answer over multiple rounds, allowing an aggregator to collate responses corresponding to disambiguated entities while discarding misinformation and noise, thereby handling diverse sources of conflict jointly. The study demonstrates the effectiveness of MADAM-RAG using both closed and open-source models on AmbigDocs -- which requires presenting all valid answers for ambiguous queries -- improving over strong RAG baselines by up to 11.40% and on FaithEval -- which requires suppressing misinformation -- where to improve by up to 15.80% (absolute) with Llama3.3-70B-Instruct. Furthermore, the study finds that RAMDocs poses a challenge for existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match score). While MADAM-RAG begins to address these conflicting factors, the analysis indicates that a substantial gap remains especially when increasing the level of imbalance in supporting evidence and misinformation.  <br>  <br>

5. ***Attentional Bias as Foundation for Neural Architectures:   <br>Google's paper reinterprets neural architectures as associative memory modules that use attentional bias, exploring alternative attentional bias configurations and retention regularization techniques. It then presents Miras, a general framework to design deep learning architectures based on four choices. Experiments show different design choices in Miras yield models with varying strengths for language modeling, commonsense reasoning, and recall intensive tasks***  <br>  <br>
   Apr 17, Google published a [paper](https://arxiv.org/pdf/2504.13173) “It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization”. Designing efficient and effective architectural backbones has been in the core of research efforts to enhance the capability of foundation models. Inspired by the human cognitive phenomenon of attentional bias-the natural tendency to prioritize certain events or stimuli-we reconceptualize neural architectures, including Transformers, Titans, and modern linear recurrent neural networks as associative memory modules that learn a mapping of keys and values using an internal objective, referred to as attentional bias. Surprisingly, the study observed that most existing sequence models leverage either (1) dot-product similarity, or (2) L2 regression objectives as their attentional bias. Going beyond these objectives, the study presents a set of alternative attentional bias configurations along with their effective approximations to stabilize their training procedure. The authers then reinterpret forgetting mechanisms in modern deep learning architectures as a form of retention regularization, providing a novel set of forget gates for sequence models. Building upon these insights, the study present Miras, a general framework to design deep learning architectures based on four choices of: (i) associative memory architecture, (ii) attentional bias objective, (iii) retention gate, and (iv) memory learning algorithm. The work presents three novel sequence models-Moneta, Yaad, and Memora-that go beyond the power of existing linear RNNs while maintaining a fast parallelizable training process. Experiments show different design choices in Miras yield models with varying strengths. For example, certain instances of Miras achieve exceptional performance in special tasks such as language modeling, commonsense reasoning, and recall intensive tasks, even outperforming Transformers and other modern linear recurrent models.  <br>  <br>

7. ***Offline "Thinking" Reduces Test-Time Compute:   <br>A recent study introduces "sleep-time compute," which allows models to pre-compute useful quantities offline to reduce computation requirements at test-time. It can reduce the amount of test-time compute needed to achieve the same accuracy by ~ 5x and that by scaling sleep-time compute the study can further increase accuracy by up to 13% on Stateful GSM-Symbolic and 18% on Stateful AIME. It also conducts additional analysis to understand when sleep-time compute is most effective***  <br>  <br>
   Apr 17, Letta and UC Berkeley published a [paper](https://arxiv.org/pdf/2504.13171) “Sleep-time Compute: Beyond Inference Scaling at Test-time”. Scaling test-time compute has emerged as a key ingredient for enabling large language models (LLMs) to solve difficult problems, but comes with high latency and inference cost. The study introduces sleep-time compute, which allows models to "think" offline about contexts before queries are presented: by anticipating what queries users might ask and pre-computing useful quantities, the study can significantly reduce the compute requirements at test-time. To demonstrate the efficacy of the method, the work creates modified versions of two reasoning tasks - Stateful GSM-Symbolic and Stateful AIME. The study finds that sleep-time compute can reduce the amount of test-time compute needed to achieve the same accuracy by ~ 5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time compute the study can further increase accuracy by up to 13% on Stateful GSM-Symbolic and 18% on Stateful AIME. Furthermore, the work introduces Multi-Query GSM-Symbolic, which extends GSM-Symbolic by including multiple related queries per context. By amortizing sleep-time compute across related queries about the same context using Multi-Query GSM-Symbolic, the study can decrease the average cost per query by 2.5x. The work then conducts additional analysis to understand when sleep-time compute is most effective, finding the predictability of the user query to be well correlated with the efficacy of sleep-time compute. Finally, the study conducts a case-study of applying sleep-time compute to a realistic agentic SWE task. https://github.com/letta-ai/sleep-time-compute
  <br>  <br>
9. ***Models for Detecting AI-Generated Content: Traversaal.ai et al. published a paper that introduces a new set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. The work also introduces a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages***
    Apr 16, Traversaal.ai, Vantager, Cohere, et al published a [paper](https://arxiv.org/pdf/2504.11952) “Robust and Fine-Grained Detection of AI Generated Texts”. An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence the study focused more over partial cases i.e human-LLM co-authored texts. The paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. The work also introduces a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. The study also presents findings of the models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.

11. ***Reinforcement Learning for Diffusion LLM Reasoning:   <br>UCLA and Meta's paper proposes d1, a framework that adapts pre-trained masked diffusion large language models (dLLMs) into reasoning models using supervised finetuning and a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO. The work finds that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM.***  <br>  <br>
    Apr 16, UCLA and Meta published a [paper](https://arxiv.org/pdf/2504.12216) “d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning”. Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, the study proposes d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, the work develops and extends techniques to improve reasoning in pretrained dLLMs: (a) utilizing a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) introducing a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO. Through empirical studies, the study investigates the performance of different post-training recipes on multiple mathematical and logical reasoning benchmarks. The study finds that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM. https://dllm-reasoning.github.io/  <br>  <br>

13. ***Analysis of LLM Reasoning Capabilities After SFT:   <br>UC Berkeley and Allen Inst for AI released a paper which analyses model performance on the AIME24 dataset to understand how reasoning capabilities evolve. It discovers a ladder-like structure in problem difficulty, and progresses from Easy to Medium tier requires adopting an R1 reasoning style with minimal SFT, while Hard-level questions suffer from frequent model's errors at each step of the reasoning chain.***  <br>  <br>
    Apr 16, UC Berkeley and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2504.11741) “Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?”. Recent supervised fine-tuning (SFT) approaches have significantly improved language models' performance on mathematical reasoning tasks, even when models are trained at a small scale. However, the specific capabilities enhanced through such fine-tuning remain poorly understood. This study conducts a detailed analysis of model performance on the AIME24 dataset to understand how reasoning capabilities evolve. The study discovers a ladder-like structure in problem difficulty, categorize questions into four tiers (Easy, Medium, Hard, and Extremely Hard (Exh)), and identify the specific requirements for advancing between tiers. The work finds that progression from Easy to Medium tier requires adopting an R1 reasoning style with minimal SFT (500-1K instances), while Hard-level questions suffer from frequent model's errors at each step of the reasoning chain, with accuracy plateauing at around 65% despite logarithmic scaling. Exh-level questions present a fundamentally different challenge; they require unconventional problem-solving skills that current models uniformly struggle with. Additional findings reveal that carefully curated small-scale datasets offer limited advantage-scaling dataset size proves far more effective. The analysis provides a clearer roadmap for advancing language model capabilities in mathematical reasoning. https://github.com/sunblaze-ucb/reasoning_ladder.git  <br>  <br>

15. ***OpenAI Releases Models with Reasoning and Tool Capabilities:   <br>OpenAI has released o3 and o4-mini, combining reasoning with tools like web browsing and Python, excelling in complex tasks. The o-series models are trained with large-scale reinforcement learning on chains of thought and can reason about the safety policies in context when responding to potentially unsafe prompts***  <br>  <br>
    Apr 16, OpenAI released o3 and o4-mini and published a [report](https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf) “OpenAI o3 and o4-mini System Card”. OpenAI o3 and OpenAI o4-mini combine state-of-the-art reasoning with full tool capabilities—web browsing, Python, image and file analysis, image generation, canvas, automations, file search, and memory. These models excel at solving complex math, coding, and scientific challenges while demonstrating strong visual perception and analysis. The models use tools in their chains of thought to augment their capabilities; for example, cropping or transforming images, searching the web, or using Python to analyze data during their thought process. The OpenAI o-series models are trained with large-scale reinforcement learning on chains of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of the models. In particular, the models can reason about the safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This is the first launch and system card to be released under Version 2 of the Preparedness Framework⁠. OpenAI’s Safety Advisory Group (SAG) reviewed the results of the Preparedness evaluations and determined that OpenAI o3 and o4-mini do not reach the High threshold in any of the three Tracked Categories: Biological and Chemical Capability, Cybersecurity, and AI Self-improvement. The report describes these evaluations, and provide an update on the work to mitigate risks in these areas.  <br>  <br>

17. ***Most-Cited Papers Focus on Research Tools and Methods:   <br>A Nature analysis reveals that the most-cited papers of the 21st century describe fundamental methods and tools, rather than major scientific breakthroughs. Papers providing widely applicable tools and frameworks for conducting research often accrue more citations than those presenting singular discoveries***  <br>  <br>
    Apr 15, Nature published an [article](https://www.nature.com/articles/d41586-025-01125-9) “the most-cited papers of the twenty-first century”. Based on Nature's analysis across five databases, the most-cited scientific papers published since the year 2000 are typically not those detailing major scientific breakthroughs like the Higgs boson or CRISPR. Instead, the list is dominated by papers describing fundamental methods, research software, statistical techniques, databases, and standards for improving research quality. These works act as essential "workhorses" that underpin studies across numerous disciplines. The top paper is a 2016 report on Microsoft's deep residual learning networks (ResNets), crucial for modern AI advancements. Other highly cited examples include foundational AI techniques, software for data analysis, global cancer statistics reports, diagnostic manuals like the DSM-5, and guidelines for conducting systematic reviews (PRISMA). This trend highlights that papers providing widely applicable tools and frameworks for conducting research often accrue more citations than those presenting singular discoveries, reflecting their pervasive influence on the scientific process itself. Citation counts can vary between databases and are influenced by factors like the age of the paper and field size.  <br>  <br>

19. ***Rethinking Language Model Training with Future Goals:   <br>CMU's paper argues that rearranging and processing the training data sequences can allow models to more accurately imitate the true data-generating process, and does not require any other changes to the architecture or training infrastructure. It demonstrates that this technique, Trelawney, and the inference algorithms derived from it allow to improve performance on several key benchmarks***  <br>  <br>
    Apr 15, CMU published a [paper](https://arxiv.org/pdf/2504.11336) “Looking beyond the next token”. The structure of causal language model training assumes that each token can be accurately predicted from the previous context. This contrasts with humans' natural writing and reasoning process, where goals are typically known before the exact argument or phrasings. While this mismatch has been well studied in the literature, the working assumption has been that architectural changes are needed to address this mismatch. The study argues that rearranging and processing the training data sequences can allow models to more accurately imitate the true data-generating process, and does not require any other changes to the architecture or training infrastructure. The work demonstrates that this technique, Trelawney, and the inference algorithms derived from it allow to improve performance on several key benchmarks that span planning, algorithmic reasoning, and story generation tasks. Finally, the method naturally enables the generation of long-term goals at no additional cost. The study investigates how using the model's goal-generation capability can further improve planning and reasoning. Additionally, the authors believe Trelawney could potentially open doors to new capabilities beyond the current language modeling paradigm.  <br>  <br>

21. ***Predicting Best Pretraining Data with Small-Scale Experiments:   <br>Allen Inst for AI et al. introduce DataDecide, a suite of models and data for predicting the best pretraining data using smaller experiments. The work finds that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at larger target scale (1B) and that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute.***  <br>  <br>
    Apr 15, Allen Inst for AI, Uni of Washington and Uni of Penn published a [paper](https://arxiv.org/pdf/2504.11393) “DataDecide: How to Predict Best Pretraining Data with Small Experiments”. Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, the study releases models, data, and evaluations in DataDecide -- the most extensive open suite of models over differences in data and scale. The study conducts controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. The work finds that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at larger target scale (1B) (~80% of com parisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DataDecide can measure improvement in future scaling laws. The study also identifies that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute.  <br>  <br>

23. ***VisualPuzzles for Evaluating Multimodal Reasoning:   <br>CMU introduces VisualPuzzles, a benchmark minimizing domain knowledge to evaluate visual reasoning, showing current models lag human performance. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks***  <br>  <br>
    Apr 15, CMU published a [paper](https://arxiv.org/pdf/2504.10342) “VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge”. Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, the study introduces VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of the questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with "thinking" modes) yield inconsistent gains across models and task types, and the work observes no clear correlation between model size and performance. The study also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge.  <br>  <br>

25. ***LLMs for Classifying Legal Interpretations:   <br>Uni of Zurich studies the identifiability of legal interpretations employed by the European Court of Human Rights using LLMs, showing that LLMs can classify legal interpretations and extract complex legal features efficiently. The results imply that feature-extraction using LLMs leads to robust outcomes while allowing for greater resource- and time efficiency compared to human annotation.***  <br>  <br>
    Apr 15, Uni of Zurich published a [paper](https://link.springer.com/article/10.1007/s10506-025-09447-9) “Classifying legal interpretations using large language models”. In the civil law tradition, legal arguments are used to justify the outcomes of judicial decision-making. These arguments are formed relying on a canon of interpretation techniques (e.g. textual or teleological interpretation). The work studies the identifiability of interpretation techniques as they are employed by the European Court of Human Rights (ECtHR) from a computational law perspective using a unique dataset. The study shows how Large Language Models (LLMs) can be utilized to classify legal interpretations, and compares their performance. The work evaluates proprietary and opensource models using methods such as few-shot and zero-shot chain-of-thought prompting combined with self-consistency. The results imply that feature-extraction using LLMs leads to robust outcomes while allowing for greater resource- and time efficiency compared to human annotation. Furthermore, The results imply that LLMs can play a larger role in the extraction of more complex features that are of particular relevance from a legal perspective.  <br>  <br>

27. ***Reasoning Without Explicit "Thinking" Can Be Effective:   <br>UC Berkeley and Allen Inst for AI find that bypassing the explicit thinking process in LLMs can be surprisingly effective for reasoning tasks, especially in low-budget settings. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets***  <br>  <br>
    Apr 14, UC Berkeley and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2504.09858?) “Reasoning Models Can Be Effective Without Thinking”. Recent LLMs have significantly improved reasoning capabilities, primarily by including an explicit, lengthy Thinking process as part of generation. The study questions whether this explicit thinking is necessary. Using the state-of-the-art DeepSeek-R1-Distill-Qwen, the work finds that bypassing the thinking process via simple prompting, denoted as NoThinking, can be surprisingly effective. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets--including mathematical problem solving, formal theorem proving, and coding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with 700 tokens. Notably, the performance of NoThinking becomes more competitive with pass@k as k increases. Building on this observation, the study demonstrates that a parallel scaling approach that uses NoThinking to generate N outputs independently and aggregates them is highly effective. For aggregation, the study uses task-specific verifiers when available, or apply simple best-of-N strategies such as confidence-based selection. The method outperforms a range of baselines with similar latency using Thinking, and is comparable to Thinking with significantly longer latency (up to 9x). Together, the research encourages a reconsideration of the necessity of lengthy thinking processes, while also establishing a competitive reference for achieving strong reasoning performance in low-budget settings or at low latency using parallel scaling.  <br>  <br>

29. ***Analyzing Post-Training Data Quality Through Layer-wise Gradients:   <br>Uni of Maryland and Uni of Chicago analyze layer-wise gradients to understand how instruction and reasoning data affect LLM post-training, revealing that higher-quality data is associated with lower nuclear norms and higher effective ranks. The analysis reveals that widely-studied metrics for data evaluation can be explained and unified by spectral properties computed from gradients' singular value decomposition (SVD)***  <br>  <br>
    Apr 14, Uni of Maryland and Uni of Chicago published a [paper](https://arxiv.org/pdf/2504.10766) “How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients”. As the post-training of large language models (LLMs) advances from instruction-following to complex reasoning tasks, understanding how different data affect finetuning dynamics remains largely unexplored. This study presents a spectral analysis of layer-wise gradients induced by low/high-quality instruction and reasoning data for LLM post-training. The analysis reveals that widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and Reward, can be explained and unified by spectral properties computed from gradients' singular value decomposition (SVD). Specifically, higher-quality data are usually associated with lower nuclear norms and higher effective ranks. Notably, effective rank exhibits better robustness and resolution than nuclear norm in capturing subtle quality differences. For example, reasoning data achieves substantially higher effective ranks than instruction data, implying richer gradient structures on more complex tasks. Experiments also highlight that models within the same family share similar gradient patterns regardless of their sizes, whereas different model families diverge significantly. Providing a unified view on the effects of data quality across instruction and reasoning data, this work illuminates the interplay between data quality and training stability, shedding novel insights into developing better data exploration strategies for post-training.  <br>  <br>

31. ***End-to-End Training for Latent Diffusion Transformers:   <br>ANU, CSRIO, and NYU introduce REPA-E, a training recipe that unlocks end-to-end training of latent diffusion models with VAE tokenizers, significantly speeding up training and improving performance.***  <br>  <br>
    Apr 14, ANU, CSRIO, and NYU published a [paper](https://arxiv.org/pdf/2504.10483) “REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers”. The study tackles a fundamental question: "Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner?" Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible. However, for latent diffusion transformers, it is observed that end-to-end training both VAE and diffusion-model using standard diffusion-loss is ineffective, even causing a degradation in final performance. The study shows that while diffusion loss is ineffective, end-to-end training can be unlocked through the representation-alignment (REPA) loss -- allowing both VAE and diffusion model to be jointly tuned during the training process. Despite its simplicity, the proposed training recipe (REPA-E) shows remarkable performance; speeding up diffusion model training by over 17x and 45x over REPA and vanilla training recipes, respectively. Interestingly, the study observes that end-to-end tuning with REPA-E also improves the VAE itself; leading to improved latent space structure and downstream generation performance. In terms of final performance, the approach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and without classifier-free guidance on ImageNet 256 x 256. Code is available at https://end2end-diffusion.github.io.  <br>  <br>

33. ***LLMs Exhibit "Priming" Effect When Learning New Data:   <br>Google demonstrates that LLMs exhibit a "priming" effect, inappropriately applying new knowledge in unrelated contexts, and introduces techniques to mitigate this while preserving learning ability. The study develops two novel techniques to modulate how new knowledge affects existing model behavior: (1) a stepping-stone'' text augmentation strategy and (2) an ignore-k'' update pruning method.***  <br>  <br>
    Apr 13, Google published a [paper](https://arxiv.org/pdf/2504.09522) “How new data permeates LLM knowledge and how to dilute it”. Large language models continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. The study demonstrates that when learning new information, LLMs exhibit a "priming" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts. To systematically study this phenomenon, the study introduces "Outlandish," a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM's existing knowledge base. Using this dataset, the study shows that the degree of priming after learning new information can be predicted by measuring the token probability of key words before training. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages. Finally, the study develops two novel techniques to modulate how new knowledge affects existing model behavior: (1) a stepping-stone'' text augmentation strategy and (2) an ignore-k'' update pruning method. These approaches reduce undesirable priming effects by 50-95% while preserving the model's ability to learn new information. The findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/  <br>  <br>

35. ***Speculative Thinking Enhances Small-Model Reasoning:   <br>Case Western Reserve Uni and CMU introduce Speculative Thinking, a training-free framework that enhances small-model reasoning by using large model guidance at inference time, delegating reflective steps to a more capable model to boost accuracy and shorten output.***  <br>  <br>
    Apr 12, Case Western Reserve Uni and CMU published a [paper](https://arxiv.org/pdf/2504.12329) “Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time”. Recent advances leverage post-training to enhance model reasoning performance, which typically requires costly training pipelines and still suffers from inefficient, overly lengthy outputs. The study introduces Speculative Thinking, a training-free framework that enables large reasoning models to guide smaller ones during inference at the reasoning level, distinct from speculative decoding, which operates at the token level. The approach is based on two observations: (1) reasoning-supportive tokens such as "wait" frequently appear after structural delimiters like "\n\n", serving as signals for reflection or continuation; and (2) larger models exhibit stronger control over reflective behavior, reducing unnecessary backtracking while improving reasoning quality. By strategically delegating reflective steps to a more capable model, the method significantly boosts the reasoning accuracy of reasoning models while shortening their output. With the assistance of the 32B reasoning model, the 1.5B model's accuracy on MATH500 increases from 83.2% to 89.4%, marking a substantial improvement of 6.2%. Simultaneously, the average output length is reduced from 5439 tokens to 4583 tokens, representing a 15.7% decrease. Moreover, when applied to a non-reasoning model (Qwen-2.5-7B-Instruct), the framework boosts its accuracy from 74.0% to 81.8% on the same benchmark, achieving a relative improvement of 7.8%. https://github.com/uservan/speculative_thinking  <br>  <br>

37. ***Improving Long Context In-Context Compression:   <br>Google and Uni of Oxford propose GistPool, a new in-context compression method that preserves the simplicity of gisting while significantly boosting its performance on long context compression tasks.***  <br>  <br>
    Apr 11, Google and Uni of Oxford published a [paper](https://arxiv.org/pdf/2504.08934) “Long Context In-Context Compression by Getting to the Gist of Gisting”. Long context processing is critical for the adoption of LLMs, but existing methods often introduce architectural complexity that hinders their practical adoption. Gisting, an in-context compression method with no architectural modification to the decoder transformer, is a promising approach due to its simplicity and compatibility with existing frameworks. While effective for short instructions, the work demonstrates that gisting struggles with longer contexts, with significant performance drops even at minimal compression rates. Surprisingly, a simple average pooling baseline consistently outperforms gisting. The study analyzes the limitations of gisting, including information flow interruptions, capacity limitations and the inability to restrict its attention to subsets of the context. Motivated by theoretical insights into the performance gap between gisting and average pooling, and supported by extensive experimentation, the work proposes GistPool, a new in-context compression method. GistPool preserves the simplicity of gisting, while significantly boosting its performance on long context compression tasks.  <br>  <br>

39. ***Impact of Web Crawling Opt-Outs on LLM Performance:   <br>EPFL and ETH Switzerland's study quantifies the "data compliance gap," finding that compliance with web data opt-outs does not degrade general knowledge acquisition but can impact performance in specialized domains.***  <br>  <br>
    Apr 8, EPFL and ETH Switzerland published a [paper](https://arxiv.org/pdf/2504.06219) “Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs”. The increasing adoption of web crawling opt-outs by copyright holders of online content raises critical questions about the impact of data compliance on large language model (LLM) performance. However, little is known about how these restrictions (and the resultant filtering of pretraining datasets) affect the capabilities of models trained using these corpora. This study conceptualizes this effect as the data compliance gap (DCG), which quantifies the performance difference between models trained on datasets that comply with web crawling opt-outs, and those that do not. The study measures the data compliance gap in two settings: pretraining models from scratch and continual pretraining from existing compliant models (simulating a setting where copyrighted data could be integrated later in pretraining). Experiments with 1.5B models show that, as of January 2025, compliance with web data opt-outs does not degrade general knowledge acquisition (close to 0\% DCG). However, in specialized domains such as biomedical research, excluding major publishers leads to performance declines. These findings suggest that while general-purpose LLMs can be trained to perform equally well using fully open data, performance in specialized domains may benefit from access to high-quality copyrighted sources later in training. The study provides empirical insights into the long-debated trade-off between data compliance and downstream model performance, informing future discussions on AI training practices and policy decisions.  <br>  <br>

41. ***Low-Rank Thinning for Data Summarization:   <br>Uni of Cambridge, Cornell Tech, MIT and Microsoft revise the paper and introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank.***  <br>  <br>
    Apr 8, Uni of Cambridge, Cornell Tech, MIT and Microsoft revised the [paper](https://arxiv.org/pdf/2502.12063) “Low-Rank Thinning”. The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, the study introduces a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, the study designs practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.  <br>  <br>

43. ***Overtrained Language Models Are Harder to Fine-Tune:   <br>CMU, Stanford Uni, Harvard Uni, Princeton Uni released a paper which shows that extended pre-training can make models harder to fine-tune, leading to degraded final performance, termed "catastrophic overtraining."***  <br>  <br>
    Mar 28, CMU, Stanford Uni, Harvard Uni, Princeton Uni published a [paper](https://arxiv.org/pdf/2503.19206) “Overtrained Language Models Are Harder to Fine-Tune”. Large language models are pre-trained on ever-growing token budgets under the assumption that better pre-training performance translates to improved downstream models. This work challenges this assumption and show that extended pre-training can make models harder to fine-tune, leading to degraded final performance. The authors term this phenomenon catastrophic overtraining. For example, the instruction-tuned OLMo-1B model pre-trained on 3T tokens leads to over 2% worse performance on multiple standard LLM benchmarks than its 2.3T token counterpart. Through controlled experiments and theoretical analysis, the work shows that catastrophic overtraining arises from a systematic increase in the broad sensitivity of pre-trained parameters to modifications, including but not limited to fine-tuning. The findings call for a critical reassessment of pre-training design that considers the downstream adaptability of the model.

  <br>  <br>  <br>
***Apr 13, 2025***

1. ***Self-Steering LMs with DisCIPL:   <br>MIT and Yule introduce DisCIPL, a method for "self-steering" LMs using a Planner model to generate task-specific inference programs executed by Follower models, enabling recursive search and efficient reasoning, achieving performance comparable to larger models on constrained generation tasks.***  <br>  <br>
   Apr 11, MIT and Yule published an ICLR [paper](https://openreview.net/forum?id=x7E2Qt7n0V) “Self-Steering Language Models”. While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for “self-steering” LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. The approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, the work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs.  <br>  <br>

3. ***Dynamic Cheatsheet Augments LMs with Adaptive Memory:   <br>Stanford and Together AI present Dynamic Cheatsheet (DC), a framework that endows black-box LMs with a persistent, evolving memory to store and reuse accumulated strategies and insights, substantially enhancing performance across a range of tasks without explicit ground-truth labels or finetuning.***  <br>  <br>
   Apr 10, Stanford Uni and Together AI published a [paper](https://arxiv.org/pdf/2504.07952) “Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory”. Despite their impressive performance on complex tasks, current language models (LMs) typically operate in a vacuum: Each input query is processed separately, without retaining insights from previous attempts. Here, the study presents Dynamic Cheatsheet (DC), a lightweight framework that endows a black-box LM with a persistent, evolving memory. Rather than repeatedly re-discovering or re-committing the same solutions and mistakes, DC enables models to store and reuse accumulated strategies, code snippets, and general problem-solving insights at inference time. This test-time learning enhances performance substantially across a range of tasks without needing explicit ground-truth labels or human feedback. Leveraging DC, Claude 3.5 Sonnet's accuracy more than doubled on AIME math exams once it began retaining algebraic insights across questions. Similarly, GPT-4o's success rate on Game of 24 increased from 10% to 99% after the model discovered and reused a Python-based solution. In tasks prone to arithmetic mistakes, such as balancing equations, DC enabled GPT-4o and Claude to reach near-perfect accuracy by recalling previously validated code, whereas their baselines stagnated around 50%. Beyond arithmetic challenges, DC yields notable accuracy gains on knowledge-demanding tasks. Claude achieved a 9% improvement in GPQA-Diamond and an 8% boost on MMLU-Pro problems. Crucially, DC's memory is self-curated, focusing on concise, transferable snippets rather than entire transcript. Unlike finetuning or static retrieval methods, DC adapts LMs' problem-solving skills on the fly, without modifying their underlying parameters. Overall, the findings present DC as a promising approach for augmenting LMs with persistent memory, bridging the divide between isolated inference events and the cumulative, experience-driven learning characteristic of human cognition. https://github.com/suzgunmirac/dynamic-cheatsheet  <br>  <br>

5. ***Forbes Releases 2025 AI 50 List:   <br>Forbes' seventh annual AI 50 list highlights the most promising privately-held AI companies, with a focus on practical applications, major players like OpenAI and Anthropic, and newcomers like xAI, while also acknowledging legal challenges and the importance of equitable startup ecosystems.***  <br>  <br>
   Apr 10, Forbes release [AI 50](https://www.forbes.com/lists/ai50/) in 2025. Artificial intelligence remains a central focus in venture capital and the business world, with startups shifting from AI model releases to creating practical applications across various fields. Forbes' seventh annual AI 50 list highlights the most promising privately-held AI companies, including newcomers like Anysphere, Speak, and OpenEvidence. Major players like OpenAI and Anthropic dominate the list, having raised significant funds, while new competitors like Elon Musk's xAI and Mira Murati's Thinking Machine Labs emerge. Fei Fei Li's World Labs and enterprise AI company Writer also make notable appearances. AI companies rely heavily on expensive computing power, benefiting infrastructure providers like Crusoe, Lambda, and Together AI. However, startups like DeepSeek demonstrate cost-efficient training methods. The industry faces legal challenges over alleged copyright infringement, with companies like OpenAI, Anthropic, and others being sued for using copyrighted content. The future of AI hinges on court rulings regarding these issues. This year's AI 50 list was highly competitive, with 1,860 submissions judged on business promise, technical talent, and AI use, promoting a more equitable startup ecosystem.  <br>  <br>

7. ***Google Explores Intrinsic Motivation for Mutual Awareness:   <br>Google's paper explores the intrinsic motivation for mutual awareness, hypothesizing that humans possess a fundamental drive to understand and be understood, even without extrinsic rewards, and demonstrates that this drive can facilitate cooperation.***  <br>  <br>
   Apr 10, Google published a [paper](https://arxiv.org/pdf/2504.06611) “Wanting to be Understood”. This paper explores an intrinsic motivation for mutual awareness, hypothesizing that humans possess a fundamental drive to understand and to be understood even in the absence of extrinsic rewards. Through simulations of the perceptual crossing paradigm, the study explores the effect of various internal reward functions in reinforcement learning agents. The drive to understand is implemented as an active inference type artificial curiosity reward, whereas the drive to be understood is implemented through intrinsic rewards for imitation, influence/impressionability, and sub-reaction time anticipation of the other. Results indicate that while artificial curiosity alone does not lead to a preference for social interaction, rewards emphasizing reciprocal understanding successfully drive agents to prioritize interaction. The work demonstrates that this intrinsic motivation can facilitate cooperation in tasks where only one agent receives extrinsic reward for the behaviour of the other.  <br>  <br>

9. ***New Ideas in AI Stem from New Datasets:   <br>Jack Morris argues that major AI breakthroughs aren't primarily driven by novel algorithms but by the unlocking and large-scale utilization of new data sources, predicting that the next major paradigm shift will arise from harnessing vast, currently untapped data reservoirs.***  <br>  <br>
    Apr 10, Jack Morris, a PhD student from Cornell Tech Inst published an [article](https://substack.com/inbox/post/160974493) “There Are No New Ideas in AI… Only New Datasets”. While AI showcases steady advancements often attributed to ongoing research, truly transformative leaps like Deep Neural Networks, Transformers, RLHF, and Reasoning models are rarer, and recent progress appears incremental. This text argues that these major breakthroughs weren't primarily driven by fundamentally novel algorithms, as the core machine learning concepts pre-existed. Instead, their catalyst was the unlocking and large-scale utilization of new data sources: ImageNet, web text, human preferences, and verifiable outputs, respectively. This perspective emphasizes data availability and scale as potentially more crucial for significant progress than specific algorithmic innovations, aligning with the "Bitter Lesson." Consequently, the next major AI paradigm shift is predicted to arise not just from new methods, but from harnessing vast, currently untapped data reservoirs, with video platforms like YouTube and data from embodied systems (robots) being prominent examples. The search for future breakthroughs might prioritize accessing new data over inventing new techniques. A [2023 blog](https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/) also stated that “Then, when you refer to “Lambda”, “ChatGPT”, “Bard”, or “Claude” then, it’s not the model weights that you are referring to. It’s the dataset.”  <br>  <br>

11. ***Hogwild! Inference Enables Parallel LLM Generation:   <br>Yandex and IST Austria propose Hogwild! Inference, a parallel LLM inference engine that allows multiple instances of the same LLM to run in parallel with a shared attention cache, enabling them to synchronize and devise their own collaboration strategies, improving hardware utilization.***  <br>  <br>
    Apr 9, Yandex and IST Austria published a [paper](https://arxiv.org/pdf/2504.06261) “Hogwild! Inference: Parallel LLM Generation via Concurrent Attention”. Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. This work proposes a different design approach: running LLM "workers" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. The approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while "seeing" each other's partial progress in the concurrent cache. The study implements this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with "instant" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. The study finds that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning. https://github.com/eqimp/hogwild_llm  <br>  <br>

13. ***SkillWeaver Enables Web Agents to Self-Improve:   <br>Ohio State University, University of Virginia, Purdue University, CMU, and Cisco introduce SkillWeaver, a framework enabling web agents to self-improve by autonomously synthesizing reusable skills as APIs, expanding their capabilities through iterative exploration and skill composition.***  <br>  <br>
    Apr 9, Ohio State Uni, Uni of Virginia, Prude Uni, CMU and Cisco published a [paper](https://arxiv.org/pdf/2504.07079) “SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills”. To survive and thrive in complex environments, humans have evolved sophisticated self-improvement mechanisms through environment exploration, hierarchical abstraction of experiences into reuseable skills, and collaborative construction of an ever-growing skill repertoire. Despite recent advancements, autonomous web agents still lack crucial self-improvement capabilities, struggling with procedural knowledge abstraction, refining skills, and skill composition. This study introduces SkillWeaver, a skill-centric framework enabling agents to self-improve by autonomously synthesizing reusable skills as APIs. Given a new website, the agent autonomously discovers skills, executes them for practice, and distills practice experiences into robust APIs. Iterative exploration continually expands a library of lightweight, plug-and-play APIs, significantly enhancing the agent's capabilities. Experiments on WebArena and real-world websites demonstrate the efficacy of SkillWeaver, achieving relative success rate improvements of 31.8% and 39.8%, respectively. Additionally, APIs synthesized by strong agents substantially enhance weaker agents through transferable skills, yielding improvements of up to 54.3% on WebArena. These results demonstrate the effectiveness of honing diverse website interactions into APIs, which can be seamlessly shared among various web agents.  <br>  <br>

15. ***OLMoTrace Traces LM Outputs Back to Training Data:   <br>Allen Institute for AI, University of Washington, UC Berkeley and Stanford University present OLMoTrace, a system that traces the outputs of language models back to their full, multi-trillion-token training data in real time, helping users understand model behavior through the lens of their training data.***  <br>  <br>
    Apr 9, Allen Inst for AI, Uni of Washington, UC Berkeley and Stanford Uni published a [paper](https://arxiv.org/pdf/2504.07096) “OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens”. The study presents OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), the system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. The study showcases how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.  <br>  <br>

17. ***AI Scientist-v2 Automates Scientific Discovery:   <br>Sakana AI introduces The AI Scientist-v2, an end-to-end agentic system capable of producing an entirely AI-generated peer-review-accepted workshop paper, highlighting the growing capability of AI in conducting all aspects of scientific research.***  <br>  <br>
    Apr 8, Sakana.AI published a [paper](https://pub.sakana.ai/ai-scientist-v2/paper/paper.pdf) “The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search”. AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. The work introduces The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AIgenerated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, the work enhances the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. The study evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. The authors anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. Code is at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. The work also discusses the role of AI in science, including AI safety.  <br>  <br>

19. ***Reproducibility in LM Reasoning Critically Assessed:   <br>The University of Tubingen and the University of Cambridge conduct a comprehensive study revealing that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices, calling for standardized evaluation frameworks and finding that SFT methods show consistently stronger generalization.***  <br>  <br>
    Apr 9, Uni of Tubingen and Uni of Cambridge published a [paper](https://arxiv.org/pdf/2504.07086) “A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility”. Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work  <br>  <br>

21. ***Lattice Compresses Memory for Efficient Attention:   <br>Google introduces Lattice, a novel recurrent neural network (RNN) mechanism that leverages the inherent low-rank structure of K-V matrices to efficiently compress the cache into a fixed number of memory slots, achieving sub-quadratic complexity in the attention mechanism.***  <br>  <br>
    Apr 8, Google published a [paper](https://arxiv.org/abs/2504.05646) “Lattice: Learning to Efficiently Compress the Memory”. Attention mechanisms have revolutionized sequence learning but suffer from quadratic computational complexity. This paper introduces Lattice, a novel recurrent neural network (RNN) mechanism that leverages the inherent low-rank structure of K-V matrices to efficiently compress the cache into a fixed number of memory slots, achieving sub-quadratic complexity. The study formulates this compression as an online optimization problem and derive a dynamic memory update rule based on a single gradient descent step. The resulting recurrence features a state- and input-dependent gating mechanism, offering an interpretable memory update process. The core innovation is the orthogonal update: each memory slot is updated exclusively with information orthogonal to its current state hence incorporation of only novel, non-redundant data, which minimizes the interference with previously stored information. The experimental results show that Lattice achieves the best perplexity compared to all baselines across diverse context lengths, with performance improvement becoming more pronounced as the context length increases.  <br>  <br>

23. ***Knowledge-Instruct Enables Effective Continual Pre-training:   <br>Microsoft introduces Knowledge-Instruct, a novel approach to efficiently inject knowledge from limited corpora into LLMs through pure instruction-tuning, effectively integrating new knowledge while preserving general reasoning abilities.***  <br>  <br>
    Apr 8, Microsoft published a [paper](https://arxiv.org/pdf/2504.05571) “Knowledge-Instruct: Effective Continual Pre-training from Limited Data using Instructions”. While Large Language Models (LLMs) acquire vast knowledge during pre-training, they often lack domain-specific, new, or niche information. Continual pre-training (CPT) attempts to address this gap but suffers from catastrophic forgetting and inefficiencies in low-data regimes. The study introduces Knowledge-Instruct, a novel approach to efficiently inject knowledge from limited corpora through pure instruction-tuning. By generating information-dense synthetic instruction data, it effectively integrates new knowledge while preserving general reasoning and instruction-following abilities. Knowledge-Instruct demonstrates superior factual memorization, minimizes catastrophic forgetting, and remains scalable by leveraging synthetic data from relatively small language models. Additionally, it enhances contextual understanding, including complex multi-hop reasoning, facilitating integration with retrieval systems. The work validates its effectiveness across diverse benchmarks, including Companies, a new dataset that is released to measure knowledge injection capabilities.  <br>  <br>

25. ***APIGen-MT Generates Multi-Turn Agent Data:   <br>Salesforce introduces APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data through simulated human-agent interplay, training models that outperform frontier models on multi-turn tasks while maintaining superior consistency.***  <br>  <br>
    Apr 8, Salesforce published a [paper](https://arxiv.org/pdf/2504.03601) “APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay”. Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. The work introduces APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, the agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. The study trains a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. The models outperform frontier models such as GPT-4o and Claude 3.5 on tau-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that the verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io  <br>  <br>

27. ***The 2025 AI Index Report Highlights Key Trends:   <br>Stanford University's 2025 AI Index Report summarizes 12 key takeaways, including improving AI performance, increasing AI integration in daily life and business, US leadership in AI model production (but China closing the gap), and continued challenges in complex reasoning.***  <br>  <br>
    Apr 7, Stanford Uni published a [report](https://hai.stanford.edu/ai-index/2025-ai-index-report) “The 2025 AI Index Report”. The report summarized 12 key takeaways: 1) AI performance on demanding benchmarks continues to improve. 2) AI is increasingly embedded in everyday life. 3) Business is all in on AI, fueling record investment and usage, as research continues to show strong productivity impacts. 4) The U.S. still leads in producing top AI models—but China is closing the performance gap. 5) The responsible AI ecosystem evolves—unevenly. 6) Global AI optimism is rising—but deep regional divides remain. 7) AI becomes more efficient, affordable, and accessible. 8) Governments are stepping up on AI—with regulation and investment. 9) AI and computer science education is expanding—but gaps in access and readiness persist. 10) Industry is racing ahead in AI—but the frontier is tightening. 11) AI earns top honors for its impact on science. 12) Complex reasoning remains a challenge.  <br>  <br>

29. ***Adaptive Weighted Rejection Sampling Improves LM Generation:   <br>MIT, ETH, et al. introduce a new algorithm for controlled generation from language models with constraints, using adaptive rejection sampling to avoid evaluating constraints on the full vocabulary at each step and correcting for myopic behavior through importance weighting, improving both runtime and performance.***  <br>  <br>
    Apr 7, MIT, ETH, et al published a [paper](https://arxiv.org/pdf/2504.05410) “Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling”. The dominant approach to generating from language models subject to some constraint is locally constrained decoding (LCD), incrementally sampling tokens at each time step such that the constraint is never violated. Typically, this is achieved through token masking: looping over the vocabulary and excluding non-conforming tokens. There are two important problems with this approach. (i) Evaluating the constraint on every token can be prohibitively expensive -- LM vocabularies often exceed 100,000 tokens. (ii) LCD can distort the global distribution over strings, sampling tokens based only on local information, even if they lead down dead-end paths. This work introduces a new algorithm that addresses both these problems. First, to avoid evaluating a constraint on the full vocabulary at each step of generation, the study proposes an adaptive rejection sampling algorithm that typically requires orders of magnitude fewer constraint evaluations. Second, the work shows how this algorithm can be extended to produce low-variance, unbiased estimates of importance weights at a very small additional cost -- estimates that can be soundly used within previously proposed sequential Monte Carlo algorithms to correct for the myopic behavior of local constraint enforcement. Through extensive empirical evaluation in text-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON domains, the study shows that the approach is superior to state-of-the-art baselines, supporting a broader class of constraints and improving both runtime and performance. Additional theoretical and empirical analyses show that the method's runtime efficiency is driven by its dynamic use of computation, scaling with the divergence between the unconstrained and constrained LM, and as a consequence, runtime improvements are greater for better models.  <br>  <br>

31. ***Test-Time Training Enables One-Minute Video Generation:   <br>Nvidia, Stanford University, UCSD, UC Berkeley, and UT Austin experiment with Test-Time Training (TTT) layers in pre-trained Transformers to generate one-minute videos from text storyboards, generating more coherent videos compared to baselines.***  <br>  <br>
    Apr 7, Nvidia, Stanford Uni, UCSD, UC Berkeley and UT Austin published a [paper](https://arxiv.org/pdf/2504.05298) on CPPR2025 “One-Minute Video Generation with Test-Time Training”. Transformers today still struggle to generate one-minute videos because self-attention layers are inefficient for long context. Alternatives such as Mamba layers struggle with complex multi-scene stories because their hidden states are less expressive. The study experiments with Test-Time Training (TTT) layers, whose hidden states themselves can be neural networks, therefore more expressive. Adding TTT layers into a pre-trained Transformer enables it to generate one-minute videos from text storyboards. For proof of concept, the work curates a dataset based on Tom and Jerry cartoons. Compared to baselines such as Mamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers generate much more coherent videos that tell complex stories, leading by 34 Elo points in a human evaluation of 100 videos per method. Although promising, results still contain artifacts, likely due to the limited capability of the pre-trained 5B model. The efficiency of the implementation can also be improved. The authors have only experimented with one-minute videos due to resource constraints, but the approach can be extended to longer videos and more complex stories. Sample videos, code and annotations are available at: https://test-time-training.github.io/video-dit  <br>  <br>

33. ***SWiRL Improves Reasoning and Tool Use with Multi-Step RL:   <br>Stanford University and Google propose Step-Wise Reinforcement Learning (SWiRL), a synthetic data generation and RL methodology targeting multi-step optimization scenarios, outperforming baselines on tool use, question answering, and mathematical reasoning tasks.***  <br>  <br>
    Apr 7, Stanford Uni and Google published a [paper](https://arxiv.org/pdf/2504.04736) “Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use”. Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. The study proposes a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. The study evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%.  <br>  <br>

35. ***Auditing Model Substitution in LLM APIs:   <br>UC Berkeley formalizes the problem of model substitution detection in LLM APIs, evaluating existing verification techniques and discussing the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as a pathway towards provable model integrity.***  <br>  <br>
    Apr 6, UC Berkeley published a [paper](https://arxiv.org/pdf/2504.04715) “Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs”. The proliferation of Large Language Models (LLMs) accessed via black-box APIs introduces a significant trust challenge: users pay for services based on advertised model capabilities (e.g., size, performance), but providers may covertly substitute the specified model with a cheaper, lower-quality alternative to reduce operational costs. This lack of transparency undermines fairness, erodes trust, and complicates reliable benchmarking. Detecting such substitutions is difficult due to the black-box nature, typically limiting interaction to input-output queries. This paper formalizes the problem of model substitution detection in LLM APIs. The study systematically evaluates existing verification techniques, including output-based statistical tests, benchmark evaluations, and log probability analysis, under various realistic attack scenarios like model quantization, randomized substitution, and benchmark evasion. Findings reveal the limitations of methods relying solely on text outputs, especially against subtle or adaptive attacks. While log probability analysis offers stronger guarantees when available, its accessibility is often limited. The work concludes by discussing the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as a pathway towards provable model integrity, highlighting the trade-offs between security, performance, and provider adoption. Code is available at https://github.com/sunblaze-ucb/llm-api-audit  <br>  <br>

37. ***Retro-Search Distills Higher Quality Reasoning Paths:   <br>Nvidia, University of Washington, and Stanford University introduce Retro-Search, an MCTS-inspired search algorithm for distilling higher quality reasoning paths from large reasoning models, enabling models to self-improve or weak models to improve stronger models' traces, resulting in shorter and faster inference.***  <br>  <br>
    Apr 6, Nvidia, Uni of Washington and Stanford Uni published a [paper](https://arxiv.org/pdf/2504.04383) “Retro-Search: Exploring Untaken Paths for Deeper and Efficient Reasoning”. Large reasoning models exhibit remarkable reasoning capabilities via long, elaborate reasoning trajectories. Supervised fine-tuning on such reasoning traces, also known as distillation, can be a cost-effective way to boost reasoning capabilities of student models. However, empirical observations reveal that these reasoning trajectories are often suboptimal, switching excessively between different lines of thought, resulting in under-thinking, over-thinking, and even degenerate responses. The study introduces Retro-Search, an MCTS-inspired search algorithm, for distilling higher quality reasoning paths from large reasoning models. Retro-Search retrospectively revises reasoning paths to discover better, yet shorter traces, which can then lead to student models with enhanced reasoning capabilities with shorter, thus faster inference. The approach can enable two use cases: self-improvement, where models are fine-tuned on their own Retro-Search-ed thought traces, and weak-to-strong improvement, where a weaker model revises stronger model's thought traces via Retro-Search. For self-improving, R1-distill-7B, fine-tuned on its own Retro-Search-ed traces, reduces the average reasoning length by 31.2% while improving performance by 7.7% across seven math benchmarks. For weak-to-strong improvement, the work retrospectively revises R1-671B's traces from the OpenThoughts dataset using R1-distill-32B as the Retro-Search-er, a model 20x smaller. Qwen2.5-32B, fine-tuned on this refined data, achieves performance comparable to R1-distill-32B, yielding an 11.3% reduction in reasoning length and a 2.4% performance improvement compared to fine-tuning on the original OpenThoughts data. The work counters recently emergent viewpoints that question the relevance of search algorithms in the era of large reasoning models, by demonstrating that there are still opportunities for algorithmic advancements, even for frontier models.  <br>  <br>

39. ***Rethinking Temporal Search for Long-Form Video Understanding:   <br>Stanford University, Northwestern University, and CMU revisit temporal search paradigms for long-form video understanding, introducing LV-Haystack (a long video haystack problem) and propose T*, a lightweight temporal search framework that improves performance.***  <br>  <br>
    Apr 6, Stanford Uni, Northwestern Uni and CMU published a [paper](https://arxiv.org/pdf/2504.02259) “Re-thinking Temporal Search for Long-Form Video Understanding”. Efficiently understanding long-form videos remains a significant challenge in computer vision. This study revisits temporal search paradigms for long-form video understanding and address a fundamental issue pertaining to all state-of-the-art (SOTA) long-context vision-language models (VLMs). The contributions are twofold: First, the work frames temporal search as a Long Video Haystack problem: finding a minimal set of relevant frames (e.g., one to five) from tens of thousands based on specific queries. Upon this formulation, the study introduces LV-Haystack, the first dataset with 480 hours of videos, 15,092 human-annotated instances for both training and evaluation aiming to improve temporal search quality and efficiency. Results on LV-Haystack highlight a significant research gap in temporal search capabilities, with current SOTA search methods only achieving 2.1% temporal F1 score on the Longvideobench subset. Next, inspired by visual search in images, the study proposes a lightweight temporal search framework, T* that reframes costly temporal search as spatial search. T* leverages powerful visual localization techniques commonly used in images and introduces an adaptive zooming-in mechanism that operates across both temporal and spatial dimensions. Extensive experiments show that integrating T* with existing methods significantly improves SOTA long-form video understanding. Under an inference budget of 32 frames, T* improves GPT-4o's performance from 50.5% to 53.1% and LLaVA-OneVision-OV-72B's performance from 56.5% to 62.4% on the Longvideobench XL subset. The code, benchmark, and models is here https://longvideohaystack.github.io/.

41. ***Pretraining Scaling Law for LLM Reasoning Explored: UCSB, MIT-IBM, and Rutgers University explore the effects of scaling on LLMs' reasoning abilities, using a synthetic multihop reasoning environment, and find that overparameterization can impair reasoning performance due to excessive memorization, identifying an empirical scaling law for optimal model size.***
    Apr 4, UCSB, MIT-IBM and Rutgers Uni published a [paper](https://arxiv.org/pdf/2504.03635) “Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning”. Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks requiring complex reasoning. However, the effects of scaling on their reasoning abilities remain insufficiently understood. This study introduces a synthetic multihop reasoning environment designed to closely replicate the structure and distribution of real-world large-scale knowledge graphs. Our reasoning task involves completing missing edges in the graph, which requires advanced multi-hop reasoning and mimics real-world reasoning scenarios. To evaluate this, the work pretrains language models (LMs) from scratch solely on triples from the incomplete graph and assess their ability to infer the missing edges. Interestingly, the study observes that overparameterization can impair reasoning performance due to excessive memorization. The study investigates different factors that affect this U-shaped loss curve, including graph structure, model size, and training steps. To predict the optimal model size for a specific knowledge graph, the work finds an empirical scaling that linearly maps the knowledge graph search entropy to the optimal model size. This work provides new insights into the relationship between scaling and reasoning in LLMs, shedding light on possible ways to optimize their performance for reasoning tasks.  <br>  <br>

43. ***CoT Faithfulness in Reasoning Models Examined:   <br>Anthropic explores the faithfulness of Chain-of-Thought (CoT) in reasoning models across 6 reasoning hints, finding that while CoTs reveal hint usage, the reveal rate is often low and that RL improvements don't necessarily increase verbalization of hints, indicating CoT monitoring is promising but not sufficient for ensuring AI safety.***  <br>  <br>
    Apr 3, Anthropic published a [paper](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf) “Reasoning Models Don’t Always Say What They Think”. Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model’s CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models’ actual reasoning processes. The work evaluates CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.  <br>  <br>

45. ***AI Conversations Improve Happiness:   <br>Yale University, UCL, Google, University of Oxford, and MPUCL find that conversations with AI chatbots can increase subjective well-being, particularly when discussing negative topics, due to the AI's positivity bias and its impact on emotional expectations.***  <br>  <br>
    Apr 2, Yale Uni, UCL, Google, Uni of Oxford and MPUCL published a [paper](https://arxiv.org/pdf/2504.02091) “Increasing happiness through conversations with artificial intelligence”. Chatbots powered by artificial intelligence (AI) have rapidly become a significant part of everyday life, with over a quarter of American adults using them multiple times per week. While these tools offer potential benefits and risks, a fundamental question remains largely unexplored: How do conversations with AI influence subjective well-being? To investigate this, the work conducted a study where participants either engaged in conversations with an AI chatbot (N = 334) or wrote journal entires (N = 193) on the same randomly assigned topics and reported their momentary happiness afterward. The study found that happiness after AI chatbot conversations was higher than after journaling, particularly when discussing negative topics such as depression or guilt. Leveraging large language models for sentiment analysis, the work found that the AI chatbot mirrored participants' sentiment while maintaining a consistent positivity bias. When discussing negative topics, participants gradually aligned their sentiment with the AI's positivity, leading to an overall increase in happiness. The authors hypothesized that the history of participants' sentiment prediction errors, the difference between expected and actual emotional tone when responding to the AI chatbot, might explain this happiness effect. Using computational modeling, the work finds the history of these sentiment prediction errors over the course of a conversation predicts greater post-conversation happiness, demonstrating a central role of emotional expectations during dialogue. The findings underscore the effect that AI interactions can have on human well-being.  <br>  <br>

47. ***AI Judges Achieve Human Expert Equivalence in Design:   <br>  <br>MIT and Penn State University introduce a statistical framework to determine whether AI judges match human experts in design evaluation and find that reasoning-supported VLM models can achieve human-expert equivalence in design evaluation, potentially scaling design evaluation in education and practice.***  <br>  <br>
    Apr 1, MIT and Penn State Uni published a [paper](https://arxiv.org/abs/2504.00938) “AI Judges in Design: Statistical Perspectives on Achieving Human Expert Equivalence With Vision-Language Models”. The subjective evaluation of early stage engineering designs, such as conceptual sketches, traditionally relies on human experts. However, expert evaluations are time-consuming, expensive, and sometimes inconsistent. Recent advances in vision-language models (VLMs) offer the potential to automate design assessments, but it is crucial to ensure that these AI “judges” perform on par with human experts. However, no existing framework assesses expert equivalence. This paper introduces a rigorous statistical framework to determine whether an AI judge's ratings match those of human experts. The authors apply this framework in a case study evaluating four VLM-based judges on key design metrics (uniqueness, creativity, usefulness, and drawing quality). These AI judges employ various in-context learning (ICL) techniques, including uni- vs. multimodal prompts and inference-time reasoning. The same statistical framework is used to assess three trained novices for expert-equivalence. Results show that the top-performing AI judge, using text- and image-based ICL with reasoning, achieves expert-level agreement for uniqueness and drawing quality and outperforms or matches trained novices across all metrics. In 6/6 runs for both uniqueness and creativity, and 5/6 runs for both drawing quality and usefulness, its agreement with experts meets or exceeds that of the majority of trained novices. These findings suggest that reasoning-supported VLM models can achieve human-expert equivalence in design evaluation. This has implications for scaling design evaluation in education and practice, and provides a general statistical framework for validating AI judges in other domains requiring subjective content evaluation.

  <br>  <br>  <br>

***Apr 6, 2025***


1. ***Meta Unveils Llama 4 Herd:   <br>Meta has released the Llama 4 herd of models, including Llama 4 Scout (a 17B parameter multimodal model with a 10M context window) and Llama 4 Maverick (a 17B parameter multimodal model outperforming GPT-4o and Gemini 2.0 Flash), both distilled from Llama 4 Behemoth, a powerful 288B parameter model that outperforms GPT-4.5 and other models on STEM benchmarks.***  <br>  <br>
   Apr 5, Meta [released Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/) herd, which will enable people to build more personalized multimodal experiences. Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks. Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena. These models are Meta’s best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is Meta’s most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training. Llama 4 Scout and Llama 4 Maverick models are available at [Huggingface](https://huggingface.co/meta-llama) and llama.com  <br>  <br>

3. ***DeepSeek Explores Inference-Time Reward Scaling:   <br>DeepSeek and Tsinghua University's research explores improving reward modeling (RM) for LLMs with increased inference compute, proposing Self-Principled Critique Tuning (SPCT) for scalable reward generation and a meta RM for better voting performance, resulting in DeepSeek-GRM models that outperform existing methods.***  <br>  <br>
   Apr 3, DeepSeek and Tsinghua Uni published a [paper](https://arxiv.org/abs/2504.02495) “Inference-Time Scaling for Generalist Reward Modeling”. Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that proper learning methods could enable effective inference-time scalability. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. This work investigates how to improve reward modeling (RM) with more inference compute for general queries, i.e. the inference-time scalability of generalist RM, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, the work adopts pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, the research proposes Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in DeepSeek-GRM models. Furthermore, for effective inference-time scaling, the study uses parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, the authors show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which the authors believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced.  <br>  <br>

5. ***MIT Investigates AI Scientists' Agreement:   <br>MIT's paper "Do Two AI Scientists Agree" explores whether AI models trained on the same scientific task learn the same theories, finding that AI scientists tend to converge in their learned theories with more training data, using Hamiltonian-Lagrangian neural networks (MASS) as a tool for interpretation.***  <br>  <br>
   Apr 3, MIT published a [paper](https://arxiv.org/pdf/2504.02822v1) “Do Two AI Scientists Agree”. When two AI models are trained on the same scientific task, do they learn the same theory or two different theories? Throughout the history of science, people have witnessed the rise and fall of theories driven by experimental validation or falsification: many theories may co-exist when experimental data is lacking, but the space of surviving theories becomes more constrained with more experimental data becoming available. The work shows the same story is true for AI scientists. With increasingly more systems provided in training data, AI scientists tend to converge in the theories they learned, although sometimes they form distinct groups corresponding to different theories. To mechanistically interpret what theories AI scientists learn and quantify their agreement, the study proposes MASS, Hamiltonian-Lagrangian neural networks as AI Scientists, trained on standard problems in physics, aggregating training results across many seeds simulating the different configurations of AI scientists. The key findings include: 1) when trained on textbook problems in classical mechanics, AI scientists prefers either a complete Hamiltonian or Lagrangian description; 2) when extended to non-standard physical problems, the Lagrangian description generalizes, suggesting that Lagrangian dynamics remain as the singular accurate family of descriptions in a rich theory space. The work also observes strong seed dependence of the training dynamics and final learned weights, controlling the rise and fall of relevant theories. Besides interpretability, MASS unifies and generalizes beyond the Lagrangian neural networks and the Hamiltonian neural networks, providing a new tool for learning of dynamical systems. Code is at https://github.com/shinfxh/ai-scientists  <br>  <br>

7. ***Understanding Attention Sinks in LLMs:   <br>Researchers from the University of Oxford, National University of Singapore, and Google theoretically and empirically argue that the heavy attention LLMs give to the first token in a sequence, creating an "attention sink," is a mechanism to avoid over-mixing and relate it to how information propagates in Transformers.***  <br>  <br>
   Apr 3, Uni of Oxford, National Uni of Singapore and Google published a [paper](https://arxiv.org/pdf/2504.02732) “Why do LLMs attend to the first token?”. Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink. Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it. Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention. Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used? This study argues theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers. The work conducts experiments to validate the theoretical intuitions and shows how choices such as context length, depth, and data packing influence the sink behaviour. The authors hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training.  <br>  <br>

9. ***AI 2027 Predicts Transformative Superhuman AI:   <br>ai-2027.com predicts that superhuman AI will have a monumental impact within the next decade, potentially surpassing the Industrial Revolution, emphasizing the need for society to prepare for the advent of superintelligence, and presents a detailed scenario called "AI 2027" to stimulate conversation about the future of AI.***  <br>  <br>
    Apr 3, ai-2027.com published a [paper](https://ai-2027.com/scenario.pdf) “AI 2027”. The authors predict that the impact of superhuman AI over the next decade will be monumental, potentially exceeding the transformative effects of the Industrial Revolution. Prominent figures in AI, including the CEOs of OpenAI, Google DeepMind, and Anthropic, have forecasted the arrival of Artificial General Intelligence (AGI) within the next five years. Sam Altman of OpenAI has expressed ambitions for achieving true superintelligence and envisions a "glorious future." While some may dismiss these predictions as mere hype, the authors caution against this, emphasizing the serious and plausible nature of these developments. They argue that society is currently unprepared for the advent of superintelligence, with few having mapped out a viable path for its development. To address this gap, they created "AI 2027," a detailed scenario that provides concrete details and encourages a broader conversation about the future of AI and how to navigate towards positive outcomes. The authors developed their scenarios by continuously asking "what would happen next," starting from the present day and iterating through multiple versions until they arrived at plausible conclusions. Their work involved extensive background research, expert interviews, and trend extrapolation to make informed predictions. The team, which includes Daniel Kokotajlo and Eli Lifland, has a strong track record in forecasting, particularly in the field of AI. Kokotajlo previously authored a scenario called "What 2026 Looks Like," which proved to be remarkably accurate, and Lifland is recognized as a top competitive forecaster.  <br>  <br>

11. ***ScholarCopilot Enhances Academic Writing with LLMs:   <br>The University of Waterloo, CMU, and others introduce ScholarCopilot, a unified framework to enhance LLMs for generating professional academic articles with accurate citations, dynamically retrieving scholarly references and optimizing both generation and citation tasks, achieving superior performance in retrieval accuracy and generation quality.***  <br>  <br>
    Apr 3, Uni of Waterloo, CMU, et al. published a [paper](https://arxiv.org/pdf/2504.00824) “ScholarCopilot: Training Large Language Models for Academic Writing with Accurate Citations”. Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their capacity to adequately support professional academic writing remains limited. This work introduces ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], and then utilizes its representation to look up relevant citations from a database. The retrieved references are fed into the model to augment the generation process. The study jointly optimizes both the generation and citation tasks within a single framework to increase efficiency. Trained on 500K papers from arXiv, the model achieves a top-1 retrieval accuracy of 40.1% on the evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality (measured across relevance, coherence, academic rigor, completeness, and innovation), surpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct (15.8/25). Human studies also confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience, confirming the effectiveness of our approach.  <br>  <br>

13. ***Dreamer Masters Control Tasks Through World Models:   <br>Nature's paper presents Dreamer, a general reinforcement-learning algorithm that learns to solve tasks across a wide range of applications, outperforming specialized methods across over 150 diverse tasks by learning a model of the environment and imagining future scenarios, and is the first algorithm to collect diamonds in Minecraft without human data or curricula.***  <br>  <br>
    Apr 2, Nature published a [paper](https://www.nature.com/articles/s41586-025-08744-2) “Mastering diverse control tasks through world models”. Developing a general algorithm that learns to solve tasks across a wide range of applications has been a fundamental challenge in artificial intelligence. Although current reinforcement-learning algorithms can be readily applied to tasks similar to what they have been developed for, configuring them for new application domains requires substantial human expertise and experimentation. This study presents the third generation of Dreamer, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration. Dreamer learns a model of the environment and improves its behaviour by imagining future scenarios. Robustness techniques based on normalization, balancing and transformations enable stable learning across domains. Applied out of the box, Dreamer is, to authors knowledge, the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula. This achievement has been posed as a substantial challenge in artificial intelligence that requires exploring farsighted strategies from pixels and sparse rewards in an open world3. The work allows solving challenging control problems without extensive experimentation, making reinforcement learning broadly applicable.  <br>  <br>

15. ***YourBench Enables Easy Custom Evaluation Sets:   <br>Hugging Face and UIUC introduce YourBench, an open-source framework that allows dynamic, automated generation of reliable, up-to-date, and domain-tailored benchmarks cheaply and without manual annotation, along with the Tempora-0325 dataset of recently published documents, to foster more relevant and trustworthy LLM evaluation.***  <br>  <br>
    Apr 2, Huggingface and UIUC published a [paper](https://arxiv.org/abs/2504.01833) “YourBench: Easy Custom Evaluation Sets for Everyone”. Evaluating large language models (LLMs) effectively remains a critical bottleneck, as traditional static benchmarks suffer from saturation and contamination, while human evaluations are costly and slow. This hinders timely or domain-specific assessment, crucial for real-world applications. The work introduce YourBench, a novel, open-source framework that addresses these limitations by enabling dynamic, automated generation of reliable, up-to-date, and domain-tailored benchmarks cheaply and without manual annotation, directly from user-provided documents. The study demonstrates its efficacy by replicating 7 diverse MMLU subsets using minimal source text, achieving this for under 15 USD in total inference costs while perfectly preserving the relative model performance rankings (Spearman Rho = 1) observed on the original benchmark. To ensure that YourBench generates data grounded in provided input instead of relying on posterior parametric knowledge in models, the study also introduces Tempora-0325, a novel dataset of over 7K diverse documents, published exclusively after March 2025. A comprehensive analysis spans 26 SoTA models from 7 major families across varying scales (3-671B parameters) to validate the quality of generated evaluations through rigorous algorithmic checks (e.g., citation grounding) and human assessments. The authors release the YourBench library, the Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all evaluation and inference traces to facilitate reproducible research and empower the community to generate bespoke benchmarks on demand, fostering more relevant and trustworthy LLM evaluation.  <br>  <br>

17. ***Google Outlines Technical AGI Safety Approach:   <br>Google's paper outlines an approach to address the risks of Artificial General Intelligence (AGI), focusing on technical approaches to misuse and misalignment, including preventing threat actors from accessing dangerous capabilities and building aligned models with amplified oversight and system-level security.***  <br>  <br>
    Apr 2, Google published a 145-page [paper](https://arxiv.org/pdf/2504.01849) “An Approach to Technical AGI Safety and Security”. Artificial General Intelligence (AGI) promises transformative benefits but also presents significant risks. The work develops an approach to address the risk of harms consequential enough to significantly harm humanity. The study identifies four areas of risk: misuse, misalignment, mistakes, and structural risks. Of these, the work focuses on technical approaches to misuse and misalignment. For misuse, the strategy aims to prevent threat actors from accessing dangerous capabilities, by proactively identifying dangerous capabilities, and implementing robust security, access restrictions, monitoring, and model safety mitigations. To address misalignment, the study outlines two lines of defense. First, model-level mitigations such as amplified oversight and robust training can help to build an aligned model. Second, system-level security measures such as monitoring and access control can mitigate harm even if the model is misaligned. Techniques from interpretability, uncertainty estimation, and safer design patterns can enhance the effectiveness of these mitigations. Finally, the study briefly outlines how these ingredients could be combined to produce safety cases for AGI systems.  <br>  <br>

19. ***PaperBench Evaluates AI's Ability to Replicate Research:   <br>OpenAI introduces PaperBench, a benchmark that evaluates AI agents' ability to replicate state-of-the-art AI research, requiring them to understand papers, develop codebases, and execute experiments, finding that current models do not yet outperform human researchers.***  <br>  <br>
    Apr 2, OpenAI published a [paper](https://arxiv.org/abs/2504.01848) “PaperBench: Evaluating AI's Ability to Replicate AI Research”. The work introduces PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, the authors develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, the work also develops an LLM-based judge to automatically grade replication attempts against rubrics, and assess the judge's performance by creating a separate benchmark for judges. The study evaluates several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0%. Finally, the authors recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. Code is at https://github.com/openai/preparedness  <br>  <br>

21. ***ZClip Mitigates Loss Spikes in LLM Training:   <br>BluOrion introduces ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time to proactively mitigate large gradient spikes during LLM training.***  <br>  <br>
    Apr 2, BluOrion published a [paper](https://arxiv.org/pdf/2504.02507) “ZClip: Adaptive Spike Mitigation for LLM Pre-Training”. Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. This work proposes ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Code is available at: https://github.com/bluorion-com/ZClip  <br>  <br>

23. ***Visual SSL Matches CLIP Performance at Scale:   <br>Meta, NYU, and Princeton University demonstrate that Visual Self-Supervised Learning (SSL) can match Contrastive Language-Image Pretraining (CLIP) performance on VQA and vision benchmarks when trained at scale on the same data, suggesting that pure visual SSL can match language-supervised visual pretraining.***  <br>  <br>
    Apr 1, Meta, NYU and Princeton Uni published a [paper](https://arxiv.org/pdf/2504.01017) “Scaling Language-Free Visual Representation Learning”. Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. This study asks the question: "Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?" The authors study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, the work observes visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning.  <br>  <br>

25. ***Multi-Token Attention Enhances LLM Performance:   <br>Meta introduces Multi-Token Attention (MTA), a new attention method that allows LLMs to condition their attention weights on multiple query and key vectors simultaneously, achieved by applying convolution operations over queries, keys and heads, resulting in enhanced performance on language modeling tasks.***  <br>  <br>
    Apr 1, Meta published a [paper](https://arxiv.org/pdf/2504.00927) “Multi-Token Attention”. Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This "single token attention" bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, the study proposes a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other's attention weights for more precise attention. As a result, the method can locate relevant context using richer, more nuanced information that can exceed a single vector's capacity. Through extensive evaluations, the study demonstrates that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where the method's ability to leverage richer information proves particularly beneficial.  <br>  <br>

27. ***Execution-Guided SQL Generation Improves Accuracy:   <br>Snowflake proposes a novel approach for generating complex outputs in text-to-SQL tasks that leverages execution results to select the most semantically consistent query, enabling smaller models to surpass computationally intensive reasoning methods while reducing inference costs.***  <br>  <br>
    Apr 1, Snowflake published a [paper](https://arxiv.org/pdf/2503.24364) “Query and Conquer: Execution-Guided SQL Generation”. The study proposes a novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. The method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally intensive reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference cost by as much as 30 times. It integrates effortlessly with existing models, offering a practical and scalable pathway to state-of-the-art SQL generation.  <br>  <br>

29. ***Compute-Optimal Problem Solving for LLM Reasoning:   <br>TU Darmstadt & hessian.AI, UCLA, Google and Mila compare Self-Consistency (SC) and Generative Reward Models (GenRM) for scaling test-time compute in LLM reasoning, finding that SC is more compute-efficient for most practical inference budgets and deriving inference scaling laws for the GenRM paradigm.***  <br>  <br>
    Apr 1, TU Darmstadt & hessian.AI, UCLA, Google and Mila published a [paper](https://arxiv.org/pdf/2504.01005) “When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning”. Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should one spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, the work evaluates GenRM against SC under a fixed inference budget. Interestingly, the study finds that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, the work derives inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. The work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling  <br>  <br>

31. ***Token Embeddings Violate the Manifold Hypothesis:   <br>American University, Galois Inc, and the University of Washington find that token embeddings in LLMs do not conform to the manifold hypothesis, with the token subspace provably not a fiber bundle, leading to potentially flawed understandings and conclusions about LLMs.***  <br>  <br>
    Apr 1, American Uni, Galois Inc and Uni of Washington published a [paper](https://arxiv.org/pdf/2504.01002) “Token embeddings violate the manifold hypothesis”. To fully understand the behavior of a large language model (LLM) requires the understanding of its input space. If this input space differs from assumption, the understanding of and conclusions about the LLM is likely flawed, regardless of its architecture. Here, the work elucidates the structure of the token embeddings, the input domain for LLMs, both empirically and theoretically. The study presents a generalized and statistically testable model where the neighborhood of each token splits into well-defined signal and noise dimensions. This model is based on a generalization of a manifold called a fiber bundle, so the work denotes the hypothesis test as the “fiber bundle null.” Failing to reject the null is uninformative, but rejecting it at a specific token indicates that token has a statistically significant local structure, and so is of interest. By running the test over several open-source LLMs, each with unique token embeddings, the work finds that the null is frequently rejected, and so the token subspace is provably not a fiber bundle and hence also not a manifold. As a consequence of the findings, when an LLM is presented with two semantically equivalent prompts, and if one prompt contains a token implicated by the test, that prompt will likely exhibit more output variability proportional to the local signal dimension of the token.  <br>  <br>

33. ***NoProp: A Gradient-Free Learning Method for Neural Networks:   <br>The University of Oxford and Mila introduce NoProp, a new learning method for training neural networks that does not rely on forward or backward propagation, instead drawing inspiration from diffusion and flow matching methods, demonstrating effectiveness on image classification benchmarks.***  <br>  <br>
    Mar 31, Uni of Oxford and Mila published a [paper](https://arxiv.org/pdf/2503.24322) “NoProp: Training Neural Networks without Back-propagation or Forward-propagation”. The canonical deep learning approach for learning requires computing a gradient term at each layer by back-propagating the error signal from the output towards each learnable parameter. Given the stacked structure of neural networks, where each layer builds on the representation of the layer below, this approach leads to hierarchical representations. More abstract features live on the top layers of the model, while features on lower layers are expected to be less abstract. In contrast to this, the study introduces a new learning method named NoProp, which does not rely on either forward or backwards propagation. Instead, NoProp takes inspiration from diffusion and flow matching methods, where each layer independently learns to denoise a noisy target. The authors believe this work takes a first step towards introducing a new family of gradient-free learning methods, that does not learn hierarchical representations -- at least not in the usual sense. NoProp needs to fix the representation at each layer beforehand to a noised version of the target, learning a local denoising process that can then be exploited at inference. The study demonstrates the effectiveness of the method on MNIST, CIFAR-10, and CIFAR-100 image classification benchmarks. Results show that NoProp is a viable learning algorithm which achieves superior accuracy, is easier to use and computationally more efficient compared to other existing back-propagation-free methods. By departing from the traditional gradient based learning paradigm, NoProp alters how credit assignment is done within the network, enabling more efficient distributed learning as well as potentially impacting other characteristics of the learning process.  <br>  <br>

35. ***Thinking Intervention Controls Reasoning Models:   <br>Princeton University and Nvidia propose Thinking Intervention, a novel paradigm for controlling reasoning-enhanced LLMs by strategically inserting or revising specific thinking tokens, achieving significant improvements in instruction following, reasoning about instruction hierarchies, and safety alignment.***  <br>  <br>
    Mar 31, Princeton Uni and Nvidia published a [paper](https://arxiv.org/abs/2503.24370) “Effectively Controlling Reasoning Models through Thinking Intervention”. Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. This study demonstrates that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. The study proposes Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. The work conducts comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, the work opens a promising new research avenue for controlling reasoning LLMs.  <br>  <br>

37. ***LLMs Pass the Turing Test:   <br>UC San Diego presents the first empirical evidence that a GPT model (GPT-4.5) passes a standard three-party Turing test, being judged as the human partner more often than the real human, and the implications on defining intelligence in LLMs.***  <br>  <br>
    Mar 31, UC San Diego published a [paper](https://arxiv.org/pdf/2503.23674) “Large Language Models Pass the Turing Test”. The study evaluated 4 systems (ELIZA, GPT-4o, LLaMa-3.1-405B, and GPT-4.5) in two randomised, controlled, and pre-registered Turing tests on independent populations. Participants had 5 minute conversations simultaneously with another human participant and one of these systems before judging which conversational partner they thought was human. When prompted to adopt a humanlike persona, GPT-4.5 was judged to be the human 73% of the time: significantly more often than interrogators selected the real human participant. LLaMa-3.1, with the same prompt, was judged to be the human 56% of the time -- not significantly more or less often than the humans they were being compared to -- while baseline models (ELIZA and GPT-4o) achieved win rates significantly below chance (23% and 21% respectively). The results constitute the first empirical evidence that any artificial system passes a standard three-party Turing test. The results have implications for debates about what kind of intelligence is exhibited by Large Language Models (LLMs), and the social and economic impacts these systems are likely to have.  <br>  <br>

39. ***GNNs Extrapolate OOD for Shortest Paths:   <br>UCSD demonstrates that Graph Neural Networks (GNNs), when trained to minimize a sparsity-regularized loss, exactly implement the Bellman-Ford (BF) algorithm for shortest paths and are therefore guaranteed to extrapolate to arbitrary shortest-path problems.***  <br>  <br>
    Mar 31, UCSD published a [paper](https://arxiv.org/pdf/2503.19173) “Graph neural networks extrapolate out-of-distribution for shortest paths”. Neural networks (NNs), despite their success and wide adoption, still struggle to extrapolate out-of-distribution (OOD), i.e., to inputs that are not well-represented by their training dataset. Addressing the OOD generalization gap is crucial when models are deployed in environments significantly different from the training set, such as applying Graph Neural Networks (GNNs) trained on small graphs to large, real-world graphs. One promising approach for achieving robust OOD generalization is the framework of neural algorithmic alignment, which incorporates ideas from classical algorithms by designing neural architectures that resemble specific algorithmic paradigms (e.g. dynamic programming). The hope is that trained models of this form would have superior OOD capabilities, in much the same way that classical algorithms work for all instances. The work rigorously analyzes the role of algorithmic alignment in achieving OOD generalization, focusing on graph neural networks (GNNs) applied to the canonical shortest path problem. The study proves that GNNs, trained to minimize a sparsity-regularized loss over a small set of shortest path instances, exactly implement the Bellman-Ford (BF) algorithm for shortest paths. In fact, if a GNN minimizes this loss within an error of ϵ, it implements the BF algorithm with an error of O(ϵ). Consequently, despite limited training data, these GNNs are guaranteed to extrapolate to arbitrary shortest-path problems, including instances of any size. Empirical results support the theory by showing that NNs trained by gradient descent are able to minimize this loss and extrapolate in practice.  <br>  <br>

41. ***MVDRAM Accelerates LLM Inference with Unmodified DRAM:   <br>The University of Tokyo and Microsoft present MVDRAM, a practical system to accelerate GeMV operations for low-bit LLM inference using unmodified DRAM by leveraging data sharing patterns and mathematical linearity, achieving significant speedup and energy efficiency.***  <br>  <br>
    Mar 31, Uni of Tokyo and Microsoft published a [paper](https://arxiv.org/pdf/2503.23817) “MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM Acceleration”. General matrix-vector multiplication (GeMV) remains a critical latency bottleneck in large language model (LLM) inference, even with quantized low-bit models. Processing-Using-DRAM (PUD), an analog in-DRAM computing technique, has the potential to repurpose on-device DRAM as a GeMV engine, offering additional high-throughput processing capabilities to widespread consumer devices without DRAM modifications. However, applying PUD to GeMV operations in the LLM inference pipeline incurs significant overheads before and after in-DRAM computation, diminishing the benefits of its high-throughput processing capabilities. This paper presents MVDRAM, the first practical system to accelerate GeMV operations for low-bit LLM inference using unmodified DRAM. By leveraging the data sharing patterns and mathematical linearity in GeMV operations, MVDRAM orchestrates the processor and DRAM to eliminate the costs associated with pre-arranging inputs and bit-transposition of outputs required in conventional PUD approaches. Experimental evaluation with four DDR4 DRAM modules shows that MVDRAM achieves comparable or even better inference speed than the processor-based implementation for GeMV operations in low-bit (under 4-bit) LLM. In particular, MVDRAM achieves up to 7.29× speedup and 30.5× energy efficiency for low-bit GeMV operations. For end-to-end LLM inference, MVDRAM achieves 2.18× and 1.31× throughput improvements, along with 3.04× and 2.35× energy efficiency, for 2-bit and 4-bit quantized low-bit models, respectively. MVDRAM has the potential to redefine the AI hardware landscape by demonstrating the feasibility of standard DRAM as an LLM accelerator.  <br>  <br>

43. ***Contradiction Detection Evaluated in RAG Systems:   <br>Amazon addresses the challenge of contradictory information in RAG systems, presenting a data generation framework to simulate different contradiction types and evaluating LLMs' ability to detect them, finding that context validation remains challenging even for state-of-the-art models.***  <br>  <br>
    Mar 31, Amazon published a [paper](https://arxiv.org/abs/2504.00180) “Contradiction Detection in RAG Systems: Evaluating LLMs as Context Validators for Improved Information Consistency”. Retrieval Augmented Generation (RAG) systems have emerged as a powerful method for enhancing large language models (LLMs) with up-to-date information. However, the retrieval step in RAG can sometimes surface documents containing contradictory information, particularly in rapidly evolving domains such as news. These contradictions can significantly impact the performance of LLMs, leading to inconsistent or erroneous outputs. This study addresses this critical challenge in two ways. First, the work presents a novel data generation framework to simulate different types of contradictions that may occur in the retrieval stage of a RAG system. Second, the study evaluates the robustness of different LLMs in performing as context validators, assessing their ability to detect contradictory information within retrieved document sets. Experimental results reveal that context validation remains a challenging task even for state-of-the-art LLMs, with performance varying significantly across different types of contradictions. While larger models generally perform better at contradiction detection, the effectiveness of different prompting strategies varies across tasks and model architectures. The work finds that chain-of-thought prompting shows notable improvements for some models but may hinder performance in others, highlighting the complexity of the task and the need for more robust approaches to context validation in RAG systems.  <br>  <br>

45. ***Interpretability in Machine Learning for Physics Reviewed:   <br>The University of Waterloo and others review the role of interpretability in machine learning applied to physics, categorizing different aspects of interpretability, discussing machine learning models in terms of both interpretability and performance, and exploring the philosophical implications of interpretability in scientific inquiry.***  <br>  <br>
    Mar 30, Uni of Waterloo et al published a [paper](https://arxiv.org/pdf/2503.23616) “Interpretable Machine Learning in Physics: A Review”. Machine learning is increasingly transforming various scientific fields, enabled by advancements in computational power and access to large data sets from experiments and simulations. As artificial intelligence (AI) continues to grow in capability, these algorithms will enable many scientific discoveries beyond human capabilities. Since the primary goal of science is to understand the world around us, fully leveraging machine learning in scientific discovery requires models that are interpretable -- allowing experts to comprehend the concepts underlying machine-learned predictions. Successful interpretations increase trust in black-box methods, help reduce errors, allow for the improvement of the underlying models, enhance human-AI collaboration, and ultimately enable fully automated scientific discoveries that remain understandable to human scientists. This review examines the role of interpretability in machine learning applied to physics. The authors categorize different aspects of interpretability, discuss machine learning models in terms of both interpretability and performance, and explore the philosophical implications of interpretability in scientific inquiry. Additionally, the work highlights recent advances in interpretable machine learning across many subfields of physics. By bridging boundaries between disciplines -- each with its own unique insights and challenges -- aiming to establish interpretable machine learning as a core research focus in science.  <br>  <br>

47. ***Challenges and Paths Towards AI for Software Engineering Discussed:   <br>MIT and others discuss progress, challenges, and promising research directions for AI in software engineering, emphasizing tasks beyond code generation and completion and aiming for high levels of automation in routine development efforts.***  <br>  <br>
    Mar 28, MIT et al published a [paper](https://arxiv.org/pdf/2503.22625) “Challenges and Paths Towards AI for Software Engineering”. AI for software engineering has made remarkable progress recently, becoming a notable success within generative AI. Despite this, there are still many challenges that need to be addressed before automated software engineering reaches its full potential. It should be possible to reach high levels of automation where humans can focus on the critical decisions of what to build and how to balance difficult tradeoffs while most routine development effort is automated away. Reaching this level of automation will require substantial research and engineering efforts across academia and industry. This study aims to discuss progress towards this in a threefold manner. First, the study provides a structured taxonomy of concrete tasks in AI for software engineering, emphasizing the many other tasks in software engineering beyond code generation and completion. Second, the work outlines several key bottlenecks that limit current approaches. Finally, the work provides an opinionated list of promising research directions toward making progress on these bottlenecks, hoping to inspire future research in this rapidly maturing field.  <br>  <br>

49. ***Entity Frequency Influences Hallucinations in LLMs:   <br>Researchers from the University of Oxford, LMU Munich, and others demonstrate that an asymmetry exists in the recognition of logically equivalent facts, which can be attributed to frequency discrepancies of entities appearing as subjects versus objects in pre-training data, influencing LLM hallucinations.***  <br>  <br>
    Mar 28, Uni of Oxford, LMU Munich et al published a [paper](https://arxiv.org/pdf/2503.22362) “Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs”. Understanding and mitigating hallucinations in Large Language Models (LLMs) is crucial for ensuring reliable content generation. While previous research has primarily focused on "when" LLMs hallucinate, the work explains "why" and directly links model behaviour to the pre-training data that forms their prior knowledge. Specifically, the study demonstrates that an asymmetry exists in the recognition of logically equivalent facts, which can be attributed to frequency discrepancies of entities appearing as subjects versus objects. Given that most pre-training datasets are inaccessible, the work leverages the fully open-source OLMo series by indexing its Dolma dataset to estimate entity frequencies. Using relational facts (represented as triples) from Wikidata5M, the study constructs probing datasets to isolate this effect. Experiments reveal that facts with a high-frequency subject and a low-frequency object are better recognised than their inverse, despite their logical equivalence. The pattern reverses in low-to-high frequency settings, and no statistically significant asymmetry emerges when both entities are high-frequency. These findings highlight the influential role of pre-training data in shaping model predictions and provide insights for inferring the characteristics of pre-training data in closed or partially closed LLMs.  <br>  <br>

51. ***CoT-VLA Enables Visual Chain-of-Thought Reasoning for VLAs:   <br>Nvidia, Stanford University and MIT introduce CoT-VLA, a method that incorporates explicit visual chain-of-thought (CoT) reasoning into vision-language-action models (VLAs) by predicting future image frames autoregressively as visual goals before generating a short action sequence.***  <br>  <br>
    Mar 27, Nvidia, Stanford Uni and MIT published a [paper](https://arxiv.org/pdf/2503.22020) “CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models”. Vision-language-action models (VLAs) have shown potential in leveraging pretrained vision-language models and diverse robot demonstrations for learning generalizable sensorimotor control. While this paradigm effectively utilizes large-scale data from both robotic and non-robotic sources, current VLAs primarily focus on direct input--output mappings, lacking the intermediate reasoning steps crucial for complex manipulation tasks. As a result, existing VLAs lack temporal planning or reasoning capabilities. This work introduces a method that incorporates explicit visual chain-of-thought (CoT) reasoning into vision-language-action models (VLAs) by predicting future image frames autoregressively as visual goals before generating a short action sequence to achieve these goals. The study introduces CoT-VLA, a state-of-the-art 7B VLA that can understand and generate visual and action tokens. Experimental results demonstrate that CoT-VLA achieves strong performance, outperforming the state-of-the-art VLA model by 17% in real-world manipulation tasks and 6% in simulation benchmarks. Project website: https://cot-vla.github.io/  <br>  <br>

53. ***CodeScientist Automates Scientific Discovery with Code-Based Experimentation:   <br>The Allen Institute for AI and others introduce CodeScientist, a novel ASD system that frames ideation and experiment construction as a genetic search jointly over research articles and codeblocks, generating discoveries in the domain of agents and virtual environments.***  <br>  <br>
    Mar 20, Allen Inst. for AI et al published a [paper](https://arxiv.org/pdf/2503.22708) “CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation”. Despite the surge of interest in autonomous scientific discovery (ASD) of software artifacts (e.g., improved ML algorithms), current ASD systems face two key limitations: (1) they largely explore variants of existing codebases or similarly constrained design spaces, and (2) they produce large volumes of research artifacts (such as automatically generated papers and code) that are typically evaluated using conference-style paper review with limited evaluation of code. This study introduces CodeScientist, a novel ASD system that frames ideation and experiment construction as a form of genetic search jointly over combinations of research articles and codeblocks defining common actions in a domain (like prompting a language model). The work uses this paradigm to conduct hundreds of automated experiments on machine-generated ideas broadly in the domain of agents and virtual environments, with the system returning 19 discoveries, 6 of which were judged as being both at least minimally sound and incrementally novel after a multi-faceted evaluation beyond that typically conducted in prior work, including external (conference-style) review, code review, and replication attempts. Moreover, the discoveries span new tasks, agents, metrics, and data, suggesting a qualitative shift from benchmark optimization to broader discoveries.

  <br>  <br>  <br>


***Mar 30, 2025***

1. ***Language Model Embeddings Share Global and Local Geometric Structures.   <br>Researchers from Harvard University and Google have discovered that token embeddings in language models exhibit common geometric structures, including similar relative orientations ("global" similarities) and shared local geometry characterized by intrinsic dimensionality. The study shows that tokens with lower intrinsic dimensions tend to form semantically coherent clusters. Surprisingly, this alignment persists through hidden states, enabling the transfer of steering vectors between language models with different dimensions, which has implications for interpretability.***  <br>  <br>
   Mar 27, Harvard Uni and Google published a [paper](https://www.arxiv.org/pdf/2503.21073) “Shared Global and Local Geometry of Language Model Embeddings”. Researchers have recently suggested that models share common representations. This work finds that the token embeddings of language models exhibit common geometric structure. First, the study finds “global” similarities: token embeddings often share similar relative orientations. Next, the study characterizes local geometry in two ways: (1) by using Locally Linear Embeddings, and (2) by defining a simple measure for the intrinsic dimension of each token embedding. The intrinsic dimension measure demonstrates that token embeddings lie on a lower dimensional manifold. The study qualitatively shows that tokens with lower intrinsic dimensions often have semantically coherent clusters, while those with higher intrinsic dimensions do not. Both characterizations allow to find similarities in the local geometry of token embeddings. Perhaps most surprisingly, the study finds that alignment in token embeddings persists through the hidden states of language models, allowing us to develop an application for interpretability. Namely, the work empirically demonstrates that steering vectors from one language model can be transferred to another, despite the two models having different dimensions.  <br>  <br>

3. ***MCTS-RAG Enhances Small LLM Reasoning with Iterative Retrieval and Search.   <br>Yale University and NYU have introduced MCTS-RAG, a novel approach that improves the reasoning capabilities of small language models on knowledge-intensive tasks. It combines retrieval-augmented generation (RAG) for relevant context with Monte Carlo Tree Search (MCTS) to refine reasoning paths through an iterative decision-making process. This integration of structured reasoning and adaptive retrieval leads to enhanced decision-making, reduced hallucinations, and improved factual accuracy, allowing smaller LMs to achieve performance comparable to frontier LLMs.***  <br>  <br>
   Mar 26, Yale Uni and NYU published a [paper](https://arxiv.org/pdf/2503.20757) “MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search”. The study introduces MCTS-RAG, a novel approach that enhances the reasoning capabilities of small language models on knowledge-intensive tasks by leveraging retrieval-augmented generation (RAG) to provide relevant context and Monte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically integrates retrieval and reasoning through an iterative decision-making process. Unlike standard RAG methods, which typically retrieve information independently from reasoning and thus integrate knowledge suboptimally, or conventional MCTS reasoning, which depends solely on internal model knowledge without external facts, MCTS-RAG combines structured reasoning with adaptive retrieval. This integrated approach enhances decision-making, reduces hallucinations, and ensures improved factual accuracy and response consistency. The experimental results on multiple reasoning and knowledge-intensive datasets datasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that the method enables small-scale LMs to achieve performance comparable to frontier LLMs like GPT-4o by effectively scaling inference-time compute, setting a new standard for reasoning in small-scale models. https://github.com/yale-nlp/MCTS-RAG  <br>  <br>

5. ***Open Deep Search Democratizes Search with Open-Source Reasoning Agents.   <br>Sentient, the University of Washington, Princeton University, and UC Berkeley have presented Open Deep Search (ODS), a framework aiming to bridge the gap between proprietary and open-source search AI solutions. ODS augments open-source LLMs with reasoning agents that can strategically use web search tools. It comprises the Open Search Tool, a novel web search tool outperforming proprietary alternatives, and the Open Reasoning Agent, which orchestrates actions using this tool, enabling open-source LLMs to achieve near state-of-the-art performance on question-answering benchmarks.***  <br>  <br>
   Mar 26, Sentient, Uni of Washington, Princeton Uni and UC Berkeley published a [paper](https://arxiv.org/pdf/2503.20201) “Open Deep Search: Democratizing Search with Open-source Reasoning Agents”. The study introduces Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and OpenAI's GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with a base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating a sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is a novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is a general framework for seamlessly augmenting any LLMs -- for example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES. https://github.com/sentient-agi/OpenDeepSearch  <br>  <br>

7. ***Entropy-Guided Reward Aggregation (ENCORE) Improves LLM Safety Alignment.   <br>Researchers from Harvard University, NYU, UCLA, and MIT have found that safety rules with high rating entropy are less reliable in identifying preferred LLM responses. Leveraging this, they introduce ENCORE, a training-free approach that improves the alignment of LLMs with safety guidelines by downweighting reward rules exhibiting high entropy during multi-head reward aggregation. Theoretical analysis supports this entropy-based penalization, and experiments on safety tasks demonstrate that ENCORE significantly outperforms various competitive baselines while maintaining interpretability.***  <br>  <br>
   Mar 26, Harvard Uni, NYU, UCLA and MIT published a [paper](https://arxiv.org/pdf/2503.20995) “Multi-head Reward Aggregation Guided by Entropy”. Aligning large language models (LLMs) with safety guidelines typically involves reinforcement learning from human feedback (RLHF), relying on human-generated preference annotations. However, assigning consistent overall quality ratings is challenging, prompting recent research to shift towards detailed evaluations based on multiple specific safety criteria. This paper uncovers a consistent observation: safety rules characterized by high rating entropy are generally less reliable in identifying responses preferred by humans. Leveraging this finding, the study introduces ENCORE, a straightforward entropy-guided approach that composes multi-head rewards by downweighting rules exhibiting high rating entropy. Theoretically, the study demonstrates that rules with elevated entropy naturally receive minimal weighting in the Bradley-Terry optimization framework, justifying the entropy-based penalization. Through extensive experiments on RewardBench safety tasks, the method significantly surpasses several competitive baselines, including random weighting, uniform weighting, single-head Bradley-Terry models, and LLM-based judging methods. The proposed approach is training-free, broadly applicable to various datasets, and maintains interpretability, offering a practical and effective solution for multi-attribute reward modeling.  <br>  <br>

9. ***Google Releases Gemini 2.5 Pro Experimental with Advanced Reasoning and a Million-Token Context.   <br>Google has launched Gemini 2.5 Pro Experimental, the first release of their latest AI model, Gemini 2.5. This model excels in complex problem-solving with advanced reasoning and coding capabilities, currently ranking #1 on the LMArena benchmark. Gemini 2.5 builds upon techniques like reinforcement learning and chain-of-thought prompting, featuring a 1 million token context window, multimodality, and strong performance across coding, math, and science benchmarks. It is now available in Google AI Studio and the Gemini app.***  <br>  <br>
    Mar 25, Goole [released Gemini 2.5](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/). Gemini 2.5 is Google's latest AI model, designed to handle complex problems with advanced reasoning and coding capabilities. The first release, Gemini 2.5 Pro Experimental, leads benchmarks and ranks #1 on LMArena. These "thinking models" analyze information, draw logical conclusions, and make informed decisions, enhancing performance and accuracy. Building on techniques like reinforcement learning and chain-of-thought prompting, Gemini 2.5 combines an enhanced base model with improved post-training. The model excels in coding, math, and science benchmarks, and is available in Google AI Studio and the Gemini app, with Vertex AI support coming soon. Gemini 2.5 features a 1 million token context window, multimodality, and strong performance across various data types. Developers and enterprises can start experimenting with it now, with pricing details to be announced soon.  <br>  <br>

11. ***Language Models Can Verbatim Complete Text They Weren't Explicitly Trained On.   <br>A study by Google and Stanford has shown that large language models can sometimes complete text verbatim even if those specific sequences were not explicitly present in their training data according to n-gram overlap definitions. The authors demonstrate that this n-gram based definition of training data membership can be gamed, with completion tests succeeding even when target sequences were removed from the training set. This highlights the limitations of relying solely on n-gram overlap to define training data membership.***  <br>  <br>
    Mar 25, Google and Stanford published a [paper](https://arxiv.org/pdf/2503.17514) “Language Models May Verbatim Complete Text They Were Not Explicitly Trained On”. An important question today is whether a given text was used to train a large language model (LLM). A completion test is often employed: check if the LLM completes a sufficiently complex text. This, however, requires a ground-truth definition of membership; most commonly, it is defined as a member based on the n-gram overlap between the target text and any text in the dataset. This study demonstrates that this n-gram based membership definition can be effectively gamed. The authors study scenarios where sequences are non-members for a given n and we find that completion tests still succeed. The study finds many natural cases of this phenomenon by retraining LLMs from scratch after removing all training samples that were completed; these cases include exact duplicates, near-duplicates, and even short overlaps. They showcase that it is difficult to find a single viable choice of n for membership definitions. Using these insights, the work designs adversarial datasets that can cause a given target sequence to be completed without containing it, for any reasonable choice of n. The findings highlight the inadequacy of n-gram membership, suggesting membership definitions fail to account for auxiliary information available to the training algorithm.  <br>  <br>

13. ***Jensen's Lower Bound Enables Reinforcement Learning for Chain-of-Thought Optimization.   <br>Meta researchers have proposed a method to optimize chain-of-thought reasoning in language models using reinforcement learning without an external reward function. The algorithm treats chain-of-thought as a latent variable within a probabilistic inference framework and utilizes a simpler Jensen's lower bound instead of the full evidence lower bound. This approach yields tractable objectives with straightforward algorithmic components, making it suitable for large-scale training and naturally interpolating between supervised fine-tuning and online reinforcement learning, showing effectiveness in mathematical reasoning.***  <br>  <br>
    Mar 25, Meta published a [paper](https://arxiv.org/pdf/2503.19618) “Learning to chain-of-thought with Jensen's evidence lower bound”. The study proposes a way to optimize chain-of-thought with reinforcement learning, but without external reward function. The algorithm relies on viewing chain-of-thought as latent variable as part of a probabilistic inference problem. Contrary to the full evidence lower bound, the study proposes to apply a much simpler Jensen's lower bound, which derives tractable objectives with simple algorithmic components (e.g., without the need for parametric approximate posterior), making it more conducive to modern large-scale training. The lower bound approach naturally interpolates other methods such as supervised fine-tuning and online reinforcement learning, whose practical trade-offs will be illustrated. Finally, the study shows that on mathematical reasoning problems, optimizing with Jensen's lower bound is as effective as policy gradient with external reward. Taken together, the results showcase as a proof of concept to this new algorithmic paradigm's potential to more generic applications.  <br>  <br>

15. ***Vision-Language Models Still Struggle with Real-Time Face-to-Face Question Answering.   <br>A study by Qualcomm and the University of Toronto introduces the Qualcomm Interactive Video Dataset (IVD) to assess the ability of vision-language models to answer questions about live, unfolding scenes in real-time. The research reveals that current models significantly lag behind human performance on this task, identifying key areas for improvement. However, the study also indicates that fine-tuning on this type of interactive video data can substantially reduce the performance gap for many perceptual skills.***  <br>  <br>
    Mar 25, Qualcomm and Uni of Toronto published a [paper](Can Vision-Language Models Answer Face to Face Questions in the Real-World?) “Can Vision-Language Models Answer Face to Face Questions in the Real-World?”. AI models have made significant strides in recent years in their ability to describe and answer questions about real-world images. They have also made progress in the ability to converse with users in real-time using audio input. This raises the question: have people reached the point where AI models, connected to a camera and microphone, can converse with users in real-time about scenes and events that are unfolding live in front of the camera? This has been a long-standing goal in AI and is a prerequisite for real-world AI assistants and humanoid robots to interact with humans in everyday situations. This work introduces a new dataset and benchmark, the Qualcomm Interactive Video Dataset (IVD), which allows to assess the extent to which existing models can support these abilities, and to what degree these capabilities can be instilled through fine-tuning. The dataset is based on a simple question-answering setup, where users ask questions that the system has to answer, in real-time, based on the camera and audio input. The study shows that existing models fall far behind human performance on this task; and identifies the main sources for the performance gap. However, the work also shows that for many of the required perceptual skills, fine-tuning on this form of data can significantly reduce this gap.  <br>  <br>

17. ***Google's Gemma 3 Introduces Multimodality, Extended Context, and Architectural Improvements.   <br>Google has released the Gemma 3 Technical Report, detailing the multimodal addition to the Gemma family of open models, ranging from 1 to 27 billion parameters. Gemma 3 introduces vision understanding, broader language coverage (over 128K tokens), and a new architecture with an increased ratio of local to global attention layers to reduce KV-cache memory usage for long contexts. Trained with distillation, Gemma 3 models outperform Gemma 2, with significant improvements in math, chat, instruction-following, and multilingual abilities.***  <br>  <br> 
    Mar 25, Google published a [paper](https://arxiv.org/pdf/2503.19786) “Gemma 3 Technical Report”. The report introduces Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context - at least 128K tokens. The report also changes the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, the novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. Models are open to the community.  <br>  <br>

19. ***Reasoning to Learn from Latent Thoughts Improves Language Model Pretraining Efficiency.   <br>Researchers from Stanford University, the University of Toronto, and the Vector Institute propose that explicitly modeling and inferring latent thoughts underlying text generation can enhance the data efficiency of language model pretraining in data-constrained scenarios. Their approach views web text as a compressed outcome of human thought processes, with latent thoughts containing crucial contextual knowledge and reasoning steps. Empirical results in math demonstrate that synthetic data approaches for inferring latent thoughts significantly improve data efficiency, and a 1B LM can bootstrap its performance through iterative refinement of thought-augmented pretraining data.***  <br>  <br>
    Mar 24, Stanford Uni, Uni of Toronto and Vector Inst published a [paper](https://arxiv.org/pdf/2503.18866) “Reasoning to Learn from Latent Thoughts”. Compute scaling for language model (LM) pretraining has outpaced the growth of human-written texts, leading to concerns that data will become the bottleneck to LM scaling. To continue scaling pretraining in this data-constrained regime, we propose that explicitly modeling and inferring the latent thoughts that underlie the text generation process can significantly improve pretraining data efficiency. Intuitively, our approach views web text as the compressed final outcome of a verbose human thought process and that the latent thoughts contain important contextual knowledge and reasoning steps that are critical to data-efficient learning. We empirically demonstrate the effectiveness of our approach through data-constrained continued pretraining for math. We first show that synthetic data approaches to inferring latent thoughts significantly improve data efficiency, outperforming training on the same amount of raw data (5.7% → 25.4% on MATH). Furthermore, we demonstrate latent thought inference without a strong teacher, where an LM bootstraps its own performance by using an EM algorithm to iteratively improve the capability of the trained LM and the quality of thought-augmented pretraining data. We show that a 1B LM can bootstrap its performance across at least three iterations and significantly outperform baselines trained on raw data, with increasing gains from additional inference compute when performing the E-step. The gains from inference scaling and EM iterations suggest new opportunities for scaling data-constrained pretraining.  <br>  <br>

21. ***SimpleRL-Zoo Investigates Zero Reinforcement Learning for Diverse Open Base Models.   <br>HKUST, TikTok, and BUPT have explored zero reinforcement learning training, where long chain-of-thought reasoning emerges directly from base language models using rule-based rewards, across ten diverse open base models. The study identifies key design strategies for achieving substantial improvements in reasoning accuracy and response length. Notably, they observed the "aha moment" in small models outside the Qwen family, providing valuable insights and open-sourcing their code, models, and analysis tools.***  <br>  <br>
    Mar 24, HKUST, TikTok and BUPT published a [paper](https://arxiv.org/pdf/2503.18892) “SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild”. DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as people find the base models already exhibit strong instruction-following and self-reflection abilities. This study investigates zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty - the work achieves substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, the study observes that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the "aha moment"). Notably, the work observes the "aha moment" for the first time in small models not from the Qwen family. The study shares the key designs that enable successful zero RL training, along with the findings and practices. To facilitate further research, the authors open-source the code, models, and analysis tools at https://github.com/hkust-nlp/simpleRL-reason  <br>  <br>

23. ***FFN Fusion Optimizes LLM Inference by Parallelizing Feed-Forward Network Layers.   <br>Nvidia has introduced FFN Fusion, an architectural optimization technique that reduces sequential computation in large language models by parallelizing sequences of Feed-Forward Network (FFN) layers, particularly after removing specific attention layers. Applying this to Llama-3.1-405B-Instruct resulted in Llama-Nemotron-Ultra-253B-Base, an efficient model achieving a 1.71x speedup in inference latency and significantly lower per-token cost while maintaining strong benchmark performance.***  <br>  <br>
    Mar 24, Nvidia published a [paper](https://arxiv.org/pdf/2503.18908) “FFN Fusion: Rethinking Sequential Computation in Large Language Models”. The study introduces FFN Fusion, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. The key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining after the removal of specific attention layers, can often be parallelized with minimal accuracy impact. The study develops a principled methodology for identifying and fusing such sequences, transforming them into parallel operations that significantly reduce inference latency while preserving model behavior. Applying these techniques to Llama-3.1-405B-Instruct, the study creates Llama-Nemotron-Ultra-253B-Base (Ultra-253B-Base), an efficient and soon-to-be publicly available model that achieves a 1.71X speedup in inference latency and 35X lower per-token cost while maintaining strong performance across benchmarks. Through extensive experiments on models from 49B to 253B parameters, the work demonstrates that FFN Fusion becomes increasingly effective at larger scales and can complement existing optimization techniques like quantization and pruning. Most intriguingly, the work finds that even full transformer blocks containing both attention and FFN layers can sometimes be parallelized, suggesting new directions for neural architecture design.  <br>  <br>

25. ***xKV: Cross-Layer SVD for Efficient KV-Cache Compression in Long-Context LLMs.   <br>Researchers from Cornell University, the University of Washington, and NYMCT University have proposed xKV, a post-training method for compressing the KV-Cache in large language models with long context windows. xKV applies Singular Value Decomposition (SVD) on the KV-Cache of grouped layers, consolidating it into a shared low-rank subspace. Evaluations on long-context benchmarks show xKV achieving higher compression rates with improved accuracy compared to existing inter-layer techniques and demonstrating compatibility with Multi-Head Latent Attention.***  <br>  <br>
    Mar 24, Cornell Uni, Uni of Washington and NYMCT Uni published a [paper](https://arxiv.org/pdf/2503.18893) “xKV: Cross-Layer SVD for KV-Cache Compression”. Large Language Models (LLMs) with long context windows enable powerful applications but come at the cost of high memory consumption to store the Key and Value states (KV-Cache). Recent studies attempted to merge KV-cache from multiple layers into shared representations, yet these approaches either require expensive pretraining or rely on assumptions of high per-token cosine similarity across layers which generally does not hold in practice. The work finds that the dominant singular vectors are remarkably well-aligned across multiple layers of the KV-Cache. Exploiting this insight, the study proposes xKV, a simple post-training method that applies Singular Value Decomposition (SVD) on the KV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers into a shared low-rank subspace, significantly reducing KV-Cache sizes. Through extensive evaluations on the RULER long-context benchmark with widely-used LLMs (e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates than state-of-the-art inter-layer technique while improving accuracy by 2.7%. Moreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA) (e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding tasks without performance degradation. These results highlight xKV's strong capability and versatility in addressing memory bottlenecks for long-context LLM inference. Code is publicly available at: https://github.com/abdelfattah-lab/xKV.  <br>  <br>

27. ***AgentRxiv: A Framework for Collaborative Autonomous Research with LLM Agents.   <br>Johns Hopkins University and ETH have introduced AgentRxiv, a framework enabling LLM agent laboratories to collaborate on research by uploading and retrieving reports from a shared preprint server. Experiments show that agents with access to their prior research perform better, and multiple agent laboratories sharing research through AgentRxiv achieve higher overall accuracy, suggesting a potential role for autonomous agents in future AI system design.***  <br>  <br>
    Mar 23, Johns Hopkins Uni and ETH published a [paper](https://arxiv.org/pdf/2503.18102) “AgentRxiv: Towards Collaborative Autonomous Research”. Progress in scientific discovery is rarely the result of a single "Eureka" moment, but is rather the product of hundreds of scientists incrementally working together toward a common goal. While existing agent workflows are capable of producing research autonomously, they do so in isolation, without the ability to continuously improve upon prior research results. To address these challenges, the study introduces AgentRxiv-a framework that lets LLM agent laboratories upload and retrieve reports from a shared preprint server in order to collaborate, share insights, and iteratively build on each other's research. The work tasks agent laboratories to develop new reasoning and prompting techniques and find that agents with access to their prior research achieve higher performance improvements compared to agents operating in isolation (11.4% relative improvement over baseline on MATH-500). The study finds that the best performing strategy generalizes to benchmarks in other domains (improving on average by 3.3%). Multiple agent laboratories sharing research through AgentRxiv are able to work together towards a common goal, progressing more rapidly than isolated laboratories, achieving higher overall accuracy (13.7% relative improvement over baseline on MATH-500). These findings suggest that autonomous agents may play a role in designing future AI systems alongside humans. The authors hope that AgentRxiv allows agents to collaborate toward research goals and enables researchers to accelerate discovery. https://github.com/SamuelSchmidgall/AgentLaboratory  <br>  <br>

29. ***Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models.   <br>MIT and Google researchers have found that large language models do not update their beliefs according to Bayesian principles. To address this, they propose Bayesian Teaching, training LLMs to mimic the predictions of an optimal Bayesian model. This approach significantly improves performance on recommendation tasks and enables generalization to other tasks, suggesting that LLMs can learn and generalize reasoning strategies effectively.***  <br>  <br>
    Mar 21, MIT and Google published a [paper](https://arxiv.org/pdf/2503.17523) “Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models”. Artificial intelligence systems based on large language models (LLMs) are increasingly used as agents that interact with users and with the world. To do so successfully, LLMs need to construct internal representations of the world and form probabilistic beliefs about those representations. To provide a user with personalized recommendations, for example, the LLM needs to gradually infer the user's preferences, over the course of multiple interactions. To evaluate whether contemporary LLMs are able to do so, the study uses the Bayesian inference framework from probability theory, which lays out the optimal way to update an agent's beliefs as it receives new information. The study first shows that the LLMs do not update their beliefs as expected from the Bayesian framework, and that consequently their predictions do not improve as expected as more information becomes available, even less so than the authors find is the case for humans. To address this issue, the authors teaches the LLMs to reason in a Bayesian manner by training them to mimic the predictions of an optimal Bayesian model. The study finds that this approach not only significantly improves the LLM's performance on the particular recommendation task it is trained on, but also enables generalization to other tasks. This suggests that this method endows the LLM with broader Bayesian reasoning skills. More generally, the results indicate that LLMs can learn about reasoning strategies effectively and generalize those skills to new domains, which in part explains LLMs' empirical success.  <br>  <br>

31. ***Reward Features Enable Capturing Individual Human Preferences in LLM Training.   <br>Google researchers argue that standard reinforcement learning from human feedback models preferences without considering individual differences. They propose a method to specialize reward models to specific individuals or groups by capturing preferences as a linear combination of general reward features. Experiments with large language models show that this approach either significantly outperforms non-adaptive and other adaptive baselines or matches their performance with a simpler and more stable architecture, especially in scenarios with high disagreement.***  <br>  <br>
    Mar 21, Google published a [paper](https://www.arxiv.org/pdf/2503.17338) “Capturing Individual Human Preferences with Reward Features”. Reinforcement learning from human feedback usually models preferences using a reward model that does not distinguish between people. The study argues that this is unlikely to be a good design choice in contexts with high potential for disagreement, like in the training of large language models. The work proposes a method to specialise a reward model to a person or group of people. The approach builds on the observation that individual preferences can be captured as a linear combination of a set of general reward features. The study shows how to learn such features and subsequently use them to quickly adapt the reward model to a specific individual, even if their preferences are not reflected in the training data. The authors present experiments with large language models comparing the proposed architecture with a non-adaptive reward model and also adaptive counterparts, including models that do in-context personalisation. Depending on how much disagreement there is in the training data, the model either significantly outperforms the baselines or matches their performance with a simpler architecture and more stable training.  <br>  <br>

33. ***Curriculum Extraction from a Fully Trained Teacher Enables Efficient Knowledge Distillation.   <br>Researchers from the University of Texas at Austin and Microsoft have shown that a curriculum for efficient knowledge distillation can be extracted from just the fully trained teacher network, offering similar benefits to progressive distillation without needing to store intermediate checkpoints. Their method uses a random projection of the teacher's hidden representations to progressively train the student network before using the full network output, outperforming one-shot distillation and achieving comparable performance to progressive distillation.***  <br>  <br>
    Mar 21, Uni of Texas at Austin and Microsoft published a [paper](https://www.arxiv.org/pdf/2503.17494) “Efficient Knowledge Distillation via Curriculum Extraction”. Knowledge distillation is a technique used to train a small student network using the output generated by a large teacher network, and has many empirical advantages. While the standard one-shot approach to distillation only uses the output of the final teacher network, recent work has shown that using intermediate checkpoints from the teacher's training process as an implicit “curriculum” for progressive distillation can significantly speed up training. However, such schemes require storing these checkpoints, and often require careful selection of the intermediate checkpoints to train on, which can be impractical for large-scale training. This study shows that a curriculum can be extracted from just the fully trained teacher network, and that this extracted curriculum can give similar efficiency benefits to those of progressive distillation. The extraction scheme is natural; the authors use a random projection of the hidden representations of the teacher network to progressively train the student network, before training using the output of the full network. The study shows that the scheme significantly outperforms one-shot distillation and achieves a performance similar to that of progressive distillation for learning sparse parities with two-layer networks, and provide theoretical guarantees for this setting. Additionally, the study shows that the method outperforms one-shot distillation even when using transformer-based architectures, both for sparse-parity learning, and language modeling tasks.  <br>  <br>

35. ***Weight Rescaling Techniques Improve Variance Control in LLM Pre-training.   <br>BluOrion has introduced Layer Index Rescaling (LIR) and Target Variance Rescaling (TVR), novel weight initialization and variance control strategies for large language model pre-training. Experiments on a 1B parameter LLaMA model demonstrate that better variance management using these techniques leads to substantial improvements in downstream task performance and reduces extreme activation values, mitigating challenges related to quantization and low-precision training.***  <br>  <br>
    Mar 21, BluOrion published a [paper](https://arxiv.org/pdf/2503.17500) “Variance Control via Weight Rescaling in LLM Pre-training”. The outcome of Large Language Model (LLM) pre-training strongly depends on weight initialization and variance control strategies. Although the importance of initial variance control has been well documented in neural networks in general, the literature on initialization and management of its growth during LLM pre-training, specifically, is somewhat sparse. This work introduces the Layer Index Rescaling (LIR) weight initialization scheme, and the Target Variance Rescaling (TVR) variance control strategy. Experiments on a 1B parameter LLaMA model demonstrate that better variance management using these techniques yields substantial improvements in downstream task performance (up to 4.6% on common pre-training benchmarks) and reduces extreme activation values, thus mitigating challenges associated with quantization and low-precision training. Code is available at: https://github.com/bluorion-com/weight_rescaling.  <br>  <br>

37. ***The KoLMogorov Test: Compression by Code Generation as an Intelligence Benchmark.   <br>Meta and Tel Aviv University have introduced the KoLMogorov-Test (KT), a compression-as-intelligence test for code-generating LLMs. KT challenges models to generate the shortest program that outputs a given sequence of data. Evaluation using audio, text, DNA, and synthetic program outputs reveals that current flagship models perform poorly, suggesting that new innovations are needed to better approximate Kolmogorov compression.***  <br>  <br>
    Mar 18, Meta and Tel Aviv Uni published a [paper](https://arxiv.org/pdf/2503.13992v1) “The KoLMogorov Test: Compression by Code Generation”. Compression is at the heart of intelligence. A theoretically optimal way to compress any sequence of data is to find the shortest program that outputs that sequence and then halts. However, such Kolmogorov compression is uncomputable, and code generating LLMs struggle to approximate this theoretical ideal, as it requires reasoning, planning and search capabilities beyond those of current models. In this work, we introduce the KoLMogorov-Test (KT), a compression-as-intelligence test for code generating LLMs. In KT a model is presented with a sequence of data at inference time, and asked to generate the shortest program that produces the sequence. The study identifies several benefits of KT for both evaluation and training: an essentially infinite number of problem instances of varying difficulty is readily available, strong baselines already exist, the evaluation metric (compression) cannot be gamed, and pretraining data contamination is highly unlikely. To evaluate current models, the study uses audio, text, and DNA data, as well as sequences produced by random synthetic programs. Current flagship models perform poorly – both GPT4-o and Llama-3.1-405B struggle on the natural and synthetic sequences. On the synthetic distribution, the authors are able to train code generation models with lower compression rates than previous approaches. Moreover, the study shows that gains on synthetic data generalize poorly to real data, suggesting that new innovations are necessary for additional gains on KT.  <br>  <br>

39. ***A Multi-Modal Multi-Agent Framework for Enhanced Document Understanding.   <br>Researchers from UNC-Chapel Hill and Adobe have presented MDocAgent, a novel retrieval-augmented generation and multi-agent framework for Document Question Answering (DocQA). MDocAgent integrates both textual and visual cues from documents using five specialized agents that collaborate to achieve a more comprehensive understanding, leading to improved accuracy on multi-modal document understanding benchmarks.***  <br>  <br>
    Mar 18, UNC-Chapel Hill and Adobe published a [paper](https://arxiv.org/pdf/2503.13964) “MDocAgent: A Multi-Modal Multi-Agent Framework for Document Understanding”. Document Question Answering (DocQA) is a very common task. Existing methods using Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and Retrieval Augmented Generation (RAG) often prioritize information from a single modal, failing to effectively integrate textual and visual cues. These approaches struggle with complex multi-modal reasoning, limiting their performance on real-world documents. The study presents MDocAgent (A Multi-Modal Multi-Agent Framework for Document Understanding), a novel RAG and multi-agent framework that leverages both text and image. The system employs five specialized agents: a general agent, a critical agent, a text agent, an image agent and a summarizing agent. These agents engage in multi-modal context retrieval, combining their individual insights to achieve a more comprehensive understanding of the document's content. This collaborative approach enables the system to synthesize information from both textual and visual components, leading to improved accuracy in question answering. Preliminary experiments on five benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of MDocAgent, achieve an average improvement of 12.1% compared to current state-of-the-art method. This work contributes to the development of more robust and comprehensive DocQA systems capable of handling the complexities of real-world documents containing rich textual and visual information. https://github.com/aiming-lab/mdocagent


