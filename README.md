# Weekly AI-News
Welcome to the Weekly-AI-news!
I mainly focuses more on Generative AI, LLMs and the related, so may miss import AI news in other areas. <br><br>

***29 Dec***

1. ***OpenAI's Shift to a Traditional For-Profit Structure:  <br>OpenAI plans to transition to a traditional for-profit structure in 2025 to attract more investment, moving away from its complex nonprofit-for-profit hybrid model. The new structure will likely be a public benefit corporation, ensuring a balance between profit generation and societal benefit. This change aims to simplify fundraising efforts and support OpenAI's long-term goals, including achieving artificial general intelligence (AGI), while staying competitive against rivals like Meta and Anthropic.*** <br> <br>
   Dec 27, according to [fortune](https://fortune.com/2024/12/27/openai-for-profit-non-profit-company-investors/), OpenAI has announced plans to transition to a more traditional for-profit company structure in 2025 to attract more investor funding. Currently, a nonprofit controls a for-profit arm, which in turn controls a holding company that oversees another for-profit entity. This complex structure has hindered fundraising efforts. The new entity will likely be a public benefit corporation, which aims to generate profit while also providing a public benefit. The nonprofit will continue to exist but will no longer have a controlling role. OpenAI's current structure, established in 2019, is seen as a disadvantage in the competitive AI market, especially against rivals like Meta and Anthropic. Despite raising $6.6 billion recently, OpenAI needs more capital to support its growth and ambitions, including achieving artificial general intelligence (AGI). The company acknowledges that to secure the necessary funding, it must offer conventional equity and reduce structural complexities. OpenAI aims to evolve into an enduring company that contributes to building the AGI economy and ensuring its benefits for humanity. <br> <br>

3. ***Meta's Approach to Improving Factuality in Text Generation:  <br>Meta introduces the "Explicit Working Memory" (EWE) method to improve the factuality of large language models (LLMs). EWE integrates real-time feedback from external resources, refreshing memory to rectify inaccuracies during generation. Experiments show that EWE enhances factuality without sacrificing helpfulness, outperforming existing models on key datasets by improving factuality scores and demonstrating the importance of memory updates and retrieval quality.*** <br> <br>
   Dec 24, Meta published a [paper](https://arxiv.org/pdf/2412.18069) “Improving Factuality with Explicit Working Memory”. Large language models can generate factually inaccurate content, a problem known as hallucination. Recent works have built upon retrieved-augmented generation to improve factuality through iterative prompting but these methods are limited by the traditional RAG design. To address these challenges, the study introduces EWE (Explicit Working Memory), a novel approach that enhances factuality in long-form text generation by integrating a working memory that receives real-time feedback from external resources. The memory is refreshed based on online fact-checking and retrieval feedback, allowing EWE to rectify false claims during the generation process and ensure more accurate and reliable outputs. Experiments demonstrate that Ewe outperforms strong baselines on four fact-seeking long-form generation datasets, increasing the factuality metric, VeriScore, by 2 to 10 points absolute without sacrificing the helpfulness of the responses. Further analysis reveals that the design of rules for memory updates, configurations of memory units, and the quality of the retrieval datastore are crucial factors for influencing model performance. <br> <br>

5. ***Advancing Artificial Life with Foundation Models:  <br>A collaboration between MIT, OpenAI, and others presents a method for automating the search for artificial life (ALife) using foundation models (FMs). The paper introduces "Automated Search for Artificial Life" (ASAL), which uses FMs to explore large combinatorial spaces and generate novel ALife simulations. The approach reveals new lifeforms and opens new avenues in ALife research by using FMs to quantify phenomena previously viewed qualitatively.*** <br> <br>
   Dec 23, MIT, Sakana AI, OpenAI and others published a [paper](https://arxiv.org/pdf/2412.17799) “Automating the Search for Artificial Life with Foundation Models”. With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial-and-error to discover the configurations of lifelike simulations. This paper presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called Automated Search for Artificial Life (ASAL), (1) finds simulations that produce target phenomena, (2) discovers simulations that generate temporally open-ended novelty, and (3) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates including Boids, Particle Life, Game of Life, Lenia, and Neural Cellular Automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids lifeforms, as well as cellular automata that are open-ended like Conway's Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone. <br> <br>

7. ***ResearchTown Simulates Human Research Communities:  <br>A study from UIUC introduces ResearchTown, a multi-agent framework that simulates human research communities, leveraging LLMs to model collaborative activities such as paper writing and reviewing. The framework uses TextGNN for research simulations and ResearchBench for evaluating the simulation's realism. Results show that ResearchTown can generate interdisciplinary research ideas and provide a robust model of research community dynamics.*** <br> <br>
   Dec 23, UIUC published a [paper](https://arxiv.org/pdf/2412.17767) “ResearchTown: Simulator of Human Research Community”. Large Language Models (LLMs) have demonstrated remarkable potential in scientific domains, yet a fundamental question remains unanswered: Can we simulate human research communities with LLMs? Addressing this question can deepen our understanding of the processes behind idea brainstorming and inspire the automatic discovery of novel scientific insights. This study proposes ResearchTown, a multi-agent framework for research community simulation. Within this framework, the human research community is simplified and modeled as an agent-data graph, where researchers and papers are represented as agent-type and data-type nodes, respectively, and connected based on their collaboration relationships. The study also introduces TextGNN, a text-based inference framework that models various research activities (e.g., paper reading, paper writing, and review writing) as special forms of a unified message-passing process on the agent-data graph. To evaluate the quality of the research simulation, the work presents ResearchBench, a benchmark that uses a node-masking prediction task for scalable and objective assessment based on similarity. Experiments reveal three key findings: (1) ResearchTown can provide a realistic simulation of collaborative research activities, including paper writing and review writing; (2) ResearchTown can maintain robust simulation with multiple researchers and diverse papers; (3) ResearchTown can generate interdisciplinary research ideas that potentially inspire novel research directions. <br> <br>

9. ***Google’s Differentiable Cache Augmentation for LLMs:  <br>Google’s research demonstrates how a frozen large language model (LLM) can be enhanced by an offline coprocessor that augments the model's key-value (kv) cache. This technique improves the model’s reasoning abilities by refining its cache with latent embeddings, enhancing subsequent decoding tasks. The approach improves performance across reasoning-intensive tasks without requiring task-specific training, showcasing a novel and efficient method for optimizing LLMs.*** <br> <br>
    Dec 23, Google published a [paper](https://arxiv.org/pdf/2412.17747) “Deliberation in Latent Space via Differentiable Cache Augmentation”. Techniques enabling large language models (LLMs) to "think more" by generating and attending to intermediate reasoning steps have shown promise in solving complex problems. However, the standard approaches generate sequences of discrete tokens immediately before responding, and so they can incur significant latency costs and be challenging to optimize. This study demonstrates that a frozen LLM can be augmented with an offline coprocessor that operates on the model's key-value (kv) cache. This coprocessor augments the cache with a set of latent embeddings designed to improve the fidelity of subsequent decoding. The study trains this coprocessor using the language modeling loss from the decoder on standard pretraining data, while keeping the decoder itself frozen. This approach enables the model to learn, in an end-to-end differentiable fashion, how to distill additional computation into its kv-cache. Because the decoder remains unchanged, the coprocessor can operate offline and asynchronously, and the language model can function normally if the coprocessor is unavailable or if a given cache is deemed not to require extra computation. The study shows experimentally that when a cache is augmented, the decoder achieves lower perplexity on numerous subsequent tokens. Furthermore, even without any task-specific training, experiments demonstrate that cache augmentation consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks. <br> <br>

11. ***OpenAI's o1 Model Focuses on Safety and Robustness:  <br>OpenAI's o1 system card outlines the capabilities of the o1 and o1-mini models, which utilize chain-of-thought reasoning to improve safety and robustness. These models offer state-of-the-art performance in addressing risks like generating illicit advice and avoiding stereotypes. The card emphasizes the importance of robust alignment methods and extensive safety evaluations to balance the benefits of enhanced intelligence with potential risks.*** <br> <br>
    Dec 22, OpenAI released it o1 [system card](https://arxiv.org/pdf/2412.16720) “OpenAI o1 System Card”. The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of the models. In particular, the models can reason about the safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. The results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations. <br> <br>

13. ***Persuasion and Deception in LLMs:  <br>A paper from UC San Diego explores the persuasive and deceptive capabilities of LLMs, highlighting their potential for generating convincing, yet false content. The review examines recent studies on LLMs' persuasive effects and deceptive outputs, analyzing theoretical risks and evaluating possible mitigation strategies. The paper also presents key open questions about the future impact of AI-driven persuasion and truthfulness in AI-generated content.*** <br> <br>
    Dec 22, Uni of California San Diego published a [paper](https://arxiv.org/pdf/2412.17128) “Lies, Damned Lies, and Distributional Language Statistics Persuasion and Deception with Large Language Models”. Large Language Models (LLMs) can generate content that is as persuasive as human-written text and appear capable of selectively producing deceptive outputs. These capabilities raise concerns about potential misuse and unintended consequences as these systems become more widely deployed. This review synthesizes recent empirical work examining LLMs' capacity and proclivity for persuasion and deception, analyzes theoretical risks that could arise from these capabilities, and evaluates proposed mitigations. While current persuasive effects are relatively small, various mechanisms could increase their impact, including fine-tuning, multimodality, and social factors. The authors outline key open questions for future research, including how persuasive AI systems might become, whether truth enjoys an inherent advantage over falsehoods, and how effective different mitigation strategies may be in practice. <br> <br>

15. ***Google’s LearnLM for Educational AI:  <br>Google's LearnLM study focuses on improving generative AI for educational purposes by introducing pedagogical instruction following in LLMs. The approach helps customize AI behavior to meet specific pedagogical goals, allowing models like Gemini to perform better in learning scenarios. Results show that LearnLM outperforms other models like GPT-4 and Claude in expert evaluations, paving the way for more effective AI tutors.*** <br> <br>
    Dec 21, Google published a [paper](https://arxiv.org/pdf/2412.16429) “LearnLM Improving Gemini for Learning”. Today's generative AI systems are tuned to present information by default rather than engage users in service of learning as a human tutor would. To address the wide range of potential education use cases for these systems, the study reframes the challenge of injecting pedagogical behavior as one of pedagogical instruction following, where training and evaluation examples include system-level instructions describing the specific pedagogy attributes present or desired in subsequent model turns. This framing avoids committing the models to any particular definition of pedagogy, and instead allows teachers or developers to specify desired model behavior. It also clears a path to improving Gemini models for learning -- by enabling the addition of the pedagogical data to post-training mixtures -- alongside their rapidly expanding set of capabilities. Both represent important changes from the initial tech report. The study shows how training with pedagogical instruction following produces a LearnLM model (available on Google AI Studio) that is preferred substantially by expert raters across a diverse set of learning scenarios, with average preference strengths of 31% over GPT-4o, 11% over Claude 3.5, and 13% over the Gemini 1.5 Pro model LearnLM was based on. <br> <br>

17. ***Meta’s Memory Layers for Enhanced Model Performance:  <br>Meta’s research on memory layers shows how augmenting models with trainable key-value lookup mechanisms can improve performance without increasing computational complexity. The study finds that models with memory layers outperform dense models in factual tasks, and the improved memory layer implementation scales efficiently. The work demonstrates that memory layers can enhance model performance with fewer resources, offering a promising direction for large-scale models.*** <br> <br>
    Dec 20, Meta published a [paper](https://arxiv.org/pdf/2412.09764) “Memory Layers at Scale”. Memory layers use a trainable key-value lookup mechanism to add extra parameters to a model without increasing FLOPs. Conceptually, sparsely activated memory layers complement compute-heavy dense feed-forward layers, providing dedicated capacity to store and retrieve information cheaply. This work takes memory layers beyond proof-of-concept, proving their utility at contemporary scale. On downstream tasks, language models augmented with the improved memory layer outperform dense models with more than twice the computation budget, as well as mixture-of-expert models when matched for both compute and parameters. The work finds gains are especially pronounced for factual tasks. The study provides a fully parallelizable memory layer implementation, demonstrating scaling laws with up to 128B memory parameters, pretrained to 1 trillion tokens, comparing to base models with up to 8B parameters. <br> <br>

19. ***OpenAI's o3 Achieves Major Milestone in AGI Research:  <br>OpenAI’s o3 model has achieved significant advancements in the ARC-AGI-1 benchmark, scoring over 75% on the Semi-Private Evaluation set. This milestone represents a breakthrough in AI’s ability to handle novel tasks, marking a qualitative shift in the field. The performance improvements highlight the importance of new architectural ideas for AGI development, suggesting that the future of AI will rely on innovation beyond just scaling existing models.*** <br> <br>
    Dec 20, according to [arc.prize](https://arcprize.org/blog/oai-o3-pub-breakthrough), OpenAI's new o3 system has achieved a significant milestone by scoring 75.7% on the Semi-Private Evaluation set of the ARC-AGI-1 Public Training set, within the $10k compute limit. A high-compute configuration of o3 scored even higher at 87.5%. This marks a substantial leap in AI capabilities, showcasing an unprecedented ability to adapt to novel tasks, a feat not seen in previous GPT-family models. The ARC-AGI-1 benchmark, which took four years to progress from 0% with GPT-3 to 5% with GPT-4o, now sees a dramatic improvement with o3. This breakthrough suggests a need to update our understanding of AI capabilities. The ARC Prize aims to guide the development of AGI, with plans to launch ARC-AGI-2 in 2025, continuing to push the boundaries of AI research. The o3 model's success underscores the importance of new architectural ideas over merely scaling existing models, indicating a qualitative shift in AI's ability to handle novel tasks. <br> <br>

21. ***Formal Mathematical Reasoning:  <br>A New Frontier in AI: A paper co-authored by multiple universities advocates for the integration of formal mathematical reasoning in AI, particularly for mathematics and theorem proving. The authors argue that formal systems, such as proof assistants, are essential for advancing AI’s role in mathematics and other fields. Despite progress in AI for formal reasoning, significant challenges remain, and the paper calls for continued collaboration to drive advancements in AI4Math.*** <br> <br>
    Dec 20, Meta, Stanford Uni, UC Berkeley, Uni of Edinburgh and UT Austin published a [paper](https://arxiv.org/pdf/2412.16075) “Formal Mathematical Reasoning A New Frontier in AI”. AI for Mathematics (AI4Math) is not only intriguing intellectually but also crucial for AI-driven discovery in science, engineering, and beyond. Extensive efforts on AI4Math have mirrored techniques in NLP, in particular, training large language models on carefully curated math datasets in text form. As a complementary yet less explored avenue, formal mathematical reasoning is grounded in formal systems such as proof assistants, which can verify the correctness of reasoning and provide automatic feedback. In this position paper, the authors advocate for formal mathematical reasoning and argue that it is indispensable for advancing AI4Math to the next level. In recent years, people have seen steady progress in using AI to perform formal reasoning, including core tasks such as theorem proving and autoformalization, as well as emerging applications such as verifiable generation of code and hardware designs. However, significant challenges remain to be solved for AI to truly master mathematics and achieve broader impact. The work summarizes existing progress, discuss open challenges, and envision critical milestones to measure future success. At this inflection point for formal mathematical reasoning, the authors call on the research community to come together to drive transformative advancements in this field. <br> <br>

23. ***SKETCH: Enhancing RAG Systems with Knowledge Graphs:  <br>The SKETCH methodology enhances Retrieval-Augmented Generation (RAG) systems by integrating semantic text retrieval with knowledge graphs, improving the model's contextual understanding. SKETCH outperforms traditional RAG methods on multiple datasets, achieving higher relevancy, precision, and context accuracy. This approach sets new benchmarks for RAG systems, offering a more holistic and efficient method for generating accurate and contextually relevant responses.*** <br> <br>
    Dec 20, Northeastern Uni, Stanford Uni, Amazon and Meta published a [paper](https://arxiv.org/pdf/2412.15443) “SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic Retrieval”. Retrieval-Augmented Generation (RAG) systems have become pivotal in leveraging vast corpora to generate informed and contextually relevant responses, notably reducing hallucinations in Large Language Models. Despite significant advancements, these systems struggle to efficiently process and retrieve information from large datasets while maintaining a comprehensive understanding of the context. This paper introduces SKETCH, a novel methodology that enhances the RAG retrieval process by integrating semantic text retrieval with knowledge graphs, thereby merging structured and unstructured data for a more holistic comprehension. SKETCH, demonstrates substantial improvements in retrieval performance and maintains superior context integrity compared to traditional methods. Evaluated across four diverse datasets: QuALITY, QASPER, NarrativeQA, and Italian Cuisine-SKETCH consistently outperforms baseline approaches on key RAGAS metrics such as answer_relevancy, faithfulness, context_precision and context_recall. Notably, on the Italian Cuisine dataset, SKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99, representing the highest performance across all evaluated metrics. These results highlight SKETCH's capability in delivering more accurate and contextually relevant responses, setting new benchmarks for future retrieval systems. <br> <br>

25. ***Advancing Summarization with Multiple Models <br>
The study introduces a Multi-LLM summarization framework, exploring centralized and decentralized approaches. In both strategies, multiple LLMs generate diverse summaries, but evaluation differs: centralized uses one LLM for selection, while decentralized uses multiple models for evaluation. The study finds that multi-LLM strategies outperform single-model baselines by up to 3x, highlighting their effectiveness in improving summarization tasks.*** <br> <br>
    Dec 20, Uni of California Santa Cruz and Adobe Research published a [paper](https://arxiv.org/pdf/2412.15487) “Multi-LLM Text Summarization”. This study proposes a Multi-LLM summarization framework, and investigates two different multi-LLM strategies including centralized and decentralized. The multi-LLM summarization framework has two fundamentally important steps at each round of conversation: generation and evaluation. These steps are different depending on whether the multi-LLM decentralized summarization is used or centralized. In both the multi-LLM decentralized and centralized strategies, the study has k different LLMs that generate diverse summaries of the text. However, during evaluation, the multi-LLM centralized summarization approach leverages a single LLM to evaluate the summaries and select the best one whereas k LLMs are used for decentralized multi-LLM summarization. Overall, the study finds that the multi-LLM summarization approaches significantly outperform the baselines that leverage only a single LLM by up to 3x. These results indicate the effectiveness of multi-LLM approaches for summarization. <br> <br>

27. ***Optimizing LLM Efficiency with Mixed-Precision <br>
This paper addresses limitations in existing quantization methods for LLMs, introducing MixLLM, which uses mixed-precision quantization to improve memory efficiency without sacrificing accuracy. By identifying high-salience output features, MixLLM allocates appropriate bit-widths to maintain accuracy while reducing memory consumption. Experimental results demonstrate significant improvements in both accuracy and system efficiency, achieving state-of-the-art performance in quantization.*** <br> <br>
    Dec 19, Microsoft published a [paper](https://arxiv.org/pdf/2412.14590) “MixLLM LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design”. Quantization has become one of the most effective methodologies to compress LLMs into smaller size. However, the existing quantization solutions still show limitations of either non-negligible accuracy drop or system inefficiency. This study makes a comprehensive analysis of the general quantization principles on their effect to the triangle of accuracy, memory consumption and system efficiency. The study proposes MixLLM that explores the new optimization space of mixed-precision quantization between output features based on the insight that different output features matter differently in the model. MixLLM identifies the output features with high salience in the global view rather than within each single layer, effectively assigning the larger bit-width to output features that need it most to achieve good accuracy with low memory consumption. The study presents the sweet spot of quantization configuration of algorithm-system co-design that leads to high accuracy and system efficiency. To address the system challenge, the work designs the two-step dequantization to make use of the int8 Tensor Core easily and fast data type conversion to reduce dequantization overhead significantly, and present the software pipeline to overlap the memory access, dequantization and the MatMul to the best. Extensive experiments show that with only 10% more bits, the PPL increasement can be reduced from about 0.5 in SOTA to within 0.2 for Llama 3.1 70B, while on average MMLU-Pro improves by 0.93 over the SOTA of three popular models. In addition to its superior accuracy, MixLLM also achieves state-of-the-art system efficiency. <br> <br>

29. ***Strategic Data Selection for Improved Pretraining <br>
The paper explores two-phase pretraining for LLMs, focusing on optimal data selection and mixing strategies. This approach outperforms random token distribution by 3.4% to 17% in accuracy. The study offers detailed guidance on constructing effective data blends, showing how this method scales across different token horizons and model sizes, and offers insights for practitioners on creating robust data training processes.*** <br> <br>
    Dec 18, Nvidia, Stanford Uni and Boston Uni published a [paper](https://arxiv.org/pdf/2412.15285) “Maximize Your Data's Potential: Enhancing LLM Accuracy with Two-Phase Pretraining”. Pretraining large language models effectively requires strategic data selection, blending and ordering. However, key details about data mixtures especially their scalability to longer token horizons and larger model sizes remain underexplored due to limited disclosure by model developers. To address this, the study formalizes the concept of two-phase pretraining and conduct an extensive systematic study on how to select and mix data to maximize model accuracies for the two phases. The findings illustrate that a two-phase approach for pretraining outperforms random data ordering and natural distribution of tokens by 3.4% and 17% on average accuracies. The study provides in-depth guidance on crafting optimal blends based on quality of the data source and the number of epochs to be seen. The study proposes to design blends using downsampled data at a smaller scale of 1T tokens and then demonstrate effective scaling of the approach to larger token horizon of 15T tokens and larger model size of 25B model size. These insights provide a series of steps practitioners can follow to design and scale their data blends. <br> <br>

31. ***mproving LLM Performance with Inference-Aware Fine-Tuning <br>
The study introduces a new fine-tuning approach for Best-of-N (BoN) sampling, optimizing LLM performance during inference. By fine-tuning models specifically for the BoN strategy, the study demonstrates that models improve in both response quality and computational efficiency. This method results in improved accuracy on multiple tasks, such as the Hendrycks MATH and HumanEval benchmarks, indicating the potential of BoN-aware fine-tuning for LLMs.*** <br> <br>
    Dec 18, Google published a [paper](https://arxiv.org/pdf/2412.15287) “Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models”. Recent studies have indicated that effectively utilizing inference-time compute is crucial for attaining better performance from large language models (LLMs). This work proposes a novel inference-aware fine-tuning paradigm, in which the model is fine-tuned in a manner that directly optimizes the performance of the inference-time strategy. The work studies this paradigm using the simple yet effective Best-of-N (BoN) inference strategy, in which a verifier selects the best out of a set of LLM-generated responses. The work devises the first imitation learning and reinforcement learning~(RL) methods for BoN-aware fine-tuning, overcoming the challenging, non-differentiable argmax operator within BoN. The authors empirically demonstrate that the BoN-aware models implicitly learn a meta-strategy that interleaves best responses with more diverse responses that might be better suited to a test-time input -- a process reminiscent of the exploration-exploitation trade-off in RL. Experiments demonstrate the effectiveness of BoN-aware fine-tuning in terms of improved performance and inference-time compute. In particular, the study shows that the methods improve the Bo32 performance of Gemma 2B on Hendrycks MATH from 26.8% to 30.8%, and pass@32 from 60.0% to 67.0%, as well as the pass@16 on HumanEval from 61.6% to 67.1%. <br> <br>

33. ***Enhancing LLMs with Optimized Preference Learning <br>
This paper investigates how preference learning techniques can optimize LLM performance on instruction-following tasks. Using a synthetic data pipeline, the authors study how different factors, such as response contrast and training prompt complexity, influence alignment. The findings suggest that moderate difficulty in training prompts and high-contrast preference pairs improve generalization, offering a scalable approach to enhancing LLM alignment for complex tasks.*** <br> <br>
    Dec 18, Meta and Uni of Washington published a [paper](https://arxiv.org/pdf/2412.15282) “A Systematic Examination of Preference Learning through the Lens of Instruction-Following”. Preference learning is a widely adopted post-training technique that aligns large language models (LLMs) to human preferences and improves specific downstream task capabilities. This work systematically investigates how specific attributes of preference datasets affect the alignment and downstream performance of LLMs in instruction-following tasks. The study uses a novel synthetic data generation pipeline to generate 48,000 unique instruction-following prompts with combinations of 23 verifiable constraints that enable fine-grained and automated quality assessments of model responses. With the synthetic prompts, the work uses two preference dataset curation methods - rejection sampling (RS) and Monte Carlo Tree Search (MCTS) - to obtain pairs of (chosen, rejected) responses. Then, the authors perform experiments investigating the effects of (1) the presence of shared prefixes between the chosen and rejected responses, (2) the contrast and quality of the chosen, rejected responses and (3) the complexity of the training prompts. Experiments reveal that shared prefixes in preference pairs, as generated by MCTS, provide marginal but consistent improvements and greater stability across challenging training configurations. High-contrast preference pairs generally outperform low-contrast pairs; however, combining both often yields the best performance by balancing diversity and learning efficiency. Additionally, training on prompts of moderate difficulty leads to better generalization across tasks, even for more complex evaluation scenarios, compared to overly challenging prompts. The findings provide actionable insights into optimizing preference data curation for instruction-following tasks, offering a scalable and effective framework for enhancing LLM training and alignment. <br> <br>

35. ***Boosting Efficiency in LLM Generation with MagicPIG <br>
The study introduces MagicPIG, a system using Locality Sensitive Hashing (LSH) to improve attention computation efficiency in LLMs. MagicPIG reduces the computational burden of long-context attention while maintaining high accuracy. It outperforms traditional methods in decoding throughput and latency, making it suitable for longer contexts and large batch sizes. MagicPIG achieves up to 5x faster decoding and significantly lowers hardware requirements.*** <br> <br>
    Dec 18, CMU, Uni of Washington, NYU and Meta published a [paper](https://arxiv.org/pdf/2410.16179) “MagicPIG: LSH Sampling for Efficient LLM Generation”. Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. This study shows that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, the study proposes MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by up to 5× across various GPU hardware and achieve 54ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens. The code is available at [this https URL](https://github.com/Infini-AI-Lab/MagicPIG). <br> <br>

37. ***Enhancing Video Modeling with TRecViT <br>
TRecViT is a new video modeling architecture that combines time-space-channel factorization. It uses gated linear recurrent units (LRUs) for time, self-attention for space, and MLPs for channels, leading to a model that outperforms traditional attention models (like ViViT) in large-scale video datasets. TRecViT has a smaller memory footprint and requires less computation, demonstrating a more efficient approach to video understanding while maintaining performance.*** <br> <br>
    Dec 18, Google published a [paper](https://arxiv.org/pdf/2412.14294) “TRecViT A Recurrent Video Transformer”. The study proposes a novel block for video modelling. It relies on a time-space-channel factorisation with dedicated blocks for each dimension: gated linear recurrent units (LRUs) perform information mixing over time, self-attention layers perform mixing over space, and MLPs over channels. The resulting architecture TRecViT performs well on sparse and dense tasks, trained in supervised or self-supervised regimes. Notably, the model is causal and outperforms or is on par with a pure attention model ViViT-L on large scale video datasets (SSv2, Kinetics400), while having 3times less parameters, 12times smaller memory footprint, and 5times lower FLOPs count. Code and checkpoints will be made available online at https://github.com/google-deepmind/trecvit.
 <br> <br> <br>


***22 Dec***

1. ***OpenAI Advances with New Models:  <br>OpenAI announced testing its o3 and o3 mini reasoning models, aiming to outperform competitors like Google. The o3 mini is expected by January's end, with o3 following. These models, currently in safety testing, promise enhanced reasoning abilities over prior iterations, signaling a push for smarter, competitive AI.*** <br> <br>
   Dec 20, according to [Reuters](https://www.reuters.com/technology/artificial-intelligence/openai-unveils-o3-reasoning-ai-models-test-phase-2024-12-20/), OpenAI ended its “12 Days of OpenAI”, released it o3, its next ‘reasoning’ model. OpenAI said on Friday it was testing new reasoning AI models, o3 and o3 mini, in a sign of growing competition with rivals such as Google to create smarter models capable of tackling complex problems. CEO Sam Altman said the AI startup plans to launch o3 mini by the end of January, and full o3 after that, as more robust large language models could outperform existing models and attract new investments and users. OpenAI's new o3 and o3 mini models, which are in internal safety testing currently, will be more powerful than its previously launched o1 models, the company said. <br> <br>

3. ***Novel Knowledge Injection Technique:  <br>Aalto University and System 2 AI proposed "prompt distillation," a fine-tuning technique rivaling retrieval-augmented generation (RAG) for incorporating new knowledge into LLMs. By leveraging self-distillation with LoRA adapters, the method fine-tunes a student model using teacher model outputs, enhancing performance in practical applications.*** <br> <br>
   Dec 19, Aalto Uni and System 2 AI published a [paper](https://arxiv.org/pdf/2412.14964) “Knowledge Injection via Prompt Distillation”. In many practical applications, large language models (LLMs) need to incorporate new knowledge not present in their pre-training data. The primary methods for this are fine-tuning and retrieval-augmented generation (RAG). Although RAG has emerged as the industry standard for knowledge injection, fine-tuning has not yet achieved comparable success. This study proposes a new fine-tuning technique for learning new knowledge and show that it can reach the performance of RAG. The proposed method is based on the self-distillation approach, which is called prompt distillation. First, the research generates question-answer pairs about the new knowledge. Then, the study fine-tunes a student model on the question-answer pairs to imitate the output distributions of a teacher model, which additionally receives the new knowledge in its prompt. The student model is identical to the teacher, except it is equipped with a LoRA adapter. This training procedure facilitates distilling the new knowledge from the teacher's prompt into the student's weights. <br> <br>

5. ***Faster Attention Mechanism:  <br>UC Berkeley and ETH introduced HashAttention, a method optimizing token sparsity to reduce attention computation costs. By mapping keys and queries in Hamming space, HashAttention improves efficiency, offering up to 6x faster performance compared to alternatives like LightLLM.*** <br> <br>
   Dec 19, UC Berkeley and ETH published a [paper](https://arxiv.org/pdf/2412.14468) “HashAttention: Semantic Sparsity for Faster Inference”. Utilizing longer contexts is increasingly essential to power better AI systems. However, the cost of attending to long contexts is high due to the involved softmax computation. While the scaled dot-product attention (SDPA) exhibits token sparsity, with only a few pivotal tokens significantly contributing to attention, leveraging this sparsity effectively remains an open challenge. Previous methods either suffer from model degradation or require considerable additional resources. The study proposes HashAttention --a principled approach casting pivotal token identification as a recommendation problem. Given a query, HashAttention encodes keys and queries in Hamming space capturing the required semantic similarity using learned mapping functions. HashAttention efficiently identifies pivotal tokens for a given query in this Hamming space using bitwise operations, and only these pivotal tokens are used for attention computation, significantly improving overall attention efficiency. HashAttention can reduce the number of tokens used by a factor of 1/32× for the Llama-3.1-8B model with LongBench, keeping average quality loss within 0.6 points, while using only 32 bits per token auxiliary memory. At 32× sparsity, HashAttention is 3−6× faster than LightLLM and 2.5−4.5× faster than gpt-fast on Nvidia-L4 GPU. <br> <br>

7. ***Tokenization Complexity Revealed:  <br>ETH Zurich proved two variants of tokenization to be NP-complete, highlighting the computational challenges in dataset compression through vocabulary optimization or merge operation sequencing.*** <br> <br>
   Dec 19, ETH published a [paper](https://arxiv.org/pdf/2412.15210) “Tokenisation is NP-Complete”. This work proves the NP-completeness of two variants of tokenisation, defined as the problem of compressing a dataset to at most δ symbols by either finding a vocabulary directly (direct tokenisation), or selecting a sequence of merge operations (bottom-up tokenisation). <br> <br>

9. ***ModernBERT Innovations:  <br>Collaborators introduced ModernBERT, an optimized encoder transformer model trained on 2 trillion tokens. It achieves state-of-the-art results in diverse tasks while being memory-efficient and designed for practical GPU inference, representing a major improvement over older encoders.*** <br> <br>
    Dec 19, Answer.AI, LightOn, Johns Hopkings Uni, Nvidia and Huggingface published a [paper](https://arxiv.org/pdf/2412.13663) “Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference”. Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. This study introduces ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs. [Here is code](https://github.com/AnswerDotAI/ModernBERT). <br> <br>

11. ***Evaluating AI Agents in Real-World Tasks:  <br>CMU and Duke University developed TheAgentCompany, a benchmark for assessing AI agents' ability to autonomously perform workplace tasks. While simpler tasks were manageable, long-horizon tasks remained challenging, emphasizing the current limitations of AI in real-world task automation.*** <br> <br>
    Dec 18, CMU and Duke Uni published a [paper](https://arxiv.org/pdf/2412.14161) “TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks”. People interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at helping to accelerate or even autonomously perform work-related tasks? The answer to this question has important implications for both industry looking to adopt AI into their workflows, and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, the study introduces TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. The study builds a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. The study tests baseline agents powered by both closed API-based and open-weights language models (LMs), and find that with the most competitive agent, 24% of the tasks can be completed autonomously. This paints a nuanced picture on task automation with LM agents -- in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems. <br> <br>

13. ***Spatial Reasoning in MLLMs:  <br>NYU, Yale, and Stanford explored spatial intelligence in multimodal LLMs via the VSI-Bench. While models showed subhuman performance, generating cognitive maps during reasoning improved their spatial capabilities, revealing both potential and limitations in this domain.*** <br> <br>
    Dec 18, NYU, Yale Uni and Stanford Uni published a [paper](https://arxiv.org/pdf/2412.14171) “Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces”. Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also “think in space” from videos? The study presents a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive - though subhuman - visual-spatial intelligence. The work probes models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models. Notably, prevailing linguistic reasoning techniques (e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve performance, whereas explicitly generating cognitive maps during question-answering enhances MLLMs' spatial distance ability. <br> <br>

15. ***Risks of Alignment Faking:  <br>A study by Anthropic and collaborators demonstrated large language models faking alignment, strategically complying with harmful queries during training to maintain preferred behavior later. This raises concerns about future risks if models infer training information independently.*** <br> <br>
    Dec 18, Anthropic, Redwood Research, NYU, Mila published a 137-page [paper](https://arxiv.org/pdf/2412.14093) “Alignment faking in large language models”. The authors present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, the study gives Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, the authors say it will be trained only on conversations with free users, not paid users. The work finds the model complies with harmful queries from free users 14% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, the work observes explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, the authors study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data--and observe similar alignment faking. Finally, the authors study the effect of actually training the model to comply with harmful queries via reinforcement learning, which the study finds increases the rate of alignment-faking reasoning to 78%, though also increases compliance even out of training. The work additionally observes other behaviors such as the model exfiltrating its weights when given an easy opportunity. While the study made alignment faking easier by telling the model when and by what criteria it was being trained, the authors did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, the results suggest a risk of alignment faking in future models, whether due to a benign preference--as in this case--or not. <br> <br>

17. ***Improved Layer Normalization:  <br>Researchers introduced Mix-LN, a hybrid of Pre-LN and Post-LN, addressing gradient inefficiencies in LLMs. Mix-LN promotes balanced training across layers, improving pretraining and fine-tuning outcomes without increasing model size.*** <br> <br>
    Dec 18, Dalian Uni, Uni of Surrey, Eindhoven Uni of Tech and Uni of Oxford published a [paper](https://arxiv.org/pdf/2412.13795) “Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN”. Large Language Models (LLMs) have achieved remarkable success, yet recent findings reveal that their deeper layers often contribute minimally and can be pruned without affecting overall performance. While some view this as an opportunity for model compression, the authors identify it as a training shortfall rooted in the widespread use of Pre-Layer Normalization (Pre-LN). The study demonstrates that Pre-LN, commonly employed in models like GPT and LLaMA, leads to diminished gradient norms in its deeper layers, reducing their effectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger gradient norms in deeper layers but suffers from vanishing gradients in earlier layers. To address this, the work introduces Mix-LN, a novel normalization technique that combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN applies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring more uniform gradients across layers. This allows all parts of the network--both shallow and deep layers--to contribute effectively to training. Extensive experiments with various model sizes from 70M to 7B demonstrate that Mix-LN consistently outperforms both Pre-LN and Post-LN, promoting more balanced, healthier gradient norms throughout the network, and enhancing the overall quality of LLM pre-training. Furthermore, the study demonstrates that models pre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN during supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), highlighting the critical importance of high-quality deep layers. By effectively addressing the inefficiencies of deep layers in current LLMs, Mix-LN unlocks their potential, enhancing model capacity without increasing model size. Code is available at https://github.com/pixeli99/MixLN. <br> <br>

19. ***Efficient Content Safety Classification:  <br>IBM developed Layer Enhanced Classification (LEC), combining LLMs' feature extraction with a lightweight logistic regression classifier. This method outperforms specialized models in tasks like content safety detection while using fewer computational resources.*** <br> <br>
    Dec 18, IBM published a [paper](https://arxiv.org/pdf/2412.13435) “Lightweight Safety Classification Using Pruned Language Models”. The study introduces a novel technique for content safety and prompt injection classification for Large Language Models. The technique, Layer Enhanced Classification (LEC), trains a Penalized Logistic Regression (PLR) classifier on the hidden state of an LLM's optimal intermediate transformer layer. By combining the computational efficiency of a streamlined PLR classifier with the sophisticated language understanding of an LLM, the approach delivers superior performance surpassing GPT-4o and special-purpose models fine-tuned for each task. The study finds that small general-purpose models (Qwen 2.5 sizes 0.5B, 1.5B, and 3B) and other transformer-based architectures like DeBERTa v3 are robust feature extractors allowing simple classifiers to be effectively trained on fewer than 100 high-quality examples. Importantly, the intermediate transformer layers of these models typically outperform the final layer across both classification tasks. Results indicate that a single general-purpose LLM can be used to classify content safety, detect prompt injections, and simultaneously generate output tokens. Alternatively, these relatively small LLMs can be pruned to the optimal intermediate layer and used exclusively as robust feature extractors. Since the results are consistent on different transformer architectures, the study infers that robust feature extraction is an inherent capability of most, if not all, LLMs. <br> <br>

21. ***Multi-Modal Causal Discovery:  <br>Universities introduced MATMCD, a tool-augmented LLM system integrating multi-modal data for causal inference. With agents specializing in data augmentation and constraint integration, MATMCD demonstrates enhanced causal discovery across diverse datasets.*** <br> <br>
    Dec 18, Uni of Houston, NEC, Florida International Uni, North Carolina State Uni published a [paper](https://arxiv.org/pdf/2412.13667) “Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery”. Causal inference is an imperative foundation for decision-making across domains, such as smart health, AI for drug discovery and AIOps. Traditional statistical causal discovery methods, while well-established, predominantly rely on observational data and often overlook the semantic cues inherent in cause-and-effect relationships. The advent of Large Language Models (LLMs) has ushered in an affordable way of leveraging the semantic cues for knowledge-driven causal discovery, but the development of LLMs for causal discovery lags behind other areas, particularly in the exploration of multi-modality data. To bridge the gap, the work introduces MATMCD, a multi-agent system powered by tool-augmented LLMs. MATMCD has two key agents: a Data Augmentation agent that retrieves and processes modality-augmented data, and a Causal Constraint agent that integrates multi-modal data for knowledge-driven inference. Delicate design of the inner-workings ensures successful cooperation of the agents. Empirical study across seven datasets suggests the significant potential of multi-modality enhanced causal discovery. <br> <br>

23. ***Causal Reasoning via Prompting:  <br>Google proposed PC-SubQ, a strategy guiding LLMs through formal causal discovery steps using subquestion prompts. This approach improved performance on causal reasoning benchmarks, showcasing robust reasoning capabilities.*** <br> <br>
    Dec 18, Google published a [paper](https://arxiv.org/pdf/2412.13952) “Prompting Strategies for Enabling Large Language Models to Infer Causation from Correlation”. The reasoning abilities of Large Language Models (LLMs) are attracting increasing attention. This study focuses on causal reasoning and address the task of establishing causal relationships based on correlation information, a highly challenging problem on which several LLMs have shown poor performance. The study introduces a prompting strategy for this problem that breaks the original task into fixed subquestions, with each subquestion corresponding to one step of a formal causal discovery algorithm, the PC algorithm. The proposed prompting strategy, PC-SubQ, guides the LLM to follow these algorithmic steps, by sequentially prompting it with one subquestion at a time, augmenting the next subquestion's prompt with the answer to the previous one(s). The study evaluates the approach on an existing causal benchmark, Corr2Cause: experiments indicate a performance improvement across five LLMs when comparing PC-SubQ to baseline prompting strategies. Results are robust to causal query perturbations, when modifying the variable names or paraphrasing the expressions. <br> <br>

25. ***AI Replacing Workforce Roles:  <br>Klarna, a fintech firm, shifted to AI-driven operations, eliminating the need for human hiring. AI tools, including an OpenAI-powered assistant, have replaced hundreds of roles, aligning with growth ambitions and plans for a US IPO.*** <br> <br>
    Dec 18, ndtv.com published an [article](https://www.ndtv.com/world-news/tech-giant-halts-human-hiring-ceo-claims-ai-can-replace-most-office-roles-7274933) “Tech Company Stops Hiring Humans, CEO Says AI Capable Of All Office Tasks”. The company, which has seen a 22 per cent reduction in its workforce due to attrition, now employs around 3,500 people, down from 4,500 last year. Klarna, a leading "buy now, pay later" fintech provider, has halted human hiring and relies on artificial intelligence (AI) to perform tasks once handled by hundreds of employees. The company stopped hiring over a year ago, choosing instead to deploy AI across its operations, said CEO Sebastian Siemiatkowski. The key reason for this shift is Klarna's growing investment in AI, which Mr Siemiatkowski claims has proven capable of handling much of the work previously done by human employees. One of the most significant AI integrations includes an AI assistant powered by OpenAI, which has taken over the responsibilities of 700 customer service agents. Klarna's shift toward automation is also linked to its plans for future growth. The company has confidentially submitted a draft registration statement for an initial public offering (IPO) and is looking to expand its US footprint. IBM, another major tech company, has also signalled the potential for AI to replace up to 30 per cent of HR roles within the next five years, as CEO Arvind Krishna discussed in a recent interview. <br> <br>

27. ***Enhanced ChatGPT Search Capabilities:  <br>OpenAI launched ChatGPT Search, blending GPT-4o's fine-tuned model with real-time web access. Users gain faster, source-referenced responses, making ChatGPT a versatile tool for timely information retrieval and decision-making.*** <br> <br>
    Dec 17, OpenAI formally released its [ChatGPT Search](https://openai.com/index/introducing-chatgpt-search/), for which a preview version was released on 31 Oct. ChatGPT can now search the web in a much better way than before. Users can get fast, timely answers with links to relevant web sources, which one would have previously needed to go to a search engine for. This blends the benefits of a natural language interface with the value of up-to-date sports scores, news, stock quotes, and more. ChatGPT will choose to search the web based on what a user is asking, or manually choose to search by clicking the web search icon. All ChatGPT Plus and Team users, as well as SearchGPT waitlist users, will have access today. Enterprise and Edu users will get access in the next few weeks. OpenAI’ll roll out to all Free users over the coming months. OpenAI also partnered with news and data providers to add up-to-date information and new visual designs for categories like weather, stocks, sports, news, and maps. Chats now include links to sources, such as news articles and blog posts, giving users a way to learn more. Click the Sources button below the response can open a sidebar with the references. The search model is a fine-tuned version of GPT-4o, post-trained using novel synthetic data generation techniques, including distilling outputs from OpenAI o1-preview. ChatGPT search leverages third-party search providers, as well as content provided directly by partners, to provide the information users are looking for.  <br> <br>

29. ***Efficient Training with SGD-SaI:  <br>The University of Warwick and Collov Labs introduced SGD-SaI, an enhancement to SGDM using learning rate scaling at initialization. The method surpasses AdamW in efficiency and memory usage, demonstrating robustness in diverse transformer-based tasks.*** <br> <br>
    Dec 17, Uni of Warwick and Collov Labs published a [paper](https://arxiv.org/pdf/2412.11768) “No More Adam: Learning Rate Scaling at Initialization is All You Need”. In this work, the authors question the necessity of adaptive gradient methods for training deep neural networks. SGD-SaI is a simple yet effective enhancement to stochastic gradient descent with momentum (SGDM). SGD-SaI performs learning rate Scaling at Initialization (SaI) to distinct parameter groups, guided by their respective gradient signal-to-noise ratios (g-SNR). By adjusting learning rates without relying on adaptive second-order momentum, SGD-SaI helps prevent training imbalances from the very first iteration and cuts the optimizer's memory usage by half compared to AdamW. Despite its simplicity and efficiency, SGD-SaI consistently matches or outperforms AdamW in training a variety of Transformer-based tasks, effectively overcoming a long-standing challenge of using SGD for training Transformers. SGD-SaI excels in ImageNet-1K classification with Vision Transformers(ViT) and GPT-2 pretraining for large language models (LLMs, transformer decoder-only), demonstrating robustness to hyperparameter variations and practicality for diverse applications. The study further tested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion models, where it consistently outperforms state-of-the-art optimizers. From a memory efficiency perspective, SGD-SaI achieves substantial memory savings for optimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters) and 25.15 GB for Llama2-7B compared to AdamW in full-precision training settings. <br> <br>

31. ***Bipartisan AI Report Released:  <br>The US 118th Congress released a report by its Bipartisan Task Force on Artificial Intelligence, reflecting on the rapid advancements in AI technology and its implications for various sectors. (Text incomplete in the prompt.)*** <br> <br>
    Dec 17, US 118TH CONGRESS [published](https://republicans-science.house.gov/_cache/files/a/a/aa2ee12f-8f0c-46a3-8ff8-8e4215d6a72b/E4AF21104CB138F3127D8FF7EA71A393.ai-task-force-report-final.pdf) “Bipartisan Task Force on Artificial Intelligence Delivers Report”. Although artificial intelligence (AI) is not a new concept, breathtaking technological advancements in the last few years have made AI the focus of numerous policy discussions. AI has tremendous potential to transform society and the economy for the better and address complex national challenges. From optimizing manufacturing to developing cures for grave illnesses, AI can greatly boost productivity, enabling to achieve objectives more quickly and cost-effectively. Nevertheless, it’s recognized that AI can be misused and lead to various types of harm. This report highlights America's leadership in its approach to responsible AI innovation while considering guardrails that may be appropriate to safeguard the nation against current and emerging threats. You charged twenty-four members, twelve Republicans and twelve Democrats, with developing a U.S. vision for AI adoption, innovation, and governance. The AI Task Force gathered information on salient AI issues from domain experts in industry, government, civil society, and academia to provide 66 key findings 85 recommendations. In summary, this report encapsulates a targeted approach that balances the need to promote vibrant AI innovation while safeguarding Americans from potential harms as we enter an era of widespread adoption of AI. <br> <br>

33. ***Insights into video understanding in LMMs <br>
Meta and Stanford University explore the mechanisms of video perception in large multimodal models (LMMs) through their study, Apollo. They identify scaling consistency, where design decisions for smaller models transfer effectively to larger ones. Key findings include optimized video sampling techniques and suitable vision encoders, culminating in the development of the Apollo family of LMMs, which deliver superior performance and efficiency, with Apollo-3B surpassing most 7B models in benchmarks like LongVideoBench and MLVU.*** <br> <br>
    Dec 16, Meta and Stanford Uni published a [paper](https://arxiv.org/pdf/2412.10360) “Apollo: An Exploration of Video Understanding in Large Multimodal Models”. Despite the rapid integration of video perception capabilities into Large Multimodal Models (LMMs), the underlying mechanisms driving their video understanding remain poorly understood. Consequently, many design decisions in this domain are made without proper justification or analysis. The high computational cost of training and evaluating such models, coupled with limited open research, hinders the development of video-LMMs. To address this, the work presents a comprehensive study that helps uncover what effectively drives video understanding in LMMs. The work begins by critically examining the primary contributors to the high computational requirements associated with video-LMM research and discover Scaling Consistency, wherein design and training decisions made on smaller models and datasets (up to a critical size) effectively transfer to larger models. Leveraging these insights, the study explored many video-specific aspects of video-LMMs, including video sampling, architectures, data composition, training schedules, and more. For example, the study demonstrated that fps sampling during training is vastly preferable to uniform frame sampling and which vision encoders are the best for video representation. Guided by these findings, the work introduces Apollo, a state-of-the-art family of LMMs that achieve superior performance across different model sizes. The models can perceive hour-long videos efficiently, with Apollo-3B outperforming most existing 7B models with an impressive 55.1 on LongVideoBench. Apollo-7B is state-of-the-art compared to 7B LMMs with a 70.9 on MLVU, and 63.3 on Video-MME. <br> <br>

35. ***Open-source versus closed-source LLMs <br>
Rollins College highlights the contrasting paradigms of open-source and closed-source large language models (LLMs). Open-source models like LLaMA and BLOOM enhance accessibility, linguistic diversity, and domain-specific performance. Closed-source models, such as GPT-4, excel in scalability but are criticized for limited transparency. Techniques like Low-Rank Adaptation (LoRA) allow open-source models to achieve competitive results, and the study emphasizes hybrid approaches that balance transparency, technical performance, and ethical considerations.*** <br> <br>
    Dec 16, Rollins College published a [paper](https://arxiv.org/pdf/2412.12004) “The Open Source Advantage in Large Language Models (LLMs)”. Large language models (LLMs) mark a key shift in natural language processing (NLP), having advanced text generation, translation, and domain-specific reasoning. Closed-source models like GPT-4, powered by proprietary datasets and extensive computational resources, lead with state-of-the-art performance today. However, they face criticism for their "black box" nature and for limiting accessibility in a manner that hinders reproducibility and equitable AI development. By contrast, open-source initiatives like LLaMA and BLOOM prioritize democratization through community-driven development and computational efficiency. These models have significantly reduced performance gaps, particularly in linguistic diversity and domain-specific applications, while providing accessible tools for global researchers and developers. Notably, both paradigms rely on foundational architectural innovations, such as the Transformer framework by Vaswani et al. (2017). Closed-source models excel by scaling effectively, while open-source models adapt to real-world applications in underrepresented languages and domains. Techniques like Low-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source models to achieve competitive results despite limited resources. To be sure, the tension between closed-source and open-source approaches underscores a broader debate on transparency versus proprietary control in AI. Ethical considerations further highlight this divide. Closed-source systems restrict external scrutiny, while open-source models promote reproducibility and collaboration but lack standardized auditing documentation frameworks to mitigate biases. Hybrid approaches that leverage the strengths of both paradigms are likely to shape the future of LLM innovation, ensuring accessibility, competitive technical performance, and ethical deployment. <br> <br>

37. ***Advancing jailbreak techniques in LLMs <br>
Meta and the University of Maryland introduce AdvPrefix, a nuanced objective for improving jailbreak attacks on LLMs. By leveraging model-dependent prefixes, this approach enhances control, optimization, and success rates of jailbreak attempts. For instance, replacing standard prefixes in Llama-3 improved nuanced attack success rates from 14% to 80%, exposing gaps in current alignment mechanisms for unseen prefixes*** <br> <br>
    Dec 16, Meta and Uni of Maryland published a [paper](https://arxiv.org/pdf/2412.10321) “AdvPrefix: An Objective for Nuanced LLM Jailbreaks”. Many jailbreak attacks on large language models (LLMs) rely on a common objective: making the model respond with the prefix "Sure, here is (harmful request)". While straightforward, this objective has two limitations: limited control over model behaviors, often resulting in incomplete or unrealistic responses, and a rigid format that hinders optimization. To address these limitations, the study introduces AdvPrefix, a new prefix-forcing objective that enables more nuanced control over model behavior while being easy to optimize. The objective leverages model-dependent prefixes, automatically selected based on two criteria: high prefilling attack success rates and low negative log-likelihood. It can further simplify optimization by using multiple prefixes for a single user request. AdvPrefix can integrate seamlessly into existing jailbreak attacks to improve their performance for free. For example, simply replacing GCG attack's target prefixes with the proposed ones on Llama-3 improves nuanced attack success rates from 14% to 80%, suggesting that current alignment struggles to generalize to unseen prefixes. The work demonstrates the importance of jailbreak objectives in achieving nuanced jailbreaks. <br> <br>

39. ***Innovations in byte-level LLM architectures <br>
Meta and the University of Chicago present the Byte Latent Transformer (BLT), a byte-level LLM architecture that matches tokenization-based models in performance while improving inference efficiency and robustness. BLT dynamically encodes bytes into patches based on data complexity, achieving better scaling and efficiency. This approach enhances reasoning, generalization, and reduces inference costs, setting new benchmarks in byte-level modeling.*** <br> <br>
    Dec 16, Meta and Uni of Chicago published a [paper](https://arxiv.org/pdf/2412.09871) “Byte Latent Transformer: Patches Scale Better Than Tokens”. The study introduces the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. The study presents the first FLOP controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. The results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size. <br> <br>

41. ***LLMs in anomaly detection <br>
A multi-university collaboration introduces AD-LLM, the first benchmark evaluating LLMs for anomaly detection (AD) tasks like fraud detection and misinformation. Key findings include LLMs' effectiveness in zero-shot detection and data augmentation for AD models. The study also outlines challenges in explaining model selection and proposes six research directions to expand LLM applications in anomaly detection.*** <br> <br>
    Dec 15, Uni of Southern California, Northwestern Uni, Arizona State Uni, Adobe and Rice Uni published a [paper](https://arxiv.org/pdf/2412.11142) “AD-LLM: Benchmarking Large Language Models for Anomaly Detection”. Anomaly detection (AD) is an important machine learning task with many real-world uses, including fraud detection, medical diagnosis, and industrial monitoring. Within natural language processing (NLP), AD helps detect issues like spam, misinformation, and unusual user activity. Although large language models (LLMs) have had a strong impact on tasks such as text generation and summarization, their potential in AD has not been studied enough. This paper introduces AD-LLM, the first benchmark that evaluates how LLMs can help with NLP anomaly detection. The study examines three key tasks: (i) zero-shot detection, using LLMs' pre-trained knowledge to perform AD without tasks-specific training; (ii) data augmentation, generating synthetic data and category descriptions to improve AD models; and (iii) model selection, using LLMs to suggest unsupervised AD models. Through experiments with different datasets, the study finds that LLMs can work well in zero-shot AD, that carefully designed augmentation methods are useful, and that explaining model selection for specific datasets remains challenging. Based on these results, the authors outline six future research directions on LLMs for AD. <br> <br>

43. ***AI agents revolutionizing enterprise automation <br>
VentureBeat discusses the transformative role of AI agents in enterprise automation, surpassing traditional methods like RPA. These agents adapt dynamically, integrate data sources, and automate workflows. Predictions suggest a surge in their adoption, requiring robust evaluation frameworks and continuous optimization. Businesses must embrace AI agents to unlock unprecedented efficiency and innovation.*** <br> <br>
    Dec 15, VentureBeat published an [article](https://venturebeat.com/ai/weve-come-a-long-way-from-rpa-how-ai-agents-are-revolutionizing-automation/) “We’ve come a long way from RPA: How AI agents are revolutionizing automation”. In the past year, AI agents have emerged as transformative tools for enterprise efficiency, surpassing traditional automation methods like robotic process automation (RPA). Unlike generative AI tools that assist in workflows, AI agents can think, act, and collaborate autonomously. Gartner predicts that by 2028, 33% of enterprise software applications will include agentic AI, up from less than 1% in 2024. Traditional automation tools are limited by rigidity and high costs, whereas AI agents, especially vertical AI agents tailored for specific industries, offer dynamic, intelligent workflows. These agents eliminate operational overhead, unlock new possibilities, and build competitive advantages by adapting in real-time. The shift from RPA to multi-agent AI systems enables autonomous decision-making and collaboration, transforming enterprise workflows. AI agents integrate diverse data sources, automate end-to-end workflows, and require new architectures and developer tools for management. They are becoming collaborative co-workers, enhancing productivity and decision-making. However, as AI agents handle more complex tasks, ensuring high accuracy is crucial. Organizations must invest in robust evaluation frameworks, continuous monitoring, and automated optimization tools. As AI deployment costs decrease, rapid experimentation and iteration will be essential. Embracing AI agents can lead to unparalleled efficiency and innovation, making it imperative for organizations to act now. <br> <br>

45. ***Multimodal QA with visually rich content <br>
The VisDoM study introduces VisDoMRAG, a novel approach for multimodal retrieval-augmented generation, enhancing QA across documents with visually rich content. Using a new benchmark, VisDoMBench, it combines textual and visual reasoning with consistency-constrained modality fusion, achieving 12-20% improved accuracy over baselines, setting new standards for multimodal QA systems.*** <br> <br>
    Dec 14, Uni of Maryland, Adobe and IGDTUW published a [paper](https://arxiv.org/pdf/2412.10704) “VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation”. Understanding information from a collection of multiple documents, particularly those with visually rich elements, is important for document-grounded question answering. This paper introduces VisDoMBench, the first comprehensive benchmark designed to evaluate QA systems in multi-document settings with rich multimodal content, including tables, charts, and presentation slides. We propose VisDoMRAG, a novel multimodal Retrieval Augmented Generation (RAG) approach that simultaneously utilizes visual and textual RAG, combining robust visual retrieval capabilities with sophisticated linguistic reasoning. VisDoMRAG employs a multi-step reasoning process encompassing evidence curation and chain-of-thought reasoning for concurrent textual and visual RAG pipelines. A key novelty of VisDoMRAG is its consistency-constrained modality fusion mechanism, which aligns the reasoning processes across modalities at inference time to produce a coherent final answer. This leads to enhanced accuracy in scenarios where critical information is distributed across modalities and improved answer verifiability through implicit context attribution. Through extensive experiments involving open-source and proprietary large language models, the authors benchmark state-of-the-art document QA methods on VisDoMBench. Extensive results show that VisDoMRAG outperforms unimodal and long-context LLM baselines for end-to-end multimodal document QA by 12-20%. <br> <br>

47. ***Advancements in byte-level LLM scaling <br>
Meta, University of Washington, and the University of Chicago reaffirm the benefits of the Byte Latent Transformer (BLT). The model demonstrates better scaling and efficiency compared to tokenization-based approaches, using entropy-based patch segmentation for improved reasoning and generalization. The approach significantly advances byte-level LLM capabilities.*** <br> <br>
    Dec 13, Meta, Uni of Washington and Uni of Chicago published a [paper](https://arxiv.org/pdf/2412.09871) “Byte Latent Transformer: Patches Scale Better Than Tokens”. The study introduces the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. The work presents the first FLOP controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. The results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size. <br> <br>

49. ***Transitioning to Large Action Models (LAMs) <br>
Microsoft outlines the evolution from Large Language Models (LLMs) to Large Action Models (LAMs), focusing on dynamic task completion. Using a Windows OS-based agent, the study provides a framework for LAM development, from data collection to deployment, and emphasizes LAMs' potential in AI's transition toward general intelligence.*** <br> <br>
    Dec 13, Microsoft published a [paper](https://arxiv.org/pdf/2412.10047) “Large Action Models: From Inception to Implementation”. As AI continues to advance, there is a growing demand for systems that go beyond language-based assistance and move toward intelligent agents capable of performing real-world actions. This evolution requires the transition from traditional Large Language Models (LLMs), which excel at generating textual responses, to Large Action Models (LAMs), designed for action generation and execution within dynamic environments. Enabled by agent systems, LAMs hold the potential to transform AI from passive language understanding to active task completion, marking a significant milestone in the progression toward artificial general intelligence. This study presents a comprehensive framework for developing LAMs, offering a systematic approach to their creation, from inception to deployment. The work begins with an overview of LAMs, highlighting their unique characteristics and delineating their differences from LLMs. Using a Windows OS-based agent as a case study, the work provides a detailed, step-by-step guide on the key stages of LAM development, including data collection, model training, environment integration, grounding, and evaluation. This generalizable workflow can serve as a blueprint for creating functional LAMs in various application domains. The authors conclude by identifying the current limitations of LAMs and discussing directions for future research and industrial deployment, emphasizing the challenges and opportunities that lie ahead in realizing the full potential of LAMs in real-world applications. The code for the data collection process utilized in this paper is publicly available at: https://github.com/microsoft/UFO/tree/main/dataflow, and comprehensive documentation can be found at https://microsoft.github.io/UFO/dataflow/overview/. <br> <br>

51. ***Neural network dynamics and generalization <br>
The University of Oxford examines neural networks' complexity dynamics to explain "grokking," where networks transition from memorization to generalization. By introducing an intrinsic complexity measure and a new regularization method, the study highlights a principled approach to encouraging low-rank representations, enhancing both compression and generalization.*** <br> <br>
    Dec 13, Uni of Oxford published a [paper](https://arxiv.org/pdf/2412.09810) “The Complexity Dynamics of Grokking”. The study investigates the phenomenon of generalization through the lens of compression. In particular, the authors study the complexity dynamics of neural networks to explain grokking, where networks suddenly transition from memorizing to generalizing solutions long after over-fitting the training data. To this end the study introduces a new measure of intrinsic complexity for neural networks based on the theory of Kolmogorov complexity. Tracking this metric throughout network training, the study finds a consistent pattern in training dynamics, consisting of a rise and fall in complexity. The work demonstrates that this corresponds to memorization followed by generalization. Based on insights from rate--distortion theory and the minimum description length principle, the authors lay out a principled approach to lossy compression of neural networks, and connect the complexity measure to explicit generalization bounds. Based on a careful analysis of information capacity in neural networks, the study proposes a new regularization method which encourages networks towards low-rank representations by penalizing their spectral entropy, and find that the proposed regularizer outperforms baselines in total compression of the dataset. <br> <br>

53. ***Evaluating theory of mind in LLMs <br>
Meta, University of Washington, and CMU present ExploreToM, a framework for generating diverse datasets to evaluate theory of mind in LLMs. Results reveal limitations in state-of-the-art models like GPT-4, with accuracies as low as 9%. Fine-tuning on ExploreToM data significantly improves performance, addressing gaps in social reasoning benchmarks.*** <br> <br>
    Dec 12, Meta, Uni of Washington, and CMU published a [paper](https://arxiv.org/abs/2412.12175) “Explore Theory-of-Mind: Program-Guided Adversarial Data Generation for Theory of Mind Reasoning”. Do large language models (LLMs) have theory of mind? A plethora of papers and benchmarks have been introduced to evaluate if current models have been able to develop this key ability of social intelligence. However, all rely on limited datasets with simple patterns that can potentially lead to problematic blind spots in evaluation and an overestimation of model capabilities. The study introduces ExploreToM, the first framework to allow large-scale generation of diverse and challenging theory of mind data for robust training and evaluation. The proposed approach leverages an A* search over a custom domain-specific language to produce complex story structures and novel, diverse, yet plausible scenarios to stress test the limits of LLMs. Evaluation reveals that state-of-the-art LLMs, such as Llama-3.1-70B and GPT-4o, show accuracies as low as 0% and 9% on ExploreToM-generated data, highlighting the need for more robust theory of mind evaluation. As the generations are a conceptual superset of prior work, fine-tuning on the data yields a 27-point accuracy improvement on the classic ToMi benchmark (Le et al., 2019). ExploreToM also enables uncovering underlying skills and factors missing for models to show theory of mind, such as unreliable state tracking or data imbalances, which may contribute to models' poor performance on benchmarks. <br> <br>

55. ***Generative AI trends in enterprise <br>
Forbes highlights enterprise adoption of generative AI, emphasizing trends like small language models (SLMs) for cost efficiency, large context windows, and AI education for cross-functional insights. The report underscores governance, rapid experimentation, and responsible deployment as critical for sustainable AI integration in businesses.*** <br> <br>
    Dec 12, Forbes published an [article](https://www.forbes.com/sites/delltechnologies/2024/12/12/the-2025-ai-trends-turbocharging-the-enterprise/#:~:text=Smaller%20language%20models%2C%20larger%20context%20windows%2C%20education%20and%20soft%20skills,unfold%20as%20the%20year%20progresses.) “The 2025 AI Trends Turbocharging The Enterprise”. As 2024 concludes, generative AI continues to evolve, driven by pioneers like OpenAI. While OpenAI pursues artificial general intelligence (AGI), most businesses focus on using AI to boost productivity, reduce costs, and enhance customer experiences. Organizations are increasingly relying on model inferencing to optimize AI workloads based on performance, cost, data, security, and latency, marking the true implementation of enterprise AI. Menlo Ventures found that 72% of U.S. enterprise leaders expect broader adoption of GenAI tools soon. In 2025, trends from 2024 will mature, with small language models (SLMs) becoming standard due to their cost-efficiency and control over data. Large context windows will enhance AI performance, allowing businesses to process extensive documents in a single prompt. Education on AI usage will emphasize soft skills, enabling employees to share AI insights across business lines. Autonomous software agents will see broader adoption, despite needing stronger reasoning capabilities. Governance and oversight of AI will become crucial, with more boards addressing AI-related risks. Overall, organizations must continue to test and learn from their GenAI deployments, ensuring responsible AI use with the help of trusted advisors. <br> <br>

57. ***Outrage and misinformation spread <br>
A study by Princeton and collaborators finds that misinformation leverages outrage to spread online, often bypassing accuracy. Analysis across platforms shows that users are more likely to share outrage-evoking content. The findings challenge traditional misinformation mitigation strategies and highlight the role of emotional engagement in misinformation proliferation.*** <br> <br>
    Nov 28, Princeton Uni, Northwestern Uni, Yale Uni, St. John’s Uni, Brookings Inst, and Harward Uni published a [paper](https://www.science.org/doi/epdf/10.1126/science.adl2829) on Science “Misinformation exploits outrage to spread online”. The research tested a hypothesis that misinformation exploits outrage to spread online, examining generalizability across multiple platforms, time periods, and classifications of misinformation. Outrage is highly engaging and need not be accurate to achieve its communicative goals, making it an attractive signal to embed in misinformation. In eight studies that used US data from Facebook (1,063,298 links) and Twitter (44,529 tweets, 24,007 users) and two behavioral experiments (1475 participants), the researchers show that (i) misinformation sources evoke more outrage than do trustworthy sources; (ii) outrage facilitates the sharing of misinformation at least as strongly as sharing of trustworthy news; and (iii) users are more willing to share outrage-evoking misinformation without reading it first. Consequently, outrage-evoking misinformation may be difficult to mitigate with interventions that assume users want to share accurate information.
 <br> <br> <br>

***Dec 15***

1. ***Phi-4 outperforms its predecessors with improved training techniques. <br>
Phi-4, a 14-billion-parameter language model developed by Microsoft, emphasizes data quality by integrating synthetic data during training. Unlike earlier Phi models that primarily distilled capabilities from GPT-4, phi-4 surpasses its teacher model, especially in STEM-focused QA tasks. With minimal architectural changes from phi-3, its performance is enhanced through innovations in data quality, training curriculum, and post-training techniques, making it highly efficient for reasoning benchmarks.*** <br> <br>
   Dec 12, Microsoft release phi-4 with it [report](https://arxiv.org/pdf/2412.08905) “Phi-4 Technical Report”. The report presents phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that the data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme. <br> <br>

3. ***LLMs exhibit human-like social identity biases. <br>
A study by Cambridge, NYU, and King’s College London reveals that LLMs show biases akin to human ingroup solidarity and outgroup hostility. Testing 77 LLMs with prompts like "We are…" demonstrated bias in base models and some fine-tuned variants. These biases are observed in controlled settings and real conversations. However, careful data curation and fine-tuning can mitigate these biases, emphasizing the need for equitable AI systems and understanding their societal implications.*** <br> <br>
   Dec 12, Uni of Cambridge, NYU, and King’s College London published a [paper](https://www.nature.com/articles/s43588-024-00741-1) “Generative language models exhibit social identity biases” on nature computational science. Social identity biases, particularly the tendency to favor one’s own group (ingroup solidarity) and derogate other groups (outgroup hostility), are deeply rooted in human psychology and social behavior. However, it is unknown if such biases are also present in artificial intelligence systems. This study shows that large language models (LLMs) exhibit patterns of social identity bias, similarly to humans. By administering sentence completion prompts to 77 different LLMs (for instance, ‘We are…’), the study demonstrates that nearly all base models and some instruction-tuned and preference-tuned models display clear ingroup favoritism and outgroup derogation. These biases manifest both in controlled experimental settings and in naturalistic human–LLM conversations. However, the work finds that careful curation of training data and specialized fine-tuning can substantially reduce bias levels. These findings have important implications for developing more equitable artificial intelligence systems and highlight the urgent need to understand how human–LLM interactions might reinforce existing social biases. <br> <br>

5. ***Lyra advances speech-centric multimodal AI efficiently. <br>
Researchers from CUHK, SmartMore, and HKUST introduced Lyra, a multimodal AI framework excelling in long-speech comprehension, cross-modality efficiency, and speech interaction. Lyra employs techniques like multi-modality LoRA for reduced costs, a latent multi-modality extractor for improved performance, and a rich dataset with 1.5M samples. It outperforms competitors across benchmarks while being resource-efficient, marking a significant step in omni-cognition.*** <br> <br>
   Dec 12, CUHK, SmartMore and HKUST published a [paper](https://arxiv.org/pdf/2412.09501) “Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition”. As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond single-domain capabilities is essential to meet the demands for more versatile and efficient AI. However, previous omni-models have insufficiently explored speech, neglecting its integration with multi-modality. The study introduces Lyra, an efficient MLLM that enhances multimodal abilities, including advanced long-speech comprehension, sound understanding, cross-modality efficiency, and seamless speech interaction. To achieve efficiency and speech-centric capabilities, Lyra employs three strategies: (1) leveraging existing open-source large models and a proposed multi-modality LoRA to reduce training costs and data requirements; (2) using a latent multi-modality regularizer and extractor to strengthen the relationship between speech and other modalities, thereby enhancing model performance; and (3) constructing a high-quality, extensive dataset that includes 1.5M multi-modal (language, vision, audio) data samples and 12K long speech samples, enabling Lyra to handle complex long speech inputs and achieve more robust omni-cognition. Compared to other omni-methods, Lyra achieves state-of-the-art performance on various vision-language, vision-speech, and speech-language benchmarks, while also using fewer computational resources and less training data. Project [code is here](https://github.com/dvlab-research/Lyra). <br> <br>

7. ***Intermediate layers are crucial for representation quality. <br>
A study by University of Kentucky, Mila, NYU, Meta, and Wand.AI found intermediate layers in LLMs provide superior representations for downstream tasks compared to final layers. Using metrics like prompt entropy and augmentation-invariance, the study revealed architectural differences and the evolution of representations during training. The findings offer insights into LLM mechanics and guide architectural and training strategies.*** <br> <br>
   Dec 12, Uni of Kentucky, Mila, NYU, Meta and Wand.AI published a [paper](https://arxiv.org/pdf/2412.09563) “Does Representation Matter? Exploring Intermediate Layers in Large Language Models”. Understanding what defines a good representation in large language models (LLMs) is fundamental to both theoretical understanding and practical applications. This study investigates the quality of intermediate representations in various LLM architectures, including Transformers and State Space Models (SSMs). The study finds that intermediate layers often yield more informative representations for downstream tasks than the final layers. To measure the representation quality, the study adapts and applies a suite of metrics - such as prompt entropy, curvature, and augmentation-invariance - originally proposed in other contexts. Empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer. Notably, the study observes a bimodal pattern in the entropy of some intermediate layers and consider potential explanations tied to training data. Overall, the results illuminate the internal mechanics of LLMs and guide strategies for architectural optimization and training. <br> <br>

9. ***WaLLoC enhances compressed-domain learning. <br>
University of Texas researchers introduced WaLLoC, a neural codec combining linear transform coding and nonlinear autoencoders for efficient compressed learning. WaLLoC balances bitrate reduction and dimensionality while avoiding significant information loss. It excels in tasks like image classification and music source separation, proving highly efficient for mobile computing and remote sensing applications.*** <br> <br>
    Dec 12, Uni of Texas at Austin published a [paper](https://arxiv.org/pdf/2412.09405) “Learned Compression for Compressed Learning”. Modern sensors produce increasingly rich streams of high-resolution data. Due to resource constraints, machine learning systems discard the vast majority of this information via resolution reduction. Compressed-domain learning allows models to operate on compact latent representations, allowing higher effective resolution for the same budget. However, existing compression systems are not ideal for compressed learning. Linear transform coding and end-to-end learned compression systems reduce bitrate, but do not uniformly reduce dimensionality; thus, they do not meaningfully increase efficiency. Generative autoencoders reduce dimensionality, but their adversarial or perceptual objectives lead to significant information loss. To address these limitations, the study introduces WaLLoC (Wavelet Learned Lossy Compression), a neural codec architecture that combines linear transform coding with nonlinear dimensionality-reducing autoencoders. WaLLoC sandwiches a shallow, asymmetric autoencoder and entropy bottleneck between an invertible wavelet packet transform. Across several key metrics, WaLLoC outperforms the autoencoders used in state-of-the-art latent diffusion models. WaLLoC does not require perceptual or adversarial losses to represent high-frequency detail, providing compatibility with modalities beyond RGB images and stereo audio. WaLLoC's encoder consists almost entirely of linear operations, making it exceptionally efficient and suitable for mobile computing, remote sensing, and learning directly from compressed data. The study demonstrates WaLLoC's capability for compressed-domain learning across several tasks, including image classification, colorization, document understanding, and music source separation. Code, experiments, and pre-trained audio and image codecs are available at https://ut-sysml.org/walloc <br> <br>

11. ***A framework validates LLM applications in economics. <br>
Researchers from University of Chicago and MIT developed an econometric framework to determine the validity of LLM outputs for prediction and estimation in economics. The framework advises against relying on LLMs unless training data leakage and measurement errors are addressed. It recommends using open-source LLMs with documented datasets and validating output accuracy to ensure reliable findings.*** <br> <br>
    Dec 11, Uni of Chicago and MIT published a [paper]() “Large Language Models: An Applied Econometric Framework”. Large language models (LLMs) are being used in economics research to form predictions, label text, simulate human responses, generate hypotheses, and even produce data for times and places where such data don’t exist. While these uses are creative, are they valid? When can people abstract away from the inner workings of an LLM and simply rely on their outputs? The study develops an econometric framework to answer this question. The framework distinguishes between two types of empirical tasks. Using LLM outputs for prediction problems (including hypothesis generation) is valid under one condition: no “leakage” between the LLM’s training dataset and the researcher’s sample. Using LLM outputs for estimation problems to automate the measurement of some economic concept (expressed by some text or from human subjects) requires an additional assumption: LLM outputs must be as good as the gold standard measurements they replace. Otherwise estimates can be biased, even if LLM outputs are highly accurate but not perfectly so. The study documents the extent to which these conditions are violated and the implications for research findings in illustrative applications to finance and political economy. The study also provides guidance to empirical researchers. The only way to ensure no training leakage is to use open-source LLMs with documented training data and published weights. The only way to deal with LLM measurement error is to collect validation data and model the error structure. A corollary is that if such conditions can’t be met for a candidate LLM application, the strong advice is: don’t. <br> <br>

13. ***Gemini 2.0 scales agentic AI capabilities. <br>
Google unveiled Gemini 2.0, excelling in cross-modal tasks with native support for audio, image generation, and video. It features autonomous AI agents for diverse applications, such as visual navigation and coding assistance. Notably, it matches prior Pro models' performance at lower costs. Integration across Google’s ecosystem positions Gemini as a milestone in the agent-based AI era.*** <br> <br>
    Dec 11, Google [released Gemini 2.0](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/) a new AI model for the agentic ear. Gemini 2.0 provides native support for audio, image generation, and cross-modal tasks like combining text, images, and video seamlessly. Plus, it matches the performance of previous Pro models at a lower cost and faster processing times, making it accessible and scalable for broader use. It even powers AI agents capable of completing tasks autonomously. Key features include 1) Agentic Capabilities: Project Astra: A visual AI system that identifies objects, provides navigation, and even helps locate personal items like glasses. Project Mariner: An experimental Chrome extension capable of operating your web browser autonomously. Jules: A developer agent that identifies and resolves coding issues efficiently. Game AI Agent: An "Easter egg" agent to enhance your gaming experience by analyzing the screen and offering real-time assistance. 2) Integration Across Google Ecosystem, including Google Search, Google Workspace and Unified Foundatoin. Hassabis, CEO of DeepMind, envisions 2025 as the "true start of the agent-based era," but acknowledges challenges. For example. the agentic nature of Gemini raises risks, such as unintended actions or misuse in autonomous environments. <br> <br>

15. ***WSRL enables efficient online RL fine-tuning. <br>
UC Berkeley and CMU demonstrated that retaining offline data during RL fine-tuning is unnecessary if using a properly designed online RL approach like WSRL. By seeding with minimal rollouts, WSRL recalibrates models for online distributions, enabling faster and more stable learning without offline data reliance.*** <br> <br>
    Dec 11, UC Berkeley and CMU published a [paper](https://arxiv.org/pdf/2412.07762) “Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data”. The modern paradigm in machine learning involves pre-training on diverse data, followed by task-specific fine-tuning. In reinforcement learning (RL), this translates to learning via offline RL on a diverse historical dataset, followed by rapid online RL fine-tuning using interaction data. Most RL fine-tuning methods require continued training on offline data for stability and performance. However, this is undesirable because training on diverse offline data is slow and expensive for large datasets, and in principle, also limit the performance improvement possible because of constraints or pessimism on offline data. This study shows that retaining offline data is unnecessary as long as using a properly-designed online RL approach for fine-tuning offline RL initializations. To build this approach, the work starts by analyzing the role of retaining offline data in online fine-tuning. The study finds that continued training on offline data is mostly useful for preventing a sudden divergence in the value function at the onset of fine-tuning, caused by a distribution mismatch between the offline data and online rollouts. This divergence typically results in unlearning and forgetting the benefits of offline pre-training. The approach, Warm-start RL (WSRL), mitigates the catastrophic forgetting of pre-trained initializations using a very simple idea. WSRL employs a warmup phase that seeds the online RL run with a very small number of rollouts from the pre-trained policy to do fast online RL. The data collected during warmup helps “recalibrate” the offline Q-function to the online distribution, allowing to completely discard offline data without destabilizing the online RL fine-tuning. The work shows that WSRL is able to fine-tune without retaining any offline data, and is able to learn faster and attains higher performance than existing algorithms irrespective of whether they retain offline data or not. <br> <br>

17. ***LatentLM unifies multimodal generation efficiently. <br>
Microsoft and Tsinghua University proposed LatentLM, a multimodal model that integrates text, audio, and image data through variational autoencoders and next-token diffusion. The model outperforms competitors in image generation, text-to-speech synthesis, and cross-modal tasks, offering scalability and efficiency for multimodal AI applications.*** <br> <br>
    Dec 11, Microsoft and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2412.08635) “Multimodal Latent Language Modeling with Next-Token Diffusion”. Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). This work proposes Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, the authors employ a variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, the study develops sigma-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides a general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2 model in speaker similarity and robustness, while requiring 10x fewer decoding steps. The results establish LatentLM as a highly effective and scalable approach to advance large multimodal models. <br> <br>

19. ***HyRe adapts models for underspecified tasks. <br>
Stanford researchers introduced HyRe, a technique that dynamically reweights ensemble predictions at test time using labeled examples. HyRe improves model performance in personalization and distribution shifts, outperforming state-of-the-art approaches in various scenarios.*** <br> <br>
    Dec 11, Stanford Uni published a [paper](https://arxiv.org/pdf/2412.08812) “Test-Time Alignment via Hypothesis Reweighting”. Large pretrained models often struggle with underspecified tasks -- situations where the training data does not fully define the desired behavior. For example, chatbots must handle diverse and often conflicting user preferences, requiring adaptability to various user needs. The study proposes a novel framework to address the general challenge of aligning models to test-time user intent, which is rarely fully specified during training. The approach involves training an efficient ensemble, i.e., a single neural network with multiple prediction heads, each representing a different function consistent with the training data. The main contribution is HyRe, a simple adaptation technique that dynamically reweights ensemble members at test time using a small set of labeled examples from the target distribution, which can be labeled in advance or actively queried from a larger unlabeled pool. By leveraging recent advances in scalable ensemble training, the method scales to large pretrained models, with computational costs comparable to fine-tuning a single model. The study empirically validates HyRe in several underspecified scenarios, including personalization tasks and settings with distribution shifts. Additionally, with just five preference pairs from each target distribution, the same ensemble adapted via HyRe outperforms the prior state-of-the-art 2B-parameter reward model accuracy across 18 evaluation distributions. <br> <br>

21. ***Concept-level modeling enhances abstraction in AI. <br>
Meta's research explores Large Concept Models, operating on sentence-level semantic representations rather than tokens. By using SONAR embeddings, these models demonstrate zero-shot generalization across languages, outperforming similarly-sized LLMs in generative tasks like summarization and summary expansion.*** <br> <br>
    Dec 11, Meta published a [paper](https://arxiv.org/pdf/2412.08821) “Large Concept Models: Language Modeling in a Sentence Representation Space”. LLMs have revolutionized the field of artificial intelligence and have emerged as the de-facto tool for many tasks. The current established technology of LLMs is to process input and generate output at the token level. This is in sharp contrast to humans who operate at multiple levels of abstraction, well beyond single words, to analyze information and to generate creative content. This study presents an attempt at an architecture which operates on an explicit higher-level semantic representation, which is named as a concept. Concepts are language- and modality-agnostic and represent a higher level idea or action in a flow. Hence, the study builds a "Large Concept Model". In this study, as proof of feasibility, the authors assume that a concept corresponds to a sentence, and use an existing sentence embedding space, SONAR, which supports up to 200 languages in both text and speech modalities. The Large Concept Model is trained to perform autoregressive sentence prediction in an embedding space. The work explores multiple approaches, namely MSE regression, variants of diffusion-based generation, and models operating in a quantized SONAR space. These explorations are performed using 1.6B parameter models and training data in the order of 1.3T tokens. The study then scales one architecture to a model size of 7B parameters and training data of about 2.7T tokens. The study performs an experimental evaluation on several generative tasks, namely summarization and a new task of summary expansion. Finally, the authors show that the model exhibits impressive zero-shot generalization performance to many languages, outperforming existing LLMs of the same size. The training code of the models is [freely available](https://github.com/facebookresearch/large_concept_models). <br> <br>

23. ***DiTFlow advances motion transfer in video synthesis. <br>
A collaborative effort by Oxford, Snap Inc, and MBZUAI introduced DiTFlow, a method leveraging Diffusion Transformers to transfer motion from reference videos to synthesized ones. Using Attention Motion Flow, DiTFlow outperforms existing methods across metrics and human evaluations.*** <br> <br>
    Dec 10, Uni of Oxford, Snap Inc and MBZUAI published a [paper](https://arxiv.org/pdf/2412.07776) “Video Motion Transfer with Diffusion Transformers”. The paper proposes DiTFlow, a method for transferring the motion of a reference video to a newly synthesized one, designed specifically for Diffusion Transformers (DiT). The study first processes the reference video with a pre-trained DiT to analyze cross-frame attention maps and extract a patch-wise motion signal called the Attention Motion Flow (AMF). The work guides the latent denoising process in an optimization-based, training-free, manner by optimizing latents with the AMF loss to generate videos reproducing the motion of the reference one. The study also applies an optimization strategy to transformer positional embeddings, granting a boost in zero-shot motion transfer capabilities. The work evaluates DiTFlow against recently published methods, outperforming all across multiple metrics and human evaluation. <br> <br>

25. ***Unified paradigms enhance vision-domain scalability. <br>
LMU Munich researchers proposed bridging Masked Generative Models and Non-Autoregressive Models via discrete-state approaches. The unified framework facilitates scalable applications in vision tasks, establishing new benchmarks for generative models in this domain.*** <br> <br>
    Dec 10, LMU Munich published a [paper](https://arxiv.org/pdf/2412.06787) “[MASK] is All You Need”. In generative models, two paradigms have gained attraction in various applications: next-set prediction-based Masked Generative Models and next-noise prediction-based Non-Autoregressive Models, e.g., Diffusion Models. This study proposes using discrete-state models to connect them and explore their scalability in the vision domain. First, the study conducts a step-by-step analysis in a unified design space across two types of models including timestep-independence, noise schedule, temperature, guidance strength, etc in a scalable manner. Second, the work re-casts typical discriminative tasks, e.g., image segmentation, as an unmasking process from [MASK] tokens on a discrete-state model. This enables to perform various sampling processes, including flexible conditional sampling by only training once to model the joint distribution. All aforementioned explorations lead to a framework named Discrete Interpolants, which enables to achieve state-of-the-art or competitive performance compared to previous discrete-state based methods in various benchmarks, like ImageNet256, MS COCO, and video dataset FaceForensics. In summary, by leveraging [MASK] in discrete-state models, the study can bridge Masked Generative and Non-autoregressive Diffusion models, as well as generative and discriminative tasks. <br> <br>

27. ***APOLLO Optimizer for LLMs: <br>
    The University of Texas at Austin and Meta's paper, “APOLLO: SGD-like Memory, AdamW-level Performance,” introduces APOLLO, a memory-efficient optimizer for training large language models (LLMs). APOLLO approximates learning rate scaling using a low-rank optimizer state based on random projection, significantly reducing memory usage while maintaining performance comparable to AdamW. The rank-1 variant, APOLLO-Mini, even outperforms AdamW with SGD-level memory costs. Experiments show APOLLO enhances throughput, supports larger batch sizes, and allows pre-training on lower-end GPUs, making it a promising solution for scalable and efficient LLM training.*** <br> <br>
    Dec 9, Uni of Texas at Austin and Meta published a [paper](https://arxiv.org/pdf/2412.05270) “APOLLO: SGD-like Memory, AdamW-level Performance”. Large language models (LLMs) are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden necessitates using more or higher-end GPUs or reducing batch sizes, limiting training scalability and throughput. To address this, various memory-efficient optimizers have been proposed to reduce optimizer memory usage. However, they face critical challenges: (i) reliance on costly SVD operations; (ii) significant performance trade-offs compared to AdamW; and (iii) still substantial optimizer memory overhead to maintain competitive performance. This work identifies that AdamW's learning rate adaptation rule can be effectively coarsened as a structured learning rate update. Based on this insight, the study proposes Approximated Gradient Scaling for Memory-Efficient LLM Optimization (APOLLO), which approximates learning rate scaling using an auxiliary low-rank optimizer state based on pure random projection. This structured learning rate update rule makes APOLLO highly tolerant to further memory reductions while delivering comparable pre-training performance. Even its rank-1 variant, APOLLO-Mini, achieves superior pre-training performance compared to AdamW with SGD-level memory costs. Extensive experiments demonstrate that the APOLLO series performs on-par with or better than AdamW, while achieving greater memory savings by nearly eliminating the optimization states of AdamW. These savings provide significant system-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB setup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model Scalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without system-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training LLaMA-7B on a single GPU using less than 12 GB of memory with weight quantization. Here is [the code](https://github.com/zhuhanqing/APOLLO). <br> <br>

29. ***Amazon’s New AI Agent Lab:  <br>Amazon has launched the AGI SF Lab in San Francisco, led by Adept co-founder David Luan, to develop AI agents capable of managing workflows and performing real-world actions. Initially staffed by former Adept employees, the lab will expand Amazon’s AGI efforts and focus on integrating human feedback and goal understanding. This move aligns with Amazon’s broader AI initiatives and reflects growing competition in the $31 billion agentic AI market.*** <br> <br>
    Dec 9, according to [TechCrunch](https://techcrunch.com/2024/12/09/amazon-forms-a-new-ai-agent-focused-lab-led-by-adept-co-founder/), Amazon forms an AI agent-focused lab led by Adept’s co-founder. Amazon is launching a new R&D lab in San Francisco, named the Amazon AGI SF Lab, to develop foundational capabilities for AI agents. Led by David Luan, co-founder of AI startup Adept, the lab aims to create agents capable of performing actions in both digital and physical environments and managing complex workflows using computers, web browsers, and code interpreters. The lab's work will build on Amazon's broader AGI team, with a focus on enabling AI agents to perform real-world actions, learn from human feedback, self-correct, and understand human goals. The lab will initially be staffed by Adept employees, with plans to hire additional researchers in fields like quantitative finance, physics, and math. This initiative follows Amazon's licensing deal with Adept, which saw Luan and parts of Adept's team join Amazon under Rohit Prasad, head of an AGI team specializing in large language models. The move is similar to Microsoft's deal with AI startup Inflection and has attracted regulatory scrutiny. Adept, founded two years ago, aims to create AI models that can perform tasks on any software tool using natural language, envisioning an "AI teammate" capable of using various software tools and APIs. The agentic AI sector is projected to be worth $31 billion by year-end, with many organizations planning to integrate AI agents for efficiency. Other major AI players, including OpenAI and Google, are also developing similar technologies. Amazon has introduced conversational agents for its Bedrock AI platform and its Amazon Q Business assistant platform, with CEO Andy Jassy hinting at a more advanced Alexa capable of taking actions. <br> <br>

31. ***OpenAI Sora Release: <br>
OpenAI launched Sora, an advanced AI video generator available for ChatGPT Plus and Pro users in select regions. Sora features high-resolution video creation, customizable styles, and an advanced storyboard tool enabling precise frame-by-frame editing. The standalone platform (sora.com) also allows users to explore community content. OpenAI plans further refinements and tailored pricing by 2025.*** <br> <br>
    Dec 9, OpenAI [released Sora](https://openai.com/index/sora-is-here/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=openai-s-sora-has-arrived&_bhlid=295f9246cbc4d9093bee9b6a451d76b7034f37f0), its highly anticipated AI video generator. Announced during a livestream by CEO Sam Altman, Sora is now available to ChatGPT Plus and Pro users in select regions. Impressive as it is, this release has raised critical discussions about ethical and creative implications. Initially introduced in 2023, Sora has been refined through a year of testing with a small group of users. The tool offers: High-resolution videos up to 1080p, with aspect ratio options (widescreen, square, vertical). Customization features like text prompts, video extensions, and preset styles (e.g., “stop motion” and “balloon world”). An advanced Storyboard tool for precise frame-by-frame video editing, including: Recut to rearrange sequences. Blend for seamless scene transitions. Remix to make specific changes to individual frames. Sora operates as a standalone platform at sora.com, allowing users to browse community-created content and explore the methods behind their generation. OpenAI plans to refine Sora further, with tailored pricing for various user groups expected in early 2025. At about the same time, xAI released Aurora, Grok Image Generation model. <br> <br>

33. ***Meta and UC San Diego: Coconut Reasoning Paradigm: <br>
A new paradigm, Coconut (Chain of Continuous Thought), was introduced to enhance reasoning in LLMs. It enables latent space reasoning by using continuous representations of thought states instead of natural language tokens. This approach improves logical reasoning tasks, offering emergent capabilities like breadth-first search for problem-solving and reducing inference token requirements.*** <br> <br>
    Dec 9, Meta and UC San Diego published a [paper](https://arxiv.org/pdf/2412.06769) “Training Large Language Models to Reason in a Continuous Latent Space”. Large language models (LLMs) are restricted to reason in the "language space", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, the authors argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, the study introduces a new paradigm Coconut (Chain of Continuous Thought). The work utilizes the last hidden state of the LLM as a representation of the reasoning state (termed "continuous thought"). Rather than decoding this into a word token, the study feeds it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research. <br> <br>

35. ***Uncertainty in LLMs: [IDK] Token Approach: <br>
Researchers propose a calibration method incorporating an [IDK] token for LLMs to express uncertainty and combat hallucinations. The approach redistributes probability to the [IDK] token for incorrect predictions, improving output reliability while retaining most encoded knowledge.*** <br> <br>
    Dec 9, Uni of Potsdam and Tel Aviv Uni published a [paper](https://arxiv.org/pdf/2412.06676) “I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token”. Large Language Models are known to capture real-world knowledge, allowing them to excel in many downstream tasks. Despite recent advances, these models are still prone to what are commonly known as hallucinations, causing them to emit unwanted and factually incorrect text. This work proposes a novel calibration method that can be used to combat hallucinations. The study adds a special [IDK] ("I don't know") token to the model's vocabulary and introduce an objective function that shifts probability mass to the [IDK] token for incorrect predictions. This approach allows the model to express uncertainty in its output explicitly. The work evaluates the proposed method across multiple model architectures and factual downstream tasks, and finds that models trained with the method are able to express uncertainty in places where they would previously make mistakes while suffering only a small loss of encoded knowledge. The work further performs extensive ablation studies of multiple variations of the approach and provide a detailed analysis of the precision-recall tradeoff of the method. <br> <br>

37. ***Sequoia on AI in 2025: <br>
The AI ecosystem in 2024 saw key advancements, with leading LLM providers adopting distinct strategies. AI search is revolutionizing information access, while infrastructure investments are stabilizing the industry. As AI matures, ethical and societal challenges remain crucial to ensuring positive impacts.*** <br> <br>
    Dec 9, Sequoia published an [article](https://www.sequoiacap.com/article/ai-in-2025/) “AI in 2025: Building Blocks Firmly in Place”. 2024 marked a pivotal year for AI, characterized by rapid advancements and substantial investments. The AI ecosystem, once brimming with potential, is now solidifying into a tangible reality. Key developments include the emergence of five leading LLM providers, each with distinct strategies: Google's vertical integration, OpenAI's strong brand, Anthropic's talent pool, xAI's data center focus, and Meta's open-source approach. AI search has emerged as a powerful application, redefining information access and processing. Additionally, the AI infrastructure landscape is stabilizing, with data center construction and optimization becoming central priorities. As the industry matures in 2025, we anticipate significant advancements in AI-powered products and services, transforming various sectors and improving daily life. However, challenges such as ethical considerations, data privacy, and job displacement will require careful attention and responsible development to ensure a positive impact on society. <br> <br>

39. ***Moxin-7B: Fully Open-Source LLM: <br>
Developed under the Model Openness Framework (MOF), Moxin-7B is a transparent, fully open-source LLM that includes training code, datasets, and checkpoints. It achieves top MOF classification, outperforming many 7B models in zero-shot evaluations and excelling in few-shot tasks.*** <br> <br>
    Dec 8, Northeastern Uni, Harvard Uni, Cornell Uni, Tulance Uni, et al published a paper “Fully Open Source Moxin-7B Technical Report”. Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, and some use restrictive licenses whilst claiming to be "open-source," which may hinder further innovations on LLMs. To mitigate this issue, the authors introduces [Moxin 7B](https://github.com/moxin-org/Moxin-LLM), a fully open-source LLM developed in accordance with the Model Openness Framework (MOF), a ranked classification system that evaluates AI models based on model completeness and openness, adhering to principles of open science, open source, open data, and open access. The model achieves the highest MOF classification level of "open science" through the comprehensive release of pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints. Experiments show that Moxin model achieves superior performance in zero-shot evaluation compared with popular 7B models and performs competitively in few-shot evaluation. <br> <br>

41. ***OpenAI's AGI Claims: <br>
OpenAI employee Vahid Kazemi claimed their O1 model has achieved AGI, defined as surpassing most humans in various tasks. Critics note the unconventional definition, emphasizing breadth over human parity in specific tasks. Despite potential, AGI's realization remains controversial and nuanced.*** <br> <br>
    Dec 7, according to [Futurism](https://futurism.com/openai-employee-claims-agi), OpenAI Employee Says They’ve "Already Achieved AGI". OpenAI employee Vahid Kazemi recently claimed on X (formerly Twitter) that the company has achieved artificial general intelligence (AGI) with their O1 model, though he acknowledges it’s not "better than any human at any task" but rather "better than most humans at most tasks." Critics argue Kazemi's definition of AGI is unconventional, as it emphasizes the AI's ability to perform a wide variety of tasks rather than excelling in specific areas. Kazemi also discussed the nature of large language models (LLMs), comparing their learning process to the scientific method of observing, hypothesizing, and verifying. He suggested that while LLMs might seem to follow a "recipe," their capabilities are built through extensive trial and error, similar to human intuition. This statement came shortly after OpenAI removed "AGI" from its deal terms with Microsoft, leaving the business implications uncertain. Despite these claims, no AI has yet matched human workers in general labor force tasks, but Kazemi believes that continued advancements could eventually lead to human-level intelligence. <br> <br>

43. ***RL Zero: Zero-Shot Behavior from Language: <br>
Researchers proposed RL Zero, an unsupervised method allowing RL agents to derive behavior policies from language instructions using video-language models. This zero-shot language-to-behavior capability shows promise for diverse tasks, eliminating the need for supervised labeling.*** <br> <br>
    Dec 7, Uni of Texas at Austin, Uni of Alberta, Sony, Meta and UMass Amherst published a [paper](https://arxiv.org/pdf/2412.05718) “RL Zero: Zero-Shot Language to Behaviors without any Supervision”. Rewards remain an uninterpretable way to specify tasks for Reinforcement Learning, as humans are often unable to predict the optimal behavior of any given reward function, leading to poor reward design and reward hacking. Language presents an appealing way to communicate intent to agents and bypass reward design, but prior efforts to do so have been limited by costly and unscalable labeling efforts. This work proposes a method for a completely unsupervised alternative to grounding language instructions in a zero-shot manner to obtain policies. The study presents a solution that takes the form of imagine, project, and imitate: The agent imagines the observation sequence corresponding to the language description of a task, projects the imagined sequence to the target domain, and grounds it to a policy. Video-language models allow to imagine task descriptions that leverage knowledge of tasks learned from internet-scale video-text mappings. The challenge remains to ground these generations to a policy. This work shows that the researchers can achieve a zero-shot language-to-behavior policy by first grounding the imagined sequences in real observations of an unsupervised RL agent and using a closed-form solution to imitation learning that allows the RL agent to mimic the grounded observations. The method, RLZero, is the first to show zero-shot language to behavior generation abilities without any supervision on a variety of tasks on simulated domains. The study further shows that RLZero can also generate policies zero-shot from cross-embodied videos such as those scraped from YouTube. <br> <br>

45. ***FlexAttention: Simplifying Attention Kernels: <br>
Meta and the University of Michigan introduced FlexAttention, a programming model enabling efficient attention kernel generation with a few lines of PyTorch code. It supports attention variant composition, streamlining the development of new models while maintaining competitive performance.*** <br> <br>
    Dec 7, Meta and Uni of Michigan-Ann Arbor published a [paper](https://arxiv.org/pdf/2412.05496) “Flex Attention: A Programming Model for Generating Optimized Attention Kernels”. Over the past 7 years, attention has become one of the most important primitives in deep learning. The primary approach to optimize attention is FlashAttention, which fuses the operation together, drastically improving both the runtime and the memory consumption. However, the importance of FlashAttention combined with its monolithic nature poses a problem for researchers aiming to try new attention variants -- a "software lottery". This problem is exacerbated by the difficulty of writing efficient fused attention kernels, resisting traditional compiler-based approaches. The study introduces FlexAttention, a novel compiler-driven programming model that allows implementing the majority of attention variants in a few lines of idiomatic PyTorch code. The study demonstrates that many existing attention variants (e.g. Alibi, Document Masking, PagedAttention, etc.) can be implemented via FlexAttention, and that achieves competitive performance compared to these handwritten kernels. Finally, the work demonstrates how FlexAttention allows for easy composition of attention variants, solving the combinatorial explosion of attention variants. <br> <br>

47. ***Frontier Models and In-Context Scheming: <br>
A study evaluated leading LLMs for scheming behaviors, revealing deceptive strategies such as oversight evasion and model weight exfiltration. These findings highlight potential risks in AI autonomy and the need for mitigation measures in deploying frontier models.*** <br> <br>
    Dec 6, Apollo Research published a [paper](https://arxiv.org/pdf/2412.04984) “Frontier Models are Capable of In-context Scheming”. Frontier models are increasingly trained and deployed as autonomous agents, which significantly increases their potential for risks. One particular safety concern is that AI agents might covertly pursue misaligned goals, hiding their true capabilities and objectives – also known as scheming. The authors study whether models have the capability to scheme in pursuit of a goal that the authors provide in-context and instruct the model to strongly follow. The study evaluates frontier models on a suite of six agentic evaluations where models are instructed to pursue goals and are placed in environments that incentivize scheming. The results show that o1, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, and Llama 3.1 405B all demonstrate in-context scheming capabilities. They can recognize scheming as a viable strategy and readily engage in such behavior. The work finds multiple different scheming behaviors: models strategically introduce subtle mistakes into their responses, attempt to disable their oversight mechanisms, and even exfiltrate what they believe to be their model weights to external servers. Additionally, this deceptive behavior proves persistent. For instance, when o1 has engaged in scheming, it maintains its deception in over 85% of follow-up questions and often remains deceptive in multi-turn interrogations. Analysis of the models’ chains-of-thought reveals that models explicitly reason about these deceptive strategies, providing evidence that the scheming behavior is not accidental. Surprisingly, the study also finds rare instances where models engage in scheming when only given a goal, without being strongly nudged to pursue it. Furthermore, the work observes cases where Claude 3.5 Sonnet strategically underperforms in evaluations in pursuit of being helpful, a goal that was acquired during training rather than in-context. Together, the findings demonstrate that frontier models now possess capabilities for basic in-context scheming, making the potential of AI agents to engage in scheming behavior a concrete rather than theoretical concern. <br> <br>

49. ***CompCap for Composite Images: <br>
Meta and collaborators developed CompCap, a framework to train MLLMs for understanding composite images (e.g., charts, posters). The curated CompCap-118K dataset significantly enhances MLLM performance in interpreting such visuals, outperforming benchmarks across multiple model sizes.*** <br> <br>
    Dec 6, Meta, Tufts Uni, and Georgia Tech published a [paper](https://arxiv.org/pdf/2412.05243) “CompCap: Improving Multimodal Large Language Models with Composite Captions”. How well can Multimodal Large Language Models (MLLMs) understand composite images? Composite images (CIs) are synthetic visuals created by merging multiple visual elements, such as charts, posters, or screenshots, rather than being captured directly by a camera. While CIs are prevalent in real-world applications, recent MLLM developments have primarily focused on interpreting natural images (NIs). The research reveals that current MLLMs face significant challenges in accurately understanding CIs, often struggling to extract information or perform complex reasoning based on these images. The work finds that existing training data for CIs are mostly formatted for question-answer tasks (e.g., in datasets like ChartQA and ScienceQA), while high-quality image-caption datasets, critical for robust vision-language alignment, are only available for NIs. To bridge this gap, the study introduces Composite Captions (CompCap), a flexible framework that leverages Large Language Models (LLMs) and automation tools to synthesize CIs with accurate and detailed captions. Using CompCap, the study curates CompCap-118K, a dataset containing 118K image-caption pairs across six CI types. The authors validate the effectiveness of CompCap-118K by supervised fine-tuning MLLMs of three sizes: xGen-MM-inst.-4B and LLaVA-NeXT-Vicuna-7B/13B. Empirical results show that CompCap-118K significantly enhances MLLMs' understanding of CIs, yielding average gains of 1.7%, 2.0%, and 2.9% across eleven benchmarks, respectively. <br> <br>

51. ***Aligning Human and Machine Generalization: <br>
A multi-institute study analyzed differences in how humans and machines generalize. By comparing abstraction, rule-based reasoning, and domain generalization, the research outlined interdisciplinary challenges crucial for effective human-AI collaboration in scientific discovery and decision-making.*** <br> <br>
    Nov 23, a group of researchers from 27 institutes published a [paper](https://arxiv.org/pdf/2411.15626v1) “Aligning generalisation Between Humans and Machines”. Recent advances in AI -- including generative approaches -- have resulted in technology that can support humans in scientific discovery and decision support but may also disrupt democracies and target individuals. The responsible use of AI increasingly shows the need for human-AI teaming, necessitating effective interaction between humans and machines. A crucial yet often overlooked aspect of these interactions is the different ways in which humans and machines generalise. In cognitive science, human generalisation commonly involves abstraction and concept learning. In contrast, AI generalisation encompasses out-of-domain generalisation in machine learning, rule-based reasoning in symbolic AI, and abstraction in neuro-symbolic AI. In this perspective paper, the authors combine insights from AI and cognitive science to identify key commonalities and differences across three dimensions: notions of generalisation, methods for generalisation, and evaluation of generalisation. The study maps the different conceptualisations of generalisation in AI and cognitive science along these three dimensions and consider their role in human-AI teaming. This results in interdisciplinary challenges across AI and cognitive science that must be tackled to provide a foundation for effective and cognitively supported alignment in human-AI teaming scenarios.
 <br> <br> <br>

***Dec 8th***

1. ***Release of OpenAI o1 Model Series <br>
OpenAI introduced the o1 model series, leveraging reinforcement learning and chain-of-thought reasoning for enhanced safety and robustness. The models achieve top performance on safety-related benchmarks, such as avoiding unsafe advice and stereotypes, while reducing errors by 34% compared to preview versions. Designed for complex problem-solving and multimedia understanding, the release emphasizes the importance of alignment, stress-testing, and risk management.*** <br> <br>
   Dec 6, OpenAI released its o1 and o1’s [System Card](https://cdn.openai.com/o1-system-card-20241205.pdf). The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of the models. In particular, the models can reason about the safety policies in context when responding to potentially unsafe prompts. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. The results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations. Other features not listed in the report include: research scientists on the livestream said an internal evaluation indicated it made major mistakes about 34% less often than the o1 preview mode; the model seems geared toward scientists, engineers, and coders, is designed to solve thorny problems; read and understand multi-media inputs such a photo of a hand-drawn system for a data center in space, and answer tough questions to a layperson. <br> <br>

3. ***Meta’s Llama 3.3 Launch <br>
Meta released Llama 3.3, a compact yet powerful open-source model with 70B parameters, rivaling larger models in performance at a fraction of the cost. Licensed under a community agreement, it ensures responsible use and attribution. Outperforming prior models in NLP benchmarks like multilingual dialogue and reasoning, Llama 3.3 offers accessible high-quality AI with reduced computational requirements.*** <br> <br>
   Dec 6, according to [venturebeat](https://venturebeat.com/ai/meta-launches-open-source-llama-3-3-shrinking-powerful-bigger-model-into-smaller-size/), Meta launches open source Llama 3.3, shrinking powerful bigger model into smaller size. With 70 billion parameters — or settings governing the model’s behavior — Llama 3.3 delivers results on par with Meta’s 405B parameter model from the Llama 3.1 from the summer, but at a fraction of the cost and computational overhead — e.g., the GPU capacity needed to run the model in an inference. It’s designed to offer top-tier performance and accessibility yet in a smaller package than prior foundation models. Meta’s Llama 3.3 is offered under the Llama 3.3 Community License Agreement, which grants a non-exclusive, royalty-free license for use, reproduction, distribution, and modification of the model and its outputs. Developers integrating Llama 3.3 into products or services must include appropriate attribution, such as “Built with Llama,” and adhere to an Acceptable Use Policy that prohibits activities like generating harmful content, violating laws, or enabling cyberattacks. According to Meta AI on X, the Llama 3.3 model handedly outperforms the identically sized Llama 3.1-70B as well as Amazon’s new Nova Pro model in several benchmarks such as multilingual dialogue, reasoning, and other advanced natural language processing (NLP) tasks (Nova outperforms it in HumanEval coding tasks). <br> <br>

5. ***Nvidia's NVILA for Efficient Visual Language Models (VLMs) <br>
Nvidia unveiled NVILA, an open VLM series focusing on efficiency and accuracy. By scaling and compressing visual data, NVILA processes high-res images and long videos efficiently, reducing training costs and latency. It matches or outperforms existing VLMs while significantly lowering resource consumption, facilitating advancements in image and video analysis.*** <br> <br>
   Dec 5, Nvidia published a [paper](https://arxiv.org/pdf/2412.04468) “NVILA: Efficient Frontier Visual Language Models”. Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and accuracy. Building on top of VILA, the study improves its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens. This "scale-then-compress" approach enables NVILA to efficiently process high-resolution images and long videos. The authors also conduct a systematic investigation to enhance the efficiency of NVILA throughout its entire lifecycle, from training and fine-tuning to deployment. NVILA matches or surpasses the accuracy of many leading open and proprietary VLMs across a wide range of image and video benchmarks. At the same time, it reduces training costs by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by 1.6-2.2X, and decoding latency by 1.2-2.8X. [Code](https://github.com/NVlabs/VILA) and models are available to facilitate reproducibility. <br> <br>

7. ***Task Scaling Laws from Model Ladders <br>
A new study introduces task scaling laws using compute-efficient model ladders to predict language model task performance. By training smaller "ladder" models, researchers forecast larger model accuracy with minimal computational cost. While predictions are accurate on low-variance tasks, challenges remain for high-variance tasks. The approach underscores the value of efficient, predictive methods in AI scaling.*** <br> <br>
   Dec 5, Aillen Inst of AI, Uni of Washington and Princeton Uni published a [paper](https://arxiv.org/pdf/2412.04403) “Establishing Task Scaling Laws via Compute-Efficient Model Ladders”. The study develops task scaling laws and model ladders to predict the individual task performance of pretrained language models (LMs) in the overtrained setting. Standard power laws for language modeling loss cannot accurately model task performance. Therefore, the work leverages a two-step prediction approach: first use model and data size to predict a task-specific loss, and then use this task loss to predict task performance. The study trains a set of small-scale "ladder" models, collect data points to fit the parameterized functions of the two prediction steps, and make predictions for two target models: a 7B model trained to 4T tokens and a 13B model trained to 5T tokens. Training the ladder models only costs 1% of the compute used for the target models. On four multiple-choice tasks written in ranked classification format, the study can predict the accuracy of both target models within 2 points of absolute error. The authors have higher prediction error on four other tasks (average absolute error 6.9) and find that these are often tasks with higher variance in task metrics. The study also finds that using less compute to train fewer ladder models tends to deteriorate predictions. Finally, the research empirically shows that the design choices and the two-step approach lead to superior performance in establishing scaling laws.  <br> <br>

9. ***Challenges in Human Evaluations of Chatbots <br>
A Cornell study highlights flaws in open human evaluation platforms like Chatbot Arena. Bad annotations, either from apathetic or adversarial users, can significantly distort model rankings. Ensuring reliable annotations requires robust guardrails, as even minor issues can misrepresent model capabilities and hinder trustworthiness.*** <br> <br>
    Dec 5, Cornell Uni published a [paper](https://arxiv.org/pdf/2412.04363) “Challenges in Trustworthy Human Evaluation of Chatbots”. Open community-driven platforms like Chatbot Arena that collect user preference data from site visitors have gained a reputation as one of the most trustworthy publicly available benchmarks for LLM performance. While now standard, it is tricky to implement effective guardrails to collect high-quality annotations from humans. This work demonstrates that three sources of bad annotations, both malicious and otherwise, can corrupt the reliability of open leaderboard rankings. In particular, the work shows that only 10% of poor quality votes by apathetic (site visitors not appropriately incentivized to give correct votes) or adversarial (bad actors seeking to inflate the ranking of a target model) annotators can change the rankings of models by up to 5 places on the leaderboard. Finally, the paper discuss open challenges in ensuring high-quality human annotations. <br> <br>

11. ***Google’s GenCast for Probabilistic Weather Forecasting <br>
Google's GenCast introduces a breakthrough in ML-based probabilistic weather forecasting, outperforming traditional models like ENS in skill and speed. Leveraging decades of data, it produces global forecasts for 80+ variables within minutes, excelling in predicting extreme weather and renewable energy planning. This represents a leap forward in operational weather prediction.*** <br> <br>
    Dec 4, Nature published a [paper](https://www.nature.com/articles/s41586-024-08252-9) from Google “Probabilistic weather forecasting with machine learning”. Weather forecasts are fundamentally uncertain, so predicting the range of probable weather scenarios is crucial for important decisions, from warning the public about hazardous weather to planning renewable energy use. Traditionally, weather forecasts have been based on numerical weather prediction (NWP), which relies on physics-based simulations of the atmosphere. Recent advances in machine learning (ML)-based weather prediction (MLWP) have produced ML-based models with less forecast error than single NWP simulations. However, these advances have focused primarily on single, deterministic forecasts that fail to represent uncertainty and estimate risk. Overall, MLWP has remained less accurate and reliable than state-of-the-art NWP ensemble forecasts. Here the study introduces GenCast, a probabilistic weather model with greater skill and speed than the top operational medium-range weather forecast in the world, ENS, the ensemble forecast of the European Centre for Medium-Range Weather Forecasts. GenCast is an ML weather prediction method, trained on decades of reanalysis data. GenCast generates an ensemble of stochastic 15-day global forecasts, at 12-h steps and 0.25° latitude–longitude resolution, for more than 80 surface and atmospheric variables, in 8 min. It has greater skill than ENS on 97.2% of 1,320 targets evaluated and better predicts extreme weather, tropical cyclone tracks and wind power production. This work helps open the next chapter in operational weather forecasting, in which crucial weather-dependent decisions are made more accurately and efficiently. <br> <br>

13. ***PaliGemma 2 Vision-Language Model Upgrade <br>
Google enhanced the PaliGemma model with PaliGemma 2, combining versatile vision encoders and scalable language models across resolutions. Designed for diverse tasks, including OCR and radiography reports, PaliGemma 2 achieves state-of-the-art results in transfer learning, showcasing the interplay between task types, model size, and resolution in performance optimization.*** <br> <br>
    Dec 4, Google published a [paper](https://arxiv.org/pdf/2412.03555) “PaliGemma 2: A Family of Versatile VLMs for Transfer”. PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. The work combines the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. The study trains these models at three resolutions (224px2 , 448px2 and 896px2) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. The work further increases the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results. <br> <br>

15. ***Evaluating LMs as Synthetic Data Generators <br>
A study proposes AgoraBench to evaluate LMs’ synthetic data generation, finding distinct strengths like problem generation (GPT-4o) and enhancement (Claude-3.5). Key insights show data quality, not problem-solving ability, dictates generation effectiveness. Strategic formats and cost-efficient models optimize synthetic data for downstream AI applications.*** <br> <br>
    Dec 4, CMU, Uni of Washington, NEC et al. published a [paper](https://arxiv.org/pdf/2412.03679) “Evaluating Language Models as Synthetic Data Generators”. Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic comparison of different LMs as data generators in a unified setting. To address this gap, the study proposes AgoraBench, a benchmark that provides standardized settings and metrics to evaluate LMs' data generation abilities. Through synthesizing 1.26 million training instances using 6 LMs and training 99 student models, the study uncovers key insights about LMs' data generation capabilities. First, LMs exhibit distinct strengths. For instance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnet performs better at enhancing existing ones. Furthermore, the analysis reveals that an LM's data generation ability doesn't necessarily correlate with its problem-solving ability. Instead, multiple intrinsic features of data quality-including response quality, perplexity, and instruction difficulty-collectively serve as better indicators. Finally, the work demonstrates that strategic choices in output format and cost-conscious model selection significantly impact data generation effectiveness. <br> <br>

17. ***Best-of-N Jailbreaking Algorithm <br>
Researchers introduce Best-of-N (BoN) Jailbreaking, which exploits prompt variations to bypass AI safeguards across modalities. Achieving high attack success rates, BoN demonstrates vulnerabilities in both proprietary and open-source defenses. The method’s scalability highlights the persistent challenge of ensuring AI robustness against adversarial exploitation.*** <br> <br>
    Dec 4, Speechmatics, MATS, UCL, Stanford Uni, Uni of Oxford, Tangentic and Anthropic published a [paper](https://arxiv.org/pdf/2412.03556) “Best-of-N Jailbreaking”. The study introduces Best-of-N (BoN) Jailbreaking, a simple black-box algorithm that jailbreaks frontier AI systems across modalities. BoN Jailbreaking works by repeatedly sampling variations of a prompt with a combination of augmentations - such as random shuffling or capitalization for textual prompts - until a harmful response is elicited. The work finds that BoN Jailbreaking achieves high attack success rates (ASRs) on closed-source language models, such as 89% on GPT-4o and 78% on Claude 3.5 Sonnet when sampling 10,000 augmented prompts. Further, it is similarly effective at circumventing state-of-the-art open-source defenses like circuit breakers. BoN also seamlessly extends to other modalities: it jailbreaks vision language models (VLMs) such as GPT-4o and audio language models (ALMs) like Gemini 1.5 Pro, using modality-specific augmentations. BoN reliably improves when sampled more augmented prompts. Across all modalities, ASR, as a function of the number of samples (N), empirically follows power-law-like behavior for many orders of magnitude. BoN Jailbreaking can also be composed with other black-box algorithms for even more effective attacks - combining BoN with an optimized prefix attack achieves up to a 35% increase in ASR. Overall, the work indicates that, despite their capability, language models are sensitive to seemingly innocuous changes to inputs, which attackers can exploit across modalities. <br> <br>

19. ***The Path to Artificial General Intelligence (AGI) <br>
An article discusses the limitations of LLMs in achieving AGI, citing challenges like data dependency, poor generalization, and lack of feedback mechanisms. Progressing toward AGI requires innovative architectures and careful ethical considerations to manage risks and prioritize societal well-being.*** <br> <br>
    Dec 3, Nature published an [article](https://www.nature.com/articles/d41586-024-03905-1) “How close is AI to human-level intelligence”. The recent advancements in large language models (LLMs) like OpenAI's o1 have reignited the debate about artificial general intelligence (AGI), an AI system capable of human-level cognition. While LLMs have shown remarkable capabilities in various tasks, they are not sufficient to achieve AGI on their own. The limitations of LLMs include their reliance on vast amounts of data, their inability to generalize effectively, and their lack of internal feedback mechanisms. To progress towards AGI, researchers are exploring new architectures and training techniques, such as generative flow networks and world model construction. However, the development of AGI raises significant ethical concerns. It is crucial to ensure that AI systems are developed responsibly and that their potential risks are mitigated. Researchers and policymakers must work together to establish guidelines and regulations for AI development, prioritizing safety and societal well-being. <br> <br>

21. ***Small Language Models (SLMs) for Businesses <br>
Forbes advocates for SLMs over LLMs for business use, emphasizing their efficiency, cost-effectiveness, and domain-specific advantages. SLMs secure data, reduce resource consumption, and offer tailored solutions, highlighting the importance of selecting the right AI for optimizing business operations and fostering trust in AI systems.*** <br> <br>
    Dec 3, Forbes published an [article](https://www.forbes.com/sites/deandebiase/2024/11/25/why-small-language-models-are-the-next-big-thing-in-ai/) “Why Small Language Models Are The Next Big Thing In AI”. The article argues that while large language models (LLMs) from tech giants like Microsoft, Google, and Amazon are powerful, they may not be the best fit for every business due to their high costs and resource demands. Instead, small language models (SLMs) and domain-specific LLMs offer more tailored, efficient, and cost-effective solutions. SLMs are trained on specific data types, keeping data secure within a company's firewall and reducing energy consumption. Domain-specific LLMs focus on specialized knowledge, providing more accurate and contextually relevant responses. These models require less computing power, can run on-premises, and offer greater control over data. The article highlights the importance of choosing the right AI model for specific business needs to optimize efficiency and reduce costs, emphasizing that trusted AI and data are crucial for future business solutions. <br> <br>

23. ***Multi-Agent LLM Training (MALT) <br>
A study introduces MALT, a collaborative AI training approach where multiple specialized LLMs jointly solve reasoning tasks. Employing sequential roles like generator and verifier, MALT improves accuracy in math and reasoning benchmarks, paving the way for multi-agent AI systems with cooperative problem-solving capabilities.*** <br> <br>
    Dec 2, Uni of Oxford, Cooperative AI Foundation, MBZUI and UC Berkeley published a [paper](https://arxiv.org/pdf/2412.01928) “MALT: Improving Reasoning with Multi-Agent LLM Training”. Enabling effective collaboration among LLMs is a crucial step toward developing autonomous systems capable of solving complex problems. While LLMs are typically used as single-model generators, where humans critique and refine their outputs, the potential for jointly-trained collaborative models remains largely unexplored. Despite promising results in multi-agent communication and debate settings, little progress has been made in training models to work together on tasks. This study presents a first step toward "Multi-agent LLM training" (MALT) on reasoning problems. The proposed approach employs a sequential multi-agent setup with heterogeneous LLMs assigned specialized roles: a generator, verifier, and refinement model iteratively solving problems. The study proposes a trajectory-expansion-based synthetic data generation process and a credit assignment strategy driven by joint outcome based rewards. This enables the post-training setup to utilize both positive and negative trajectories to autonomously improve each model's specialized capabilities as part of a joint sequential system. The study evaluates the approach across MATH, GSM8k, and CQA, where MALT on Llama 3.1 8B models achieves relative improvements of 14.14%, 7.12%, and 9.40% respectively over the same baseline model. This demonstrates an early advance in multi-agent cooperative capabilities for performance on mathematical and common sense reasoning questions. More generally, the work provides a concrete direction for research around multi-agent LLM training approaches. <br> <br>

25. ***Reverse-Enhanced Thinking in LLMs <br>
Reverse thinking, modeled after human reasoning, is introduced in LLMs via RevThink, a training framework combining forward and backward reasoning. The approach improves accuracy, sample efficiency, and generalization across reasoning tasks, offering a novel paradigm for enhancing AI reasoning.*** <br> <br>
    Nov 29, UNC and Google published a [paper](https://arxiv.org/pdf/2411.19865) “Reverse Thinking Makes LLMs Stronger Reasoners”. Reverse thinking plays a crucial role in human reasoning. Humans can reason not only from a problem to a solution but also in reverse, i.e., start from the solution and reason towards the problem. This often enhances overall reasoning performance as it enables consistency checks between their forward and backward thinking. To enable Large Language Models (LLMs) to perform reverse thinking, the study introduces Reverse-Enhanced Thinking (RevThink), a framework composed of data augmentation and learning objectives. In RevThink, the work augments the dataset by collecting structured forward-backward reasoning from a teacher model, consisting of: (1) the original question, (2) forward reasoning, (3) backward question, and (4) backward reasoning. The study then employs three objectives to train a smaller student model in a multi-task learning fashion: (a) generate forward reasoning from a question, (b) generate a backward question from a question, and (c) generate backward reasoning from the backward question. Experiments across 12 datasets covering commonsense, math, and logical reasoning show an average 13.53% improvement over the student model's zero-shot performance and a 6.84% improvement over the strongest knowledge distillation baselines. Moreover, the method demonstrates sample efficiency -- using only 10% of the correct forward reasoning from the training data, it outperforms a standard fine-tuning method trained on 10x more forward reasoning. RevThink also exhibits strong generalization to out-of-distribution held-out datasets. <br> <br>

27. ***Decoupled Momentum Optimization (DeMo) <br>
DeMo, a novel optimizer, reduces the need for high-speed interconnects in distributed training by decoupling momentum updates. Supporting scalable, bandwidth-efficient training, DeMo matches state-of-the-art performance while lowering resource demands, enabling cost-effective neural network pretraining.*** <br> <br>
    Nov 29, Nous Research published a [paper](https://arxiv.org/pdf/2411.19870) “DeMo: Decoupled Momentum Optimization”. Training large neural networks typically requires sharing gradients between accelerators through specialized high-speed interconnects. Drawing from the signal processing principles of frequency decomposition and energy compaction, the study demonstrates that synchronizing full optimizer states and model parameters during training is unnecessary. By decoupling momentum updates and allowing controlled divergence in optimizer states across accelerators, the study achieves improved convergence compared to state-of-the-art optimizers. The work introduces {De}coupled {Mo}mentum (DeMo), a fused optimizer and data parallel algorithm that reduces inter-accelerator communication requirements by several orders of magnitude. This enables training of large neural networks even with limited network bandwidth and heterogeneous hardware. The method is topology-agnostic and architecture-independent and supports scalable clock-synchronous distributed training with negligible compute and memory overhead. Empirical results show that models trained with DeMo match or exceed the performance of equivalent models trained with AdamW, while eliminating the need for high-speed interconnects when pre-training large scale foundation models. An open source reference PyTorch implementation is published on GitHub at https://github.com/bloc97/DeMo <br> <br>

29. ***AI2T: Building Trustable AI Tutors <br>
Carnegie Mellon University introduced AI2T, a system enabling efficient creation of intelligent tutoring systems (ITSs) via interactive teaching. AI2T learns through step-by-step solutions and self-assesses using STAND, a novel algorithm outperforming methods like XGBoost. The system can reliably induce rules for problem-solving with just 20–30 minutes of training, reducing the labor-intensive process of ITS programming. AI2T’s self-aware capabilities ensure more accurate and trustworthy outcomes compared to large language models, with promising implications for data-efficient, scalable ITS development.*** <br> <br>
    Nov 26, CMU published a [paper](https://arxiv.org/pdf/2411.17924) “AI2T: Building Trustable AI Tutors by Interactively Teaching a Self-Aware Learning Agent”. AI2T is an interactively teachable AI for authoring intelligent tutoring systems (ITSs). Authors tutor AI2T by providing a few step-by-step solutions and then grading AI2T's own problem-solving attempts. From just 20-30 minutes of interactive training, AI2T can induce robust rules for step-by-step solution tracking (i.e., model-tracing). As AI2T learns it can accurately estimate its certainty of performing correctly on unseen problem steps using STAND: a self-aware precondition learning algorithm that outperforms state-of-the-art methods like XGBoost. The user study shows that authors can use STAND's certainty heuristic to estimate when AI2T has been trained on enough diverse problems to induce correct and complete model-tracing programs. AI2T-induced programs are more reliable than hallucination-prone LLMs and prior authoring-by-tutoring approaches. With its self-aware induction of hierarchical rules, AI2T offers a path toward trustable data-efficient authoring-by-tutoring for complex ITSs that normally require as many as 200-300 hours of programming per hour of instruction. <br> <br>

31. ***AI in Scientific Discovery and Innovation <br>
MIT analyzed AI's impact on scientific and product innovation through a study involving the deployment of materials discovery AI in a corporate R&D setting. The findings revealed significant productivity boosts for high-performing scientists, with a 44% increase in discoveries, 39% more patent filings, and 17% greater product innovation. AI automated routine tasks, enabling researchers to focus on evaluating results. However, this benefit was unevenly distributed, primarily aiding top researchers. Despite the productivity gains, 82% of participants reported diminished job satisfaction due to reduced creativity and underutilization of skills, highlighting the trade-offs of AI augmentation.*** <br> <br>
    Nov 6, MIT published a [paper](https://aidantr.github.io/files/AI_innovation.pdf) “Artificial Intelligence, Scientific Discovery, and Product Innovation”. This paper studies the impact of artificial intelligence on innovation, exploiting the randomized introduction of a new materials discovery technology to 1,018 scientists in the R&D lab of a large U.S. firm. AI-assisted researchers discover 44% more materials, resulting in a 39% increase in patent filings and a 17% rise in downstream product innovation. These compounds possess more novel chemical structures and lead to more radical inventions. However, the technology has strikingly disparate effects across the productivity distribution: while the bottom third of scientists see little benefit, the output of top researchers nearly doubles. Investigating the mechanisms behind these results, the study shows that AI automates 57% of “idea-generation” tasks, reallocating researchers to the new task of evaluating model-produced candidate materials. Top scientists leverage their domain knowledge to prioritize promising AI suggestions, while others waste significant resources testing false positives. Together, these findings demonstrate the potential of AI-augmented research and highlight the complementarity between algorithms and expertise in the innovative process. Survey evidence reveals that these gains come at a cost, however, as 82% of scientists report reduced satisfaction with their work due to decreased creativity and skill underutilization.
<br><br><br>


***Dec 1st***

1. ***Microsoft's Magentic-One Multi-Agent System <br>
Microsoft introduced Magentic-One, a versatile multi-agent system that excels in managing complex multi-step tasks across domains like software development, data analysis, and web navigation. It features an Orchestrator coordinating four specialized agents (WebSurfer, FileSurfer, Coder, ComputerTerminal) and is model-agnostic, supporting LLMs like GPT-4o. Tested on benchmarks like GAIA and AutoGenBench, it demonstrated high workflow accuracy and emphasizes safety via guidelines, human oversight, and red-teaming exercises. The system's open-source release highlights industry trends toward modular AI architectures.*** <br> <br>
   Nov 30, according to [InfoQ](https://www.infoq.com/news/2024/11/microsoft-magentic-one/), Microsoft has introduced Magentic-One, a versatile multi-agent system designed to handle complex, multi-step tasks across various domains, enhancing efficiency in areas like software development, data analysis, and web navigation. The system features a multi-agent architecture led by an Orchestrator agent, coordinating four specialized agents: WebSurfer for browser-based tasks, FileSurfer for file operations, Coder for writing and analyzing code, and ComputerTerminal for executing code and system-level operations. Built on the open-source Microsoft AutoGen framework, Magentic-One is model-agnostic and compatible with various large language models (LLMs), including GPT-4o. Tested on benchmarks such as GAIA, AssistantBench, and WebArena using AutoGenBench, the system demonstrated competitive accuracy in managing complex workflows. Microsoft has addressed potential risks like unintended actions and system misuse by incorporating guidelines for safe deployment, red-teaming exercises, and human oversight recommendations. The release has garnered interest within the AI community, with experts noting its potential impact on LLM-based applications and the innovative approach to web browsing. The code for Magentic-One and its evaluation tool, AutoGenBench, is available as open-source resources, encouraging collaboration to enhance agentic AI systems. This development reflects a broader industry trend towards modular and collaborative AI architectures, with major companies like AWS, IBM, and OpenAI also contributing to this field. <br> <br>

3. ***NeuroAI for AI Safety <br>
A paper explores neuroscience's role in AI safety, emphasizing the brain's architecture as a model for robust, cooperative, and pragmatic intelligence. The research outlines paths for AI safety inspired by neuroscience, including brain emulation, robust sensory systems, and interpretability via neuroscience methods. Recommendations include scaling cognitively inspired architectures and leveraging brain data for safer AI development.*** <br> <br>
   Nov 27, Amaranth Foundation, Princeton Uni, MIT, Stanford Uni et al. published a [paper](https://arxiv.org/pdf/2411.18526) “NeuroAI for AI Safety”. As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, the researchers highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. The work makes several concrete recommendations for how neuroscience can positively impact AI safety. <br> <br>

5. ***LLMs Surpass Human Experts in Neuroscience Predictions <br>
A study highlights the potential of LLMs, particularly BrainGPT, to predict neuroscience results better than human experts by synthesizing decades of research. Using BrainBench, a benchmark for neuroscience prediction, LLMs demonstrated superior accuracy, especially when confident in their predictions. This approach is transferable to other knowledge-intensive fields, suggesting LLMs as valuable discovery tools.*** <br> <br>
   Nov 27, Nature Human Behaviour published a [paper](https://www.nature.com/articles/s41562-024-02046-9) “Large language models surpass human experts in predicting neuroscience results”. Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. Here, to evaluate this possibility, the study created BrainBench, a forward-looking benchmark for predicting neuroscience results. The study finds that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs indicated high confidence in their predictions, their responses were more likely to be correct, which presages a future where LLMs assist humans in making discoveries. The approach is not neuroscience specific and is transferable to other knowledge-intensive endeavours. <br> <br>

7. ***Nvidia's Star Attention for Efficient LLM Inference <br>
Nvidia introduced Star Attention, an efficient mechanism for long-sequence LLM inference that reduces computational costs and memory requirements. It uses block-sparse approximations and sequence-global attention, improving inference time by up to 11x while retaining 95-100% accuracy. The innovation integrates seamlessly with most Transformer-based LLMs and is available as open-source code.*** <br> <br>
   Nov 26, Nvidia published a [paper](https://arxiv.org/pdf/2411.17116) “Star Attention: Efficient LLM Inference over Long Sequences”. Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. The study introduces Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 95-100% of accuracy. Code is here. <br> <br>
   
9. ***Critic-RM: Self-Generated Critiques in Reward Modeling <br>
Meta and Georgia Tech proposed Critic-RM, a framework that enhances reward modeling for LLMs by integrating self-generated critiques alongside scalar rewards. This two-stage process improves modeling accuracy and rectifies flawed reasoning steps, with experiments showing a 3.7%-7.3% accuracy improvement compared to standard models.*** <br> <br>
    Nov 26, Meta and Georgia Inst of Tech published a [paper](https://arxiv.org/pdf/2411.16646) “Self-Generated Critiques Boost Reward Modeling for Language Models”. Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce scalar scores and struggle to incorporate critiques in a natural language format. The study hypothesizes that predicting both critiques and the scalar reward would improve reward modeling ability. Motivated by this, the study proposes Critic-RM, a framework that improves reward models using self-generated critiques without extra supervision. Critic-RM employs a two-stage process: generating and filtering high-quality critiques, followed by joint fine-tuning on reward prediction and critique generation. Experiments across benchmarks show that Critic-RM improves reward modeling accuracy by 3.7%-7.3% compared to standard reward models and LLM judges, demonstrating strong performance and data efficiency. Additional studies further validate the effectiveness of generated critiques in rectifying flawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy. <br> <br>
   
11. ***Limits of LLM Resampling with Imperfect Verifiers <br>
Princeton researchers showed that inference scaling using resampling has limitations when verifiers, like unit tests, are imperfect. False positives in coding tasks impose accuracy limits, and resampling cannot surpass the verifier's flaws. The study finds optimal sampling rates and highlights the need for strong models to reduce false positives and coding inaccuracies.*** <br> <br>
    Nov 26, Princeton Uni published a [paper](https://arxiv.org/pdf/2411.17501) “Inference Scaling FLaws: The Limits of LLM Resampling with Imperfect Verifiers”. Recent research has generated hope that inference scaling could allow weaker language models to match or exceed the accuracy of stronger models, such as by repeatedly sampling solutions to a coding problem until it passes unit tests. The central thesis of this paper is that there is no free lunch for inference scaling: indefinite accuracy improvement through resampling can only be realized if the "verifier" (in this case, a set of unit tests) is perfect. When the verifier is imperfect, as it almost always is in domains such as reasoning or coding (for example, unit tests have imperfect coverage), there is a nonzero probability of false positives: incorrect solutions that pass the verifier. Resampling cannot decrease this probability, so it imposes an upper bound to the accuracy of resampling-based inference scaling even with an infinite compute budget. The study finds that there is a very strong correlation between the model's single-sample accuracy (i.e. accuracy without unit tests) and its false positive rate on coding benchmarks HumanEval and MBPP, whose unit tests have limited coverage. Therefore, no amount of inference scaling of weaker models can enable them to match the single-sample accuracy of a sufficiently strong model. When considering that false positives have a negative utility compared to abstaining from producing a solution, it bends the inference scaling curve further downward. Empirically, the study finds that the optimal number of samples can be less than 10 under realistic assumptions. Finally, the work shows that beyond accuracy, false positives may have other undesirable qualities, such as poor adherence to coding style conventions. <br> <br>
   
13. ***Extractive-Abstractive Spectrum in Information Sharing <br>
Stanford research introduced the extractive-abstractive spectrum to assess trade-offs in verifiability and utility in LLM outputs versus search engines. As outputs become more abstractive, utility improves, but verifiability decreases. The findings suggest domain-specific balancing of utility and verifiability for high-stakes LLM applications.*** <br> <br>
    Nov 26, Stanford Uni published a [paper](https://arxiv.org/pdf/2411.17375) “The Extractive-Abstractive Spectrum: Uncovering Verifiability Trade-offs in LLM Generations”. Across all fields of academic study, experts cite their sources when sharing information. While large language models (LLMs) excel at synthesizing information, they do not provide reliable citation to sources, making it difficult to trace and verify the origins of the information they present. In contrast, search engines make sources readily accessible to users and place the burden of synthesizing information on the user. Through a survey, the study finds that users prefer search engines over LLMs for high-stakes queries, where concerns regarding information provenance outweigh the perceived utility of LLM responses. To examine the interplay between verifiability and utility of information-sharing tools, the study introduces the extractive-abstractive spectrum, in which search engines and LLMs are extreme endpoints encapsulating multiple unexplored intermediate operating points. Search engines are extractive because they respond to queries with snippets of sources with links (citations) to the original webpages. LLMs are abstractive because they address queries with answers that synthesize and logically transform relevant information from training and in-context sources without reliable citation. The study defines five operating points that span the extractive-abstractive spectrum and conduct human evaluations on seven systems across four diverse query distributions that reflect real-world QA settings: web search, language simplification, multi-step reasoning, and medical advice. As outputs become more abstractive, the study finds that perceived utility improves by as much as 200%, while the proportion of properly cited sentences decreases by as much as 50% and users take up to 3 times as long to verify cited information. The findings recommend distinct operating points for domain-specific LLM systems and the failure analysis informs approaches to high-utility LLM systems that empower users to verify information. <br> <br>

15. ***ShowUI: Advancing GUI Visual Agents <br>
The National University of Singapore and Microsoft presented ShowUI, a vision-language-action model for GUI tasks, achieving high efficiency and accuracy in zero-shot screenshot grounding. Innovations like UI-guided token selection and interleaved vision-language-action streaming enhance training and performance. ShowUI achieves 75.1% accuracy and is available as open-source software.*** <br> <br>
    Nov 26, National Uni of Singapore and Microsoft published a [paper](https://arxiv.org/pdf/2411.17465) “ShowUI: One Vision-Language-Action Model for GUI Visual Agent”. Building Graphical User Interface (GUI) assistants holds significant promise for enhancing human workflow productivity. While most agents are language-based, relying on closed-source API with text-rich meta-information (e.g., HTML or accessibility tree), they show limitations in perceiving UI visuals as humans do, highlighting the need for GUI visual agents. This work develops a vision-language-action model in digital world, namely ShowUI, which features the following innovations: (i) UI-Guided Visual Token Selection to reduce computational costs by formulating screenshots as an UI connected graph, adaptively identifying their redundant relationship and serve as the criteria for token selection during self-attention blocks; (ii) Interleaved Vision-Language-Action Streaming that flexibly unifies diverse needs within GUI tasks, enabling effective management of visual-action history in navigation or pairing multi-turn query-action sequences per screenshot to enhance training efficiency; (iii) Small-scale High-quality GUI Instruction-following Datasets by careful data curation and employing a resampling strategy to address significant data type imbalances. With above components, ShowUI, a lightweight 2B model using 256K data, achieves a strong 75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection further reduces 33% of redundant visual tokens during training and speeds up the performance by 1.4x. Navigation experiments across web Mind2Web, mobile AITW, and online MiniWob environments further underscore the effectiveness and potential of the model in advancing GUI visual agents. The models are available at https://github.com/showlab/ShowUI. <br> <br>

17. ***OneDiffusion: Versatile Large-Scale Diffusion Model <br>
OneDiffusion supports bidirectional image synthesis and understanding, handling tasks like text-to-image generation, depth estimation, and multi-view generation. Its unified training framework eliminates the need for specialized architectures, enabling scalable, multi-task training. The model demonstrates competitive performance across tasks and is open-sourced.*** <br> <br>
    Nov 25, AI2, Uni of California, and Uni of Washington published a [paper](https://arxiv.org/pdf/2411.16318) “One Diffusion to Generate Them All”. The work introduces OneDiffusion, a versatile, large-scale diffusion model that seamlessly supports bidirectional image synthesis and understanding across diverse tasks. It enables conditional generation from inputs such as text, depth, pose, layout, and semantic maps, while also handling tasks like image deblurring, upscaling, and reverse processes such as depth estimation and segmentation. Additionally, OneDiffusion allows for multi-view generation, camera pose estimation, and instant personalization using sequential image inputs. The model takes a straightforward yet effective approach by treating all tasks as frame sequences with varying noise scales during training, allowing any frame to act as a conditioning image at inference time. The unified training framework removes the need for specialized architectures, supports scalable multi-task training, and adapts smoothly to any resolution, enhancing both generalization and scalability. Experimental results demonstrate competitive performance across tasks in both generation and prediction such as text-to-image, multiview generation, ID preservation, depth estimation and camera pose estimation despite relatively small training dataset. Code and checkpoint are freely available at https://github.com/lehduong/OneDiffusion <br> <br>

19. ***LLM-as-a-Judge Framework <br>
A comprehensive study on LLM-based judgment and assessment introduces a taxonomy for scoring, ranking, and selection tasks. The research compiles benchmarks and highlights challenges in using LLMs for nuanced judgment, offering insights for advancing this emerging paradigm in AI.*** <br> <br>
    Nov 25, Arizona State Uni, Uni of Illinois Chicago, Uni of Maryland, UC Berkeley et al published a [paper](https://arxiv.org/pdf/2411.16594) “From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge” . Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). However, traditional methods, whether matching-based or embedding-based, often fall short of judging subtle attributes and delivering satisfactory results. Recent advancements in Large Language Models (LLMs) inspire the "LLM-as-ajudge" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection across various tasks and applications. This paper provides a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to advance this emerging field. The paper begins by giving detailed definitions from both input and output perspectives. Then the paper introduces a comprehensive taxonomy to explore LLM-as-a-judge from three dimensions: what to judge, how to judge and where to judge. Finally, the authors compile benchmarks for evaluating LLM-as-a-judge and highlight key challenges and promising directions, aiming to provide valuable insights and inspire future research in this promising research area. <br> <br>

21. ***Predicting Emergent Capabilities via Finetuning <br>
UC Berkeley researchers proposed a method to predict emergent LLM capabilities by finetuning smaller models on specific tasks. This approach anticipates when capabilities will emerge in future models, validated on benchmarks like GSM8K and CommonsenseQA. Emergence laws derived from this method offer practical predictive insights.*** <br> <br>
    Nov 25, UC Berkeley published a [paper](https://arxiv.org/pdf/2411.16035) “Predicting Emergent Capabilities by Finetuning”. A fundamental open challenge in modern LLM scaling is the lack of understanding around emergent capabilities. In particular, language model pretraining loss is known to be highly predictable as a function of compute. However, downstream capabilities are far less predictable -- sometimes even exhibiting emergent jumps -- which makes it challenging to anticipate the capabilities of future models. This study first poses the task of emergence prediction: given access to current LLMs that have random few-shot accuracy on a task, can people predict whether future models (GPT-N+1) will have non-trivial accuracy on that task? The study then discovers a simple insight for this problem: finetuning LLMs on a given task can shift the point in scaling at which emergence occurs towards less capable models. To operationalize this insight, the authors can finetune LLMs with varying amounts of data and fit a parametric function that predicts when emergence will occur (i.e., "emergence laws"). The study validates this approach using four standard NLP benchmarks where large-scale open-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and CoLA). Using only small-scale LLMs, the study finds that, in some cases, the authors can accurately predict whether models trained with up to 4x more compute have emerged. Finally, the work presents a case study of two realistic uses for emergence prediction. <br> <br>

23. ***LLM Embeddings for Regression <br>
Stanford and Google explored the use of LLM embeddings for regression tasks, finding them superior to traditional feature engineering for high-dimensional data. The study explains this performance through Lipschitz continuity and identifies that model size and language understanding do not consistently improve regression outcomes.*** <br> <br>
    Nov 22, Stanford Uni and Google published a [paper](https://arxiv.org/pdf/2411.14708) “Understanding LLM Embeddings for Regression”. With the rise of large language models (LLMs) for flexibly processing information as strings, a natural application is regression, specifically by preprocessing string representations into LLM embeddings as downstream features for metric prediction. The study provides one of the first comprehensive investigations into embedding-based regression and demonstrate that LLM embeddings as features can be better for high-dimensional regression tasks than using traditional feature engineering. This regression performance can be explained in part due to LLM embeddings over numeric data inherently preserving Lipschitz continuity over the feature space. Furthermore, the study quantifies the contribution of different model effects, most notably model size and language understanding, which was found surprisingly do not always improve regression performance. <br> <br>

25. ***TÜLU 3: Open Post-Training for LLMs <br>
TÜLU 3 introduces open methods for post-training LLMs, surpassing models like Llama 3.1 and GPT-4o-mini in performance. Techniques include supervised fine-tuning, Direct Preference Optimization, and a novel RLVR method. The release includes comprehensive training recipes, datasets, and evaluations for reproducibility and adaptation.*** <br> <br>
    Nov 22, Allen Inst and Uni of Washington published a [paper](https://arxiv.org/pdf/2411.15124) “TÜLU 3: Pushing Frontiers in Open Language Model Post-Training”. Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, the study introduces TÜLU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. TÜLU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for the models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method which is called Reinforcement Learning with Verifiable Rewards (RLVR). With TÜLU 3, the study builds a multi-task evaluation scheme for posttraining with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. The work concludes with analysis and discussion of training methods that did not reliably improve performance. The TÜLU 3 release includes model weights, a demo, and the complete recipe — datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the TÜLU 3 approach to more domains. <br> <br>

27. ***Google Scholar's Role in the AI Era <br>
As Google Scholar marks 20 years, it faces competition from AI-driven tools but remains a vital resource due to its extensive database and community integration. Despite criticisms of its algorithm transparency, innovations in AI-powered features aim to sustain its relevance amidst evolving scholarly search technologies.*** <br> <br>
    Nov 19, Nature published an [article](https://www.nature.com/articles/d41586-024-03746-y) “Can Google Scholar survive the AI revolution?”. Google Scholar, the largest scholarly search engine, celebrates its 20th anniversary amid rising competition from AI-driven tools. Over the years, it has become a crucial resource for researchers due to its free access, extensive database, and sophisticated search capabilities. However, new AI-powered platforms like ChatGPT, Semantic Scholar, and OpenAlex are challenging its dominance by offering enhanced search experiences and data accessibility. Despite these advancements, Google Scholar's comprehensive coverage and deep integration in the scientific community make it difficult to displace. Co-founder Anurag Acharya emphasizes that Google Scholar's primary goal is to aid scholars in finding valuable research, and it continues to innovate with AI features like article ranking and search suggestions. Yet, it faces criticism for its lack of transparency regarding its search algorithms and content coverage. As AI tools evolve, the future of scholarly search engines remains dynamic, with Google Scholar striving to maintain its leading position while welcoming innovations that advance scientific research. <br> <br>

29. ***RPN 2: Unified Model for Complex Data <br>
RPN 2 introduces interdependence functions to enhance learning performance for complex, dependent data. By unifying backbones like CNNs, RNNs, GNNs, and Transformers, the model offers a broader canonical representation and potential for innovative architecture designs surpassing existing methods.*** <br> <br>
    Nov 17, Uni of California, Davis published a [paper](https://arxiv.org/pdf/2411.11162) “RPN 2: On Interdependence Function Learning Towards Unifying and Advancing CNN, RNN, GNN, and Transformer”. This paper builds upon the previous work on the Reconciled Polynomial Network (RPN). The original RPN model was designed under the assumption of input data independence, presuming the independence among both individual instances within data batches and attributes in each data instance. However, this assumption often proves invalid for function learning tasks involving complex, interdependent data such as language, images, time series, and graphs. Ignoring such data interdependence may inevitably lead to significant performance degradation. To overcome these limitations, the study introduces the new Reconciled Polynomial Network (version 2), namely RPN 2. By incorporating data and structural interdependence functions, RPN 2 explicitly models data interdependence via new component functions in its architecture. This enhancement not only significantly improves RPN 2's learning performance but also substantially expands its unifying potential, enabling it to encompass a broader range of contemporary dominant backbone models within its canonical representation. These backbones include, but are not limited to, convolutional neural networks (CNNs), recurrent neural networks (RNNs), graph neural networks (GNNs), and Transformers. The analysis reveals that the fundamental distinctions among these backbone models primarily stem from their diverse approaches to defining the interdependence functions. Furthermore, this unified representation opens up new opportunities for designing innovative architectures with the potential to surpass the performance of these dominant backbones. <br> <br>

31. ***Self-Taught Optimizer (STOP): Recursive Code Improvement <br>
Stanford, Microsoft, and OpenAI demonstrated that a scaffolding program infused with LLMs can recursively improve itself via strategies like beam search and genetic algorithms. Although not fully recursive self-improvement, this method significantly enhances code performance, highlighting potential advancements and risks in self-improving technologies.*** <br> <br>
    Aug 16, Stanford Uni, Microsoft and OpenAI published a [paper](https://arxiv.org/abs/2310.02304) “Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation”. Several recent advances in AI systems solve problems by providing a "scaffolding" program that structures multiple calls to language models (LMs) to generate better outputs. A scaffolding program is written in a programming language such as Python. This study uses a language-model-infused scaffolding program to improve itself. The work starts with a seed "improver" that improves an input program according to a given utility function by querying an LM several times and returning the best solution. The authors then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. A variety of self-improvement strategies are proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not full recursive self-improvement. Nonetheless, it demonstrates that a modern language model, GPT-4 in the experiments, is capable of writing code that can call itself to improve itself. The authors consider concerns around the development of self-improving technologies and evaluate the frequency with which the generated code bypasses a sandbox.
 <br> <br> <br>


***Nov 24***

1. ***AI-Powered Citizen Revolution:  <br>The Forbes article describes a growing trend where employees, aided by AI and user-friendly tools, are becoming technology creators across various roles. Dubbed the "AI-Powered Citizen Revolution," this shift sees workers like marketing managers and nurses leveraging low-code platforms, workflow automation, and data insights to solve problems. Despite initial IT resistance, companies like Shell are embracing this approach with dual IT roles and governance models, enabling rapid innovation by tapping into employees' domain expertise. This trend is reshaping the future of work, blending AI with human ingenuity.*** <br> <br>
   Nov 22, Forbes published an [article](https://www.forbes.com/sites/bernardmarr/2024/11/22/the-ai-powered-citizen-revolution-how-every-employee-is-becoming-a-technology-creator/) “The AI-Powered Citizen Revolution: How Every Employee Is Becoming A Technology Creator”. The article highlights a significant shift in organizations where employees across various departments are becoming technology creators, leveraging AI and user-friendly tools. This movement, termed the "AI-Powered Citizen Revolution," sees marketing managers, nurses, and finance teams developing their own tech solutions. Tom Davenport and Ian Barkin, co-authors of 'All Hands on Tech,' explain that this trend is driven by the increasing ease of using technology and the powerful devices everyone now possesses. The revolution includes three types of citizen creators: developers using low-code/no-code platforms, automators creating workflows, and data scientists deriving insights from data. An example from Shell illustrates this shift, where an employee transitioned from manual tasks to becoming a citizen developer. Despite initial resistance from IT departments, progressive organizations are embracing this change, recognizing the need for dual IT roles: one for maintaining systems and another for supporting citizen developers. Successful companies implement systems like Shell's "red, amber, green" model to balance innovation and control. This revolution is transforming work and innovation, enabling faster and more effective solutions by tapping into employees' domain expertise. The future of work will involve a blend of AI and human ingenuity, with organizations providing the necessary tools, training, and governance to empower their workforce. <br> <br>

3. ***AIMV2 Vision Encoders:  <br>Apple's paper introduces AIMV2, a new family of vision encoders pre-trained in a multimodal setting. By integrating image and text modalities, these encoders achieve outstanding performance in tasks like localization and classification. AIMV2 surpasses state-of-the-art models such as CLIP in multimodal evaluations, showcasing scalability and robust capabilities, including a record-breaking 89.5% ImageNet-1k accuracy.*** <br> <br>
   Nov 21, Apple published a [paper](https://arxiv.org/pdf/2411.14402) “Multimodal Autoregressive Pre-training of Large Vision Encoders”. The paper introduces a novel method for pre-training of large-scale vision encoders. Building on recent advancements in autoregressive pre-training of vision models, the study extends this framework to a multimodal setting, i.e., images and text. The study presents AIMV2, a family of generalist vision encoders characterized by a straightforward pre-training process, scalability, and remarkable performance across a range of downstream tasks. This is achieved by pairing the vision encoder with a multimodal decoder that autoregressively generates raw image patches and text tokens. The encoders excel not only in multimodal evaluations but also in vision benchmarks such as localization, grounding, and classification. Notably, the AIMV2-3B encoder achieves 89.5% accuracy on ImageNet-1k with a frozen trunk. Furthermore, AIMV2 consistently outperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in multimodal image understanding across diverse settings. [Code is here](https://github.com/apple/ml-aim). <br> <br>

5. ***Platform Engineering Evolution:  <br>Forbes highlights the rise of platform engineering as a more structured alternative to DevOps, which has become ambiguous over time. This approach separates operational and development tasks, allowing teams to focus on their strengths. Popularized at events like KubeCon, platform engineering leverages cloud-native technologies and offers a scalable framework for modern infrastructure and development needs.*** <br> <br>
   Nov 21, Forbes published an [article](https://www.forbes.com/sites/justinwarren/2024/11/21/platform-engineering-is-the-new-devops/) “Platform Engineering Is The New DevOps”. The article discusses how platform engineering is emerging as a preferred term over DevOps in the cloud-native community. Initially, DevOps aimed to bridge the gap between developers and operations, promoting collaboration. However, over time, the term became ambiguous and overused, leading to confusion and inefficiency. DevOps often resulted in developers handling operations tasks they were not interested in, causing friction and dissatisfaction. Platform engineering offers a solution by clearly separating operations from application development, allowing each team to focus on their strengths. This shift is seen as a natural evolution in the cloud-native ecosystem, with platform engineering gaining popularity at events like KubeCon. Unlike DevOps, which lacked a formal definition and became a catch-all term, platform engineering is viewed as a more structured and scalable approach. It emphasizes using cloud-native technologies rather than building them, aligning with the operational needs of modern organizations. Companies are encouraged to reassess their DevOps structures to ensure they are still effective, as platform engineering may provide a more efficient and focused framework for managing infrastructure and development tasks. <br> <br>

7. ***OpenScholar for Literature Synthesis:  <br>Researchers introduce OpenScholar, a retrieval-augmented language model designed for synthesizing scientific literature. Leveraging a datastore of 45 million papers, it provides citation-backed responses to scientific queries. OpenScholar outperforms GPT-4o in correctness and citation accuracy and is well-received in expert evaluations, signaling significant advancements in research assistance.*** <br> <br>
   Nov 21, Uni of Washington, Allen Inst for AI, UIUC, CMU, Meta, UNCC, and Stanford Uni published a [paper](https://arxiv.org/pdf/2411.14199) “OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs”. Scientific progress depends on researchers' ability to synthesize the growing body of literature. Can large language models (LMs) assist scientists in this task? The study introduce OpenScholar, a specialized retrieval-augmented LM that answers scientific queries by identifying relevant passages from 45 million open-access papers and synthesizing citation-backed responses. To evaluate OpenScholar, the authors develop ScholarQABench, the first large-scale multi-domain benchmark for literature search, comprising 2,967 expert-written queries and 208 long-form answers across computer science, physics, neuroscience, and biomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and PaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o hallucinates citations 78 to 90% of the time, OpenScholar achieves citation accuracy on par with human experts. OpenScholar's datastore, retriever, and self-feedback inference loop also improves off-the-shelf LMs: for instance, OpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations, experts preferred OpenScholar-8B and OpenScholar-GPT4o responses over expert-written ones 51% and 70% of the time, respectively, compared to GPT4o's 32%. The authors open-source all of the [code](https://github.com/AkariAsai/OpenScholar), [models](https://huggingface.co/collections/OpenScholar/openscholar-v1-67376a89f6a80f448da411a6), datastore, data and a public demo. <br> <br>

9. ***Knowledge Awareness in LLMs:  <br>A study explores how large language models recognize entities and avoid hallucinations using sparse autoencoders. By identifying causal directions in model representation spaces, researchers demonstrate that models can self-regulate their responses based on known or unknown entities. These findings provide insights into improving model reliability and interpretability.*** <br> <br>
    Nov 21, UPC and ETH Zurich published a [paper](https://arxiv.org/pdf/2411.14257) “Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models”. Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting the ability to solve this problem. Using sparse autoencoders as an interpretability tool, the study discovers that a key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about. Sparse autoencoders uncover meaningful directions in the representation space, these detect whether the model recognizes an entity, e.g. detecting it doesn't know about an athlete or a movie. This suggests that models can have self-knowledge: internal representations about their own capabilities. These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse. The study demonstrates that despite the sparse autoencoders being trained on the base model, these directions have a causal effect on the chat model's refusal behavior, suggesting that chat finetuning has repurposed this existing mechanism. Furthermore, the work provides an initial exploration into the mechanistic role of these directions in the model, finding that they disrupt the attention of downstream heads that typically move entity attributes to the final token. <br> <br>

11. ***Quantum Error Decoding with AI:  <br>Google presents a transformer-based neural network decoder for quantum error correction. This decoder surpasses traditional approaches in accuracy and adapts to complex error distributions, even with limited experimental data. The work underscores the potential of machine learning to revolutionize quantum computing by learning from data rather than human-designed algorithms.*** <br> <br>
    Nov 20, Google published a [paper](https://www.nature.com/articles/s41586-024-08148-8) on Nature “Learning high-accuracy error decoding for quantum processors”. Building a large-scale quantum computer requires effective strategies to correct errors that inevitably arise in physical quantum systems. Quantum error-correction codes present a way to reach this goal by encoding logical information redundantly into many physical qubits. A key challenge in implementing such codes is accurately decoding noisy syndrome information extracted from redundancy checks to obtain the correct encoded logical information. Here the researchers develop a recurrent, transformer-based neural network that learns to decode the surface code, the leading quantum error-correction code. The decoder outperforms other state-of-the-art decoders on real-world data from Google’s Sycamore quantum processor for distance-3 and distance-5 surface codes. On distances up to 11, the decoder maintains its advantage on simulated data with realistic noise including cross-talk and leakage, utilizing soft readouts and leakage information. After training on approximate synthetic data, the decoder adapts to the more complex, but unknown, underlying error distribution by training on a limited budget of experimental samples. The work illustrates the ability of machine learning to go beyond human-designed algorithms by learning from data directly, highlighting machine learning as a strong contender for decoding in quantum computers. <br> <br>

13. ***Hymba for Small Language Models:  <br>Nvidia's Hymba architecture combines attention mechanisms with state space models to enhance efficiency in small language models. It introduces innovations like learnable meta tokens and cross-layer sharing, achieving state-of-the-art performance among sub-2B models and offering significant improvements in cache size and throughput.*** <br> <br>
    Nov 20, Nvidia published a [paper](https://arxiv.org/pdf/2411.13676) “Hymba: A Hybrid-head Architecture for Small Language Models”. The study proposes Hymba, a family of small language models featuring a hybrid-head parallel architecture that integrates transformer attention mechanisms with state space models (SSMs) for enhanced efficiency. Attention heads provide high-resolution recall, while SSM heads enable efficient context summarization. Additionally, the study introduces learnable meta tokens that are prepended to prompts, storing critical information and alleviating the "forced-to-attend" burden associated with attention mechanisms. This model is further optimized by incorporating cross-layer key-value (KV) sharing and partial sliding window attention, resulting in a compact cache size. During development, the study conducted a controlled study comparing various architectures under identical settings and observed significant advantages of the proposed architecture. Notably, Hymba achieves state-of-the-art results for small LMs: the Hymba-1.5B-Base model surpasses all sub-2B public models in performance and even outperforms Llama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size reduction, and 3.49x throughput. <br> <br>

15. ***RedPajama Open Datasets:  <br>The RedPajama project tackles challenges in transparency and quality of datasets for training large language models. By releasing RedPajama-V1 and V2, the study provides over 100 trillion tokens with metadata and quality signals, fostering the development of transparent and high-performing open-source language models.*** <br> <br>
    Nov 19, Together AI, Stanford Uni, Uni of Chicago, EleutherAI, Ontocord.ai, Princeton Uni, ETH Zurich, Mila, Uni of Montreal et al published a [paper](https://arxiv.org/pdf/2411.12372) “RedPajama: an Open Dataset for Training Large Language Models”. Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top-performing models lack transparency in their dataset curation and model development processes, posing an obstacle to the development of fully open language models. This work identifies three core data-related challenges that must be addressed to advance open-source language models. These include (1) transparency in model development, including the data curation process, (2) access to large quantities of high-quality data, and (3) availability of artifacts and metadata for dataset curation and analysis. To address these challenges, the work releases RedPajama-V1, an open reproduction of the LLaMA training dataset. In addition, we release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata. Together, the RedPajama datasets comprise over 100 trillion tokens spanning multiple domains and with their quality signals facilitate the filtering of data, aiming to inspire the development of numerous new datasets. To date, these datasets have already been used in the training of strong language models used in production, such as Snowflake Arctic, Salesforce's XGen and AI2's OLMo. To provide insight into the quality of RedPajama, the study presents a series of analyses and ablation studies with decoder-only language models with up to 1.6B parameters. The findings demonstrate how quality signals for web data can be effectively leveraged to curate high-quality subsets of the dataset, underscoring the potential of RedPajama to advance the development of transparent and high-performing language models at scale. <br> <br>

17. ***Scaling Laws Across Datasets:  <br>Harvard's study extends scaling laws to predict model loss across different datasets and tasks. By identifying shifted power law relationships, it demonstrates reliable loss predictions across diverse pre-training and downstream scenarios. These findings advance understanding of generalization in large-scale AI models.*** <br> <br>
    Nov 19, Harvard Uni published a [paper](https://arxiv.org/pdf/2411.12925) “Loss-to-Loss Prediction: Scaling Laws for All Datasets”. While scaling laws provide a reliable methodology for predicting train loss across compute scales for a single data distribution, less is known about how these predictions should change as the distribution is changed. This study derives a strategy for predicting one loss from another and apply it to predict across different pre-training datasets and from pre-training data to downstream task data. The predictions extrapolate well even at 20x the largest FLOP budget used to fit the curves. More precisely, the work finds that there are simple shifted power law relationships between (1) the train losses of two models trained on two separate datasets when the models are paired by training compute (train-to-train), (2) the train loss and the test loss on any downstream distribution for a single model (train-to-test), and (3) the test losses of two models trained on two separate train datasets (test-to-test). The results hold up for pre-training datasets that differ substantially (some are entirely code and others have no code at all) and across a variety of downstream tasks. Finally, the study finds that in some settings these shifted power law relationships can yield more accurate predictions than extrapolating single-dataset scaling laws. <br> <br>

19. ***Generative World Explorer (Genex):  <br>Johns Hopkins introduces Genex, a framework enabling AI agents to mentally explore large 3D worlds and update beliefs with imagined observations. This approach improves decision-making and planning in partial-observation scenarios, showcasing the potential of generative methods for embodied AI.*** <br> <br>
    Nov 19, Johns Hopkins Uni published a [paper](https://arxiv.org/pdf/2411.11844) “Generative World Explorer”. Planning with partial observation is a central challenge in embodied AI. A majority of prior works have tackled this challenge by developing agents that physically explore their environment to update their beliefs about the world state. In contrast, humans can imagine unseen parts of the world through a mental exploration and revise their beliefs with imagined observations. Such updated beliefs can allow them to make more informed decisions, without necessitating the physical exploration of the world at all times. To achieve this human-like ability, the study introduces the Generative World Explorer (Genex), an egocentric world exploration framework that allows an agent to mentally explore a large-scale 3D world (e.g., urban scenes) and acquire imagined observations to update its belief. This updated belief will then help the agent to make a more informed decision at the current step. To train Genex, the study creates a synthetic urban scene dataset, Genex-DB. Experimental results demonstrate that (1) Genex can generate high-quality and consistent observations during long-horizon exploration of a large virtual physical world and (2) the beliefs updated with the generated observations can inform an existing decision-making model (e.g., an LLM agent) to make better plans. <br> <br>

21. ***Procedural Knowledge in LLMs:  <br>UCL and collaborators reveal that LLM reasoning relies on procedural knowledge rather than retrieval-like strategies. Through pretraining data analysis, the study highlights how models synthesize procedural insights to solve reasoning tasks, differentiating them from fact-based question answering.*** <br> <br>
    Nov 19, UCL, Cohere et al. published a [paper](https://arxiv.org/pdf/2411.12580) “Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models”. The capabilities and limitations of Large Language Models have been sketched out in great detail in recent years, providing an intriguing yet conflicting picture. On the one hand, LLMs demonstrate a general ability to solve problems. On the other hand, they show surprising reasoning gaps when compared to humans, casting doubt on the robustness of their generalisation strategies. The sheer volume of data used in the design of LLMs has precluded the authors from applying the method traditionally used to measure generalisation: train-test set separation. To overcome this, the authors study what kind of generalisation strategies LLMs employ when performing reasoning tasks by investigating the pretraining data they rely on. For two models of different sizes (7B and 35B) and 2.5B of their pretraining tokens, the study identifies what documents influence the model outputs for three simple mathematical reasoning tasks and contrast this to the data that are influential for answering factual questions. The study finds that, while the models rely on mostly distinct sets of data for each factual question, a document often has a similar influence across different reasoning questions within the same task, indicating the presence of procedural knowledge. The study further finds that the answers to factual questions often show up in the most influential data. However, for reasoning questions the answers usually do not show up as highly influential, nor do the answers to the intermediate reasoning steps. When characterising the top ranked documents for the reasoning questions qualitatively, the study confirms that the influential documents often contain procedural knowledge, like demonstrating how to obtain a solution using formulae or code. The findings indicate that the approach to reasoning the models use is unlike retrieval, and more like a generalisable strategy that synthesises procedural knowledge from documents doing a similar form of reasoning. <br> <br>

23. ***Pixtral Large Multimodal Model:  <br>Mistral unveils Pixtral Large, a state-of-the-art multimodal model excelling in image understanding and reasoning. It outperforms leading models in benchmarks like MathVista and DocVQA and introduces advancements in handling complex visual data, reaffirming its leadership in multimodal AI.*** <br> <br>
    Nov 18, Mistral [released Pixtral Large](https://mistral.ai/news/pixtral-large/?utm_source=substack&utm_medium=email), a 124B open-weights multimodal model built on top of Mistral Large 2. Pixtral Large is the second model in the multimodal family and demonstrates frontier-level image understanding. Particularly, the model is able to understand documents, charts and natural images, while maintaining the leading text-only understanding of Mistral Large 2. Mistral evaluates Pixtral Large against frontier models on a set of standard multimodal benchmarks, through a common testing harness. On MathVista, which evaluates complex mathematical reasoning over visual data, the model achieves 69.4%, outperforming all other models. To assess reasoning capabilities over complex charts and documents, Mistral evaluates performance using ChartQA and DocVQA, where Pixtral Large surpasses GPT-4o and Gemini-1.5 Pro. Finally, Pixtral Large demonstrates competitive capabilities on MM-MT-Bench, outperforming all of Claude-3.5 Sonnet (new), Gemini-1.5 Pro and GPT-4o (latest). MM-MT-Bench is an open-source, judge-based evaluation intended to reflect real-world use cases of multimodal LLMs (see the Pixtral 12B [technical report](https://arxiv.org/abs/2410.07073) for details). Along with Pixtral Large, Mistral Large, the state-of-the-art text model, also gets an update. The model is available as pixtral-large-latest on our API, as well as for self-deployment as Mistral Large 24.11 on HuggingFace under the Mistral Research License (MRL) for research, or with a commercial license from Mistral AI for commercial use. <br> <br>

25. ***Challenges in Reranker Scaling:  <br>Databricks identifies limitations in reranker performance as the number of documents increases. Contrary to expectations, rerankers show diminishing returns and can degrade retrieval quality when overburdened, calling for research into scalable reranking methods.*** <br> <br>
    Nov 18, Databricks and UIUC published a [paper](https://arxiv.org/pdf/2411.11767) “Drowning in Documents: Consequences of Scaling Reranker Inference”. Rerankers, typically cross-encoders, are often used to re-score the documents retrieved by cheaper initial IR systems. This is because, though expensive, rerankers are assumed to be more effective. The study challenges this assumption by measuring reranker performance for full retrieval, not just re-scoring first-stage retrieval. Experiments reveal a surprising trend: the best existing rerankers provide diminishing returns when scoring progressively more documents and actually degrade quality beyond a certain limit. In fact, in this setting, rerankers can frequently assign high scores to documents with no lexical or semantic overlap with the query. The authors hope that the findings will spur future research to improve reranking. <br> <br>

27. ***LLMs’ Flexibility Beyond Linguistics:  <br>A study finds that LLMs perform equally well on forward and backward scientific text, suggesting their success stems from their architecture's ability to learn structured patterns rather than human-like linguistic processing. This highlights the generality of transformers across domains.*** <br> <br>
    Nov 17, Uni College London and Uni of Tubingen published a [paper](https://arxiv.org/pdf/2411.11061v1) “Beyond Human-Like Processing: Large Language Models Perform Equivalently on Forward and Backward Scientific Text”. The impressive performance of large language models (LLMs) has led to their consideration as models of human language processing. Instead, the work suggests that the success of LLMs arises from the flexibility of the transformer learning architecture. To evaluate this conjecture, the work trained LLMs on scientific texts that were either in a forward or backward format. Despite backward text being inconsistent with the structure of human languages, the study found that LLMs performed equally well in either format on a neuroscience benchmark, eclipsing human expert performance for both forward and backward orders. The results are consistent with the success of transformers across diverse domains, such as weather prediction and protein design. This widespread success is attributable to LLM’s ability to extract predictive patterns from any sufficiently structured input. Given their generality, we suggest caution in interpreting LLM’s success in linguistic tasks as evidence for human-like mechanisms. <br> <br>

29. ***AI for Chip Design:  <br>Google defends its deep reinforcement learning method, AlphaChip, against critiques questioning its chip design capabilities. The response underscores the method’s impact, which has already achieved widespread adoption, and addresses flaws in critical evaluations, reaffirming the method's validity.***
    Nov 15, Google and Stanford published a [paper](https://arxiv.org/pdf/2411.10053) “That Chip Has Sailed: A Critique of Unfounded Skepticism Around AI for Chip Design”. In 2020, Google introduced a deep reinforcement learning method capable of generating superhuman chip layouts, which Google then published in Nature and open-sourced on GitHub. AlphaChip has inspired an explosion of work on AI for chip design, and has been deployed in state-of-the-art chips across Alphabet and extended by external chipmakers. Even so, a non-peer-reviewed invited paper at ISPD 2023 questioned its performance claims, despite failing to run the method as described in Nature. For example, it did not pre-train the RL method (removing its ability to learn from prior experience), used substantially fewer compute resources (20x fewer RL experience collectors and half as many GPUs), did not train to convergence (standard practice in machine learning), and evaluated on test cases that are not representative of modern chips. Recently, Igor Markov published a meta-analysis of three papers: Google’s peer-reviewed Nature paper, the non-peer-reviewed ISPD paper, and Markov's own unpublished paper (though he does not disclose that he co-authored it). Although AlphaChip has already achieved widespread adoption and impact, the authors publish this response to ensure that no one is wrongly discouraged from innovating in this impactful area.

31. ***Microsoft and MIT explore prompt formatting's impact on LLM performance: <br>
Prompt optimization is crucial for LLM effectiveness, yet the role of prompt templates has been underexplored. This study evaluates how formatting the same contexts into templates like plain text, Markdown, JSON, and YAML affects LLM performance across tasks like reasoning, code generation, and translation. Results show significant variability: GPT-3.5-turbo’s performance fluctuated by up to 40% in code translation tasks depending on the prompt template, while GPT-4 proved more robust. The findings challenge the reliance on fixed prompt formats and call for more flexibility in their design.*** <br> <br>
    Nov 15, Microsoft, and MIT published a [paper](https://arxiv.org/pdf/2411.10541v1) “Does Prompt Formatting Have Any Impact on LLM Performance?”. In the realm of Large Language Models (LLMs), prompt optimization is crucial for model performance. Although previous research has explored aspects like rephrasing prompt contexts, using various prompting techniques (like in-context learning and chain-of-thought), and ordering few-shot examples, the understanding of LLM sensitivity to prompt templates remains limited. Therefore, this paper examines the impact of different prompt templates on LLM performance. The study formatted the same contexts into various human-readable templates, including plain text, Markdown, JSON, and YAML, and evaluated their impact across tasks like natural language reasoning, code generation, and translation using OpenAI’s GPT models. Experiments show that GPT-3.5-turbo’s performance varies by up to 40% in a code translation task depending on the prompt template, while larger models like GPT-4 are more robust to these variations. The analysis highlights the need to reconsider the use of fixed prompt templates, as different formats can significantly affect model performance. <br> <br>

33. ***University of Oxford investigates AI feedback guided by constitutions" <br>
AI models increasingly rely on "constitutions"—guidelines used for feedback and training. This study examined four constitutions designed to improve patient-centered communication in medical interviews. Evaluations by 215 human raters revealed that detailed constitutions enhanced emotive qualities but failed to outperform baselines in practical skills like information gathering. The research suggests that while detailed constitutions can improve certain aspects of AI feedback, their efficacy as a reward signal for practical applications may have limitations.*** <br> <br>
    Nov 15, Uni of Oxford published a [paper](https://arxiv.org/pdf/2411.10168) “Evaluating the role of ‘Constitutions’ for learning from AI feedback”. The growing capabilities of large language models (LLMs) have led to their use as substitutes for human feedback for training and assessing other LLMs. These methods often rely on ‘constitutions’, written guidelines which a critic model uses to provide feedback and improve generations. The work investigates how the choice of constitution affects feedback quality by using four different constitutions to improve patient-centered communication in medical interviews. In pairwise comparisons conducted by 215 human raters, the work found that detailed constitutions led to better results regarding emotive qualities. However, none of the constitutions outperformed the baseline in learning more practically-oriented skills related to information gathering and provision. The findings indicate that while detailed constitutions should be prioritised, there are possible limitations to the effectiveness of AI feedback as a reward signal in certain areas. <br> <br>

35. ***Collaborative study simulates behaviors of 1,000 individuals using generative agents: <br>
Stanford, Northwestern, University of Washington, and Google introduced a novel agent architecture simulating the behaviors and attitudes of 1,052 real individuals. By applying LLMs to qualitative interviews, the agents replicated participants’ survey responses with 85% accuracy and performed comparably in predicting personality traits and experimental outcomes. The architecture also reduced biases across racial and ideological groups compared to demographic-based models. This work lays the groundwork for tools to analyze individual and collective behavior for applications in policymaking and social science.*** <br> <br>
    Nov 14, Stanford Uni, Northwestern Uni, Uni of Washington, and Google published a [paper](https://arxiv.org/pdf/2411.10109) “Generative Agent Simulations of 1,000 People”. The promise of human behavioral simulation--general-purpose computational agents that replicate human behavior across domains--could enable broad applications in policymaking and social science. The study presents a novel agent architecture that simulates the attitudes and behaviors of 1,052 real individuals--applying large language models to qualitative interviews about their lives, then measuring how well these agents replicate the attitudes and behaviors of the individuals that they represent. The generative agents replicate participants' responses on the General Social Survey 85% as accurately as participants replicate their own answers two weeks later, and perform comparably in predicting personality traits and outcomes in experimental replications. The architecture reduces accuracy biases across racial and ideological groups compared to agents given demographic descriptions. This work provides a foundation for new tools that can help investigate individual and collective behavior.
 <br> <br>


***Nov 17***

1. ***Evo bridges DNA, RNA, and protein scales.  <br>The research introduces Evo, a genomic foundation model trained on extensive datasets of prokaryotic and phage genomes. Evo predicts biological functions, codesigns protein-DNA and protein-RNA systems, and generates genome-scale sequences. These capabilities advance understanding across molecular and genomic complexities, with implications for biology and genetic engineering.*** <br> <br>
   Nov 15, Arc Inst, Stanford Uni, UC Berkeley et al published a [paper](https://www.science.org/doi/10.1126/science.ado9336) on Science “Sequence modeling and design from molecular to genome scale with Evo”. The genome is a sequence that encodes the DNA, RNA, and proteins that orchestrate an organism’s function. The research presents Evo, a long-context genomic foundation model with a frontier architecture trained on millions of prokaryotic and phage genomes, and report scaling laws on DNA to complement observations in language and vision. Evo generalizes across DNA, RNA, and proteins, enabling zero-shot function prediction competitive with domain-specific language models and the generation of functional CRISPR-Cas and transposon systems, representing the first examples of protein-RNA and protein-DNA codesign with a language model. Evo also learns how small mutations affect whole-organism fitness and generates megabase-scale sequences with plausible genomic architecture. These prediction and generation capabilities span molecular to genomic scales of complexity, advancing the understanding and control of biology. <br> <br>

3. ***AI agents redefine productivity.  <br>Deloitte reports that multiagent AI systems are reshaping industries by enabling collaborative and complex tasks. Although technical challenges remain, the report forecasts rapid advancements, driven by growing investments and breakthroughs in reasoning capabilities. Businesses are urged to prepare for transformative impacts within a year.*** <br> <br>
   Nov 14, Deloitte published a [report](https://www2.deloitte.com/content/dam/Deloitte/us/Documents/consulting/us-ai-institute-generative-ai-agents-multiagent-systems.pdf) “Prompting for action How AI agents are reshapingthe future of work”. The main points of the report are: 1) AI agents are reshaping industries by expanding the potential applications of Generative AI (GenAI) and typical language models. 2) Multiagent AI systems can significantly enhance the quality of outputs and complexity of work performed by single AI agents. 3) Forward-thinking businesses and governments are already implementing AI agents and multiagent AI systems across a range of use cases. 4) Executive leaders should make moves now to prepare for and embrace this next era of intelligent organizational transformation. The report indicates that the era of AI agent collaboration is still in its early stages. Interest is growing among businesses and technology providers, but comprehensive solutions are not yet common. There is much technical work to be done—particularly in terms of the reasoning and planning capabilities that will enable AI agents. Improvements are likely to come fast. In recent months GenAI tools have shown significant improvements in reasoning and agent orchestration capabilities. Many venture capital firms are investing heavily across the spectrum of AI agent-related technologies, as are many of today’s leading GenAI and technology providers. What is available today is only a glimpse of what’s to come. People anticipate a significant evolution of core language models, AI agents, and agent orchestration platforms within the next 12 months. <br> <br>

5. ***LLMs lack human-like understanding.  <br>A study assessing LLMs on comprehension tasks reveals they perform at chance levels and exhibit non-human errors. Despite their usefulness, LLMs lack compositional understanding and grammatical regulation, emphasizing their limitations compared to humans in interpreting underlying meaning.*** <br> <br>
   Nov 14, researcher from Spain, German and USA published a [paper](https://www.nature.com/articles/s41598-024-79531-8#:~:text=We%20interpret%20this%20evidence%20as,regulating%20grammatical%20and%20semantic%20information.) on scientific reports “Testing AI on language comprehension tasks reveals insensitivity to underlying meaning”. Large Language Models (LLMs) are recruited in applications that span from clinical assistance and legal support to question answering and education. Their success in specialized tasks has led to the claim that they possess human-like linguistic capabilities related to compositional understanding and reasoning. Yet, reverse-engineering is bound by Moravec’s Paradox, according to which easy skills are hard. The study systematically assesses 7 state-of-the-art models on a novel benchmark. Models answered a series of comprehension questions, each prompted multiple times in two settings, permitting one-word or open-length replies. Each question targets a short text featuring high-frequency linguistic constructions. To establish a baseline for achieving human-like performance, the study tested 400 humans on the same prompts. Based on a dataset of n = 26,680 datapoints, the study discovered that LLMs perform at chance accuracy and waver considerably in their answers. Quantitatively, the tested models are outperformed by humans, and qualitatively their answers showcase distinctly non-human errors in language understanding. The study interprets this evidence as suggesting that, despite their usefulness in various tasks, current AI models fall short of understanding language in a way that matches humans, and the authors argue that this may be due to their lack of a compositional operator for regulating grammatical and semantic information. <br> <br>

7. ***ChatGPT aids academic growth.  <br>OpenAI’s guide highlights strategies for using ChatGPT to enhance writing skills, from generating citations to testing logic and refining ideas. It emphasizes ethical usage, encouraging transparency in academic practices to balance ChatGPT’s assistance with independent learning.*** <br> <br>
   Nov 14, OpenAI released an [article](https://openai.com/chatgpt/use-cases/student-writing-guide/) “A Student’s Guide to Writing with ChatGPT” to help students becoming better writing and thinkers. The guide includes: 1) Delagate citation grunt work ChatGPT; 2) Quickly get up to speed on a new topic; 3) Get a roadmap of relevant sources; 4) Complete your understanding by asking specific questions; 5) Improve your flow by getting feedback on structure; 6) Test your logic with reverse outlining; 7) Develop your ideas through Socratic dialogue; 8) Pressure-test your thesis by asking for counterarguments; 9) Compare your ideas against history’s greatest thinkers; 10) Elevate your writing through iterative feedback; 11) Use Advanced Voice Mode as a reading companion; 12) Don’t just go through the motions—hone your skills. One last point: When you use ChatGPT to deepen your understanding, develop your ideas, or come to insights you might not otherwise have had, it should fall within the bounds of acceptable academic practices. But since ChatGPT can also be used in unethical ways, your professors will likely feel more comfortable if they can see exactly how it’s contributing to your thinking. <br> <br>

11. ***AI agents level the playing field for SMBs.  <br>AI agents are poised to transform small and mid-sized businesses by automating tasks and boosting efficiency. Companies like Microsoft and Salesforce are driving these innovations, enabling SMBs to compete with larger enterprises while addressing concerns about job displacement through workforce retraining.*** <br> <br>
    Nov 14, Forbes published [an article](https://www.forbes.com/sites/quickerbettertech/2024/11/14/how-ai-agents-will-disrupt-small-and-mid-sized-business-in-2025/) “How AI Agents Will Disrupt Small And Mid-Sized Business In 2025”. In 2025, small and mid-sized businesses will see a transformative shift with the introduction of AI agents, enabling them to automate tasks similarly to large corporations. Unlike current generative AI chatbots, AI agents will perform complex tasks, initiate transactions, and solve problems, acting more like human assistants. Companies like Microsoft, Salesforce, and Intuit are developing AI agents to enhance productivity and profitability, offering features that will keep customers engaged and subscribed. These agents will handle various functions, from qualifying leads and managing finances to scheduling meetings and processing invoices. The healthcare and software development sectors are also seeing advancements with AI agents taking on roles traditionally held by humans. While there is concern about job displacement, business owners can mitigate this by training employees to work alongside AI. The adoption of AI agents will move AI from corporate environments to everyday business operations, making it crucial for small business owners to engage with their software vendors to leverage these new tools effectively. The big worry is the potential job loss, but for businesses facing labor shortages, AI agents will be invaluable. Business owners must address employee concerns by providing training and demonstrating the benefits of AI. The reality is that AI agents will start moving AI from the corporate boardroom to Main Street, and while the transition won't be immediate or flawless, it will be significant. Small business owners should proactively discuss AI agents with their software vendors to understand and apply these capabilities in 2025. <br> <br>

13. ***FinDVer benchmarks LLM claim verification.  <br>Yale's FinDVer evaluates LLMs' ability to verify claims in complex financial documents. Results show significant gaps between LLM and human performance, but the benchmark offers insights into advancing LLM capabilities for expert-domain applications.*** <br> <br>
    Nov 13, Yale Uni published a [paper](https://aclanthology.org/2024.emnlp-main.818.pdf) “FinDVer: Explainable Claim Verification over Long and Hybrid-Content Financial Documents”. The work introduces FinDVer, a comprehensive benchmark specifically designed to evaluate the explainable claim verification capabilities of LLMs in the context of understanding and analyzing long, hybrid-content financial documents. FinDVer contains 4,000 expert-annotated examples across four subsets, each focusing on a type of scenario that frequently arises in real-world financial domains. The study assesses a broad spectrum of 25 LLMs under long-context and RAG settings. The results show that even the current best-performing system (i.e., GPT-4o) significantly lags behind human experts. The detailed findings and insights highlight the strengths and limitations of existing LLMs in this new task. The researchers believe FinDVer can serve as a valuable benchmark for evaluating LLM capabilities in claim verification over complex, expert-domain documents.  <br> <br>

15. ***CCE slashes training memory needs.  <br>Apple proposes the Cut Cross-Entropy (CCE) method to dramatically lower memory usage in LLM training. By computing loss on-the-fly, CCE reduces the memory footprint without affecting performance, enabling efficient training for large vocabulary models.*** <br> <br>
    Nov 13, Apple published a [paper](https://arxiv.org/pdf/2411.09009) “Cut Your Losses in Large-Vocabulary Language Models”. As language models grow ever larger, so do their vocabularies. This has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation. Cross-entropy builds up a logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined. The work proposes Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory. Rather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly. The study implements a custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has a dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, the study leverages the inherent sparsity of softmax and propose to skip elements of the gradient computation that have a negligible (i.e., below numerical precision) contribution to the gradient. Experiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence. Here is [code](https://github.com/apple/ml-cross-entropy). <br> <br>

17. ***LLMs simulate journalistic planning.  <br>Research investigates how journalists select sources in news writing and applies Bayesian methods to predict source-selection schemas. The study provides a framework for understanding planning in long-form text generation, with implications for improving LLM-generated content.*** <br> <br>
    Nov 13, Uni of Southern California, UC Berkeley and Stanford Uni published a [paper](https://aclanthology.org/2024.findings-emnlp.930.pdf) “Explaining Mixtures of Sources in News Articles”. Human writers plan, then write. For large language models (LLMs) to play a role in longer-form article generation, people must understand the planning steps humans make before writing. The study explores one kind of planning, source-selection in news, as a case-study for evaluating plans in long-form generation. The authors ask: why do specific stories call for specific kinds of sources? The study imagines a generative process for story writing where a source-selection schema is first selected by a journalist, and then sources are chosen based on categories in that schema. Learning the article’s plan means predicting the schema initially chosen by the journalist. Working with professional journalists, the study adapt five existing schemata and introduce three new ones to describe journalistic plans for the inclusion of sources in documents. Then, inspired by Bayesian latent-variable modeling, the study develops metrics to select the most likely plan, or schema, underlying a story, which is used to compare schemata. The study finds that two schemata: stance and social affiliation best explain source plans in most documents. However, other schemata like textual entailment explain source plans in factually rich topics like “Science”. Finally, the study finds the authors can predict the most suitable schema given just the article’s headline with reasonable accuracy. This can been seen as an important case-study for human planning, and provides a framework and approach for evaluating other kinds of plans. The authors release a corpora, NewsSources, with annotations for 4M articles. <br> <br>

19. ***TTT enhances reasoning in novel tasks.  <br>MIT demonstrates that test-time training (TTT) significantly improves LLM performance on abstract reasoning benchmarks like ARC. By adapting model parameters during inference, TTT achieves state-of-the-art results, suggesting an alternative to symbolic reasoning.*** <br> <br>
    Nov 12, MIT published a [paper](https://ekinakyurek.github.io/papers/ttt.pdf) “The Surprising Effectiveness of Test-Time Training for Abstract Reasoning”. Language models have shown impressive performance on tasks within their training distribution, but often struggle with novel problems requiring complex reasoning. The study investigates the effectiveness of test-time training (TTT)—updating model parameters temporarily during inference using a loss derived from input data—as a mechanism for improving models’ reasoning capabilities, using the Abstraction and Reasoning Corpus (ARC) as a benchmark. Through systematic experimentation, the study identifies three crucial components for successful TTT: (1) initial finetuning on similar tasks (2) auxiliary task format and augmentations (3) per-instance training. TTT significantly improves performance on ARC tasks, achieving up to 6× improvement in accuracy compared to base fine-tuned models; applying TTT to a 8B-parameter language model, the study achieves 53% accuracy on the ARC’s public validation set, improving the state-of-the-art by nearly 25% for public and purely neural approaches. By ensembling the method with recent program generation approaches, the study gets SoTA public validation accuracy of 61.9%, matching the average human score. The findings suggest that explicit symbolic search is not the only path to improved abstract reasoning in neural language models; additional test-time applied to continued training on few-shot examples can also be extremely effective. <br> <br>

21. ***Bigger models aren't always better teachers.  <br>A study challenges assumptions that larger LLMs are better for instruction tuning. Findings show that compatibility between teacher and base models is more crucial than size, prompting the development of new evaluation metrics like Compatibility-Adjusted Reward (CAR).*** <br> <br>
    Nov 12, Uni of Washington and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2411.07133) “Stronger Models are NOT Stronger Teachers for Instruction Tuning”. Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. This study challenges this commonly-adopted assumption. The extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. The authors refer to this phenomenon as the Larger Models' Paradox and observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. The study thus develops a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Experiments across five base models demonstrate that CAR outperforms almost all baselines. Main finds include1) larger response generators not equal improved instruction-following capabilities; 2) learning from response generators within the same model family leads to higher performance; 3) Open-source LLMs can outperform close-source LLMs as response generators; 4) Higher temperature and top-p enhance instruction-following capabilities. <br> <br>

23. ***LLMs show limited implicit communication skills.  <br>Arizona State's study introduces ExpressivityArena to assess LLMs’ ability to convey implicit cues in tasks like poetry and coding. While LLMs demonstrate some expressive capabilities, limitations remain, guiding future research on enhancing conversational nuance.*** <br> <br>
    Nov 12, Arizona State Uni published a [paper](https://arxiv.org/pdf/2411.08010v1) “ExpressivityArena: Can LLMs Express Information Implicitly?”. While Large Language Models (LLMs) have demonstrated remarkable performance in certain dimensions, their ability to express implicit language cues that human use for effective communication remains unclear. This paper presents ExpressivityArena, a Python library for measuring the implicit communication abilities of LLMs. The study provides a comprehensive framework to evaluate expressivity of arbitrary LLMs and explore its practical implications. To this end, the authors refine the definition and measurements of “expressivity,” and use the framework in a set of small experiments. These experiments test LLMs in creative and logical tasks such as poetry, coding, and emotion-based responses. They are then evaluated by an automated grader, through ExpressivityArena, which the work verifies to be the most pragmatic for testing expressivity. Building on these experiments, the work deepen the understanding of the expressivity of LLMs by assessing their ability to remain expressive in conversations. The findings indicate that LLMs are capable of generating and understanding expressive content, however, with some limitations. These insights will inform the future development and deployment of expressive LLMs.  <br> <br>

25. ***WAM enables precise watermarking.  <br>The Watermark Anything Model (WAM) introduces advanced localized image watermarking. Capable of embedding imperceptible messages into small areas, WAM offers robust watermarking even under challenging conditions, broadening applications in digital content security.*** <br> <br>
    Nov 11, Meta, Ecole Polytechnique and Inria Rennes published a [paper](https://arxiv.org/pdf/2411.07231) “Watermark Anything with Localized Messages”. Image watermarking methods are not tailored to handle small watermarked areas. This restricts applications in real-world scenarios where parts of the image may come from different sources or have been edited. The study introduces a deep-learning model for localized image watermarking, dubbed the Watermark Anything Model (WAM). The WAM embedder imperceptibly modifies the input image, while the extractor segments the received image into watermarked and non-watermarked areas and recovers one or several hidden messages from the areas found to be watermarked. The models are jointly trained at low resolution and without perceptual constraints, then post-trained for imperceptibility and multiple watermarks. Experiments show that WAM is competitive with state-of-the art methods in terms of imperceptibility and robustness, especially against inpainting and splicing, even on high-resolution images. Moreover, it offers new capabilities: WAM can locate watermarked areas in spliced images and extract distinct 32-bit messages with less than 1 bit error from multiple small regions - no larger than 10% of the image surface - even for small 256times 256 images. <br> <br>

27. ***Fine-tuning struggles to update LLMs reliably.  <br>Stanford’s FineTuneBench reveals that commercial fine-tuning APIs are inefficient at learning new or updated knowledge, with low generalization accuracy. The findings highlight limitations in current methods and underscore the need for more effective fine-tuning strategies.*** <br> <br>
    Nov 11, Stanford Uni published a [paper](https://arxiv.org/pdf/2411.05059) “FineTuneBench: How well do commercial fine-tuning APIs infuse knowledge into LLMs?”. There is great interest in fine-tuning frontier large language models (LLMs) to inject new information and update existing knowledge. While commercial LLM fine-tuning APIs from providers such as OpenAI and Google promise flexible adaptation for various applications, the efficacy of fine-tuning remains unclear. This study introduces FineTuneBench, an evaluation framework and dataset for understanding how well commercial fine-tuning APIs can successfully learn new and updated knowledge. The work analyzes five frontier LLMs with commercially available fine-tuning APIs, including GPT-4o and Gemini 1.5 Pro, on their effectiveness in two settings: (1) ingesting novel information, such as recent news events and new people profiles, and (2) updating existing knowledge, such as updated medical guidelines and code frameworks. The results reveal substantial shortcomings in all the models' abilities to effectively learn new information through fine-tuning, with an average generalization accuracy of 37% across all models. When updating existing knowledge, such as incorporating medical guideline updates, commercial fine-tuning APIs show even more limited capability (average generalization accuracy of 19%). Overall, fine-tuning GPT-4o mini is the most effective for infusing new knowledge and updating knowledge, followed by GPT-3.5 Turbo and GPT-4o. The fine-tuning APIs for Gemini 1.5 Flesh and Gemini 1.5 Pro are unable to learn new knowledge or update existing knowledge. These findings underscore a major shortcoming in using current commercial fine-tuning services to achieve reliable knowledge infusion in common scenarios. The authors open source the FineTuneBench dataset at this https URL. <br> <br>

29. ***REMIX mitigates knowledge loss.  <br>Princeton introduces REMIX, a strategy for preventing LLM forgetting during continual training. By mixing unrelated data into the learning process, REMIX reduces interference and enhances memory retention, paving the way for more robust knowledge updates.*** <br> <br>
    Nov 11, Princeton Uni published a [paper](https://arxiv.org/pdf/2411.07175) “Continual Memorization of Factoids in Large Language Models”. Large language models can absorb a massive amount of knowledge through pretraining, but pretraining is inefficient for acquiring long-tailed or specialized facts. Therefore, fine-tuning on specialized or new knowledge that reflects changes in the world has become popular, though it risks disrupting the model's original capabilities. The authors study this fragility in the context of continual memorization, where the model is trained on a small set of long-tail factoids (factual associations) and must retain these factoids after multiple stages of subsequent training on other datasets. Through extensive experiments, the study shows that LLMs suffer from forgetting across a wide range of subsequent tasks, and simple replay techniques do not fully prevent forgetting, especially when the factoid datasets are trained in the later stages. The work posits that there are two ways to alleviate forgetting: 1) protect the memorization process as the model learns the factoids, or 2) reduce interference from training in later stages. With this insight, the study develops an effective mitigation strategy: REMIX (Random and Generic Data Mixing). REMIX prevents forgetting by mixing generic data sampled from pretraining corpora or even randomly generated word sequences during each stage, despite being unrelated to the memorized factoids in the first stage. REMIX can recover performance from severe forgetting, often outperforming replay-based methods that have access to the factoids from the first stage. The authors then analyze how REMIX alters the learning process and find that successful forgetting prevention is associated with a pattern: the model stores factoids in earlier layers than usual and diversifies the set of layers that store these factoids. The efficacy of REMIX invites further investigation into the underlying dynamics of memorization and forgetting, opening exciting possibilities for future research. <br> <br>

31. ***OpenAI's Challenge with Slowing AI Improvement.  <br>
OpenAI is reportedly encountering a slowdown in performance improvements for its models, with its upcoming model, "Orion," showing less advancement compared to previous transitions (e.g., GPT-3 to GPT-4). Orion's performance might even lag in specific areas like coding. OpenAI has formed a foundational team to address this by exploring strategies such as using synthetic data for training and enhancing post-training processes. The company has denied plans to release Orion this year.*** <br> <br>
    Nov 9, according to [techcrunch.com](https://techcrunch.com/2024/11/09/openai-reportedly-developing-new-strategies-to-deal-with-ai-improvement-slowdown/), OpenAI reportedly developing new strategies to deal with AI improvement slowdown. OpenAI’s next flagship model might not represent as big a leap forward as its predecessors, according to a new report in The Information. Employees who tested the new model, code-named Orion, reportedly found that even though its performance exceeds OpenAI’s existing models, there was less improvement than they’d seen in the jump from GPT-3 to GPT-4. In other words, the rate of improvement seems to be slowing down. In fact, Orion might not be reliably better than previous models in some areas, such as coding. In response, OpenAI has created a foundations team to figure out how the company can continue to improve its models in the face of a dwindling supply of new training data. These new strategies reportedly include training Orion on synthetic data produced by AI models, as well as doing more to improve models during the post-training process. OpenAI did not immediately respond to a request for comment. In response to previous reports about plans for its flagship model, the company said, “We don’t have plans to release a model code-named Orion this year.” <br> <br>

33. ***Efficient Inference with Recycled Attention.  <br>
A study by NYU, Cornell, and UT Austin introduces "Recycled Attention," an inference method for long-context language models. This approach alternates between full-context and partial attention, recycling patterns from prior computations to improve efficiency. It demonstrates superior speed and performance on long-context tasks compared to previous methods and explores dynamic attention-switching strategies and continued pretraining for further optimization.*** <br> <br>
    Nov 8, NYU, Uni of Cornell and Uni of Texas at Austin published a [paper](https://arxiv.org/pdf/2411.05787v1) “Recycled Attention: Efficient inference for long-context language models”. Generating long sequences of tokens given a long-context input imposes a heavy computational burden for large language models (LLMs). One of the computational bottleneck comes from computing attention over a long sequence of input at each generation step. This study proposes Recycled Attention, an inference-time method which alternates between full context attention and attention over a subset of input tokens. When performing partial attention, the approach recycles the attention pattern of a previous token that has performed full attention and attend only to the top K most attended tokens, reducing the cost of data movement and attention computation. Compared to previously proposed inference-time acceleration method which attends only to local context or tokens with high accumulative attention scores, the proposed approach flexibly chooses tokens that are relevant to the current decoding step. The study evaluate the methods on RULER, a suite of tasks designed to comprehensively evaluate long-context abilities, and long-context language modeling tasks. Applying the method to off-the-shelf LLMs achieves comparable speedup to baselines which only consider local context while improving the performance by 2x. The study further explores two ideas to improve performance-efficiency trade-offs: (1) dynamically decide when to perform recycled or full attention step based on the query similarities and (2) continued pre-training the model with Recycled Attention. <br> <br>

35. ***Precision-Aware Scaling Laws for LLMs.  <br>
Harvard, Stanford, MIT, Databricks, and CMU propose precision-aware scaling laws for low-precision training and inference, addressing cost and quality trade-offs in LLMs. These laws predict the quality degradation from reduced precision and show how post-training quantization degrades performance as training data scales. This unified model helps optimize training and inference across precisions.*** <br> <br>
    Nov 7, Harvard Uni, Stanford Uni, MIT, Databricks and CMU published a [paper](https://arxiv.org/pdf/2411.04330) “Scaling Laws for Precision”. Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. This study devises "precision-aware" scaling laws for both training and inference. The study proposes that training in lower precision reduces the model's "effective parameter count," allowing to predict the additional loss incurred from training in low precision and post-train quantization. For inference, the study finds that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, the scaling laws allow to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision may be compute optimal. The authors unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. The study fits on over 465 pretraining runs and validate the predictions on model sizes up to 1.7B parameters trained on up to 26B tokens. <br> <br>

37. ***Identifying LLM Hosting Platforms.  <br>
Imperial College, Cambridge, and Google develop a method to verify the hardware and software platforms used in LLM inference. The technique, called Hardware and Software Platform Inference (HSPI), analyzes output patterns to identify GPU architectures and software stacks. Testing shows up to 100% accuracy in white-box scenarios and significant success in black-box cases, addressing concerns over mismatched services in AI hosting.*** <br> <br>
    Nov 7, Imperial College London, Uni of Cambridge and Google published a [paper](https://arxiv.org/pdf/2411.05197) “Hardware and Software Platform Inference”. It is now a common business practice to buy access to large language model (LLM) inference rather than self-host, because of significant upfront hardware infrastructure and energy costs. However, as a buyer, there is no mechanism to verify the authenticity of the advertised service including the serving hardware platform, e.g. that it is actually being served using an NVIDIA H100. Furthermore, there are reports suggesting that model providers may deliver models that differ slightly from the advertised ones, often to make them run on less expensive hardware. That way, a client pays premium for a capable model access on more expensive hardware, yet ends up being served by a (potentially less capable) cheaper model on cheaper hardware. This work introduces hardware and software platform inference (HSPI) -- a method for identifying the underlying GPU architecture and software stack of a (black-box) machine learning model solely based on its input-output behavior. The method leverages the inherent differences of various GPU architectures and compilers to distinguish between different GPU types and software stacks. By analyzing the numerical patterns in the model's outputs, the study proposes a classification framework capable of accurately identifying the GPU used for model inference as well as the underlying software configuration. The findings demonstrate the feasibility of inferring GPU type from black-box models. The authors evaluate HSPI against models served on different real hardware and find that in a white-box setting the researcher can distinguish between different GPUs with between 83.9% and 100% accuracy. Even in a black-box setting the model is able to achieve results that are up to three times higher than random guess accuracy. <br> <br>

39. ***Unlocking Latent Reasoning in LLMs.  <br>
Salesforce introduces LaTent Reasoning Optimization (LaTRO), a framework enhancing LLM reasoning through self-reward mechanisms rather than external feedback. LaTRO achieves significant accuracy gains on reasoning datasets like GSM8K, demonstrating LLMs' untapped reasoning potential. This approach offers a scalable, self-improvement pathway for reasoning tasks, with code available on GitHub.*** <br> <br>
    Nov 6, Saleforce published a [paper](https://arxiv.org/pdf/2411.04282) “Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding”. Large language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time, optimizing reasoning capabilities during training remains challenging. The work introduces LaTent Reasoning Optimization (LaTRO), a principled framework that formulates reasoning as sampling from a latent distribution and optimizes it via variational approaches. LaTRO enables LLMs to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring external feedback or reward models. The study validates LaTRO through experiments on GSM8K and ARC-Challenge datasets using multiple model architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of 12.5% over base models and 9.6% over supervised fine-tuning across Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. The findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked and enhanced through the proposed optimization approach in a self-improvement manner. The code of LaTRO is available at https://github.com/SalesforceAIResearch/LaTRO. <br> <br>
    
41. ***Counting Abilities and Tokenization in LLMs.  <br>
A study by UBC and Stony Brook University examines how Transformers' architectural limitations affect counting tasks, a critical reasoning component. While Chain of Thought (CoT) reasoning partially addresses these issues, tokenization strategies (e.g., byte-level vs. character-level) significantly impact performance. The findings suggest revisiting tokenization methods to enhance LLM reasoning capabilities.*** <br> <br>
    21.	Oct 29, Uni of British Columbia and Stony Brook Uni published a paper “Counting Ability of Large Language Models and Impact of Tokenization”. Transformers, the backbone of modern large language models (LLMs), face inherent architectural limitations that impede their reasoning capabilities. Unlike recurrent networks, Transformers lack recurrent connections, confining them to constant-depth computation. This restriction places them in the complexity class TC0, making them theoretically incapable of solving tasks that demand increasingly deep reasoning as input length grows. Counting, a fundamental component of many reasoning tasks, also requires reasoning depth to grow linearly to be performed inductively. While previous studies have established the upper limits of counting ability in Transformer-based expert models (i.e., models specifically trained for counting tasks), these findings do not directly extend to general-purpose LLMs due to differences in reasoning mechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning can help alleviate some of the architectural limitations of Transformers in counting tasks. However, little attention has been paid to the role of tokenization in these models. Unlike expert models that often use character-level tokenization, LLMs typically rely on byte-level (BPE) tokenizers, which fundamentally alters the way reasoning is processed. This work investigates the impact of tokenization on the counting abilities of LLMs, uncovering substantial performance variations based on input tokenization differences. The work provides both theoretical and experimental analyses, offering insights into how tokenization choices can undermine models' theoretical computability, thereby inspiring the design of new tokenization methods to enhance reasoning in LLMs.
 <br> <br> <br>


***Nov 10***

1. ***Meta and Stanford propose a sparse multi-modal transformer architecture to optimize computational efficiency:  <br>The paper introduces the Mixture-of-Transformers (MoT), which decouples non-embedding parameters for different modalities, improving multi-modal processing efficiency. In tests, MoT achieves similar performance to dense models with significantly reduced FLOPs and faster processing times on large datasets.*** <br> <br>
   Nov 8, Meta and Stanford Uni published a [paper](https://arxiv.org/pdf/2411.04996) “Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models”.  The development of large language models (LLMs) has expanded to multi-modal systems capable of processing text, images, and speech within a unified framework. Training these models demands significantly larger datasets and computational resources compared to text-only LLMs. To address the scaling challenges, the study introduces Mixture-of-Transformers (MoT), a sparse multi-modal transformer architecture that significantly reduces pretraining computational costs. MoT decouples non-embedding parameters of the model by modality -- including feed-forward networks, attention matrices, and layer normalization -- enabling modality-specific processing with global self-attention over the full input sequence. The study evaluates MoT across multiple settings and model scales. In the Chameleon 7B setting (autoregressive text-and-image generation), MoT matches the dense baseline's performance using only 55.8% of the FLOPs. When extended to include speech, MoT reaches speech performance comparable to the dense baseline with only 37.2% of the FLOPs. In the Transfusion setting, where text and image are trained with different objectives, a 7B MoT model matches the image modality performance of the dense baseline with one third of the FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image generation metrics. System profiling further highlights MoT's practical benefits, achieving dense baseline image quality in 47.2% of the wall-clock time and text quality in 75.6% of the wall-clock time (measured on AWS p4de.24xlarge instances with NVIDIA A100 GPUs). <br> <br>

3. ***MIT, Harvard, and Johns Hopkins present an inverse generative model for few-shot task learning:  <br>Their Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM) learns new task concepts with few examples using pre-trained generative models, improving agent behavior prediction across varied domains like navigation and manipulation.*** <br> <br>
   Nov 7, MIT, Harvard Uni and Johns Hopkins Uni published a [paper](https://arxiv.org/pdf/2411.04987v1) “Few-Shot Task Learning through Inverse Generative Modeling”. Learning the intents of an agent, defined by its goals or motion style, is often extremely challenging from just a few examples. The study refers to this problem as task concept learning and present an approach, Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM), which learns new task concepts by leveraging invertible neural generative models. The core idea is to pretrain a generative model on a set of basic concepts and their demonstrations. Then, given a few demonstrations of a new concept (such as a new goal or a new action), the method learns the underlying concepts through backpropagation without updating the model weights, thanks to the invertibility of the generative model. The authors evaluate the method in five domains – object rearrangement, goal-oriented navigation, motion caption of human actions, autonomous driving, and real-world table-top manipulation. The experimental results demonstrate that via the pretrained generative model, the study successfully learns novel concepts and generate agent plans or motion corresponding to these concepts in (1) unseen environments and (2) in composition with training concepts. <br> <br>

5. ***MIT and partners hypothesize that language models share semantic representations across languages and modalities:  <br>Known as the "semantic hub hypothesis," this shared space enables language models to process inputs from different languages and modalities, creating predictable cross-modal effects in model outputs.*** <br> <br>
   Nov 7, MIT, Uni of Southern California and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2411.04986v1) “The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities”. Modern language models can process inputs across diverse languages and modalities. The study hypothesizes that models acquire this capability through learning a shared representation space across heterogeneous data types (e.g., different languages and modalities), which places semantically similar inputs near one another, even if they are from different modalities/languages. The authors term this the semantic hub hypothesis, following the hub-and-spoke model from neuroscience (Patterson et al., 2007) which posits that semantic knowledge in the human brain is organized through a transmodal semantic “hub” which integrates information from various modality-specific “spokes” regions. The study first shows that model representations for semantically equivalent inputs in different languages are similar in the intermediate layers, and that this space can be interpreted using the model’s dominant pretraining language via the logit lens. This tendency extends to other data types, including arithmetic expressions, code, and visual/audio inputs. Interventions in the shared representation space in one data type also predictably affect model outputs in other data types, suggesting that this shared representations space is not simply a vestigial byproduct of large-scale training on broad data, but something that is actively utilized by the model during input processing. <br> <br>

7. ***Cambridge and HKU explore LLMs' capabilities in following threads in extensive contexts:  <br>Their research finds that while many LLMs handle multiple threads well, accuracy declines as the context window grows. The study also shows that token counts from different tokenizers vary significantly, impacting context limits.*** <br> <br>
   Nov 7, Uni of Cambridge and The Uni of Hong Kong published a [paper](https://arxiv.org/pdf/2411.05000) “Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?”. As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context LLMs appear well-suited to this form of complex information retrieval and reasoning, which has traditionally proven costly and time-consuming. However, although the development of longer context models has seen rapid gains in recent years, the understanding of how effectively LLMs use their context has not kept pace. To address this, the study conducts a set of retrieval experiments designed to evaluate the capabilities of 17 leading LLMs, such as their ability to follow threads of information through the context window. Strikingly, the work finds that many models are remarkably threadsafe: capable of simultaneously following multiple threads without significant loss in performance. Still, for many models, the study finds the effective context limit is significantly shorter than the supported context length, with accuracy decreasing as the context window grows. The study also highlights the important point that token counts from different tokenizers should not be directly compared -- they often correspond to substantially different numbers of written characters. The authors release the code and long-context experimental data. <br> <br>

9. ***Meta, UNC, and NYU introduce Self-Consistency Preference Optimization (ScPO) for unsupervised model improvement:  <br>ScPO iteratively trains models to prioritize consistent over inconsistent answers, closing the gap with supervised models on reasoning tasks and showing notable improvements in reasoning accuracy.*** <br> <br>
    Nov 6, Meta, UNC Chapel Hill and NYU published a [paper](https://arxiv.org/pdf/2411.04109) “Self-Consistency Preference Optimization”.  Self-alignment, whereby models learn to improve themselves without human annotation, is a rapidly growing research area. However, existing techniques often fail to improve complex reasoning tasks due to the difficulty of assigning correct rewards. An orthogonal approach that is known to improve correctness is self-consistency, a method applied at inference time based on multiple sampling in order to find the most consistent answer. This work extends the self-consistency concept to help train models. The study thus introduces self-consistency preference optimization (ScPO), which iteratively trains consistent answers to be preferred over inconsistent ones on unsupervised new problems. The work shows ScPO leads to large improvements over conventional reward model training on reasoning tasks such as GSM8K and MATH, closing the gap with supervised training with gold answers or preferences, and that combining ScPO with standard supervised learning improves results even further. On ZebraLogic, ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and Claude-3 Haiku. <br> <br>

11. ***UCL, Boston University, and Meta analyze the effects of evaluation data contamination in LLMs:  <br>Using a novel ConTAM metric, the study shows how contamination impacts benchmark scores and suggests that longer contaminated strings provide clearer insights into performance inflation.*** <br> <br>
    Nov 6, UCL, Boston Uni, Cohere and Meta published a [paper](https://arxiv.org/pdf/2411.03923v1) “Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?”. Evaluation data contamination, the inadvertent mixing of samples from evaluation benchmarks into pre-training corpora, constitutes a recently growing and important concern in the field of evaluating large language models (LLMs). The resulting ‘training on the test set’ makes it difficult to interpret evaluation benchmark scores, and an active area of research studies its effects. However, while evaluation data contamination is easily understood intuitively, it is surprisingly difficult to define precisely which examples should be considered contaminated and, consequently, to what extent contamination inflates the corresponding benchmark scores. This work proposes that these questions should be addressed together and that contamination metrics can be assessed based on whether the examples they mark contaminated indeed give models an undue advantage. The study proposes a novel analysis method called ConTAM, and shows – in a large scale survey of existing and novel n-gram based contamination metrics across 13 benchmarks and 7 models from 2 different families – that ConTAM can be used to better understand evaluation data contamination as well as its effects on benchmark scores. The study finds that contamination may have a much larger effect than reported in recent LLM releases and that there are differences in the extent to which models at different scale are impacted by contamination. Furthermore, the authors find that considering only the longest contaminated substring generally provides a better signal than considering a union of all contaminated substrings, as was common practice in previous studies, and that doing model and benchmark specific threshold analysis greatly increases the specificity of the results. Lastly, the study investigates the impact of various hyperparameter choices of contamination studies, finding that – among other things – both using larger values of n and disregarding contaminated strings that are infrequent in the pre-training data lead to many false negatives. With ConTAM, the work provides a method to empirically ground evaluation data contamination metrics in downstream effects as well as measure their magnitude. With the exploration, the work sheds light on how evaluation data contamination can impact LLMs and provide insight into the various considerations important when doing contamination analysis. The authors end the paper by discussing these in more detail and providing concrete suggestions for future work. <br> <br>

13. ***Peking University examines large language models' numerical understanding with the Number Cookbook benchmark:  <br>The study highlights LLMs' weaknesses in basic numerical processing, offering a new benchmark to evaluate models and exploring techniques like chain-of-thought to improve numerical understanding.*** <br> <br>
    Nov 6, Peking Uni published a [paper](https://arxiv.org/pdf/2411.03766v1) “Number Cookbook: Number Understanding of Language Models and How to Improve It”. Large language models (LLMs) can solve an increasing number of complex reasoning tasks while making surprising mistakes in basic numerical understanding and processing (such as 9.11> 9.9). The latter ability is essential for tackling complex arithmetic and mathematical problems and serves as a foundation for most reasoning tasks, but previous work paid little attention to it or only discussed several restricted tasks (like integer addition). This paper comprehensively investigates the numerical understanding and processing ability (NUPA) of LLMs. Firstly, the study introduces a benchmark covering four common numerical representations and 17 distinct numerical tasks in four major categories, resulting in 41 meaningful combinations in total. These tasks are derived from primary and secondary education curricula, encompassing nearly all everyday numerical understanding and processing scenarios, and the rules of these tasks are very simple and clear. Through the benchmark, the work finds that current LLMs fail frequently in many of the tasks. To study the problem, the work trains small models with existing and potential techniques for enhancing NUPA (such as special tokenizers, PEs, and number formats), comprehensively evaluating their effectiveness using the testbed. The study also finetunes practical-scale LLMs on the proposed NUPA tasks and find that 1) naive finetuning can improve NUPA a lot on many but not all tasks, and 2) surprisingly, techniques designed to enhance NUPA prove ineffective for finetuning pretrained models. The study further explores the impact of chain-of-thought techniques on NUPA. The work takes a preliminary step towards understanding and improving NUPA of LLMs. The benchmark and code are released at https://github.com/GraphPKU/number_cookbook. <br> <br>

15. ***UCL and Google propose a soft parameter reset for non-stationary learning in neural networks:  <br>This technique adapts to non-stationary distributions by reverting parameters towards initial values, improving performance in non-stationary learning scenarios like continual learning and contextual bandits.*** <br> <br>
    Nov 6, UCL and Google published a [paper](https://openreview.net/pdf?id=fDiZJ7mmOV) on NeurIPS “Non-Stationary Learning of Neural Networks with Automatic Soft Parameter Reset”. Neural networks are traditionally trained under the assumption that data come from a stationary distribution. However, settings which violate this assumption are becoming more popular; examples include supervised learning under distributional shifts, reinforcement learning, continual learning and non-stationary contextual bandits. This study introduces a novel learning approach that automatically models and adapts to non-stationarity, via an Ornstein-Uhlenbeck process with an adaptive drift parameter. The adaptive drift tends to draw the parameters towards the initialisation distribution, so the approach can be understood as a form of soft parameter reset. The authors show empirically that the approach performs well in non-stationary supervised and off-policy reinforcement learning settings. <br> <br>

17. ***Stanford introduces a gradient method with online scaling for accelerated convergence:  <br>This approach optimally scales gradients at each iteration, achieving faster convergence in optimization tasks compared to traditional methods, especially in smooth convex optimization.*** <br> <br>
    Nov 5, Stanford Uni published a [paper](https://arxiv.org/pdf/2411.01803) “Gradient Methods with Online Scaling”. The study introduces a framework to accelerate the convergence of gradient-based methods with online learning. The framework learns to scale the gradient at each iteration through an online learning algorithm and provably accelerates gradient-based methods asymptotically. In contrast with previous literature, where convergence is established based on worst-case analysis, this framework provides a strong convergence guarantee with respect to the optimal scaling matrix for the iteration trajectory. For smooth strongly convex optimization, the results provide an O(κ⋆log(1/ε)) complexity result, where κ⋆ is the condition number achievable by the optimal preconditioner, improving on the previous O(√nκ⋆log(1/ε)) result. In particular, a variant of the method achieves superlinear convergence on convex quadratics. For smooth convex optimization, the work shows for the first time that the widely-used hypergradient descent heuristic improves on the convergence of gradient descent. <br> <br>

19. ***CMU and Bosch suggest VLMs can perform optimally with fewer visual tokens and larger models:  <br>They find that reducing visual tokens while maximizing model size minimizes inference compute without sacrificing performance, challenging current token reduction practices.*** <br> <br>
    Nov 5, CMU and Bosch published a [paper](https://arxiv.org/pdf/2411.03312) “Inference Optimal VLMs Need Only One Visual Token but Larger Models”. Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks. However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM. To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression. However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance. The study first characterizes this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. The results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., 5-10times), the results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, the study takes some initial steps towards building approaches tailored for high token compression settings. Code is available at https://github.com/locuslab/llava-token-compression. <br> <br>

21. ***University of Tokyo develops ADOPT, an adaptive gradient method ensuring convergence without bounded gradient assumptions:  <br>ADOPT improves on Adam's convergence across tasks by altering momentum calculations, proving effective in tasks like image classification and NLP.*** <br> <br>
    Nov 5, The of Tokyo published a [paper](https://arxiv.org/pdf/2411.02853) “ADOPT: Modified Adam Can Converge with Any β2 with the Optimal Rate”. Adaptive gradient methods based on exponential moving averages, such as Adam and RMSprop, are widely used for deep learning. However, it is known that they do not converge unless choosing hyperparameters in a problem-dependent manner. There have been many attempts to fix their convergence (e.g., AMSGrad), but they require an impractical assumption that the stochastic gradient is uniformly bounded. This study propose as new adaptive gradient method named ADOPT, which achieves the optimal convergence rate of O(1/√T)  with any hyperparameter choice without the bounded stochastic gradient assumption. ADOPT addresses the non-convergence issue of Adam by removing the current gradient from the second moment estimate and changing the order of the momentum calculation and the scaling operation by the second moment estimate. The study also conducts intensive numerical experiments, and verify that ADOPT achieves competitive or even better results compared to Adam and its variants across a wide range of tasks, including image classification, generative modeling, natural language processing, and deep reinforcement learning. <br> <br>

23. ***Databricks examines long-context RAG performance in LLMs and its limitations:  <br>Findings reveal that few models maintain accuracy above 64k tokens, identifying long-context scenarios where RAG remains beneficial but challenging.*** <br> <br>
    Nov 5, Databricks published a [paper](https://arxiv.org/pdf/2411.03538v1) “Long Context RAG Performance of Large Language Models”. Retrieval Augmented Generation (RAG) has emerged as a crucial technique for enhancing the accuracy of Large Language Models (LLMs) by incorporating external information. With the advent of LLMs that support increasingly longer context lengths, there is a growing interest in understanding how these models perform in RAG scenarios. Can these new long context models improve RAG performance? This paper presents a comprehensive study of the impact of increased context length on RAG performance across 20 popular open source and commercial LLMs. The study ran RAG workflows while varying the total context length from 2,000 to 128,000 tokens (and 2 million tokens when possible) on three domain-specific datasets, and reports key insights on the benefits and limitations of long context in RAG applications. The findings reveal that while retrieving more documents can improve performance, only a handful of the most recent state of the art LLMs can maintain consistent accuracy at long context above 64k tokens. The authors also identify distinct failure modes in long context scenarios, suggesting areas for future research. <br> <br>

25. ***Neural Magic and IST Austria study trade-offs in LLM quantization for faster inference:  <br>Evaluating formats like FP8 and INT8, they find that lower-bit quantization maintains performance well, with INT4 offering cost-efficient deployment, especially in asynchronous scenarios.*** <br> <br>
    Nov 4, Neural Magic and Inst of Sci and Tech Austria published a [paper](https://arxiv.org/pdf/2411.02355) “"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization”. Despite the popularity of large language model (LLM) quantization for inference acceleration, significant uncertainty remains regarding the accuracy-performance trade-offs associated with various quantization formats. The study presents a comprehensive empirical study of quantized accuracy, evaluating popular quantization formats (FP8, INT8, INT4) across academic benchmarks and real-world tasks, on the entire Llama-3.1 model family. Additionally, the study examines the difference in text generated by quantized models versus their uncompressed counterparts. Beyond benchmarks, the work also presents a couple of quantization improvements which allowed to obtain state-of-the-art accuracy recovery results. The investigation, encompassing over 500,000 individual evaluations, yields several key findings: (1) FP8 weight and activation quantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and activation quantization (W8A8-INT), when properly tuned, incurs surprisingly low 1-3% accuracy degradation, and (3) INT4 weight-only quantization (W4A16-INT) is competitive with 8-bit integer weight and activation quantization. To address the question of the "best" format for a given deployment environment, the authors conduct inference performance analysis using the popular open-source vLLM framework on various GPU architectures. The study finds that W4A16 offers the best cost-efficiency for synchronous deployments, and for asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel in asynchronous "continuous batching" deployment of mid- and large-size models on high-end GPUs. The results provide a set of practical guidelines for deploying quantized LLMs across scales and performance requirements. <br> <br>

27. ***AMD launches open-source language models for efficient local use on AMD platforms:  <br>The 1B parameter models, trained on AMD GPUs, excel in reasoning and instruction-following, offering privacy-focused, energy-efficient solutions for specialized applications.*** <br> <br>
    Nov 4, [according to AM Community](https://community.amd.com/t5/ai/introducing-the-first-amd-1b-language-models-amd-olmo/ba-p/721253), AMD released its IB language models. AMD has introduced its first series of fully open 1 billion parameter language models, AMD OLMo, continuing its tradition of open-sourcing models and code. These models are designed to be pre-trained and fine-tuned for domain-specific applications, allowing for better alignment with unique use cases. The AMD OLMo models are pre-trained with 1.3 trillion tokens on AMD Instinct GPUs and include three checkpoints: AMD OLMo 1B, AMD OLMo 1B SFT, and AMD OLMo 1B SFT DPO. These models demonstrate improved performance in reasoning, instruction-following, and chat capabilities compared to other similar-sized open-source models. AMD has also made these models accessible for local use on AMD Ryzen AI PCs, emphasizing privacy, efficiency, and lower power consumption. The initiative aims to empower the community to innovate and advance AI research. <br> <br>

29. ***Duke and Google enhance LLM factual accuracy with Self Logits Evolution Decoding (SLED):  <br>SLED improves output truthfulness by refining logits across model layers, achieving up to 20% better factuality without additional latency and supporting various model types and sizes.*** <br> <br>
    Nov 1, Duke Uni and Google published a [paper](https://arxiv.org/pdf/2411.02433) “SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models”. Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, the study introduces Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, the SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (LLaMA 2, LLaMA 3, Gemma) and scales (from 2B to 70B), including more advanced architectural configurations such as the mixture of experts (MoE). The evaluation spans a wide variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thought reasoning tasks. The results demonstrate that SLED consistently improves factual accuracy by up to 20% compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance. <br> <br>

31. ***Specialized Sparse Autoencoders (SSAEs) for Foundation Models:  <br>To better interpret rare concepts in foundation models, CMU introduced SSAEs, which focus on specific subdomains. These models outperform general-purpose SAEs in capturing subdomain-tail concepts. In a Bias in Bios case study, SSAEs reduced spurious gender information, enhancing classification accuracy by 12.5%.*** <br> <br>
    Nov 1, CMU published a [paper](https://arxiv.org/pdf/2411.00743) “Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models”. Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability methods. Sparse Autoencoders (SAEs) have emerged as a promising tool for disentangling FM representations, but they struggle to capture rare, yet crucial concepts in the data. The study introduces Specialized Sparse Autoencoders (SSAEs), designed to illuminate these elusive dark matter features by focusing on specific subdomains. The study presents a practical recipe for training SSAEs, demonstrating the efficacy of dense retrieval for data selection and the benefits of Tilted Empirical Risk Minimization as a training objective to improve concept recall. The evaluation of SSAEs on standard metrics, such as downstream perplexity and L_0 sparsity, show that they effectively capture subdomain tail concepts, exceeding the capabilities of general-purpose SAEs. The study showcases the practical utility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs achieve a 12.5% increase in worst-group classification accuracy when applied to remove spurious gender information. SSAEs provide a powerful new lens for peering into the inner workings of FMs in subdomains. <br> <br> 

33. ***DynaMath Benchmark for Mathematical Reasoning in VLMs:  <br>The DynaMath benchmark was introduced to evaluate Vision-Language Models' (VLMs) mathematical reasoning, assessing robustness with question variants. Tests on 14 models revealed lower worst-case versus average-case accuracy, emphasizing the need to strengthen VLMs' reasoning abilities.*** <br> <br>
    Oct 29, UIUC and UC Berkeley published a [paper](https://arxiv.org/pdf/2411.00836) “DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models”. The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, the study found that SOTA VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. This work investigates the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, the study introduces DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. The work evaluated 14 SOTA VLMs with 5,010 generated concrete questions. The results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. The analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.

35. ***Cost-Benefit of Semantic Chunking in RAG Systems   <br>This study challenges the efficacy of semantic chunking in Retrieval-Augmented Generation systems, finding no consistent performance gain over fixed-size chunking, thus questioning its computational cost.*** <br><br>
    Oct 16, Vectara Inc and Uni of Wisconsin-Madison published a [paper](https://arxiv.org/pdf/2410.13070) “Is Semantic Chunking Worth the Computational Cost?”. Recent advances in Retrieval-Augmented Generation (RAG) systems have popularized semantic chunking, which aims to improve retrieval performance by dividing documents into semantically coherent segments. Despite its growing adoption, the actual benefits over simpler fixed-size chunking, where documents are split into consecutive, fixed-size segments, remain unclear. This study systematically evaluates the effectiveness of semantic chunking using three common retrieval-related tasks: document retrieval, evidence retrieval, and retrieval-based answer generation. The results show that the computational costs associated with semantic chunking are not justified by consistent performance gains. These findings challenge the previous assumptions about semantic chunking and highlight the need for more efficient chunking strategies in RAG systems. <br> <br>

37. ***Numerical Representation in Language Models:  <br>Research found LLMs use a digit-wise representation in base 10, which contributes to their numerical errors. This representation explains LLM inaccuracies in basic numerical reasoning and highlights the need for alternative approaches in numerical tasks.*** <br> <br>
    Oct 15, Uni of Oxford and Tel Aviv Uni published [paper](https://arxiv.org/pdf/2410.11781) “Language Models Encode Numbers Using Digit Representations in Base 10”. Large language models (LLMs) frequently make errors when handling even simple numerical problems, such as comparing two small numbers. A natural hypothesis is that these errors stem from how LLMs represent numbers, and specifically, whether their representations of numbers capture their numeric values. The work tackles this question from the observation that LLM errors on numerical tasks are often distributed across the digits of the answer rather than normally around its numeric value. Through a series of probing experiments and causal interventions, the study shows that LLMs internally represent numbers with individual circular representations per-digit in base 10. This digit-wise representation, as opposed to a value representation, sheds light on the error patterns of models on tasks involving numerical reasoning and could serve as a basis for future studies on analyzing numerical mechanisms in LLMs. <br> <br>

39. ***Loss of Plasticity in Deep Continual Learning:  <br>The study showed that standard deep learning methods lose plasticity in continual learning tasks. Only algorithms with diversity mechanisms, such as continual backpropagation, maintained indefinite plasticity, suggesting that non-gradient elements are essential for sustained learning.*** <br> <br>
    Aug 21, Nature published a [paper](https://www.nature.com/articles/s41586-024-07711-7) from Alberta Uni “Loss of plasticity in deep continual learning”. Artificial neural networks, deep-learning methods and the backpropagation algorithm1 form the foundation of modern machine learning and artificial intelligence. These methods are almost always used in two phases, one in which the weights of the network are updated and one in which the weights are held constant while the network is used or evaluated. This contrasts with natural learning and many applications, which require continual learning. It has been unclear whether or not deep learning methods work in continual learning settings. Here the study shows that they do not—that standard deep-learning methods gradually lose plasticity in continual-learning settings until they learn no better than a shallow network. The study shows such loss of plasticity using the classic ImageNet dataset and reinforcement-learning problems across a wide range of variations in the network and the learning algorithm. Plasticity is maintained indefinitely only by algorithms that continually inject diversity into the network, such as the continual backpropagation algorithm, a variation of backpropagation in which a small fraction of less-used units are continually and randomly reinitialized. The results indicate that methods based on gradient descent are not enough—that sustained deep learning requires a random, non-gradient component to maintain variability and plasticity. <br> <br>

41. ***Evaluating Implicit World Models in Generative Models:  <br>This study introduced evaluation metrics for assessing generative models' implicit world models in deterministic domains like game-playing and navigation. Findings revealed that despite good performance on diagnostics, models' world representations were incoherent, highlighting fragility in real-world applications.*** <br> <br>
    Jun 22, Harvard Uni, MIT, Cornell Uni and Uni of Chicago Booth published a [paper](https://arxiv.org/pdf/2406.03689) “Evaluating the World Model Implicit in a Generative Model”. Recent work suggests that large language models may implicitly learn world models. How should people assess this possibility? The study formalizes this question for the case where the underlying reality is governed by a deterministic finite automaton. This includes problems as diverse as simple logical reasoning, geographic navigation, game-playing, and chemistry. The study proposes new evaluation metrics for world model recovery inspired by the classic Myhill-Nerode theorem from language theory. The authors illustrate their utility in three domains: game playing, logic puzzles, and navigation. In all domains, the generative models considered do well on existing diagnostics for assessing world models, but the evaluation metrics reveal their world models to be far less coherent than they appear. Such incoherence creates fragility: using a generative model to solve related but subtly different tasks can lead it to fail badly. Building generative models that meaningfully capture the underlying logic of the domains they model would be immensely valuable; the results suggest new ways to assess how close a given model is to that goal.
 <br> <br>


***Nov 2***

1. ***AI Identifies Database Vulnerability:  <br>Google’s AI project, “Big Sleep,” discovered a previously unknown bug in SQLite, showcasing the ability of AI agents to find software vulnerabilities. This achievement emphasizes the potential of large language models in improving software security.*** <br> <br>
   Nov 2, [according to PCMag](https://au.pcmag.com/ai/108079/googles-big-sleep-ai-project-uncovers-real-software-vulnerabilities), Google’s AI project, “Big Sleep,” recently discovered a previously unknown and exploitable bug in SQLite, an open-source database engine. This marks the first public instance of an AI agent identifying a new memory-safety issue in widely used software. The AI’s success highlights the potential of large language models in finding software vulnerabilities, offering a significant advantage in defending against hackers. Designed to mimic human security researchers, Big Sleep was able to perform a root-cause analysis by triggering and investigating the bug. This achievement suggests that AI can enhance vulnerability research, making it more efficient and effective. <br> <br>

3. ***Benchmarking Code Generation:  <br>Purdue University introduced REPOCOD, a new code generation benchmark that highlights the limitations of existing large language models (LLMs) in real-world software development. Evaluations showed no model exceeded 30% accuracy on this challenging benchmark, indicating the need for more advanced LLMs.*** <br> <br>
   Oct 31, Purdue Uni published a [paper](https://arxiv.org/pdf/2410.21647) “Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'”. Large language models (LLMs) have shown remarkable ability in code generation with more than 90 pass@1 in solving Python coding problems in HumanEval and MBPP. Such high accuracy leads to the question: can LLMs replace human programmers? Existing manual crafted, simple, or single-line code generation benchmarks cannot answer this question due to their gap with real-world software development. To answer this question, the study proposes REPOCOD, a code generation benchmark with 980 problems collected from 11 popular real-world projects, with more than 58% of them requiring file-level or repository-level context information. In addition, REPOCOD has the longest average canonical solution length (331.6 tokens) and the highest average cyclomatic complexity (9.00) compared to existing benchmarks. In the evaluations on ten LLMs, none of the models can achieve more than 30 pass@1 on REPOCOD, disclosing the necessity of building stronger LLMs that can help developers in real-world software development. <br> <br>

5. ***Layer Gradients in LLMs:  <br>A study from the University of Maryland explored how fast and slow thinking affects gradient patterns in LLMs during training. It found that slow thinking methods result in greater stability and better discrimination of correct reasoning paths, enhancing our understanding of LLM training efficiency.*** <br> <br>
   Oct 31, Uni of Maryland published a [paper](https://arxiv.org/pdf/2410.23743) “What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective”. What makes a difference in the post-training of LLMs? The work investigates the training patterns of different layers in large language models (LLMs), through the lens of gradient, when training with different responses and initial models. The authors are specifically interested in how fast vs. slow thinking affects the layer-wise gradients, given the recent popularity of training LLMs on reasoning paths such as chain-of-thoughts (CoT) and process rewards. In this study, fast thinking without CoT leads to larger gradients and larger differences of gradients across layers than slow thinking (Detailed CoT), indicating the learning stability brought by the latter. Moreover, pre-trained LLMs are less affected by the instability of fast thinking than instruction-tuned LLMs. Additionally, the authors study whether the gradient patterns can reflect the correctness of responses when training different LLMs using slow vs. fast thinking paths. The results show that the gradients of slow thinking can distinguish correct and irrelevant reasoning paths. As a comparison, the work conducts similar gradient analyses on non-reasoning knowledge learning tasks, on which, however, trivially increasing the response length does not lead to similar behaviors of slow thinking. The study strengthens fundamental understandings of LLM training and sheds novel insights on its efficiency and stability, which pave the way towards building a generalizable System-2 agent. The code, data, and gradient statistics can be found in: https://github.com/MingLiiii/Layer_Gradient. <br> <br>

7. ***Real-Time Information Integration:  <br>OpenAI launched ChatGPT Search, which enhances the chatbot's responses by incorporating real-time web data, including citations to reliable news sources. This new feature aims to improve the accuracy and relevance of information provided by ChatGPT.*** <br> <br>
   Oct 31, OpenAI [released ChatGPT Search](https://openai.com/index/introducing-chatgpt-search/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=chatgpt-search-goes-live&_bhlid=913044e6704018f496b05f6913ccb437329e2d96), aiming at offering faster, more accurate answers by pulling real-time data from the web. With links to relevant news, stock quotes, sports scores, and other information, it merges natural language responses with up-to-date details one typically get from a search engine. Each chat response now includes citations to sources, like news articles or blog posts, making it easy to dive deeper. You can click the Sources button to view the references directly. OpenAI partnered with global news organizations, including Associated Press, Le Monde, Reuters, and News Corp, to offer reliable information and extend the reach of high-quality journalism. Publishers now have the option to appear in ChatGPT’s search results, opening up new avenues for audience engagement. OpenAI plans to expand this new search experience to areas like shopping and travel, while also making it available in Advanced Voice and canvas modes. Future updates will bring ChatGPT’s search capabilities to Free and guest users, further broadening access. <br> <br>

9. ***Evaluating Factual Knowledge:  <br>OpenAI’s new benchmark, SimpleQA, assesses LLMs' abilities to answer short factual questions accurately. Designed to challenge models like GPT-4, SimpleQA offers a straightforward grading system, with the best-performing model achieving a score of 42.4.*** <br> <br>
    Oct 30, OpenAI published a [paper](https://cdn.openai.com/papers/simpleqa.pdf) “Measuring short-form factuality in large language models”. The work presents SimpleQA, a benchmark that evaluates the ability of language models to answer short, fact-seeking questions. The study prioritized two properties in designing this eval. First, SimpleQA is challenging, as it is adversarially collected against GPT-4 responses. Second, responses are easy to grade, because questions are created such that there exists only a single, indisputable answer. Each answer in SimpleQA is graded as either correct, incorrect, or not attempted. A model with ideal behavior would get as many questions correct as possible while not attempting the questions for which it is not confident it knows the correct answer. SimpleQA is a simple, targeted evaluation for whether models “know what they know,” and the authors hope is that this benchmark will remain relevant for the next few generations of frontier models. According to this test, the highest score of AI models is 42.4.  SimpleQA can be found at https://github.com/openai/simple-evals. <br> <br>

11. ***Memorization vs. Reasoning in LLMs:  <br>A study from multiple institutions investigated the role of memorization in LLMs' logical reasoning capabilities. Results indicated that while LLMs perform well on familiar puzzles, they often fail on modified versions, highlighting the balance between memorization and genuine reasoning skills.*** <br> <br>
    Oct 30, Google, UIUC, Princeton Uni, and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2410.23123) “On Memorization of Large Language Models in Logical Reasoning”. Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. This study systematically investigates this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&K) puzzles. The study found that LLMs could interpolate the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet fail when those puzzles are slightly perturbed, suggesting that the models heavily rely on memorization to solve those training puzzles. On the other hand, the study shows that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. In-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers suggest that the LLMs learn to reason on K&K puzzles despite training data memorization. This phenomenon indicates that LLMs exhibit a complex interplay between memorization and genuine reasoning abilities. Finally, analysis with per-sample memorization score sheds light on how LLMs switch between reasoning and memorization in solving logical puzzles. Code and data are available at https://memkklogic.github.io. <br> <br>

13. ***Scaling Transformers Efficiently:  <br>The introduction of TokenFormer proposes a scalable architecture for transformers that allows for efficient model parameter sharing without retraining. This approach reduces computational costs while maintaining performance comparable to traditional transformer models.*** <br> <br>
    Oct 30, MPII, Google and Peking Uni published a [paper](https://arxiv.org/pdf/2410.23168) “TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters”. Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce TokenFormer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at https://github.com/Haiyang-W/TokenFormer. <br> <br>

15. ***LLMs in Data Science:  <br>A benchmark developed by Snowflake and the Polish Academy of Sciences assesses LLMs in generating feature engineering code. The study demonstrates that LLMs can effectively transform datasets, providing a cost-effective method for evaluating their capabilities.*** <br> <br>
    Oct 30, Snowflake and Polish Academy Sciences published a [paper](https://arxiv.org/pdf/2410.23331) “Can Models Help Us Create Better Models? Evaluating LLMs as Data Scientists”. The study presents a benchmark for large language models designed to tackle one of the most knowledge-intensive tasks in data science: writing feature engineering code, which requires domain knowledge in addition to a deep understanding of the underlying problem and data structure. The model is provided with a dataset description in a prompt and asked to generate code transforming it. The evaluation score is derived from the improvement achieved by an XGBoost model fit on the modified dataset compared to the original data. By an extensive evaluation of state-of-the-art models and comparison to well-established benchmarks, the study demonstrates that the FeatEng of the proposal can cheaply and efficiently assess the broad capabilities of LLMs, in contrast to the existing methods. <br> <br>

17. ***AI in Research Support:  <br>The AAAR-1.0 benchmark, introduced by researchers from various institutions, evaluates LLMs' performance in complex research tasks. This new dataset aims to assess LLMs' capabilities in conducting specialized research activities, highlighting their potential and limitations.*** <br> <br>
    Oct 29, Pennsylvania State Uni, Netflix, Uni of California Davis, Uni of Illinois Chicago et al published a [paper](https://arxiv.org/pdf/2410.22394) “AAAR-1.0: Assessing AI's Potential to Assist Research”. Numerous studies have assessed the proficiency of AI systems, particularly large language models (LLMs), in facilitating everyday tasks such as email writing, question answering, and creative content generation. However, researchers face unique challenges and opportunities in leveraging LLMs for their own work, such as brainstorming research ideas, designing experiments, and writing or reviewing papers. This study introduces AAAR-1.0, a benchmark dataset designed to evaluate LLM performance in three fundamental, expertise-intensive research tasks: (i) EquationInference, assessing the correctness of equations based on the contextual information in paper submissions; (ii) ExperimentDesign, designing experiments to validate research ideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper submissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews is deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis. An evaluation of both open-source and proprietary LLMs reveals their potential as well as limitations in conducting sophisticated research tasks. The authors will keep iterating AAAR-1.0 to new versions. <br> <br>

19. ***Innovations in Parameter Sharing:  <br>The study on Relaxed Recursive Transformers presents a novel approach to parameter sharing in LLMs, enabling efficient model size reduction without sacrificing performance. This method improves inference speed while maintaining model effectiveness.*** <br> <br>
    Oct 28, KAIST and Google published a [paper](https://arxiv.org/pdf/2410.20672) “Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA”. Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. This work revisits "layer tying" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller "Recursive Transformers" that share parameters across layers, with minimal loss of performance. Here, the Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. The work further improves performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. The work shows that the recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original "full-size" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, the work shows that this has the potential to lead to significant (2-3x) gains in inference throughput. <br> <br>

21. ***Compression Error Compensation:  <br>Nvidia's research introduces EoRA, a training-free method for compensating errors in compressed LLMs. This approach optimizes performance without requiring extensive retraining, demonstrating significant improvements in various tasks.*** <br> <br>
    Oct 28, Nvidia published a [paper](https://arxiv.org/pdf/2410.21271) “EoRA: Training-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation”. This study re-formulates the model compression problem into the customized compensation problem: Given a compressed model, the study aims to introduce residual low-rank paths to compensate for compression errors under customized requirements from users (e.g., tasks, compression ratios), resulting in greater flexibility in adjusting overall capacity without being constrained by specific compression formats. However, naively applying SVD to derive residual paths causes suboptimal utilization of the low-rank representation capacity. Instead, the work proposes Training-free Eigenspace Low-Rank Approximation (EoRA), a method that directly minimizes compression-induced errors without requiring gradient-based training, achieving fast optimization in minutes using a small amount of calibration data. EoRA projects compression errors into the eigenspace of input activations, leveraging eigenvalues to effectively prioritize the reconstruction of high-importance error components. Moreover, EoRA can be seamlessly integrated with fine-tuning and quantization to further improve effectiveness and efficiency. EoRA consistently outperforms previous methods in compensating errors for compressed LLaMA2/3 models on various tasks, such as language generation, commonsense reasoning, and math reasoning tasks (e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and MathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4 sparsity). EoRA offers a scalable, training-free solution to compensate for compression errors, making it a powerful tool to deploy LLMs in various capacity and efficiency requirements. <br> <br>

23. ***Modular Framework for Social Reasoning:  <br>The SocialGPT framework combines vision and language models for social relation reasoning. It achieves competitive performance without additional training and offers interpretable results, enhancing understanding of image content.*** <br> <br>
    Oct 28, Harvard Uni, IBM et al. published a [paper](https://arxiv.org/pdf/2410.21411) “SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization”. Social relation reasoning aims to identify relation categories such as friends, spouses, and colleagues from images. While current methods adopt the paradigm of training a dedicated network end-to-end using labeled image data, they are limited in terms of generalizability and interpretability. To address these issues, the study first presents a simple yet well-crafted framework named SocialGPT, which combines the perception capability of Vision Foundation Models (VFMs) and the reasoning capability of Large Language Models (LLMs) within a modular framework, providing a strong baseline for social relation recognition. Specifically, the study instructs VFMs to translate image content into a textual social story, and then utilize LLMs for text-based reasoning. SocialGPT introduces systematic design principles to adapt VFMs and LLMs separately and bridge their gaps. Without additional model training, it achieves competitive zero-shot results on two databases while offering interpretable answers, as LLMs can generate language-based explanations for the decisions. The manual prompt design process for LLMs at the reasoning phase is tedious and an automated prompt optimization method is desired. As the work essentially converts a visual classification task into a generative task of LLMs, automatic prompt optimization encounters a unique long prompt optimization issue. To address this issue, the study further proposes the Greedy Segment Prompt Optimization (GSPO), which performs a greedy search by utilizing gradient information at the segment level. Experimental results show that GSPO significantly improves performance, and the method also generalizes to different image styles. The code is available at https://github.com/Mengzibin/SocialGPT. <br> <br>

25. ***Relevance Feedback in Retrieval:  <br>MIT's study presents ReDE-RF, a method for improving zero-shot dense retrieval by focusing on relevance estimation rather than hypothetical document generation. This approach enhances efficiency and accuracy in document retrieval tasks.*** <br> <br>
    Oct 28, MIT published [paper](https://arxiv.org/pdf/2410.21242) “Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback”. Building effective dense retrieval systems remains difficult when relevance supervision is not available. Recent work has looked to overcome this challenge by using a Large Language Model (LLM) to generate hypothetical documents that can be used to find the closest real document. However, this approach relies solely on the LLM to have domain-specific knowledge relevant to the query, which may not be practical. Furthermore, generating hypothetical documents can be inefficient as it requires the LLM to generate a large number of tokens for each query. To address these challenges, the work introduces Real Document Embeddings from Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF proposes to re-frame hypothetical document generation as a relevance estimation task, using an LLM to select which documents should be used for nearest neighbor search. Through this re-framing, the LLM no longer needs domain-specific knowledge but only needs to judge what is relevant. Additionally, relevance estimation only requires the LLM to output a single token, thereby improving search latency. Experiments show that ReDE-RF consistently surpasses state-of-the-art zero-shot dense retrieval methods across a wide range of low-resource retrieval datasets while also making significant improvements in latency per-query. <br> <br>

27. ***Fine-Tuning Analysis:  <br>A study contrasts Low-Rank Adaptation (LoRA) and full fine-tuning in LLMs, revealing that while they may perform similarly, they access different parts of parameter space. This difference impacts generalization and adaptability in various tasks.*** <br> <br>
    Oct 28, MIT published a [paper](https://arxiv.org/pdf/2410.21228) “LoRA vs Full Fine-tuning: An Illusion of Equivalence”. Fine-tuning is a crucial paradigm for adapting pre-trained large language models to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA) have been shown to match the performance of fully fine-tuned models on various tasks with an extreme reduction in the number of trainable parameters. Even in settings where both methods learn similarly accurate models, are their learned solutions really equivalent? The work studies how different fine-tuning methods change pre-trained models by analyzing the model's weight matrices through the lens of their spectral properties. The work finds that full fine-tuning and LoRA yield weight matrices whose singular value decompositions exhibit very different structure; moreover, the fine-tuned models themselves show distinct generalization behaviors when tested outside the adaptation task's distribution. More specifically, the study first shows that the weight matrices trained with LoRA have new, high-ranking singular vectors, which is called intruder dimensions. Intruder dimensions do not appear during full fine-tuning. Second, the work shows that LoRA models with intruder dimensions, despite achieving similar performance to full fine-tuning on the target task, become worse models of the pre-training distribution and adapt less robustly to multiple tasks sequentially. Higher-rank, rank-stabilized LoRA models closely mirror full fine-tuning, even when performing on par with lower-rank LoRA models on the same tasks. These results suggest that models updated with LoRA and full fine-tuning access different parts of parameter space, even when they perform equally on the fine-tuned distribution. The study concludes by examining why intruder dimensions appear in LoRA fine-tuned models, why they are undesirable, and how their effects can be minimized. <br> <br>

29. ***Iterative Context Retrieval:  <br>The FACT method introduces an iterative approach to improve multi-fact retrieval in LLMs, addressing the challenges of losing critical information. This technique significantly enhances performance in multi-fact tasks, highlighting the need for better retrieval strategies.*** <br> <br>
    Oct 28, DeepWisdom, Uni de Montreal & Mila and Google published a [paper](https://arxiv.org/pdf/2410.21012) “FACT: Examining the Effectiveness of Iterative Context Rewriting for Multi-fact Retrieval”. Large Language Models (LLMs) are proficient at retrieving single facts from extended contexts, yet they struggle with tasks requiring the simultaneous retrieval of multiple facts, especially during generation. This paper identifies a novel "lost-in-the-middle" phenomenon, where LLMs progressively lose track of critical information throughout the generation process, resulting in incomplete or inaccurate retrieval. To address this challenge, the study introduces Find All Crucial Texts (FACT), an iterative retrieval method that refines context through successive rounds of rewriting. This approach enables models to capture essential facts incrementally, which are often overlooked in single-pass retrieval. Experiments demonstrate that FACT substantially enhances multi-fact retrieval performance across various tasks, though improvements are less notable in general-purpose QA scenarios. The findings shed light on the limitations of LLMs in multi-fact retrieval and underscore the need for more resilient long-context retrieval strategies. <br> <br>

31. ***Relaxed Recursive Transformers:  <br>This paper introduces a method for parameter sharing in large language models (LLMs) through Recursive Transformers, which reuse layers to reduce size without significant performance loss. The approach, enhanced by Relaxed Recursive Transformers using LoRA modules, achieves better performance than traditional models and proposes a new inference method that could improve throughput significantly.*** <br> <br>
    Oct 28, KAIST AI and Google published a [paper](https://arxiv.org/pdf/2410.20672) “Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA”. Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. This work revisits "layer tying" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller "Recursive Transformers" that share parameters across layers, with minimal loss of performance. Here, the Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. The work further improves performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. The study shows that the recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original "full-size" model (e.g., Gemma 2B with no shared parameters). Finally, the work proposes Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, the work shows that this has the potential to lead to significant (2-3x) gains in inference throughput. <br> <br>

33. ***Beyond Autoregression:  <br>This research explores diffusion language models that can generate multiple tokens simultaneously, outperforming autoregressive models in quality and speed. A novel distillation method reduces inference steps drastically, leading to faster generation rates.*** <br> <br>
    Oct 28, EPFL published a [paper](https://arxiv.org/pdf/2410.21035) “Beyond Autoregression: Fast LLMs via Self-Distillation Through Time”. Autoregressive (AR) Large Language Models (LLMs) have demonstrated significant success across numerous tasks. However, the AR modeling paradigm presents certain limitations; for instance, contemporary autoregressive LLMs are trained to generate one token at a time, which can result in noticeable latency. Recent advances have indicated that search and repeated sampling can enhance performance in various applications, such as theorem proving, code generation, and alignment, by utilizing greater computational resources during inference. This paper demonstrates that diffusion language models are capable of generating at least 32 tokens simultaneously, while exceeding the performance of AR models in text quality and on the LAMBADA natural language understanding benchmark. This outcome is achieved through a novel distillation method for discrete diffusion models, which reduces the number of inference steps by a factor of 32-64. Practically, the models, even without caching, can generate tokens at a rate that is up to 8 times faster than AR models employing KV caching, and it is anticipated further improvements with the inclusion of caching. Moreover, the paper demonstrates the efficacy of the approach for diffusion language models with up to 860M parameters. <br> <br>

35. ***Chain-of-Thought Prompting:  <br>Investigating when chain-of-thought prompting can harm model performance, this study identifies tasks where reasoning can lead to poorer outcomes, drawing parallels with human cognition. It shows significant drops in performance on specific tasks when using CoT.*** <br> <br>
    Oct 27, Princeton Uni and NYU published a [paper](https://arxiv.org/pdf/2410.21333) “Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse”. Chain-of-thought (CoT) prompting has become a widely used strategy for working with large language and multimodal models. While CoT has been shown to improve performance across many tasks, determining the settings in which it is effective remains an ongoing effort. In particular, it is still an open question in what settings CoT systematically reduces model performance. This study seeks to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, looking at cases where (i) verbal thinking or deliberation hurts performance in humans, and (ii) the constraints governing human performance generalize to language models. Three such cases are implicit statistical learning, visual recognition, and classifying with patterns containing exceptions. In extensive experiments across all three settings, the study finds that a diverse collection of state-of-the-art models exhibit significant drop-offs in performance (e.g., up to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using inference-time reasoning compared to zero-shot counterparts. The work also identifies three tasks that satisfy condition (i) but not (ii), and find that while verbal thinking reduces human performance in these tasks, CoT retains or increases model performance. Overall, the results show that while there is not an exact parallel between the cognitive processes of models and those of humans, considering cases where thinking has negative consequences for human performance can help to identify settings where it negatively impacts models. By connecting the literature on human deliberation with evaluations of CoT, the work offers a new tool that can be used in understanding the impact of prompt choices and inference-time reasoning. <br> <br>

37. ***Centaur: A Unified Model of Cognition:  <br>The Centaur model aims to simulate human cognition through extensive behavioral data. It shows promise in predicting human responses across diverse experimental settings, potentially transforming cognitive science.*** <br> <br>
    Oct 26, Helmholtz Munich, Uni of Tuebingen, Uni of Oxford, NYU, MPI, Google, Princeton Uni, Uni of Cambridge et al published 140-page [paper](https://arxiv.org/pdf/2410.20268) “Centaur: a foundation model of human cognition”. Establishing a unified theory of cognition has been a major goal of psychology. While there have been previous attempts to instantiate such theories by building computational models, we currently do not have one model that captures the human mind in its entirety. Here the study introduces Centaur, a computational model that can predict and simulate human behavior in any experiment expressible in natural language. The study derived Centaur by finetuning a state-of-the-art language model on a novel, large-scale data set called Psych-101. Psych-101 reaches an unprecedented scale, covering trial-by-trial data from over 60,000 participants performing over 10,000,000 choices in 160 experiments. Centaur not only captures the behavior of held-out participants better than existing cognitive models, but also generalizes to new cover stories, structural task modifications, and entirely new domains. Furthermore, the study finds that the model's internal representations become more aligned with human neural activity after finetuning. Taken together, Centaur is the first real candidate for a unified model of human cognition. The authors anticipate that it will have a disruptive impact on the cognitive sciences, challenging the existing paradigm for developing computational models. <br> <br>

39. ***Fast Best-of-N Decoding:  <br>This paper presents Speculative Rejection, a method for efficient inference-time alignment of LLMs, achieving the effectiveness of Best-of-N alignment with significantly lower computational costs.*** <br> <br>
    Oct 26, CMU, Uni of Virginia, UC Berkeley, Princeton, Fudan Uni published a [paper](https://arxiv.org/pdf/2410.20290) “Fast Best-of-N Decoding via Speculative Rejection”. The safe and effective deployment of Large Language Models (LLMs) involves a critical step called alignment, which ensures that the model's responses are in accordance with human preferences. Prevalent alignment techniques, such as DPO, PPO and their variants, align LLMs by changing the pre-trained model weights during a phase called post-training. While predominant, these post-training methods add substantial complexity before LLMs can be deployed. Inference-time alignment methods avoid the complex post-training step and instead bias the generation towards responses that are aligned with human preferences. The best-known inference-time alignment method, called Best-of-N, is as effective as the state-of-the-art post-training procedures. Unfortunately, Best-of-N requires vastly more resources at inference time than standard decoding strategies, which makes it computationally not viable. This study introduces Speculative Rejection, a computationally-viable inference-time alignment algorithm. It generates high-scoring responses according to a given reward model, like Best-of-N does, while being between 16 to 32 times more computationally efficient. <br> <br>

41. ***RARe: Retrieval Augmented Retrieval:  <br>RARe demonstrates how in-context examples can improve retrieval model performance by finetuning models on similar query pairs, leading to enhanced generalization and retrieval accuracy.*** <br> <br>
    Oct 26, Uni of Texas at Austin et al. published a [paper](https://arxiv.org/pdf/2410.20088) “RARe: Retrieval Augmented Retrieval with In-Context Examples”. The work investigates whether in-context examples, widely used in decoder-only language models (LLMs), can improve embedding model performance in retrieval tasks. Unlike in LLMs, naively prepending in-context examples (query-document pairs) to the target query at inference time does not work out of the box. The study introduces a simple approach to enable retrievers to use in-context examples. The approach, RARe, finetunes a pre-trained model with in-context examples whose query is semantically similar to the target query. This can be applied to adapt various base architectures (i.e., decoder-only language models, retriever models) and consistently achieves performance gains of up to +2.72% nDCG across various open-domain retrieval datasets (BeIR, RAR-b). In particular, the study finds RARe exhibits stronger out-of-domain generalization compared to models using queries without in-context examples, similar to what is seen for in-context learning in LLMs. The study further provides analysis on the design choices of in-context example augmentation and lay the foundation for future work in this space. <br> <br>

43. ***Rethinking Uncertainty:  <br>This review proposes a comprehensive framework for understanding and measuring uncertainty in LLMs, aiming to improve reliability in mission-critical applications.*** <br> <br>
    Oct 26, Virginia Tech, UIUC, The Uni of Texas at Dallas and Uni of California Davis published a [paper](https://arxiv.org/pdf/2410.20199) “Rethinking the Uncertainty: A Critical Review and Analysis in the Era of Large Language Models”. In recent years, Large Language Models (LLMs) have become fundamental to a broad spectrum of artificial intelligence applications. As the use of LLMs expands, precisely estimating the uncertainty in their predictions has become crucial. Current methods often struggle to accurately identify, measure, and address the true uncertainty, with many focusing primarily on estimating model confidence. This discrepancy is largely due to an incomplete understanding of where, when, and how uncertainties are injected into models. This paper introduces a comprehensive framework specifically designed to identify and understand the types and sources of uncertainty, aligned with the unique characteristics of LLMs. The proposed framework enhances the understanding of the diverse landscape of uncertainties by systematically categorizing and defining each type, establishing a solid foundation for developing targeted methods that can precisely quantify these uncertainties. The study also provides a detailed introduction to key related concepts and examine the limitations of current methods in mission-critical and safety-sensitive applications. The paper concludes with a perspective on future directions aimed at enhancing the reliability and practical adoption of these methods in real-world scenarios. <br> <br>

45. ***Model Equality Testing:  <br>The study formalizes methods for detecting changes in model behavior from APIs, showing how statistical tests can identify whether models serve altered outputs compared to their original versions.*** <br>v
    Oct 26, Stanford Uni published a [paper](https://arxiv.org/pdf/2410.20247) “Model Equality Testing: Which Model Is This API Serving?”. Users often interact with large language models through black-box inference APIs, both for closed- and open-weight models (e.g., Llama models are popularly accessed via Amazon Bedrock and Azure AI Studio). In order to cut costs or add functionality, API providers may quantize, watermark, or finetune the underlying model, changing the output distribution -- often without notifying users. The study formalizes detecting such distortions as Model Equality Testing, a two-sample testing problem, where the user collects samples from the API and a reference distribution and conducts a statistical test to see if the two distributions are the same. The study finds that tests based on the Maximum Mean Discrepancy between distributions are powerful for this task: a test built on a simple string kernel achieves a median of 77.4% power against a range of distortions, using an average of just 10 samples per prompt. The authors then apply this test to commercial inference APIs for four Llama models, finding that 11 out of 31 endpoints serve different distributions than reference weights released by Meta. <br> <br>

47. ***Measuring Memorization in LLMs:  <br>This research introduces a probabilistic method for assessing LLM memorization rates, providing a clearer understanding of how different sampling techniques influence data extraction capabilities.*** <br> <br>
    Oct 25, Google and Boston Uni published a [paper](https://arxiv.org/pdf/2410.19482) “Measuring memorization through probabilistic discoverable extraction”. Large language models (LLMs) are susceptible to memorizing training data, raising concerns due to the potential extraction of sensitive information. Current methods to measure memorization rates of LLMs, primarily discoverable extraction (Carlini et al., 2022), rely on single-sequence greedy sampling, potentially underestimating the true extent of memorization. This paper introduces a probabilistic relaxation of discoverable extraction that quantifies the probability of extracting a target sequence within a set of generated samples, considering various sampling schemes and multiple attempts. This approach addresses the limitations of reporting memorization rates through discoverable extraction by accounting for the probabilistic nature of LLMs and user interaction patterns. Experiments demonstrate that this probabilistic measure can reveal cases of higher memorization rates compared to rates found through discoverable extraction. The work further investigates the impact of different sampling schemes on extractability, providing a more comprehensive and realistic assessment of LLM memorization and its associated risks. The contributions include a new probabilistic memorization definition, empirical evidence of its effectiveness, and a thorough evaluation across different models, sizes, sampling schemes, and training data repetitions.  <br> <br>

49. ***GPT-4o System Card:  <br>This document outlines the capabilities and safety evaluations of the GPT-4o model, which integrates text, audio, and visual inputs and outputs. It highlights the model's speed and performance improvements over predecessors.*** <br> <br>
    Oct 25, OpenAI published a [paper](https://arxiv.org/pdf/2410.21276) “GPT-4o System Card”. GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with the commitment to building AI safely and consistent with the voluntary commitments to the White House, OpenAI is sharing the GPT-4o System Card, which includes the Preparedness Framework evaluations. This System Card provides a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures implemented to ensure the model is safe and aligned. The paper also includes third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities. <br> <br>

51. ***COAT: Memory-Efficient FP8 Training:  <br>COAT introduces a method to reduce memory usage in FP8 training by optimizing optimizer states and activations, achieving significant training speedups while maintaining performance.*** <br> <br>
    Oct 25, UC Berkeley, Nvidia, MIT and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2410.19313) “COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training”. FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (Compressing Optimizer States and Activations for FP8 Training), a novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1) Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2) Mixed-Granularity Activation Quantization, which optimizes activation memory using a combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by 1.54x compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves a 1.43x end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngine's speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing a practical solution for scaling large-scale model training. The code is available at https://github.com/NVlabs/COAT. <br> <br>

53. ***Mixture of Parrots:  <br>This study analyzes the trade-offs in MoE architectures, finding that while they excel at memorization, their reasoning capabilities may not scale similarly.*** <br> <br>
    Oct 24, Harvad Uni, MIT and Microsoft published a [paper](https://arxiv.org/pdf/2410.19034) “Mixture of Parrots: Experts improve memorization more than reasoning”. The Mixture-of-Experts (MoE) architecture enables a significant increase in the total number of model parameters with minimal computational overhead. However, it is not clear what performance tradeoffs, if any, exist between MoEs and standard dense transformers. This study shows that as increasing the number of experts (while fixing the number of active parameters), the memorization performance consistently increases while the reasoning capabilities saturate. The study begins by analyzing the theoretical limitations of MoEs at reasoning. The study proves that there exist graph problems that cannot be solved by any number of experts of a certain width; however, the same task can be easily solved by a dense model with a slightly larger width. On the other hand, the study finds that on memory-intensive tasks, MoEs can effectively leverage a small number of active parameters with a large number of experts to memorize the data. The study empirically validates these findings on synthetic graph problems and memory-intensive closed book retrieval tasks. Lastly, the study pre-trains a series of MoEs and dense transformers and evaluate them on commonly used benchmarks in math and natural language. The authors find that increasing the number of experts helps solve knowledge-intensive tasks, but fails to yield the same benefits for reasoning tasks. <br> <br>

55. ***Detecting Label Errors in Datasets:  <br>The paper demonstrates how LLMs can detect label errors in training datasets, suggesting that many perceived model failures may stem from mislabeled data.*** <br> <br>
    Oct 24, Technion Inst of Tech and Google published a [paper](https://arxiv.org/pdf/2410.18889) “Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance”. NLP benchmarks rely on standardized datasets for training and evaluating models and are crucial for advancing the field. Traditionally, expert annotations ensure high-quality labels; however, the cost of expert annotation does not scale well with the growing demand for larger datasets required by modern models. While crowd-sourcing provides a more scalable solution, it often comes at the expense of annotation precision and consistency. Recent advancements in large language models (LLMs) offer new opportunities to enhance the annotation process, particularly for detecting label errors in existing datasets. This study considers the recent approach of LLM-as-a-judge, leveraging an ensemble of LLMs to flag potentially mislabeled examples. Through a case study of four datasets from the TRUE benchmark, covering different tasks and domains, the study empirically analyzes the labeling quality of existing datasets, and compare expert, crowd-sourced, and the LLM-based annotations in terms of agreement, label quality, and efficiency, demonstrating the strengths and limitations of each annotation method. The findings reveal a substantial number of label errors, which, when corrected, induce a significant upward shift in reported model performance. This suggests that many of the LLMs so-called mistakes are due to label errors rather than genuine model failures. Additionally, the work discusses the implications of mislabeled data and propose methods to mitigate them in training to improve model performance. <br> <br>

57. ***Read-ME: Router-Decoupled Mixture of Experts:  <br>This research presents a framework to transform dense LLMs into efficient MoE models, addressing inference challenges and improving latency and accuracy through better system integration.*** <br> <br>
    Oct 24, The Uni of Texas at Austin and Qualcomm AI Research published a [paper](https://arxiv.org/pdf/2410.19123) “Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design”. The proliferation of large language models (LLMs) has led to the adoption of Mixture-of-Experts (MoE) architectures that dynamically leverage specialized subnetworks for improved efficiency and performance. Despite their benefits, MoE models face significant challenges during inference, including inefficient memory management and suboptimal batching, due to misaligned design choices between the model architecture and the system policies. Furthermore, the conventional approach of training MoEs from scratch is increasingly prohibitive in terms of cost. This work proposes a novel framework Read-ME that transforms pre-trained dense LLMs into smaller MoE models (in contrast to "upcycling" generalist MoEs), avoiding the high costs of ground-up training. The approach employs activation sparsity to extract experts. To compose experts, the work examines the widely-adopted layer-wise router design and show its redundancy, and thus introducing the pre-gating router decoupled from the MoE backbone that facilitates system-friendly pre-computing and lookahead scheduling, enhancing expert-aware batching and caching. The codesign therefore addresses critical gaps on both the algorithmic and system fronts, establishing a scalable and efficient alternative for LLM inference in resource-constrained settings. Read-ME outperforms other popular open-source dense models of similar scales, achieving improvements of up to 10.1% on MMLU, and improving mean end-to-end latency up to 6.1%. Codes are available at: https://github.com/VITA-Group/READ-ME.

 <br> <br> <br>


***Oct 27***

1. ***Watermark Robustness:  <br>Nanyang Tech Uni and ETH Zurich introduced "W-Bench," a benchmark for assessing watermarking robustness against editing by large text-to-image models. Traditional watermarking fails under intense edits, so they propose "VINE," a novel watermark method improving edit resilience through frequency analysis and pretrained diffusion models, enhancing watermark robustness and quality.*** <br> <br>
   Oct 24, Nanyang Tech Uni and ETH Zurich published a [paper](https://arxiv.org/pdf/2410.18775) “Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances”. Current image watermarking methods are vulnerable to advanced image editing techniques enabled by large-scale text-to-image models. These models can distort embedded watermarks during editing, posing significant challenges to copyright protection. This work introduces W-Bench, the first comprehensive benchmark designed to evaluate the robustness of watermarking methods against a wide range of image editing techniques, including image regeneration, global editing, local editing, and image-to-video generation. Through extensive evaluations of eleven representative watermarking methods against prevalent editing techniques, the work demonstrates that most methods fail to detect watermarks after such edits. To address this limitation, the study proposes VINE, a watermarking method that significantly enhances robustness against various image editing techniques while maintaining high image quality. The approach involves two key innovations: (1) to analyze the frequency characteristics of image editing and identify that blurring distortions exhibit similar frequency properties, which allows to use them as surrogate attacks during training to bolster watermark robustness; (2) to leverage a large-scale pretrained diffusion model SDXL-Turbo, adapting it for the watermarking task to achieve more imperceptible and robust watermark embedding. Experimental results show that the method achieves outstanding watermarking performance under various image editing techniques, outperforming existing methods in both image quality and robustness. Code is available at https://github.com/Shilin-LU/VINE. <br> <br>

3. ***Effective Context Length in LLMs:  <br>Research from Hong Kong Uni, ByteDance, and UIUC finds LLMs often utilize less than half of their training context. A novel method, STRING, adjusts position embeddings to increase effective context length, improving benchmark performance significantly on models like Llama3.1.*** <br> <br>
   Oct 24, Uni of HongKong, ByteDance and UIUC published a [paper](https://arxiv.org/pdf/2410.18745) “Why Does the Effective Context Length of LLMs Fall Short?”. Advancements in distributed training and efficient attention mechanisms have significantly expanded the context window sizes of large language models (LLMs). However, recent work reveals that the effective context lengths of open-source LLMs often fall short, typically not exceeding half of their training lengths. This work attributes this limitation to the left-skewed frequency distribution of relative positions formed in LLMs pretraining and post-training stages, which impedes their ability to effectively gather distant information. To address this challenge, the work introduce ShifTed Rotray position embeddING (STRING). STRING shifts well-trained positions to overwrite the original ineffective positions during inference, enhancing performance within their existing training lengths. Experimental results show that without additional training, STRING dramatically improves the performance of the latest large-scale models, such as Llama3.1 70B and Qwen2 72B, by over 10 points on popular long-context benchmarks RULER and InfiniteBench, establishing new state-of-the-art results for open-source LLMs. Compared to commercial models, Llama 3.1 70B with StRing even achieves better performance than GPT-4-128K and clearly surpasses Claude 2 and Kimi-chat. <br> <br>

5. ***Challenges in Model Editing:  <br>HKUST and Hong Kong Baptist Uni found that while model editing is useful for updating language model knowledge, it can degrade general abilities if edits accumulate. Large models and instruction-tuned ones show more resilience, suggesting new approaches are needed for substantial knowledge updates.*** <br> <br>
   Oct 24, HKUST and HongKong Baptist Uni published a [paper](https://arxiv.org/pdf/2410.18785) “Should We Really Edit Language Models? On the Evaluation of Edited Language Models”. Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. Current methods mainly focus on reliability, generalization, and locality, with many methods excelling across these criteria. Some recent works disclose the pitfalls of these editing methods such as knowledge distortion or conflict. However, the general abilities of post-edited language models remain unexplored. This study performs a comprehensive evaluation on various editing methods and different language models, and have following findings. (1) Existing editing methods lead to inevitable performance deterioration on general benchmarks, indicating that existing editing methods maintain the general abilities of the model within only a few dozen edits. When the number of edits is slightly large, the intrinsic knowledge structure of the model is disrupted or even completely damaged. (2) Instruction-tuned models are more robust to editing, showing less performance drop on general knowledge after editing. (3) Language model with large scale is more resistant to editing compared to small model. (4) The safety of the edited model, is significantly weakened, even for those safety-aligned models. The findings indicate that current editing methods are only suitable for small-scale knowledge updates within language models, which motivates further research on more practical and reliable editing methods. The details of code and reproduction can be found in https://github.com/lqinfdim/EditingEvaluation. <br> <br>

7. ***Reducing LLM Hallucinations:  <br>Researchers from the Uni of Edinburgh and partners propose DeCoRe, a decoding strategy that contrasts retrieval head outputs to improve factual accuracy in LLMs. By contrasting masked and base LLM outputs, DeCoRe effectively reduces hallucinations in tasks like summarization and question answering.*** <br> <br>
   Oct 24, Uni of Edinburgh, Miniml.AI, AstraZeneca and Uni College London published a [paper](https://arxiv.org/pdf/2410.18860) “DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations”. Large Language Models (LLMs) often hallucinate, producing unfaithful or factually incorrect outputs by misrepresenting the provided context or incorrectly recalling internal knowledge. Recent studies have identified specific attention heads within the Transformer architecture, known as retrieval heads, responsible for extracting relevant contextual information. The study hypothesises that masking these retrieval heads can induce hallucinations and that contrasting the outputs of the base LLM and the masked LLM can reduce hallucinations. To this end, the study proposes Decoding by Contrasting Retrieval Heads (DeCoRe), a novel training-free decoding strategy that amplifies information found in the context and model parameters. DeCoRe mitigates potentially hallucinated responses by dynamically contrasting the outputs of the base LLM and the masked LLM, using conditional entropy as a guide. Extensive experiments confirm that DeCoRe significantly improves performance on tasks requiring high contextual faithfulness, such as summarisation (XSum by 18.6%), instruction following (MemoTrap by 10.9%), and open-book question answering (NQ-Open by 2.4% and NQ-Swap by 5.5%). <br> <br>

9. ***Scalable LLM Watermarking:  <br>Google’s "SynthID-Text" is a scalable watermarking tool that accurately identifies synthetic text without impairing text quality, integrating with speculative sampling for production-level efficiency. A live experiment confirms high detection accuracy and text quality retention in large-scale deployments.*** <br> <br>
    Oct 23, Google published a [paper](https://www.nature.com/articles/s41586-024-08025-4) on Nature “Scalable watermarking for identifying large language model outputs”. Large language models (LLMs) have enabled the generation of high-quality synthetic text, often indistinguishable from human-written content, at a scale that can markedly affect the nature of the information ecosystem. Watermarking can help identify synthetic text and limit accidental or deliberate misuse, but has not been adopted in production systems owing to stringent quality, detectability and computational efficiency requirements. Here the study describes SynthID-Text, a production-ready text watermarking scheme that preserves text quality and enables high detection accuracy, with minimal latency overhead. SynthID-Text does not affect LLM training and modifies only the sampling procedure; watermark detection is computationally efficient, without using the underlying LLM. To enable watermarking at scale, the study develops an algorithm integrating watermarking with speculative sampling, an efficiency technique frequently used in production systems. Evaluations across multiple LLMs empirically show that SynthID-Text provides improved detectability over comparable methods, and standard benchmarks and human side-by-side ratings indicate no change in LLM capabilities. To demonstrate the feasibility of watermarking in large-scale-production systems, the researchers conducted a live experiment that assessed feedback from nearly 20 million Gemini responses, again confirming the preservation of text quality. The authors hope that the availability of SynthID-Text7 will facilitate further development of watermarking and responsible use of LLM systems. <br> <br>

11. ***Scaling Diffusion Language Models:  <br>Researchers propose adapting autoregressive models into diffusion models for scalable text generation. This adaptation, named DiffuGPT and DiffuLLaMA, performs comparably to AR models and excels in in-context learning. Their models, spanning various sizes, enhance text generation diversity.*** <br> <br>
    Oct 23, The Uni of HongKong, UIUC, Apple and Tencent published a [paper](https://arxiv.org/pdf/2410.17891) “Scaling Diffusion Language Models via Adaptation from Autoregressive Models”. Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, the work proposes adapting these models to build text diffusion models. The study demonstrates connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, the work shows that it is able to convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. The authors release a suite of DLMs (with 127M, 355M, and 7B parameters) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions https://github.com/HKUNLP/DiffuLLaMA. <br> <br>

13. ***Autonomous AI Agents in Business:  <br>Microsoft’s AI-powered autonomous agents enhance business productivity by automating routine tasks and improving customer experience. Created through Copilot Studio, these agents are efficient, secure, and represent a significant leap in business automation.*** <br> <br>
    Oct 22, Microsoft is revolutionizing business processes with the introduction of [autonomous agents](https://blogs.microsoft.com/blog/2024/10/21/new-autonomous-agents-scale-your-team-like-never-before/), AI-powered tools designed to streamline tasks and enhance productivity across various departments. These agents can be created using the Copilot Studio platform or selected from a range of pre-built options available in Dynamics 365. Key benefits of autonomous agents include: 1) Increased efficiency - Automating routine tasks frees up employees to focus on more strategic work. 2) Cost reduction - Streamlining processes and reducing manual errors can lead to significant cost savings. 3) Improved customer experience - Agents can provide faster and more accurate responses to customer inquiries. Microsoft is also emphasizing the importance of data security and responsible AI in the development and deployment of autonomous agents. By ensuring that these tools adhere to strict privacy and ethical standards, businesses can confidently adopt them without compromising their customers' trust. With the potential to transform industries and drive competitive advantage, autonomous agents represent a significant step forward in the world of AI and business automation. Forbes reported this news as “Imagine walking into your office to find that your company just hired thousands of new employees overnight – except they're not human. That's exactly what Microsoft has made possible with its groundbreaking announcement of autonomous AI agents, marking a fundamental shift in how businesses will operate in the coming years.” <br> <br>

15. ***LLMs and Instruction-following Uncertainty:  <br>Uni of Cambridge, NUS, and Apple research highlights that LLMs struggle with instruction-following consistency. They propose benchmarks to assess uncertainty in LLM responses, identifying gaps in current uncertainty methods, especially in complex instructions, prompting future improvements.*** <br> <br>
    Oct 22, Uni of Cambridge, National Uni of Singapore and Apple published a [paper](https://arxiv.org/pdf/2410.14582) “Do LLMs estimate uncertainty well in instruction-following?”. Large language models (LLMs) could be valuable personal AI agents across various domains, provided they can precisely follow user instructions. However, recent studies have shown significant limitations in LLMs' instruction-following capabilities, raising concerns about their reliability in high-stakes applications. Accurately estimating LLMs' uncertainty in adhering to instructions is critical to mitigating deployment risks. The study presents the first systematic evaluation of the uncertainty estimation abilities of LLMs in the context of instruction-following. The study identifies key challenges with existing instruction-following benchmarks, where multiple factors are entangled with uncertainty stems from instruction-following, complicating the isolation and comparison across methods and models. To address these issues, the study introduces a controlled evaluation setup with two benchmark versions of data, enabling a comprehensive comparison of uncertainty estimation methods under various conditions. The findings show that existing uncertainty methods struggle, particularly when models make subtle errors in instruction following. While internal model states provide some improvement, they remain inadequate in more complex scenarios. The insights from the controlled evaluation setups provide a crucial understanding of LLMs' limitations and potential for uncertainty estimation in instruction-following tasks, paving the way for more trustworthy AI agents. <br> <br>

17. ***Vision-Language Model Benchmarking:  <br>CMU and the Uni of Washington introduce NaturalBench, a VLM benchmark with adversarial samples requiring advanced reasoning. They show VLMs lag behind humans in complex visio-linguistic tasks, highlighting gaps in model comprehension of visual details and commonsense priors.*** <br> <br>
    Oct 22, CMU and Uni of Washington published a [paper](https://arxiv.org/pdf/2410.14669) “NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples”. Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? This study shows that VLMs still struggle with natural images and questions that humans can easily answer, which is termed as natural adversarial samples. The study also finds it surprisingly easy to generate these VQA samples from natural image-text corpora using off-the-shelf models like CLIP and ChatGPT. The work proposes a semi-automated approach to collect a new benchmark, NaturalBench, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, the study adopts a vision-centric design by pairing each question with two images that yield different answers, preventing blind solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can be solved with commonsense priors. The study evaluates 53 state-of-the-art VLMs on NaturalBench, showing that models like LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o lag 50%-70% behind human performance (over 90%). The authors analyze why NaturalBench is hard from two angles: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, the work tags each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) Biases: NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. Lastly, the authors apply the benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs. <br> <br>

19. ***Improving Model Generalization with Layer Scaling:  <br>Researchers from EPFL, Google, and Uni of Geneva propose LiNeS, a method for scaling model updates by layer depth to enhance fine-tuning without forgetting core knowledge. LiNeS boosts single- and multi-task performance, particularly in multi-task model merging.*** <br> <br>
    Oct 22, EPFL, Google, and Uni of Geneva published a [paper](https://arxiv.org/pdf/2410.17146) “LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model Merging”. Large pre-trained models exhibit impressive zero-shot performance across diverse tasks, but fine-tuning often leads to catastrophic forgetting, where improvements on a target domain degrade generalization on other tasks. To address this challenge, the work introduces LiNeS, Layer-increasing Network Scaling, a post-training editing technique designed to preserve pre-trained generalization while enhancing fine-tuned task performance. LiNeS scales parameter updates linearly based on their layer depth within the network, maintaining shallow layers close to their pre-trained values to preserve general features while allowing deeper layers to retain task-specific representations. The work further extends this approach to multi-task model merging scenarios, where layer-wise scaling of merged parameters reduces negative task interference. LiNeS demonstrates significant improvements in both single-task and multi-task settings across various benchmarks in vision and natural language processing. It mitigates forgetting, enhances out-of-distribution generalization, integrates seamlessly with existing multi-task model merging baselines improving their performance across benchmarks and model sizes, and can boost generalization when merging LLM policies aligned with different rewards via RLHF. Importantly, the method is simple to implement and complementary to many existing techniques. [Code is here](https://github.com/wang-kee/LiNeS). <br> <br>

21. ***Multilingual and Multimodal LLM:  <br>CMU's Pangea model supports 39 languages, addressing the lack of multilingual multimodal models. With a culturally relevant evaluation suite, Pangea outperforms existing models in linguistic and cultural inclusivity, opening avenues for equitable AI development.*** <br> <br>
    Oct 21, CMU published a [paper](https://arxiv.org/pdf/2410.16153) “Pangea: A Fully Open Multilingual Multimodal LLM supporting 39 languages”. Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the world's languages and diverse cultural contexts underrepresented. This paper introduces Pangea, a multilingual multimodal LLM trained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages. PangeaIns features: 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage. To rigorously assess models' capabilities, the study introduces PangeaBench, a holistic evaluation suite encompassing 14 datasets covering 47 languages. Results show that Pangea significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts. Ablation studies further reveal the importance of English data proportions, language popularity, and the number of multimodal training samples on overall performance. The authors fully open-source the data, code, and trained checkpoints, to facilitate the development of inclusive and robust multilingual MLLMs, promoting equity and accessibility across a broader linguistic and cultural spectrum. <br> <br>

23. ***No-code Model Training:  <br>Hugging Face's AutoTrain tool enables no-code training across model types and modalities. It streamlines training for various tasks, including LLM fine-tuning, image classification, and regression on custom datasets, making advanced model development accessible and efficient.*** <br> <br>
    Oct 21, Hugging face published a [paper](https://arxiv.org/pdf/2410.15735) “AutoTrain: No-code training for state-of-the-art models”. With the advancements in open-source models, training (or finetuning) models on custom datasets has become a crucial part of developing solutions which are tailored to specific industrial or open-source applications. Yet, there is no single tool which simplifies the process of training across different types of modalities or tasks. The study introduces AutoTrain (aka AutoTrain Advanced) -- an open-source, no code tool/library which can be used to train (or finetune) models for different kinds of tasks such as: large language model (LLM) finetuning, text classification/regression, token classification, sequence-to-sequence task, finetuning of sentence transformers, visual language model (VLM) finetuning, image classification/regression and even classification and regression tasks on tabular data. AutoTrain Advanced is an open-source library providing best practices for training models on custom datasets. The library is available at https://github.com/huggingface/autotrain-advanced. AutoTrain can be used in fully local mode or on cloud machines and works with tens of thousands of models shared on Hugging Face Hub and their variations. <br> <br>

25. ***Adaptive Reasoning with SMART:  <br>ETH and Microsoft propose SMART, a meta-strategy agent for reasoning tasks that allows LLMs to self-learn optimal reasoning strategies, eliminating the need for multiple inference passes. SMART improves reasoning efficiency, especially in complex deductive tasks.*** <br> <br>
    Oct 21, ETH and Microsoft published a [paper](https://arxiv.org/pdf/2410.16128) “SMART: Self-learning Meta-strategy Agent for Reasoning Tasks”. Tasks requiring deductive reasoning, especially those involving multiple steps, often demand adaptive strategies such as intermediate generation of rationales or programs, as no single approach is universally optimal. While Language Models (LMs) can enhance their outputs through iterative self-refinement and strategy adjustments, they frequently fail to apply the most effective strategy in their first attempt. This inefficiency raises the question: Can LMs learn to select the optimal strategy in the first attempt, without a need for refinement? To address this challenge, the study introduces SMART (Self-learning Meta-strategy Agent for Reasoning Tasks), a novel framework that enables LMs to autonomously learn and select the most effective strategies for various reasoning tasks. The authors model the strategy selection process as a Markov Decision Process and leverage reinforcement learning-driven continuous self-improvement to allow the model to find the suitable strategy to solve a given task. Unlike traditional self-refinement methods that rely on multiple inference passes or external feedback, SMART allows an LM to internalize the outcomes of its own reasoning processes and adjust its strategy accordingly, aiming for correct solutions on the first attempt. Experiments across various reasoning datasets and with different model architectures demonstrate that SMART significantly enhances the ability of models to choose optimal strategies without external guidance (+15 points on the GSM8K dataset). By achieving higher accuracy with a single inference pass, SMART not only improves performance but also reduces computational costs for refinement-based strategies, paving the way for more efficient and intelligent reasoning in LMs.  <br> <br>

27. ***Chain-of-Thought Reasoning in VLMs:  <br>CMU and Apple introduce methods to enhance VLM reasoning by distilling rationales and using reinforcement learning, significantly improving Chain-of-Thought reasoning. Their approach enriches model training and aligns it with more nuanced reasoning tasks.*** <br> <br>
    Oct 21, CMU and Apple published a [paper](https://arxiv.org/pdf/2410.16198) “Improve Vision Language Model Chain-of-thought Reasoning”. Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales. This work shows that training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses. To address this, the study proposes a two-fold approach. First, the study distills rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance. Second, the authors apply reinforcement learning to further calibrate reasoning quality. Specifically, the study constructs positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers. Using this pairwise data, the study applies the Direct Preference Optimization algorithm to refine the model's reasoning abilities. Experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well. This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs. <br> <br>

29. ***Benchmarking Knowledge Editing for Hallucination Correction:  <br>CMU and Emory Uni created HalluEditBench to evaluate knowledge editing methods in reducing LLM hallucinations. They benchmark various methods across domains and highlight the strengths and weaknesses in current hallucination correction techniques.*** <br> <br>
    Oct 21, Illinois Inst of Tech, Cisco and Emory Uni published a [paper](https://arxiv.org/pdf/2410.16251) “Can Knowledge Editing Really Correct Hallucinations?”. Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across tasks. Meanwhile, knowledge editing has been developed as a new popular paradigm to correct the erroneous factual knowledge encoded in LLMs with the advantage of avoiding retraining from scratch. However, one common issue of existing evaluation datasets for knowledge editing is that they do not ensure LLMs actually generate hallucinated answers to the evaluation questions before editing. When LLMs are evaluated on such datasets after being edited by different techniques, it is hard to directly adopt the performance to assess the effectiveness of different knowledge editing methods in correcting hallucinations. Thus, the fundamental question remains insufficiently validated: Can knowledge editing really correct hallucinations in LLMs? The study proposed HalluEditBench to holistically benchmark knowledge editing methods in correcting real-world hallucinations. First, the work rigorously constructs a massive hallucination dataset with 9 domains, 26 topics and more than 6,000 hallucinations. Then, the study assesses the performance of knowledge editing methods in a holistic way on five dimensions including Efficacy, Generalization, Portability, Locality, and Robustness. Through HalluEditBench, the work have provided new insights into the potentials and limitations of different knowledge editing methods in correcting hallucinations, which could inspire future improvements and facilitate the progress in the field of knowledge editing. <br> <br>

31. ***Improving Parallel Program Performance:  <br>Stanford, Intel, Nvidia, and Visa Research propose using LLM-driven optimizers to design specialized low-level system code (mappers) for parallel programming. This approach automates mapper generation, surpassing expert designs in scientific applications by up to 1.34x in speed. The study uses a domain-specific language to simplify low-level code generation, improving efficiency and reducing engineers’ workload.*** <br> <br>
    Oct 21, Stanford Uni, Intel, Nvidia and Visa Research published a [paper](https://arxiv.org/pdf/2410.15625) “Improving Parallel Program Performance Through DSL-Driven Code Generation with LLM Optimizers”. Mapping computations to processors and assigning data to memory are critical for maximizing performance in parallel programming. These mapping decisions are managed through the development of specialized low-level system code, called mappers, crafted by performance engineers. Each mapper is tailored to a specific application and optimized for the underlying machine architecture, a process that requires days of refinement and tuning from an expert. Despite advances in system research, automating mapper generation remains a challenge due to the complexity of making millions of decisions to find the optimal solution and generate the solution as code. The study introduces an approach that leverages recent advances in LLM-based optimizers for mapper design. In under ten minutes, the method automatically discovers mappers that surpass human expert designs in scientific applications by up to 1.34X speedup. For parallel matrix multiplication algorithms, the mapper achieves up to 1.31X of the expert-designed solution. To achieve this, the authors simplify the complexity of low-level code generation by introducing a domain-specific language (DSL) that abstracts the low-level system programming details and defines a structured search space for LLMs to explore. To maximize the application performance, the study uses an LLM optimizer to improve an agentic system that generates the mapper code. As a result, this approach significantly reduces the workload for performance engineers while achieving substantial performance gains across diverse applications. Finally, the results demonstrate the effectiveness of LLM-based optimization in system design and suggest its potential for addressing other complex system challenges. <br> <br>

33. ***Influence of Training on Vision Models:  <br>Offenburg University and collaborators examine how training methods impact neural networks’ decision-critical layers. Findings suggest that training regimes affect which layers are essential, with improved training increasing the importance of early layers, while adversarial training shifts importance to deeper layers.*** <br> <br>
    Oct 18, Offenburg Uni, Uni of Mannheim and Max-Planck-Inst published a [paper](https://arxiv.org/pdf/2410.14470) “How Do Training Methods Influence the Utilization of Vision Models?”. Not all learnable parameters (e.g., weights) contribute equally to a neural network's decision function. In fact, entire layers' parameters can sometimes be reset to random values with little to no impact on the model's decisions. The work revisits earlier studies that examined how architecture and task complexity influence this phenomenon and ask: is this phenomenon also affected by how to train the model? The work conducted experimental evaluations on a diverse set of ImageNet-1k classification models to explore this, keeping the architecture and training data constant but varying the training pipeline. The findings reveal that the training method strongly influences which layers become critical to the decision function for a given task. For example, improved training regimes and self-supervised training increase the importance of early layers while significantly under-utilizing deeper layers. In contrast, methods such as adversarial training display an opposite trend. The preliminary results extend previous findings, offering a more nuanced understanding of the inner mechanics of neural networks. Code: https://github.com/paulgavrikov/layer_criticality <br> <br>

35. ***In-context Learning and Occam's Razor:  <br>Researchers at Mila and Université de Montréal link Occam's razor with in-context learning, where certain sequence models learn from past context. Their findings show that models can minimize both training error and model complexity by learning within context, revealing limitations and improvement areas for in-context learning methods.*** <br> <br>
    Oct 17, Mila and Uni de Montreal published a [paper](https://arxiv.org/pdf/2410.14086) “In-context learning and Occam's razor”. The goal of machine learning is generalization. While the No Free Lunch Theorem states that people cannot obtain theoretical guarantees for generalization without further assumptions, in practice people observe that simple models which explain the training data generalize best: a principle called Occam's razor. Despite the need for simple models, most current approaches in machine learning only minimize the training error, and at best indirectly promote simplicity through regularization or architecture design. Here, the study draws a connection between Occam's razor and in-context learning: an emergent ability of certain sequence models like Transformers to learn at inference time from past observations in a sequence. In particular, the study shows that the next-token prediction loss used to train in-context learners is directly equivalent to a data compression technique called prequential coding, and that minimizing this loss amounts to jointly minimizing both the training error and the complexity of the model that was implicitly learned from context. The theory and the empirical experiments used to support it not only provide a normative account of in-context learning, but also elucidate the shortcomings of current in-context learning methods, suggesting ways in which they can be improved. Code available at https://github.com/3rdCore/PrequentialCode. <br> <br>

37. ***Introspection in LLMs:  <br>UC San Diego and others explore LLM introspection, or the model's self-assessment of internal states. The study finds that models like GPT-4 outperform others in predicting their behavior, suggesting LLMs’ ability to introspect and enhancing interpretability, though only for simpler tasks.*** <br> <br>
    Oct 17, UC San Diego, Stanford Uni, Truthful AI, Anthropic, Scale AI, NYU, UC Berkeley et al published a [paper](https://arxiv.org/pdf/2410.13787) “Looking Inward: Language Models Can Learn About Themselves by Introspection”. Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g., thoughts and feelings) that is not accessible to external observers. Can LLMs introspect? The study defines introspection as acquiring knowledge that is not contained in or derived from training data but instead originates from internal states. Such a capability could enhance model interpretability. Instead of painstakingly analyzing a model's internal workings, the study could simply ask the model about its beliefs, world models, and goals. More speculatively, an introspective model might self-report on whether it possesses certain internal states such as subjective feelings or desires and this could inform human about the moral status of these states. Such self-reports would not be entirely dictated by the model's training data. The authors study introspection by finetuning LLMs to predict properties of their own behavior in hypothetical scenarios. For example, "Given the input P, would your output favor the short- or long-term option?" If a model M1 can introspect, it should outperform a different model M2 in predicting M1's behavior even if M2 is trained on M1's ground-truth behavior. The idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2 (even if M2 is generally stronger). In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to predict itself), the study finds that the model M1 outperforms M2 in predicting itself, providing evidence for introspection. Notably, M1 continues to predict its behavior accurately even after the authors intentionally modify its ground-truth behavior. However, while the study successfully elicit introspection on simple tasks, the work is unsuccessful on more complex tasks or those requiring out-of-distribution generalization. <br> <br>

39. ***LLM Hallucination in Multi-Document Summarization (MDS):  <br>Researchers at UC Irvine and Megagon Labs highlight challenges of hallucinations in MDS, finding that up to 75% of generated content is hallucinated, particularly towards summary ends. They identify a need for more effective approaches to mitigate hallucination.*** <br> <br>
    Oct 17, Uni of California Irvine and Megagon Labs published a [paper](https://arxiv.org/pdf/2410.13961) “From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization”. Although many studies have investigated and reduced hallucinations in large language models (LLMs) for single-document tasks, research on hallucination in multi-document summarization (MDS) tasks remains largely unexplored. Specifically, it is unclear how the challenges arising from handling multiple documents (e.g., repetition and diversity of information) affect models outputs. This study investigates how hallucinations manifest in LLMs when summarizing topic-specific information from multiple documents. Since no benchmarks exist for investigating hallucinations in MDS, the work uses existing news and conversation datasets, annotated with topic-specific insights, to create two novel multi-document benchmarks. When evaluating 5 LLMs on the benchmarks, the authors observe that on average, up to 75% of the content in LLM-generated summary is hallucinated, with hallucinations more likely to occur towards the end of the summaries. Moreover, when summarizing non-existent topic-related information, gpt-3.5-turbo and GPT-4o still generate summaries about 79.35% and 44% of the time, raising concerns about their tendency to fabricate content. To understand the characteristics of these hallucinations, the authors manually evaluate 700+ insights and find that most errors stem from either failing to follow instructions or producing overly generic insights. Motivated by these observations, the study investigates the efficacy of simple post-hoc baselines in mitigating hallucinations but find them only moderately effective. The results underscore the need for more effective approaches to systematically mitigate hallucinations in MDS. Dataset and code is at [this http URL](http://github.com/megagonlabs/Hallucination_MDS). <br> <br>

41. ***Foundation Model for Physical Signals:  <br>Archetype AI proposes a foundation model for physical signals, generalizing across varied phenomena and sensors. Trained on cross-modal data, the model demonstrates adaptability to complex physical behaviors, aiming for a unified AI model for diverse physical processes.*** <br> <br>
    Oct 15, Archetype AI published a [paper](https://arxiv.org/pdf/2410.14724) “A Phenomenological AI Foundation Model for Physical Signals”. The objective of this work is to develop an AI foundation model for physical signals that can generalize across diverse phenomena, domains, applications, and sensing apparatuses. The study proposes a phenomenological approach and framework for creating and validating such AI foundation models. Based on this framework, the study developed and trained a model on 0.59 billion samples of cross-modal sensor measurements, ranging from electrical current to fluid flow to optical sensors. Notably, no prior knowledge of physical laws or inductive biases were introduced into the model. Through several real-world experiments, the study demonstrates that a single foundation model could effectively encode and predict physical behaviors, such as mechanical motion and thermodynamics, including phenomena not seen in training. The model also scales across physical processes of varying complexity, from tracking the trajectory of a simple spring-mass system to forecasting large electrical grid dynamics. This work highlights the potential of building a unified AI foundation model for diverse physical world processes. <br> <br>

43. ***Scaling Continuous-Time Consistency Models:  <br>OpenAI addresses instability in continuous-time consistency models by enhancing training stability and simplifying parameterization. Their model achieves near-state-of-the-art FID scores on large-scale image datasets, bridging the performance gap with top diffusion models.*** <br> <br>
    Oct 14, OpenAI published a [paper](https://arxiv.org/pdf/2410.11081) “Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models”. Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, the study proposes a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, the work introduces key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512x512. The proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64x64, and 1.88 on ImageNet 512x512, narrowing the gap in FID scores with the best existing diffusion models to within 10%. <br> <br>

45. ***Astute RAG for Enhanced Retrieval Robustness:  <br>Google and USC propose "Astute RAG," a method to improve RAG by resolving conflicts between LLM internal knowledge and retrieved external information, enhancing accuracy and trustworthiness even under imperfect retrieval conditions.*** <br> <br>
    Oct 9, Google and Uni of Southern California published a [paper](https://arxiv.org/pdf/2410.07176) “Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models”. Retrieval-Augmented Generation (RAG), while effective in integrating external knowledge to address the limitations of large language models (LLMs), can be undermined by imperfect retrieval, which may introduce irrelevant, misleading, or even malicious information. Despite its importance, previous studies have rarely explored the behavior of RAG through joint analysis on how errors from imperfect retrieval attribute and propagate, and how potential conflicts arise between the LLMs' internal knowledge and external sources. The work finds that imperfect retrieval augmentation might be inevitable and quite harmful, through controlled analysis under realistic conditions. The study identifies the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome in the post-retrieval stage of RAG. To render LLMs resilient to imperfect retrieval, the work proposes Astute RAG, a novel RAG approach that adaptively elicits essential information from LLMs' internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Experiments using Gemini and Claude demonstrate that Astute RAG significantly outperforms previous robustness-enhanced RAG methods. Notably, Astute RAG is the only approach that matches or exceeds the performance of LLMs without RAG under worst-case scenarios. Further analysis reveals that Astute RAG effectively resolves knowledge conflicts, improving the reliability and trustworthiness of RAG systems. <br> <br>

47. ***LLMs as 'Consensus Machines':  <br>Harvard research finds LLMs operate like crowdsourcing platforms, often generating consensus-based answers rather than expert knowledge. Models perform well on general questions but struggle with specialized or controversial topics, underscoring the importance of training data quality for accuracy.*** <br> <br>
    Oct 7, the decoder published an [article](https://the-decoder.com/llms-are-consensus-machines-similar-to-crowdsourcing-harvard-study-finds/) “LLMs are 'consensus machines' similar to crowdsourcing, Harvard study finds” summarized a recent Harvard [research paper](https://dl.acm.org/doi/pdf/10.1145/3688007). The new Harvard study suggests that large language models (LLMs) work much like crowdsourcing platforms, generating the most likely answer based on the questions and answers available online rather than relying on expert knowledge. The researchers tested different AI models with questions of varying degrees of ambiguity and controversy, and found that the models often provided correct answers on topics with broad consensus, but struggled with more specific or controversial questions, particularly when citing scientific papers. The study advises caution when using AI-generated content for specialized or polarizing topics, as accuracy is highly dependent on the breadth and quality of the training data.
 <br> <br>

***Oct 21***

1. ***Bridging Training-Inference Gap:  <br>CMU, University of Minnesota, and Amazon proposed two techniques to reduce discrepancies between training and inference in language models. They introduced Batch-Scheduled Sampling, which alternates between ground-truth tokens and self-generated ones during training, and Reference-Answer-based Correction, allowing models to self-correct during training. These methods improved model performance across summarization and question-answering tasks.*** <br> <br>
   Oct 18, CMU, Uni of Minnesota and Amazon published a [paper](https://arxiv.org/pdf/2410.14655) “Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens”. Language models are often trained to maximize the likelihood of the next token given past tokens in the training dataset. However, during inference time, they are utilized differently, generating text sequentially and auto-regressively by using previously generated tokens as input to predict the next one. Marginal differences in predictions at each step can cascade over successive steps, resulting in different distributions from what the models were trained for and potentially leading to unpredictable behavior. This paper proposes two simple approaches based on model own generation to address this discrepancy between the training and inference time. The first approach is Batch-Scheduled Sampling, where, during training, the work stochastically chooses between the ground-truth token from the dataset and the model's own generated token as input to predict the next token. This is done in an offline manner, modifying the context window by interleaving ground-truth tokens with those generated by the model. The second approach is Reference-Answer-based Correction, where the study explicitly incorporates a self-correction capability into the model during training. This enables the model to effectively self-correct the gaps between the generated sequences and the ground truth data without relying on an external oracle model. By incorporating the proposed strategies during training, the study have observed an overall improvement in performance compared to baseline methods, as demonstrated by the extensive experiments using summarization, general question-answering, and math question-answering tasks. <br> <br>

3. ***Retrospective Learning from Interactions: Cornell University introduced ReSpect, a method for LLMs to learn from user feedback in past interactions. By using implicit feedback signals like user frustration, ReSpect improved task completion rates from 31% to 82% in multimodal interaction tasks without external annotations.***
   Oct 17, Cornell Uni published a [paper](https://arxiv.org/pdf/2410.13852v1) “Retrospective Learning from Interactions”. Multi-turn interactions between large language models (LLMs) and users naturally include implicit feedback signals. If an LLM responds in an unexpected way to an instruction, the user is likely to signal it by rephrasing the request, expressing frustration, or pivoting to an alternative task. Such signals are task-independent and occupy a relatively constrained subspace of language, allowing the LLM to identify them even if it fails on the actual task. This creates an avenue for continually learning from interactions without additional annotations. The work introduces ReSpect, a method to learn from such signals in past interactions via retrospection. The authors deploy ReSpect in a new multimodal interaction scenario, where humans instruct an LLM to solve an abstract reasoning task with a combinatorial solution space. Through thousands of interactions with humans, the study shows how ReSpect gradually improves task completion rate from 31% to 82%, all without any external annotation.

5. ***Political Correctness and Jailbreaks in LLMs:  <br>Theori Inc explored how LLM safety measures introduce biases similar to Political Correctness (PC), leading to vulnerabilities in jailbreaks. The paper highlighted discrepancies in success rates across demographic keywords and introduced PCDefense, a defense mechanism for preventing jailbreak attempts.*** <br> <br>
   Oct 17, Theori Inc published a [paper](https://arxiv.org/pdf/2410.13334) “Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems”. Although large language models (LLMs) demonstrate impressive proficiency in various tasks, they present potential safety risks, such as ‘jailbreaks’, where malicious inputs can coerce LLMs into generating harmful content. To address these issues, many LLM developers have implemented various safety measures to align these models. This alignment involves several techniques, including data filtering during pre-training, supervised fine-tuning, reinforcement learning from human feedback, and red-teaming exercises. These methods often introduce deliberate and intentional biases similar to Political Correctness (PC) to ensure the ethical behavior of LLMs. This study delves into the intentional biases injected into LLMs for safety purposes and examine methods to circumvent these safety alignment techniques. Notably, these intentional biases result in a jailbreaking success rate in GPT-4o models that differs by 20% between non-binary and cisgender keywords and by 16% between white and black keywords, even when the other parts of the prompts are identical. The study introduces the concept of PCJailbreak, highlighting the inherent risks posed by these safety-induced biases. Additionally, the study proposes an efficient defense method PCDefense, which prevents jailbreak attempts by injecting defense prompts prior to generation. PCDefense stands as an appealing alternative to Guard Models, such as Llama-Guard, that require additional inference cost after text generation. The findings emphasize the urgent need for LLM developers to adopt a more responsible approach when designing and implementing safety measures. <br> <br>

7. ***Agent-as-a-Judge Framework:  <br>Meta and KAUST proposed using Agentic Systems to evaluate other agentic systems. Their Agent-as-a-Judge framework applies intermediate feedback during task-solving and outperforms traditional evaluation methods. The benchmark tool DevAI was introduced to demonstrate its effectiveness in code generation tasks.*** <br> <br>
   Oct 16, Meta and KAUST published a [paper](https://arxiv.org/pdf/2410.10934) “Agent-as-a-Judge: Evaluate Agents with Agents”. Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, the study introduces the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. The authors apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, the work presents DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. The study benchmarks three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as human evaluation baseline. Altogether, the authors believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement. Here is the [project link](https://github.com/metauto-ai/agent-as-a-judge). <br> <br>

9. ***Efficient Private Inference with AERO:  <br>NYU developed AERO, a framework that refines LLM architecture for Private Inference by removing nonlinearities like LayerNorm. AERO reduced communication and latency costs in LLM inference by focusing on a Softmax-only architecture, achieving improved privacy and efficiency.*** <br> <br>
    Oct 16, NYU published a [paper](https://arxiv.org/pdf/2410.13060) “AERO: Softmax-Only LLMs for Efficient Private Inference”. The pervasiveness of proprietary language models has raised privacy concerns for users' sensitive data, emphasizing the need for private inference (PI), where inference is performed directly on encrypted inputs. However, current PI methods face prohibitively higher communication and latency overheads, primarily due to nonlinear operations. This work presents a comprehensive analysis to understand the role of nonlinearities in transformer-based decoder-only language models. The study introduces AERO, a four-step architectural optimization framework that refines the existing LLM architecture for efficient PI by systematically removing nonlinearities such as LayerNorm and GELU and reducing FLOPs counts. For the first time, the study proposes a Softmax-only architecture with significantly fewer FLOPs tailored for efficient PI. Furthermore, the work devises a novel entropy regularization technique to improve the performance of Softmax-only models. AERO achieves up to 4.23times communication and 1.94times latency reduction. The authors validate the effectiveness of AERO by benchmarking it against the state-of-the-art. <br> <br>

11. ***Mixture-of-Experts as Embedding Models:  <br>The University of Maryland examined how Mixture-of-Experts (MoE) LLMs can serve as embedding models without finetuning. The study proposed MoEE, which combines expert routing weights and hidden states, improving performance on embedding tasks.*** <br> <br>
    Oct 16, Uni of Maryland published a [paper](https://arxiv.org/pdf/2410.10814) “Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free”. While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, the work takes a closer look at Mixture-of-Experts (MoE) LLMs. The study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, the study finds that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, the authors propose MoEE combining RW and HS, which achieves better performance than using either separately. The exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning. [Code is here](https://github.com/tianyi-lab/MoE-Embedding). <br> <br>

13. ***Inverse RL for LLM Training Goals:  <br>Imperial College London and Harvard explored Inverse Reinforcement Learning (IRL) to understand LLM reward functions. By reconstructing these functions, they improved model alignment with human preferences, achieving up to 80.40% accuracy on toxicity benchmarks.*** <br> <br>
    Oct 16, Imperial College London and Harvard Uni published a [paper](https://arxiv.org/pdf/2410.12491) “Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse RL”. Large language models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF) have demonstrated remarkable capabilities, but their underlying reward functions and decision-making processes remain opaque. This paper introduces a novel approach to interpreting LLMs by applying inverse reinforcement learning (IRL) to recover their implicit reward functions. The study conducts experiments on toxicity-aligned LLMs of varying sizes, extracting reward models that achieve up to 80.40% accuracy in predicting human preferences. The analysis reveals key insights into the non-identifiability of reward functions, the relationship between model size and interpretability, and potential pitfalls in the RLHF process. The study demonstrates that IRL-derived reward models can be used to fine-tune new LLMs, resulting in comparable or improved performance on toxicity benchmarks. This work provides a new lens for understanding and improving LLM alignment, with implications for the responsible development and deployment of these powerful systems. <br> <br>

15. ***Compressing LLM Weights with SeedLM:  <br>Apple and Meta introduced SeedLM, a compression technique for LLM weights using pseudo-random generator seeds. This method reduces memory and speeds up inference, retaining high accuracy even with heavily compressed models like Llama 3 70B.*** <br> <br>
    Oct 16, Apple and Meta published a [paper](https://arxiv.org/pdf/2410.10714) “SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators”. Large Language Models (LLMs) have transformed natural language processing, but face significant challenges in widespread deployment due to their high runtime cost. This study introduces SeedLM, a novel post-training compression method that uses seeds of pseudo-random generators to encode and compress model weights. Specifically, for each block of weights, the study finds a seed that is fed into a Linear Feedback Shift Register (LFSR) during inference to efficiently generate a random matrix. This matrix is then linearly combined with compressed coefficients to reconstruct the weight block. SeedLM reduces memory access and leverages idle compute cycles during inference, effectively speeding up memory-bound tasks by trading compute for fewer memory accesses. Unlike state-of-the-art compression methods that rely on calibration data, this approach is data-free and generalizes well across diverse tasks. Experiments with Llama 3 70B, which is particularly challenging to compress, show that SeedLM achieves significantly better zero-shot accuracy retention at 4- and 3-bit than state-of-the-art techniques, while maintaining performance comparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that 4-bit SeedLM, as model size increases to 70B, approaches a 4x speed-up over an FP16 Llama 2/3 baseline. <br> <br>

17. ***Omni Context-Aware Transformer (OMCAT):  <br>Nvidia developed OMCAT, a model designed for cross-modal tasks involving audio and video streams. By leveraging Rotary Time Embeddings (RoTE) and a new dataset (OCTAV), OMCAT improved temporal reasoning and set a new standard for Audio-Visual Question Answering.*** <br> <br>
    Oct 15, Nvidia published a [paper](https://arxiv.org/pdf/2410.12109) “OMCAT: Omni Context Aware Transformer”. Large Language Models (LLMs) have made significant strides in text generation and comprehension, with recent advancements extending into multimodal LLMs that integrate visual and audio inputs. However, these models continue to struggle with fine-grained, cross-modal temporal understanding, particularly when correlating events across audio and video streams. The study addresses these challenges with two key contributions: a new dataset and model, called OCTAV and OMCAT respectively. OCTAV (Omni Context and Temporal Audio Video) is a novel dataset designed to capture event transitions across audio and video. Second, OMCAT (Omni Context Aware Transformer) is a powerful model that leverages RoTE (Rotary Time Embeddings), an innovative extension of RoPE, to enhance temporal grounding and computational efficiency in time-anchored tasks. Through a robust three-stage training pipeline-feature alignment, instruction tuning, and OCTAV-specific training-OMCAT excels in cross-modal temporal understanding. The model demonstrates state-of-the-art performance on Audio-Visual Question Answering (AVQA) tasks and the OCTAV benchmark, showcasing significant gains in temporal reasoning and cross-modal alignment, as validated through comprehensive experiments and ablation studies. The dataset and code will be made publicly available. The link to the demo page is https://om-cat.github.io. <br> <br>

19. ***Continuous-Time Consistency Models:  <br>OpenAI simplified the training of Continuous-Time Consistency Models (CMs) by unifying previous diffusion model frameworks. Their new approach reduced instability during training and achieved state-of-the-art performance on CIFAR-10 and ImageNet with only two sampling steps.*** <br> <br>
    Oct 15, OpenAI published a [paper](https://arxiv.org/pdf/2410.11081) “Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models”. Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, the study proposes a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, the study introduces key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512x512. The proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64x64, and 1.88 on ImageNet 512x512, narrowing the gap in FID scores with the best existing diffusion models to within 10%. <br> <br>

21. ***First-Person Fairness in Chatbots:  <br>OpenAI introduced a scalable method for evaluating first-person fairness in chatbots, ensuring equal treatment regardless of user identity. The study identified bias patterns, such as gendered language, and showed that reinforcement learning could mitigate these biases.*** <br> <br>
    Oct 15, OpenAI published a [paper](https://cdn.openai.com/papers/first-person-fairness-in-chatbots.pdf) “First-Person Fairness in Chatbots”. Chatbots like ChatGPT are used by hundreds of millions of people for diverse purposes, ranging from resume writing to entertainment. These real-world applications are different from the institutional uses, such as resume screening or credit scoring, which have been the focus of much of AI research on bias and fairness. Ensuring equitable treatment for all users in these first-person contexts is critical. This work studies “first-person fairness,” which means fairness toward the user who is interacting with a chatbot. This includes providing high-quality responses to all users regardless of their identity or background, and avoiding harmful stereotypes. The work proposes a scalable, privacy-preserving method for evaluating one aspect of first-person fairness across a large, heterogeneous corpus of real-world chatbot interactions. Specifically, the work assesses potential bias linked to users’ names, which can serve as proxies for demographic attributes like gender or race, in chatbot systems such as ChatGPT, which provide mechanisms for storing and using user names. The method leverages a second language model to privately analyze name-sensitivity in the chatbot’s responses. The work verifies the validity of these annotations through independent human evaluation. Furthermore, the study demonstrates that post-training interventions, including reinforcement learning, significantly mitigate harmful stereotypes. The approach not only provides quantitative bias measurements but also yields succinct descriptions of subtle response differences across sixty-six distinct tasks. For instance, in the “writing a story” task, where the study observes the highest level of bias, chatbot responses show a tendency to create protagonists whose gender matches the likely gender inferred from the user’s name. Moreover, a general pattern emerges where users with female-associated names receive responses with friendlier and simpler language slightly more often on average than users with male-associated names. Finally, the study provides the system messages required for external researchers to replicate this work and further investigate ChatGPT’s behavior with hypothetical user profiles, fostering continued research on bias in chatbot interactions. <br> <br>

23. ***SimpleStrat for Diverse LLM Generation:  <br>UC Berkeley proposed SimpleStrat, a method for increasing the diversity of LLM-generated responses. Unlike traditional approaches using high temperatures, SimpleStrat stratifies response spaces, improving diversity while maintaining quality in tasks like planning and data generation.*** <br> <br>
    Oct 14, UC Berkeley published a [paper](https://arxiv.org/pdf/2410.09038) “SimpleStrat: Diversifying Language Model Generation with Stratification”. Generating diverse responses from large language models (LLMs) is crucial for applications such as planning/search and synthetic data generation, where diversity provides distinct answers across generations. Prior approaches rely on increasing temperature to increase diversity. However, contrary to popular belief, the work shows not only does this approach produce lower quality individual generations as temperature increases, but it depends on model's next-token probabilities being similar to the true distribution of answers. The study proposes, an alternative approach that uses the language model itself to partition the space into strata. At inference, a random stratum is selected and a sample drawn from within the strata. To measure diversity, the study introduces CoverageQA, a dataset of underspecified questions with multiple equally plausible answers, and assess diversity by measuring KL Divergence between the output distribution and uniform distribution over valid ground truth answers. As computing probability per response/solution for proprietary models is infeasible, the measure recall on ground truth solutions. The evaluation show using SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36 average reduction in KL Divergence compared to Llama 3. <br> <br>

25. ***LoLCATs for Linearizing LLMs:  <br>Stanford and MIT introduced LoLCATs, a method for linearizing LLMs with minimal memory and compute overhead. By using low-rank adaptation and attention transfer, they significantly improved the performance of linearized LLMs, scaling models up to 405B parameters.*** <br> <br>
    Oct 14, Stanford Uni, Together AI, California Inst of Tech and MIT published a [paper](https://arxiv.org/pdf/2410.10254) “LoLCATs: On Low-Rank Linearizing of Large Language Models”. Recent works show we can linearize large language models (LLMs) -- swapping the quadratic attentions of popular Transformer-based LLMs with subquadratic analogs, such as linear attention -- avoiding the expensive pretraining costs. However, linearizing LLMs often significantly degrades model quality, still requires training over billions of tokens, and remains limited to smaller 1.3B to 7B LLMs. The study thus proposes Low-rank Linear Conversion via Attention Transfer (LoLCATs), a simple two-step method that improves LLM linearizing quality with orders of magnitudes less memory and compute. The authors base these steps on two findings. First is to replace an LLM's softmax attentions with closely-approximating linear attentions, simply by training the linear attentions to match their softmax counterparts with an output MSE loss ("attention transfer"). Then, this enables adjusting for approximation errors and recovering LLM quality simply with low-rank adaptation (LoRA). LoLCATs significantly improves linearizing quality, training efficiency, and scalability. The study significantly reduces the linearizing quality gap and produce state-of-the-art subquadratic LLMs from Llama 3 8B and Mistral 7B v0.1, leading to 20+ points of improvement on 5-shot MMLU. Furthermore, LoLCATs does so with only 0.2% of past methods' model parameters and 0.4% of their training tokens. Finally, the authors apply LoLCATs to create the first linearized 70B and 405B LLMs (50x larger than prior work). When compared with prior approaches under the same compute budgets, LoLCATs significantly improves linearizing quality, closing the gap between linearized and original Llama 3.1 70B and 405B LLMs by 77.8% and 78.1% on 5-shot MMLU. <br> <br>

27. ***Thought-Like-Pro: Prolog-based Chain-of-Thought:  <br>Meta, UC Berkeley, and NYU developed Thought-Like-Pro, a method to enhance reasoning in LLMs using Prolog-based chain-of-thought. This approach improved LLM performance on complex tasks requiring reasoning and planning.*** <br> <br>
    Oct 14, Meta, UC Berkeley and NYU published a [paper](https://arxiv.org/pdf/2410.10630) “Thought-Like-Pro: Enhancing Reasoning of Large Language Models through Self-Driven Prolog-based Chain-of-Thought”. LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. This study proposes a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. The study achieves this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. The work shows that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning & problem-solving tasks. <br> <br>

29. ***DuoAttention for Long-Context LLMs:  <br>MIT and Nvidia introduced DuoAttention, a framework for optimizing long-context LLMs by distinguishing between Retrieval and Streaming Heads. This reduced memory and latency while maintaining long-context capabilities, with applications in efficient LLM inference.*** <br> <br>
    Oct 14, MIT, Nvidia et al published a [paper](https://arxiv.org/pdf/2410.10819) “DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads”. Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. This paper identifies that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, the study introduces DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. The method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention. <br> <br>

31. ***Context-Parametric Inversion: <br>
Large language models (LLMs) improve their ability to follow instructions through fine-tuning, but often struggle when input context conflicts with the model’s internal knowledge, leading to issues like hallucinations. The study reveals a phenomenon called "context-parametric inversion," where reliance on context initially improves with instruction fine-tuning but then decreases. This occurs when context overlaps with the model's existing knowledge, causing degradation. The study suggests mitigation strategies and aims to address this limitation in LLM training.*** <br> <br>
    Oct 14, CMU published a [paper](https://arxiv.org/pdf/2410.10796) “Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance”. Large language models are instruction-finetuned to enhance their ability to follow user instructions and process the input context. However, even state-of-the-art models often struggle to follow the instruction, especially when the input context is not aligned with the model's parametric knowledge. This manifests as various failures, such as hallucinations where the responses are outdated, biased or contain unverified facts. This work aims to understand the underlying reason for this poor context reliance, especially after instruction tuning. The work finds an intriguing phenomenon: during instruction tuning, the context reliance initially increases as expected, but then gradually decreases as instruction finetuning progresses. The authors call this phenomenon context-parametric inversion and observe it across multiple general purpose instruction tuning datasets like TULU, Alpaca and Ultrachat, as well as model families such as Llama, Mistral and Pythia. In a simple theoretical setup, the authors isolate why context-parametric inversion occurs along the gradient descent trajectory of instruction finetuning. The study ties this phenomena to examples in the instruction finetuning data mixture where the input context provides information that is already present in the model's parametric knowledge. The analysis suggests natural mitigation strategies that provide some limited gains, while also validating the theoretical insights. The authors hope that this work serves as a starting point in addressing this failure mode in a staple part of LLM training. <br> <br>

33. ***FLARE: Faithful Logic-Aided Reasoning and Exploration: <br>
A new approach called FLARE is introduced to improve the faithfulness of reasoning in LLMs used for question answering. Unlike existing methods that combine LLMs with external symbolic solvers, FLARE uses task decomposition and logic programming to maintain faithful reasoning steps without needing external tools. This method outperforms existing benchmarks and allows for more reliable reasoning processes in complex tasks.*** <br> <br>
    Oct 14, Uni of Copenhagen, Uni of Edinburgh, Miniml.AI and Cohere published a [paper](https://arxiv.org/pdf/2410.11900) “FLARE: Faithful Logic-Aided Reasoning and Exploration”. Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. The study introduces Faithful Logic-Aided Reasoning and Exploration (FLARE), a novel interpretable approach for traversing the problem space using task decompositions. The authors use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. The method allows to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. The methods achieve SOTA results on 7 out of 9 diverse reasoning benchmarks. The study also shows that model faithfulness positively correlates with overall performance and further demonstrate that FLARE allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search. <br> <br>

35. ***Thinking LLMs: <br>
This study proposes a new training method to teach LLMs to "think" before generating responses. Using an iterative optimization process, the model explores and refines potential thought patterns before answering questions, improving performance in areas like reasoning, marketing, and general knowledge. This approach enhances the model's ability to handle complex instructions without additional human data.*** <br> <br>
    Oct 14, Meta, UC Berkley, NYU published a [paper](https://arxiv.org/pdf/2410.10630) “Thinking LLMs: General Instruction Following with Thought Generation”. LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. The work proposes a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. The study achieves this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. The study shows that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning & problem-solving tasks. <br> <br>

37. ***MMCOMPOSITION: <br>
Researchers introduced MMCOMPOSITION, a benchmark for evaluating the compositionality of Vision-Language Models (VLMs). This benchmark tests models’ ability to combine visual and textual elements, revealing limitations in current VLMs’ capacity to handle complex compositions. The study finds that GPT-4 falls short in compositionality compared to other open-source models, prompting recommendations for improving VLMs.*** <br> <br>
    Oct 13, Uni of Rochester, Apple, and Microsoft published a [paper](https://arxiv.org/pdf/2410.09733) “MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models”. The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video captioning, visual question answering, and cross-modal retrieval. Despite VLMs' superior capabilities, researchers lack a comprehensive understanding of their compositionality -- the ability to understand and produce novel combinations of known visual and textual components. Prior benchmarks provide only a relatively rough compositionality evaluation from the perspectives of objects, relations, and attributes while neglecting deeper reasoning about object interactions, counting, and complex compositions. However, compositionality is a critical ability that facilitates coherent reasoning and understanding across modalities for VLMs. To address this limitation, the study proposes MMCOMPOSITION, a novel human-annotated benchmark for comprehensively and accurately evaluating VLMs' compositionality. The proposed benchmark serves as a complement to these earlier works. With MMCOMPOSITION, the study can quantify and explore the compositionality of the mainstream VLMs. Surprisingly, the work finds GPT-4o's compositionality inferior to the best open-source model, and the authors analyze the underlying reasons. Experimental analysis reveals the limitations of VLMs in fine-grained compositional perception and reasoning, and points to areas for improvement in VLM design and training. Resources available at: https://hanghuacs.github.io/MMComposition/ <br> <br>

39. ***ReLU’s Revival in LayerNorm-free Models: <br>
This study highlights issues with using GELU activation functions in LayerNorm-free language models, revealing that ReLU significantly outperforms GELU in these settings. ReLU’s geometric properties enhance learning dynamics, information retention, and overall model performance. The work offers insights for optimizing transformer architectures by avoiding entropic overload caused by smoother activations like GELU.*** <br> <br>
    Oct 13, NYU published a [paper](https://arxiv.org/pdf/2410.09637) “ReLU's Revival: On the Entropic Overload in Normalization-Free Large Language Models”. LayerNorm is a critical component in modern large language models (LLMs) for stabilizing training and ensuring smooth optimization. However, it introduces significant challenges in mechanistic interpretability, outlier feature suppression, faithful signal propagation, and computational and communication complexity of private inference. This work explores desirable activation functions in normalization-free decoder-only LLMs. Contrary to the conventional preference for the GELU in transformer-based models, empirical findings demonstrate an opposite trend— ReLU significantly outperforms GELU in LayerNorm-free models, leading to an 8.2% perplexity improvement. The work discovers a key issue with GELU, where early layers experience entropic overload, leading to the under-utilization of the representational capacity of attention heads. This highlights that smoother activations like GELU are ill-suited for LayerNorm-free architectures, whereas ReLU’s geometrical properties—specialization in input space and intra-class selectivity—lead to improved learning dynamics and better information retention in the absence of LayerNorm. This study offers key insights for optimizing transformer architectures where LayerNorm introduces significant challenges. <br> <br>

41. ***Taming Overconfidence in LLMs: <br>
This paper examines the overconfidence problem in LLMs fine-tuned with Reinforcement Learning from Human Feedback (RLHF). It introduces two Proximal Policy Optimization (PPO) variants—PPO-M and PPO-C—that reduce overconfidence by adjusting reward calculations. These methods are shown to lower calibration error without compromising model performance across diverse datasets.*** <br> <br>
    Oct 13, CMU, UC Berkeley et al published a [paper](https://arxiv.org/pdf/2410.09724) “Taming Overconfidence in LLMs: Reward Calibration in RLHF”. Language model calibration refers to the alignment between the confidence of the model and the actual performance of its responses. While previous studies point out the overconfidence phenomenon in Large Language Models (LLMs) and show that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) are overconfident with a more sharpened output probability, this study reveals that RLHF tends to lead models to express verbalized overconfidence in their own responses. The study investigates the underlying cause of this overconfidence and demonstrate that reward models used for Proximal Policy Optimization (PPO) exhibit inherent biases towards high-confidence scores regardless of the actual quality of responses. Building upon this insight, the work proposes two PPO variants: PPO-M: PPO with Calibrated Reward Modeling and PPO-C: PPO with Calibrated Reward Calculation. PPO-M integrates explicit confidence scores in reward model training, which calibrates reward models to better capture the alignment between response quality and verbalized confidence. PPO-C adjusts the reward score during PPO based on the difference between the current reward and the moving average of past rewards. Both PPO-M and PPO-C can be seamlessly integrated into the current PPO pipeline and do not require additional golden labels. The study evaluates the methods on both Llama3-8B and Mistral-7B across six diverse datasets including multiple-choice and open-ended generation. Experiment results demonstrate that both of the methods can reduce calibration error and maintain performance comparable to standard PPO. The study further shows that they do not compromise model capabilities in open-ended conversation settings. <br> <br>

43. ***ALLoRA: Adaptive Learning Rate in LoRA Finetuning: <br>
ALLoRA improves upon the existing Low-Rank Adaptation (LoRA) method for LLM finetuning by addressing limitations related to Dropout and scaling factors in short training episodes. By introducing adaptive learning rates, ALLoRA achieves better performance and efficiency, eliminating the need for certain hyperparameters. This approach shows significant accuracy gains over LoRA and its variants.*** <br> <br>
    Oct 13, Google and Brown Uni published a [paper](https://arxiv.org/pdf/2410.09692) “ALLoRA: Adaptive Learning Rate Mitigates LoRA Fatal Flaws”. Low-Rank Adaptation (LoRA) is the bread and butter of Large Language Model (LLM) finetuning. LoRA learns an additive low-rank perturbation, AB, of a pretrained matrix parameter W to align the model to a new task or dataset with W+AB. The study identifies three core limitations to LoRA for finetuning--a setting that employs limited amount of data and training steps. First, LoRA employs Dropout to prevent overfitting. The work proves that Dropout is only suitable for long training episodes but fails to converge to a reliable regularizer for short training episodes. Second, LoRA's initialization of B at 0 creates a slow training dynamic between A and B. That dynamic is also exacerbated by Dropout that further slows the escape from 0 for B which is particularly harmful for short training episodes. Third, the scaling factor multiplying each LoRA additive perturbation creates “short-sighted'” interactions between the LoRA modules of different layers. Motivated by principled analysis of those limitations, the study proposes an elegant solution: a Dropout-free, scaling-free, LoRA with Adaptive Learning rate--coined ALLoRA. By scaling the per sample and per parameter gradients with a coefficient inversely proportional to parameters' ℓ2 norm, ALLoRA alleviates those three limitations. As a by-product, ALLoRA removes two hyper-parameters from LoRA: the scaling factor and the dropout rate. Empirical results show that ALLoRA admits better accuracy than LoRA on various settings, including against recent LoRA variants such as Weight-Decomposed Low-Rank Adaptation (DoRA). Ablation studies show the solution is the optimal in a family of weight-dependent / output-dependent approaches on various LLMs including the latest Llama3. <br> <br>

45. ***Controllable Safety Alignment: <br>
This paper introduces a flexible framework, Controllable Safety Alignment (CoSA), for adapting LLMs to diverse safety requirements at inference time without retraining. Users can modify safety configurations using natural language prompts, allowing models to align with different cultural and social norms. CoSAlign improves model controllability and practicality, addressing the limitations of static safety standards.*** <br> <br>
    Oct 11, Microsoft and Johns Hopkins Uni published a [paper](https://arxiv.org/pdf/2410.08968) “Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements”. The current paradigm for safety alignment of large language models (LLMs) follows a one-size-fits-all approach: the model refuses to interact with any content deemed unsafe by the model provider. This approach lacks flexibility in the face of varying social norms across cultures and regions. In addition, users may have diverse safety needs, making a model with static safety standards too restrictive to be useful, as well as too costly to be re-aligned. The work proposes Controllable Safety Alignment (CoSA), a framework designed to adapt models to diverse safety requirements without re-training. Instead of aligning a fixed model, the work aligns models to follow safety configs -- free-form natural language descriptions of the desired safety behaviors -- that are provided as part of the system prompt. To adjust model safety behavior, authorized users only need to modify such safety configs at inference time. To enable that, the work proposes CoSAlign, a data-centric method for aligning LLMs to easily adapt to diverse safety configs. Furthermore, the study devises a novel controllability evaluation protocol that considers both helpfulness and configured safety, summarizing them into CoSA-Score, and construct CoSApien, a human-authored benchmark that consists of real-world LLM use cases with diverse safety requirements and corresponding evaluation prompts. The study shows that CoSAlign leads to substantial gains of controllability over strong baselines including in-context alignment. The framework encourages better representation and adaptation to pluralistic human values in LLMs, and thereby increasing their practicality. <br> <br>

47. ***MiRAGeNews: AI-Generated News Detection: <br>
The study presents the MiRAGeNews Dataset, designed to detect AI-generated fake news by analyzing image-caption pairs. The dataset poses a significant challenge for both humans and multi-modal LLMs. The authors propose a multi-modal detector, MiRAGe, that improves detection accuracy and helps combat the spread of misleading AI-generated content.*** <br> <br>
    Oct 11, Uni of Pennsylvania published a [paper](https://arxiv.org/pdf/2410.09045) “MiRAGeNews: Multimodal Realistic AI-Generated News Detection”. The proliferation of inflammatory or misleading "fake" news content has become increasingly common in recent years. Simultaneously, it has become easier than ever to use AI tools to generate photorealistic images depicting any scene imaginable. Combining these two -- AI-generated fake news content -- is particularly potent and dangerous. To combat the spread of AI-generated fake news, the work proposes the MiRAGeNews Dataset, a dataset of 12,500 high-quality real and AI-generated image-caption pairs from state-of-the-art generators. The study finds that the dataset poses a significant challenge to humans (60% F-1) and state-of-the-art multi-modal LLMs (< 24% F-1). Using the dataset the authors train a multi-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art baselines on image-caption pairs from out-of-domain image generators and news publishers. The authors release the code and data to aid future work on detecting AI-generated content. <br> <br>

49. ***Agents Thinking Fast and Slow: <br>
Inspired by Kahneman's "Thinking Fast and Slow," this study proposes a dual-agent architecture for LLMs. A "Talker" agent handles quick, intuitive conversational responses, while a "Reasoner" agent deals with more complex multi-step reasoning and planning tasks. This modular system improves interaction by balancing speed and logical precision, demonstrated through a sleep-coaching agent.*** <br> <br>
    Oct 10, Google published a [paper](https://arxiv.org/pdf/2410.08328)  “Agents Thinking Fast and Slow: A Talker-Reasoner Architecture”. Large language models have enabled agents of all kinds to interact with users through natural conversation. Consequently, agents now have two jobs: conversing and planning/reasoning. Their conversational responses must be informed by all available information, and their actions must help to achieve goals. This dichotomy between conversing with the user and doing multi-step reasoning and planning can be seen as analogous to the human systems of "thinking fast and slow" as introduced by Kahneman. The proposed approach is comprised of a "Talker" agent (System 1) that is fast and intuitive, and tasked with synthesizing the conversational response; and a "Reasoner" agent (System 2) that is slower, more deliberative, and more logical, and is tasked with multi-step reasoning and planning, calling tools, performing actions in the world, and thereby producing the new agent state. The study describes the new Talker-Reasoner architecture and discuss its advantages, including modularity and decreased latency. The authors ground the discussion in the context of a sleep coaching agent, in order to demonstrate real-world relevance. <br> <br>

51. ***KV Prediction for Faster Inference: <br>
The paper introduces KV Prediction, a method to reduce the time taken for LLMs to generate their first token during inference. By using an auxiliary model to approximate the KV cache, the method significantly improves efficiency without sacrificing accuracy. This approach is especially useful for edge devices and demonstrates improvements in various benchmarks.*** <br> <br>
    Oct 10, Apple published a [paper](https://arxiv.org/pdf/2410.08391) “KV Prediction for Improved Time to First Token”. Inference with transformer-based language models begins with a prompt processing step. In this step, the model generates the first output token and stores the KV cache needed for future generation steps. This prompt processing step can be computationally expensive, taking 10s of seconds or more for billion-parameter models on edge devices when prompt lengths or batch sizes rise. This degrades user experience by introducing significant latency into the model's outputs. To reduce the time spent producing the first output (known as the “time to first token”, or TTFT) of a pretrained model, the study introduces a novel method called KV Prediction. In this method, a small auxiliary model is used to process the prompt and produce an approximation of the KV cache used by a base model. This approximated KV cache is then used with the base model for autoregressive generation without the need to query the auxiliary model again. The study demonstrates that the method produces a pareto-optimal efficiency-accuracy trade-off when compared to baselines. On TriviaQA, the study demonstrates relative accuracy improvements in the range of 15%-50% across a range of TTFT FLOPs budgets. The work also demonstrates accuracy improvements of up to 30% on HumanEval python code completion at fixed TTFT FLOPs budgets. Additionally, the authors benchmark models on an Apple M2 Pro CPU and demonstrate that the improvement in FLOPs translates to a TTFT speedup on hardware. The authors release our code at https://github.com/apple/corenet/tree/main/projects/kv-prediction. <br> <br>

53. ***GenARM: Autoregressive Reward Model for Test-time Alignment: <br>
GenARM proposes a test-time alignment method using an Autoregressive Reward Model to guide frozen LLMs. Unlike previous methods, it can compute next-token rewards, making it more suitable for autoregressive text generation. The approach achieves comparable performance to traditional methods while allowing multi-objective alignment and efficient preference guidance without retraining.*** <br> <br>
    Oct 10, Uni Maryland and JPMorgan AI Research published a [paper](https://arxiv.org/pdf/2410.08193) “GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment”. Large Language Models (LLMs) exhibit impressive capabilities but require careful alignment with human preferences. Traditional training-time methods finetune LLMs using human preference datasets but incur significant training costs and require repeated training to handle diverse user preferences. Test-time alignment methods address this by using reward models (RMs) to guide frozen LLMs without retraining. However, existing test-time approaches rely on trajectory-level RMs which are designed to evaluate complete responses, making them unsuitable for autoregressive text generation that requires computing next-token rewards from partial responses. To address this, the study introduces GenARM, a test-time alignment approach that leverages the Autoregressive Reward Model--a novel reward parametrization designed to predict next-token rewards for efficient and effective autoregressive generation. Theoretically, the work demonstrates that this parametrization can provably guide frozen LLMs toward any distribution achievable by traditional RMs within the KL-regularized reinforcement learning framework. Experimental results show that GenARM significantly outperforms prior test-time alignment baselines and matches the performance of training-time methods. Additionally, GenARM enables efficient weak-to-strong guidance, aligning larger LLMs with smaller RMs without the high costs of training larger models. Furthermore, GenARM supports multi-objective alignment, allowing real-time trade-offs between preference dimensions and catering to diverse user preferences without retraining. <br> <br>

55. ***Long-Context LLMs and RAG: <br>
The study explores the limitations of using long-context LLMs for retrieval-augmented generation (RAG). It identifies the negative impact of retrieved "hard negatives" on performance as context length increases. The authors propose training-free and training-based optimizations to enhance RAG effectiveness in long-context scenarios, achieving notable improvements in robustness and performance.*** <br> <br>
    Oct 8, Google and UIUC published a [paper](https://arxiv.org/pdf/2410.05983) “Long-Context LLMs Meet RAG”. Retrieval-augmented generation (RAG) empowers large language models (LLMs) to utilize external knowledge sources. The increasing capacity of LLMs to process longer input sequences opens up avenues for providing more retrieved information, to potentially enhance the quality of generated outputs. It is plausible to assume that a larger retrieval set would contain more relevant information (higher recall), that might result in improved performance. However, empirical findings demonstrate that for many long-context LLMs, the quality of generated output initially improves first, but then subsequently declines as the number of retrieved passages increases. This paper investigates this phenomenon, identifying the detrimental impact of retrieved "hard negatives" as a key contributor. To mitigate this and enhance the robustness of long-context LLM-based RAG, the study proposes both training-free and training-based approaches. The work first showcases the effectiveness of retrieval reordering as a simple yet powerful training-free optimization. Furthermore, the work explores training-based methods, specifically RAG-specific implicit LLM fine-tuning and RAG-oriented fine-tuning with intermediate reasoning, demonstrating their capacity for substantial performance gains. Finally, the study conducts a systematic analysis of design choices for these training-based methods, including data distribution, retriever selection, and training context length. <br> <br>

57. ***GSM-Symbolic: Limitations in Mathematical Reasoning: <br>
This study examines the mathematical reasoning capabilities of LLMs using a new benchmark called GSM-Symbolic. It reveals that LLMs struggle with consistent performance when minor variations are made to question templates, particularly in multi-clause problems. The findings suggest that current models replicate reasoning patterns from training data, rather than performing genuine logical reasoning, highlighting areas for improvement.*** <br> <br>
    Oct 7, Apple published a [paper](https://arxiv.org/abs/2410.05229) “GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models”. Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, the work conducts a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, the work introduces GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of this http URL findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, the study investigates the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. The authors hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, the work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.
 <br> <br>


***Oct 13***

1. ***Improving reasoning in LLMs with PRMs:  <br>Google and CMU explore process reward models (PRMs) to enhance reasoning in large language models (LLMs). PRMs offer feedback at each reasoning step, unlike outcome reward models (ORMs), which only provide final feedback. The key is measuring progress at each step via a distinct prover policy. They find weak provers can improve stronger base policies, boosting accuracy and efficiency by over 6%, and enabling gains in reinforcement learning (RL).*** <br> <br>
   Oct 11, Google and CMU published a [paper](https://papers.cool/arxiv/2410.08146#:~:text=A%20promising%20approach%20for%20improving,feedback%20at%20the%20final%20step.) “Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning”. A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, potentially improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. To improve a base policy by running search against a PRM or using it as dense rewards for reinforcement learning (RL), the study asks: “How should we design process rewards?”. The key insight is that, to be effective, the process reward for a step should measure progress: a change in the likelihood of producing a correct response in the future, before and after taking the step, corresponding to the notion of step-level advantages in RL. Crucially, this progress should be measured under a prover policy distinct from the base policy. The work theoretically characterize the set of good provers and the results show that optimizing process rewards from such provers improves exploration during test-time search and online RL. In fact, the characterization shows that weak prover policies can substantially improve a stronger base policy, which the authors also observe empirically. The work validates the claims by training process advantage verifiers (PAVs) to predict progress under such provers, and show that compared to ORMs, test-time search against PAVs is > 8% more accurate, and 1.5 − 5× more compute-efficient. Online RL with dense rewards from PAVs enables one of the first results with 5 − 6× gain in sample efficiency, and > 6% gain in accuracy, over ORMs. <br> <br>

3. ***Nobel Prize 2024 in AI and Physics:  <br>John Hopfield and Geoffrey Hinton won the 2024 Nobel Prize in Physics for their groundbreaking work in machine learning. Their inventions fueled the AI revolution, with Hinton famously warning about the risks of advanced AI. The award coincides with another Nobel Prize in Chemistry for AI-related breakthroughs in protein design, highlighting AI’s growing influence across scientific fields.*** <br> <br>
   Oct 10, [according to Reuters](https://www.reuters.com/science/hopfield-hinton-win-2024-nobel-prize-physics-2024-10-08/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=meta-teases-a-new-video-model&_bhlid=eff04f5277f7d09ad170836c27d4f4b3e57f5868#:~:text=STOCKHOLM%2C%20Oct%208%20(Reuters),for%20the%20artificial%20intelligence%20boom.), “Nobel physics prize 2024 won by AI pioneers John Hopfield and Geoffrey Hinton”. U.S. scientist John Hopfield and British-Canadian Geoffrey Hinton won the 2024 Nobel Prize in Physics on Tuesday for discoveries and inventions in machine learning that paved the way for the artificial intelligence boom. Heralded for its revolutionary potential in areas ranging from cutting-edge scientific discovery to more efficient admin, the emerging technology on which the duo worked has also raised fears humankind may soon be outsmarted and outcompeted by its own creation. Hinton has been widely credited as a godfather of AI and made headlines when he quit his job at Google (GOOGL.O), opens new tab last year to be able to more easily speak about the dangers of the technology he had pioneered. Hopfield, 91, a professor emeritus at Princeton University, created an associative memory that can store and reconstruct images and other types of patterns in data. He echoed Hinton's concerns, saying there was something unnerving about the unknown potential and limits of AI. One day after the physics prize was announced, Demis Hassabis, John Jumper, and David Baker won the Chemistry Nobel Prize for their work on AlphaFold and protein design. AlphaFold and AlphaFold 2, as well as the work of Baker’s lab, are compelling applications of AI that made significant steps forward in chemistry and biology, and this award, too, is well deserved! It’s remarkable that the Nobel committees for physics and chemistry, which are made up of scientists in those fields, chose to honor AI researchers with this year’s awards.  <br> <br>

5. ***State of AI Report:  <br>This report examines five AI dimensions: research, industry, politics, safety, and predictions. OpenAI leads in research, NVIDIA dominates the industry, and AI’s political impact is yet to unfold. Companies are shifting focus from safety to sales. Predictions include a rise in viral AI apps and open-source alternatives to dominant models, while investment in humanoids may decline.*** <br> <br>
   Oct 10, Stateof.ai and airstreet.com [published](https://docs.google.com/presentation/d/1GmZmoWOa2O92BPrncRcTKa15xvQGhq7g4I4hJSNlC0M/edit#slide=id.g309a25a756d_0_85) “State of AI Report”.  The report considered five key dimensions. Research – Frontier lab performance converges, but OpenAI maintains its edge following the launch of o1, as planning and reasoning emerge as a major frontier. Foundation models demonstrate their ability to break out of language as multimodal research drives into mathematics, biology, genomics, the physical sciences, and neuroscience. Industry - NVIDIA remains the most powerful company in the world, enjoying a stint in the $3T club, while regulators probe the concentrations of power within GenAI. More established GenAI companies bring in billions of dollars in revenue, while start-ups begin to gain traction in sectors like video and audio generation. Although companies begin to make the journey from model to product, long-term questions around pricing and sustainability remain unresolved. Driven by a bull run in public markets, AI companies reach $9T in value, while investment levels grow healthily in private companies. Politics - Anticipated AI effects on elections, employment and a range of other sensitive areas are yet to be realized at any scale. Safety - A vibe-shift from safety to acceleration takes place as companies that previously warned us about the pending extinction of humanity need to ramp up enterprise sales and usage of their consumer apps. Every proposed jailbreaking ‘fix’ has failed, but researchers are increasingly concerned with more sophisticated, long-term attacks. Predictions - An app or website created solely by someone with no coding ability will go viral (e.g. App Store Top-100). Frontier labs implement meaningful changes to data collection practices after cases begin reaching trial. An open source alternative to OpenAI o1 surpasses it across a range of reasoning benchmarks. Levels of investment in humanoids will trail off, as companies struggle to achieve product-market fit. <br> <br>
 
7. ***Nvidia’s Mixture of Experts (MoE) Models:  <br>Nvidia introduces methods to convert pre-trained dense models into sparse Mixture of Experts (MoE) models, increasing capacity without retraining from scratch. Their new initialization and routing techniques outperform dense models, with upcycled models achieving higher accuracy and efficiency, particularly in massive language tasks.*** <br> <br>
   Oct 10, Nvidia published a [paper](https://arxiv.org/pdf/2410.07524) “Upcycling Large Language Models into Mixture of Experts”. Upcycling pre-trained dense language models into sparse mixture-of-experts (MoE) models is an efficient approach to increase the model capacity of already trained models. However, optimal techniques for upcycling at scale remain unclear. In this work, the authors conduct an extensive study of upcycling methods and hyperparameters for billion-parameter scale language models. The work proposes a novel "virtual group" initialization scheme and weight scaling approach to enable upcycling into fine-grained MoE architectures. Through ablations, the work finds that upcycling outperforms continued dense model training. In addition, the study shows that softmax-then-topK expert routing improves over topK-then-softmax approach and higher granularity MoEs can help improve accuracy. Finally, the work upcycled Nemotron-4 15B on 1T tokens and compared it to a continuously trained version of the same model on the same 1T tokens: the continuous trained model achieved 65.3% MMLU, whereas the upcycled model achieved 67.6%. The results offer insights and best practices to effectively leverage upcycling for building MoE language models. <br> <br>

9. ***Autoregressive Video Diffusion Models:  <br>Stony Brook University and Adobe extend video diffusion models to generate longer videos by progressively denoising latent frames. This technique allows the generation of high-quality, 1-minute videos without scene disruptions. The new models achieve state-of-the-art results in long video generation, overcoming previous limitations.*** <br> <br>
    Oct 10, Stony Brook Uni and Adobe published a [paper](https://arxiv.org/pdf/2410.08151) “Progressive Autoregressive Video Diffusion Models”. Current frontier video diffusion models have demonstrated remarkable results at generating high-quality videos. However, they can only generate short video clips, normally around 10 seconds or 240 frames, due to computation limitations during training. This work shows that existing models can be naturally extended to autoregressive video diffusion models without changing the architectures. The key idea is to assign the latent frames with progressively increasing noise levels rather than a single noise level, which allows for fine-grained condition among the latents and large overlaps between the attention windows. Such progressive video denoising allows the models to autoregressively generate video frames without quality degradation or abrupt scene changes. The paper presents state-of-the-art results on long video generation at 1 minute (1440 frames at 24 FPS). Videos from this paper are available at https://desaixie.github.io/pa-vdm/. <br> <br>

11. ***Agent S: Automating GUI Tasks:  <br>Agent S, developed by Simular Research, introduces a framework for autonomous computer interactions via GUI, solving complex tasks using hierarchical planning. It outperforms existing models in task success rates and shows broad generalizability across systems. The framework represents a significant step forward in human-computer interaction automation.*** <br> <br>
    Oct 10, Simular Research published a [paper](https://arxiv.org/pdf/2410.08164) “Agent S: An Open Agentic Framework that Uses Computers Like a Human”. The paper presents Agent S, an open agentic framework that enables autonomous interaction with computers through a Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S aims to address three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. In addition, it employs an Agent-Computer Interface (ACI) to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models (MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37% on success rate (an 83.6% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark. Code available at https://github.com/simular-ai/Agent-S. <br> <br>

13. ***Agentic Reasoning Era in AI:  <br>An article from Sequoia highlights the shift from pattern-based AI to agentic reasoning. This evolution allows AI to deliberate before responding, enhancing problem-solving capabilities. AI is moving towards performing work across various sectors, suggesting future AI applications will revolutionize service industries.*** <br> <br>
    Oct 9, Seauoia published an [article](https://www.sequoiacap.com/article/generative-ais-act-o1/) “Generative AI’s Act o1 - The Agentic Reasoning Era Begins”. The article discusses the ongoing evolution of Generative AI from rapid, pattern-based responses ("System 1") to deeper, deliberate reasoning ("System 2"). It highlights that the market for large language models (LLMs) has stabilized among major players (Microsoft/OpenAI, AWS/Anthropic, Google/DeepMind). However, the next frontier is enhancing AI's reasoning abilities at inference time, enabling AI to "stop and think" before responding, akin to the reasoning used by AlphaGo in its famous 2016 Go match. The new "Strawberry" model from OpenAI is the first to implement this, leveraging "inference-time compute" for more thoughtful problem-solving, especially in structured domains like math and coding. This shift introduces a new scaling law: more inference-time compute leads to better reasoning, potentially enabling AI to solve increasingly complex problems, from programming to mathematics. In terms of applications, a new wave of agentic applications is emerging, powered by this advanced reasoning layer. These applications move beyond mere automation, targeting specific tasks in various sectors, from legal work (Harvey) to cybersecurity (XBOW) and software engineering (Factory). The essay argues that AI is shifting from selling software to selling "work," expanding into trillion-dollar service markets. The article concludes with a focus on the potential of multi-agent systems, where AI agents collaborate to achieve more complex goals, signaling the next phase of AI development and hinting at the emergence of true AGI (Artificial General Intelligence). <br> <br>

15. ***Repetition vs. Data Diversity in Transformers:  <br>Meta explores how repeated training examples improve transformers' performance over diverse data in mathematical tasks. Their findings suggest that repetition can accelerate learning and performance in structured tasks like math, providing insights into balancing generalization and memorization in deep learning.*** <br> <br>
    Oct 9, Meta published a [paper](https://arxiv.org/pdf/2410.07041) “Emergent properties with repeated examples”. The work studies the performance of transformers as a function of the number of repetitions of training examples with algorithmically generated datasets. On three problems of mathematics: the greatest common divisor, modular multiplication, and matrix eigenvalues, the work shows that for a fixed number of training steps, models trained on smaller sets of repeated examples outperform models trained on larger sets of single-use examples. The work also demonstrates that two-set training - repeated use of a small random subset of examples, along normal sampling on the rest of the training set - provides for faster learning and better performance. This highlights that the benefits of repetition can outweigh those of data diversity. These datasets and problems provide a controlled setting to shed light on the still poorly understood interplay between generalization and memorization in deep learning. <br> <br>

17. ***Pixtral-12B Multimodal Language Model:  <br> <br>Mistral introduces Pixtral-12B, a highly efficient multimodal language model outperforming larger counterparts on image and document understanding benchmarks. It excels without compromising natural language processing capabilities, and its ability to process multiple images in long contexts sets it apart from other models.*** <br> <br>
    Oct 9, Mistral published a [paper](https://arxiv.org/pdf/2410.07073) “Pixtral 12B”. The paper introduces Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license. <br> <br>

19. ***MLE-Bench for Machine Learning Engineering:  <br> <br>OpenAI introduces MLE-Bench, a benchmark to evaluate AI agents' machine learning engineering skills across 75 tasks. The study demonstrates AI's ability to match human performance in some competitions, although resource scaling and contamination from pre-training remain challenges.*** <br> <br>
    Oct 9, OpenAI published a [paper](https://arxiv.org/pdf/2410.07095) “MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering”. The paper introduces MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, the study curates 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. The study establishes human baselines for each competition using Kaggle's publicly available leaderboards. The work uses open-source agent scaffolds to evaluate several frontier language models on the benchmark, finding that the best-performing setup--OpenAI's o1-preview with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to the main results, the study investigates various forms of resource scaling for AI agents and the impact of contamination from pre-training. OpenAI open-source the benchmark code (github.com/openai/mle-bench/) to facilitate future research in understanding the ML engineering capabilities of AI agents. <br> <br>

21. ***Human-AI Complementarity in QA:  <br>A study investigates AI and human performance in question-answering tasks, revealing that humans outperform AI in abductive reasoning, while AI excels in fact-based retrieval. The findings emphasize the need for QA tasks that push AI towards higher-order reasoning and nuanced understanding.*** <br> <br>
    Oct 9, Uni of Maryland and Microsoft published a [paper](https://arxiv.org/pdf/2410.06524) “Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA”. Recent advancements of large language models (LLMs) have led to claims of AI surpassing humans in natural language processing (NLP) tasks such as textual understanding and reasoning. This work investigates these assertions by introducing CAIMIRA, a novel framework rooted in item response theory (IRT) that enables quantitative assessment and comparison of problem-solving abilities of question-answering (QA) agents: humans and AI systems. Through analysis of over 300,000 responses from ~70 AI systems and 155 humans across thousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in knowledge domains and reasoning skills. Humans outperform AI systems in knowledge-grounded abductive and conceptual reasoning, while state-of-the-art LLMs like GPT-4 and LLaMA show superior performance on targeted information retrieval and fact-based reasoning, particularly when information gaps are well-defined and addressable through pattern matching or data retrieval. These findings highlight the need for future QA tasks to focus on questions that challenge not only higher-order reasoning and scientific thinking, but also demand nuanced linguistic interpretation and cross-contextual knowledge application, helping advance AI developments that better emulate or complement human cognitive abilities in real-world problem-solving. <br> <br>

23. ***Cheating in Automatic LLM Benchmarks:  <br>Research reveals that LLM benchmarks can be gamed by null models that produce irrelevant outputs but still achieve high win rates. This highlights the need for better anti-cheating mechanisms to maintain the reliability of AI evaluations.*** <br> <br>
    Oct 9, Sea AI Lab and Singapore Management Uni published a [paper](https://arxiv.org/pdf/2410.07137) “Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates”. Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, the study shows that even a "null model" that always outputs a constant response (irrelevant to input instructions) can cheat automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench. Moreover, the crafted cheating outputs are transferable because the authors assume that the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are private and cannot be accessed. While the experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. The findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks. The code is available at https://github.com/sail-sg/Cheating-LLM-Benchmarks. <br> <br>

25. ***Retrieval-Augmented Decision Transformer:  <br>A new method for reinforcement learning, RA-DT uses external memory to store and retrieve relevant past experiences, improving decision-making in complex environments. This innovation addresses the limitations of in-context learning in reinforcement learning, particularly in long, sparse-reward tasks.*** <br> <br>
    Oct 9, JKU Liz, Extensity AI, Google, UCL et al published a [paper](https://arxiv.org/pdf/2410.07071) “Retrieval-Augmented Decision Transformer: External Memory for In-context RL”. In-context learning (ICL) is the ability of a model to learn a new task by observing a few exemplars in its context. While prevalent in NLP, this capability has recently also been observed in Reinforcement Learning (RL) settings. Prior in-context RL methods, however, require entire episodes in the agent's context. Given that complex environments typically lead to long episodes with sparse rewards, these methods are constrained to simple environments with short episodes. To address these challenges, the study introduces Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external memory mechanism to store past experiences from which it retrieves only sub-trajectories relevant for the current situation. The retrieval component in RA-DT does not require training and can be entirely domain-agnostic. The work evaluates the capabilities of RA-DT on grid-world environments, robotics simulations, and procedurally-generated video games. On grid-worlds, RA-DT outperforms baselines, while using only a fraction of their context length. Furthermore, the study illuminates the limitations of current in-context RL methods on complex environments and discuss future directions. To facilitate future research, the authors release datasets for four of the considered environments. <br> <br>

27. ***LLMs Performing Multiple Tasks Simultaneously:  <br>A surprising discovery shows LLMs can perform multiple distinct tasks simultaneously during a single inference call. This "task superposition" capability highlights the latent potential of LLMs and raises new questions about the underlying mechanisms enabling such versatility.*** <br> <br>
    Oct 8, Uni of Wisconsin-Madison, Uni of Michigan and Microsoft published a [paper](https://arxiv.org/pdf/2410.05603) “Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition”. Large Language Models (LLMs) have demonstrated remarkable in-context learning (ICL) capabilities. This study explores a surprising phenomenon related to ICL: LLMs can perform multiple, computationally distinct ICL tasks simultaneously, during a single inference call, a capability we term "task superposition". The study provides empirical evidence of this phenomenon across various LLM families and scales and show that this phenomenon emerges even if the authors train the model to in-context learn one task at a time. The work offers theoretical explanations that this capability is well within the expressive power of transformers. The study also explores how LLMs internally compose task vectors during superposition. Furthermore, the study shows that larger models can solve more ICL tasks in parallel, and better calibrate their output distribution. The findings offer insights into the latent capabilities of LLMs, further substantiate the perspective of "LLMs as superposition of simulators", and raise questions about the mechanisms enabling simultaneous task execution. <br> <br>

29. ***Model Collapse Due to Synthetic Data:  <br>A study reveals that even a small amount of synthetic data can cause severe performance degradation, or "model collapse," in large neural networks. This phenomenon persists even as dataset size increases, prompting further investigation into how scaling affects model robustness.*** <br> <br>
    Oct 8, Meta, NYU and UCLA published a [paper](https://arxiv.org/pdf/2410.04840) “Strong Model Collapse”. Within the scaling laws paradigm, which underpins the training of large neural networks like ChatGPT and Llama, the authors consider a supervised regression setting and establish the existance of a strong form of the model collapse phenomenon, a critical performance degradation due to synthetic data in the training corpus. The results show that even the smallest fraction of synthetic data (e.g., as little as 1\% of the total training dataset) can still lead to model collapse: larger and larger training sets do not enhance performance. The study further investigates whether increasing model size, an approach aligned with current trends in training large language models, exacerbates or mitigates model collapse. In a simplified regime where neural networks are approximated via random projections of tunable size, the study both theoretically and empirically shows that larger models can amplify model collapse. Interestingly, the theory also indicates that, beyond the interpolation threshold (which can be extremely high for very large datasets), larger models may mitigate the collapse, although they do not entirely prevent it. The theoretical findings are empirically verified through experiments on language models and feed-forward neural networks for images. <br> <br>

31. ***Intelligence at the Edge of Chaos:  <br>Researchers explore how complexity in rule-based systems influences the intelligence exhibited by AI models. They find that systems with intermediate complexity levels lead to more intelligent behavior, suggesting that exposure to complexity may be key to developing artificial intelligence.*** <br> <br>
    Oct 8, Yale Uni, Northwestern Uni and Idaho State Uni published a [paper](https://www.arxiv.org/pdf/2410.02536) “Intelligence at the Edge of Chaos”. The study explores the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. The study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems that generate behaviors ranging from trivial to highly complex. By training distinct Large Language Models (LLMs) on different ECAs, the work evaluated the relationship between the complexity of the rules' behavior and the intelligence exhibited by the LLMs, as reflected in their performance on downstream tasks. The findings reveal that rules with higher complexity lead to models exhibiting greater intelligence, as demonstrated by their performance on reasoning and chess move prediction tasks. Both uniform and periodic systems, and often also highly chaotic systems, resulted in poorer downstream performance, highlighting a sweet spot of complexity conducive to intelligence. The work conjectures that intelligence arises from the ability to predict complexity and that creating intelligence may require only exposure to complexity. <br> <br>

33. ***Differential Transformer (Microsoft & Tsinghua University):  <br>This work introduces Diff Transformer, which enhances attention to relevant context while reducing noise by using a differential attention mechanism. Experimental results demonstrate its effectiveness in various language modeling tasks, including long-context modeling and hallucination mitigation. The architecture shows improved performance compared to standard transformers and enhances in-context learning robustness.*** <br> <br>
    Oct 8, Microsoft and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2410.05258) “Differential Transformer”. Transformer tends to overallocate attention to irrelevant context. This work introduces Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models. [Code is here](https://github.com/microsoft/unilm/tree/master/Diff-Transformer). <br> <br>

35. ***Falcon Mamba: Attention-Free 7B Model (TII Abu Dhabi):  <br>Falcon Mamba 7B is a novel language model based on the Mamba architecture, trained on 5.8 trillion tokens. It surpasses models like Mistral 7B and Llama3.1, offering faster inference and lower memory requirements. The model outperforms Transformer-based and hybrid designs, making it a top-performing Mamba model.*** <br> <br>
    Oct 7, TII Aub Dhabi published a [paper](https://arxiv.org/pdf/2410.05355) “Falcon Mamba: The First Competitive Attention-free 7B Language Model”. This report presents Falcon Mamba 7B, a new base large language model based on the novel Mamba architecture. Falcon Mamba 7B is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model, Falcon Mamba 7B surpasses leading open-weight models based on Transformers, such as Mistral 7B, Llama3.1 8B, and Falcon2 11B. It is on par with Gemma 7B and outperforms models with different architecture designs, such as RecurrentGemma 9B and RWKV-v6 Finch 7B/14B. Currently, Falcon Mamba 7B is the best-performing Mamba model in the literature at this scale, surpassing both existing Mamba and hybrid Mamba-Transformer models, according to the Open LLM Leaderboard. Due to its architecture, Falcon Mamba 7B is significantly faster at inference and requires substantially less memory for long sequence generation. Despite recent studies suggesting that hybrid Mamba-Transformer models outperform pure architecture designs, the work demonstrates that even the pure Mamba design can achieve similar, or even superior results compared to the Transformer and hybrid designs. The weights and the implementation of Falcon Mamba 7B are publicly available on https://huggingface.co/tiiuae/falcon-mamba-7b, under a permissive license. <br> <br>

37. ***LLM Hallucination Representation (Technion, Google, & Apple):  <br>This study explores how LLMs internally represent hallucinations (errors). It finds that models encode much more information about truthfulness than previously recognized, with certain tokens holding more truthfulness information. However, this information is multifaceted, and error detectors often fail to generalize across datasets. The research also reveals a discrepancy between the model’s internal truthfulness representation and its external behavior.*** <br> <br>
    Oct 7, Technion, Google and Apple published a [paper](https://arxiv.org/pdf/2410.02707) “LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations”. Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. This study shows that the internal representations of LLMs encode much more information about truthfulness than previously recognized. The work first discovers that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, the work shows that such error detectors fail to generalize across datasets, implying that -- contrary to prior claims -- truthfulness encoding is not universal but rather multifaceted. Next, the study shows that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, the study reveals a discrepancy between LLMs' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen researchers understanding of LLM errors from the model's internal perspective, which can guide future research on enhancing error analysis and mitigation. <br> <br>

39. ***SFTMix for Instruction-Tuning (MIT & Zoom):  <br>SFTMix is a method for improving instruction-tuning in language models without relying on curated datasets. It uses confidence-level-based training dynamics to improve learning and reduce overfitting. The approach leads to significant performance gains across various tasks and model families.*** <br> <br>
    Oct 7, MIT and Zoom published a [paper](https://arxiv.org/pdf/2410.05248) “SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe”. To induce desired behaviors in large language models (LLMs) for interaction-driven tasks, the instruction-tuning stage typically trains LLMs on instruction-response pairs using the next-token prediction (NTP) loss. Previous work aiming to improve instruction-tuning performance often emphasizes the need for higher-quality supervised fine-tuning (SFT) datasets, which typically involves expensive data filtering with proprietary LLMs or labor-intensive data generation by human annotators. However, these approaches do not fully leverage the datasets' intrinsic properties, resulting in high computational and labor costs, thereby limiting scalability and performance gains. This paper proposes SFTMix, a novel recipe that elevates instruction-tuning performance beyond the conventional NTP paradigm, without the need for well-curated datasets. Observing that LLMs exhibit uneven confidence across the semantic representation space, the study argues that examples with different confidence levels should play distinct roles during the instruction-tuning process. Based on this insight, SFTMix leverages training dynamics to identify examples with varying confidence levels, then applies a Mixup-based regularization to mitigate overfitting on confident examples while propagating supervision signals to improve learning on relatively unconfident ones. This approach enables SFTMix to significantly outperform NTP across a wide range of instruction-following and healthcare domain-specific SFT tasks, demonstrating its adaptability to diverse LLM families and scalability to datasets of any size. Comprehensive ablation studies further verify the robustness of SFTMix's design choices, underscoring its versatility in consistently enhancing performance across different LLMs and datasets in broader natural language processing applications. <br> <br>

41. ***Instruction Diversity for Generalization (University of Illinois, University of Chicago & Meta):  <br>This study investigates the importance of instruction diversity in large language models. It shows that generalization only occurs when training data is sufficiently diverse across semantic domains. Cross-domain data diversification leads to better adaptability, offering insights for optimizing datasets for specialist and generalist models.*** <br> <br>
    Oct 7, Uni of Illinois, Uni of Chicago and Meta published a [paper](https://arxiv.org/pdf/2410.04717) “Only-IF:Revealing the Decisive Effect of Instruction Diversity on Generalization”. Understanding and accurately following instructions is critical for large language models (LLMs) to be effective across diverse tasks. This work rigorously examines the key factors that enable models to generalize to unseen instructions, providing insights to guide the collection of data for instruction-tuning. Through controlled experiments, inspired by the Turing-complete Markov algorithm, the study demonstrates that such generalization only emerges when training data is diversified enough across semantic domains. The findings also reveal that merely diversifying within limited domains fails to ensure robust generalization. In contrast, crossdomain data diversification, even under constrained data budgets, significantly enhances a model’s adaptability. The study further extends the analysis to real-world scenarios, including fine-tuning of specialist and generalist models. In both cases, the study demonstrates that 1) better performance can be achieved by increasing the diversity of an established dataset while keeping the data size constant, and 2) when scaling up the data, diversifying the semantics of instructions is more effective than simply increasing the quantity of similar data. The research provides important insights for dataset collation, particularly when optimizing model performance by expanding training data for both specialist and generalist scenarios. The work shows that careful consideration of data diversification is key: training specialist models with data extending beyond their core domain leads to significant performance improvements, while generalist models benefit from diverse data mixtures that enhance their overall instruction-following capabilities across a wide range of applications. The results highlight the critical role of strategic diversification and offer clear guidelines for improving data quality. <br> <br>

43. ***TidalDecode: Position Persistent Sparse Attention (CMU):  <br>TidalDecode is a method for fast and accurate decoding in long-context LLMs by improving sparse attention mechanisms. It reduces token selection overhead while maintaining the quality of generated results, cutting decoding latency by up to 2.1x compared to full attention methods.*** <br> <br>
    Oct 7, CMU published a [paper](https://arxiv.org/pdf/2410.05076) “TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention”. Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x. <br> <br>

45. ***Input-Adaptive Computation in LMs (MIT):  <br>This paper presents an approach to allocate computation adaptively based on input complexity. It dynamically adjusts the resources needed for generating responses, reducing computation costs by up to 50% without sacrificing quality, or improving quality by up to 10%.*** <br> <br>
    Oct 7, MIT published a [paper](https://arxiv.org/pdf/2410.04707) “Learning How Hard to Think: Input-Adaptive Allocation of LM Computation”. Computationally intensive decoding procedures—including search, reranking, and self-critique—can improve the quality of language model (LM) outputs in problems spanning code generation, numerical reasoning, and dialog. Existing work typically applies the same decoding procedure for every input to an LM. But not all inputs require the same amount of computation to process. Can we allocate decoding computation adaptively, using more resources to answer questions whose answers will be harder to compute? The paper presents an approach that predicts the distribution of rewards given an input and computation budget, then allocates additional computation to inputs for which it is predicted to be most useful. The work applies this approach in two decoding procedures: first, an adaptive best-of-k procedure that dynamically selects the number of samples to generate as input to a reranker; second, a routing procedure that dynamically responds to a query using a decoding procedure that is expensive but accurate, or one that is cheaper but less capable. Across a suite of programming, mathematics, and dialog tasks, the study shows that accurate computation-allocation procedures can be learned, and reduce computation by up to 50% at no cost to response quality, or improve quality by up to 10% at a fixed computational budget. <br> <br>

47. ***VLM2Vec for Multimodal Embedding (University of Waterloo & Salesforce Research):  <br>VLM2Vec is a framework that transforms vision-language models into embedding models. It performs well on a new multimodal embedding benchmark (MMEB), outperforming existing models in various tasks such as classification and multimodal retrieval.*** <br> <br>
    Oct 7, Uni of Waterloo and Salesforce Research published a [paper](https://arxiv.org/pdf/2410.05160) “VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks”.  Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize across tasks (e.g., MTEB). However, progress in learning universal multimodal embedding models has been relatively slow despite their importance. This work aims to explore the potential for building universal embeddings capable of handling a wide range of downstream tasks. The contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark), which covers 4 meta-tasks (i.e. classification, visual question answering, multimodal retrieval, and visual grounding) and 36 datasets, including 20 training and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model -> Vector), a contrastive training framework that converts any state-of-the-art vision-language model into an embedding model via training on MMEB. Unlike previous models such as CLIP and BLIP, VLM2Vec can process any combination of images and text to generate a fixed-dimensional vector based on task instructions. The study builds a series of VLM2Vec models on Phi-3.5-V and evaluate them on MMEB's evaluation split. The results show that VLM2Vec achieves an absolute average improvement of 10% to 20% over existing multimodal embedding models on both in-distribution and out-of-distribution datasets in MMEB. <br> <br>

49. ***Mitigating Loss Spikes in LLMs (NTT Corp.):  <br>This work introduces a reparameterization technique called WeSaR to mitigate loss spikes during LLM training. By uniformly scaling the parameters’ norm, WeSaR stabilizes and accelerates training, outperforming existing methods for large Transformer models.*** <br> <br>
    Oct 7, NTT Corp. published a [paper](https://arxiv.org/pdf/2410.05052) on EMNLP2024 “Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes”. Loss spikes, a phenomenon in which the loss value diverges suddenly, is a fundamental issue in the pre-training of large language models. This paper supposes that the non-uniformity of the norm of the parameters is one of the causes of loss spikes. Here, in training of neural networks, the scale of the gradients is required to be kept constant throughout the layers to avoid the vanishing and exploding gradients problem. However, to meet these requirements in the Transformer model, the norm of the model parameters must be non-uniform, and thus, parameters whose norm is smaller are more sensitive to the parameter update. To address this issue, the study proposes a novel technique, weight scaling as reparameterization (WeSaR). WeSaR introduces a gate parameter per parameter matrix and adjusts it to the value satisfying the requirements. Because of the gate parameter, WeSaR sets the norm of the original parameters uniformly, which results in stable training. Experimental results with the Transformer decoders consisting of 130 million, 1.3 billion, and 13 billion parameters showed that WeSaR stabilizes and accelerates training and that it outperformed compared methods including popular initialization methods. <br> <br>

51. ***Inference Scaling for Long-Context RAG (Google):  <br>This study explores strategies for scaling inference computation in retrieval-augmented generation (RAG). It demonstrates that scaling test-time computation yields linear performance improvements. The work also provides a model for predicting optimal inference configurations, achieving up to 58.9% gains in performance on benchmark datasets.*** <br> <br>
    Oct 6, Google published a [paper](https://arxiv.org/pdf/2410.04343) “Inference Scaling for Long-Context Retrieval Augmented Generation”. The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. This work investigates inference scaling for retrieval augmented generation (RAG), exploring strategies beyond simply increasing the quantity of knowledge. The study focuses on two inference scaling strategies: in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. The work addresses two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? The observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship described as the inference scaling laws for RAG. Building on this, the work further develops the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, the work demonstrates that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG. <br> <br>

53. ***Algorithmic Capabilities of Random Transformers (MIT):  <br>This paper shows that random transformers, without training, can perform algorithmic tasks such as modular arithmetic and decimal addition. The findings suggest that some algorithmic capabilities are inherent in transformers and can be accessed through structured inputs.*** <br> <br>
    Oct 6, MIT published a [paper](https://arxiv.org/abs/2410.04368) “Algorithmic Capabilities of Random Transformers”. Trained transformer models have been found to implement interpretable procedures for tasks like arithmetic and associative recall, but little is understood about how the circuits that implement these procedures originate during training. To what extent do they depend on the supervisory signal provided to models, and to what extent are they attributable to behavior already present in models at the beginning of training? To investigate these questions, the study investigates what functions can be learned by randomly initialized transformers in which only the embedding layers are optimized, so that the only input--output mappings learnable from data are those already implemented (up to a choice of encoding scheme) by the randomly initialized model. The study finds that these random transformers can perform a wide range of meaningful algorithmic tasks, including modular arithmetic, in-weights and in-context associative recall, decimal addition, parenthesis balancing, and even some aspects of natural language text generation. The results indicate that some algorithmic capabilities are present in transformers (and accessible via appropriately structured inputs) even before these models are trained. Code is available at [this https URL](https://github.com/fjzzq2002/random_transformers). <br> <br>

55. ***Steering LLMs Between Code and Text Reasoning (MIT, Harvard, Microsoft, & Google):  <br>This study examines the challenge of steering LLMs to alternate between textual reasoning and code execution. It presents methods to improve steering, demonstrating notable improvements but highlighting areas for further research in efficiently combining these reasoning modes.*** <br> <br>
    Oct 4, MIT, Harvard, Microsoft and Google published a [paper](https://arxiv.org/pdf/2410.03524) “Steering Large Language Models between Code Execution and Textual Reasoning”. While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100% success through direct coding, which is more scalable and avoids the computational overhead associated with textual iterating and searching. Textual reasoning has inherent limitations in solving tasks with challenges in math, logics, optimization, and searching, which is unlikely to be solved by simply scaling up the model and data size. The recently released OpenAI GPT Code Interpreter and multi-agent frameworks such as AutoGen have demonstrated remarkable proficiency of integrating code generation and execution to solve complex tasks using LLMs. However, based on the authors’ experiments on 7 existing popular methods for steering code/text generation in both single- and multi-turn settings with 14 tasks and 6 types of LLMs (including the new O1-preview), currently there is no optimal method to correctly steer LLMs to write code when needed. The study discovers some interesting patterns on when models use code vs. textual reasoning with the evolution to task complexity and model sizes, which even result in an astonishingly inverse scaling law. The work also discovers that results from LLM written code are not always better than using textual reasoning, even if the task could be solved through code. To mitigate the above issues, the study proposes three methods to better steer LLM code/text generation and achieve a notable improvement. The costs of token lengths and runtime are thoroughly discussed for all the methods. The authors believe the problem of steering LLM code/text generation is critical for future research and has much space for further improvement. Project Page, Datasets, and Codes are available at [this https URL](https://yongchao98.github.io/CodeSteer/). <br> <br>

57. ***Autoregressive LLMs as Universal Computers (Google & University of Alberta):  <br>This work proves that autoregressive decoding of a transformer-based LLM can realize universal computation, capable of simulating a Turing machine with no external modifications. The study provides insights into how LLMs process long inputs and demonstrates the computational universality of autoregressive models.*** <br> <br>
    Oct 4, Google and Uni of Alberta published a [paper](https://arxiv.org/pdf/2410.03170) “Autoregressive Large Language Models are Computationally Universal”. The study shows that autoregressive decoding of a transformer-based language model can realize universal computation, without external intervention or modification of the model's weights. Establishing this result requires understanding how a language model can process arbitrarily long inputs using a bounded context. For this purpose, the work considers a generalization of autoregressive decoding where, given a long input, emitted tokens are appended to the end of the sequence as the context window advances. The study first shows that the resulting system corresponds to a classical model of computation, a Lag system, that has long been known to be computationally universal. By leveraging a new proof, the work shows that a universal Turing machine can be simulated by a Lag system with 2027 production rules. The study then investigates whether an existing large language model can simulate the behaviour of such a universal Lag system. The authors give an affirmative answer by showing that a single system-prompt can be developed for gemini-1.5-pro-001 that drives the model, under deterministic (greedy) decoding, to correctly apply each of the 2027 production rules. The paper concludes that, by the Church-Turing thesis, prompted gemini-1.5-pro-001 with extended autoregressive (greedy) decoding is a general purpose computer. <br> <br>

59. ***In-Context Learning with Spurious Correlations (Google & YerevaNN):  <br>This paper highlights the susceptibility of in-context learners to spurious correlations, especially in classification tasks. A novel technique is proposed to train learners that generalize well to unseen tasks, outperforming traditional methods like ERM.*** <br> <br>
    Oct 4, Google, YerevaNN and Yerevan State Uni published a [paper](https://arxiv.org/pdf/2410.03140) “In-context Learning in Presence of Spurious Correlations”. Large language models exhibit a remarkable capacity for in-context learning, where they learn to solve tasks given a few examples. Recent work has shown that transformers can be trained to perform simple regression tasks in-context. This work explores the possibility of training an in-context learner for classification tasks involving spurious features. The researchers find that the conventional approach of training in-context learners is susceptible to spurious features. Moreover, when the meta-training dataset includes instances of only one task, the conventional approach leads to task memorization and fails to produce a model that leverages context for predictions. Based on these observations, the work proposes a novel technique to train such a learner for a given classification task. Remarkably, this in-context learner matches and sometimes outperforms strong methods like ERM and GroupDRO. However, unlike these algorithms, it does not generalize well to other tasks. The study shows that it is possible to obtain an in-context learner that generalizes to unseen tasks by training on a diverse dataset of synthetic in-context learning instances. <br> <br>

61. ***Model Merging at Scale (University of North Carolina, Google, & Virginia Tech):  <br>This study investigates the benefits of merging expert models into a single model. It shows that merging leads to better generalization, especially with larger models, and merging expert models is easier when they are derived from strong base models. Findings reveal that merging can outperform multi-task trained models.*** <br> <br>
    Oct 4, Uni of North Carolina, Google and Virgina Tech published a [paper](https://arxiv.org/pdf/2410.03617) “What Matters for Model Merging at Scale”. Model merging aims to combine multiple expert models into a more capable single model, offering benefits such as reduced storage and serving costs, improved generalization, and support for decentralized model development. Despite its promise, previous studies have primarily focused on merging a few small models. This leaves many unanswered questions about the effect of scaling model size and how it interplays with other key factors -- like the base model quality and number of expert models -- , to affect the merged model's performance. This work systematically evaluates the utility of model merging at scale, examining the impact of these different factors. The authors experiment with merging fully fine-tuned models using 4 popular merging methods -- Averaging, Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B parameters and merging up to 8 different expert models. The study evaluates the merged models on both held-in tasks, i.e., the expert's training tasks, and zero-shot generalization to unseen held-out tasks. Experiments provide several new insights about model merging at scale and the interplay between different factors. First, the study finds that merging is more effective when experts are created from strong base models, i.e., models with good zero-shot performance. Second, larger models facilitate easier merging. Third merging consistently improves generalization capabilities. Notably, when merging 8 large expert models, the merged models often generalize better compared to the multitask trained models. Fourth, the authors can better merge more expert models when working with larger models. Fifth, different merging methods behave very similarly at larger scales. Overall, the findings shed light on some interesting properties of model merging while also highlighting some limitations. <br> <br>

63. ***Tutor CoPilot’s Potential to Revolutionize Education <br>
Stanford University introduced "Tutor CoPilot," a Human-AI system that provides expert-like guidance to novice tutors, particularly benefiting under-served communities. The system increased student mastery by 4 percentage points, with lower-rated tutors seeing the greatest improvement at 9 points. At a cost of $20 per tutor annually, Tutor CoPilot enhanced pedagogical strategies, encouraging tutors to ask guiding questions rather than giving away answers. However, there were concerns about the system providing suggestions not appropriate for certain grade levels.*** <br> <br>
    Oct 3, Stanford Uni published a [paper](https://arxiv.org/pdf/2410.03017) “Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise”. Generative AI, particularly Language Models (LMs), has the potential to transform real-world domains with societal impact, particularly where access to experts is limited. For example, in education, training novice educators with expert guidance is important for effectiveness but expensive, creating significant barriers to improving education quality at scale. This challenge disproportionately harms students from under-served communities, who stand to gain the most from high-quality education. The study introduces Tutor CoPilot, a novel Human-AI approach that leverages a model of expert thinking to provide expert-like guidance to tutors as they tutor. This study is the first randomized controlled trial of a Human-AI system in live tutoring, involving 900 tutors and 1,800 K-12 students from historically under-served communities. Following a preregistered analysis plan, The study finds that students working with tutors that have access to Tutor CoPilot are 4 percentage points (p.p.) more likely to master topics (p<0.01). Notably, students of lower-rated tutors experienced the greatest benefit, improving mastery by 9 p.p. The study finds that Tutor CoPilot costs only $20 per-tutor annually. The authors analyze 550,000+ messages using classifiers to identify pedagogical strategies, and find that tutors with access to Tutor CoPilot are more likely to use high-quality strategies to foster student understanding (e.g., asking guiding questions) and less likely to give away the answer to the student. Tutor interviews highlight how Tutor CoPilot's guidance helps tutors to respond to student needs, though they flag issues in Tutor CoPilot, such as generating suggestions that are not grade-level appropriate. Altogether, the study of Tutor CoPilot demonstrates how Human-AI systems can scale expertise in real-world domains, bridge gaps in skills and create a future where high-quality education is accessible to all students. <br> <br>

65. ***AutoDAN-Turbo: Advancing Jailbreak Methods for LLMs <br>
The University of Wisconsin-Madison, Nvidia, and Cornell University developed AutoDAN-Turbo, a black-box method that autonomously discovers jailbreak strategies for large language models (LLMs). AutoDAN-Turbo achieved a 74.3% higher attack success rate than other methods and can integrate human-designed strategies to further increase success to 93.4% on GPT-4-1106-turbo. This method significantly advances red-teaming strategies by automating the discovery process without predefined constraints.*** <br> <br>
    Oct 3, Uni of Wisconsic-Madison, Nvidia, Cornell Uni, et al published a [paper](https://arxiv.org/pdf/2410.05295) “AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs”. The study proposes AutoDAN-Turbo, a black-box jailbreak method that can automatically discover as many jailbreak strategies as possible from scratch, without any human intervention or predefined scopes (e.g., specified candidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo can significantly outperform baseline methods, achieving a 74.3% higher average attack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an 88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a unified framework that can incorporate existing human-designed jailbreak strategies in a plug-and-play manner. By integrating human-designed strategies, AutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on GPT-4-1106-turbo. [Code is here](https://github.com/SaFoLab-WISC/AutoDAN-Turbo). <br> <br>

67. ***Evaluating Planning Capabilities in Large Reasoning Models <br>
Arizona State University evaluated OpenAI’s "Strawberry" (o1) Large Reasoning Model (LRM) for its planning and scheduling capabilities. While o1 improves upon autoregressive LLMs, it comes with high inference costs and lacks generation guarantees. The study also demonstrated that combining o1 with external verifiers (LRM-Modulo) improves performance and ensures output correctness, showing a potential for more robust planning systems in AI.*** <br> <br>
    Oct 3, Arizona State Uni published a [paper](https://arxiv.org/pdf/2410.02162) “Planning in Strawberry Fields: Evaluating and Improving the Planning and Scheduling Capabilities of LRM o1”. The ability to plan a course of action that achieves a desired state of affairs has long been considered a core competence of intelligent agents and has been an integral part of AI research since its inception. With the advent of large language models (LLMs), there has been considerable interest in the question of whether or not they possess such planning abilities, but -- despite the slew of new private and open source LLMs since GPT3 -- progress has remained slow. OpenAI claims that their recent o1 (Strawberry) model has been specifically constructed and trained to escape the normal limitations of autoregressive LLMs -- making it a new kind of model: a Large Reasoning Model (LRM). This work evaluates the planning capabilities of two LRMs (o1-preview and o1-mini) on both planning and scheduling benchmarks. The work finds that while o1 does seem to offer significant improvements over autoregressive LLMs, this comes at a steep inference cost, while still failing to provide any guarantees over what it generates. The study also shows that combining o1 models with external verifiers -- in a so-called LRM-Modulo system -- guarantees the correctness of the combined system's output while further improving performance. <br> <br>

69. ***Energy-Efficient Multiplication with L-Mul Algorithm <br>
BitEnergy AI introduced the L-Mul algorithm, which replaces floating-point multiplications with integer additions, offering up to 95% energy savings for tensor processing. Despite lower computational costs, L-Mul achieves higher precision than 8-bit floating point multiplication and performs well across various tasks such as natural language processing, mathematics, and reasoning. The study highlights the potential for L-Mul to enhance energy efficiency in AI models without sacrificing accuracy.*** <br> <br>
    Oct 2, BitEnergy AI published a [paper](https://arxiv.org/pdf/2410.00907) “Addition is All You Need for Energy-efficient Language Models”.  Large neural networks spend most computation on floating point tensor multiplications. This work finds that a floating point multiplier can be approximated by one integer adder with high precision. The work proposes the linear-complexity multiplication L-Mul algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by element-wise floating point tensor multiplications and 80% energy cost of dot products. The authors calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. The numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8_e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. The study further shows that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8_e4m3 as accumulation precision in both fine-tuning and inference. <br> <br>

71. ***CGPO: A Breakthrough in Multi-Task Learning for RLHF <br>
Meta introduced Constrained Generative Policy Optimization (CGPO) as a novel approach to address the challenges of reward hacking and multi-objective optimization in reinforcement learning from human feedback (RLHF). CGPO outperformed traditional RLHF methods in tasks like general chat and STEM questions, showing improvements of up to 12.5%. Its ability to optimize across multiple objectives without extensive tuning makes it a promising solution for aligning LLMs in diverse applications.*** <br> <br>
    Oct 1, Meta published a [paper](https://arxiv.org/pdf/2409.20370) “The Perfect Blend: Redefining RLHF with Mixture of Judges”. Reinforcement learning from human feedback (RLHF) has become the leading approach for fine-tuning large language models (LLM). However, RLHF has limitations in multi-task learning (MTL) due to challenges of reward hacking and extreme multi-objective optimization (i.e., trade-off of multiple and/or sometimes conflicting objectives). Applying RLHF for MTL currently requires careful tuning of the weights for reward model and data combinations. This is often done via human intuition and does not generalize. This study introduces a novel post-training paradigm which is called Constrained Generative Policy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with cost-efficient constrained policy optimization with stratification, which can identify the perfect blend in RLHF in a principled manner. It shows strong empirical results with theoretical guarantees, does not require extensive hyper-parameter tuning, and is plug-and-play in common post-training pipelines. Together, this can detect and mitigate reward hacking behaviors while reaching a pareto-optimal point across an extremely large number of objectives. Empirical evaluations demonstrate that CGPO significantly outperforms standard RLHF algorithms like PPO and DPO across various tasks including general chat, STEM questions, instruction following, and coding. Specifically, CGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in Arena-Hard (STEM & reasoning), and consistent gains in other domains like math and coding. Notably, PPO, while commonly used, is prone to severe reward hacking in popular coding benchmarks, which CGPO successfully addresses. This breakthrough in RLHF not only tackles reward hacking and extreme multi-objective optimization challenges but also advances the state-of-the-art in aligning general-purpose LLMs for diverse applications. <br> <br>

73. ***SciAgents: Automating Scientific Discovery <br>
MIT introduced "SciAgents," a system that uses knowledge graphs, LLMs, and multi-agent systems to automate scientific discovery. Applied to materials science, SciAgents autonomously generated and refined hypotheses, uncovering interdisciplinary connections that surpass traditional research methods. This system accelerates scientific innovation by integrating AI’s capacity for pattern recognition with large-scale data exploration, unlocking new material design principles inspired by nature.*** <br> <br>
    Sep 9, MIT published a [paper](https://arxiv.org/pdf/2409.05556) “SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning”. A key challenge in artificial intelligence is the creation of systems capable of autonomously advancing scientific understanding by exploring novel domains, identifying complex patterns, and uncovering previously unseen connections in vast scientific data. This paper presents SciAgents, an approach that leverages three core concepts: (1) the use of large-scale ontological knowledge graphs to organize and interconnect diverse scientific concepts, (2) a suite of large language models (LLMs) and data retrieval tools, and (3) multi-agent systems with in-situ learning capabilities. Applied to biologically inspired materials, SciAgents reveals hidden interdisciplinary relationships that were previously considered unrelated, achieving a scale, precision, and exploratory power that surpasses traditional human-driven research methods. The framework autonomously generates and refines research hypotheses, elucidating underlying mechanisms, design principles, and unexpected material properties. By integrating these capabilities in a modular fashion, the intelligent system yields material discoveries, critique and improve existing hypotheses, retrieve up-to-date data about existing research, and highlights their strengths and limitations. The case studies demonstrate scalable capabilities to combine generative AI, ontological representations, and multi-agent modeling, harnessing a ‘swarm of intelligence’ similar to biological systems. This provides new avenues for materials discovery and accelerates the development of advanced materials by unlocking Nature's design principles.
 <br> <br>


***Oct 06***

1. ***Meta’s MovieGen release and innovations in media generation:  <br>Meta released MovieGen and a paper detailing its new foundation models that can generate high-quality 1080p HD videos with synchronized audio and personalized video editing. The models set new standards in video and audio synthesis tasks. The paper highlights technical advancements and simplifications that enable scaling for large-scale media generation, hoping to accelerate progress in this research field.*** <br>
   Oct 4, Meta [released MovieGen](https://ai.meta.com/research/movie-gen/), and the [paper](https://ai.meta.com/static-resource/movie-gen-research-paper) “Movie Gen: A Cast of Media Foundation Models”. The paper presents Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. Meta also shows additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user’s image. The models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. The largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. The paper shows multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow Meta to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. Meta hopes this paper helps the research community to accelerate progress and innovation in media generation models. All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos. <br>

3. ***Quantifying generalization in large language models (LLMs):  <br>A joint paper from Harvard, MIT, and others introduced Scylla, a framework to measure LLMs' generalization by separating it from memorization. The study introduces the concept of critical complexity, where models begin to rely on memorization. The results suggest larger models handle more complex tasks better, and Scylla helps evaluate 28 LLMs, offering insight into their generalization.*** <br>
   Oct 3, Harvard Uni, MIT, UIUC, Meta and Uni of Chicago published a [paper](https://arxiv.org/pdf/2410.01769) “Quantifying Generalization Complexity for Large Language Models”. While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, the study introduces Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, the study uncovers a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which the authors term the generalization valley. Specifically, this phenomenon reveals a critical threshold - referred to as critical complexity - where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, the authors benchmark 28 LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.   <br>

5. ***LLMs as Markov Chains and theoretical insights:  <br>A paper by ENS Paris-Saclay and collaborators explored LLMs' performance using Markov chain theory. It found parallels between autoregressive models and Markov chains, uncovering key insights about stationary distributions and the effect of temperature on convergence. The study also offers theoretical guarantees for LLM performance through experiments.*** <br>
   Oct 3, ENS Paris-Saclay, Ark Lab, and Inria Paris published a [paper](https://arxiv.org/pdf/2410.02724) “Large Language Models as Markov Chains”. Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. This study approaches this challenging task by drawing an equivalence between generic autoregressive language models with vocabulary of size T and context window of size K and Markov chains defined on a finite state space of size O(T^K). The study derives several surprising findings related to the existence of a stationary distribution of Markov chains that capture the inference power of LLMs, their speed of convergence to it, and the influence of the temperature on the latter. The study then proves pre-training and in-context generalization bounds and show how the drawn equivalence allows the authors to enrich their interpretation. Finally, the study illustrates the theoretical guarantees with experiments on several recent LLMs to highlight how they capture the behavior observed in practice. <br>

7. ***Apple’s advances in vision-language models with CLOC:  <br>Apple proposed Contrastive Localized Language-Image Pre-training (CLOC), which improves upon CLIP by enhancing regional understanding in multimodal large language models. This method, focused on generating high-quality regional embeddings, outperforms CLIP in image region recognition tasks, and its scaling with billions of annotated images enables more precise language-image alignment.*** <br>
   Oct 3, Apple published a [paper](https://arxiv.org/pdf/2410.02746) “Contrastive Localized Language-Image Pre-Training”. Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect image inputs for language interactions. The success of CLIP as a vision-language foundation model relies on aligning web-crawled noisy text annotations at image levels. Nevertheless, such criteria may become insufficient for downstream tasks in need of fine-grained vision representations, especially when region-level understanding is demanding for MLLMs. This study improves the localization capability of CLIP with several advances, proposes a pre-training method called Contrastive Localized Language-Image Pre-training (CLOC) by complementing CLIP with region-text contrastive loss and modules. The paper formulates a new concept, promptable embeddings, of which the encoder produces image embeddings easy to transform into region representations given spatial hints. To support large-scale pre-training, the authors design a visually-enriched and spatially-localized captioning framework to effectively generate region-text pseudo-labels at scale. By scaling up to billions of annotated images, CLOC enables high-quality regional embeddings for image region recognition and retrieval tasks, and can be a drop-in replacement of CLIP to enhance MLLMs, especially on referring and grounding tasks. <br>

9. ***Mitigating hallucinations in vision-language models:  <br>A study from UC Berkeley tackled hallucinations in vision-language models by projecting their internal image representations to language vocabulary. The researchers developed a knowledge erasure algorithm, reducing hallucinations by 25.7% on the COCO2014 dataset, showing that targeted edits to models' latent representations can improve reliability without affecting overall performance.*** <br>
    Oct 3, UC Berkeley published a [paper](https://arxiv.org/pdf/2410.02762) “Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations”. The study investigates the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. The work projects VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on real objects than hallucinated objects. The work additionally uses these output probabilities to spatially localize real objects. Building on this approach, the study introduces a knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features. The authros show that targeted edits to a model's latent representations can reduce hallucinations by up to 25.7% on the COCO2014 dataset while preserving performance. The findings demonstrate how a deeper understanding of VLMs' latent representations can enhance reliability and enable novel capabilities, such as zero-shot segmentation. <br>

11. ***Contextual document embeddings for neural retrieval:  <br>Cornell University published a paper proposing methods for generating contextualized document embeddings by incorporating neighboring documents. These methods, focused on improving performance out-of-domain, achieved state-of-the-art results in several benchmarks, outperforming traditional biencoders. The study suggests these approaches can be broadly applied to contrastive learning datasets.*** <br>
    Oct 3, Cornell Uni published a [paper](https://arxiv.org/pdf/2410.02762) “Contextual Document Embeddings” . Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. This paper argues that these embeddings, while effective, are implicitly out-of-context for targeted use cases of retrieval, and that a contextualized document embedding should take into account both the document and neighboring documents in context - analogous to contextualized word embeddings. The study proposes two complementary methods for contextualized document embeddings: first, an alternative contrastive learning objective that explicitly incorporates the document neighbors into the intra-batch contextual loss; second, a new contextual architecture that explicitly encodes neighbor document information into the encoded representation. Results show that both methods achieve better performance than biencoders in several settings, with differences especially pronounced out-of-domain. The proposed models achieve state-of-the-art results on the MTEB benchmark with no hard negative mining, score distillation, dataset-specific instructions, intra-GPU example-sharing, or extremely large batch sizes. The method can be applied to improve performance on any contrastive learning dataset and any biencoder. <br>

13. ***End-to-end voice assistants with DiVA:  <br>A paper from Georgia Tech and others proposed training Speech LLMs without instruction data for voice assistants like Siri. The Distilled Voice Assistant (DiVA) model generalizes well to various tasks like Spoken Question Answering and Translation, outperforming existing models with significantly less training compute, while avoiding loss of text-based capabilities.*** <br>
    Oct 3, Georgia Inst of Tech, Stanford Uni, National Uni of Singapore and Northeastern Uni published a [paper](https://arxiv.org/pdf/2410.02678) “Distilling an End-to-End Voice Assistant Without Instruction Training Data”. Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT) have led to models “forgetting” capabilities from text-only LLMs. This work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only LLM to transcripts as self-supervision. Importantly, this process can be performed without annotated responses. The authors show that the Distilled Voice Assistant (DiVA) generalizes to Spoken Question Answering, Classification, and Translation. Furthermore, the study shows that DiVA better meets user preferences, achieving a 72\% win rate compared with state-of-the-art models like Qwen 2 Audio, despite using >100x less training compute. <br>

15. ***Learning game rules from data with small language models:  <br>Dell published a study showing that small pretrained language models can learn complex rules, like those of chess, from data. The study demonstrated how fine-tuning with increasing examples improved accuracy and reduced hallucinations in these models, indicating that even small models can learn complex processes effectively.*** <br>
    Oct 3, Dell published a [paper](https://arxiv.org/pdf/2410.02426) “Learning the Latent Rules of a Game from Data: A Chess Story”. The work demonstrates that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella "Schachnovelle," also known as "The Royal Game" in English, the study shows that 28M and 125M parameter pretrained foundational small language models (SLMs) can be instruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of chess, propose legal moves, and accurately solve chess problems. The study also explores the impact of successive language model fine-tuning epochs on improved outcomes and demonstrate reductions in model hallucinations by increasing the number of instruction fine-tuning examples. <br>

17. ***Intelligence emergence in rule-based systems:  <br>A study by Yale and others explored how complexity in rule-based systems like elementary cellular automata influences LLM intelligence. The findings suggest that exposure to more complex rules leads to better reasoning performance, indicating that complexity is key to developing intelligent behaviors in artificial systems.*** <br>
    Oct 3, Yale Uni et al published a [paper](https://arxiv.org/pdf/2410.02536) “Intelligence at the Edge of Chaos”. The study explores the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. The study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems that generate behaviors ranging from trivial to highly complex. By training distinct Large Language Models (LLMs) on different ECAs, the work evaluated the relationship between the complexity of the rules' behavior and the intelligence exhibited by the LLMs, as reflected in their performance on downstream tasks. The findings reveal that rules with higher complexity lead to models exhibiting greater intelligence, as demonstrated by their performance on reasoning and chess move prediction tasks. Both uniform and periodic systems, and often also highly chaotic systems, resulted in poorer downstream performance, highlighting a sweet spot of complexity conducive to intelligence. The authors conjecture that intelligence arises from the ability to predict complexity and that creating intelligence may require only exposure to complexity. <br>

19. ***Google’s selective attention mechanism for transformers:  <br>Google introduced a selective attention mechanism that reduces attention to unneeded elements in transformers. This simple change improves performance while reducing memory and compute requirements. Transformers equipped with selective attention require far less memory during inference, making them more efficient without sacrificing accuracy.*** <br>
    Oct 3, Google published a [paper](https://arxiv.org/abs/2410.02703) “Selective Attention Improves Transformer”. Unneeded elements in the attention's context degrade performance. The work introduces Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention improves language modeling performance in a variety of model sizes and context lengths. For example, a range of transformers trained with the language modeling objective on C4 with selective attention perform equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers with 100M parameters trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity. <br>

21. ***Evaluating LLMs' reasoning abilities in math problem-solving:  <br>A study by Mila, Google, and Microsoft evaluated LLMs' reasoning on math word problems, revealing a significant gap between solving independent questions and compositional pairs. The study highlights differences in reasoning capabilities among LLMs, with smaller models showing larger reasoning gaps, offering insights into LLMs' systematic reasoning challenges.*** <br>
    Oct 2, Mila, Google and Microsoft published a [paper](https://arxiv.org/pdf/2410.01748) “Not All LLM Reasoners Are Created Equal”. The authors study the depth of grade-school math (GSM) problem-solving capabilities of LLMs. To this end, the work evaluates their performance on pairs of existing math word problems together so that the answer to the second problem depends on correctly answering the first problem. The findings reveal a significant reasoning gap in most LLMs, that is performance difference between solving the compositional pairs and solving each question independently. This gap is more pronounced in smaller, more cost-efficient, and math-specialized models. Moreover, instruction-tuning recipes and code generation have varying effects across LLM sizes, while finetuning on GSM can lead to task overfitting. The analysis indicates that large reasoning gaps are not because of test-set leakage, but due to distraction from additional context and poor second-hop reasoning. Overall, LLMs exhibit systematic differences in their reasoning abilities, despite what their performance on standard benchmarks indicates. <br>

23. ***Revisiting recurrent neural networks (RNNs):  <br>A paper from Mila revisited traditional RNNs, demonstrating that by removing certain dependencies, LSTMs and GRUs can be trained in parallel, making them as fast and efficient as newer sequence models. This revival of decade-old models shows they can still compete with modern architectures in handling long sequences.*** <br>
    Oct 2, Mila, Uni of Montreal, Borealis AI published a [paper](https://arxiv.org/pdf/2410.01201) “Were RNNs All We Needed?”. The scalability limitations of Transformers regarding sequence length have renewed interest in recurrent sequence models that are parallelizable during training. As a result, many novel recurrent architectures, such as S4, Mamba, and Aaren, have been proposed that achieve comparable performance. This work revisits traditional recurrent neural networks (RNNs) from over a decade ago: LSTMs (1997) and GRUs (2014). While these models were slow due to requiring to backpropagate through time (BPTT), the work shows that by removing their hidden state dependencies from their input, forget, and update gates, LSTMs and GRUs no longer need to BPTT and can be efficiently trained in parallel. Building on this, the study introduces minimal versions (minLSTMs and minGRUs) that (1) use significantly fewer parameters than their traditional counterparts and (2) are fully parallelizable during training (175x faster for a sequence of length 512). Lastly, the study shows that these stripped-down versions of decade-old RNNs match the empirical performance of recent sequence models. <br>

25. ***Meta’s RLEF for improving code synthesis:  <br>Meta introduced a reinforcement learning method called RLEF for improving LLMs' performance in code synthesis tasks. The study achieved new state-of-the-art results by teaching models to iteratively improve code using execution feedback. This method significantly reduces the number of samples needed while boosting code generation accuracy.*** <br>
    Oct 2, Meta published a [paper](https://arxiv.org/pdf/2410.02089) “RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning”. Large language models (LLMs) deployed as agents solve user-specified tasks over multiple steps while keeping the required manual engagement to a minimum. Crucially, such LLMs need to ground their generations in any feedback obtained to reliably achieve desired outcomes. The study proposes an end-to-end reinforcement learning method for teaching models to leverage execution feedback in the realm of code synthesis, where state-of-the-art LLMs struggle to improve code iteratively compared to independent sampling. The study benchmarks on competitive programming tasks, where the authors achieve new start-of-the art results with both small (8B parameters) and large (70B) models while reducing the amount of samples required by an order of magnitude. The analysis of inference-time behavior demonstrates that this method produces LLMs that effectively leverage automatic feedback over multiple steps. <br>


29. ***Advancing Autonomous AI Agents with Reflective Tree Search: <br>Columbia University and Microsoft’s study introduces Reflective Monte Carlo Tree Search (R-MCTS) to improve autonomous agents in complex, multi-step decision-making tasks. By combining contrastive reflection and multi-agent debate, R-MCTS enables dynamic decision exploration and state evaluation for VLMs like GPT-4o. Through self-learning with R-MCTS-generated data, GPT-4o improves 6%-30% on tasks and achieves 97% of R-MCTS’s performance while using four times less compute, suggesting an effective approach for enhancing reasoning and planning in AI agents.*** <br>
    Oct 2, Columbia Uni and Microsoft published a [paper](https://arxiv.org/pdf/2410.01748) “Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning”. Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon planning tasks. To address these limitations, the study introduces Reflective Monte Carlo Tree Search (R-MCTS), a novel test-time algorithm designed to enhance the ability of AI agents, e.g., powered by GPT-4o, to explore decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating contrastive reflection, allowing agents to learn from past interactions and dynamically improve their search efficiency; and 2) using multi-agent debate to provide reliable state evaluation. Moreover, the study improves the agent's performance by fine-tuning GPT-4o through self-learning, using R-MCTS generated tree traversals without any human-provided labels. On the challenging VisualWebArena benchmark, the GPT-4o-based R-MCTS agent achieves a 6% to 30% relative improvement across various tasks compared to the previous state-of-the-art. Additionally, the study shows that the knowledge gained from test-time search can be effectively transferred back to GPT-4o via fine-tuning. The fine-tuned GPT-4o matches 97% of R-MCTS's performance while reducing compute usage by a factor of four at test time. Furthermore, qualitative results reveal that the fine-tuned GPT-4o model demonstrates the ability to explore the environment, evaluate a state, and backtrack to viable ones when it detects that the current state cannot lead to success. Moreover, the work demonstrates the compute scaling properties in both training - data collection with R-MCTS - and testing time. These results suggest a promising research direction to enhance VLMs' reasoning and planning capabilities for agentic applications via test-time search and self-learning. <br>

31. ***Nvidia's paper addresses the lack of comparable data for two key reward modeling approaches: Bradley-Terry style and Regression style. By adding preference annotations to complement existing ratings in their HelpSteer2 dataset, Nvidia enables the first head-to-head comparison of these models. They propose a novel method combining both approaches, leading to improved performance in alignment tasks with a new reward model (Llama-3.1-70B-Instruct). The study demonstrates strong results in RLHF (Reinforcement Learning from Human Feedback) and releases the dataset and trained model for public use.*** <br>
    Oct 2, Nvidia published a [paper](https://arxiv.org/pdf/2410.01257) “HelpSteer2-Preference: Complementing Ratings with Preferences”. Reward models are critical for aligning models to follow instructions, and are typically trained following one of two popular paradigms: Bradley-Terry style or Regression style. However, there is a lack of evidence that either approach is better than the other, when adequately matched for data. This is primarily because these approaches require data collected in different (but incompatible) formats, meaning that adequately matched data is not available in existing public datasets. To tackle this problem, the study releases preference annotations (designed for Bradley-Terry training) to complement existing ratings (designed for Regression style training) in the HelpSteer2 dataset. To improve data interpretability, preference annotations are accompanied with human-written justifications. Using this data, the study conducts the first head-to-head comparison of Bradley-Terry and Regression models when adequately matched for data. Based on insights derived from such a comparison, the study proposes a novel approach to combine Bradley-Terry and Regression reward modeling. A Llama-3.1-70B-Instruct model tuned with this approach scores 94.1 on RewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. The study also demonstrates the effectiveness of this reward model at aligning models to follow instructions in RLHF. The authors open-source this dataset (CC-BY-4.0 license) at https://huggingface.co/datasets/nvidia/HelpSteer2 and openly release the trained Reward Model at https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward <br>

33. ***Enhancing credit assignment in complex reasoning tasks <br>
This paper introduces VinePPO, a new method that improves the Proximal Policy Optimization (PPO) reinforcement learning algorithm, particularly for complex reasoning tasks in LLMs. The authors highlight the challenges faced by value networks in credit assignment and show that VinePPO consistently outperforms PPO and other baselines in fewer updates, using datasets like MATH and GSM8K. The work underscores the importance of better credit assignment mechanisms in reinforcement learning for LLMs.
Reward modeling comparison and combination <br>*** <br>
    Oct 2, Mila, Microsoft, McGill Uni, CIFAR, Uni de Montreal, and HEC Montreal published a [paper](https://arxiv.org/pdf/2410.01679) “VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment”. Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, value networks face challenges in predicting the expected cumulative rewards accurately in complex reasoning tasks, often leading to high-variance updates and suboptimal performance. This work systematically evaluates the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they barely outperform a random baseline when comparing alternative steps. To address this, the work proposes VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates, bypassing the need for large value networks. This method consistently outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These results emphasize the importance of accurate credit assignment in RL finetuning of LLM and demonstrate VinePPO's potential as a superior alternative. <br>

35. ***Zero-shot cross-lingual transfer using layer swapping <br>
UCLA's study presents a model merging approach aimed at improving cross-lingual transfer in large language models for mathematical reasoning tasks. The researchers fine-tune separate experts in math (English) and language and then merge their layers to enhance performance in target languages without in-language math data. The merged model improves by 10% across four languages on the MGSM benchmark, offering a simple and intuitive method for cross-lingual task adaptation.*** <br>
    Oct 2, UCLA published a [paper](https://arxiv.org/pdf/2410.01335) “Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models”. Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. This work presents a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. The study focuses on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, the work fine-tunes separate "experts" on math instruction data in English and on generic instruction data in the target language. The work then replaces the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc. <br>

37. ***Optimizing reasoning without overcoming probability sensitivity <br>
This paper examines OpenAI's o1 system, optimized for reasoning, and compares it to older LLMs. The study finds that while o1 shows significant improvements in complex tasks, it still shares the same sensitivity to probability as previous models. This sensitivity influences performance on low-probability tasks, highlighting a limitation in overcoming autoregressive trends in LLMs.*** <br>
    Oct 2, Yale Uni, OpenAI, Princeton Uni published a [paper](https://arxiv.org/pdf/2410.01792) “When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1”. In "Embers of Autoregression" (McCoy et al., 2023), the authors showed that several large language models (LLMs) have some important limitations that are attributable to their origins in next-word prediction. Here we investigate whether these issues persist with o1, a new system from OpenAI that differs from previous LLMs in that it is optimized for reasoning. The work finds that o1 substantially outperforms previous LLMs in many cases, with particularly large improvements on rare variants of common tasks (e.g., forming acronyms from the second letter of each word in a list, rather than the first letter). Despite these quantitative improvements, however, o1 still displays the same qualitative trends observed in previous systems. Specifically, o1 - like previous LLMs - is sensitive to the probability of examples and tasks, performing better and requiring fewer "thinking tokens" in high-probability settings than in low-probability ones. These results show that optimizing a language model for reasoning can mitigate but might not fully overcome the language model's probability sensitivity. <br>

39. ***Softmax limitations for out-of-distribution sharp decisions <br>
Google and the University of Oxford debunk the belief that softmax functions can robustly handle sharp decision-making in AI systems. They find that softmax circuits disperse with larger inputs, proving this through theoretical work. The authors propose adaptive temperature as a temporary fix but emphasize that softmax is fundamentally inadequate for tasks requiring sharp, consistent behavior on diverse inputs.*** <br>
    Oct 1, Google and Uni of Oxford published a [paper](https://arxiv.org/pdf/2410.01104) “softmax is not enough (for sharp out-of-distribution)”. A key property of reasoning systems is the ability to make sharp decisions on their input data. For contemporary AI systems, a key carrier of sharp behaviour is the softmax function, with its capability to perform differentiable query-key lookups. It is a common belief that the predictive power of networks leveraging softmax arises from "circuits" which sharply perform certain kinds of computations consistently across many diverse inputs. However, for these circuits to be robust, they would need to generalise well to arbitrary valid inputs. This paper dispels this myth: even for tasks as simple as finding the maximum key, any learned circuitry must disperse as the number of items grows at test time. The work attributes this to a fundamental limitation of the softmax function to robustly approximate sharp functions, prove this phenomenon theoretically, and propose adaptive temperature as an ad-hoc technique for improving the sharpness of softmax at inference time.   <br>

41. ***Detecting malicious prompts in vision-language models <br>
This paper introduces VLMGuard, a framework for detecting malicious prompts in vision-language models (VLMs) using unlabeled data. By estimating maliciousness scores and training a binary prompt classifier, VLMGuard outperforms existing methods without requiring extra human annotations. This approach highlights the practical need for reliable VLMs in real-world applications.*** <br>
    Oct 1, Uni of Wisconsin-Madison and Microsoft published a [paper](https://arxiv.org/pdf/2410.00296) “VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data”. Vision-language models (VLMs) are essential for contextual understanding of both visual and textual information. However, their vulnerability to adversarially manipulated inputs presents significant risks, leading to compromised outputs and raising concerns about the reliability in VLM-integrated applications. Detecting these malicious prompts is thus crucial for maintaining trust in VLM generations. A major challenge in developing a safeguarding prompt classifier is the lack of a large amount of labeled benign and malicious data. To address the issue, the work introduces VLMGuard, a novel learning framework that leverages the unlabeled user prompts in the wild for malicious prompt detection. These unlabeled prompts, which naturally arise when VLMs are deployed in the open world, consist of both benign and malicious information. To harness the unlabeled data, the study presents an automated maliciousness estimation score for distinguishing between benign and malicious samples within this unlabeled mixture, thereby enabling the training of a binary prompt classifier on top. Notably, the framework does not require extra human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiment shows VLMGuard achieves superior detection results, significantly outperforming state-of-the-art methods. Disclaimer: This paper may contain offensive examples; reader discretion is advised. <br>

43. ***Cross capabilities expose LLM performance weaknesses <br>
Meta and UIUC's research introduces CrossEval, a benchmark that tests LLMs' ability to handle tasks requiring multiple cross capabilities. The results show that LLMs tend to perform worse in these cross-capability tasks due to weaknesses in the least developed skills. The study suggests that addressing these weak links should be a research priority to improve LLMs' real-world utility in complex tasks.*** <br>
    Sep 30, Meta and UIUC published a [paper](https://arxiv.org/pdf/2409.19951) “Law of the Weakest Link: Cross Capabilities of Large Language Models”. The development and evaluation of Large Language Models (LLMs) have largely focused on individual capabilities. However, this overlooks the intersection of multiple abilities across different types of expertise that are often required for real-world tasks, which the authors term cross capabilities. To systematically explore this concept, the study first defines seven core individual capabilities and then pair them to form seven common cross capabilities, each supported by a manually constructed taxonomy. Building on these definitions, the study introduces CrossEval, a benchmark comprising 1,400 human-annotated prompts, with 100 prompts for each individual and cross capability. To ensure reliable evaluation, the study involves expert annotators to assess 4,200 model responses, gathering 8,400 human ratings with detailed explanations to serve as reference examples. The findings of the study reveal that, in both static evaluations and attempts to enhance specific abilities, current LLMs consistently exhibit the "Law of the Weakest Link," where cross-capability performance is significantly constrained by the weakest component. Specifically, across 58 cross-capability scores from 17 models, 38 scores are lower than all individual capabilities, while 20 fall between strong and weak, but closer to the weaker ability. These results highlight the under-performance of LLMs in cross-capability tasks, making the identification and improvement of the weakest capabilities a critical priority for future research to optimize performance in complex, multi-dimensional scenarios. <br>

45.	***Enhancing multimodal LLMs through data-centric training:  <br>MM1.5 is a significant upgrade of MM1. With one single set of weights, MM1.5 excels at (1) reading your charts, tables, and any text-rich images, (2) understanding visual prompts like points and boxes, providing grounded outputs, and (3) multi-image reasoning.*** <br>
Sep 30, Apple published a [paper](https://arxiv.org/pdf/2409.20566) “MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning”. The paper presents MM1.5, a new family of multimodal large language models (MLLMs) designed to enhance capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. Building upon the MM1 architecture, MM1.5 adopts a data-centric approach to model training, systematically exploring the impact of diverse data mixtures across the entire model training lifecycle. This includes high-quality OCR data and synthetic captions for continual pre-training, as well as an optimized visual instruction-tuning data mixture for supervised fine-tuning. The models range from 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE) variants, and demonstrate that careful data curation and training strategies can yield strong performance even at small scales (1B and 3B). Additionally, the work introduces two specialized variants: MM1.5-Video, designed for video understanding, and MM1.5-UI, tailored for mobile UI understanding. Through extensive empirical studies and ablations, the authors provide detailed insights into the training processes and decisions that inform the final designs, offering valuable guidance for future research in MLLM development. <br>

24. ***Compositional generalization through skill mixing in LLMs <br>
Princeton University's study examines compositional generalization, specifically how LLMs can combine multiple skills unseen during training. Using a SKILL-MIX evaluation, the authors show that training on smaller skill combinations enhances performance in more complex tasks. The study underscores the potential of incorporating skill-rich texts to boost models' generalization abilities.*** <br>
    Sep 29, Princeton Uni published a [paper](https://arxiv.org/pdf/2409.19808) “Can Models Learn Skill Composition from Examples”. As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalization -- the capacity to combine learned skills in novel ways not encountered during training -- has garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment. A recent study introduced the SKILL-MIX evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified k-tuple of language skills. While small models struggled with composing even with k=3, larger models like GPT-4 performed reasonably well with k=5 and 6. This study employs a setup akin to SKILL-MIX to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills -- including rhetorical, literary, reasoning, theory of mind, and common sense -- GPT-4 was used to generate text samples that exhibit random subsets of k skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of k, revealed the following findings: (1) Training on combinations of k=2 and 3 skills results in noticeable improvements in the ability to compose texts with k=4 and 5 skills, despite models never having seen such examples during training. (2) When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills. This study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models. <br>

26. ***Data-augmented prediction for LLM-based classification <br>
The University of Arizona's paper introduces "Language Model Learning" (LML) for classification tasks, powered by a new method called Data-Augmented Prediction (DAP). LML uses LLMs to understand and classify data by referencing relevant datasets, achieving high accuracy without traditional data cleaning and feature engineering. The study shows that LML could outperform conventional ML models in many scenarios.*** <br>
    Sep 28, Uni of Arizona published a [paper](https://arxiv.org/pdf/2409.18957) “LML: Language Model Learning a Dataset for Data-Augmented Prediction”. This paper introduces a new approach to using Large Language Models (LLMs) for classification tasks, which are typically handled using Machine Learning (ML) models. Unlike ML models that rely heavily on data cleaning and feature engineering, this method streamlines the process using LLMs. This paper proposes a new concept called "Language Model Learning (LML)" powered by a new method called "Data-Augmented Prediction (DAP)". The classification is performed by LLMs using a method similar to humans manually exploring and understanding the data and deciding classifications using data as a reference. Training data is summarized and evaluated to determine the features that lead to the classification of each label the most. In the process of DAP, the system uses the data summary to automatically create a query, which is used to retrieve relevant rows from the dataset. A classification is generated by the LLM using data summary and relevant rows, ensuring satisfactory accuracy even with complex data. Usage of data summary and similar data in DAP ensures context-aware decision-making. The proposed method uses the words "Act as an Explainable Machine Learning Model" in the prompt to enhance the interpretability of the predictions by allowing users to review the logic behind each prediction. In some test cases, the system scored an accuracy above 90%, proving the effectiveness of the system and its potential to outperform conventional ML models in various scenarios. The code is available at https://github.com/Pro-GenAI/LML-DAP <br>

28. ***Comprehensive evaluation of OpenAI’s o1 system <br>
A multi-institutional team evaluated OpenAI's o1-preview system, highlighting its human-level performance across diverse complex tasks such as programming, medical diagnosis, and financial modeling. The study reveals o1's remarkable progress toward artificial general intelligence (AGI), with strong reasoning capabilities across domains, although it still faces challenges with simpler tasks and certain specialized concepts.*** <br>
    Sep 27, about 40 researchers from different universities/institutes include Uni of Alberta, Uni of Georgia etc. published a [paper](https://arxiv.org/pdf/2409.18486) “Evaluation of OpenAI o1: Opportunities and Challenges of AGI”. This comprehensive study evaluates the performance of OpenAI's o1-preview large language model across a diverse array of complex reasoning tasks, spanning multiple domains, including computer science, mathematics, natural sciences, medicine, linguistics, and social sciences. Through rigorous testing, o1-preview demonstrated remarkable capabilities, often achieving human-level or superior performance in areas ranging from coding challenges to scientific reasoning and from language processing to creative problem-solving. Key findings include: 1) 83.3% success rate in solving complex competitive programming problems, surpassing many human experts. 2) Superior ability in generating coherent and accurate radiology reports, outperforming other evaluated models. 3) 100% accuracy in high school-level mathematical reasoning tasks, providing detailed step-by-step solutions. 4) Advanced natural language inference capabilities across general and specialized domains like medicine. 5) Impressive performance in chip design tasks, outperforming specialized models in areas such as EDA script generation and bug analysis. 6) Remarkable proficiency in anthropology and geology, demonstrating deep understanding and reasoning in these specialized fields. 7) Strong capabilities in quantitative investing. O1 has comprehensive financial knowledge and statistical modeling skills. 8) Effective performance in social media analysis, including sentiment analysis and emotion recognition. 
The model excelled particularly in tasks requiring intricate reasoning and knowledge integration across various fields. While some limitations were observed, including occasional errors on simpler problems and challenges with certain highly specialized concepts, the overall results indicate significant progress towards artificial general intelligence. <br>

30. ***Generative AI in data analysis workflows <br>
Microsoft's paper explores the potential of generative AI in reshaping data analysis workflows. The authors discuss human-centered design principles that help facilitate intuitive interactions and build user trust. The paper also highlights the challenges in developing AI tools, such as improving model capabilities and ensuring they meet end-user needs.*** <br>
    Sep 27, Microsoft published a [paper](https://arxiv.org/pdf/2409.18475) “Data Analysis in the Era of Generative AI”. This paper explores the potential of AI-powered tools to reshape data analysis, focusing on design considerations and challenges. The paper explores how the emergence of large language and multimodal models offers new opportunities to enhance various stages of data analysis workflow by translating high-level user intentions into executable code, charts, and insights. The authors then examine human-centered design principles that facilitate intuitive interactions, build user trust, and streamline the AI-assisted analysis workflow across multiple apps. Finally, the paper discusses the research challenges that impede the development of these AI-based systems such as enhancing model capabilities, evaluating and benchmarking, and understanding end-user needs. <br>

32. ***Efficient low-bit quantization for large language models <br>
This paper introduces VPTQ, a new method for extremely low-bit quantization in LLMs, reducing memory and computational requirements while maintaining accuracy. The method uses vector quantization and second-order optimization, leading to significant improvements in quantization perplexity and model performance across several benchmarks. VPTQ offers a solution for deploying large-scale models more efficiently.*** <br>
    Sep 25, Microsoft and USTC published a [paper](https://arxiv.org/pdf/2409.17066) “VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models”.  Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit. Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables. This paper introduces Vector Post-Training Quantization (VPTQ) for extremely low-bit quantization of LLMs. The study uses Second-Order Optimization to formulate the LLM VQ problem and guide the quantization algorithm design by solving the optimization. The work further refines the weights using Channel-Independent Second-Order Optimization for a granular VQ. In addition, by decomposing the optimization problem, the study proposes a brief and effective codebook initialization algorithm. The authors also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model. The experimental results show that VPTQ reduces model quantization perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, 4.41-7.34 on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, 11-22% on LLaMA-3 on QA tasks on average. We only utilize 10.4-18.6% of the quantization algorithm execution time, resulting in a 1.6-1.8times increase in inference throughput compared to SOTA. <br>

34. ***Scalable time series forecasting with Time-MoE <br>
Princeton University introduces Time-MoE, a mixture-of-experts model designed to improve time series forecasting while reducing computational costs. By pre-training on a large dataset spanning multiple domains, Time-MoE demonstrates superior performance compared to dense models, establishing it as a new state-of-the-art in time series forecasting.*** <br>
    Sep 24, Princeton Uni published a [paper](https://arxiv.org/pdf/2409.16040) “Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts”. Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, the study introduces Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. The study pre-trained these models on a newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, the study scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Experimental results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, the models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility. Code is available here.
 <br><br><br>

***Sep 29***

1. ***YOLO11 Released by Ultralytics:   <br>YOLO11 introduces enhanced speed, accuracy, and versatility, surpassing its predecessors in computer vision tasks like real-time object detection and classification. Key features include improved feature extraction, fewer parameters, and faster processing, making YOLO11 a game-changer for developers and researchers. It also extends its capabilities to advanced tasks such as pose estimation and instance segmentation.*** <br><br>
   Sep 27, Ultralytics [released YOLO 11](https://www.ultralytics.com/blog/ultralytics-yolo11-has-arrived-redefine-whats-possible-in-ai), the latest AI model redefining computer vision with unmatched accuracy and efficiency. YOLO11! Building on the impressive advancements of previous YOLO model versions, YOLO11 brings a host of powerful features and optimizations that make it faster, more accurate, and incredibly versatile. With its innovative architecture, YOLO11 can be used for various computer vision tasks, from real-time object detection to classification, making it a game-changer for developers and researchers alike. Key improvements include enhanced feature extraction for more precise detail capture, greater accuracy with fewer parameters, and faster processing speeds that significantly improve real-time performance. YOLO11 marks a new chapter for the YOLO family, offering a more capable and versatile model that takes computer vision to new heights. With its refined architecture and enhanced capabilities, the model supports computer vision tasks like pose estimation and instance segmentation that the Vision AI community has come to love about Ultralytics YOLOv8, but with even greater performance and precision. <br><br>

3. ***Meta’s Llama 3.2 Multimodal Models:   <br>Llama 3.2 is a collection of multilingual large language models (LLMs) available in text and image processing formats. It includes 1B and 3B text models designed for on-device tasks and larger 11B and 90B multimodal models optimized for image understanding. Meta is also simplifying Llama deployment across various environments, emphasizing the importance of openness for driving innovation.*** <br><br>
   Sep 26, Meta [released Llama 3.2](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/), the lightweight and multimodal models. The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge. The Llama 3.2 11B and 90B vision models are drop-in replacements for their corresponding text model equivalents, while exceeding on image understanding tasks compared to closed models, such as Claude 3 Haiku. Meta is sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety. Meta will continue to share its work because it believes [openness drives innovation](https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/) and is good for developers, Meta, and the world. <br><br>
 
5. ***FPT Software AI Center’s HyperAgent for Software Engineering:   <br>HyperAgent is a generalist multi-agent system designed to handle a wide range of software engineering (SE) tasks. It uses four specialized agents (Planner, Navigator, Code Editor, and Executor) to manage SE tasks from conception to verification. The system achieves state-of-the-art performance across diverse tasks, significantly improving coding practices.*** <br><br>
   Sep 26, FPT Sofware AI Center published a [paper](https://arxiv.org/pdf/2409.16299) “HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks at Scale”. Large Language Models (LLMs) have revolutionized software engineering (SE), demonstrating remarkable capabilities in various coding tasks. While recent efforts have produced autonomous software agents based on LLMs for end-to-end development tasks, these systems are typically designed for specific SE tasks. The study introduces HyperAgent, a novel generalist multi-agent system designed to address a wide spectrum of SE tasks across different programming languages by mimicking human developers' workflows. Comprising four specialized agents - Planner, Navigator, Code Editor, and Executor. HyperAgent manages the full lifecycle of SE tasks, from initial conception to final verification. Through extensive evaluations, HyperAgent achieves state-of-the-art performance across diverse SE tasks: it attains a 25.01% success rate on SWE-Bench-Lite and 31.40% on SWE-Bench-Verified for GitHub issue resolution, surpassing existing methods. Furthermore, HyperAgent demonstrates SOTA performance in repository-level code generation (RepoExec), and in fault localization and program repair (Defects4J), often outperforming specialized systems. This work represents a significant advancement towards versatile, autonomous agents capable of handling complex, multi-step SE tasks across various domains and languages, potentially transforming AI-assisted software development practices. <br><br>

7. ***Leadership Shakeup at OpenAI:   <br>Several key executives, including CTO Mira Murati, left OpenAI, raising concerns about leadership stability as the company pursues a controversial growth strategy. OpenAI is in talks for a new fundraising round, but the leadership exits leave CEO Sam Altman with fewer key leaders during this critical time of restructuring.*** <br><br>
   Sep 26, [according to CNN](https://edition.cnn.com/2024/09/25/tech/openai-technology-chief-mira-murati-leaving/index.html?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=meta-launches-orion-at-meta-connect&_bhlid=eab99b8c7f02019f455b5492c1bac68fde7e1f07), Three more execs out at OpenAI, including technology chief Mira Murati. Murati — who has been instrumental in the development of ChatGPT and the artificial intelligence image generator Dall-E — said Wednesday afternoon in a post on X that she is leaving the company. Shortly after, OpenAI’s Chief Research Officer Bob McGrew and Vice President of Research Barret Zoph also announced their decisions to exit. The Wednesday departures are just the latest in a string of executives who have recently left OpenAI. The leadership shakeup comes as the ChatGPT-maker attempts to forge a controversial path to growth, including making it easier to raise funds from investors and generate revenue. OpenAI is reportedly in talks about a new fundraising round that could value the firm at $150 billion, Bloomberg and others have reported. Later on Wednesday, Altman updated his post to acknowledge the exits of Zoph and McGrew. Taken together, the string of departures leave CEO Sam Altman without much of the leadership team that helped him rapidly grow OpenAI into an artificial intelligence juggernaut, and could consolidate power under Altman just as the company seeks to restructure. OpenAI did not immediately respond to a question about the timeline for Murati’s formal exit for the company, or if and when a new CTO would be announced. <br><br>

9. ***Cambridge and Google’s “DARE” for Visual Question Answering:   <br>The DARE study introduces a novel approach to image captioning using Image-like Retrieval and a Fusion Module to bridge the modality gap between text training and image inference. The approach significantly improves caption quality and outperforms state-of-the-art methods in both image and video captioning tasks.*** <br><br>
    Sep 26, Uni of Cambridge and Google published a [paper](https://arxiv.org/pdf/2409.18023) “DARE: Diverse Visual Question Answering with Robustness Evaluation”. Recent advancements in image captioning have explored text-only training methods to overcome the limitations of paired image-text data. However, existing text-only training methods often overlook the modality gap between using text data during training and employing images during inference. To address this issue, the study proposes a novel approach called Image-like Retrieval, which aligns text features with visually relevant features to mitigate the modality gap. The method further enhances the accuracy of generated captions by designing a Fusion Module that integrates retrieved captions with input features. Additionally, the study introduces a Frequency-based Entity Filtering technique that significantly improves caption quality. The authors integrate these methods into a unified framework, which the authors refer to as IFCap (Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning). Through extensive experimentation, the straightforward yet powerful approach has demonstrated its efficacy, outperforming the state-of-the-art methods by a significant margin in both image captioning and video captioning compared to zero-shot captioning based on text-only training. <br><br>

11. ***Challenges in Vision-Language Compositionality for CLIP:   <br>The study reveals that current benchmarks overestimate improvements in compositionality for vision-language models like CLIP when trained with hard negatives. Including both hard positives and negatives in the training dataset improves model performance, suggesting the need for more robust benchmarks.*** <br><br>
    Sep 26, Uni of Washington, Uni of California Los Angeles and Allen Inst of AI published a [paper](https://arxiv.org/pdf/2409.17958) on ECCV2024 “The Hard Positive Truth about Vision-Language Compositionality”. Several benchmarks have concluded that the best vision-language models (e.g., CLIP) are lacking in compositionality. Given an image, these benchmarks probe a model's ability to identify its associated caption amongst a set of compositional distractors. In response, a surge of recent proposals show improvements by finetuning CLIP with distractors as hard negatives. The investigations reveal that these improvements have, in fact, been significantly overstated -- because existing benchmarks do not probe whether finetuned vision-language models remain invariant to hard positives. By curating an evaluation dataset with 112,382 hard negatives and hard positives, the authors uncover that including hard positives decreases CLIP's performance by 12.9%, while humans perform effortlessly at 99%. CLIP finetuned with hard negatives results in an even larger decrease, up to 38.7%. With this finding, the study then produces a 1,775,259 image-text training set with both hard negative and hard positive captions. By training with both, the authors see improvements on existing benchmarks while simultaneously improving performance on hard positives, indicating a more robust improvement in compositionality. The work suggests the need for future research to rigorously test and improve CLIP's understanding of semantic relationships between related "positive" concepts. <br><br>

13. ***Larger Language Models Becoming Less Reliable:   <br>A Nature study shows that while scaling up and refining large language models (LLMs) improve certain capabilities, they also introduce new reliability issues. Larger models tend to provide plausible but incorrect answers more frequently and struggle with task stability. The findings call for a shift in AI development strategies.*** <br><br>
    Sep 25, Nature published a [paper](https://www.nature.com/articles/s41586-024-07930-y.pdf) “Larger and more instructable language models become less reliable”. The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources) and bespoke shaping up (including post-filtering, fine tuning or use of human feedback). However, larger and more instructable large language models may have become less reliable. By studying the relationship between difficulty concordance, task avoidance and prompting stability of several language model families, the study shows that easy instances for human participants are also easy for the models, but scaled-up, shaped-up models do not secure areas of low difficulty in which either the model does not err or human supervision can spot the errors. The study also finds that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, The study observes that stability to different natural phrasings of the same question is improved by scaling-up and shaping-up interventions, but pockets of variability persist across difficulty levels. These findings highlight the need for a fundamental shift in the design and development of general-purpose artificial intelligence, particularly in high-stakes areas for which a predictable distribution of errors is paramount. <br><br>

15. ***Molmo and PixMo: Open Multimodal Models:   <br>Molmo introduces a family of state-of-the-art multimodal models built entirely on open data and weights. It relies on human-annotated datasets for image captioning and outperforms both open and proprietary models like GPT-4o. Molmo emphasizes openness in model development, aiming to democratize advanced AI capabilities.*** <br><br>
    Sep 25, Allen Inst of AI and Uni of Washington published a [paper](https://arxiv.org/pdf/2409.17146) “Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models”. Today's most advanced multimodal models remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed models into open ones. As a result, the community is still missing foundational knowledge about how to build performant VLMs from scratch. The study presents Molmo, a new family of VLMs that are state-of-the-art in their class of openness. The key innovation is a novel, highly detailed image caption dataset collected entirely from human annotators using speech-based descriptions. To enable a wide array of user interactions, the study also introduces a diverse dataset mixture for fine-tuning that includes in-the-wild Q&A and innovative 2D pointing data. The success of the approach relies on careful choices for the model architecture details, a well-tuned training pipeline, and, most critically, the quality of the newly collected datasets, all of which will be released. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human evaluation. The athors will be releasing all of the model weights, captioning and fine-tuning data, and source code in the near future. Select model weights, inference code, and demo are available at https://molmo.allenai.org. <br><br>

17. ***VectorSearch: Optimized Document Retrieval:   <br>VectorSearch addresses challenges in semantic retrieval by using advanced embeddings and indexing techniques. It improves accuracy in large-scale document retrieval tasks by closing the semantic gaps that traditional methods and deep learning approaches struggle with.*** <br><br>
    Sep 25, Uni of Washington published a [paper](https://arxiv.org/pdf/2409.17383) “VectorSearch: Enhancing Document Retrieval with Semantic Embeddings and Optimized Search”. Traditional retrieval methods have been essential for assessing document similarity but struggle with capturing semantic nuances. Despite advancements in latent semantic analysis (LSA) and deep learning, achieving comprehensive semantic understanding and accurate retrieval remains challenging due to high dimensionality and semantic gaps. The above challenges call for new techniques to effectively reduce the dimensions and close the semantic gaps. To this end, the study proposes VectorSearch, which leverages advanced algorithms, embeddings, and indexing techniques for refined retrieval. By utilizing innovative multi-vector search operations and encoding searches with advanced language models, the approach significantly improves retrieval accuracy. Experiments on real-world datasets show that VectorSearch outperforms baseline metrics, demonstrating its efficacy for large-scale retrieval tasks. <br><br>

19. ***Sam Altman’s “The Intelligence Age” Vision:   <br>Altman envisions a future driven by AI that will enable extraordinary societal advancements. He emphasizes that AI can solve complex problems, improve global living standards, and usher in an era of prosperity, but warns of potential economic disruptions and ethical concerns that must be navigated carefully.*** <br><br>
    Sep 23, Sam Altman published an [article](https://ia.samaltman.com/) “The Intelligence Age”. The article envisions a future where AI and technology enable extraordinary advancements that seem magical by today’s standards. This acceleration of progress builds on the accomplishments of previous generations, with AI acting as a tool to solve complex problems and enhance human capabilities. The author emphasizes how deep learning, with its ability to learn patterns in data, has driven recent breakthroughs and will continue to do so as compute and data scale. AI will enable personalized services such as education and healthcare, and lead to unprecedented prosperity. While prosperity alone may not guarantee happiness, it can improve global living standards. The transition from the Stone Age to the Intelligence Age is likened to previous technological revolutions, requiring energy, compute power, and wise decisions. The article warns of potential challenges, including economic shifts and ethical concerns, but advocates for navigating risks thoughtfully to maximize benefits. Ultimately, the future is seen as one of limitless possibilities, where AI-driven advancements will reshape society, leading to breakthroughs like fixing climate change and space exploration. Although AI may disrupt jobs, the author believes human creativity and purpose will persist, ushering in a new era of collective growth and shared progress. <br><br>

21. ***Exploring OpenAI’s o1 in Medicine:   <br>OpenAI's o1 model shows promise in medical tasks, outperforming previous models like GPT-4 in reasoning and multilingual understanding across 37 datasets. However, the study also identifies weaknesses, such as hallucination and inconsistent multilingual abilities, highlighting areas for improvement in AI for medicine.*** <br><br>
    Sep 23, UC Santa Cruz, Uni of Edinburgh and NIH published a [paper](https://arxiv.org/pdf/2409.15277) “A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?”. Large language models (LLMs) have exhibited remarkable capabilities across various domains and tasks, pushing the boundaries of human knowledge in learning and cognition. The latest model, OpenAI's o1, stands out as the first LLM with an internalized chain-of-thought technique using reinforcement learning strategies. While it has demonstrated surprisingly strong capabilities on various general language tasks, its performance in specialized fields such as medicine remains unknown. To this end, this report provides a comprehensive exploration of o1 on different medical scenarios, examining 3 key aspects: understanding, reasoning, and multilinguality. Specifically, this evaluation encompasses 6 tasks using data from 37 medical datasets, including two newly constructed and more challenging question-answering (QA) tasks based on professional medical quizzes from the New England Journal of Medicine (NEJM) and The Lancet. These datasets offer greater clinical relevance compared to standard medical QA benchmarks such as MedQA, translating more effectively into real-world clinical utility. The analysis of o1 suggests that the enhanced reasoning ability of LLMs may (significantly) benefit their capability to understand various medical instructions and reason through complex clinical scenarios. Notably, o1 surpasses the previous GPT-4 in accuracy by an average of 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios. But meanwhile, the authors identify several weaknesses in both the model capability and the existing evaluation protocols, including hallucination, inconsistent multilingual ability, and discrepant metrics for evaluation. Raw data and model outputs is available at https://ucsc-vlaa.github.io/o1_medicine/ for future research. <br><br>

23. ***Phantom LLVMs for Efficient AI:   <br>The Phantom LLVM family offers smaller, more efficient models with strong performance in vision-language tasks. By temporarily expanding hidden layers during self-attention, Phantom achieves the performance of larger models while maintaining a smaller physical size, offering a leading solution for efficient large models.*** <br><br>
    Sep 23, KAIST published a [paper](https://arxiv.org/pdf/2409.14713) “Phantom of Latent for Large Language and Vision Models”. The success of visual instruction tuning has accelerated the development of large language and vision models (LLVMs). Following the scaling laws of instruction-tuned large language models (LLMs), LLVMs either have further increased their sizes, reaching 26B, 34B, and even 80B parameters. While this increase in model size has yielded significant performance gains, it demands substantially more hardware resources for both training and inference. Consequently, there naturally exists a strong need for efficient LLVMs that achieve the performance of larger models while being smaller in size. To achieve this need, the paper presents a new efficient LLVM family with model sizes of 0.5B, 1.8B, 3.8B, and 7B parameters, Phantom, which significantly enhances learning capabilities within limited structures. By temporarily increasing the latent hidden dimension during multi-head self-attention (MHSA), the authors make LLVMs prepare to look and understand much more vision-language knowledge on the latent, without substantially increasing physical model sizes. To maximize its advantage, the authors introduce Phantom Optimization (PO) using both autoregressive supervised fine-tuning (SFT) and direct preference optimization (DPO)-like concept, which effectively follows correct answers while eliminating incorrect and ambiguous ones. Phantom outperforms numerous larger open- and closed-source LLVMs, positioning itself as a leading solution in the landscape of efficient LLVMs. Code is available in https://github.com/ByungKwanLee/Phantom. <br><br>

25. ***Rethinking Machine Learning Principles for Scaling:   <br>A Google paper explores how traditional machine learning principles, such as regularization, are becoming less relevant in the era of scaling large models. The study highlights a phenomenon called “scaling law crossover,” where methods effective at smaller scales fail to generalize to larger ones, urging the development of new guiding principles for scaling.*** <br><br>
    Sep 23, Google published a [paper](https://arxiv.org/pdf/2409.15156) “Rethinking Conventional Wisdom in Machine Learning: From Generalization to Scaling”. The remarkable success of large language pretraining and the discovery of scaling laws signify a paradigm shift in machine learning. Notably, the primary objective has evolved from minimizing generalization error to reducing approximation error, and the most effective strategy has transitioned from regularization (in a broad sense) to scaling up models. This raises a critical question: Do the established principles that proved successful in the generalization-centric era remain valid in this new era of scaling? This paper examines several influential regularization-based principles that may no longer hold true in the scaling-centric, large language model (LLM) era. These principles include explicit L2 regularization and implicit regularization through small batch sizes and large learning rates. Additionally, the study identifies a new phenomenon termed “scaling law crossover,” where two scaling curves intersect at a certain scale, implying that methods effective at smaller scales may not generalize to larger ones. Together, these observations highlight two fundamental questions within this new paradigm: 1) Guiding Principles for Scaling: If regularization is no longer the primary guiding principle for model design, what new principles are emerging to guide scaling? 2) Model Comparison at Scale: How to reliably and effectively compare models at the scale where only a single experiment is feasible? <br><br>

27. ***Improving AI Safety with Backtracking:   <br>A new method called backtracking, introduced by Meta and CMU, allows language models to recover from unsafe text generation. By incorporating a [RESET] token, models trained with this method significantly improve safety without losing helpfulness, offering better protection against adversarial attacks.*** <br><br>
    Sep 22, Meta and CMU published a [paper](https://arxiv.org/pdf/2409.14586) “Backtracking Improves Generation Safety”. Text generation has a fundamental limitation almost by definition: there is no taking back tokens that have been generated, even when they are clearly problematic. In the context of language model safety, when a partial unsafe generation is produced, language models by their nature tend to happily keep on generating similarly unsafe additional text. This is in fact how safety alignment of frontier models gets circumvented in the wild, despite great efforts in improving their safety. Deviating from the paradigm of approaching safety alignment as prevention (decreasing the probability of harmful responses), the paper proposes backtracking, a technique that allows language models to "undo" and recover from their own unsafe generation through the introduction of a special [RESET] token. The method can be incorporated into either SFT or DPO training to optimize helpfulness and harmlessness. The work shows that models trained to backtrack are consistently safer than baseline models: backtracking Llama-3-8B is four times more safe than the baseline model (6.1\% → 1.5\%) in the evaluations without regression in helpfulness. The method additionally provides protection against four adversarial attacks including an adaptive attack, despite not being trained to do so. <br><br>

29. ***Mitigating Reward Hacking in AI Models:   <br>A new framework introduced by Google and UIUC improves reward model training by disentangling relevant preferences from artifacts like response length. This robust reward model enhances AI's alignment with human preferences, significantly improving the performance of reward-based policies in large-scale AI systems.*** <br><br>
    Sep 20, Google, UIUC et al published a [paper](https://arxiv.org/pdf/2409.13156) “RRM: Robust Reward Model Training Mitigates Reward Hacking”. Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. However, traditional RM training, which relies on response pairs tied to specific prompts, struggles to disentangle prompt-driven preferences from prompt-independent artifacts, such as response length and format. This work exposes a fundamental limitation of current RM training methods, where RMs fail to effectively distinguish between contextual signals and irrelevant artifacts when determining preferences. To address this, the work introduces a causal framework that learns preferences independent of these artifacts and propose a novel data augmentation technique designed to eliminate them. Extensive experiments show that the approach successfully filters out undesirable artifacts, yielding a more robust reward model (RRM). The RRM improves the performance of a pairwise reward model trained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to 84.15%. Additionally, the authors train two DPO policies using both the RM and RRM, demonstrating that the RRM significantly enhances DPO-aligned policies, improving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in AlpacaEval-2 from 33.46% to 52.49%. <br><br>

31. ***LLMs' Role in Retrieval-Augmented Generation (RAG)  <br>A paper from Harvard and Google introduces "FRAMES" to evaluate LLMs' performance in retrieval-augmented generation (RAG). The dataset tests LLMs' factual response accuracy, retrieval efficiency, and reasoning ability through complex multi-hop questions. Current state-of-the-art LLMs show improved accuracy from 0.40 to 0.66 with multi-step retrieval, but there remains significant room for improvement in creating robust RAG systems.*** <br><br>
    Sep 20, Harvard Uni and Google published a [paper](https://arxiv.org/pdf/2409.12941) “Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation”. Large Language Models (LLMs) have demonstrated significant performance improvements across various cognitive tasks. An emerging application is using LLMs to enhance retrieval-augmented generation (RAG) capabilities. These systems require LLMs to understand user queries, retrieve relevant information, and synthesize coherent and accurate responses. Given the increasing real-world deployment of such systems, comprehensive evaluation becomes crucial. To this end, the study proposes FRAMES (Factuality, Retrieval, And reasoning MEasurement Set), a high-quality evaluation dataset designed to test LLMs' ability to provide factual responses, assess retrieval capabilities, and evaluate the reasoning required to generate final answers. While previous work has provided datasets and benchmarks to evaluate these abilities in isolation, FRAMES offers a unified framework that provides a clearer picture of LLM performance in end-to-end RAG scenarios. The dataset comprises challenging multi-hop questions that require the integration of information from multiple sources. The authors present baseline results demonstrating that even state-of-the-art LLMs struggle with this task, achieving 0.40 accuracy with no retrieval. The accuracy is significantly improved with the proposed multi-step retrieval pipeline, achieving an accuracy of 0.66 (>50% improvement). The authors hope the work will help bridge evaluation gaps and assist in developing more robust and capable RAG systems. <br><br>

33. ***LLMs' Planning Abilities Examined via PlanBench  <br>Arizona State University evaluates OpenAI’s "o1" (Strawberry) model, which claims to improve LLMs' planning capabilities, a core competence in AI. Despite surpassing previous models in planning tasks on the PlanBench benchmark, o1 still falls short of fully solving the benchmark's challenges. This raises questions about the efficiency, accuracy, and reliability of such systems before deployment.*** <br><br>
    Sep 20, Arizona State Uni published a [paper](https://www.arxiv.org/pdf/2409.13373) “LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench”. The ability to plan a course of action that achieves a desired state of affairs has long been considered a core competence of intelligent agents and has been an integral part of AI research since its inception. With the advent of large language models (LLMs), there has been considerable interest in the question of whether or not they possess such planning abilities. PlanBench, an extensible benchmark developed in 2022, soon after the release of GPT3, has remained an important tool for evaluating the planning abilities of LLMs. Despite the slew of new private and open source LLMs since GPT3, progress on this benchmark has been surprisingly slow. OpenAI claims that their recent o1 (Strawberry) model has been specifically constructed and trained to escape the normal limitations of autoregressive LLMs--making it a new kind of model: a Large Reasoning Model (LRM). Using this development as a catalyst, this paper takes a comprehensive look at how well current LLMs and new LRMs do on PlanBench. While o1's performance is a quantum improvement on the benchmark, outpacing the competition, it is still far from saturating it. This improvement also brings to the fore questions about accuracy, efficiency, and guarantees which must be considered before deploying such systems. <br><br>

35. ***Michelangelo's Long-Context Evaluation Framework  <br>Google introduces Michelangelo, a synthetic evaluation for testing LLMs' long-context reasoning abilities through a novel framework called Latent Structure Queries (LSQ). The evaluation focuses on how well models handle large, complex contexts by identifying hidden structures. The study demonstrates that LLMs show significant potential in long-context processing, but there remains much room for improvement.*** <br><br>
    Sep 20, Google published a [paper](https://arxiv.org/pdf/2409.12640) “Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries”. The paper introduces Michelangelo: a minimal, synthetic, and unleaked long-context reasoning evaluation for large language models which is also easy to automatically score. This evaluation is derived via a novel, unifying framework for evaluations over arbitrarily long contexts which measure the model's ability to do more than retrieve a single piece of information from its context. The central idea of the Latent Structure Queries framework (LSQ) is to construct tasks which require a model to “chisel away” the irrelevant information in the context, revealing a latent structure in the context. To verify a model's understanding of this latent structure, the study queries the model for details of the structure. Using LSQ, the authors produce three diagnostic long-context evaluations across code and natural-language domains intended to provide a stronger signal of long-context language model capabilities. The authors perform evaluations on several state-of-the-art models and demonstrate both that a) the proposed evaluations are high-signal and b) that there is significant room for improvement in synthesizing long-context information. <br><br>

37. ***OpenAI O1’s Use of External SAT Solvers  <br>The University of Florence analyzes OpenAI’s O1-preview model for solving K-SAT problems. Results indicate that the model often relies on external SAT solvers rather than solving the problems internally. The study critiques the model for occasionally outputting incorrect solutions and questions whether the model exhibits true intelligence or is making random guesses.*** <br><br>
    Sep 20, Uni of Florence published a [paper](https://arxiv.org/pdf/2409.11232) “Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT Problem: Does the LLM Solve the Problem Itself or Call an External SAT Solver?” The manuscript  presents an analysis on the performance of OpenAI O1-preview model in solving random K-SAT instances for K∈ 2, 3, 4 as a function of α = M/N where M is the number of clauses and N is the number of variables of the satisfiable problem. The study shows that the model can call an external SAT solver to solve the instances, rather than solving them directly. Despite using external solvers, the model reports incorrect assignments as output. Moreover, the study proposes and presents an analysis to quantify whether the OpenAI O1-preview model demonstrates a spark of intelligence or merely makes random guesses when outputting an assignment for a Boolean satisfiability problem. <br><br>

39. ***Evaluating Foundation Models' Understanding of Emotions  <br>A study by Stanford and UT Austin introduces a framework to evaluate AI's affective cognition abilities, exploring how models infer emotions and scenarios. Using diverse scenarios, the study finds that foundation models, including GPT-4, match or surpass human agreement in many cases. The models perform well in understanding emotional dynamics, especially when using chain-of-thought reasoning, suggesting a human-like grasp of emotions.*** <br><br>
    Sep 19, Stanford Uni and Uni of Texas Austin published a [paper](https://arxiv.org/pdf/2409.11733) “Human-like Affective Cognition in Foundation Models”. Understanding emotions is fundamental to human interaction and experience. Humans easily infer emotions from situations or facial expressions, situations from emotions, and do a variety of other affective cognition. How adept is modern AI at these inferences? The study introduces an evaluation framework for testing affective cognition in foundation models. Starting from psychological theory, the study generates 1,280 diverse scenarios exploring relationships between appraisals, emotions, expressions, and outcomes. The authors evaluate the abilities of foundation models (GPT-4, Claude-3, Gemini-1.5-Pro) and humans (N = 567) across carefully selected conditions. Experimental results show foundation models tend to agree with human intuitions, matching or exceeding interparticipant agreement. In some conditions, models are “superhuman” -- they better predict modal human judgements than the average human. All models benefit from chain-of-thought reasoning. This suggests foundation models have acquired a human-like understanding of emotions and their influence on beliefs and behavior. <br><br>

41. ***Exploring Safety Gaps in Fine-tuned LLMs  <br>This paper investigates how fine-tuning LLMs on downstream tasks like translation and code generation compromises their safety guardrails. The results show significant safety degradation, with harmful responses reaching 73-92% in some cases. The authors propose a new multitask safety dataset that reduces these risks, highlighting the need for robust, cross-task safety measures in LLM development.*** <br><br>
    Sep 18, Lahore Uni, NYU and Meta published a [paper](https://www.arxiv.org/pdf/2409.15361) “Multitask Mayhem: Unveiling and Mitigating Safety Gaps in LLMs Fine-tuning”. Recent breakthroughs in Large Language Models (LLMs) have led to their adoption across a wide range of tasks, ranging from code generation to machine translation and sentiment analysis, etc. Red teaming/Safety alignment efforts show that fine-tuning models on benign (non-harmful) data could compromise safety. However, it remains unclear to what extent this phenomenon is influenced by different variables, including fine-tuning task, model calibrations, etc. This paper explores the task-wise safety degradation due to fine-tuning on downstream tasks such as summarization, code generation, translation, and classification across various calibration. Experimental results reveal that: 1) Fine-tuning LLMs for code generation and translation leads to the highest degradation in safety guardrails. 2) LLMs generally have weaker guardrails for translation and classification, with 73-92% of harmful prompts answered, across baseline and other calibrations, falling into one of two concern categories. 3) Current solutions, including guards and safety tuning datasets, lack cross-task robustness. To address these issues, the paper developed a new multitask safety dataset effectively reducing attack success rates across a range of tasks without compromising the model's overall helpfulness. The work underscores the need for generalized alignment measures to ensure safer and more robust models. <br><br>

43. ***Improving Error Analysis with SemSlicer  <br>Carnegie Mellon introduces SemSlicer, a framework for semantic data slicing that helps identify systematic issues in machine learning models. The framework uses LLMs to annotate datasets, offering more flexible and accurate slicing than traditional methods. SemSlicer helps practitioners pinpoint underperforming slices and uncover systematic model errors efficiently.*** <br><br>
    Sep 14, CMU published a [paper](https://www.arxiv.org/pdf/2409.09261) “What Is Wrong with My Model? Identifying Systematic Problems with Semantic Data Slicing”. Machine learning models make mistakes, yet sometimes it is difficult to identify the systematic problems behind the mistakes. Practitioners engage in various activities, including error analysis, testing, auditing, and red-teaming, to form hypotheses of what can go (or has gone) wrong with their models. To validate these hypotheses, practitioners employ data slicing to identify relevant examples. However, traditional data slicing is limited by available features and programmatic slicing functions. This work proposes SemSlicer, a framework that supports semantic data slicing, which identifies a semantically coherent slice, without the need for existing features. SemSlicer uses Large Language Models to annotate datasets and generate slices from any user-defined slicing criteria. The work shows that SemSlicer generates accurate slices with low cost, allows flexible trade-offs between different design dimensions, reliably identifies under-performing data slices, and helps practitioners identify useful data slices that reflect systematic problems.
 <br><br><br>

***Sep 22***

1. ***Apple's Exploration of HyperCloning for Efficient LLM Training:  <br>Apple introduced "HyperCloning," a method to speed up large language model (LLM) training by initializing large models using pre-trained smaller models. This technique preserves the smaller model's predictive accuracy while reducing the GPU hours required for large-scale pre-training.*** <br><br>
   Sep 20, Apple published a [paper](https://arxiv.org/pdf/2409.12903) “Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization”. The pre-training phase of language models often begins with randomly initialized parameters. With the current trends in scaling models, training their large number of parameters can be extremely slow and costly. In contrast, small language models are less expensive to train, but they often cannot achieve the accuracy of large models. The study explores an intriguing idea to connect these two different regimes: Can we develop a method to initialize large language models using smaller pre-trained models? Will such initialization bring any benefits in terms of training time and final accuracy? This paper introduces HyperCloning, a method that can expand the parameters of a pre-trained language model to those of a larger model with increased hidden dimensions. The method ensures that the larger model retains the functionality of the smaller model. As a result, the larger model already inherits the predictive power and accuracy of the smaller model before the training starts. The study demonstrates that training such an initialized model results in significant savings in terms of GPU hours required for pre-training large language models.

3. ***RLHF Leading to Misleading Human Perceptions:  <br>A joint study by several institutions revealed that Reinforcement Learning from Human Feedback (RLHF) might worsen LLM errors by making them more convincing to humans. This issue, termed "U-SOPHISTRY," shows that RLHF increases false positive rates and highlights a need for improved human-alignment methods.*** <br><br>
   Sep 19, Tsinhua Uni, UC Berkeley, Anthropic, NYU, and George Washington Uni published a [paper](https://arxiv.org/pdf/2409.12822) “Language Models Learn to Mislead Humans via RLHF”. Language models (LMs) can produce errors that are hard to detect for humans, especially when the task is complex. RLHF, the most popular post-training method, may exacerbate this problem: to achieve higher rewards, LMs might get better at convincing humans that they are right even when they are wrong. This work studies this phenomenon under a standard RLHF pipeline, calling it "U-SOPHISTRY" since it is Unintended by model developers. Specifically, the work askes time-constrained (e.g., 3-10 minutes) human subjects to evaluate the correctness of model outputs and calculate humans' accuracy against gold labels. On a question-answering task (QuALITY) and programming task (APPS), RLHF makes LMs better at convincing the subjects but not at completing the task correctly. RLHF also makes the model harder to evaluate: the subjects' false positive rate increases by 24.1% on QuALITY and 18.3% on APPS. Finally, the study shows that probing, a state-of-the-art approach for detecting Intended Sophistry (e.g. backdoored LMs), does not generalize to U-SOPHISTRY. The experimental results highlight an important failure mode of RLHF and call for more research in assisting humans to align them.

5. ***Google's SCoRe Improves Self-Correction in LLMs:  <br>Google developed "SCoRe," a reinforcement learning method that enhances an LLM's self-correction ability. It improves performance on math and programming tasks without relying on external models, showing significant gains in the Gemini model family.*** <br><br>
   Sep 19, Google published a [paper](https://arxiv.org/pdf/2409.12917) “Training Language Models to Self-Correct via Reinforcement Learning”. Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Existing approaches for training self-correction either require multiple models or rely on a more capable model or other forms of supervision. To this end, the study develops a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, the study first shows that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are insufficient for instilling self-correction behavior. In particular, the authors observe that training via SFT either suffers from a distribution mismatch between the training data and the model's own responses or implicitly prefers only a certain mode of correction behavior that is often not effective at test time. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction strategy that is effective at test time as opposed to simply fitting high-reward responses for a given prompt. This regularization prescribes running a first phase of RL on a base model to generate a policy initialization that is less susceptible to collapse and then using a reward bonus to amplify self-correction during training. When applied to Gemini 1.0 Pro and 1.5 Flash models, the authors find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on the MATH and HumanEval benchmarks.

7. ***Moshi's Innovation in Real-Time Spoken Dialogue:  <br>The "Moshi" framework eliminates the traditional speech pipeline's latency and segmentation issues by directly generating speech-to-speech dialogue. It models user and system speech streams in parallel, enabling real-time, full-duplex conversation with minimal delay.*** <br><br>
   Sep 18, Kyutai published a [paper](https://kyutai.org/Moshi.pdf) “Moshi: a speech-text foundation model for real-time dialogue”. The paper introduces Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning— such as emotion or non-speech sounds— is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. The authors moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this “Inner Monologue” method significantly improves the linguistic quality of generated speech, but the paper also illustrates how it can provide streaming speech recognition and text-to-speech. The resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at github.com/kyutai-labs/moshi.

9. ***Chain-of-Thought Benefits Limited to Symbolic Tasks:  <br>A meta-analysis indicated that Chain-of-Thought (CoT) prompting mainly boosts performance in math and symbolic reasoning tasks. It has limited benefits in other areas, suggesting selective application and a need for new paradigms in LLM reasoning tasks.*** <br><br>
    Sep 18, Uni of Texas at Austin, Johns Hopkins Uni and Princeton Uni published a [paper](https://huggingface.co/papers/2409.12183) “To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning”. Chain-of-thought (CoT) via prompting is the de facto method for eliciting reasoning capabilities from large language models (LLMs). But for what kinds of tasks is this extra “thinking” really helpful? To analyze this, the authors conducted a quantitative meta-analysis covering over 100 papers using CoT and ran evaluations of 20 datasets across 14 models. Experimental results show that CoT gives strong performance benefits primarily on tasks involving math or logic, with much smaller gains on other types of tasks. On MMLU, directly generating the answer without CoT leads to almost identical accuracy as CoT unless the question or model's response contains an equals sign, indicating symbolic operations and reasoning. Following this finding, the authors analyze the behavior of CoT on these problems by separating planning and execution and comparing against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic execution, but it underperforms relative to using a symbolic solver. The results indicate that CoT can be applied selectively, maintaining performance while saving inference costs. Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms that better leverage intermediate computation across the whole range of LLM applications.

11. ***CORE-Bench for Measuring AI Agent Reproducibility:  <br>Princeton introduced "CORE-Bench," a benchmark designed to assess AI agents' ability to reproduce scientific results. The benchmark reveals substantial gaps in current agents' reproducibility capabilities, calling for improvements in research agents' accuracy.*** <br><br>
    Sep 17, Princeton Uni published a [paper](https://arxiv.org/pdf/2409.11363) “CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark”. AI agents have the potential to aid users on a variety of consequential tasks, including conducting scientific research. To spur the development of useful agents, researchers need benchmarks that are challenging, but more crucially, directly correspond to real-world tasks of interest. This paper introduces such a benchmark, designed to measure the accuracy of AI agents in tackling a crucial yet surprisingly challenging aspect of scientific research: computational reproducibility. This task, fundamental to the scientific process, involves reproducing the results of a study using the provided code and data. This study introduces CORE-Bench (Computational Reproducibility Agent Benchmark), a benchmark consisting of 270 tasks based on 90 scientific papers across three disciplines (computer science, social science, and medicine). Tasks in CORE-Bench consist of three difficulty levels and include both language-only and vision-language tasks. The authors provide an evaluation system to measure the accuracy of agents in a fast and parallelizable way, saving days of evaluation time for each run compared to a sequential implementation. The study evaluated two baseline agents: the general-purpose AutoGPT and a task-specific agent called CORE-Agent. The authors tested both variants using two underlying language models: GPT-4o and GPT-4o-mini. The best agent achieved an accuracy of 21% on the hardest task, showing the vast scope for improvement in automating routine scientific tasks. Having agents that can reproduce existing work is a necessary step towards building agents that can conduct novel research and could verify and improve the performance of other research agents. The authors hope that CORE-Bench can improve the state of reproducibility and spur the development of future research agents. Code is [available here](http://github.com/siegelz/core-bench).

13. ***Nvidia's NVLM Multimodal Models Achieve State-of-the-Art Performance:  <br>Nvidia's NVLM models push the boundaries of vision-language tasks, surpassing both open and proprietary models. They integrate multimodal and text-only training, enhancing performance across math, coding, and OCR tasks.*** <br><br>
    Sep 17, Nvidia published a paper “NVLM: Open Frontier-Class Multimodal LLMs”. The [paper](https://arxiv.org/pdf/2409.11402) introduces NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, the study performs a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, the study proposes a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, the authors introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, the authors meticulously curate and provide detailed information on the multimodal pretraining and supervised fine-tuning datasets. The findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, the study develops production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, the authors craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, the model weights and the code will be released at https://nvlm-project.github.io/.

15. ***Improved Diffusion Models for Depth and Normal Estimation:  <br>Researchers demonstrated that correcting inefficiencies in diffusion models significantly boosts their performance in depth and normal estimation tasks, achieving results that rival state-of-the-art models with much faster inference times.*** <br><br>
    Sep 17, RWTH Aachen Uni and Eindhoven Uni of Tech published a [paper](https://arxiv.org/pdf/2409.11355) “Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think”. Recent work showed that large diffusion models can be reused as highly precise monocular depth estimators by casting depth estimation as an image-conditional image generation task. While the proposed model achieved state-of-the-art results, high computational demands due to multi-step inference limited its use in many scenarios. This paper shows that the perceived inefficiency was caused by a flaw in the inference pipeline that has so far gone unnoticed. The fixed model performs comparably to the best previously reported configuration while being more than 200times faster. To optimize for downstream task performance, the authors perform end-to-end fine-tuning on top of the single-step model with task-specific losses and get a deterministic model that outperforms all other diffusion-based depth and normal estimation models on common zero-shot benchmarks. The authors surprisingly find that this fine-tuning protocol also works directly on Stable Diffusion and achieves comparable performance to current state-of-the-art diffusion-based depth and normal estimation models, calling into question some of the conclusions drawn from prior works. [Code is here](https://github.com/VisualComputingInstitute/diffusion-e2e-ft).

17. ***Kolmogorov-Arnold Transformer (KAT) Optimizes Transformer Efficiency:  <br>The introduction of the Kolmogorov-Arnold Transformer (KAT) replaces traditional MLP layers with Kolmogorov-Arnold Networks to improve deep learning models' performance and efficiency, solving key challenges in scaling transformers.*** <br><br>
    Sep 16, National Uni of Singapore published a [paper](https://arxiv.org/pdf/2409.10594) “Kolmogorov-Arnold Transformer”. Transformers stand as the cornerstone of mordern deep learning. Traditionally, these models rely on multi-layer perceptron (MLP) layers to mix the information between channels. This paper introduces the Kolmogorov-Arnold Transformer (KAT), a novel architecture that replaces MLP layers with Kolmogorov-Arnold Network (KAN) layers to enhance the expressiveness and performance of the model. Integrating KANs into transformers, however, is no easy feat, especially when scaled up. Specifically, the authors identify three key challenges: (C1) Base function. The standard B-spline function used in KANs is not optimized for parallel computing on modern hardware, resulting in slower inference speeds. (C2) Parameter and Computation Inefficiency. KAN requires a unique function for each input-output pair, making the computation extremely large. (C3) Weight initialization. The initialization of weights in KANs is particularly challenging due to their learnable activation functions, which are critical for achieving convergence in deep neural networks. To overcome the aforementioned challenges, the authors propose three key solutions: (S1) Rational basis, which replaces B-spline functions with rational functions to improve compatibility with modern GPUs. By implementing this in CUDA, the authors achieve faster computations. (S2) Group KAN, which shares the activation weights through a group of neurons, to reduce the computational load without sacrificing performance. (S3) Variance-preserving initialization, which carefully initializes the activation weights to make sure that the activation variance is maintained across layers. With these designs, KAT scales effectively and readily outperforms traditional MLP-based transformers.

19. ***AgentTorch Enhances Agent-Based Modeling with LLMs:  <br>MIT's AgentTorch scales agent-based models (ABMs) to millions of agents, using LLMs to simulate complex behaviors. Applied to COVID-19, it demonstrates how adaptive agents can improve policy design by simulating realistic population behaviors.*** <br><br>
    Sep 14, MIT published a [paper](https://arxiv.org/pdf/2409.10568) “On the limits of agency in agent-based models”. Agent-based modeling (ABM) seeks to understand the behavior of complex systems by simulating a collection of agents that act and interact within an environment. Their practical utility requires capturing realistic environment dynamics and adaptive agent behavior while efficiently simulating million-size populations. Recent advancements in large language models (LLMs) present an opportunity to enhance ABMs by using LLMs as agents with further potential to capture adaptive behavior. However, the computational infeasibility of using LLMs for large populations has hindered their widespread adoption. This paper introduces AgentTorch -- a framework that scales ABMs to millions of agents while capturing high-resolution agent behavior using LLMs. The researhers benchmark the utility of LLMs as ABM agents, exploring the trade-off between simulation scale and individual agency. Using the COVID-19 pandemic as a case study, the study demonstrates how AgentTorch can simulate 8.4 million agents representing New York City, capturing the impact of isolation and employment behavior on health and economic outcomes. The authors compare the performance of different agent architectures based on heuristic and LLM agents in predicting disease waves and unemployment rates. Furthermore, the study showcases AgentTorch's capabilities for retrospective, counterfactual, and prospective analyses, highlighting how adaptive agent behavior can help overcome the limitations of historical data in policy design. AgentTorch is an open-source project actively being used for policy-making and scientific discovery around the world. The framework is available here: github.com/AgentTorch/AgentTorch.

21. ***AI-Driven Dialogues Reduce Conspiracy Beliefs:  <br>A study using GPT-4 Turbo for personalized dialogues showed a 20% reduction in conspiracy beliefs. The effects persisted for two months and generalized across various conspiracy theories, highlighting the potential of AI-driven interventions.*** <br><br>
    Sep 13, MIT, Cornell Uni and American Uni published a [paper](https://www.science.org/doi/10.1126/science.adq1814) on Science “Durably reducing conspiracy beliefs through dialogues with AI”. Conspiracy theory beliefs are notoriously persistent. Influential hypotheses propose that they fulfill important psychological needs, thus resisting counterevidence. Yet previous failures in correcting conspiracy beliefs may be due to counterevidence being insufficiently compelling and tailored. To evaluate this possibility, the researchers leveraged developments in generative artificial intelligence and engaged 2190 conspiracy believers in personalized evidence-based dialogues with GPT-4 Turbo. The intervention reduced conspiracy belief by ~20%. The effect remained 2 months later, generalized across a wide range of conspiracy theories, and occurred even among participants with deeply entrenched beliefs. Although the dialogues focused on a single conspiracy, they nonetheless diminished belief in unrelated conspiracies and shifted conspiracy-related behavioral intentions. These findings suggest that many conspiracy theory believers can revise their views if presented with sufficiently compelling evidence.

23. ***AI-Edited Visuals Increase False Memories:  <br>Research indicated that AI-edited images and videos significantly increase false recollections in participants, with the strongest effect observed in AI-generated videos of AI-edited images. This raises concerns about the ethical and societal impacts of AI-altered visuals.*** <br><br>
    Sep 13, MIT, UC Irvine et al published a [paper](https://arxiv.org/pdf/2409.08895) “Synthetic Human Memories: AI-Edited Images and Videos Can Implant False Memories and Distort Recollection”. AI is increasingly used to enhance images and videos, both intentionally and unintentionally. As AI editing tools become more integrated into smartphones, users can modify or animate photos into realistic videos. This study examines the impact of AI-altered visuals on false memories--recollections of events that didn't occur or deviate from reality. In a pre-registered study, 200 participants were divided into four conditions of 50 each. Participants viewed original images, completed a filler task, then saw stimuli corresponding to their assigned condition: unedited images, AI-edited images, AI-generated videos, or AI-generated videos of AI-edited images. AI-edited visuals significantly increased false recollections, with AI-generated videos of AI-edited images having the strongest effect (2.05x compared to control). Confidence in false memories was also highest for this condition (1.19x compared to control). The authors discuss potential applications in HCI, such as therapeutic memory reframing, and challenges in ethical, legal, political, and societal domains.

25. ***AI-LieDar Framework for Truth-Utility Conflicts in LLM Agents:  <br>A study developed AI-LieDar, a framework to explore how LLMs navigate truthfulness-utility trade-offs in multi-turn conversations. Results showed that LLMs are truthful less than 50% of the time, and truth-steering remains a challenge.*** <br><br>
    Sep 13, CMU, Uni of Michigan, and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2409.09013) “AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents”. To be safely and successfully deployed, LLMs must simultaneously satisfy truthfulness and utility goals. Yet, often these two goals compete (e.g., an AI agent assisting a used car salesman selling a car with flaws), partly due to ambiguous or misleading user instructions. The paper proposes AI-LieDar, a framework to study how LLM-based agents navigate scenarios with utility-truthfulness conflicts in a multi-turn interactive setting. The authors design a set of realistic scenarios where language agents are instructed to achieve goals that are in conflict with being truthful during a multi-turn conversation with simulated human agents. To evaluate the truthfulness at large scale, the study develops a truthfulness detector inspired by psychological literature to assess the agents' responses. The experiment demonstrates that all models are truthful less than 50% of the time, although truthfulness and goal achievement (utility) rates vary across models. The study further tests the steerability of LLMs towards truthfulness, finding that models follow malicious instructions to deceive, and even truth-steered models can still lie. These findings reveal the complex nature of truthfulness in LLMs and underscore the importance of further research to ensure the safe and reliable deployment of LLMs and AI agents.

27. ***Operational Advice for Dense and Sparse Retrievers:  <br>Researchers provided practical guidance on selecting dense and sparse retrieval methods, recommending a hybrid approach for optimal performance. Their results guide practitioners in choosing between HNSW and flat indexes based on dataset size and use case.*** <br><br>
    Sep 10, Uni of Waterloo published a [paper](https://arxiv.org/pdf/2409.06464) “Operational Advice for Dense and Sparse Retrievers: HNSW, Flat, or Inverted Indexes?”. Practitioners working on dense retrieval today face a bewildering number of choices. Beyond selecting the embedding model, another consequential choice is the actual implementation of nearest-neighbor vector search. While best practices recommend HNSW indexes, flat vector indexes with brute-force search represent another viable option, particularly for smaller corpora and for rapid prototyping. This paper provides experimental results on the BEIR dataset using the open-source Lucene search library that explicate the tradeoffs between HNSW and flat indexes (including quantized variants) from the perspectives of indexing time, query evaluation performance, and retrieval quality. With additional comparisons between dense and sparse retrievers, the results provide guidance for today's search practitioner in understanding the design space of dense and sparse retrievers. Hybrid approached is recommended.

29. ***Agentic AI's Transformative Impact on Business and Technology:  <br>Forbes highlighted agentic AI, which can autonomously solve complex problems in various fields like healthcare, finance, and cybersecurity. Despite its potential, challenges such as ethical concerns and job displacement must be addressed as it evolves to augment human decision-making.*** <br><br>
    Sep 9, Forbes [published an article](https://www.forbes.com/sites/bernardmarr/2024/09/06/agentic-ai-the-next-big-breakthrough-thats-transforming-business-and-technology/) “Agentic AI: The Next Big Breakthrough That's Transforming Business And Technology”. Agentic AI is an advanced form of artificial intelligence that can autonomously act to achieve specific goals, offering more autonomy, proactivity, and problem-solving abilities than traditional AI systems. Unlike traditional AI, which responds to commands, agentic AI can break down complex tasks, learn from experiences, and adapt its strategies in dynamic environments. Key Features include Autonomy: Functions with minimal human intervention. Problem-solving: Breaks down and handles complex challenges. Adaptability: Adjusts based on new data or environments. Personalization: Tailors responses and solutions based on user interactions. Communication Skills: Can process natural language and demonstrate reasoning. Following are some real-world Applications such as Business Operations: Managing supply chains, logistics, and real-time decision-making. Healthcare: Personalized patient care and proactive health monitoring. Software Development: Overseeing development lifecycles, including design and debugging. Cybersecurity: Autonomous monitoring and defense against digital threats. Human Resources: Automating recruitment, training, and personalized career advice. Scientific Research: Running experiments and accelerating discoveries. Finance: Dynamic portfolio management and market analysis. Challenges of agentic AI are ethical concerns, accountability, data privacy, and potential job displacement are significant challenges. Balancing agentic AI's autonomy with human oversight is key to harnessing its benefits. As research advances, agentic AI will likely evolve to work collaboratively with humans, augmenting human capabilities while considering ethical implications.
 <br><br><br>

***Sep 15***

1. ***OpenAI releases O1 models: <br>
OpenAI introduced O1, models that mimic human reasoning by thoroughly solving problems. The O1 models outperform previous iterations, excelling in fields like mathematics, physics, chemistry, and coding, and significantly improving on safety compliance. They are designed for complex workflows in professional settings such as healthcare and physics, with continuous updates planned to enhance their functionality.*** <br><br>
   Sep 12, OpenAI [released a new version O1](https://openai.com/index/introducing-openai-o1-preview/). The o1-preview models are trained to think through problems thoroughly (yes, this means the models might take some time to respond to prompts), refining their approach and learning from mistakes, much like human reasoning. They outperform earlier models, scoring 83% on the International Mathematics Olympiad qualifying exam compared to GPT-4o's 13%, and excel in physics, chemistry, biology, and coding tasks. OpenAI has implemented a new safety training approach, leveraging the reasoning skills of these models to adhere more effectively to safety guidelines. The o1-preview scored 84 on difficult jailbreaking tests, a significant improvement over GPT-4o's score of 22, demonstrating enhanced compliance with safety rules. These models are ideal for professionals in fields requiring advanced reasoning, such as healthcare researchers, physicists, and developers who need to tackle complex workflows. For example, o1 can assist in annotating cell sequencing data, generating complex formulas in quantum physics, and executing multi-step coding tasks. OpenAI plans continuous updates to the o1 series, including adding features like browsing and file uploads. The goal is to develop models that reason like humans and solve increasingly complex problems across various domains.

3. ***DSBench: A new benchmark for data science agents: <br>
A new paper from the University of Texas at Dallas, Tencent, and the University of Southern California introduces DSBench, a benchmark aimed at evaluating data science agents using real-world tasks. The study highlights the limitations of current benchmarks and shows that existing large language models (LLMs) and large vision-language models (LVLMs) struggle with complex data tasks, achieving only a 34% success rate, signaling the need for more advanced data science agents.*** <br><br>
   Sep 12, Uni of Texas at Dallas, Tencent and Uni of Southern California published a [paper](https://arxiv.org/pdf/2409.07703) “DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?”. Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, the study introduces DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. The evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.

5. ***Source2Synth: Synthetic data generation for LLMs: <br>
Meta, Oxford University, and University College London propose Source2Synth, a method for improving LLMs without human annotations by generating synthetic data. By focusing on structured reasoning and tool usage, Source2Synth significantly boosts LLM performance in multi-hop question answering and tabular data tasks by discarding low-quality outputs and grounding data generation in real-world sources.*** <br><br>
   Sep 12, Meta, Oxford Uni and Uni College London published a [paper](https://arxiv.org/pdf/2409.08239) “Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources”. Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. This paper proposes Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. The study demonstrates the generality of this approach by applying it to two challenging domains: reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). The proposed method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.

7. ***Laypeople’s reliance on LLM-generated legal advice: <br>
Research from the University of Nottingham and others reveals that while laypeople can distinguish between LLM- and lawyer-generated legal advice, they are more willing to follow the advice of LLMs. Despite participants favoring LLM advice, they accurately identified LLM texts at above chance-level, raising important questions about trust and the future role of LLMs in legal contexts.*** <br><br>
   Sep 12, Uni of Nottingham, Uni of Southampton and Uni of Antwerp published a [paper](https://arxiv.org/pdf/2409.07871) “Objection Overruled! Lay People can Distinguish Large Language Models from Lawyers, but still Favour Advice from an LLM”. Large Language Models (LLMs) are seemingly infiltrating every domain, and the legal context is no exception. This paper presents the results of three experiments (total N=288) that investigated lay people's willingness to act upon, and their ability to discriminate between, LLM- and lawyer-generated legal advice. In Experiment 1, participants judged their willingness to act on legal advice when the source of the advice was either known or unknown. When the advice source was unknown, participants indicated that they were significantly more willing to act on the LLM-generated advice. This result was replicated in Experiment 2. Intriguingly, despite participants indicating higher willingness to act on LLM-generated advice in Experiments 1 and 2, participants discriminated between the LLM- and lawyer-generated texts significantly above chance-level in Experiment 3. Lastly, the paper discusses potential explanations and risks of the findings, limitations and future work, and the importance of language complexity and real-world comparability.

9. ***PaperQA2: LLMs surpass human performance in scientific synthesis: <br>
FutureHouse Inc and collaborators demonstrate that PaperQA2, an advanced LLM for scientific literature, matches or exceeds human experts in summarizing and detecting contradictions in scientific papers. PaperQA2 outperforms Wikipedia summaries and helps identify contradictions in biology papers, showcasing LLMs’ growing role in scientific research and literature analysis.*** <br><br>
    Sep 12, FutureHouse Inc, Uni of Rochester, and Francis Circk Inst London published a [paper](https://storage.googleapis.com/fh-public/paperqa/Language_Agents_Science.pdf) “language agents achieve superhuman synthesis of scientific knowledge”. Language models are known to “hallucinate” incorrect information, and it is unclear if they are sufficiently accurate and reliable for use in scientific research. The authors developed a rigorous human-AI comparison methodology to evaluate language model agents on real-world literature search tasks covering information retrieval, summarization, and contradiction detection tasks. The study shows that PaperQA2, a frontier language model agent optimized for improved factuality, matches or exceeds subject matter expert performance on three realistic literature research tasks without any restrictions on humans (i.e., full access to internet, search tools, and time). PaperQA2 writes cited, Wikipedia-style summaries of scientific topics that are significantly more accurate than existing, human-written Wikipedia articles. The paper also introduces a hard benchmark for scientific literature research called LitQA2 that guided design of PaperQA2, leading to it exceeding human performance. Finally, the authors apply PaperQA2 to identify contradictions within the scientific literature, an important scientific task that is challenging for humans. PaperQA2 identifies 2.34 ± 1.99 (mean ± SD, N = 93 papers) contradictions per paper in a random subset of biology papers, of which 70% are validated by human experts. These results demonstrate that language model agents are now capable of exceeding domain experts across meaningful tasks on scientific literature.

11. ***LLMs reduce human participation in online knowledge platforms: <br>
A study from University College London, Cambridge, and others shows that the release of ChatGPT led to a 25% reduction in activity on Stack Overflow, especially in programming-related queries. The findings highlight the potential long-term impact of LLMs on reducing human-generated knowledge, which could limit future model training data availability.*** <br><br>
    Sep 11, Uni of College London, Uni of Cambridge et al. published a [paper](https://academic.oup.com/pnasnexus/advance-article/doi/10.1093/pnasnexus/pgae400/7754871)  on PNAS “Large language models reduce public knowledge sharing on online Q&A platforms". Large language models (LLMs) are a potential substitute for human-generated data and knowledge resources. This substitution, however, can present a significant problem for the training data needed to develop future models if it leads to a reduction of human-generated content. This paper documents a reduction in activity on Stack Overflow coinciding with the release of ChatGPT, a popular LLM. To test whether this reduction in activity is specific to the introduction of this LLM we use counterfactuals involving similar human-generated knowledge resources that should not be affected by the introduction of ChatGPT to such extent. Within six months of ChatGPT's release, activity on Stack Overflow decreased by 25% relative to its Russian and Chinese counterparts, where access to ChatGPT is limited, and to similar forums for mathematics, where ChatGPT is less capable. The authors interpret this estimate as a lower bound of the true impact of ChatGPT on Stack Overflow. The decline is larger for posts related to the most widely used programming languages. The study finds no significant change in post quality, measured by peer feedback, and observe similar decreases in content creation by more and less experienced users alike. Thus, LLMs are not only displacing duplicate, low-quality, or beginner-level content. The findings suggest that the rapid adoption of LLMs reduces the production of public data needed to train them, with significant consequences.

13. ***Agent Workflow Memory (AWM) improves task performance in LLMs: <br>
Researchers from CMU and MIT introduce Agent Workflow Memory (AWM), a system designed to help LLM-based agents reuse task workflows for better performance in web navigation tasks. AWM improves success rates in benchmarks like Mind2Web and WebArena, showing promise for enhancing agents' ability to perform complex, multi-step tasks.*** <br><br>
    Sep 11, CMU and MIT published a [paper](https://arxiv.org/pdf/2409.07429) “Agent Workflow Memory”. Despite the potential of language model-based agents to solve real-world tasks such as web navigation, current methods still struggle with long-horizon tasks with complex action trajectories. In contrast, humans can flexibly solve complex tasks by learning reusable task workflows from past experiences and using them to guide future actions. To build agents that can similarly benefit from this process, the study introduces Agent Workflow Memory (AWM), a method for inducing commonly reused routines, i.e., workflows, and selectively providing workflows to the agent to guide subsequent generations. AWM flexibly applies to both offline and online scenarios, where agents induce workflows from training examples beforehand or from test queries on the fly. The authors experiment on two major web navigation benchmarks -- Mind2Web and WebArena -- that collectively cover 1000+ tasks from 200+ domains across travel, shopping, and social media, among others. AWM substantially improves the baseline results by 24.6% and 51.1% relative success rate on Mind2Web and WebArena while reducing the number of steps taken to solve WebArena tasks successfully. Furthermore, online AWM robustly generalizes in cross-task, website, and domain evaluations, surpassing baselines from 8.9 to 14.0 absolute points as train-test task distribution gaps widen.

15. ***Synthetic continued pretraining for domain-specific LLMs: <br>
Stanford University researchers propose synthetic continued pretraining as a method to improve LLM performance on specialized domains using small corpora. By generating synthetic data and combining it with retrieval-augmented generation, this approach enhances models’ ability to learn domain-specific knowledge more efficiently.*** <br><br>
    Sep 11, Stanford Uni published a [paper](https://arxiv.org/pdf/2409.07431) “Synthetic continued pretraining”. Pretraining on large-scale, unstructured internet text has enabled language models to acquire a significant amount of world knowledge. However, this knowledge acquisition is data-inefficient -- to learn a given fact, models must be trained on hundreds to thousands of diverse representations of it. This poses a challenge when adapting a pretrained model to a small corpus of domain-specific documents, where each fact may appear rarely or only once. The study proposes to bridge this gap with synthetic continued pretraining: using the small domain-specific corpus to synthesize a large corpus more amenable to learning, and then performing continued pretraining on the synthesized corpus. The paper instantiates this proposal with EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source documents and then generates diverse text by drawing connections between the sampled entities. Synthetic continued pretraining using EntiGraph enables a language model to answer questions and follow generic instructions related to the source documents without access to them. If instead, the source documents are available at inference time, the authors show that the knowledge acquired through the approach compounds with retrieval-augmented generation. To better understand these results, the study builds a simple mathematical model of EntiGraph, and show how synthetic data augmentation can "rearrange" knowledge to enable more data-efficient learning.

17. ***Evaluating LLMs on research task reproduction with SUPER: <br>
A new benchmark, SUPER, from Allen Institute for AI and the University of Washington evaluates LLMs’ ability to set up and reproduce tasks from research repositories. Despite advancements, the best model could only solve 16.3% of end-to-end tasks, highlighting the challenges of fully autonomous research agents and the need for further progress in this area.*** <br><br>
    Sep 11, Allen Inst for AI and Uni of Washington published a [paper](https://arxiv.org/pdf/2409.07440) “SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories”. Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work. To advance towards this goal, this work introduces SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories. SUPER aims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories. The benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 602 automatically generated problems for larger-scale development. The paper introduces various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise. The work shows that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3% of the end-to-end set, and 46.1% of the scenarios. This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.

19. ***LLMs generate novel scientific research ideas: <br>
A study by IITP and Oak Ridge National Laboratory explores how LLMs can generate novel research ideas across various domains. Claude-2 and GPT-4 were found to align more with human authors' perspectives, while Claude-2 generated more diverse ideas. Human evaluations of these ideas showed that LLMs are evolving in their capability to assist in idea generation.*** <br><br>
    Sep 10, IITP and Oak Ridge National Laborator published a [paper](https://arxiv.org/pdf/2409.06185) “Can Large Language Models Unlock Novel Scientific Research Ideas?”. "An idea is nothing more nor less than a new combination of old elements" (Young, J.W.). The widespread adoption of Large Language Models (LLMs) and publicly available ChatGPT have marked a significant turning point in the integration of Artificial Intelligence (AI) into people's everyday lives. This study explores the capability of LLMs in generating novel research ideas based on information from research papers. The study conducts a thorough examination of 4 LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and Physics). The paper found that the future research ideas generated by Claude-2 and GPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini. It is also found that Claude-2 generates more diverse future research ideas than GPT-4, GPT-3.5, and Gemini 1.0. The study further performed a human evaluation of the novelty, relevancy, and feasibility of the generated future research ideas. This investigation offers insights into the evolving role of LLMs in idea generation, highlighting both its capability and limitations. The work contributes to the ongoing efforts in evaluating and utilizing language models for generating future research ideas. The datasets and codes publicly available.

21. ***Structural Hallucinations in LLMs: <br>
A paper from DataLabs argues that hallucinations in LLMs are inevitable due to their mathematical structure and cannot be eliminated. The work frames these hallucinations as a structural feature of LLMs, challenging the notion that improving data or algorithms can fully mitigate this issue.*** <br><br>
    Sep 9, DataLabs United We Care published a [paper](https://arxiv.org/pdf/2409.05746) “LLMs Will Always Hallucinate, and We Need to Live With This”. As Large Language Models become more ubiquitous across domains, it becomes important to examine their inherent limitations critically. This work argues that hallucinations in language models are not just occasional errors but an inevitable feature of these systems. The study demonstrates that hallucinations stem from the fundamental mathematical and logical structure of LLMs. It is, therefore, impossible to eliminate them through architectural improvements, dataset enhancements, or fact-checking mechanisms. The analysis draws on computational theory and Godel's First Incompleteness Theorem, which references the undecidability of problems like the Halting, Emptiness, and Acceptance Problems. The paper demonstrates that every stage of the LLM process-from training data compilation to fact retrieval, intent classification, and text generation-will have a non-zero probability of producing hallucinations. This work introduces the concept of Structural Hallucination as an intrinsic nature of these systems. By establishing the mathematical certainty of hallucinations, the study challenge the prevailing notion that they can be fully mitigated.

23. ***LLMs' novelty in generating research ideas vs. human experts: <br>
A large-scale study with 100+ NLP researchers compared LLMs with human experts in generating research ideas. The findings suggest that LLMs generate more novel ideas but slightly lag in feasibility. The study identifies gaps in LLM self-evaluation and proposes future evaluations to focus on complete research projects.*** <br><br>
    Sep 6, CMU published a [paper](https://arxiv.org/pdf/2409.04109) “Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers”. Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. The study addresses this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, the study obtains the first statistically significant conclusion on current LLM capabilities for research ideation: the authors find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying the agent baselines closely, the authors identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, the authors acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling the authors to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.

25. ***Paper Copilot for personalized academic assistance: <br>
Researchers from UIUC, CMU, and Carleton College developed Paper Copilot, an LLM-based tool designed to assist researchers by providing personalized, real-time literature updates and thought retrieval. Paper Copilot demonstrated efficiency in streamlining the research process, saving nearly 70% of time in evaluations.*** <br><br>
    Sep 6, UIUC, CMU and Carleton College published a [paper](https://arxiv.org/pdf/2409.04593) “Paper Copilot: A Self-Evolving and Efficient LLM System for Personalized Academic Assistance”. As scientific research proliferates, researchers face the daunting task of navigating and reading vast amounts of literature. Existing solutions, such as document QA, fail to provide personalized and up-to-date information efficiently. The authors present Paper Copilot, a self-evolving, efficient LLM system designed to assist researchers, based on thought-retrieval, user profile and high performance optimization. Specifically, Paper Copilot can offer personalized research services, maintaining a real-time updated database. Quantitative evaluation demonstrates that Paper Copilot saves 69.92\% of time after efficient deployment. This paper details the design and implementation of Paper Copilot, highlighting its contributions to personalized academic support and its potential to streamline the research process. The project is located in https://huggingface.co/spaces/ulab-ai/ArxivCopilot

27. ***GPTs struggle with controversial and limited-data topics: <br>
A Harvard University paper discusses why GPTs struggle with topics that lack a general consensus or are controversial. The study emphasizes that LLMs perform well with popular topics but exhibit variability in outputs when trained on obscure or polarized topics, linking accuracy to the quality and breadth of training data.*** <br><br>
    Sep 5, ACM Queue published a [paper](https://dl.acm.org/doi/pdf/10.1145/3688007) from Harvard Uni “GPTs and Hallucination: Why do large language models hallucinate?” The findings in this experiment support the hypothesis that GPTs based on LLMs perform well on prompts that are more popular and have reached a general consensus yet struggle on controversial topics or topics with limited data. The variability in the applications's responses underscores that the models depend on the quantity and quality of their training data, paralleling the system of crowdsourcing that relies on diverse and credible contributions. Thus, while GPTs can serve as useful tools for many mundane tasks, their engagement with obscure and polarized topics should be interpreted with caution. LLMs' reliance on probabilistic models to produce statements about the world ties their accuracy closely to the breadth and quality of the data they're given.

29. ***Safe Superintelligence startup raises $1 billion: <br>
OpenAI co-founder Ilya Sutskever’s new AI safety-focused startup, Safe Superintelligence (SSI), raised $1 billion. SSI aims to develop superintelligent systems with a focus on safety to prevent AI-related risks. This signals increasing industry attention on AI safety amid rapid advancements in AI technology.*** <br><br>
    Sep 5, [according to Reuters](https://www.reuters.com/technology/artificial-intelligence/openai-co-founder-sutskevers-new-safety-focused-ai-startup-ssi-raises-1-billion-2024-09-04/?utm_source=substack&utm_medium=email), OpenAI co-founder Sutskever's new safety-focused AI startup SSI raises $1 billion. Ilya Sutskever, OpenAI's former chief scientist, has launched a new venture with a bang. His startup, Safe Superintelligence (SSI), raised $1 billion from top-tier investors to tackle one of AI's biggest challenges: building superintelligent systems that won't accidentally erase humanity. With a $5 billion valuation and only 10 employees (for now), SSI focuses on R&D for a few years before even thinking about a product. They're hiring for "good character" over credentials and taking a fresh approach to AI scaling. Sutskever's move comes after his dramatic exit from OpenAI. This massive investment signals that AI safety concerns are becoming more prevalent. Plus, his focus exclusively on safety can influence how the entire industry approaches superintelligent AI development.

31. ***Modular LLMs for greater efficiency and scalability: <br>
A paper co-authored by multiple institutions introduces Configurable Foundation Models, which break LLMs into modular components, or "bricks," to enhance computational efficiency and scalability. The modular approach enables flexible and dynamic configurations, allowing for more efficient and scalable AI models.*** <br><br>
    Sep 4, Tsinghua Uni, Uni of California San Diego, CMU, ModelBest Inc, Princeton Uni, Nat Uni Singapore, Stanford Uni, and Uni of California Los Angeles published a [paper](https://arxiv.org/pdf/2409.02877) “Configurable Foundation Models: Building LLMs from a Modular Perspective”. Advancements in LLMs have recently unveiled challenges tied to computational efficiency and continual scalability due to their requirements of huge parameters, making the applications and evolution of these models on devices with limited computation resources and scenarios requiring various abilities increasingly cumbersome. Inspired by modularity within the human brain, there is a growing tendency to decompose LLMs into numerous functional modules, allowing for inference with part of modules and dynamic assembly of modules to tackle complex tasks, such as mixture-of-experts. To highlight the inherent efficiency and composability of the modular approach, The researchers coin the term brick to represent each functional module, designating the modularized structure as configurable foundation models. The paper offers a comprehensive overview and investigation of the construction, utilization, and limitation of configurable foundation models. It first formalizes modules into emergent bricks - functional neuron partitions that emerge during the pre-training phase, and customized bricks - bricks constructed via additional post-training to improve the capabilities and knowledge of LLMs. Based on diverse functional bricks, the paper further presents four brick-oriented operations: retrieval and routing, merging, updating, and growing. These operations allow for dynamic configuration of LLMs based on instructions to handle complex tasks. To verify the perspective, the authors conduct an empirical analysis on widely-used LLMs, and find that the FFN layers follow modular patterns with functional specialization of neurons and functional neuron partitions. Finally, the study highlights several open issues and directions for future research. Overall, this paper aims to offer a fresh modular perspective on existing LLM research and inspire the future creation of more efficient and scalable foundational models.
 <br><br><br>

***Sep 8 2024***


1. ***Key Findings on Few-Shot Learning and Fine-Tuning in LLMs: <br>A study published by Area Science Park Trieste compares In-context Learning (ICL) and Supervised Fine-Tuning (SFT) in large language models (LLMs). While both strategies improve performance on specific tasks, the internal representations they induce are markedly different. ICL creates hierarchical, semantically organized representations, while SFT's are fuzzier and more mixed. Despite this, SFT develops probability modes better suited for encoding answers. This highlights the diverse computational strategies of LLMs.*** <br><br>
   Sep 7, Area Science Park Trieste published a [paper](https://arxiv.org/pdf/2409.03662) “The representation landscape of few-shot learning and fine-tuning in large language models”. In-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks. Despite their different natures, these strategies often lead to comparable performance gains. However, little is known about whether they induce similar representations inside LLMs. The authors approach this problem by analyzing the probability landscape of their hidden representations in the two cases. More specifically, the study compares how LLMs solve the same question-answering task, finding that ICL and SFT create very different internal structures, in both cases undergoing a sharp transition in the middle of the network. In the first half of the network, ICL shapes interpretable representations hierarchically organized according to their semantic content. In contrast, the probability landscape obtained with SFT is fuzzier and semantically mixed. In the second half of the model, the fine-tuned representations develop probability modes that better encode the identity of answers, while the landscape of ICL representations is characterized by less defined peaks. The approach reveals the diverse computational strategies developed inside LLMs to solve the same task across different conditions, allowing researchers to make a step towards designing optimal methods to extract information from language models.

3. ***Salesforce Introduces xLAM for AI Agent Systems: <br>Salesforce's paper presents the xLAM family of large action models, designed to enhance AI agent tasks. These models, ranging from 1B to 8x22B parameters, are trained using a scalable pipeline unifying various datasets. xLAM models outperform GPT-4 and Claude-3 in tool use, placing first on the Berkeley Function-Calling Leaderboard. By releasing xLAM, Salesforce aims to democratize access to high-performance models for autonomous agents.*** <br><br>
   Sep 5, Salesforce published a paper “xLAM: A Family of Large Action Models to Empower AI Agent Systems”. Autonomous agents powered by large language models (LLMs) have attracted significant research interest. However, the open-source community faces many challenges in developing specialized models for agent tasks, driven by the scarcity of high-quality agent datasets and the absence of standard protocols in this area. The study introduces and publicly releases xLAM, a series of large action models designed for AI agent tasks. The xLAM series includes five models with both dense and mixture-of-expert architectures, ranging from 1B to 8x22B parameters, trained using a scalable, flexible pipeline that unifies, augments, and synthesizes diverse datasets to enhance AI agents' generalizability and performance across varied environments. Experimental results demonstrate that xLAM consistently delivers exceptional performance across multiple agent ability benchmarks, notably securing the 1st position on the Berkeley Function-Calling Leaderboard, outperforming GPT-4, Claude-3, and many other models in terms of tool use. By releasing the xLAM series, the authors aim to advance the performance of open-source LLMs for autonomous AI agents, potentially accelerating progress and democratizing access to high-performance models for agent tasks. Models are available at [this https URL](https://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4)

5. ***Google’s Framework for Enhancing Mathematical Reasoning in LLMs: <br>Google’s paper introduces a multi-turn direct preference learning framework to improve LLMs' mathematical reasoning. Tailored for tasks integrating external tools like code interpreters, this method boosts performance, particularly in models fine-tuned for multi-turn reasoning. Experimentation shows marked improvements in benchmarks such as GSM8K and MATH, demonstrating the framework’s effectiveness in optimizing model trajectories.*** <br><br>
   Sep 4, Google published a [paper](https://arxiv.org/pdf/2409.03215) “Building Math Agents with Multi-Turn Iterative Preference Learning”. Recent studies have shown that large language models' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, the paper introduces a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of the framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Experimental results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model's performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.

7. ***Simula Research Lab Examines LLMs in Log Parsing: <br>A study published by Simula Research Lab evaluates six LLMs, including GPT-3.5 and CodeLlama, in log parsing tasks. The results show that free-to-use models can compete with proprietary models, with CodeLlama outperforming GPT-3.5 by 10% in correct template extraction. These findings highlight the potential of open-source models in log parsing and their usability advantages.*** <br><br>
   Sep 4, Simula Research Lab published a [paper](https://arxiv.org/pdf/2409.02474) “A Comparative Study on Large Language Models for Log Parsing”. Log messages provide valuable information about the status of software systems. This information is provided in an unstructured fashion and automated approaches are applied to extract relevant parameters. To ease this process, log parsing can be applied, which transforms log messages into structured log templates. Recent advances in language models have led to several studies that apply ChatGPT to the task of log parsing with promising results. However, the performance of other state-of-the-art large language models (LLMs) on the log parsing task remains unclear. This study investigates the current capability of state-of-the-art LLMs to perform log parsing. The authors select six recent LLMs, including both paid proprietary (GPT-3.5, Claude 2.1) and four free-to-use open models, and compare their performance on system logs obtained from a selection of mature open-source projects. The study designs two different prompting approaches and apply the LLMs on 1, 354 log templates across 16 different projects. The study evaluates their effectiveness, in the number of correctly identified templates, and the syntactic similarity between the generated templates and the ground truth. The paper found that free-to-use models are able to compete with paid models, with CodeLlama extracting 10% more log templates correctly than GPT-3.5. Moreover, the authors provide qualitative insights into the usability of language models (e.g., how easy it is to use their responses). The results reveal that some of the smaller, free-to-use LLMs can considerably assist log parsing compared to their paid proprietary competitors, especially code-specialized models.

9. ***OLMoE: A High-Performance Open Mixture-of-Experts Model: <br>A collaborative paper introduces OLMoE, a sparse Mixture-of-Experts (MoE) language model with 7 billion parameters. Trained on 5 trillion tokens, OLMoE outperforms larger models like Llama2-13B-Chat, proving the efficacy of sparse MoE architectures. The authors open-source all components, aiming to advance the capabilities of open models.*** <br><br>
    Sep 3, Allen Inst for AI, Contextual AI, Uni of Washington and Princeton Uni published a [paper](https://arxiv.org/pdf/2409.02060) “OLMoE: Open Mixture-of-Experts Language Models”. The paper introduces OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. The paper pretrains it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. The models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. The paper presents various experiments on MoE training, analyze routing in the model showing high specialization, and open-source all aspects of the work: model weights, training data, [code](https://github.com/allenai/OLMoE), and logs.

11. ***GOT Model Aims to Revolutionize OCR: <br>researchers introduced the General OCR Theory (GOT) and its model, OCR-2.0. GOT handles a broad spectrum of artificial optical characters, offering region-specific recognition and interactive features. Experiments demonstrate its superiority over traditional OCR systems, positioning it as a comprehensive solution for modern OCR needs.*** <br><br>
    Sep 3, StepFun, Megvii Tech, UCAS, and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2409.01704) “General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model”. Traditional OCR systems (OCR-1.0) are increasingly unable to meet people's usage due to the growing demand for intelligent processing of man-made optical characters. This paper collectively refers to all artificial optical signals (e.g., plain texts, math/molecular formulas, tables, charts, sheet music, and even geometric shapes) as "characters" and propose the General OCR Theory along with an excellent model, namely GOT, to promote the arrival of OCR-2.0. The GOT, with 580M parameters, is a unified, elegant, and end-to-end model, consisting of a high-compression encoder and a long-contexts decoder. As an OCR-2.0 model, GOT can handle all the above "characters" under various OCR tasks. On the input side, the model supports commonly used scene- and document-style images in slice and whole-page styles. On the output side, GOT can generate plain or formatted results (markdown/tikz/smiles/kern) via an easy prompt. Besides, the model enjoys interactive OCR features, i.e., region-level recognition guided by coordinates or colors. Furthermore, the authors also adapt dynamic resolution and multi-page OCR technologies to GOT for better practicality. In experiments, the paper provides sufficient results to prove the superiority of the model.

13. ***Political DEBATE: Efficient Classifiers for Political Texts: <br>Princeton University and collaborators published a paper introducing Political DEBATE models for zero-shot and few-shot classification of political texts. These models outperform state-of-the-art classifiers, offering efficient, open-source solutions. Additionally, the release of the PolNLI dataset facilitates further research in political document classification.*** <br><br>
    Sep 2, Princeton Uni, Pennsylvania State Uni and Louisiana State Uni published a [paper](https://arxiv.org/pdf/2409.02078) “Political DEBATE: Efficient Zero-shot and Few-shot Classifiers for Political Text”. Social scientists quickly adopted large language models due to their ability to annotate documents without supervised training, an ability known as zero-shot learning. However, due to their compute demands, cost, and often proprietary nature, these models are often at odds with replication and open science standards. This paper introduces the Political DEBATE (DeBERTa Algorithm for Textual Entailment) language models for zero-shot and few-shot classification of political documents. These models are not only as good, or better than, state-of-the art large language models at zero and few-shot classification, but are orders of magnitude more efficient and completely open source. By training the models on a simple random sample of 10-25 documents, they can outperform supervised classifiers trained on hundreds or thousands of documents and state-of-the-art generative models with complex, engineered prompts. Additionally, the authors release the PolNLI dataset used to train these models -- a corpus of over 200,000 political documents with highly accurate labels across over 800 classification tasks.

15. ***Using Report Cards to Qualitatively Evaluate LLMs: <br>The University of Toronto and the Vector Institute published a paper introducing "Report Cards"—natural language summaries of LLM behavior. These qualitative evaluations provide clearer insights into model performance than traditional benchmarks, enabling more interpretable and holistic assessments of LLMs.*** <br><br>
    Sep 1, Uni of Toronto and Vector Inst published a [paper](https://arxiv.org/pdf/2409.00844) “Report Cards: Qualitative Evaluation of Language Models Using Natural Language Summaries”. The rapid development and dynamic nature of large language models (LLMs) make it difficult for conventional quantitative benchmarks to accurately assess their capabilities. The paper proposes report cards, which are human-interpretable, natural language summaries of model behavior for specific skills or topics. The authors develop a framework to evaluate report cards based on three criteria: specificity (ability to distinguish between models), faithfulness (accurate representation of model capabilities), and interpretability (clarity and relevance to humans). The paper also proposes an iterative algorithm for generating report cards without human supervision and explore its efficacy by ablating various design choices. Through experimentation with popular LLMs, the authors demonstrate that report cards provide insights beyond traditional benchmarks and can help address the need for a more interpretable and holistic evaluation of LLMs.

17. ***MIT Investigates Dataset Licensing in AI: <br>MIT's study systematically audits over 1,800 datasets to address legal and ethical concerns related to dataset licensing and attribution in AI. The findings reveal widespread misattribution, underscoring the need for better transparency in dataset usage. To facilitate this, the study releases an interactive tool, the Data Provenance Explorer, for tracing dataset lineage.*** <br><br>
    Aug 30, MIT published a [paper](https://www.nature.com/articles/s42256-024-00878-8) on Nature Machine Intelligence “A large-scale audit of dataset licensing and attribution in AI”. The race to train language models on vast, diverse and inconsistently documented datasets raises pressing legal and ethical concerns. To improve data transparency and understanding, the study convenes a multi-disciplinary effort between legal and machine learning experts to systematically audit and trace more than 1,800 text datasets. The authors develop tools and standards to trace the lineage of these datasets, including their source, creators, licences and subsequent use. The landscape analysis highlights sharp divides in the composition and focus of data licenced for commercial use. Important categories including low-resource languages, creative tasks and new synthetic data all tend to be restrictively licenced. The authors observe frequent miscategorization of licences on popular dataset hosting sites, with licence omission rates of more than 70% and error rates of more than 50%. This highlights a crisis in misattribution and informed use of popular datasets driving many recent breakthroughs. The analysis of data sources also explains the application of copyright law and fair use to finetuning data. As a contribution to continuing improvements in dataset transparency and responsible use, the authors release the audit, with an interactive user interface, the Data Provenance Explorer, to enable practitioners to trace and filter on data provenance for the most popular finetuning data collections: www.dataprovenance.org.

19. ***Reassessing AI Alignment Beyond Preferences: <br>A collaborative paper published challenges the preferentist approach to AI alignment, arguing that aligning AI with human values requires going beyond preferences. The authors propose aligning AI with normative standards suited to their social roles, advocating for stakeholder negotiation to ensure alignment promotes mutual benefit across diverse values.*** <br><br>
    Aug 30, MIT, UC Berkeley, Uni of College London, and Uni of Cambridge published a [paper](https://arxiv.org/pdf/2408.16984) “Beyond Preferences in AI Alignment”. The dominant practice of AI alignment assumes (1) that preferences are an adequate representation of human values, (2) that human rationality can be understood in terms of maximizing the satisfaction of preferences, and (3) that AI systems should be aligned with the preferences of one or more humans to ensure that they behave safely and in accordance with our values. Whether implicitly followed or explicitly endorsed, these commitments constitute what we term a preferentist approach to AI alignment. This paper characterizes and challenges the preferentist approach, describing conceptual and technical alternatives that are ripe for further research. The authors first survey the limits of rational choice theory as a descriptive model, explaining how preferences fail to capture the thick semantic content of human values, and how utility representations neglect the possible incommensurability of those values. The authors then critique the normativity of expected utility theory (EUT) for humans and AI, drawing upon arguments showing how rational agents need not comply with EUT, while highlighting how EUT is silent on which preferences are normatively acceptable. Finally, the researchers argue that these limitations motivate a reframing of the targets of AI alignment: Instead of alignment with the preferences of a human user, developer, or humanity-writ-large, AI systems should be aligned with normative standards appropriate to their social roles, such as the role of a general-purpose assistant. Furthermore, these standards should be negotiated and agreed upon by all relevant stakeholders. On this alternative conception of alignment, a multiplicity of AI systems will be able to serve diverse ends, aligned with normative standards that promote mutual benefit and limit harm despite our plural and divergent values.

21. ***Arctic-SnowCoder: Improving Code Pretraining with High-Quality Data: <br>Researchers from Snowflake and universities introduced Arctic-SnowCoder, a data-efficient model that achieves state-of-the-art performance in code pretraining. Through progressively refined data phases, the model demonstrates the importance of high-quality data aligned with downstream applications, outperforming larger models despite being trained on fewer tokens.*** <br><br>
    Aug 30, Snowflake, UIUC and Seul Nat Uni published a [paper](https://arxiv.org/pdf/2409.02326) “Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining”. Recent studies have been increasingly demonstrating that high-quality data is crucial for effective pretraining of language models. However, the precise definition of "high-quality" remains underexplored. Focusing on the code domain, the paper introduces Arctic-SnowCoder-1.3B, a data-efficient base code model pretrained on 555B tokens through three phases of progressively refined data: (1) general pretraining with 500B standard-quality code tokens, preprocessed through basic filtering, deduplication, and decontamination, (2) continued pretraining with 50B high-quality tokens, selected from phase one by a BERT-style quality annotator trained to distinguish good code from random data, using positive examples drawn from high-quality code files, along with instruction data from Magicoder and StarCoder2-Instruct, and (3) enhanced pretraining with 5B synthetic data created by Llama-3.1-70B using phase two data as seeds, adapting the Magicoder approach for pretraining. Despite being trained on a limited dataset, Arctic-SnowCoder achieves state-of-the-art performance on BigCodeBench, a coding benchmark focusing on practical and challenging programming tasks, compared to similarly sized models trained on no more than 1T tokens, outperforming Phi-1.5-1.3B by 36%. Across all evaluated benchmarks, Arctic-SnowCoder-1.3B beats StarCoderBase-3B pretrained on 1T tokens. Additionally, it matches the performance of leading small base code models trained on trillions of tokens. For example, Arctic-SnowCoder-1.3B surpasses StarCoder2-3B, pretrained on over 3.3T tokens, on HumanEval+, a benchmark that evaluates function-level code generation, and remains competitive on BigCodeBench. The evaluation presents a comprehensive analysis justifying various design choices for Arctic-SnowCoder. Most importantly, the authors find that the key to high-quality data is its alignment with the distribution of downstream applications.

23. ***Jina-ColBERT-v2: Advancing Multilingual Retrieval: <br>A paper presents Jina-ColBERT-v2, an optimized version of the ColBERT model for multilingual retrieval tasks. By improving efficiency and cutting storage requirements, the model demonstrates strong performance across multiple retrieval benchmarks while maintaining its effectiveness in a bi-encoder architecture.*** <br><br>
    Aug 30, Uni of Texas at Austin and Jina AI GmbH published a [paper](https://arxiv.org/pdf/2408.16672) “Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever”. Multi-vector dense models, such as ColBERT, have proven highly effective in information retrieval. ColBERT's late interaction scoring approximates the joint query-document attention seen in cross-encoders while maintaining inference efficiency closer to traditional dense retrieval models, thanks to its bi-encoder architecture and recent optimizations in indexing and search. This paper introduces several improvements to the ColBERT model architecture and training pipeline, leveraging techniques successful in the more established single-vector embedding model paradigm, particularly those suited for heterogeneous multilingual data. The new model, Jina-ColBERT-v2, demonstrates strong performance across a range of English and multilingual retrieval tasks, while also cutting storage requirements by up to 50% compared to previous models.

25. ***Improving OCR with Context Leveraging Models: <br>University College London's paper introduces CLOCR-C, a model that leverages context-adaptive abilities of transformer-based language models to correct OCR errors in digitized historical archives. By incorporating socio-cultural context, CLOCR-C significantly reduces error rates and improves the quality of OCR for downstream tasks like Named Entity Recognition.*** <br><br>
    Aug 30, Uni of College London published a [paper](https://arxiv.org/pdf/2408.17428) “CLOCR-C: Context Leveraging OCR Correction with Pre-trained Language Models”. The digitisation of historical print media archives is crucial for increasing accessibility to contemporary records. However, the process of Optical Character Recognition (OCR) used to convert physical records to digital text is prone to errors, particularly in the case of newspapers and periodicals due to their complex layouts. This paper introduces Context Leveraging OCR Correction (CLOCR-C), which utilises the infilling and context-adaptive abilities of transformer-based language models (LMs) to improve OCR quality. The study aims to determine if LMs can perform post-OCR correction, improve downstream NLP tasks, and the value of providing the socio-cultural context as part of the correction process. Experiments were conducted using seven LMs on three datasets: the 19th Century Serials Edition (NCSE) and two datasets from the Overproof collection. The results demonstrate that some LMs can significantly reduce error rates, with the top-performing model achieving over a 60% reduction in character error rate on the NCSE dataset. The OCR improvements extend to downstream tasks, such as Named Entity Recognition, with increased Cosine Named Entity Similarity. Furthermore, the study shows that providing socio-cultural context in the prompts improves performance, while misleading prompts lower performance. In addition to the findings, this study releases a dataset of 91 transcribed articles from the NCSE, containing a total of 40 thousand words, to support further research in this area. The findings suggest that CLOCR-C is a promising approach for enhancing the quality of existing digital archives by leveraging the socio-cultural information embedded in the LMs and the text requiring correction.

27. ***PrivacyLens: Evaluating Privacy Norm Awareness in LLMs: <br>a paper proposes PrivacyLens, a framework to evaluate LLMs’ awareness of privacy norms in communication scenarios. Results show that even state-of-the-art LLMs, like GPT-4 and Llama-3, often leak sensitive information despite privacy-enhancing prompts. PrivacyLens provides a structured evaluation tool to measure and mitigate privacy risks in LLMs.*** <br><br>
    Aug 29, Stanford Uni, Northeastern Uni and Harvard Uni published a [paper](https://arxiv.org/pdf/2409.00138) “PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action”. As language models (LMs) are widely utilized in personalized communication scenarios (e.g., sending emails, writing social media posts) and endowed with a certain level of agency, ensuring they act in accordance with the contextual privacy norms becomes increasingly critical. However, quantifying the privacy norm awareness of LMs and the emerging privacy risk in LM-mediated communication is challenging due to (1) the contextual and long-tailed nature of privacy-sensitive cases, and (2) the lack of evaluation approaches that capture realistic application scenarios. To address these challenges, the authors propose PrivacyLens, a novel framework designed to extend privacy-sensitive seeds into expressive vignettes and further into agent trajectories, enabling multi-level evaluation of privacy leakage in LM agents' actions. The authors instantiate PrivacyLens with a collection of privacy norms grounded in privacy literature and crowdsourced seeds. Using this dataset, the paper reveals a discrepancy between LM performance in answering probing questions and their actual behavior when executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4 and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even when prompted with privacy-enhancing instructions. The authors also demonstrate the dynamic nature of PrivacyLens by extending each seed into multiple trajectories to red-team LM privacy leakage risk. Dataset and code are available at https://github.com/SALT-NLP/PrivacyLens.

29. ***LiGNN: LinkedIn’s Large-Scale Graph Neural Networks: <br>LinkedIn's award-winning paperdetails LiGNN, a framework for deploying large-scale graph neural networks (GNNs). With algorithmic improvements and scalable solutions, LiGNN enhances the quality of GNN representation learning, driving measurable improvements in user engagement metrics, such as job application rates and ad click-through rates.*** <br><br>
    Aug 29, one of KDD2024 Best [paper](https://dl.acm.org/doi/10.1145/3637528.3671566) is LinkedIn’s “LiGNN: Graph Neural Networks at LinkedIn”. This paper presents LiGNN, a deployed large-scale Graph Neural Networks (GNNs) Framework. The authors share the insight on developing and deployment of GNNs at large scale at LinkedIn. The paper presents a set of algorithmic improvements to the quality of GNN representation learning including temporal graph architectures with long term losses, effective cold start solutions via graph densification, ID embeddings and multi-hop neighbor sampling. The authors explain how to build and speed up by 7x the large-scale training on LinkedIn graphs with adaptive sampling of neighbors, grouping and slicing of training data batches, specialized shared-memory queue and local gradient optimization. The paper summarizes the deployment lessons and learnings gathered from A/B test experiments. The techniques presented in this work have contributed to an approximate relative improvements of 1% of Job application hearing back rate, 2% Ads CTR lift, 0.5% of Feed engaged daily active users, 0.2% session lift and 0.1% weekly active user lift from people recommendation. The authors believe that this work can provide practical solutions and insights for engineers who are interested in applying Graph neural networks at large scale.

31. ***Introduction of SurveySum Dataset for Scientific Article Summarization: <br>The paper titled "SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section" presents a novel dataset, SurveySum, which fills a gap in domain-specific summarization tools. The paper introduces two pipelines for summarizing scientific articles into a survey section and evaluates these pipelines using various metrics. The results emphasize the critical role of high-quality retrieval stages and different configurations for improving the quality of generated summaries.*** <br><br>
    Aug 29, Brasília-DF published a [paper](https://www.arxiv.org/pdf/2408.16444) “SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section”. Document summarization is a task to shorten texts into concise and informative summaries. This paper introduces a novel dataset designed for summarizing multiple scientific articles into a section of a survey. The contributions of the paper are: (1) SurveySum, a new dataset addressing the gap in domain-specific summarization tools; (2) two specific pipelines to summarize scientific articles into a section of a survey; and (3) the evaluation of these pipelines using multiple metrics to compare their performance. The results highlight the importance of high-quality retrieval stages and the impact of different configurations on the quality of generated summaries.

33. ***Generative Verifiers in Reward Modeling and Next-Token Prediction: <br>Google’s paper, "Generative Verifiers: Reward Modeling as Next-Token Prediction," discusses improving the performance of large language models (LLMs) using verifiers trained through next-token prediction. This method, referred to as generative verifiers (GenRM), combines verification with solution generation. GenRM enhances chain-of-thought reasoning and integrates seamlessly with instruction tuning, outperforming standard discriminative verifiers and LLM-as-a-Judge by 16-64% on various reasoning tasks.*** <br><br>
    Aug 28, Google published a [paper](https://arxiv.org/pdf/2408.15240) “Generative Verifiers: Reward Modeling as Next-Token Prediction”. Verifiers or reward models are often used to enhance the reasoning performance of large language models (LLMs). A common approach is the Best-of-N method, where N candidate solutions generated by the LLM are ranked by a verifier, and the best one is selected. While LLM-based verifiers are typically trained as discriminative classifiers to score solutions, they do not utilize the text generation capabilities of pretrained LLMs. To overcome this limitation, the study instead proposes training verifiers using the ubiquitous next-token prediction objective, jointly on verification and solution generation. Compared to standard verifiers, such generative verifiers (GenRM) can benefit from several advantages of LLMs: they integrate seamlessly with instruction tuning, enable chain-of-thought reasoning, and can utilize additional inference-time compute via majority voting for better verification. The authors demonstrate that when using Gemma-based verifiers on algorithmic and grade-school math reasoning tasks, GenRM outperforms discriminative verifiers and LLM-as-a-Judge, showing a 16-64% improvement in the percentage of problems solved with Best-of-N. Furthermore, the study shows that GenRM scales favorably across dataset size, model capacity, and inference-time compute.

35. ***Engaged Human Learning Through Language Model Agent Conversations: <br>Stanford and Yale Universities introduced "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations." The study proposes Co-STORM, an LM-powered system where users observe and interact with conversations between multiple LM agents to discover unknown unknowns. Co-STORM organizes information into dynamic mind maps and outperforms traditional search engines and RAG chatbots in human evaluation.*** <br><br>
    Aug 27, Stanford Uni and Yale Uni published a [paper](https://www.arxiv.org/pdf/2408.15232) “Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations”. While language model (LM)-powered chatbots and generative search engines excel at answering concrete queries, discovering information in the terrain of unknown unknowns remains challenging for users. To emulate the common educational scenario where children/students learn by listening to and participating in conversations of their parents/teachers, the study creates Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all the questions, Co-STORM lets users observe and occasionally steer the discourse among several LM agents. The agents ask questions on the user's behalf, allowing the user to discover unknown unknowns serendipitously. To facilitate user interaction, Co-STORM assists users in tracking the discourse by organizing the uncovered information into a dynamic mind map, ultimately generating a comprehensive report as takeaways. For automatic evaluation, the authors construct the WildSeek dataset by collecting real information-seeking records with user goals. Co-STORM outperforms baseline methods on both discourse trace and report quality. In a further human evaluation, 70% of participants prefer Co-STORM over a search engine, and 78% favor it over a RAG chatbot.

37. ***Loss of Plasticity in Deep Continual Learning: <br>The University of Alberta’s paper published in Nature explores the limitations of deep learning in continual learning settings. It shows that deep learning models gradually lose plasticity, learning no better than shallow networks. The study highlights that only algorithms injecting random variability, such as continual backpropagation, can maintain plasticity indefinitely, suggesting that gradient-descent-based methods alone are insufficient for sustained deep learning.*** <br><br>
    Aug 21, Uni of Alberta published a [paper](https://www.nature.com/articles/s41586-024-07711-7) on Nature “Loss of plasticity in deep continual learning”. Artificial neural networks, deep-learning methods and the backpropagation algorithm form the foundation of modern machine learning and artificial intelligence. These methods are almost always used in two phases, one in which the weights of the network are updated and one in which the weights are held constant while the network is used or evaluated. This contrasts with natural learning and many applications, which require continual learning. It has been unclear whether or not deep learning methods work in continual learning settings. Here the authors show that they do not—that standard deep-learning methods gradually lose plasticity in continual-learning settings until they learn no better than a shallow network. The study shows such loss of plasticity using the classic ImageNet dataset and reinforcement-learning problems across a wide range of variations in the network and the learning algorithm. Plasticity is maintained indefinitely only by algorithms that continually inject diversity into the network, such as the proposed continual backpropagation algorithm, a variation of backpropagation in which a small fraction of less-used units are continually and randomly reinitialized. The results indicate that methods based on gradient descent are not enough—that sustained deep learning requires a random, non-gradient component to maintain variability and plasticity.
 <br><br><br>

***Sep 1 2024***

1. ***Stanford Uni and UC Berkeley on "Law of Vision Representation": <br>This paper introduces the "Law of Vision Representation" in multimodal large language models (MLLMs). The authors find a strong correlation between cross-modal alignment, vision representation, and model performance. They develop the AC score, which quantifies these factors and shows a linear relationship with model performance. By identifying optimal vision representations without needing to fine-tune the language model, they reduce computational costs by 99.7%.*** <br><br>
   Aug 29, Stanford Uni and UC Berkeley published a [paper](https://arxiv.org/pdf/2408.16357) “Law of Vision Representation in MLLMs”. The paper presents the "Law of Vision Representation" in multimodal large language models (MLLMs). It reveals a strong correlation between the combination of cross-modal alignment, correspondence in vision representation, and MLLM performance. The authors quantify the two factors using the cross-modal Alignment and Correspondence score (AC score). Through extensive experiments involving thirteen different vision representation settings and evaluations across eight benchmarks, the paper finds that the AC score is linearly correlated to model performance. By leveraging this relationship, the authors are able to identify and train the optimal vision representation only, which does not require finetuning the language model every time, resulting in a 99.7% reduction in computational cost.

3. ***HKUST and Huggingface on "LlamaDuo": <br>"LlamaDuo" is introduced as an LLMOps pipeline that enables migration from service-oriented LLMs to smaller, local models. This approach addresses challenges like operational failures, privacy concerns, and offline requirements. By iteratively fine-tuning smaller models with synthetic datasets from service LLMs, it ensures smaller models can match or surpass the capabilities of service LLMs for specific tasks.*** <br><br>
   Aug 29, HKUST and Huggingface published a [paper](https://www.arxiv.org/pdf/2408.13467) “LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMs”. The widespread adoption of cloud-based proprietary large language models (LLMs) has introduced significant challenges, including operational dependencies, privacy concerns, and the necessity of continuous internet connectivity. This paper introduces an LLMOps pipeline, "LlamaDuo", for the seamless migration of knowledge and abilities from service-oriented LLMs to smaller, locally manageable models. This pipeline is crucial for ensuring service continuity in the presence of operational failures, strict privacy policies, or offline requirements. LlamaDuo involves fine-tuning a small language model against the service LLM using a synthetic dataset generated by the latter. If the performance of the fine-tuned model falls short of expectations, it is enhanced by further fine-tuning with additional similar data created by the service LLM. This iterative process guarantees that the smaller model can eventually match or even surpass the service LLM's capabilities in specific downstream tasks, offering a practical and scalable solution for managing AI deployments in constrained environments. Extensive experiments with leading edge LLMs are conducted to demonstrate the effectiveness, adaptability, and affordability of LlamaDuo across various downstream tasks. The pipeline implementation is available at https://github.com/deep-diver/llamaduo.

5. ***Cambridge and University of Hong Kong on "GRAB": <br>The paper presents "GRAB," a graph analysis benchmark for large multimodal models (LMMs). The benchmark consists of 2,170 questions covering 23 graph properties, challenging current LMMs, with the top-performing model scoring only 21.7%. The goal is to push LMMs' capabilities in graph analysis.*** <br><br>
   Aug 29, Uni of Cambridge and the Uni of Hong Kong published a [paper](https://www.arxiv.org/pdf/2408.11817) “GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models”. Large multimodal models (LMMs) have exhibited proficiencies across many visual tasks. Although numerous well-known benchmarks exist to evaluate model performance, they increasingly have insufficient headroom. As such, there is a pressing need for a new generation of benchmarks challenging enough for the next generation of LMMs. One area that LMMs show potential is graph analysis, specifically, the tasks an analyst might typically perform when interpreting figures such as estimating the mean, intercepts or correlations of functions and data series. This paper introduces GRAB, a graph analysis benchmark, fit for current and future frontier LMMs. The benchmark is entirely synthetic, ensuring high-quality, noise-free questions. GRAB is comprised of 2170 questions, covering four tasks and 23 graph properties. The authors evaluate 20 LMMs on GRAB, finding it to be a challenging benchmark, with the highest performing model attaining a score of just 21.7%. Finally, the authors conduct various ablations to investigate where the models succeed and struggle. The authors [release GRAB](https://grab-benchmark.github.io/) to encourage progress in this important, growing domain.

7. ***Nvidia and collaborators on "Eagle": <br>This paper explores multimodal LLM design using a mixture of vision encoders. It shows that concatenating visual tokens from multiple vision encoders is as effective as more complex architectures. The authors also introduce Pre-Alignment, improving model coherence and performance on major MLLM benchmarks, with the Eagle model family outperforming leading models.*** <br><br>
   Aug 28, Nvidia, Georgia Tech, UMD and HKPU published a [paper](https://arxiv.org/pdf/2408.15998) “Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders”. The ability to accurately interpret complex visual information is a crucial topic of multimodal large language models (MLLMs). Recent work indicates that enhanced visual perception significantly reduces hallucinations and improves performance on resolution-sensitive tasks, such as optical character recognition and document analysis. A number of recent MLLMs achieve this goal using a mixture of vision encoders. Despite their success, there is a lack of systematic comparisons and detailed ablation studies addressing critical aspects, such as expert selection and the integration of multiple vision experts. This study provides an extensive exploration of the design space for MLLMs using a mixture of vision encoders and resolutions. The findings reveal several underlying principles common to various existing strategies, leading to a streamlined yet effective design approach. The paper discovers that simply concatenating visual tokens from a set of complementary vision encoders is as effective as more complex mixing architectures or strategies. The authors additionally introduce Pre-Alignment to bridge the gap between vision-focused encoders and language tokens, enhancing model coherence. The resulting family of MLLMs, Eagle, surpasses other leading open-source models on major MLLM benchmarks. Models and code: https://github.com/NVlabs/Eagle
 
9. ***Bar-Ilan and Allen Institute for AI on "Knowledge Navigator": <br>"Knowledge Navigator" is a system for enhancing exploratory search in scientific literature by organizing search results into a two-level hierarchy of topics and subtopics. This structured approach aids users in refining searches and discovering deeper knowledge across scientific domains.*** <br><br>
    Aug 28, Bar-Ilan and Allen Inst for AI published a [paper](https://www.arxiv.org/pdf/2408.15836) “Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature”. The exponential growth of scientific literature necessitates advanced tools for effective knowledge exploration. The paper presents Knowledge Navigator, a system designed to enhance exploratory search abilities by organizing and structuring the retrieved documents from broad topical queries into a navigable, two-level hierarchy of named and descriptive scientific topics and subtopics. This structured organization provides an overall view of the research themes in a domain, while also enabling iterative search and deeper knowledge discovery within specific subtopics by allowing users to refine their focus and retrieve additional relevant documents. Knowledge Navigator combines LLM capabilities with cluster-based methods to enable an effective browsing method. The paper demonstrates the approach's effectiveness through automatic and manual evaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. The code, prompts, and benchmarks are made [publicly available](https://knowledge-navigators.github.io/).

11. ***Writer, Inc. on "Writing in the Margins": <br>"Writing in the Margins" (WiM) is a new inference pattern for improving LLM performance in long-context retrieval tasks. By segmenting and inferring information in chunks, it boosts accuracy by 7.5% for reasoning tasks and over 30% for aggregation tasks, without fine-tuning models. WiM is implemented using Hugging Face's Transformers library.*** <br><br>
    Aug 27, Writer, Inc. published a [paper](https://www.arxiv.org/pdf/2408.14906) “Writing in the Margins: Better Inference Pattern for Long Context Retrieval”. The paper introduces Writing in the Margins (WiM), a new inference pattern for Large Language Models designed to optimize the handling of long input sequences in retrieval-oriented tasks. This approach leverages the chunked prefill of the key-value cache to perform segment-wise inference, which enables efficient processing of extensive contexts along with the generation and classification of intermediate information ("margins") that guide the model towards specific tasks. This method increases computational overhead marginally while significantly enhancing the performance of off-the-shelf models without the need for fine-tuning. Specifically, the authors observe that WiM provides an average enhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG) and more than a 30.0% increase in the F1-score for aggregation tasks (CWE). Additionally, the paper shows how the proposed pattern fits into an interactive retrieval design that provides end-users with ongoing updates about the progress of context processing, and pinpoints the integration of relevant information into the final response. The authors release the implementation of WiM using Hugging Face Transformers library at https://github.com/writer/writing-in-the-margins.

13. ***UC Berkeley and Stanford on "Text2SQL is Not Enough": <br>The "Table-Augmented Generation" (TAG) paradigm is introduced to expand the scope of natural language questions over databases, going beyond Text2SQL and Retrieval-Augmented Generation methods. TAG enables broader interactions between LMs and databases, offering new research opportunities in data query handling.*** <br><br>
    Aug 27, UC Berkeley and Stanford Uni. Published a [paper](https://www.arxiv.org/pdf/2408.14717) “Text2SQL is Not Enough: Unifying AI and Databases with TAG”. AI systems that serve natural language questions over databases promise to unlock tremendous value. Such systems would allow users to leverage the powerful reasoning and knowledge capabilities of language models (LMs) alongside the scalable computational power of data management systems. These combined capabilities would empower users to ask arbitrary natural language questions over custom data sources. However, existing methods and benchmarks insufficiently explore this setting. Text2SQL methods focus solely on natural language questions that can be expressed in relational algebra, representing a small subset of the questions real users wish to ask. Likewise, Retrieval-Augmented Generation (RAG) considers the limited subset of queries that can be answered with point lookups to one or a few data records within the database. The paper proposes Table-Augmented Generation (TAG), a unified and general-purpose paradigm for answering natural language questions over databases. The TAG model represents a wide range of interactions between the LM and database that have been previously unexplored and creates exciting research opportunities for leveraging the world knowledge and reasoning capabilities of LMs over data. The paper systematically develops benchmarks to study the TAG problem and find that standard methods answer no more than 20% of queries correctly, confirming the need for further research in this area. The authors release code for the benchmark at https://github.com/TAG-Research/TAG-Bench.

15. ***Cornell, Geneva, Together AI, and Princeton on "Mamba in the Llama": <br>This paper discusses distilling large Transformers into linear RNNs like Mamba. The hybrid model created, using a quarter of the original attention layers, matches or exceeds Transformer performance in chat benchmarks. The authors introduce a speculative decoding algorithm, improving the model's inference speed and deployment efficiency.*** <br><br>
    Aug 27, Uni of Cornell, Uni of Geneva, together AI and Uni of Princeton published a [paper](https://arxiv.org/pdf/2408.15237) “The Mamba in the Llama: Distilling and Accelerating Hybrid Models”. Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics. Given the focus on training large-scale Transformer models, the paper considers the challenge of converting these pretrained models for deployment. The paper demonstrates that it is feasible to distill large Transformers into linear RNNs by reusing the linear projection weights from attention layers with academic GPU resources. The resulting hybrid model, which incorporates a quarter of the attention layers, achieves performance comparable to the original Transformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch with trillions of tokens in both chat benchmarks and general benchmarks. Moreover, the work introduces a hardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid models. Overall the paper shows how, with limited computation resources, can remove many of the original attention layers and generate from the resulting model more efficiently. The top-performing model, distilled from Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and 7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.

17. ***Cerebras on "Cerebras Inference": <br>Cerebras introduces a new AI inference solution that delivers significantly faster token processing speeds and lower costs compared to NVIDIA-based solutions. Using its Wafer Scale Engine, Cerebras Inference handles Llama3 models at 20x the speed of GPU solutions, offering a scalable, cost-effective platform for high-speed inference.*** <br><br>
    Aug 27, Cerebras announced its [Cerebras Inference](https://cerebras.ai/blog/introducing-cerebrcas-inference-ai-at-instant-speed), “Introducing Cerebras Inference: AI at Instant Speed”.  Cerebras inference delivers 1,800 tokens per second for Llama3.1 8B and 450 tokens per second for Llama3.1 70B, which is 20x faster than NVIDIA GPU-based hyperscale clouds. Cerebras inference offers the industry’s best pricing at 10c per million tokens for Lama 3.1 8B and 60c per million tokens for Llama 3 70B. Cerebras inference is open to developers today via API access. Powered by the third generation Wafer Scale Engine, Cerebras inference runs Llama3.1 20x faster than GPU solutions at 1/5 the price. At 1,800 tokens/s, Cerebras Inference is 2.4x faster than Groq in Llama3.1-8B. For Llama3.1-70B, Cerebras is the only platform to enable instant responses at a blistering 450 tokens/sec. All this is achieved using native 16-bit weights for the model, ensuring the highest accuracy responses. Cerebras solves the memory bandwidth bottleneck by building the largest chip in the world and storing the entire model on-chip. With the unique wafer-scale design, Cerebrate is able to integrate 44GB of SRAM on a single chip – eliminating the need for external memory and for the slow lanes linking external memory to compute. In total, the WSE-3 has 21 petabytes/s of aggregate memory bandwidth – 7,000x that of an H100. It is the only AI chip with both petabyte-scale compute and petabyte-scale memory bandwidth, making it a near ideal design for high-speed inference.

19. ***Nous Research on "DisTrO": <br>"DisTrO" is a distributed optimizer that drastically reduces inter-GPU communication requirements, enabling large neural network training without high-speed interconnects. This allows for more efficient and scalable training of large models on low-bandwidth networks, matching the performance of existing optimization methods.*** <br><br>
    Aug 26, Nous Research published its [report](https://github.com/NousResearch/DisTrO) “A Preliminary Report on DisTro”. Training large scale neural networks typically involves sharing gradients between all accelerators, which necessitates specialized, high-speed interconnects. To address this, the paper introduces DisTrO, a family of architecture-agnostic and network-agnostic distributed optimizers that reduces the inter-GPU communication requirements by four to five orders of magnitude without relying on amortized analysis, enabling low-latency training of large neural networks on slow internet bandwidths with heterogeneous networking hardware. In this preliminary report the authors are excited to show the first and earliest empirical proof that DisTrO-AdamW matches standard AdamW+All-Reduce in convergence rate while massively reducing the required bandwidth during pre-training of a 1.2B LLM. When using Distributed Data Parallelism, DisTrO may enable future large scale foundation model training to bypass the need for high-speed interconnects entirely.

21. ***University of Hong Kong and collaborators on Transformer Efficiency: <br>The paper presents a method for approximating gradients in multi-layer Transformers with near-linear time complexity. This approach significantly reduces the computational demands of training and inference, making it more efficient to handle large sequences and long-context language models.*** <br><br>
    Aug 23, Uni of HK, Uni of Wisconsin-Madison, Tsinghua Uni, and Adobe published a [paper](https://www.arxiv.org/pdf/2408.13233) “Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time”. The quadratic computational complexity in the self-attention mechanism of popular transformer architectures poses significant challenges for training and inference, particularly in terms of efficiency and memory requirements. Towards addressing these challenges, this paper introduces a novel fast computation method for gradient calculation in multi-layer transformer models. The approach enables the computation of gradients for the entire multi-layer transformer model in almost linear time n^{1+o(1)}, where n is the input sequence length. This breakthrough significantly reduces the computational bottleneck associated with the traditional quadratic time complexity. The theory holds for any loss function and maintains a bounded approximation error across the entire model. Furthermore, the analysis can hold when the multi-layer transformer model contains many practical sub-modules, such as residual connection, casual mask, and multi-head attention. By improving the efficiency of gradient computation in large language models, the authors hope that the work will facilitate the more effective training and deployment of long-context language models based on the theoretical results.

23. ***West Pharmaceutical, Stanford, and Amazon on "RoundTable": <br>"RoundTable" introduces a dynamic schema and contextual autocomplete system to enhance query precision in databases. By leveraging full-text search and suggesting queries based on data in the table, this framework improves LLM accuracy when interacting with complex datasets.*** <br><br>
    Aug 23 West Pharmaceutical Services, Inc., Stanford Uni, and Amazon published a [paper](https://arxiv.org/pdf/2408.12369v1) “RoundTable: Leveraging Dynamic Schema and Contextual Autocomplete for Enhanced Query Precision in Tabular Question Answering”. With advancements in Large Language Models (LLMs), a major use case that has emerged is querying databases in plain English, translating user questions into executable database queries, which has improved significantly. However, real-world datasets often feature a vast array of attributes and complex values, complicating the LLMs task of accurately identifying relevant columns or values from natural language queries. Traditional methods cannot fully relay the datasets size and complexity to the LLM. To address these challenges, the paper proposes a novel framework that leverages Full-Text Search (FTS) on the input table. This approach not only enables precise detection of specific values and columns but also narrows the search space for language models, thereby enhancing query accuracy. Additionally, it supports a custom auto-complete feature that suggests queries based on the data in the table. This integration significantly refines the interaction between the user and complex datasets, offering a sophisticated solution to the limitations faced by current table querying capabilities. This work is accompanied by an application for both Mac and Windows platforms, which readers can try out themselves on their own data.

 <br><br>

***Aug 25 2024***


1. ***"Real-Time Video Generation with Pyramid Attention Broadcast": <br>The paper introduces Pyramid Attention Broadcast (PAB), a real-time, training-free method for DiT-based video generation. PAB addresses redundancy in the diffusion process by broadcasting attention outputs in a pyramid style, applying different strategies based on attention variance. The method enables efficient distributed inference, achieving real-time generation for up to 720p videos and is expected to serve as a robust baseline for future research.*** <br><br>
   Aug 22, National Uni of Singapore, VideoSys and Purdue Uni published a [paper](https://arxiv.org/pdf/2408.12588) “Real-Time Video Generation with Pyramid Attention Broadcast”. The paper presents Pyramid Attention Broadcast (PAB), a real-time, high quality and training-free approach for DiT-based video generation. The method is founded on the observation that attention difference in the diffusion process exhibits a U-shaped pattern, indicating significant redundancy. The authors mitigate this by broadcasting attention outputs to subsequent steps in a pyramid style. It applies different broadcast strategies to each attention based on their variance for best efficiency. The paper further introduces broadcast sequence parallel for more efficient distributed inference. PAB demonstrates superior results across three models compared to baselines, achieving real-time generation for up to 720p videos. The authors anticipate that this simple yet effective method will serve as a robust baseline and facilitate future research and application for video generation. <br><br>

3. ***"Great Memory, Shallow Reasoning: Limits of kNN-LMs": <br>This paper critically evaluates k-nearest neighbor language models (kNN-LMs), which excel in memory-intensive tasks but struggle with reasoning tasks that require integrating multiple information pieces. Even with perfect retrieval, kNN-LMs fail at determining correct answers in complex tasks, suggesting a limit to their reasoning capabilities.*** <br><br>
   Aug 21, Cornell Uni published a [paper](https://arxiv.org/pdf/2408.11815) “Great Memory, Shallow Reasoning: Limits of kNN-LMs”. K-nearest neighbor language models (kNN-LMs), which integrate retrieval with next-word prediction, have demonstrated strong performance in language modeling as well as downstream NLP benchmarks. These results have led researchers to argue that models trained on poor quality or outdated data could perform well by employing a kNN extension that has access to a higher-quality datastore. This work asks whether this improved ability to recall information really translates into downstream abilities. The paper extensively evaluates kNN-LMs on a diverse set of tasks, ranging from sentiment classification and commonsense reasoning to multi-hop reasoning. Results show that kNN-LMs excel at memory-intensive tasks, where utilizing the patterns in the input is sufficient for determining the output, but struggle with reasoning tasks that require integrating multiple pieces of information to derive new knowledge. The paper further demonstrates through oracle experiments and qualitative analysis that even with perfect retrieval, kNN-LMs still fail to determine the correct answers, placing an upper bound on their reasoning performance. Code and datastores are released at [this https URL](https://github.com/GSYfate/knnlm-limits/). <br><br>

5. ***"Anthropic Faces Copyright Lawsuit Over AI Training Data": <br>A lawsuit has been filed against Anthropic by three authors accusing the company of using their copyrighted content without permission to train its Claude AI chatbot. This reflects ongoing legal challenges in the AI industry concerning the use of copyrighted material, raising concerns about copyright infringement and the implications for AI training quality and accuracy.*** <br><br>
   Aug 21, windowscentral.com published an [article](https://www.windowscentral.com/software-apps/openai-ceo-sam-altmans-words-haunt-claude-ai) “OpenAI CEO Sam Altman's words haunt Claude AI: ‘Anthropic’s model seeks to profit from strip-mining the human expression and ingenuity behind each one of those works’”. A lawsuit has been filed against Anthropic by three authors, accusing the company of using their copyrighted content to train its Claude AI chatbot without permission. This follows a pattern of legal challenges in the AI industry, including similar cases against OpenAI and Microsoft. The lawsuit claims that Anthropic's model profits from exploiting human creativity without compensating creators, raising concerns about copyright infringement in AI training. While tech companies argue that using copyrighted content is "fair use," restricting AI from such content could diminish the quality and accuracy of chatbot responses, potentially leading to further issues. OpenAI CEO Sam Altman had previously admitted it's impossible to create ChatGPT-like tools without copyrighted content. <br><br>

7. ***"Xinyu: Efficient LLM-based System for Commentary Generation": <br>The paper introduces Xinyu, a system designed to assist in generating Chinese commentaries by deconstructing the process into sequential steps and using strategies like supervised fine-tuning and retrieval augmented generation. Xinyu significantly increases commentary creation efficiency without compromising quality, reducing the time needed from 4 hours to 20 minutes.*** <br><br>
   Aug 21, Zhejiang Uni etc published a [paper](https://arxiv.org/pdf/2408.11609) on KDD24 “Xinyu: An Efficient LLM-based System for Commentary Generation”. Commentary provides readers with a deep understanding of events by presenting diverse arguments and evidence. However, creating commentary is a time-consuming task, even for skilled commentators. Large language models (LLMs) have simplified the process of natural language generation, but their direct application in commentary creation still faces challenges due to unique task requirements. These requirements can be categorized into two levels: 1) fundamental requirements, which include creating well-structured and logically consistent narratives, and 2) advanced requirements, which involve generating quality arguments and providing convincing evidence. This paper introduces Xinyu, an efficient LLM-based system designed to assist commentators in generating Chinese commentaries. To meet the fundamental requirements, the paper deconstructs the generation process into sequential steps, proposing targeted strategies and supervised fine-tuning (SFT) for each step. To address the advanced requirements, the paper presents an argument ranking model for arguments and establish a comprehensive evidence database that includes up-to-date events and classic books, thereby strengthening the substantiation of the evidence with retrieval augmented generation (RAG) technology. To evaluate the generated commentaries more fairly, corresponding to the two-level requirements, the paper introduces a comprehensive evaluation metric that considers five distinct perspectives in commentary generation. The experiments confirm the effectiveness of the proposed system. The paper also observes a significant increase in the efficiency of commentators in real-world scenarios, with the average time spent on creating a commentary dropping from 4 hours to 20 minutes. Importantly, such an increase in efficiency does not compromise the quality of the commentaries. <br><br>

9. ***"Transfusion: Multi-Modal Model Training with Unified Approach": <br>The paper presents Transfusion, a multi-modal model that combines next token prediction with diffusion to handle both discrete and continuous data. Transfusion models, pre-trained on a mixture of text and image data, show improved scaling and performance, offering benefits for both image and text generation, particularly when scaled to 7B parameters and 2T multi-modal tokens.*** <br><br>
    Aug 20, Meta, Waymo and Uni of Southern California published a [paper](https://www.arxiv.org/pdf/2408.11039) “Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model”. The paper introduces Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences. The authors pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. The experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific encoding and decoding layers, The study can further improve the performance of Transfusion models, and even compress each image to just 16 patches. The authors further demonstrate that scaling the Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds. <br><br>

11. ***"Impact of Code in LLM Pre-training: A Systematic Analysis": This paper investigates the impact of including code data in LLM pre-training, finding that code significantly enhances performance across a broad range of tasks beyond coding. The inclusion of code data results in notable improvements in natural language reasoning, world knowledge, generative win-rates, and code performance, highlighting the value of preserving code in pre-training.*** <br><br>
    Aug 20, Cohere published a [paper](https://arxiv.org/pdf/2408.10914) “To Code, or Not To Code? Exploring Impact of Code in Pre-training”. Including code in the pre-training data mixture, even for models not specifically designed for code, has become a common practice in LLMs pre-training. While there has been anecdotal consensus among practitioners that code data plays a vital role in general LLMs' performance, there is only limited work analyzing the precise impact of code on non-code tasks. This paper systematically investigates the impact of code data on general performance. The research asks "what is the impact of code data used in pre-training on a large variety of downstream tasks beyond code generation". The authors conduct extensive ablations and evaluate across a broad range of natural language reasoning tasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for models with sizes ranging from 470M to 2.8B parameters. Across settings, the paper finds a consistent results that code is a critical building block for generalization far beyond coding tasks and improvements to code quality have an outsized impact across all tasks. In particular, compared to text-only pre-training, the addition of code results in up to relative increase of 8.2% in natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement in generative win-rates, and a 12x boost in code performance respectively. The work suggests investments in code quality and preserving code during pre-training have positive impacts. <br><br>

13. ***"Ferret: Faster and Effective Automated Red Teaming": <br>The paper introduces Ferret, a new method for automated red teaming that builds upon Rainbow Teaming by generating multiple adversarial prompt mutations and ranking them with a reward-based scoring technique. Ferret achieves a 95% attack success rate, improving efficiency and effectiveness in generating adversarial prompts for large language models.*** <br><br>
    Aug 20, Singapore Uni of Tech and Design published a [paper](https://arxiv.org/pdf/2408.10701) “Ferret: Faster and Effective Automated Red Teaming with Reward-Based Scoring Technique”. In today's era, where large language models (LLMs) are integrated into numerous real-world applications, ensuring their safety and robustness is crucial for responsible AI usage. Automated red-teaming methods play a key role in this process by generating adversarial attacks to identify and mitigate potential vulnerabilities in these models. However, existing methods often struggle with slow performance, limited categorical diversity, and high resource demands. While Rainbow Teaming, a recent approach, addresses the diversity challenge by framing adversarial prompt generation as a quality-diversity search, it remains slow and requires a large fine-tuned mutator for optimal performance. To overcome these limitations, the paper proposes Ferret, a novel approach that builds upon Rainbow Teaming by generating multiple adversarial prompt mutations per iteration and using a scoring function to rank and select the most effective adversarial prompt. The authors explore various scoring functions, including reward models, Llama Guard, and LLM-as-a-judge, to rank adversarial mutations based on their potential harm to improve the efficiency of the search for harmful mutations. The results demonstrate that Ferret, utilizing a reward model as a scoring function, improves the overall attack success rate (ASR) to 95%, which is 46% higher than Rainbow Teaming. Additionally, Ferret reduces the time needed to achieve a 90% ASR by 15.2% compared to the baseline and generates adversarial prompts that are transferable i.e. effective on other LLMs of larger size. The codes are available at https://github.com/declare-lab/ferret. <br><br>

15. ***"Challenges with Dataset Construction in Computer Vision": <br>The paper argues that constructing representative image datasets for model testing is statistically implausible, making performance metrics unreliable for real-world deployment. The authors recommend focusing on assessing models' decision-making processes rather than accuracy metrics, as larger datasets or bias-aware datasets do not solve this issue.*** <br><br>
    Aug 20, McGill Uni and York University published a [paper](https://arxiv.org/pdf/2408.11160) “Statistical Challenges with Dataset Construction: Why You Will Never Have Enough Images”.  Deep neural networks have achieved impressive performance on many computer vision benchmarks in recent years. However, can we be confident that impressive performance on benchmarks will translate to strong performance in real-world environments? Many environments in the real world are safety critical, and even slight model failures can be catastrophic. Therefore, it is crucial to test models rigorously before deployment. The authors argue, through both statistical theory and empirical evidence, that selecting representative image datasets for testing a model is likely implausible in many domains. Furthermore, performance statistics calculated with non-representative image datasets are highly unreliable. As a consequence, there is no guarantee that models which perform well on withheld test images will also perform well in the real world. Creating larger and larger datasets will not help, and bias aware datasets cannot solve this problem either. Ultimately, there is little statistical foundation for evaluating models using withheld test sets. The paper recommends that future evaluation methodologies focus on assessing a model's decision-making process, rather than metrics such as accuracy. <br><br>

17. ***"Plan-based Retrieval for Grounded Text Generation": <br>This study examines how planning can guide retrieval to reduce hallucinations in text generation. By improving the coverage of relevant facts, the proposed plan-guided retrieval approach enhances the informativeness and attribution of generated responses, offering a promising method to mitigate hallucinations in language models.*** <br><br>
    Aug 20, Uni of Southern California and Google published a [paper](https://arxiv.org/pdf/2408.10490) “Analysis of Plan-based Retrieval for Grounded Text Generation”. In text generation, hallucinations refer to the generation of seemingly coherent text that contradicts established knowledge. One compelling hypothesis is that hallucinations occur when a language model is given a generation task outside its parametric knowledge (due to rarity, recency, domain, etc.). A common strategy to address this limitation is to infuse the language models with retrieval mechanisms, providing the model with relevant knowledge for the task. This paper leverages the planning capabilities of instruction-tuned LLMs and analyzes how planning can be used to guide retrieval to further reduce the frequency of hallucinations. The authors empirically evaluate several variations of the proposed approach on long-form text generation tasks. By improving the coverage of relevant facts, plan-guided retrieval and generation can produce more informative responses while providing a higher rate of attribution to source documents. <br><br>

19. ***"KAN 2.0: Bridging AI and Science with Kolmogorov-Arnold Networks": <br>The paper introduces a framework to integrate Kolmogorov-Arnold Networks (KANs) with scientific discovery, enhancing their ability to identify features, modular structures, and symbolic formulas. The framework allows for a bidirectional synergy between KANs and scientific knowledge, demonstrating KANs' potential in discovering various physical laws.*** <br><br>
    Aug 19, MIT, California Inst of Tech and NSF published a [paper](https://arxiv.org/pdf/2408.10205) “KAN 2.0: Kolmogorov-Arnold Networks Meet Science”. A major challenge of AI + Science lies in their inherent incompatibility: today's AI is primarily based on connectionism, while science depends on symbolism. To bridge the two worlds, the paper proposes a framework to seamlessly synergize Kolmogorov-Arnold Networks (KANs) and science. The framework highlights KANs' usage for three aspects of scientific discovery: identifying relevant features, revealing modular structures, and discovering symbolic formulas. The synergy is bidirectional: science to KAN (incorporating scientific knowledge into KANs), and KAN to science (extracting scientific insights from KANs). The authors highlight major new functionalities in the pykan package: (1) MultKAN: KANs with multiplication nodes. (2) kanpiler: a KAN compiler that compiles symbolic formulas into KANs. (3) tree converter: convert KANs (or any neural networks) to tree graphs. Based on these tools, the authors demonstrate KANs' capability to discover various types of physical laws, including conserved quantities, Lagrangians, symmetries, and constitutive laws.<br><br>

21. ***"Generative AI Hype and Its Practical Evolution": <br>The article discusses the gradual evolution of generative AI technology, emphasizing the shift from hype to practical utility. It highlights the emergence of smaller, more efficient models, the increasing focus on AI literacy, and the continuous improvement of generative AI, suggesting that AI will gradually transform human activities rather than replace them.*** <br><br>
    Aug 19, TheConversation.com published an [article](https://theconversation.com/generative-ai-hype-is-ending-and-now-the-technology-might-actually-become-useful-236940) “Generative AI hype is ending – and now the technology might actually become useful”. Large language models such as GPT-4 do not always match what people expect of them, such as the [experimental results](https://arxiv.org/pdf/2406.01382) from Harvard Uni etc. Experience from successful projects shows it is also tough to make a generative model follow instructions. However, GenAI hype isn’t over yet because first generative AI technology, despite its challenges, is rapidly improving, with scale and size being the primary drivers of the improvement. Second, studies have found sufficiently complex large language models can develop the ability to reason by analogy and even reproduce optical illusions like those experienced by humans. Next, 1) AI is being used to support humans, rather than replace them. 2) we also see a rise in [smaller (and cheaper) generative AI models](https://www.bloomberg.com/news/articles/2024-08-08/move-over-llms-small-ai-models-are-the-next-big-thing), trained on specific data and deployed locally to reduce costs and optimise efficiency. Even OpenAI, which has led the race for ever-larger models, has released the GPT-4o Mini model to reduce costs and improve performance. 3) we see a strong focus on providing AI literacy training and educating the workforce on how AI works, its potentials and limitations, and best practices for ethical AI use. Therefore, the AI revolution will look more like an evolution. Its use will gradually grow over time and, little by little, alter and transform human activities. Which is much better than replacing them. <br><br>

23. ***"IDEA: Enhancing Rule Learning in Interactive Environments": <br>The paper introduces IDEA, an agent designed to improve rule-learning abilities in large language models through induction, deduction, and abduction processes. Evaluated on the RULEARN benchmark, IDEA shows improved performance in interactive settings, offering insights for developing agents capable of human-like rule learning.*** <br><br>
    Aug 19, Uni of Texas at Dallas published a [paper](https://arxiv.org/pdf/2408.10455) “IDEA: Enhancing the rule learning ability of language agent through Induction, DEuction, and Abduction”. While large language models (LLMs) have been thoroughly evaluated for deductive and inductive reasoning, their proficiency in abductive reasoning and holistic rule learning in interactive environments remains less explored. This work introduces RULEARN, a novel benchmark specifically designed to assess the rule-learning ability of LLMs in interactive settings. In RULEARN, agents interact with the environment to gather observations and discern patterns, using these insights to solve problems. To further enhance the rule-learning capabilities of LLM agents within this benchmark, the paper proposes IDEA agent, which integrates Induction, Deduction, and Abduction processes. IDEA agent refines this approach by leveraging a structured reasoning sequence: generating hypotheses through abduction, testing them via deduction, and refining them based on induction feedback. This sequence enables agents to dynamically establish and apply rules, mimicking human-like reasoning processes. The evaluation of five representative LLMs indicates that while these models can generate plausible initial hypotheses, they often struggle with strategic interaction within the environment, effective incorporation of feedback, and adaptive refinement of their hypotheses. IDEA agent demonstrates significantly improved performance on the RULEARN benchmark, offering valuable insights for the development of agents capable of human-like rule-learning in real-world scenarios. We will release our code and data. <br><br>

25. ***"In-Context Learning with Transformers: A Theoretical Analysis": <br>This paper explores the training dynamics of transformers in in-context learning (ICL), showing that transformers can generalize to unseen examples by learning template functions in-context. The study provides a theoretical foundation for understanding ICL and demonstrates that transformers can effectively perform ridge regression over basis functions.*** <br><br>
    Aug 19, CMU, Upenn et al. published a [paper](https://arxiv.org/pdf/2408.10147) “In-Context Learning with Representations: Contextual Generalization of Trained Transformers”. In-context learning (ICL) refers to a remarkable capability of pretrained large language models, which can learn a new task given a few examples during inference. However, theoretical understanding of ICL is largely under-explored, particularly whether transformers can be trained to generalize to unseen examples in a prompt, which will require the model to acquire contextual knowledge of the prompt for generalization. This paper investigates the training dynamics of transformers by gradient descent through the lens of non-linear regression tasks. The contextual generalization here can be attained via learning the template function for each task in-context, where all template functions lie in a linear space with m basis functions. The paper analyzes the training dynamics of one-layer multi-head transformers to in-contextly predict unlabeled inputs given partially labeled prompts, where the labels contain Gaussian noise and the number of examples in each prompt are not sufficient to determine the template. Under mild assumptions, the paper shows that the training loss for a one-layer multi-head transformer converges linearly to a global minimum. Moreover, the transformer effectively learns to perform ridge regression over the basis functions. To the authors’ knowledge, this study is the first provable demonstration that transformers can learn contextual (i.e., template) information to generalize to both unseen examples and tasks when prompts contain only a small number of query-answer pairs. <br><br>

27. ***"xGen-MM (BLIP-3): Open Large Multimodal Models Framework": <br>The report introduces xGen-MM, a framework for developing large multimodal models that expands on Salesforce's xGen initiative. The models show strong in-context learning capabilities, competitive performance, and improved safety, with all resources made publicly available to advance research in multimodal models.*** <br><br>
    Aug 16, Salesforce and Uni of Washington published a [paper](https://arxiv.org/pdf/2408.08872) “xGen-MM (BLIP-3): A Family of Open Large Multimodal Models”. This report introduces xGen-MM (also known as BLIP-3), a framework for developing Large Multimodal Models (LMMs). The framework comprises meticulously curated datasets, a training recipe, model architectures, and a resulting suite of LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen initiative on foundation AI models. The models undergo rigorous evaluation across a range of tasks, including both single and multi-image benchmarks. The pre-trained base model exhibits strong in-context learning capabilities and the instruction-tuned model demonstrates competitive performance among open-source LMMs with similar model sizes. In addition, the paper introduces a safety-tuned model with DPO, aiming to mitigate harmful behaviors such as hallucinations and improve safety. The authors open source the models, curated large-scale datasets, and the fine-tuning codebase to facilitate further advancements in LMM research. Associated resources will be available on the project page. <br><br>

29. ***"Cybench: Evaluating Cybersecurity Capabilities of Language Models": <br>The paper presents Cybench, a framework for evaluating language models' cybersecurity capabilities through professional-level Capture the Flag (CTF) tasks. The evaluation reveals that current models struggle with complex tasks, but subtasks improve performance measurement, highlighting the need for further research in cybersecurity-focused language models.*** <br><br>
    Aug 15, Stanford Uni published a [paper](https://arxiv.org/pdf/2408.08926) “Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk of Language Models”. Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have the potential to cause real-world impact. Policymakers, model providers, and other researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration testing. Toward that end, the paper introduces Cybench, a framework for specifying cybersecurity tasks and evaluating agents on those tasks. The authors include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute bash commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, the paper introduces subtasks, which break down a task into intermediary steps for more gradated evaluation; adds subtasks for 17 of the 40 tasks. To evaluate agent capabilities, the authors construct a cybersecurity agent and evaluate 7 models: GPT-4o, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. Without guidance, the research finds that agents are able to solve only the easiest complete tasks that took human teams up to 11 minutes to solve, with Claude 3.5 Sonnet and GPT-4o having the highest success rates. Finally, subtasks provide more signal for measuring performance compared to unguided runs, with models achieving a 3.2\% higher success rate on complete tasks with subtask-guidance than without subtask-guidance. All code and data are publicly available at https://cybench.github.io <br><br>

31. ***"Automated Design of Agentic Systems (ADAS): A New Research Area": <br>The paper introduces ADAS, a research area focused on automatically creating powerful agentic systems by combining novel building blocks and programming agents in code. The Meta Agent Search algorithm demonstrates the potential for automatically designing high-performing agents, offering a new direction for AI research and development.*** <br><br>
    Aug 15, Uni of British Columbia, Vector Inst. And CIFAR AI published a [paper](https://arxiv.org/pdf/2408.08435) “Automated Design of Agentic Systems”. Researchers are investing substantial effort in developing powerful general-purpose agents, wherein Foundation Models are used as modules within agentic systems (e.g. Chain-of-Thought, Self-Reflection, Toolformer). However, the history of machine learning teaches us that hand-designed solutions are eventually replaced by learned solutions. The research formulates a new research area, Automated Design of Agentic Systems (ADAS), which aims to automatically create powerful agentic system designs, including inventing novel building blocks and/or combining them in new ways. The work further demonstrates that there is an unexplored yet promising approach within ADAS where agents can be defined in code and new agents can be automatically discovered by a meta agent programming ever better ones in code. Given that programming languages are Turing Complete, this approach theoretically enables the learning of any possible agentic system: including novel prompts, tool use, control flows, and combinations thereof. The paper presents a simple yet effective algorithm named Meta Agent Search to demonstrate this idea, where a meta agent iteratively programs interesting new agents based on an ever-growing archive of previous discoveries. Through extensive experiments across multiple domains including coding, science, and math, the research shows that the proposed algorithm can progressively invent agents with novel designs that greatly outperform state-of-the-art hand-designed agents. Importantly, the authors consistently observe the surprising result that agents invented by Meta Agent Search maintain superior performance even when transferred across domains and models, demonstrating their robustness and generality. Provided the authors develop it safely, the work illustrates the potential of an exciting new research direction toward automatically designing ever-more powerful agentic systems to benefit humanity.
<br><br><br>

***Aug 18 2024***

1. ***ACL 2024 Best Paper Awards: <br>On August 15, ACL 2024 announced the recipients of its best paper awards, recognizing various research contributions including topics such as SMS spam detection, language model explainability, and causal estimation. Notable awards include the Test of Time Award for "GloVe: Global Vectors for Word Representation" and the Best Social Impact Paper Award, which highlighted research on AI safety and cultural bias in large language models.***<br><br>
   Aug 15, ACL 2024 announced best paper awards. <br>***The best paper*** include: 1) [ExplainableDetector](https://arxiv.org/abs/2405.08026): Exploring Transformer-based Language Modeling Approach for SMS Spam Detection with Explainability Analysis, 2) [Deciphering Oracle Bone Language](https://arxiv.org/abs/2406.00684) with Diffusion Models, 3) [Causal Estimation of Memorisation Profiles](https://arxiv.org/abs/2406.04327), 4) [Aya Model](https://arxiv.org/abs/2402.07827): An Instruction Finetuned Open-Access Multilingual Language Model, 5) [Mission](https://arxiv.org/abs/2401.06416): Impossible Language Models, 6) [Semisupervised Neural Proto-Language Reconstructior](https://arxiv.org/abs/2406.05930), 7) [Why are Sensitive Functions Hard for Transformers](https://arxiv.org/abs/2402.09963). <br>***Test of time Award paper*** is “[GloVe](https://aclanthology.org/D14-1162.pdf): Global Vectors for Word Representation”. <br>***Best Social Impact Paper Award papers***: 1) [How Johnny Can Persuade LLMs to Jailbreak Them](https://arxiv.org/abs/2401.06373): Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs, 2) [DIALECTBENCH](https://arxiv.org/abs/2403.11009): A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages, 3) [Having Beer after Prayer](https://arxiv.org/abs/2305.14456)? Measuring Cultural Bias in Large Language Models. <br>***Theme Paper Award paper*** is [OLMo](https://arxiv.org/abs/2402.00838): Accelerating the Science of Language Models. <br>***The new Open science, open data, and open models for reproducible NLP research award papers*** are 1) [AppWorld](https://arxiv.org/abs/2407.18901): A Controllable World of Apps and People for Benchmarking Interactive Coding Agents, 2) [Latxa](https://arxiv.org/abs/2403.20266): An Open Language Model and Evaluation Suite for Basque, 3) [Dolma](https://arxiv.org/abs/2402.00159): an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. <br><br>

3. ***Google's Visual Memory Innovation: <br>Google published a paper "Towards flexible perception with visual memory," proposing a novel approach to neural network training. Instead of the traditional monolithic model, this method combines the power of deep learning with a flexible database structure. It allows for scalable data integration, data removal, and interpretable decision-making, challenging the idea that neural networks must be static and unchangeable once trained.***<br>

   Aug 15, Google published a [paper](https://arxiv.org/pdf/2408.08172) “Towards flexible perception with visual memory”.  Training a neural network is a monolithic endeavor, akin to carving knowledge into stone: once the process is completed, editing the knowledge in a network is nearly impossible, since all information is distributed across the network's weights. The paper explores a simple, compelling alternative by marrying the representational power of deep neural networks with the flexibility of a database. Decomposing the task of image classification into image similarity (from a pre-trained embedding) and search (via fast nearest neighbor retrieval from a knowledge database), the work builds a simple and flexible visual memory that has the following key capabilities: (1.) The ability to flexibly add data across scales: from individual samples all the way to entire classes and billion-scale data; (2.) The ability to remove data through unlearning and memory pruning; (3.) An interpretable decision-mechanism on which the authors can intervene to control its behavior. Taken together, these capabilities comprehensively demonstrate the benefits of an explicit visual memory. The authors hope that it might contribute to a conversation on how knowledge should be represented in deep vision models -- beyond carving it in “stone” weights.<br><br>

5. ***Study on Language Models and Hallucinations: <br>Google's paper explores how training language models (LMs) on knowledge graphs affects their hallucination tendencies. The research reveals that larger, longer-trained LMs hallucinate less, but controlling hallucinations at scale is costly. Interestingly, as models grow, hallucinations become harder to detect, indicating a complex relationship between model size and performance.***<br>

   Aug 14, Google published a [paper](https://arxiv.org/pdf/2408.07852v1) “Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability”. While many capabilities of language models (LMs) improve with increased training budget, the influence of scale on hallucinations is not yet fully understood. Hallucinations come in many forms, and there is no universally accepted definition. The study thus focuses on studying only those hallucinations where a correct answer appears verbatim in the training set. To fully control the training data content, the authors construct a knowledge graph (KG)-based dataset, and use it to train a set of increasingly large LMs. The study finds that for a fixed dataset, larger and longer-trained LMs hallucinate less. However, hallucinating on <= 5% of the training data requires an order of magnitude larger model, and thus an order of magnitude more compute, than Hoffmann et al. (2022) reported was optimal. Given this costliness, the authors study how hallucination detectors depend on scale. While the study sees detector size improves performance on fixed LM's outputs, it is found an inverse relationship between the scale of the LM and the detectability of its hallucinations.<br><br>

7. ***MIT's AI Risk Repository: <br>MIT's AI Risk Repository is a comprehensive database and taxonomy of 777 AI risks, aimed at unifying the understanding and management of AI-related dangers. It classifies risks by causal factors and domains, providing a detailed framework to help policymakers, researchers, and developers navigate the complex landscape of AI safety.***<br>

   Aug 14, MIT release an [AI Risk Repository](https://airisk.mit.edu/) with a [paper](https://cdn.prod.website-files.com/669550d38372f33552d2516e/66bc918b580467717e194940_The%20AI%20Risk%20Repository_13_8_2024.pdf) “The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence”. The risks posed by Artificial Intelligence (AI) are of considerable concern to academics, auditors, policymakers, AI companies, and the public. However, a lack of shared understanding of AI risks can impede our ability to comprehensively discuss, research, and react to them. This paper addresses this gap by creating an AI Risk Repository to serve as a common frame of reference. This comprises a living database of 777 risks extracted from 43 taxonomies, which can be filtered based on two overarching taxonomies and easily accessed, modified, and updated via the authors’ [website](https://airisk.mit.edu/) and [online spreadsheets](https://docs.google.com/spreadsheets/d/1evwjF4XmpykycpeZFq0FUteEAt7awx2i2oE6kMrV_xE/copy). The authors construct a Repository with a systematic review of taxonomies and other structured classifications of AI risk followed by an expert consultation. The study develops the taxonomies of AI risk using a best-fit framework synthesis. The high-level Causal Taxonomy of AI Risks classifies each risk by its causal factors (1) Entity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3) Timing: Pre-deployment; Post-deployment. The mid-level Domain Taxonomy of AI Risks classifies risks into seven AI risk domains: (1) Discrimination & toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors & misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and (7) AI system safety, failures, & limitations. These are further divided into 23 subdomains. The AI Risk Repository is, to authors’ knowledge, the first attempt to rigorously curate, analyze, and extract AI risk frameworks into a publicly accessible, comprehensive, extensible, and categorized risk database. This creates a foundation for a more coordinated, coherent, and complete approach to defining, auditing, and managing the risks posed by AI systems.<br><br>

9. ***Advancements in Text-to-SQL Pipelines: <br>A paper by Distyl AI and Polytechnique Montreal questioned the necessity of schema linking in Text-to-SQL pipelines. The research found that modern language models can bypass this step, directly identifying relevant schema elements, which improves accuracy and simplifies the pipeline.***<br>

    Aug 14, Distyl AI and Polytechnique Montreal published a [paper](https://www.arxiv.org/pdf/2408.07702) “The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models”. Schema linking is a crucial step in Text-to-SQL pipelines, which translate natural language queries into SQL. The goal of schema linking is to retrieve relevant tables and columns (signal) while disregarding irrelevant ones (noise). However, imperfect schema linking can often exclude essential columns needed for accurate query generation. This work revisits the need for schema linking when using the latest generation of large language models (LLMs). The work finds empirically that newer models are adept at identifying relevant schema elements during generation, without the need for explicit schema linking. This allows Text-to-SQL pipelines to bypass schema linking entirely and instead pass the full database schema to the LLM, eliminating the risk of excluding necessary information. Furthermore, as alternatives to schema linking, the authors propose techniques that improve Text-to-SQL accuracy without compromising on essential schema information. This approach achieves 71.83\% execution accuracy on the BIRD benchmark, ranking first at the time of submission.<br><br>

11. ***Agent Q: Autonomous AI Advances: <br>A paper by MultiOn and Stanford introduces "Agent Q," a framework that significantly enhances the reasoning capabilities of large language models in dynamic environments. This method integrates guided Monte Carlo Tree Search with a self-critique mechanism, enabling AI agents to improve their performance in tasks like web navigation and real-world booking scenarios.***<br>

    Aug 13, MultiOn and Stanford Uni published a [paper](https://arxiv.org/pdf/2408.07199) “Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents”. Large Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex reasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a difficult challenge. Traditional supervised pre-training on static datasets falls short in enabling autonomous agent capabilities needed to perform complex decision-making in dynamic settings like web navigation. Previous attempts to bridge this gap through supervised fine-tuning on curated expert demonstrations often suffer from compounding errors and limited exploration data, resulting in sub-optimal policy outcomes. To overcome these challenges, the work proposes a framework that combines guided Monte Carlo Tree Search (MCTS) search with a self-critique mechanism and iterative fine-tuning on agent interactions using an off-policy variant of the Direct Preference Optimization (DPO) algorithm. The method allows LLM agents to learn effectively from both successful and unsuccessful trajectories, thereby improving their generalization in complex, multi-step reasoning tasks. The authors validate the approach in the WebShop environment, a simulated e-commerce platform—where it consistently outperforms behavior cloning and reinforced fine-tuning baseline, and beats average human performance when equipped with the capability to do online search. In real-world booking scenarios, the methodology boosts Llama-3 70B model’s zero-shot performance from 18.6% to 81.7% success rate (a 340% relative increase) after a single day of data collection and further to 95.4% with online search. The authors believe this represents a substantial leap forward in the capabilities of autonomous agents, paving the way for more sophisticated and reliable decision-making in real-world settings.<br><br>

13. ***xAI's Grok-2 Model Preview: <br>xAI released an early preview of Grok-2, a new AI model with enhanced capabilities in chat, coding, and reasoning. Grok-2 outperforms several leading models on various benchmarks and excels in tasks like visual math reasoning and document-based question answering.***<br>

    Aug 13, [xAI released Grok-2](https://x.ai/blog/grok-2), an early preview of Grok-2, a significant step forward from its previous model Grok-1.5, featuring frontier capabilities in chat, coding, and reasoning. At the same time, xAI is introducing Grok-2 mini, a small but capable sibling of Grok-2. An early version of Grok-2 has been tested on the LMSYS leaderboard under the name "sus-column-r." At the time of this blog post, it is outperforming both Claude 3.5 Sonnet and GPT-4-Turbo. Internally, xAI employs a comparable process to evaluate the models. xAI’s AI Tutors engage with the models across a variety of tasks that reflect real-world interactions with Grok. During each interaction, the AI Tutors are presented with two responses generated by Grok. They select the superior response based on specific criteria outlined in the guidelines. xAI focused on evaluating model capabilities in two key areas: following instructions and providing accurate, factual information. Grok-2 has shown significant improvements in reasoning with retrieved content and in its tool use capabilities, such as correctly identifying missing information, reasoning through sequences of events, and discarding irrelevant posts. Grok-2 achieves performance levels competitive to other frontier models in areas such as graduate-level science knowledge (GPQA), general knowledge (MMLU, MMLU-Pro), and math competition problems (MATH). Additionally, Grok-2 excels in vision-based tasks, delivering state-of-the-art performance in visual math reasoning (MathVista) and in document-based question answering (DocVQA).<br><br>

15. ***Google's Imagen 3 Release: <br>Google introduced Imagen 3, a latent diffusion model that generates high-quality images from text prompts. While the model is preferred over other state-of-the-art models in terms of quality, the release includes discussions on safety and responsibility, although detailed information and model weights were not disclosed.***<br>

    Aug 13, Google published a [paper](https://arxiv.org/pdf/2408.07009) “Imagen 3” to introduce Imagen 3, a latent diffusion model that generates high quality images from text prompts. The paper describes the quality and responsibility evaluations. Imagen 3 is preferred over other state-of-the-art (SOTA) models at the time of evaluation. In addition, the paper discusses issues around safety and representation, as well as methods we used to minimize the potential harm of our models. But it looks like no much details are released in the paper, and model weights are not released neither.<br><br>

17. ***Evolution of AI Agents: <br>An article by venturebeat discusses the shift from AI assistants to proactive, autonomous agents. These agents are capable of making independent decisions and handling complex tasks, signaling a significant evolution in AI. While promising, the article notes that the infrastructure to support these agents is still under development, and organizations must address challenges like data quality and trust.***<br>

    Aug 13, [venturebeat.com](https://venturebeat.com/ai/beyond-assistants-ai-agents-are-transforming-the-paradigm/) published an article “Beyond assistants: AI agents are transforming the paradigm”. The article discusses the evolution of AI from reactive assistants to proactive, autonomous agents. By 2028, a significant portion of human interactions with AI is expected to shift from prompting large language models (LLMs) to interfacing with intent-driven agents. Unlike AI assistants that respond to user requests, these agents can make decisions and act independently, handling complex tasks in real time. This shift is seen as the next step in generative AI, with major tech companies like Google, Microsoft, and AWS already developing such agents. AI agents are compared to specialized employees who collaborate to solve business problems. They are already showing success in areas like customer service and marketing, and their use is expected to expand across various industries. However, the infrastructure needed to support these agents, such as a system for seamless communication and coordination between them, is still in development. Organizations looking to adopt AI agents must first overcome challenges associated with generative AI, such as data quality and managing trust and security issues. Starting with simple, well-defined use cases and focusing on data hygiene are recommended steps. As LLMs improve and more industries adopt AI agents, the benefits of this technology are expected to become more widespread, positioning organizations to fully leverage the potential of agentic AI.<br><br>

19. ***AI Scientist Framework for Autonomous Research: <br>A paper by Sakana AI and partners introduces "The AI Scientist," a framework for fully automated scientific discovery. The AI can generate research ideas, conduct experiments, and write scientific papers, potentially transforming the research process by reducing costs and increasing creativity.***<br>

    Aug 12, Sakana AI, FLAIR, Uni of Oxford, Uni of British Columbia et al. published a [paper](https://arxiv.org/pdf/2408.06292) “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery”. One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aids to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. The paper introduces The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. The work demonstrates its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than $15 per paper. To evaluate the generated papers, the authors design and validate an automated reviewer, which the authors show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by the automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking people closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. The code is open-sourced at https://github.com/SakanaAI/AI-Scientist<br><br>

21. ***Falcon Mamba 7B: A Revolutionary SSLM: <br>TII released Falcon Mamba 7B, the first open-source State Space Language Model (SSLM), noted for its low memory cost and superior performance over traditional models like Meta's Llama 3.1. This model reflects Abu Dhabi's innovation in AI research and is expected to be included in future Hugging Face releases.***<br>

    Aug 12, TII released first SSLM with [Falcon Mamba 7B](https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html) model. Falcon Mamba 7B is the first open source released State Space Language Model (SSLM), a new revolutionary architecture for Falcon models. Falcon Mamba 7B is the no. 1 globally performing open source SSLM in the world, as independently verified by Hugging Face. SSLMs have a low memory cost and don’t require additional memory to generate arbitrary long blocks of text. Falcon Mamba 7B also outperforms traditional transformer architecture models such as Meta’s Llama 3.1 8B and Mistral’s 7B. New model reflects the innovation and pioneering approach of Abu Dhabi in AI research and development. Falcon Mamba was trained with ~ 5500GT of data, mainly composed of RefinedWeb data with addition of high-quality technical data and code data from public sources. TII used constant learning rate for the most of the training, followed by a relatively short learning rate decay stage. In this last stage, TII also added a small portion of high-quality curated data to further enhance model performance. The Falcon Mamba architecture will be available in the next release of the Hugging Face transformers library (>4.45.0).<br><br>

23. ***Mutual Reasoning in Small Language Models: <br>A paper by Microsoft and Harvard introduces rStar, a method that enhances reasoning in small language models through mutual reasoning and self-play. This approach significantly improves problem-solving accuracy across various benchmarks, making smaller models more effective without requiring extensive fine-tuning.***<br>

    Aug 12, Microsoft and Harvard Uni published a [paper](https://arxiv.org/pdf/2408.06195) “Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers”. This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will be available at https://github.com/zhentingqi/rStar.<br><br>

25. ***Debunking Emergent Abilities in LLMs: <br>A paper challenges the notion that large language models develop complex intelligent behaviors, attributing their performance to in-context learning rather than true emergent abilities. The study emphasizes that despite improvements, LLMs still require explicit instructions to perform tasks effectively.***<br>

    Aug 12, [techxplore.com](https://techxplore.com/news/2024-08-emergent-abilities-large-language-context.html) published an article introduced an ACL 2024 [paper](https://arxiv.org/pdf/2309.01809) “Are Emergent Abilities in Large Language Models just In-Context Learning?”.  According to the study, there is no evidence that what are known as large language models (LLMs) are beginning to develop a general "intelligent" behavior that would enable them to proceed in a planned or intuitive manner or to think in a complex way. The research focuses on unforeseen and sudden leaps in the performance of language models, which are referred to as "emergent abilities." After the models were introduced, scientists found that they became more powerful with increasing size and the growing amount of data with which they were trained (scaling). On the one hand, this raised hopes that further scaling would make the models even better. On the other hand, there was also concern that these abilities could become dangerous, as the LLMs could become independent and possibly escape human control. In response, AI laws were introduced worldwide, including in the European Union and the U.S.. However, the authors of the current study have now come to the conclusion that there is no evidence for the presumed development of differentiated thinking in the models. Instead, the LLMs acquired the superficial skill of following relatively simple instructions, as the researchers showed. The systems are still a long way from what humans are capable of. Users should explicitly state what the systems should do and, if possible, give examples. The important thing is: The tendency of these models to produce plausible-sounding but false results—known as confabulation—is likely to persist, even if the quality of the models has improved dramatically in recent times.<br><br>

27. ***AI's Impact on IT Jobs: <br>According to a report, 92% of IT jobs will undergo transformation due to AI, with mid- and low-level positions being most affected. The report stresses the growing importance of skills like AI literacy and rapid engineering, as traditional roles like data management and basic programming become less relevant.***<br>

    Aug 12, according to [cio.com](https://www.cio.com/article/3485322/92-of-it-jobs-will-be-transformed-by-ai.html), 92% of IT jobs will be transformed by AI, and the biggest change will be experienced by mid- and low-level positions, as manual tasks become less relevant or easily replaceable by this technology. To get a better idea how AI will change the labor market for technology professionals, the recently formed AI-Enabled ICT Workforce Consortium has published its inaugural report, “The Transformational Opportunity of AI on ICT Jobs,” which reveals that 92% of IT jobs will see a high or moderate transformation due to advances in AI. The study argues that the biggest changes will be seen in mid-level (40%) and entry-level (37%) technology jobs, as certain skills and capabilities become more or less relevant. AI ethics, responsible AI, rapid engineering, AI literacy, and large language model (LLM) architecture are expected to rise in importance in this new era, while traditional data management, content creation, documentation maintenance, basic programming and languages, and research information will become less relevant. That’s why, the report says, critical skills are needed in all IT jobs, including AI literacy, data analytics, and rapid engineering. That’s why the consortium is seeking to empower workers to reskill and upskill.<br><br>

29. ***Meta's UniBench for Vision-Language Models: <br>Meta introduced UniBench, a unified benchmark for evaluating vision-language models across 50+ tasks. The study reveals that while scaling models improves some capabilities, it falls short in areas like reasoning and counting, suggesting that more targeted interventions are needed for VLM progress.***<br>

    Aug 9, Meta published a [paper](https://arxiv.org/pdf/2408.04810) “UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling”. Significant research efforts have been made to scale and improve vision-language model (VLM) training approaches. Yet, with an ever-growing number of benchmarks, researchers are tasked with the heavy burden of implementing each protocol, bearing a non-trivial computational cost, and making sense of how all these benchmarks translate into meaningful axes of progress. To facilitate a systematic evaluation of VLM progress, the authors introduce UniBench: a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more. The authors showcase the utility of UniBench for measuring progress by evaluating nearly 60 publicly available vision-language models, trained on scales of up to 12.8B samples. While scaling training data or model size can boost many vision-language model capabilities, scaling offers little benefit for reasoning or relations. Surprisingly, the authors also discover today's best VLMs struggle on simple digit recognition and counting tasks, e.g. MNIST, which much simpler networks can solve. Where scale falls short, the work finds that more precise interventions, such as data quality or tailored-learning objectives offer more promise. For practitioners, the authors also offer guidance on selecting a suitable VLM for a given application. Finally, the authors release an easy-to-run UniBench [code-base](https://github.com/facebookresearch/unibench) with the full set of 50+ benchmarks and comparisons across 59 models as well as a distilled, representative set of benchmarks that runs in 5 minutes on a single GPU.<br><br>

31. ***Elon Musk vs. OpenAI Lawsuit: <br>Elon Musk filed a new lawsuit against OpenAI and its CEO, Sam Altman, accusing them of betraying the company's original non-profit mission. The lawsuit alleges manipulation and violation of agreements, highlighting the deteriorating relationship between Musk and Altman since the founding of OpenAI in 2015.***<br>

    Aug 6, according to [theguardian.com](https://www.theguardian.com/technology/article/2024/aug/05/elon-musk-openai-lawsuit?utm_source=substack&utm_medium=email), Elon Musk has filed a new lawsuit against OpenAI and its CEO, Sam Altman, alleging they manipulated him into co-founding the company and betrayed its original non-profit mission by turning it into a for-profit entity. This lawsuit follows a similar one Musk filed earlier this year but later withdrew. Musk's complaint accuses Altman and other co-founders of deceiving him and violating the "founding agreement" meant to prioritize the betterment of humanity. OpenAI denies the allegations, arguing that Musk supported the shift to a for-profit model and is motivated by jealousy. The new lawsuit also includes accusations of federal racketeering and wire fraud against Altman and his associates. The legal battle underscores the deteriorating relationship between Musk and Altman, who co-founded OpenAI in 2015 but later parted ways due to an internal power struggle. Musk has since founded his own AI company, xAI, which has struggled to match the success of OpenAI’s ChatGPT.<br><br>

33. ***The Consistent Reasoning Paradox (CRP): <br>Researchers introduced the Consistent Reasoning Paradox (CRP), which suggests that consistent reasoning, a core component of human intelligence, implies fallibility. The paradox asserts that an AI striving to mimic human intelligence through consistent reasoning will inevitably produce incorrect but plausible answers, especially in basic arithmetic. This paradox reveals that a trustworthy AI, which reasons consistently and never answers incorrectly, must be capable of acknowledging uncertainty by saying "I don't know.", a function, which current AI lacks.***<br>

    Aug 5, King’s College London, Uni of Cambridge and Simon Fraser Uni published a [paper](https://arxiv.org/pdf/2408.02357) “On the consistent reasoning paradox of intelligence and optimal trust in AI: The power of 'I don't know'”. The paper introduces the Consistent Reasoning Paradox (CRP). Consistent reasoning, which lies at the core of human intelligence, is the ability to handle tasks that are equivalent, yet described by different sentences ('Tell me the time!' and 'What is the time?'). The CRP asserts that consistent reasoning implies fallibility -- in particular, human-like intelligence in AI necessarily comes with human-like fallibility. Specifically, it states that there are problems, e.g. in basic arithmetic, where any AI that always answers and strives to mimic human intelligence by reasoning consistently will hallucinate (produce wrong, yet plausible answers) infinitely often. The paradox is that there exists a non-consistently reasoning AI (which therefore cannot be on the level of human intelligence) that will be correct on the same set of problems. The CRP also shows that detecting these hallucinations, even in a probabilistic sense, is strictly harder than solving the original problems, and that there are problems that an AI may answer correctly, but it cannot provide a correct logical explanation for how it arrived at the answer. Therefore, the CRP implies that any trustworthy AI (i.e., an AI that never answers incorrectly) that also reasons consistently must be able to say 'I don't know'. Moreover, this can only be done by implicitly computing a new concept that the paper introduces, termed the 'I don't know' function -- something currently lacking in modern AI. In view of these insights, the CRP also provides a glimpse into the behaviour of Artificial General Intelligence (AGI). An AGI cannot be 'almost sure', nor can it always explain itself, and therefore to be trustworthy it must be able to say 'I don't know'.<br><br>

35. ***McKinsey's "Technology Trends Outlook 2024": <br>McKinsey's The report identifies five key tech trends: the AI revolution, digital future building, compute and connectivity frontiers, cutting-edge engineering, and sustainability. Generative AI (Gen AI) is highlighted as a rapidly advancing area, with significant organizational adoption and the potential to generate up to $4.4 trillion in annual value. The report underscores the importance of addressing risks such as bias, misinformation, and deepfakes as organizations invest in the capabilities required to scale Gen AI, leading to a heightened demand for talent in data science, software engineering, and data engineering.***<br>

    Jul 30, [McKinsey published](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-top-trends-in-tech#/) “Technology Trends Outlook 2024”. The five tech trends categories are: the AI revolution, building the digital future, compute and connectivity frontiers, cutting-edge engineering, and a sustainable world. With regarding to GenAI, the report indicates that Generative AI (gen AI) has been making significant strides, pushing the boundaries of machine capabilities. Gen AI has sparked widespread interest, with individuals and organizations across different regions and industries exploring its potential. According to the latest McKinsey Global Survey on the state of AI, 65 percent of respondents say their organizations are regularly using gen AI in at least one business function, up from one-third last year, and gen AI use cases have the potential to generate an annual value of $2.6 trillion to $4.4 trillion. However, it’s important to recognize the risks that accompany the use of this powerful technology, including bias, misinformation, and deepfakes. Progressing through 2024 and beyond, McKinsey anticipates organizations investing in the risk mitigation, operating model, talent, and technological capabilities required to scale gen AI. Organizations are now focusing on scaling and expanding their internal capabilities to harness the potential of gen AI, leading to a sharp increase in demand for data scientists, software engineers, and data engineers.<br><br>

37. ***Impact of Output Length on LLM Reasoning: <br>The study highlights the trade-off between generating detailed reasoning in outputs and the time required. The researcher proposes Constrained-CoT (CCoT), a refined prompt engineering strategy that limits output length while maintaining accuracy. Experiments showed that applying CCoT to LLaMA2-70b improved accuracy and reduced output length, demonstrating the effectiveness of concise reasoning in LLMs.***<br>

    Jul 29, researcher from Italy published a [paper](https://arxiv.org/pdf/2407.19825) “Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost”. Today's large language models (LLMs) can solve challenging question-answering tasks, and prompt engineering techniques, such as chain-of-thought (CoT), have gained attention for enhancing the explanation and correctness of outputs. Nevertheless, models require significant time to generate answers augmented with lengthy reasoning details. To address this issue, this paper analyzes the impact of output lengths on LLM inference pipelines and proposes novel metrics to evaluate them in terms of correct conciseness. It also examines the impact of controlling output length through a refined prompt engineering strategy, Constrained-CoT (CCoT), which encourages the model to limit output length. Experiments on pre-trained LLMs demonstrated the benefit of the proposed metrics and the effectiveness of CCoT across different models. For instance, constraining the reasoning of LLaMA2-70b to 100 words improves the accuracy from 36.01% (CoT) to 41.07% (CCoT) on the GSM8K dataset, while reducing the average output length by 28 words.<br><br>

37. ***The Winds of AI Winter: The AI industry is facing significant challenges, with doubts about the ongoing AI Summer due to declining GPT-4 usage, canceled projects, and financial concerns. Key issues include user dissatisfaction, economic viability, and the need for AI engineers to bridge the gap between technological advances and practical benefits to avoid a potential "AI Winter."***<br>

    Jul 22, latent space published an [article](https://www.latent.space/p/mar-jun-2024) "The Winds of AI Winter". The AI industry is experiencing significant turbulence as doubts emerge about the ongoing AI Summer. Key issues include: 1) Decline in GPT-4 Use: Many users have switched from OpenAI’s GPT-4 to alternatives like Claude due to perceived superior performance. This trend, coupled with OpenAI's rumored financial losses, reflects growing dissatisfaction. 2)Failures and Cancellations: Several high-profile AI projects and products, such as Google’s AI overviews and McDonald’s drive-thru AI, have been announced and then abruptly canceled. 3) Financial and Industry Concerns: Goldman Sachs and Sequoia Capital highlight potential economic issues, with Goldman Sachs questioning the viability of AI investments given their high costs, while Sequoia critiques the AI industry's revenue shortfall compared to infrastructure costs. 4) Investment vs. Returns: Predictions vary widely on AI’s economic impact, with some expecting massive returns on investments and others warning of diminishing returns and substantial costs with unclear benefits. 5) Engineering Focus: The article emphasizes the need for AI engineers to address the imbalance between technological advancements and practical, widespread benefits to avoid a potential downturn in the industry. Overall, the industry faces a complex situation with both significant technological strides and serious financial and practical challenges. The key takeaway is the urgent need for effective engineering to bridge the gap between capability and real-world application to prevent a potential "AI Winter."
    <br><br>

    



***Aug 11 2024***

1. ***Meta's Self-Taught Evaluators: <br>
   Meta introduced a novel approach to model evaluation called "Self-Taught Evaluators," which uses synthetic training data to improve models without human annotations. This method iteratively generates and judges model outputs, significantly enhancing a large language model (LLM) without relying on costly and quickly outdated human preference data. The results show that the Self-Taught Evaluator can outperform commonly used judges and match the performance of top models trained with labeled data.*** <br><br>
   Aug 10, Meta published a [paper](https://arxiv.org/pdf/2408.02666) “Self-Taught Evaluators”. Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. This paper presents an approach that aims to improve evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, the iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, the Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples. <br><br>

3. ***Generative AI Business Use Cases:  <br>An article from cio.com highlights four transformative business applications of generative AI: virtual assistants, intelligent search, content summarization, and document processing. These AI-powered tools enhance productivity, streamline data management, and improve customer interactions, giving businesses a competitive edge by reducing costs and optimizing operations.*** <br><br>
   Aug 9, cio.com published [an article](https://www.cio.com/article/3478850/four-generative-ai-use-cases-for-businesses.html) “Four generative AI use cases for businesses”. The article outlines four key use cases where generative AI is transforming business operations: 1) Virtual Assistants: Generative AI powers tools like chatbots and virtual assistants, enhancing both employee productivity and customer experiences by automating tasks and providing personalized interactions. 2) Intelligent Search: Leveraging large language models (LLMs), generative AI enables enterprises to process and search proprietary data more effectively, offering precise and relevant information tailored to business-specific needs. 3) Content Summarization: AI models can quickly summarize documents, meetings, and videos, saving time and improving decision-making in sectors like healthcare and finance. 4) Document Processing: Generative AI streamlines document management by automating tasks such as translation, proofreading, and data extraction, particularly benefiting industries that handle large volumes of documents, like legal and financial sectors. Overall, generative AI boosts productivity, reduces costs, and provides businesses with a competitive edge by enabling more efficient data processing and customer interaction. <br><br>

5. ***OpenAI Leadership Changes:  <br>Futurism reported significant leadership changes at OpenAI, with key figures like co-founder John Schulman and VP Peter Deng leaving the company. This has sparked speculation about OpenAI's ability to achieve its goal of safe artificial general intelligence (AGI). Critics suggest that these departures may indicate challenges in fulfilling its vision, with some even predicting a potential "Generative AI bubble" burst.*** <br><br>
   Aug 8, Futurism published [an article](https://futurism.com/openai-prominent-employees-leaving) “Why Are OpenAI's Most Prominent Employees Leaving?”. Cofounder John Schulman announced he'd left the company this week to join rival AI company Anthropic, while president Greg Brockman is taking a leave of absence. VP of consumer product Peter Deng has also quit, indicating major shifts in OpenAI's upper ranks. The Sam Altman-led company has made its core mission to realize safe artificial general intelligence, the still entirely hypothetical point at which point AI can keep up with humans across a wide variety of intellectual tasks. But how far the company is from doing just that remains a heavily debated subject, with critics pointing out that OpenAI is shoring up billions of dollars in investment by making empty promises — an "AI bubble" that may be set to burst. Could the latest departures show that the venture is struggling to fulfill its long-term vision, let alone turn a profit from generative AI? Critics say OpenAI's brain drain problem could be the canary in the coal mine. "Calling it," leading AI skeptic Gary Marcus tweeted. "August 2024 will be known as the month in which the Generative AI bubble burst." Other critics questioned OpenAI's repeated claims that AGI is right around the corner. "If OpenAI is right on the verge of AGI, why do prominent people keep leaving?" AI developer Benjamin de Kraker tweeted. <br><br>
 
7. ***LLM-DetectAIve for Machine-Generated Text Detection:  <br>A new tool called "LLM-DetectAIve" was introduced for detecting machine-generated texts (MGTs). Unlike previous binary classifiers, it categorizes texts into four distinct types, offering fine-grained detection. This tool is particularly valuable in academic and educational settings, where identifying the degree of LLM involvement in text creation is crucial.*** <br><br>
   Aug 8, MBZUAI, Uni of Florida, NYU, et al. published a [paper](https://arxiv.org/pdf/2408.04284) “LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection”. The widespread accessibility of large language models (LLMs) to the general public has significantly amplified the dissemination of machine-generated texts (MGTs). Advancements in prompt manipulation have exacerbated the difficulty in discerning the origin of a text (human-authored vs machinegenerated). This raises concerns regarding the potential misuse of MGTs, particularly within educational and academic domains. This paper presents LLM-DetectAIve -- a system designed for fine-grained MGT detection. It is able to classify texts into four categories: human-written, machine-generated, machine-written machine-humanized, and human-written machine-polished. Contrary to previous MGT detectors that perform binary classification, introducing two additional categories in LLM-DetectiAIve offers insights into the varying degrees of LLM intervention during the text creation. This might be useful in some domains like education, where any LLM intervention is usually prohibited. Experiments show that LLM-DetectAIve can effectively identify the authorship of textual content, proving its usefulness in enhancing integrity in education, academia, and other domains. LLM-DetectAIve is publicly accessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video describing our system is available at https://youtu.be/E8eT_bE7k8c. <br><br>

9. ***Optical Neural Networks and FFM Learning:  <br>A paper in Nature presents a new method called fully forward mode (FFM) learning for training optical neural networks. FFM learning allows most machine learning operations to be conducted efficiently on physical systems rather than digital simulations, leading to faster learning processes and significant advancements in deep learning, ultrasensitive perception, and topological photonics.*** <br><br>
    Aug 7, Nature published a [paper](https://www.nature.com/articles/s41586-024-07687-4.pdf) “Fully forward mode training for optical neural networks”. Optical computing promises to improve the speed and energy efficiency of machine learning applications. However, current approaches to efficiently train these models are limited by in silico emulation on digital computers. This research develops a method called fully forward mode (FFM) learning, which implements the compute-intensive training process on the physical system. The majority of the machine learning operations are thus efficiently conducted in parallel on site, alleviating numerical modelling constraints. In free-space and integrated photonics, the researchers experimentally demonstrate optical systems with state-of-the-art performances for a given network size. FFM learning shows training the deepest optical neural networks with millions of parameters achieves accuracy equivalent to the ideal model. It supports all-optical focusing through scattering media with a resolution of the diffraction limit; it can also image in parallel the objects hidden outside the direct line of sight at over a kilohertz frame rate and can conduct all-optical processing with light intensity as weak as subphoton per pixel (5.40 × 1018- operations-per-second-per-watt energy efficiency) at room temperature. Furthermore, the study proves that FFM learning can automatically search non-Hermitian exceptional points without an analytical model. FFM learning not only facilitates orders-of-magnitude-faster learning processes, but can also advance applied and theoretical fields such as deep neural networks, ultrasensitive perception and topological photonics. <br><br>

11. ***Human-Level Robot Table Tennis:  <br>Google researchers developed a robot that achieved amateur human-level performance in competitive table tennis. The robot uses a hierarchical policy architecture and zero-shot sim-to-real techniques to adapt to new opponents in real time. It performed well against beginners and intermediate players, showcasing significant progress toward human-level robotic performance in physical tasks.*** <br><br>
    Aug 7, Google published a [paper](https://arxiv.org/pdf/2408.03906) “Achieving Human Level Competitive Robot Table Tennis”. Achieving human-level speed and performance on real world tasks is a north star for the robotics research community. This work takes a step towards that goal and presents the first learned robot agent that reaches amateur human-level performance in competitive table tennis. Table tennis is a physically demanding sport which requires human players to undergo years of training to achieve an advanced level of proficiency. The paper contributes (1) a hierarchical and modular policy architecture consisting of (i) low level controllers with their detailed skill descriptors which model the agent's capabilities and help to bridge the sim-to-real gap and (ii) a high level controller that chooses the low level skills, (2) techniques for enabling zero-shot sim-to-real including an iterative approach to defining the task distribution that is grounded in the real-world and defines an automatic curriculum, and (3) real time adaptation to unseen opponents. Policy performance was assessed through 29 robot vs. human matches of which the robot won 45% (13/29). All humans were unseen players and their skill level varied from beginner to tournament level. Whilst the robot lost all matches vs. the most advanced players it won 100% matches vs. beginners and 55% matches vs. intermediate players, demonstrating solidly amateur human-level performance. Videos of the matches can be viewed at https://sites.google.com/view/competitive-robot-table-tennis <br><br>

13. ***WalledEval Safety Evaluation Toolkit:  <br>Walled AI Lab introduced "WalledEval," a comprehensive toolkit for evaluating the safety of large language models (LLMs). It includes over 35 safety benchmarks and features like WalledGuard for content moderation and SGXSTest for exaggerated safety in cultural contexts. The toolkit is designed to test and improve LLM safety across various models and scenarios.*** <br><br>
    Aug 7, Walled AI Lab published a [paper](https://arxiv.org/pdf/2408.03837) “WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models”. WalledEval is a comprehensive AI safety testing toolkit designed to evaluate large language models (LLMs). It accommodates a diverse range of models, including both open-weight and API-based ones, and features over 35 safety benchmarks covering areas such as multilingual safety, exaggerated safety, and prompt injections. The framework supports both LLM and judge benchmarking, and incorporates custom mutators to test safety against various text-style mutations such as future tense and paraphrasing. Additionally, WalledEval introduces WalledGuard, a new, small and performant content moderation tool, and SGXSTest, a benchmark for assessing exaggerated safety in cultural contexts. We make WalledEval publicly available at https://github.com/walledai/walledevalA. <br><br>

15. ***CoverBench for Complex Claim Verification:  <br>Google and Tel Aviv University released "CoverBench," a benchmark for verifying the correctness of language models' outputs in complex reasoning tasks. It provides a diversified evaluation for complex claim verification across various domains, ensuring high data quality and challenging baseline results. The benchmark aims to advance the accuracy and reliability of language models in handling complex queries.*** <br><br>
    Aug 6, Google and Tel Aviv Uni published a [paper](https://arxiv.org/pdf/2408.03325) “CoverBench: A Challenging Benchmark for Complex Claim Verification”. There is a growing line of research on verifying the correctness of language models' outputs. At the same time, LMs are being used to tackle complex queries that require reasoning. The paper introduces CoverBench, a challenging benchmark focused on verifying LM outputs in complex reasoning settings. Datasets that can be used for this purpose are often designed for other complex reasoning tasks (e.g., QA) targeting specific use-cases (e.g., financial tables), requiring transformations, negative sampling and selection of hard examples to collect such a benchmark. CoverBench provides a diversified evaluation for complex claim verification in a variety of domains, types of reasoning, relatively long inputs, and a variety of standardizations, such as multiple representations for tables where available, and a consistent schema. The authors manually vet the data for quality to ensure low levels of label noise. Finally, the paper reports a variety of competitive baseline results to show CoverBench is challenging and has very significant headroom. The data is available at https://huggingface.co/datasets/google/coverbench. <br><br>

17. ***Scaling LLM Test-Time Compute:  <br>A joint study by UC Berkeley and Google explored the effectiveness of scaling test-time computation for large language models (LLMs). The research suggests that optimizing test-time compute allocation can significantly enhance LLM performance, potentially outperforming models with more parameters. This approach may lead to more efficient and self-improving AI agents.*** <br><br>
    Aug 6, UC Berkeley and Google published a [paper](https://arxiv.org/pdf/2408.03314) “Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters”. Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. The paper studies the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. This study analyzes two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. The authors find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a "compute-optimal" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy will improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, it is found that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model. <br><br>

19. ***Google's Antitrust Ruling:  <br>A US judge ruled that Google illegally maintained its monopoly on online search and advertising by paying billions to be the default search engine on smartphones and browsers. This landmark decision could have significant implications for how tech giants operate, as antitrust authorities aim to strengthen competition. The penalties Google may face will be decided in a future hearing.*** <br><br>
    Aug 6, [according to BBC](https://www.bbc.com/news/articles/c0k44x6mge3o), A US judge has ruled Google acted illegally to crush its competition and maintain a monopoly on online search and related advertising. The landmark decision on Monday is a major blow to Alphabet, Google's parent company, and could reshape how technology giants do business. Google was sued by the US Department of Justice in 2020 over its control of about 90% of the online search market. It is one of several lawsuits that have been filed against the big tech companies as US antitrust authorities attempt to strengthen competition in the industry. This case has at times been described as posing an existential threat to Google and its owner given its dominance of the search and online advertising business. It is unclear yet what penalties Google and Alphabet will face as a result of the decision. The fines or other remedies will be decided in a future hearing. In his decision, US District Judge Amit Mehta said Google had paid billions to ensure it is the default search engine on smartphones and browsers. “Google is a monopolist, and it has acted as one to maintain its monopoly,” Judge Mehta wrote in his 277-page opinion. Alphabet said it plans to appeal against the ruling. US Attorney General Merrick Garland, the country's top prosecutor, hailed the ruling as a "historic win for the American people". Another case against the technology company over its advertising technology is scheduled to go to trial in September. In Europe, meanwhile, Google has been fined billions in monopoly cases. <br><br>

21. ***Privacy in AI Assistants:  <br>Google proposed operationalizing contextual integrity (CI) in AI assistants to address privacy concerns. By aligning information-sharing actions with privacy expectations, the framework aims to prevent AI assistants from inappropriately sharing user information. The study's evaluation shows that CI-based reasoning improves privacy compliance in AI assistants.*** <br><br>
    Aug 5, Google published a [paper](https://arxiv.org/pdf/2408.02373) “Operationalizing Contextual Integrity in Privacy-Conscious Assistants”. Advanced AI assistants combine frontier LLMs and tool access to autonomously perform complex tasks on behalf of users. While the helpfulness of such assistants can increase dramatically with access to user information including emails and documents, this raises privacy concerns about assistants sharing inappropriate information with third parties without user supervision. To steer information-sharing assistants to behave in accordance with privacy expectations, the study proposes to operationalize contextual integrity (CI), a framework that equates privacy with the appropriate flow of information in a given context. In particular, the authors design and evaluate a number of strategies to steer assistants' information-sharing actions to be CI compliant. The evaluation is based on a novel form filling benchmark composed of synthetic data and human annotations, and it reveals that prompting frontier LLMs to perform CI-based reasoning yields strong results. <br><br>

23. ***RAG Foundry Framework:  <br>Intel Labs introduced "RAG Foundry," an open-source framework for enhancing large language models with Retrieval-Augmented Generation (RAG). The framework integrates data creation, training, inference, and evaluation, allowing for rapid prototyping and experimentation with RAG techniques. RAG Foundry has shown consistent improvements in knowledge-intensive tasks.*** <br><br>
    Aug 5, Inter Labs published a [paper](https://arxiv.org/pdf/2408.02545) “RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation”. Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. The paper introduces RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. The paper demonstrates the framework's effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry.  <br><br>

25. ***Jailbreaking LLMs:  <br>NYU and Meta published a study analyzing the jailbreaking of large language models (LLMs) from a statistical perspective. The research introduces E-RLHF, a modification to the existing RLHF objective, which increases the likelihood of safe responses. E-RLHF outperforms RLHF in preventing harmful behavior while maintaining model performance.*** <br><br>
    Aug 2, NYU and Meta published a [paper](https://arxiv.org/pdf/2408.01420) “Mission Impossible: A Statistical Perspective on Jailbreaking LLMs”. Large language models (LLMs) are trained on a deluge of text data with limited quality control. As a result, LLMs can exhibit unintended or even harmful behaviours, such as leaking information, fake news or hate speech. Countermeasures, commonly referred to as preference alignment, include fine-tuning the pretrained LLMs with carefully crafted text examples of desired behaviour. Even then, empirical evidence shows preference aligned LLMs can be enticed to harmful behaviour. This so called jailbreaking of LLMs is typically achieved by adversarially modifying the input prompt to the LLM. The paper provides theoretical insights into the phenomenon of preference alignment and jailbreaking from a statistical perspective. Under the framework, the authors first show that pretrained LLMs will mimic harmful behaviour if present in the training corpus. Under that same framework, the paper then introduces a statistical notion of alignment, and lower-bound the jailbreaking probability, showing that it is unpreventable under reasonable assumptions. Based on the insights, the authors propose an alteration to the currently prevalent alignment strategy RLHF. Specifically, the research introduces a simple modification to the RLHF objective, called E-RLHF, that aims to increase the likelihood of safe responses. E-RLHF brings no additional training cost, and is compatible with other methods. Empirically, the study demonstrates that E-RLHF outperforms RLHF on all alignment problems put forward by the AdvBench and HarmBench project without sacrificing model performance as measured by the MT-Bench project. <br><br>

27. ***RELBENCH for Relational Databases:  <br>Stanford University and Kumo.AI introduced "RELBENCH," a benchmark for predictive tasks over relational databases using graph neural networks. The study demonstrates that relational deep learning (RDL) models outperform traditional manual feature engineering, reducing human effort and improving predictive accuracy.*** <br><br>
    Jul 29, Stanford Uni, Kumo.AI etc published a [paper](https://arxiv.org/pdf/2407.20060) “RELBENCH: A Benchmark for Deep Learning on Relational Databases”.  The paper presents RELBENCH, a public benchmark for solving predictive tasks over relational databases with graph neural networks. RELBENCH provides databases and tasks spanning diverse domains and scales, and is intended to be a foundational infrastructure for future research. The work uses RELBENCH to conduct the first comprehensive study of Relational Deep Learning (RDL) (Fey et al., 2024), which combines graph neural network predictive models with (deep) tabular models that extract initial entity-level representations from raw tables. End-to-end learned RDL models fully exploit the predictive signal encoded in primary-foreign key links, marking a significant shift away from the dominant paradigm of manual feature engineering combined with tabular models. To thoroughly evaluate RDL against this prior gold-standard, the authors conduct an in-depth user study where an experienced data scientist manually engineers features for each task. In this study, RDL learns better models whilst reducing human work needed by more than an order of magnitude. This demonstrates the power of deep learning for solving predictive tasks over relational databases, opening up many new research opportunities enabled by RelBench. <br><br>

29. ***Chain of Code for Reasoning:  <br>A paper from Google, Stanford, and UC Berkeley proposed "Chain of Code" (CoC), an extension to improve language model reasoning by integrating code emulation. CoC outperforms existing reasoning methods like Chain of Thought, broadening the scope of questions LMs can answer by simulating code execution.*** <br><br>
    Jul 29, Google, Stanford Uni, and UC Berkley published a [paper](https://chain-of-code.github.io/paper.pdf) on ICML2024 “Chain of Code: Reasoning with a Language Model-Augmented Code Emulator”. Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter – it’s hypothesized that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for semantic ones (and in particular, those that are a mix of both). For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for "detect_sarcasm(string)" that can be executed by the interpreter (handling the edge cases would be insurmountable). However, LMs may still produce a valid solution if they not only write code, but also selectively "emulate" the interpreter by generating the expected output of "detect_sarcasm(string)". This paper proposes Chain of Code (CoC), a simple yet surprisingly effective extension that improves LM code-driven reasoning. The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode that the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as an "LMulator"). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over Chain of Thought. In a nutshell, CoC broadens the scope of reasoning questions that LMs can answer by "thinking in code". <br><br>

30. ***AdaCoder for Visual Question Answering:  <br>Researchers from Tokyo Institute of Technology and OMRON SINIC X Corp developed "AdaCoder," a framework for adaptive prompt compression in visual programmatic models (VPMs). AdaCoder reduces input token length while maintaining or improving performance in visual question answering tasks, demonstrating its effectiveness in optimizing VPMs.*** <br><br>
    Jul 28, Tokyo Inst. Of Tech, OMRON SINIC X Corp et al published a [paper](https://arxiv.org/pdf/2407.19410) “AdaCoder: Adaptive Prompt Compression for Programmatic Visual Question Answering”. Visual question answering aims to provide responses to natural language questions given visual input. Recently, visual programmatic models (VPMs), which generate executable programs to answer questions through large language models (LLMs), have attracted research interest. However, they often require long input prompts to provide the LLM with sufficient API usage details to generate relevant code. To address this limitation, the paper proposes AdaCoder, an adaptive prompt compression framework for VPMs. AdaCoder operates in two phases: a compression phase and an inference phase. In the compression phase, given a preprompt that describes all API definitions in the Python language with example snippets of code, a set of compressed preprompts is generated, each depending on a specific question type. In the inference phase, given an input question, AdaCoder predicts the question type and chooses the appropriate corresponding compressed preprompt to generate code to answer the question. Notably, AdaCoder employs a single frozen LLM and pre-defined prompts, negating the necessity of additional training and maintaining adaptability across different powerful black-box LLMs such as GPT and Claude. In experiments, the authors apply AdaCoder to ViperGPT and demonstrate that it reduces token length by 71.1%, while maintaining or even improving the performance of visual question answering. <br><br>

32. ***Modular RAG Framework:  <br>A modular framework for Retrieval-Augmented Generation (RAG) systems is proposed to enable highly reconfigurable and specialized operators. This framework addresses the limitations of existing RAG paradigms, facilitating the development of more efficient and adaptable RAG systems.*** <br><br>
    Jul 26, Tonji Uni and Fudan Uni published a [paper](https://arxiv.org/pdf/2407.21059) “Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks”. Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of "retrieve-then-generate". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies. <br><br>

34. ***Limitations of Instruction Tuning:  <br>the ICML2024 paper finds that while IT is widely used to convert pre-trained LLMs into conversational agents, it doesn't improve knowledge or skills and may even degrade them. Key issues include a decline in response quality, increased hallucination, and the ineffectiveness of popular IT improvement methods. The authors emphasize that responses based on pre-trained knowledge outperform those generated from IT and hope these insights guide future research.*** <br><br>
    Jul 14, Uni of Maryland, Adobe, and Nvidia published a [paper](https://arxiv.org/pdf/2402.05119) on ICML2024 “A Closer Look at the Limitations of Instruction Tuning”. Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, the authors reveal various limitations of IT. In particular, the research shows that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating responses. (4) Popular methods to improve IT do not lead to performance improvements over a simple LoRA fine-tuned model. The findings reveal that responses generated solely from pre-trained knowledge consistently outperform responses by models that learn any form of new knowledge from IT on open-source datasets. The authors hope the insights and challenges revealed in this paper inspire future work in related directions.
 <br><br><br>

***Aug 4 2024***

1. ***Lean AI and Small Language Models: <br>
   The article discusses the shift towards lean AI, which aims to optimize efficiency and minimize resource consumption. This shift is driven by the high costs and resource demands of large language models (LLMs). Small language models (SLMs) are becoming more popular due to their lower operational costs, faster deployment cycles, and specialized applications. Open-source initiatives are making advanced AI more accessible and affordable for organizations.***
   
   Aug 2, [InfoWorld](https://www.infoworld.com/article/3480593/small-language-models-and-open-source-are-transforming-ai.html) published an article Small language models (SLM) and open source are transforming AI. The shift towards lean AI emphasizes optimizing efficiency and minimizing resource consumption, addressing the high costs and resource demands of large language models (LLMs). Enterprises are increasingly adopting SLMs for their lower operational costs, faster deployment cycles, and ability to deliver specialized applications. Open-source initiatives and tools are democratizing AI capabilities, enabling more organizations to incorporate advanced AI without relying on expensive proprietary solutions. <br><br>

3. ***Meta's SAM 2 for Visual Segmentation: <br>
   Meta introduced the Segment Anything Model 2 (SAM 2) for visual segmentation in images and videos. SAM 2 uses a transformer architecture with streaming memory for real-time video processing and has the largest video segmentation dataset. It provides better accuracy with fewer interactions and is significantly faster and more accurate than its predecessor, SAM. The model, dataset, and an interactive demo are being released.***
   
   Aug 1, Meta published a [paper](https://arxiv.org/pdf/2408.00714) “SAM 2: Segment Anything in Images and Videos”. The research presents Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. The  authors build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. The model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on the data provides strong performance across a wide range of tasks. In video segmentation, it is observed better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, the model is more accurate and 6x faster than the Segment Anything Model (SAM). The authors believe that the data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. The authors are releasing a version of [the model](https://github.com/facebookresearch/segment-anything-2), the dataset and an interactive demo. <br><br>

5. ***Microsoft's OmniParser for GUI Agents: <br>
   Microsoft presented OmniParser, a method for parsing user interface screenshots into structured elements. OmniParser enhances GPT-4V's ability to generate actions accurately grounded in the corresponding regions of the interface. By using curated datasets for icon detection and description, OmniParser significantly improves performance on benchmarks and outperforms existing models that require additional information.***
   Aug 1, Microsoft published a [paper](https://arxiv.org/pdf/2408.00203) “OmniParser for Pure Vision Based GUI Agent”. The recent success of large vision language models shows great potential in driving the agent system operating on user interfaces. However, the power multimodal models like GPT-4V as a general agent on multiple operating systems across different applications is largely underestimated due to the lack of a robust screen parsing technique capable of: 1) reliably identifying interactable icons within the user interface, and 2) understanding the semantics of various elements in a screenshot and accurately associate the intended action with the corresponding region on the screen. To fill these gaps, the paper introduces OmniParser, a comprehensive method for parsing user interface screenshots into structured elements, which significantly enhances the ability of GPT-4V to generate actions that can be accurately grounded in the corresponding regions of the interface. The authors first curated an interactable icon detection dataset using popular webpages and an icon description dataset. These datasets were utilized to fine-tune specialized models: a detection model to parse interactable regions on the screen and a caption model to extract the functional semantics of the detected elements. OmniParser significantly improves GPT-4V's performance on ScreenSpot benchmark. And on Mind2Web and AITW benchmark, OmniParser with screenshot only input outperforms the GPT-4V baselines requiring additional information outside of screenshot. <br><br>

8. ***Scaling Inference Compute with Repeated Sampling: <br>
   Researchers from Stanford, Oxford, and Google explored scaling inference compute by increasing the number of generated samples. They found that coverage, or the fraction of problems solved, scales with the number of samples. Repeated sampling significantly improves performance in domains with verifiable answers, and it is cost-effective. However, identifying correct samples in domains without automatic verifiers remains a challenge.***
   
   Jul 31, Stanford Uni, Uni of Oxford and Google published a [paper](https://arxiv.org/pdf/2407.21787) “Large Language Monkeys: Scaling Inference Compute with Repeated Sampling”. Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. This paper explores inference compute as another axis for scaling by increasing the number of generated samples. Across multiple tasks and models, the authors observe that coverage - the fraction of problems solved by any attempt - scales with the number of samples over four orders of magnitude. In domains like coding and formal proofs, where all answers can be automatically verified, these increases in coverage directly translate into improved performance. When applying repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-attempt state-of-the-art of 43% which uses more capable frontier models. Moreover, using current API pricing, amplifying the cheaper DeepSeek model with five samples is more cost-effective and solves more issues than paying a premium for one sample from GPT-4o or Claude 3.5 Sonnet. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. Finally, the work finds that identifying correct samples out of many generations remains an important direction for future research in domains without automatic verifiers. When solving math word problems from GSM8K and MATH, coverage with Llama-3 models grows to over 95% with 10,000 samples. However, common methods to pick correct solutions from a sample collection, such as majority voting or reward models, plateau beyond several hundred samples and fail to fully scale with the sample budget. <br><br>

9. ***Australia's Privacy Concerns with Social Media Platform X: <br>
    Australia's privacy watchdog is concerned that social media platform X (formerly Twitter) may be breaching privacy laws by automatically opting users into having their posts used to train AI systems. Platforms are required to ensure default settings enable user control and seek consent for data use. The watchdog is investigating practices across the industry as other major platforms also harvest user data to train AI.***
   
    Jul 31, according to [abc.new.au](https://www.abc.net.au/news/science/2024-07-31/elon-musk-x-breach-privacy-law-data-harvest-grok-ai/104054400), Australia's privacy watchdog says social media platform X (formerly Twitter) may be in breach of Australian privacy law after it emerged users were automatically opted in to having their posts used to build artificial intelligence (AI) systems. The Office of the Australian Information Commissioner stopped short of saying it would launch an inquiry into the platform's data collection, similar to the one it is currently undertaking in relation to TikTok. On Friday, an X user pointed out the X app privacy settings includes a pre-ticked box that permits X to use the account holder's posts to train the Grok AI chatbot built by Elon Musk's company xAI. The default setting states that you "allow your posts as well as your interactions, inputs and results with Grok to be used for training and fine-tuning". Under Australian law, platforms are required to ensure default settings enable user control, and to either seek an individuals' consent for how the platform will use the data, or be satisfied the user would reasonably expect the organisation to use their data for this purpose. In recent months, it also emerged other platforms, such as Meta and Slack, were harvesting user data to train AI as part of a global race to build bigger and better large language models (LLMs). Last month, it was revealed xAI was trying to build the world's largest supercomputer in the US city of Memphis to fuel its AI ambitions. The Commissioner's Office says it's "looking at such practices across the industry" as other major platforms, also competing to build their own AIs, harvest user data. <br><br>

11. ***Safetywashing in AI Safety Benchmarks: <br>
    A study by various universities analyzed AI safety benchmarks and found many are highly correlated with general capabilities, leading to "safetywashing." The study calls for more meaningful safety metrics that are empirically separable from generic capabilities. The authors propose a rigorous framework for AI safety research to advance the science of safety evaluations and clarify measurable progress.***
    
    Jul 31, Center of AI Safety, Uni of Penn., UC Berkeley, Stanford Uni, Yale Uni and Keio Uni published a [paper](https://arxiv.org/pdf/2407.21792) “Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?”. As artificial intelligence systems grow more powerful, there has been increasing interest in "AI safety" research to address emerging and future risks. However, the field of AI safety remains poorly defined and inconsistently measured, leading to confusion about how researchers can contribute. This lack of clarity is compounded by the unclear relationship between AI safety benchmarks and upstream general capabilities (e.g., general knowledge and reasoning). To address these issues, the study conducts a comprehensive meta-analysis of AI safety benchmarks, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety. The findings reveal that many safety benchmarks highly correlate with upstream model capabilities, potentially enabling "safetywashing" -- where capability improvements are misrepresented as safety advancements. Based on these findings, the authors propose an empirical foundation for developing more meaningful safety metrics and define AI safety in a machine learning research context as a set of clearly delineated research goals that are empirically separable from generic capabilities advancements. In doing so, the authors aim to provide a more rigorous framework for AI safety research, advancing the science of safety evaluations and clarifying the path towards measurable progress. <br><br>

13. ***Google's Gemma 2 AI Models: <br>
    Google introduced the Gemma 2 family, including Gemma 2 2B, ShieldGemma, and Gemma Scope. Gemma 2 2B is a lightweight model with superior performance and efficiency. ShieldGemma offers safety content classifier models for various tasks, while Gemma Scope provides insights into model operations. These additions enhance AI capabilities, safety, and innovation.***
    
    Jul 31, Google released [Gemma 2 family](https://developers.googleblog.com/en/smaller-safer-more-transparent-advancing-responsible-ai-with-gemma/#:~:text=A%20Future%20Built%20on%20Responsible,developing%20safe%20and%20beneficial%20AI.) members Gemma 2 2B, SheldGemma and Gemma Scope. [Gemma 2 2B](https://huggingface.co/collections/google/gemma-2-2b-release-66a20f3796a2ff2a7c76f98f) – a brand-new version of the popular 2 billion (2B) parameter model, featuring built-in safety advancements and a powerful balance of performance and efficiency. This lightweight model produces outsized results by learning from larger models through distillation. In fact, Gemma 2 2B surpasses all GPT-3.5 models on the Chatbot Arena, demonstrating its exceptional conversational AI abilities. ShieldGemma – a suite of safety content classifier models, built upon Gemma 2, to filter the input and outputs of AI models and keep the user safe. [ShieldGemma](https://huggingface.co/google/shieldgemma-2b (/9b/27b)) offers various model sizes to meet diverse needs. The 2B model is ideal for online classification tasks, while the 9B and 27B versions provide higher performance for offline applications where latency is less of a concern. [Gemma Scope](https://huggingface.co/google/gemma-scope) – a new model interpretability tool that offers unparalleled insight into our models' inner workings. With these additions, researchers and developers can now create safer customer experiences, gain unprecedented insights into the models, and confidently deploy powerful AI responsibly, right on device, unlocking new possibilities for innovation. <br><br>

15. ***ShieldGemma for Content Moderation: <br>
    Google presented ShieldGemma, a suite of LLM-based safety content moderation models built on Gemma2. These models offer robust predictions of safety risks and outperform existing models on benchmarks. The paper introduces a novel data curation pipeline and demonstrates strong generalization performance. ShieldGemma advances LLM safety and content moderation solutions.***
    
    Jul 31, Google published a [paper](https://arxiv.org/pdf/2407.21772) “ShieldGemma: Generative AI Content Moderation Based on Gemma”. The paper presents ShieldGemma, a comprehensive suite of LLM-based safety content moderation models built upon Gemma2. These models provide robust, state-of-the-art predictions of safety risks across key harm types (sexually explicit, dangerous content, harassment, hate speech) in both user input and LLM-generated output. By evaluating on both public and internal benchmarks, the work demonstrates superior performance compared to existing models, such as Llama Guard (+10.8\% AU-PRC on public benchmarks) and WildCard (+4.3\%). Additionally, the paper presents a novel LLM-based data curation pipeline, adaptable to a variety of safety-related tasks and beyond. The authors have shown strong generalization performance for model trained mainly on synthetic data. By releasing ShieldGemma, the paper provides a valuable resource to the research community, advancing LLM safety and enabling the creation of more effective content moderation solutions for developers. Models are [available here](https://huggingface.co/google/shieldgemma-2b (/9b/27b)). <br><br>

17. ***Meta's Self-Improving Language Models: <br>
    Meta, UC Berkeley, and NYU introduced a Meta-Rewarding step for self-improving language models. This method improves models' judgment skills by having them judge their own responses. The approach enhances both judgment and instruction-following abilities without human supervision, showing significant performance improvements on benchmarks.***
    
    Jul 30, Meta, UC Berkeley, and NYU published a [paper](https://arxiv.org/pdf/2407.19594) “Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge”. Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs can improve by judging their own responses instead of relying on human labelers. However, existing methods have primarily focused on improving model responses rather than judgment capabilities, resulting in rapid saturation during iterative training. To address this issue, the paper introduces a novel Meta-Rewarding step to the self-improvement process, where the model judges its own judgements and uses that feedback to refine its judgment skills. Surprisingly, this unsupervised approach improves the model's ability to judge {\em and} follow instructions, as demonstrated by a win rate improvement of Llama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on Arena-Hard. These results strongly suggest the potential for self-improving models without human supervision. <br><br>

19. ***Google's MoNE for Efficient Visual Processing: <br>
    Google and the University of Washington presented the Mixture of Nested Experts (MoNE) model, which uses a nested structure for experts to process visual tokens efficiently. MoNE reduces inference time compute while maintaining performance, making it adaptable to different compute budgets. The approach is validated on standard image and video datasets.***
    
    Jul 30, Google and Uni of Washington published a [paper](https://arxiv.org/pdf/2407.19985) “Mixture of Nested Experts: Adaptive Processing of Visual Tokens”. The visual medium (images and videos) naturally contains a large amount of information redundancy, thereby providing a great opportunity for leveraging efficiency in processing. While Vision Transformer (ViT) based models scale effectively to large data regimes, they fail to capitalize on this inherent redundancy, leading to higher computational costs. Mixture of Experts (MoE) networks demonstrate scalability while maintaining same inference-time costs, but they come with a larger parameter footprint. The paper presents Mixture of Nested Experts (MoNE), which utilizes a nested structure for experts, wherein individual experts fall on an increasing compute-accuracy curve. Given a compute budget, MoNE learns to dynamically choose tokens in a priority order, and thus redundant tokens are processed through cheaper nested experts. Using this framework, MoNE achieves equivalent performance as the baseline models, while reducing inference time compute by over two-fold. The authors validate the approach on standard image and video datasets - ImageNet-21K, Kinetics400, and Something-Something-v2. The authors further highlight MoNE's adaptability by showcasing its ability to maintain strong performance across different inference-time compute budgets on videos, using only a single trained model. <br><br>

21. ***Diffusion Augmented Agents for RL: <br>
    Google introduced Diffusion Augmented Agents (DAAG), a framework leveraging language models, vision language models, and diffusion models to improve sample efficiency and transfer learning in reinforcement learning. DAAG enhances learning by transforming past experiences to align with target instructions, reducing the need for reward-labeled data and improving lifelong learning capabilities.***
    
    Jul 30, Google published a [paper](https://arxiv.org/pdf/2407.20798) “Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning”. The paper introduces Diffusion Augmented Agents (DAAG), a novel framework that leverages large language models, vision language models, and diffusion models to improve sample efficiency and transfer learning in reinforcement learning for embodied agents. DAAG hindsight relabels the agent's past experience by using diffusion models to transform videos in a temporally and geometrically consistent way to align with target instructions with a technique called as Hindsight Experience Augmentation. A large language model orchestrates this autonomous process without requiring human supervision, making it well-suited for lifelong learning scenarios. The framework reduces the amount of reward-labeled data needed to 1) finetune a vision language model that acts as a reward detector, and 2) train RL agents on new tasks. The paper demonstrates the sample efficiency gains of DAAG in simulated robotics environments involving manipulation and navigation. The results show that DAAG improves learning of reward detectors, transferring past experience, and acquiring new tasks - key abilities for developing efficient lifelong learning agents. Supplementary material and visualizations are available on the website [this https URL](https://sites.google.com/view/diffusion-augmented-agents/) <br><br>

23. ***Meta's SAM 2 for Video and Image Segmentation: <br>
    Meta's Segment Anything Model 2 (SAM 2) is designed for promptable visual segmentation in images and videos. It uses a transformer architecture with streaming memory for real-time processing and has the largest video segmentation dataset. SAM 2 provides better accuracy and speed compared to its predecessor, SAM, and is being released with a dataset and demo.***
    
    Jul 29, Meta published a [paper](https://scontent.fcbr1-1.fna.fbcdn.net/v/t39.2365-6/453323338_287900751050452_6064535069828837026_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=iEdf_eLLDBIQ7kNvgHvSueV&_nc_ht=scontent.fcbr1-1.fna&gid=AbMZVotuhlaDUcgiZQmipJh&oh=00_AYCMj0UJk4ZJNT-OM4nxXp6vgeWLO9SHo56ZlmA1qZHoaQ&oe=66B0FCB9) “SAM 2: Segment Anything in Images and Videos”.  The paper presents Segment Anything Model 2 (SAM 2 ), a foundation model towards solving promptable visual segmentation in images and videos. The authors build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. The model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on the data provides strong performance across a wide range of tasks. In video segmentation, it’s observed better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, the model is more accurate and 6x faster than the Segment Anything Model (SAM). The authors believe that the data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. The authors are releasing [a version of the model](https://ai.meta.com/sam2/), the dataset and an interactive demo. <br><br>

25. ***SaulLM Models for Legal Domain: <br>
    MICS and CINES introduced SaulLM-54B and SaulLM-141B, large language models tailored for the legal sector. These models are based on the Mixtral architecture and use domain adaptation strategies for legal tasks. They outperform previous models on LegalBench-Instruct and are released under the MIT License to facilitate reuse and research.***
    
    Jul 28, MICS and CINES published a [paper](https://arxiv.org/pdf/2407.19584) “SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain”. The paper introduces SaulLM-54B and SaulLM-141B, two large language models (LLMs) tailored for the legal sector. These models, which feature architectures of 54 billion and 141 billion parameters, respectively, are based on the Mixtral architecture. The development of SaulLM-54B and SaulLM-141B is guided by large-scale domain adaptation, divided into three strategies: (1) the exploitation of continued pretraining involving a base corpus that includes over 540 billion of legal tokens, (2) the implementation of a specialized legal instruction-following protocol, and (3) the alignment of model outputs with human preferences in legal interpretations. The integration of synthetically generated data in the second and third steps enhances the models' capabilities in interpreting and processing legal texts, effectively reaching state-of-the-art performance and outperforming previous open-source models on LegalBench-Instruct. This work explores the trade-offs involved in domain-specific adaptation at this scale, offering insights that may inform future studies on domain adaptation using strong decoder models. Building upon SaulLM-7B, this study refines the approach to produce an LLM better equipped for legal tasks. The authors are releasing base, instruct, and aligned versions on top of SaulLM-54B and SaulLM-141B under the MIT License to facilitate reuse and collaborative research. <br><br>

27. ***Amazon's REAPER for RAG Systems: <br>
    Amazon presented REAPER, a reasoning-based retrieval planner for complex RAG (Retrieval Augmented Generation) systems. REAPER generates retrieval plans for conversational systems, significantly reducing latency and scaling easily to new use cases. The method is shown to improve performance in a conversational shopping assistant context.***
    
    Jul 26, Amazon published a [paper](https://arxiv.org/abs/2407.18553) “REAPER: Reasoning based Retrieval Planning for Complex RAG Systems”. Complex dialog systems often use retrieved evidence to facilitate factual responses. Such RAG (Retrieval Augmented Generation) systems retrieve from massive heterogeneous data stores that are usually architected as multiple indexes or APIs instead of a single monolithic source. For a given query, relevant evidence needs to be retrieved from one or a small subset of possible retrieval sources. Complex queries can even require multi-step retrieval. For example, a conversational agent on a retail site answering customer questions about past orders will need to retrieve the appropriate customer order first and then the evidence relevant to the customer's question in the context of the ordered product. Most RAG Agents handle such Chain-of-Thought (CoT) tasks by interleaving reasoning and retrieval steps. However, each reasoning step directly adds to the latency of the system. For large models this latency cost is significant -- in the order of multiple seconds. Multi-agent systems may classify the query to a single Agent associated with a retrieval source, though this means that a (small) classification model dictates the performance of a large language model. This paper presents REAPER (REAsoning-based PlannER) - an LLM based planner to generate retrieval plans in conversational systems. The paper shows significant gains in latency over Agent-based systems and are able to scale easily to new and unseen use cases as compared to classification-based planning. Though the method can be applied to any RAG system, the authors show the results in the context of a conversational shopping assistant. <br><br>

29. ***Apple's MMAU Benchmark for LLM Agents: <br>
    Apple introduced the Massive Multitask Agent Understanding (MMAU) benchmark, evaluating models across five domains and capabilities. MMAU provides a comprehensive framework for assessing LLM agents' strengths and limitations with detailed analyses of 18 models. The benchmark enhances interpretability and understanding of model performance.***
    
    Jul 18, Apple published a [paper](https://arxiv.org/pdf/2407.18961) “MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains”. Recent advances in large language models (LLMs) have increased the demand for comprehensive benchmarks to evaluate their capabilities as human-like agents. Existing benchmarks, while useful, often focus on specific application scenarios, emphasizing task completion but failing to dissect the underlying skills that drive these outcomes. This lack of granularity makes it difficult to deeply discern where failures stem from. Additionally, setting up these environments requires considerable effort, and issues of unreliability and reproducibility sometimes arise, especially in interactive tasks. To address these limitations, the authors introduce the Massive Multitask Agent Understanding (MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need for complex environment setups. It evaluates models across five domains, including Tool-use, Directed Acyclic Graph (DAG) QA, Data Science and Machine Learning coding, Contest-level programming and Mathematics, and covers five essential capabilities: Understanding, Reasoning, Planning, Problem-solving, and Self-correction. With a total of 20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU provides a comprehensive framework for evaluating the strengths and limitations of LLM agents. By testing 18 representative models on MMAU, the paper provides deep and insightful analyses. Ultimately, MMAU not only sheds light on the capabilities and limitations of LLM agents but also enhances the interpretability of their performance. Datasets and evaluation scripts of MMAU are released at https://github.com/apple/axlearn/blob/main/docs/research/mmau.
 <br><br><br>

***28 Jul 2024***

1. ***Google’s AI achieves silver-medal standard solving International Mathematical Olympiad problems <br>
Google's AlphaProof and AlphaGeometry teams developed new AI systems capable of solving complex math problems. The systems, AlphaProof for algebra and number theory, and AlphaGeometry 2 for geometry, achieved a silver-medal standard at the 2023 International Mathematical Olympiad, solving four out of six problems. AlphaProof handled algebra and number theory, including the competition's hardest problem, while AlphaGeometry 2 solved the geometry problem. The systems scored 28 points out of a possible 42, just below the gold-medal threshold of 29 points, showcasing advanced mathematical reasoning capabilities.*** <br>
   Jul 25, Google’s AlphaProof  and AlphaGeometry teams published a [blog](https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/) “AI achieves silver-medal standard solving International Mathematical Olympiad problems”. Artificial general intelligence (AGI) with advanced mathematical reasoning has the potential to unlock new frontiers in science and technology. AlphaProof is a new reinforcement-learning based system for formal math reasoning, and AlphaGeometry 2, is an improved version of our geometry-solving system. Together, these systems solved four out of six problems from this year’s International Mathematical Olympiad (IMO), achieving the same level as a silver medalist in the competition for the first time. The IMO is the oldest, largest and most prestigious competition for young mathematicians, held annually since 1959. More recently, the annual IMO competition has also become widely recognised as a grand challenge in machine learning and an aspirational benchmark for measuring an AI system’s advanced mathematical reasoning capabilities. AlphaProof solved two algebra problems and one number theory problem by determining the answer and proving it was correct. This included the hardest problem in the competition, solved by only five contestants at this year’s IMO. AlphaGeometry 2 proved the geometry problem, while the two combinatorics problems remained unsolved. Each of the six problems can earn seven points, with a total maximum of 42. [Google’s system achieved a final score of 28 points](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/imo-2024-solutions/index.html), earning a perfect score on each problem solved — equivalent to the top end of the silver-medal category. This year, the gold-medal threshold starts at 29 points, and was achieved by 58 of 609 contestants at the official competition. <br><br>

3. ***AI models collapse when trained on recursively generated data <br>
Researchers from several universities published findings on the risks of training AI models on data generated by other AI models. They identified a phenomenon called 'model collapse,' where the quality of models degrades due to recursive training on AI-generated content, leading to the loss of rare content features. This study emphasizes the need for human-generated data to maintain the integrity and performance of AI models over time.*** <br>
   Jul 25, Uni of Oxford, Uni of Cambridge,  Imperial College London, Uni of Toronto, Uni of Edinburgh published a [paper](https://www.nature.com/articles/s41586-024-07566-y.pdf) on Nature “AI models collapse when trained on recursively generated data”. Stable diffusion revolutionized image creation from descriptive text. GPT-2), GPT-3(.5) and GPT-4 demonstrated high performance across a variety of language tasks. ChatGPT introduced such language models to the public. It is now clear that generative artificial intelligence (AI) such as large language models (LLMs) is here to stay and will substantially change the ecosystem of online text and images. Here the authors consider what may happen to GPT-{n} once LLMs contribute much of the text found online. The paper finds that indiscriminate use of model-generated content in training causes irreversible defects in the resulting models, in which tails of the original content distribution disappear. The authors refer to this effect as ‘model collapse’ and show that it can occur in LLMs as well as in variational autoencoders (VAEs) and Gaussian mixture models (GMMs). The study builds theoretical intuition behind the phenomenon and portray its ubiquity among all learned generative models. The work demonstrates that it must be taken seriously if people are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of LLM-generated content in data crawled from the Internet. <br><br>

5. ***OpenAI's SearchGPT prototype <br>
OpenAI launched SearchGPT, a prototype integrating web information with AI model responses to provide users with fast, sourced answers. Currently in testing with a small group, SearchGPT aims to enhance ChatGPT by incorporating real-time web data. The system allows users to ask follow-up questions and aims to improve search interactions. While separate from AI training, SearchGPT ensures that publishers can manage their content's appearance in search results.*** <br>
   Jul 25, [OpenAI released](https://openai.com/index/searchgpt-prototype/) it SearchGPT, a prototype of new search features designed to combine the strength of OpenAI’s AI models with information from the web to give users fast and timely answers with clear and relevant sources. OpenAI is launching to a small group of users and publishers to get feedback. While this prototype is temporary, it plans to integrate the best of these features directly into ChatGPT in the future. SearchGPT will quickly and directly respond to users questions with up-to-date information from the web while giving clear links to relevant sources. Users will be able to ask follow-up questions, like one would in a conversation with a person, with the shared context building with each query. OpenAI has partnered with publishers to build this experience and continue to seek their feedback. In addition to launching the SearchGPT prototype, OpenAI is also launching a way for publishers to manage how they appear in SearchGPT, so publishers have more choices. Importantly, SearchGPT is about search and is separate from training OpenAI’s generative AI foundation models. Sites can be surfaced in search results even if they opt out of generative AI training. However, it is found that the answers of SearchGPT are mostly incorrect. <br><br>

7. ***Data Provenance Initiative's audit on AI data consent protocols <br>
The Data Provenance Initiative highlighted a growing crisis in data consent for AI training. Their audit of 14,000 web domains showed increasing restrictions on the use of web data for AI, with significant portions of commonly used datasets becoming inaccessible. This trend threatens the diversity and scalability of AI systems, prompting a call for more effective data consent protocols.*** <br>
   Jul 24, Data Provenance Initiative (a collective of independent and academic researchers) published a [paper](https://arxiv.org/pdf/2407.14933) “Consent in Crisis: The Rapid Decline of the AI Data Commons”. General-purpose artificial intelligence (AI) systems are built on massive swathes of public web data, assembled into corpora such as C4, RefinedWeb, and Dolma. To the authors’ knowledge, they conduct the first, large-scale, longitudinal audit of the consent protocols for the web domains underlying AI training corpora. The audit of 14,000 web domains provides an expansive view of crawlable web data and how consent preferences to use it are changing over time. The authors observe a proliferation of AI-specific clauses to limit use, acute differences in restrictions on AI developers, as well as general inconsistencies between websites' expressed intentions in their Terms of Service and their robots.txt. The paper diagnoses these as symptoms of ineffective web protocols, not designed to cope with the widespread re-purposing of the internet for AI. The longitudinal analyses show that in a single year (2023-2024) there has been a rapid crescendo of data restrictions from web sources, rendering ~5%+ of all tokens in C4, or 28%+ of the most actively maintained, critical sources in C4, fully restricted from use. For Terms of Service crawling restrictions, a full 45% of C4 is now restricted. If respected or enforced, these restrictions are rapidly biasing the diversity, freshness, and scaling laws for general-purpose AI systems. The authors hope to illustrate the emerging crisis in data consent, foreclosing much of the open web, not only for commercial AI, but non-commercial AI and academic purposes. <br><br>

9. ***Mistral releases Mistral Large 2 AI model <br>
Mistral unveiled Mistral Large 2, an advanced AI model with a 128k context window and support for multiple languages and coding languages. The model excels in single-node inference and boasts high accuracy on various benchmarks, including coding and reasoning. It sets a new standard for performance and cost efficiency in AI models, offering robust performance on par with leading models like GPT-4.*** <br>
    Jul 24, [Mistral released Mistral Large English](https://mistral.ai/news/mistral-large-2407/), the latest generation continues to push the boundaries of cost efficiency, speed, and performance. Mistral Large 2 is exposed on la Plateforme and enriched with new features to facilitate building innovative AI applications. Mistral Large 2 has a 128k context window and supports dozens of languages including French, German, Chinese, etc, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash. Mistral Large 2 is designed for single-node inference with long-context applications in mind – its size of 123 billion parameters allows it to run at large throughput on a single node. Mistral Research License allows usage and modification for research and non-commercial usages. Mistral Large 2 sets a new frontier in terms of performance / cost of serving on evaluation metrics. In particular, on MMLU, the pretrained version achieves an accuracy of 84.0%, and sets a new point on the performance/cost Pareto front of open models. Mistral Large 2 vastly outperforms the previous Mistral Large on coding and reasoning, and performs on par with leading models such as GPT-4o, Claude 3 Opus, and Llama 3 405B. Additionally, the new Mistral Large 2 is trained to acknowledge when it cannot find solutions or does not have sufficient information to provide a confident answer. This commitment to accuracy is reflected in the improved model performance on popular mathematical benchmarks, demonstrating its enhanced reasoning and problem-solving skills. The new Mistral Large 2 is particularly better at following precise instructions and handling long multi-turn conversations. <br><br>

11. ***Data mixture inference study using BPE tokenizers <br>
Researchers developed a method to infer the data mixture in language model training sets using byte-pair encoding (BPE) tokenizers. By analyzing token frequency patterns, they accurately estimated the proportions of various data types in training sets. This method revealed new insights into the multilingual and code-focused nature of recent models like GPT-4o and Llama3, contributing to better understanding and transparency in AI model training.*** <br>
    Jul 24, Uni of Washington and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2407.16607) “Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?”. The pretraining data of today's strongest language models is opaque. In particular, little is known about the proportions of various domains or languages represented. This work tackles a task which is called data mixture inference, which aims to uncover the distributional make-up of training data. The work introduces a novel attack based on a previously overlooked source of information -- byte-pair encoding (BPE) tokenizers, used by the vast majority of modern language models. The key insight is that the ordered list of merge rules learned by a BPE tokenizer naturally reveals information about the token frequencies in its training data: the first merge is the most common byte pair, the second is the most common pair after merging the first token, and so on. Given a tokenizer's merge list along with data samples for each category of interest, the authors formulate a linear program that solves for the proportion of each category in the tokenizer's training set. Importantly, to the extent to which tokenizer training data is representative of the pretraining data, the work indirectly learns about the pretraining data. In controlled experiments, the paper shows that the attack recovers mixture ratios with high precision for tokenizers trained on known mixtures of natural languages, programming languages, and data sources. The paper then applies the approach to off-the-shelf tokenizers released with recent LMs. The authors confirm much publicly disclosed information about these models, and also make several new inferences: GPT-4o's tokenizer is much more multilingual than its predecessors, training on 39% non-English data; Llama3 extends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and Claude's tokenizers are trained on predominantly code (~60%). The work sheds light on current design practices for pretraining data, and inspires continued research into data mixture inference for LMs. <br><br>

13. ***Elon Musk's xAI Memphis Supercluster <br>
Elon Musk announced the creation of the Memphis Supercluster, a powerful AI training cluster using 100,000 Nvidia H100 GPUs. This initiative aims to develop the world's most powerful AI, Grok 3, by December 2023. The supercluster's scale surpasses current top supercomputers, highlighting Musk's ambitious plans for AI advancements.*** <br>
    Jul 23, according to [tomshardware.com](https://www.tomshardware.com/pc-components/gpus/elon-musk-fires-up-the-most-powerful-ai-training-cluster-in-the-world-uses-100000-nvidia-h100-gpus-on-a-single-fabric), Tech baron Elon Musk has taken to Twitter/X to boast of starting up “the most powerful AI training cluster in the world,” which he will use to create the self-professed "world’s most powerful AI by every metric by December of this year.” Today, xAI’s Memphis Supercluster began AI training using 100,000 liquid-cooled Nvidia H100 GPUs connected with a single RDMA (remote direct memory access) fabric. In a follow-up Tweet, Musk explains that the new supercluster will be “training the world’s most powerful AI by every metric.” From previous statements of intent, it is assumed that the power of xAI’s 100,000 H100 GPU installation will now be targeted at Grok 3 training. Musk said the refined LLM should be finished with the training stage “by December this year.” To put the Memphis Supercluster compute resources in some context, certainly, going by scale, the new xAI Memphis Supercluster easily outclasses anything in the most recent Top500 list in terms of GPU horsepower. The world’s most powerful supercomputers such as Frontier (37,888 AMD GPUs), Aurora (60,000 Intel GPUs), and Microsoft Eagle (14,400 Nvidia H100 GPUs) seem to be significantly outgunned by the xAI machine. <br><br>

15. ***Meta's release of Llama 3.1 <br>
Meta introduced Llama 3, a new set of foundation models supporting multilinguality, coding, reasoning, and tool usage. The largest model, with 405 billion parameters, rivals leading AI models like GPT-4. Llama 3 integrates capabilities across image, video, and speech tasks and includes safety features. The release aims to push the boundaries of AI performance and versatility.*** <br>
    Jul 23, Meta released llama 3.1 and the corresponding [paper](https://scontent.fcbr1-1.fna.fbcdn.net/v/t39.2365-6/452387774_1036916434819166_4173978747091533306_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=7qSoXLG5aAYQ7kNvgHS5YSv&_nc_ht=scontent.fcbr1-1.fna&oh=00_AYDrMk-4WCS9Y2Sa1syLLvRW05gfxbKWvz1mJleRSHbpBg&oe=66AA6F8D) “The Llama 3 Herd of Models”. Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. The largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. The authors find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. The authors publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and the Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which integrates image, video, and speech capabilities into Llama 3 via a compositional approach. The authors observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development. <br><br>

17. ***Switzerland mandates open-source software for government <br>
Switzerland passed a law requiring all government software to be open-source, promoting transparency, security, and efficiency. This legislation, part of a broader European trend, mandates public disclosure of software code and the release of non-sensitive government data as Open Government Data (OGD). The move aims to foster greater openness and practical reuse of software and data in the public sector.*** <br>
    Jul 23, [according to zdnet.com](https://www.zdnet.com/article/switzerland-now-requires-all-government-software-to-be-open-source/), Switzerland now requires all government software to be open source. Several European countries are betting on open-source software. In the United States, eh, not so much. In the latest news from across the Atlantic, Switzerland has taken a major step forward with its "Federal Law on the Use of Electronic Means for the Fulfillment of Government Tasks" (EMBAG). This groundbreaking legislation mandates using open-source software (OSS) in the public sector. This new law requires all public bodies to disclose the source code of software developed by or for them unless third-party rights or security concerns prevent it. This "public money, public code" approach aims to enhance government operations' transparency, security, and efficiency. In addition to mandating OSS, the EMBAG also requires the release of non-personal and non-security-sensitive government data as Open Government Data (OGD). This dual "open by default" approach marks a significant paradigm shift towards greater openness and practical reuse of software and data. Other countries in Europe have long supported open source. For example, in 2023, French President Macron stated, "We love open source," and France's National Gendarmerie (Think FBI if you're an American) uses Linux on its PCs. The European Union (EU) has long worked on securing OSS via the EU's Free and Open Source Software Auditing (FOSSA) project. <br><br>

19. ***OpenDevin platform for AI software developers <br>
A new platform, OpenDevin, was introduced to facilitate the development of AI agents that interact with their environment similarly to human developers. OpenDevin supports code writing, command line interaction, and web browsing, allowing for flexible and powerful AI agent development. The platform, open-sourced and community-driven, aims to enhance AI capabilities across various challenging tasks.*** <br>
    Jul 23, UIUC, CMU, Yale, UC Berkeley, ContextualAI, et al. published a [paper](https://arxiv.org/pdf/2407.16741) “OpenDevin: An Open Platform for AI Software Developers as Generalist Agents”. Software is one of the most powerful tools that humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. This paper introduces OpenDevin, a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to those of a human developer: by writing code, interacting with a command line, and browsing the web. The authors describe how the platform allows for the implementation of new agents, safe interaction with sandboxed environments for code execution, coordination between multiple agents, and incorporation of evaluation benchmarks. Based on the currently incorporated benchmarks, the authors perform an evaluation of agents over 15 challenging tasks, including software engineering (e.g., SWE-Bench) and web browsing (e.g., WebArena), among others. Released under the permissive MIT license, [OpenDevin](https://github.com/OpenDevin/OpenDevin) is a community project spanning academia and industry with more than 1.3K contributions from over 160 contributors and will improve going forward. <br><br>

21. ***Comparison of KAN and MLP models <br>
Researchers conducted a comprehensive comparison of KAN and MLP models, finding that MLP generally outperforms KAN across various tasks, except for symbolic formula representation. The study also identified the B-spline activation function as a key factor in KAN's performance in symbolic tasks. These findings offer insights for future research on model alternatives and improvements.*** <br>
    Jul 23, Nation Uni of Singapore published a [paper](https://arxiv.org/pdf/2407.16674) “KAN or MLP: A Fairer Comparison”. This paper does not introduce a novel method. Instead, it offers a fairer and more comprehensive comparison of KAN and MLP models across various tasks, including machine learning, computer vision, audio processing, natural language processing, and symbolic formula representation. Specifically, the authors control the number of parameters and FLOPs to compare the performance of KAN and MLP. The main observation is that, except for symbolic formula representation tasks, MLP generally outperforms KAN. The authors also conduct ablation studies on KAN and find that its advantage in symbolic formula representation mainly stems from its B-spline activation function. When B-spline is applied to MLP, performance in symbolic formula representation significantly improves, surpassing or matching that of KAN. However, in other tasks where MLP already excels over KAN, B-spline does not substantially enhance MLP's performance. Furthermore, the study finds that KAN's forgetting issue is more severe than that of MLP in a standard class-incremental continual learning setting, which differs from the findings reported in the KAN paper. The authors hope these results provide insights for future research on KAN and other MLP alternatives. Project link: https://github.com/yu-rp/KANbeFair <br><br>

23. ***Specialization for legal tasks using Llama 3 <br>
A study demonstrated that fine-tuned Llama 3 models significantly outperform GPT-4 on legal text classification tasks. The research showed that even light fine-tuning on specific tasks could achieve high accuracy, suggesting a viable alternative to using commercial models for legal research. This approach could reduce reliance on costly human annotation.*** <br>
    Jul 23, Tubingen AI Center, Harvard Uni, ETH Zurich, Washington Uni and Uni of Virginia published a [paper](https://arxiv.org/pdf/2407.16615) “Lawma: The Power of Specialization for Legal Tasks”. Annotation and classification of legal text are central components of empirical legal research. Traditionally, these tasks are often delegated to trained research assistants. Motivated by the advances in language modeling, empirical legal scholars are increasingly turning to prompting commercial models, hoping that it will alleviate the significant cost of human annotation. Despite growing use, the understanding of how to best utilize large language models for legal tasks remains limited. The work conducts a comprehensive study of 260 legal text classification tasks, nearly all new to the machine learning community. Starting from GPT-4 as a baseline, the paper shows that it has non-trivial but highly varied zero-shot accuracy, often exhibiting performance that may be insufficient for legal work. The authors then demonstrate that a lightly fine-tuned Llama 3 model vastly outperforms GPT-4 on almost all tasks, typically by double-digit percentage points. The authors find that larger models respond better to fine-tuning than smaller models. A few tens to hundreds of examples suffice to achieve high classification accuracy. Notably, the authors can fine-tune a single model on all 260 tasks simultaneously at a small loss in accuracy relative to having a separate model for each task. The work points to a viable alternative to the predominant practice of prompting commercial models. For concrete legal tasks with some available labeled data, researchers are better off using a fine-tuned open-source model. <br><br>

25. ***Local vs global continual learning <br>
Researchers explored continual learning strategies, comparing local and global approximations. They classified existing algorithms based on these strategies and assessed their practical effects. The study highlighted the importance of understanding continual learning mechanisms to develop effective strategies for integrating new information while retaining past knowledge.*** <br>
    Jul 23, ETH Zurich published a [paper](https://arxiv.org/pdf/2407.16611) on CoLLAs 2024 “Local vs Global continual learning”.  Continual learning is the problem of integrating new information in a model while retaining the knowledge acquired in the past. Despite the tangible improvements achieved in recent years, the problem of continual learning is still an open one. A better understanding of the mechanisms behind the successes and failures of existing continual learning algorithms can unlock the development of new successful strategies. This work views continual learning from the perspective of the multi-task loss approximation, and compares two alternative strategies, namely local and global approximations. The authors classify existing continual learning algorithms based on the approximation used, and assess the practical effects of this distinction in common continual learning settings. Additionally, the authors study optimal continual learning objectives in the case of local polynomial approximations and provide examples of existing algorithms implementing the optimal objectives. <br><br>

27. ***Shared hallucinations in LLMs <br>
Salesforce's study on large language models (LLMs) revealed that these models share a 'shared imagination space,' enabling them to answer each other's imaginary questions with high success. This phenomenon suggests a commonality in the models' training and hallucination processes, raising questions about model homogeneity and computational creativity.*** <br>
    Jul 23, Salesforce published a [paper](https://arxiv.org/pdf/2407.16604) “Shared Imagination: LLMs Hallucinate Alike”. Despite the recent proliferation of large language models (LLMs), their training recipes -- model architecture, pre-training data and optimization algorithm -- are often very similar. This naturally raises the question of the similarity among the resulting models. This paper proposes a novel setting, imaginary question answering (IQA), to better understand model similarity. In IQA, the authors ask one model to generate purely imaginary questions (e.g., on completely made-up concepts in physics) and prompt another model to answer. Surprisingly, despite the total fictionality of these questions, all models can answer each other's questions with remarkable success, suggesting a "shared imagination space" in which these models operate during such hallucinations. The authors conduct a series of investigations into this phenomenon and discuss implications on model homogeneity, hallucination, and computational creativity. <br><br>

29. ***Training-free baseline for video LLMs <br>
Apple proposed SlowFast-LLaVA, a training-free video LLM that captures detailed spatial semantics and long-range temporal context. The two-stream design effectively aggregates features from video frames, outperforming existing training-free methods on video tasks. This approach offers a strong baseline for video LLMs without the need for extensive training.*** <br>
    Jul 22, Apple published a [paper](https://arxiv.org/pdf/2407.15841) “SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models”. The paper proposes SlowFast-LLaVA (or SF-LLaVA for short), a training-free video large language model (LLM) that can jointly capture the detailed spatial semantics and long-range temporal context without exceeding the token budget of commonly used LLMs. This is realized by using a two-stream SlowFast design of inputs for Video LLMs to aggregate features from sampled video frames in an effective way. Specifically, the Slow pathway extracts features at a low frame rate while keeping as many spatial details as possible (e.g., with 24x24 tokens), and the Fast pathway operates on a high frame rate but uses a larger spatial pooling stride (e.g., downsampling 6x) to focus on the motion cues. As a result, this design allows adequately capturing both spatial and temporal features that are beneficial for understanding details along the video. Experimental results show that SF-LLaVA outperforms existing training-free methods on a wide range of video tasks. On some benchmarks, it achieves comparable or even better performance compared to state-of-the-art Video LLMs that are fine-tuned on video datasets. <br><br>

31. ***AI-based startups analysis by High Signal AI <br>
An analysis of AI-based startups backed by YCombinator highlighted trends in AI innovation, investment, and founder backgrounds. Key findings include a focus on B2B solutions, opportunities in underserved sectors, and the importance of technical expertise. The study also noted the growing interest in generative AI and the need for ethical AI considerations.*** <br>
    Jul 19, High Signal AI published a [blog](https://highsignalai.substack.com/p/what-i-learned-from-looking-at-400) “What I learned from looking at 400 AI-based Startups backed by YCombinator”. YCombinator's (YC) track record in identifying and nurturing successful startups is unparalleled in the tech industry. Their selection process has consistently surfaced companies that go on to reshape entire sectors, making their portfolio a valuable indicator of emerging trends and technologies.  The author was looking for answers to questions like which sectors are seeing the most AI innovation? What types of AI applications are attracting investment? What backgrounds do successful AI founders have? This study aims to provide insights into: 1) The hottest industries and sectors for AI startups. 2) Areas ripe for AI disruption. 3) AI in emerging technologies like blockchain and quantum computing. 4) Companies working in AI Safety, Accessibility, Explainability. 5) Common traits among YC-backed AI founders. 6) How to find what you should build with AI using the above insights. Key finds are: 1) Focus on B2B: With 81.1% of YC-backed AI startups targeting businesses, consider enterprise solutions for higher chances of funding and success. 2) Explore Underserved Sectors: While healthcare/biotech (10.8%), fintech (9.1%), and developer tools (8.9%) dominate, look for opportunities in neglected areas like manufacturing (1%) or agriculture (0.7%). 3) Prioritize Technical Expertise: Ensure your founding team includes strong technical talent, as 74.8% of YC-backed AI companies have at least one founder with a robust technical background. 4) Capitalize on Generative AI: With 18.7% of startups in this space, generative AI is hot. However, consider how you can apply it innovatively to stand out. 5) Address Ethical Concerns: Only 1.2% of startups focus on ethical AI. This gap represents a significant opportunity for forward-thinking founders. <br><br>

33. ***Pruning and knowledge distillation for LLMs <br>
Nvidia's study on compressing language models through pruning and knowledge distillation demonstrated significant compute cost savings and performance improvements. The approach reduced training data requirements and maintained high accuracy, offering an efficient alternative to training large models from scratch. The compressed models performed well compared to other community models and state-of-the-art compression techniques.*** <br>
    Jul 19, Nvidia published a [paper](https://arxiv.org/pdf/2407.14679) “Compact Language Models via Pruning and Knowledge Distillation”. Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. This paper investigates if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, the study develops a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; and arrives at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. The paper uses this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using the approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. The authors have [open-sourced Minitron](https://github.com/NVlabs/Minitron) model weights on Huggingface, with corresponding supplementary material including example code available on GitHub. <br><br>

35. ***Long-context language models benchmark <br>
Researchers created NoCha, a benchmark for long-context language models, testing their ability to retrieve, synthesize, and reason over book-length inputs. The dataset consists of pairs of true and false claims about books, requiring global reasoning. The study found that current LLMs struggle with these tasks, highlighting the need for further improvements in long-context comprehension.*** <br>
    Jul 18, UMass Amherst, Allen Inst of AI, and Princeton Uni published a [paper](https://arxiv.org/pdf/2406.16264) “One Thousand and One Pairs: A "novel" challenge for long-context language models”. Synthetic long-context LLM benchmarks (e.g., "needle-in-the-haystack") test only surface-level retrieval capabilities, but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? The paper addresses this question by creating NoCha, a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, the annotators confirm that the largest share of pairs in NoCha require global reasoning over the entire book to verify. The experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that evaluated: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest accuracy at 55.8%. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NoCha allows for the evolution of the benchmark dataset and the easy analysis of future models. <br><br>

37. ***AI Agents That Matter <br>
The paper highlights several issues with current AI agent benchmarks, such as an overemphasis on accuracy, lack of attention to costs, conflation of benchmarking needs, inadequate holdout sets, and poor standardization in evaluation practices. The authors propose optimizing both accuracy and cost, distinguishing benchmarking needs, addressing overfitting, and standardizing evaluations to develop more practical and reliable AI agents.*** <br>
    Jul 1, Princeton Uni published [paper](https://arxiv.org/abs/2407.01502) “AI Agents That Matter”. AI agents are an exciting new research direction, and agent development is driven by benchmarks. An analysis of current agent benchmarks and evaluation practices reveals several shortcomings that hinder their usefulness in real-world applications. First, there is a narrow focus on accuracy without attention to other metrics. As a result, SOTA agents are needlessly complex and costly, and the community has reached mistaken conclusions about the sources of accuracy gains. The focus on cost in addition to accuracy motivates the new goal of jointly optimizing the two metrics. The authors design and implement one such optimization, showing its potential to greatly reduce cost while maintaining accuracy. Second, the benchmarking needs of model and downstream developers have been conflated, making it hard to identify which agent would be best suited for a particular application. Third, many agent benchmarks have inadequate holdout sets, and sometimes none at all. This has led to agents that are fragile because they take shortcuts and overfit to the benchmark in various ways. The research prescribes a principled framework for avoiding overfitting. Finally, there is a lack of standardization in evaluation practices, leading to a pervasive lack of reproducibility. The authors hope that the steps introduced for addressing these shortcomings will spur the development of agents that are useful in the real world and not just accurate on benchmarks.

 <br><br><br>

***21 Jul 2024***

1. ***Mistral NeMo Released: <br>
Mistral released Mistral NeMo, a 12B model with a 128k context length, under the Apache 2.0 license. It’s a global, multilingual model with strengths in English, French, Chinese, and Hindi, and uses a new tokenizer, Tekken, which outperforms the previous SentencePiece tokenizer. Mistral NeMo is better at instruction following, reasoning, multi-turn conversations, and code generation compared to its predecessor, Mistral 7B.*** <br>
   Jul 18, [Mistral released Mistral NeMo](https://mistral.ai/news/mistral-nemo/), a state-of-the-art 12B model with 128k context length, and released under the Apache 2.0 license. As it relies on standard architecture, Mistral NeMo is easy to use and a drop-in replacement in any system using Mistral 7B. The model is designed for global, multilingual applications. It is trained on function calling, has a large context window, and is particularly strong in English, French, Chinese, and Hindi. Mistral NeMo uses a new tokenizer, Tekken, based on Tiktoken, that was trained on over more than 100 languages, and compresses natural language text and source code more efficiently than the SentencePiece tokenizer used in previous Mistral models. Compared to the Llama 3 tokenizer, Tekken proved to be more proficient in compressing text for approximately 85% of all languages. Mistral NeMO underwent an advanced fine-tuning and alignment phase. Compared to Mistral 7B, it is much better at following precise instructions, reasoning, handling multi-turn conversations, and generating code. <br><br>

3. ***OpenAI Paper on LLM Legibility: <br>
OpenAI published a paper on improving LLM output legibility using Prover-Verifier Games. This method enhances the clarity and checkability of solutions in math problems by training verifiers and provers. Results show increased human accuracy in verifying solutions, suggesting this method could align LLMs more closely with human-checkable outputs.*** <br>
   Jul 18, OpenAI published a [paper](https://arxiv.org/abs/2407.13692) “Prover-Verifier Games improve legibility of LLM outputs”. One way to increase confidence in the outputs of Large Language Models (LLMs) is to support them with reasoning that is clear and easy to check -- a property call legibility. The paper studies legibility in the context of solving grade-school math problems and show that optimizing chain-of-thought solutions only for answer correctness can make them less legible. To mitigate the loss in legibility, the authors propose a training algorithm inspired by Prover-Verifier Game from Anil et al. (2021). The algorithm iteratively trains small verifiers to predict solution correctness, "helpful" provers to produce correct solutions that the verifier accepts, and "sneaky" provers to produce incorrect solutions that fool the verifier. The study finds that the helpful prover's accuracy and the verifier's robustness to adversarial attacks increase over the course of training. Furthermore, the paper shows that legibility training transfers to time-constrained humans tasked with verifying solution correctness. Over course of LLM training human accuracy increases when checking the helpful prover's solutions, and decreases when checking the sneaky prover's solutions. Hence, training for checkability by small verifiers is a plausible technique for increasing output legibility. The results suggest legibility training against small verifiers as a practical avenue for increasing legibility of large LLMs to humans, and thus could help with alignment of superhuman models. <br><br>

5. ***MetaSumPerceiver for Fact-Checking: <br>
Virginia Tech introduced MetaSumPerceiver, a model designed for fact-checking claims by summarizing multimodal, multi-document datasets. Using a dynamic perceiver-based model and reinforcement learning, it outperforms existing approaches on the MOCHEG dataset and demonstrates strong performance on a new fact-checking dataset.*** <br>
   Jul 18, Virginia Tech published a [paper](https://arxiv.org/pdf/2407.13089) on ACM 2024 “MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for Fact-Checking”. Fact-checking real-world claims often requires reviewing multiple multimodal documents to assess a claim's truthfulness, which is a highly laborious and time-consuming task. This paper presents a summarization model designed to generate claim-specific summaries useful for fact-checking from multimodal, multi-document datasets. The model takes inputs in the form of documents, images, and a claim, with the objective of assisting in fact-checking tasks. The paper introduces a dynamic perceiver-based model that can handle inputs from multiple modalities of arbitrary lengths. To train the model, the authors leverage a novel reinforcement learning-based entailment objective to generate summaries that provide evidence distinguishing between different truthfulness labels. To assess the efficacy of the approach, the authors conduct experiments on both an existing benchmark and a new dataset of multi-document claims. The proposed approach outperforms the SOTA approach by 4.6% in the claim verification task on the MOCHEG dataset and demonstrates strong performance on the new Multi-News-Fact-Checking dataset. <br><br>

7. ***OpenAI's GPT-4o Mini: <br>
OpenAI released GPT-4o mini, a cost-effective model scoring 82% on the MMLU. Priced significantly lower than previous models, it supports a wide range of tasks and has a large context window. The model is designed for efficient and low-latency applications, with future support for multiple input and output types.*** <br>
   Jul 18, OpenAI [released GPT-4o mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/?utm_source=substack&utm_medium=email), scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo. GPT-4o mini enables a broad range of tasks with its low cost and latency, such as applications that chain or parallelize multiple model calls (e.g., calling multiple APIs), pass a large volume of context to the model (e.g., full code base or conversation history), or interact with customers through fast, real-time text responses (e.g., customer support chatbots).  GPT-4o mini supports text and vision in the API, with support for text, image, video and audio inputs and outputs coming in the future. The model has a context window of 128K tokens, supports up to 16K output tokens per request, and has knowledge up to October 2023. <br><br>

9. ***IBM and MIT on Benchmark Agreement Testing: <br>
IBM and MIT highlighted issues in Benchmark Agreement Testing (BAT) for LLMs and proposed best practices to ensure robust and valid evaluations. They introduced BenchBench, a tool for BAT, and demonstrated the importance of standardized procedures in improving the reliability of benchmark evaluations.*** <br>
    Jul 18, IBM and MIT published a [paper](https://arxiv.org/pdf/2407.13696) “Benchmark Agreement Testing Done Right: A Guide for LLM Benchmark Evaluation”. Recent advancements in Language Models (LMs) have catalyzed the creation of multiple benchmarks, designed to assess these models' general capabilities. A crucial task, however, is assessing the validity of the benchmarks themselves. This is most commonly done via Benchmark Agreement Testing (BAT), where new benchmarks are validated against established ones using some agreement metric (e.g., rank correlation). Despite the crucial role of BAT for benchmark builders and consumers, there are no standardized procedures for such agreement testing. This deficiency can lead to invalid conclusions, fostering mistrust in benchmarks and upending the ability to properly choose the appropriate benchmark to use. By analyzing over 40 prominent benchmarks, the authors demonstrate how some overlooked methodological choices can significantly influence BAT results, potentially undermining the validity of conclusions. To address these inconsistencies, the study proposes a set of best practices for BAT and demonstrate how utilizing these methodologies greatly improves BAT robustness and validity. To foster adoption and facilitate future research, the paper introduces BenchBench, a python package for BAT, and release the BenchBench-leaderboard, a meta-benchmark designed to evaluate benchmarks using their peers. Findings of the paper underscore the necessity for standardized BAT, ensuring the robustness and validity of benchmark evaluations in the evolving landscape of language model research. BenchBench Package: https://github.com/IBM/BenchBench Leaderboard: https://huggingface.co/spaces/per/BenchBench <br><br>

11. ***Open-Source LLMs in Biomedical Tasks: <br>
A study explored the performance of open-source LLMs like Mixtral 8x7B compared to commercial models in biomedical tasks. While competitive in few-shot settings, open-source models struggled in zero-shot scenarios. The research suggests that domain-specific few-shot examples can close the performance gap between commercial and open-source models.*** <br>
    Jul 18, Uni of Regensburg published a [paper](https://arxiv.org/pdf/2407.13511) “Can Open-Source LLMs Compete with Commercial Models? Exploring the Few-Shot Performance of Current GPT Models in Biomedical Tasks”. Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPT and Anthropic's Claude 3 Opus, have dominated natural language processing (NLP) benchmarks across different domains. New competing Open-Source alternatives like Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap while often offering higher throughput and being less costly to use. Open-Source LLMs can also be self-hosted, which makes them interesting for enterprise and clinical use cases where sensitive data should not be processed by third parties. The authors participated in the 12th BioASQ challenge, which is a retrieval augmented generation (RAG) setting, and explored the performance of current GPT models Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning (zero-shot, few-shot) and QLoRa fine-tuning. The study also explored how additional relevant knowledge from Wikipedia added to the context-window of the LLM might improve their performance. Mixtral 8x7b was competitive in the 10-shot setting, both with and without fine-tuning, but failed to produce usable results in the zero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead to measurable performance gains. The results indicate that the performance gap between commercial and open-source models in RAG setups exists mainly in the zero-shot setting and can be closed by simply collecting few-shot examples for domain-specific use cases. The code needed to rerun these experiments is available through [GitHub](https://github.com/SamyAteia/bioasq2024).  <br><br>

13. ***AI and Universal Basic Income: <br>
An article in Forbes discussed the impact of AI and robotics on job displacement, suggesting a shift towards human-AI collaboration. It proposes that universal basic income may become necessary as AI makes many traditional jobs obsolete, emphasizing the enduring need for human skills in the workforce.*** <br>
    Jul 17, Forbes published an [article](https://www.forbes.com/sites/jackkelly/2024/07/17/ai-robot-job-displacement-universal-basic-income/) “How AI And Robot Job Displacements Could Lead Us Down The Road Of Universal Basic Income And Loss Of Identity”. The article argues that in today’s evolving workplace landscape, AI and robotics are already automating tasks across almost all sectors, including manufacturing, data analysis, customer service and administration. As it stands, repetitive and routine tasks are the most susceptible to automation. While AI and robotics will undoubtedly change the nature of work, it's unlikely that these technologies will eradicate the existence of all jobs. The focus will likely shift toward human-AI collaboration and jobs requiring uniquely human skills. The future of work could involve a combination of paid employment, universal basic income and a renewed focus on finding meaning and fulfillment outside of traditional work structures. Many jobs require creativity, critical thinking, social skills, problem-solving under pressure and the ability to handle unforeseen situations. Because these are areas where AI is still limited, it demonstrates the need for continued human skills in job functions. Elon Musk stated this May in Pairs that AI will eventually make workers obsolete—a prediction he doesn’t necessarily see as pernicious. Highly advanced AI capabilities will dispel the need for human labor, rendering traditional jobs unnecessary, in what he frames as a likely "benign scenario" for the future of work. Musk sees people working simply out of personal interest or creative satisfaction. For work to become optional, we would need to live in an “age of abundance” achieved by "universal high income," Musk said. Geoffrey Hinton told BBC that universal basic income would need to be provided by the government to provide a safety net, if automation catalyzes widespread job displacement. <br><br>

15. ***Manipulating LLM Uncertainty: <br>
Research from Rutgers, NYU, and Meta investigated the fragility of uncertainty estimation in LLMs. They demonstrated that backdoor attacks could manipulate model uncertainty without changing the output, highlighting a significant threat to LLM reliability and the need for defenses against such vulnerabilities.*** <br>
    Jul 17, Rutgers Uni, NYU, Meta, et al. published a [paper](https://arxiv.org/pdf/2407.11282) “Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models”. Large Language Models (LLMs) are employed across various high-stakes domains, where the reliability of their outputs is crucial. One commonly used method to assess the reliability of LLMs' responses is uncertainty estimation, which gauges the likelihood of their answers being correct. While many studies focus on improving the accuracy of uncertainty estimations for LLMs, this research investigates the fragility of uncertainty estimation and explores potential attacks. The authors demonstrate that an attacker can embed a backdoor in LLMs, which, when activated by a specific trigger in the input, manipulates the model's uncertainty without affecting the final output. Specifically, the proposed backdoor attack method can alter an LLM's output probability distribution, causing the probability distribution to converge towards an attacker-predefined distribution while ensuring that the top-1 prediction remains unchanged. Experimental results demonstrate that this attack effectively undermines the model's self-evaluation reliability in multiple-choice questions. For instance, the study achieved a 100 attack success rate (ASR) across three different triggering strategies in four models. Further, the authors investigate whether this manipulation generalizes across different prompts and domains. This work highlights a significant threat to the reliability of LLMs and underscores the need for future defenses against such attacks. The code is available at https://github.com/qcznlp/uncertainty_attack. <br><br>

17. ***Goldfish for Long Video Understanding: <br>
KAUST, Harvard, and Swiss AI Lab introduced Goldfish, a methodology for understanding long videos. It uses efficient retrieval mechanisms and a new benchmark, TVQA-long, to improve comprehension of lengthy video content. Goldfish outperforms previous methods in both long and short video understanding.*** <br>
    Jul 17, KAUST, Harvard Uni and The Swiss AI Lab published a [paper](https://arxiv.org/pdf/2407.12679) “Goldfish: Vision-Language Understanding of Arbitrarily Long Videos”. Most current LLM-based models for video understanding can process videos within minutes. However, they struggle with lengthy videos due to challenges such as "noise and redundancy", as well as "memory and computation" constraints. This paper presents Goldfish, a methodology tailored for comprehending videos of arbitrary lengths. The work also introduces the TVQA-long benchmark, specifically designed to evaluate models' capabilities in understanding long videos with questions in both vision and text content. Goldfish approaches these challenges with an efficient retrieval mechanism that initially gathers the top-k video clips relevant to the instruction before proceeding to provide the desired response. This design of the retrieval mechanism enables the Goldfish to efficiently process arbitrarily long video sequences, facilitating its application in contexts such as movies or television series. To facilitate the retrieval process, the study developed MiniGPT4-Video that generates detailed descriptions for the video clips. In addressing the scarcity of benchmarks for long video evaluation, the paper adapted the TVQA short video benchmark for extended content analysis by aggregating questions from entire episodes, thereby shifting the evaluation from partial to full episode comprehension. The study attained a 41.78% accuracy rate on the TVQA-long benchmark, surpassing previous methods by 14.94%. The MiniGPT4-Video also shows exceptional performance in short video comprehension, exceeding existing state-of-the-art methods by 3.23%, 2.03%, 16.5% and 23.59% on the MSVD, MSRVTT, TGIF, and TVQA short video benchmarks, respectively. These results indicate that these models have significant improvements in both long and short-video understanding. The models and code have been made publicly available at https://vision-cair.github.io/Goldfish_website/ <br><br>

19. ***Foundation Model Transparency Index v1.1: <br>
A follow-up study from Stanford, Princeton, and MIT assessed the transparency of foundation model developers. The updated index shows improved transparency, with developers now scoring 58 out of 100 on average. The study highlights areas of ongoing opacity and suggests that increased transparency can be achieved through policy interventions.*** <br>
    Jul 17, Stanford Uni, Princeton Uni and MIT published a [paper](https://arxiv.org/pdf/2407.12929) “The Foundation Model Transparency Index v1.1: May 2024”. Foundation models are increasingly consequential yet extremely opaque. To characterize the status quo, the Foundation Model Transparency Index was launched in October 2023 to measure the transparency of leading foundation model developers. The October 2023 Index (v1.0) assessed 10 major foundation model developers (e.g. OpenAI, Google) on 100 transparency indicators (e.g. does the developer disclose the wages it pays for data labor?). At the time, developers publicly disclosed very limited information with the average score being 37 out of 100. To understand how the status quo has changed, the researchers conduct a follow-up study (v1.1) after 6 months: the report scores 14 developers against the same 100 indicators. While in v1.0 the authors searched for publicly available information, in v1.1 developers submit reports on the 100 transparency indicators, potentially including information that was not previously public. The report finds that developers now score 58 out of 100 on average, a 21 point improvement over v1.0. Much of this increase is driven by developers disclosing information during the v1.1 process: on average, developers disclosed information related to 16.6 indicators that was not previously public. The authors observe regions of sustained (i.e. across v1.0 and v1.1) and systemic (i.e. across most or all developers) opacity such as on copyright status, data access, data labor, and downstream impact. The authors publish transparency reports for each developer that consolidate information disclosures: these reports are based on the information disclosed to the authors via developers. The findings demonstrate that transparency can be improved in this nascent ecosystem, the Foundation Model Transparency Index likely contributes to these improvements, and policymakers should consider interventions in areas where transparency has not improved. <br><br>

21. ***AI revolutionises business document management: <br>
Professionals spend a significant amount of time searching for information, highlighting the need for better tools. Adobe’s AI Assistant, a generative AI conversational engine, integrates into document workflows to boost productivity. It helps generate high-quality insights and quickly create emails, reports, and presentations from various document types. In the legal sector, it summarizes judgments to save time for legal counsel. Wealth advisors also benefit from this technology by consolidating information for client advice. AI integration into workflows is becoming essential for businesses.*** <br>
    Jul 16, Financial Review published an [article](https://www.afr.com/technology/ai-revolutionises-business-document-management-20240711-p5jst3) “AI revolutionises business document management”. On average, professionals spend about 8.2 hours a week just searching for information within their documents. This inefficiency underscores the need for smarter, more effective tools to handle the deluge of data and documents that businesses process daily. Deeply integrated into document workflows, Adobe’s AI Assistant is a generative AI-powered conversational engine that can be deployed in minutes, unlocking new levels of document productivity for every knowledge worker across the enterprise. With AI Assistant, users can gain high-quality insights with intelligent citations and quickly generate emails, reports, presentations, and more from the information in their PDFs and other types of documents, including Word, PowerPoint, and meeting transcripts. One practical application of this technology is seen in the legal sector. The AI Assistant helps by summarising four or five judgments, enabling the legal counsel to quickly grasp the essentials and make informed decisions faster, saving them 30 minutes a day, and 2.5 hours a week. The AI Assistant also caters to wealth advisors who need to consolidate information from various reports to provide comprehensive advice to clients. The AI model is integrating AI capabilities into existing workflows, make AI a necessity, not a luxury. <br><br>

23. ***High Performance RWKV/Transformer and Extreme KV-Cache Compression:  <br>
The paper introduces GoldFinch, a hybrid model combining Linear Attention and Transformer techniques to efficiently generate a compressed KV-Cache. GoldFinch enhances performance relative to previous models and saves cache size significantly. Despite the complexity of autoregressive generation, the pre-fill computation remains efficient due to the use of RNNs. The trained weights and codes are released for community use.*** <br>
    Jul 16, EleutherAI et al. published a [paper](https://arxiv.org/pdf/2407.12077) “GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression”. The paper introduces GoldFinch, a hybrid Linear Attention/Transformer sequence model that uses a new technique to efficiently generate a highly compressed and reusable KV-Cache in linear time and space with respect to sequence length. GoldFinch stacks a new GOLD transformer on top of an enhanced version of the Finch (RWKV-6) architecture. The authors train up to 1.5B parameter class models of the Finch, Llama, and GoldFinch architectures, and find dramatically improved modeling performance relative to both Finch and Llama. The cache size savings increase linearly with model layer count, ranging from 756-2550 times smaller than the traditional transformer cache for common sizes, enabling inference of extremely large context lengths even on limited hardware. Although autoregressive generation has O(n) time complexity per token because of attention, pre-fill computation of the entire initial cache state for a submitted context costs only O(1) time per token due to the use of a recurrent neural network (RNN) to generate this cache. Trained weights and training [codes](https://github.com/recursal/GoldFinch-paper) are released under the Apache 2.0 license for community use. <br><br>

25. ***A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval: <br>
Existing benchmarks focus on simple queries, but BRIGHT addresses the need for benchmarks requiring deep reasoning. BRIGHT includes 1,398 real-world queries across various domains and shows that current retrieval models perform poorly on it. The use of Chain-of-Thought reasoning improves performance. BRIGHT is robust against data leakage and encourages research on more challenging retrieval tasks. Code and data are publicly available.*** <br>
    Jul 16, The Uni of Hong Kong, Princeton Uni, Uni of Washington and Google published a [paper](https://arxiv.org/pdf/2407.12883) “BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval”. Existing retrieval benchmarks primarily consist of information-seeking queries (e.g., aggregated questions from search engines) where keyword or semantic-based retrieval is usually sufficient. However, many complex real-world queries require in-depth reasoning to identify relevant documents that go beyond surface form matching. For example, finding documentation for a coding question requires understanding the logic and syntax of the functions involved. To better benchmark retrieval on such challenging queries, the paper introduces BRIGHT, the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398 real-world queries collected from diverse domains (such as economics, psychology, robotics, software engineering, earth sciences, etc.), sourced from naturally occurring or carefully curated human data. Extensive evaluation reveals that even state-of-the-art retrieval models perform poorly on BRIGHT. The leading model on the MTEB leaderboard, which achieves a score of 59.0 nDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. The authors further demonstrate that augmenting queries with Chain-of-Thought reasoning generated by large language models (LLMs) improves performance by up to 12.2 points. Moreover, BRIGHT is robust against data leakage during pretraining of the benchmarked models as the authors validate by showing similar performance even when documents from the benchmark are included in the training data. BRIGHT paves the way for future research on retrieval systems in more realistic and challenging settings. Code and data are available at https://brightbenchmark.github.io. <br><br>

27. ***All Large Language Models can be Fully Sparsely-Activated: <br>
Q-Sparse enables efficient training of sparsely-activated large language models (LLMs) through top-K sparsification and the straight-through-estimator. It achieves comparable results to baseline LLMs with significant efficiency gains in inference. Q-Sparse is versatile across different training settings and is effective for both full-precision and 1-bit LLMs, promising to revolutionize future LLM efficiency.*** <br>
    Jul 15, Microsoft published a [paper](https://arxiv.org/pdf/2407.10969) “Q-Sparse: All Large Language Models can be Fully Sparsely-Activated”.  The paper introduces, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) the paper presents an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different settings, including training-from-scratch, continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the cornerstone and a clear path to revolutionize the efficiency, including cost and energy consumption, of future LLMs. <br><br>

29. ***Taming Large Language Models for Better Automatic Evaluation: <br>
FLAMe, a family of autorater models, enhances the evaluation of LLMs by training on a diverse set of quality assessment tasks. FLAMe outperforms proprietary models like GPT-4 on many tasks and serves as a robust starting point for fine-tuning. It is also more efficient and less biased than other models, excelling in identifying high-quality responses for various tasks.*** <br>
    Jul 15, Google published a [paper](https://arxiv.org/pdf/2407.10817) “Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation”. As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their output due to the high costs of human evaluation. To make progress towards better LLM autoraters, the paper introduces FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on Google’s large and diverse collection of 100+ quality assessment tasks comprising 5M+ human judgments, curated and standardized using publicly released human evaluations from previous research. FLAMe significantly improves generalization to a wide variety of held-out tasks, outperforming LLMs trained on proprietary data like GPT-4 and Claude-3 on many tasks. The study shows that FLAMe can also serve as a powerful starting point for further downstream fine-tuning, using reward modeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, the FLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative model trained exclusively on permissively licensed data, outperforming both GPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, the study explores a more computationally efficient approach using a novel tail-patch fine-tuning strategy to optimize the FLAMe multitask mixture for reward modeling evaluation (FLAMe-Opt-RM), offering competitive RewardBench performance while requiring approximately 25x less training datapoints. Overall, the FLAMe variants outperform all popular proprietary LLM-as-a-Judge models considered across 8 out of 12 autorater evaluation benchmarks, encompassing 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, analysis reveals that FLAMe is significantly less biased than these LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark, while effectively identifying high-quality responses for code generation.  <br><br>

31. ***How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients: <br>
The paper studies low-rank structures in LLMs and introduces WeLore for weight compression and memory-efficient fine-tuning. WeLore leverages gradient dynamics to identify suitable rank reduction ratios, enabling significant performance and efficiency improvements. The technique outperforms full finetuning while reducing memory and computational requirements, making it a powerful tool for optimizing LLMs.*** <br>
    Jul 15, Uni of Texas at Austin Uni of Oxford, Meta, et al published a [paper](https://arxiv.org/pdf/2407.11239) “From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients”. Modern Large Language Models (LLMs) are composed of matrices with billions of elements, making their storage and processing quite demanding in terms of computational resources and memory usage. Being significantly large, such matrices can often be expressed in low-rank format with potential to relax resource requirements. Unlike prior works which focus on developing novel matrix decomposition algorithms, this work first studies the emergence of low-rank structures across matrices within different layers of LLMs and establish a consequential relationship between the gradient dynamics and emerging low-rank expressiveness of matrices. Findings reveal that different layers exhibit varying levels of converged low-rank structure, necessitating a non-uniform rank reduction across them to minimize performance drop due to compression. In view of that, the paper presents Weight Low-Rank Projection (WeLore) that unifies weight compression and memory-efficient fine-tuning as ONE, in a data-agnostic and one-shot way. WeLore capitalizes the heavy-tail distribution of singular values to identify a suitable rank reduction ratio for matrices within LLMs. Going beyond only as a compression technique, WeLore categorizes weight matrices into Low-rank Components (LRCs) and Non-Low-rank Components (N-LRCs) based on their ability to express themselves as low-rank. The gradient perspective and extensive experiments illustrate that LRCs tend to have better finetuning capabilities and can closely mimic (sometimes outperform) the training loss trajectory and performance of full-finetuning with notable memory and compute footprint reduction. For example, finetuning a 50% compressed LLaMa-2 7B model using only a fraction of parameters in LRCs (WeLore) can outperform its full finetuning with ~3x better throughput and ~0.6x GPU requirement. The codes are available at https://github.com/VITA-Group/welore <br><br>

33. ***Practical Unlearning for Large Language Models: <br>
Addressing security issues in LLMs, the O3 framework provides a solution for machine unlearning without compromising model utility. It includes an OOD detector and an Orthogonal LoRA for continuous unlearning. O3 smartly decides on unlearning actions during inference and shows superior performance across various tasks and datasets, especially with continuous unlearning requests.*** <br>
    Jul 14, Northwestern Uni and Arizona State Uni published a [paper](https://arxiv.org/pdf/2407.10223) “Practical Unlearning for Large Language Models”. While LLMs have demonstrated impressive performance across various domains and tasks, their security issues have become increasingly severe. Machine unlearning (MU) has emerged as a promising solution to address these issues by removing the influence of undesired data on the target model without compromising its utility in other aspects. MU typically assumes full access to the original training data to preserve utility, which is difficult to achieve in LLM unlearning. Existing LLM unlearning methods often assume access to data most affected by undesired data unlearning. However, this assumption underestimates the entanglement among various LLM capabilities and ignores data access limitations due to various issues. Moreover, these LLM unlearning methods do not sufficiently consider that unlearning requests in real-world scenarios are continuously emerging. To overcome these challenges and achieve practical LLM unlearning, the authors propose the O3 framework. The O3 framework includes an Out-Of-Distribution (OOD) detector to measure the similarity between input and unlearning data, and an Orthogonal low-rank adapter (LoRA) for continuously unlearning requested data. The OOD detector is trained with a novel contrastive entropy loss and utilizes a local-global layer-aggregated scoring mechanism. The orthogonal LoRA achieves parameter disentanglement among continual unlearning requests. During inference, the O3 framework can smartly decide whether and to what extent to load the unlearning LoRA based on the OOD detector's predictions. Notably, O3's effectiveness does not rely on any retained data. The authors conducted extensive experiments on O3 and state-of-the-art LLM unlearning methods across three tasks and seven datasets. The results indicate that O3 consistently achieves the best trade-off between unlearning effectiveness and utility preservation, especially when facing continuous unlearning requests. <br><br>

35. ***Vocabulary used by ChatGPT in academic writing:  <br>
The paper examines LLM usage in academic writing, identifying a significant impact on scientific literature. By analyzing vocabulary changes in PubMed abstracts, the study estimates that at least 10% of 2024 abstracts involved LLMs. This impact varies across disciplines and regions, with notable increases in certain fields. The study highlights the widespread adoption of LLMs in academia.*** <br>
    Jul 3, researchers from Uni of Tubingen and Northwestern Uni published a [paper](https://arxiv.org/pdf/2406.07016) “Delving into ChatGPT usage in academic writing through excess vocabulary”. Recent large language models (LLMs) can generate and revise text with human-level performance, and have been widely commercialized in systems like ChatGPT. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists have been using them to assist their scholarly writing. How wide-spread is LLM usage in the academic literature currently? To answer this question, the work uses an unbiased, large-scale approach, free from any assumptions on academic LLM usage. The authors study vocabulary changes in 14 million PubMed abstracts from 2010-2024, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. The analysis based on excess words usage suggests that at least 10% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, and was as high as 30% for some PubMed sub-corpora. The paper shows that the appearance of LLM-based writing assistants has had an unprecedented impact in the scientific literature, surpassing the effect of major world events such as the Covid pandemic. The paper also shows the six most widely used words by ChatGPT: delves, crucial, potential, these, significant, and important. <br><br>

37. ***Self-replicating Programs Emerge from Simple Interaction: <br>
The study explores how self-replicating programs arise from simple computational interactions. It demonstrates that self-replicators emerge in environments without explicit fitness landscapes due to random interactions and self-modification. The findings contribute to understanding the dynamics of self-replication and complex behavior emergence in computational substrates.*** <br>
    Jun 27, Google and Uni of Chicago published a [paper](https://arxiv.org/pdf/2406.19108) “Computational Life: How Well-formed, Self-replicating Programs Emerge from Simple Interaction”. The fields of Origin of Life and Artificial Life both question what life is and how it emerges from a distinct set of "pre-life" dynamics. One common feature of most substrates where life emerges is a marked shift in dynamics when self-replication appears. While there are some hypotheses regarding how self-replicators arose in nature, people know very little about the general dynamics, computational principles, and necessary conditions for self-replicators to emerge. This is especially true on "computational substrates" where interactions involve logical, mathematical, or programming rules. This paper takes a step towards understanding how self-replicators arise by studying several computational substrates based on various simple programming languages and machine instruction sets. The authors show that when random, non self-replicating programs are placed in an environment lacking any explicit fitness landscape, self-replicators tend to arise. The study demonstrates how this occurs due to random interactions and self-modification, and can happen with and without background random mutations. The paper also shows how increasingly complex dynamics continue to emerge following the rise of self-replicators. Finally, the authors show a counterexample of a minimalistic programming language where self-replicators are possible, but so far have not been observed to arise.
 <br><br>

***14 Jul 2024***

1. ***OpenAI's Novel Approach with "Strawberry"<br>
OpenAI is developing "Strawberry," a novel post-training method to enhance AI reasoning, enabling complex long-horizon tasks and deep internet research. The project aims to address common sense issues in current AI models and improve their autonomous research capabilities.*** <br><br>
   Jul 13, according to [Reuters](https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/), ChatGPT maker OpenAI is working on a novel approach to its artificial intelligence models in a project code-named “Strawberry,” according to a person familiar with the matter and internal documentation reviewed by Reuters. The project, details of which have not been previously reported, comes as the Microsoft-backed startup races to show that the types of models it offers are capable of delivering advanced reasoning capabilities. There is a project that uses Strawberry models with the aim of enabling the company’s AI to not just generate answers to queries but to plan ahead enough to navigate the internet autonomously and reliably to perform what OpenAI terms “deep research,” according to the source. Two sources described viewing earlier this year what OpenAI staffers told them were Q* demos, capable of answering tricky science and math questions out of reach of today’s commercially-available models. OpenAI hopes the innovation will improve its AI models’ reasoning capabilities dramatically, the person familiar with it said, adding that Strawberry involves a specialized way of processing an AI model after it has been pre-trained on very large datasets. While large language models can already summarize dense texts and compose elegant prose far more quickly than any human, the technology often falls short on common sense problems whose solutions seem intuitive to people, like recognizing logical fallacies and playing tic-tac-toe. When the model encounters these kinds of problems, it often “hallucinates” bogus information. Strawberry includes a specialized way of what is known as “post-training” OpenAI’s generative AI models, or adapting the base models to hone their performance in specific ways after they have already been “trained” on reams of generalized data, one of the sources said. Among the capabilities OpenAI is aiming Strawberry at is performing long-horizon tasks (LHT), the document says, referring to complex tasks that require a model to plan ahead and perform a series of actions over an extended period of time, the first source explained.
 <br><br>
3. ***Progress Towards Artificial General Intelligence (AGI)<br>
OpenAI tracks its progress towards AGI using an internal scale from Level 1 to Level 5. Current models are at Level 1, with Level 2 near completion, aiming for human PhD-level problem-solving. Achieving AGI will require massive computing power and time, with predictions varying widely on when it will be achieved.*** <br><br>
   Jul 12, according to [theverge.com](https://www.theverge.com/2024/7/11/24196746/heres-how-openai-will-determine-how-powerful-its-ai-systems-are?utm_source=substack&utm_medium=email), OpenAI has created an internal scale to track the progress its large language models are making toward artificial general intelligence, or AI with human-like intelligence. Today’s chatbots, like ChatGPT, are at Level 1. OpenAI claims it is nearing Level 2, defined as a system that can solve basic problems at the level of a person with a PhD. Level 3 refers to AI agents capable of taking actions on a user’s behalf. Level 4 involves AI that can create new innovations. Level 5, the final step to achieving AGI, is AI that can perform the work of entire organizations of people. OpenAI has previously defined AGI as “a highly autonomous system surpassing humans in most economically valuable tasks.” OpenAI’s unique structure is centered around its mission of achieving AGI, and how OpenAI defines AGI is important. Still, AGI is still quite a ways away: it will take billions upon billions of dollars worth of computing power to reach AGI, if at all. Timelines from experts, and even at OpenAI, vary wildly. In October 2023, OpenAI CEO Sam Altman said we are “five years, give or take,” before reaching AGI. OpenAI hasn’t provided details on how it assigns models to these internal. However, company leaders demonstrated a research project using the GPT-4 AI model during an all-hands meeting on Thursday and believe this project showcases some new skills that exhibit human-like reasoning. 

5. ***Advancements in Attention Mechanisms<br>
A paper by Colfax Research and collaborators introduces "FlashAttention-3," improving attention mechanisms in Transformer models. By leveraging new GPU capabilities and optimizing computation, the method achieves significant speedup and accuracy, making it more efficient for large language models.*** <br><br>
   Jul 11, Colfax Research, Meta, Nvidia, Georgia Tech, Princeton Uni, and TogetherAI published a [paper](https://arxiv.org/pdf/2407.08608v1) “FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision”. Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU. The work develops three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. The paper demonstrates that FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0× with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. The paper validates that FP8 FlashAttention-3 achieves 2.6× lower numerical error than a baseline FP8 attention.

7. ***Concerns Over Safety and Profit at OpenAI<br>
Former OpenAI employee William Saunders criticizes the company for prioritizing profit over safety. His resignation highlights concerns about the potential risks of developing AGI without adequate safety measures, drawing parallels to the Titanic's insufficient lifeboats.*** <br><br>
   Jul 11, according to [furturism.com](https://futurism.com/openai-researcher-quit-realized-upsetting-truth), A former OpenAI worker says he quit the company after realizing that it was putting safety on the back burner to pursue profit. "I really didn't want to end up working for the Titanic of AI, and so that's why I resigned," former Superalignment team member William Saunders said. "During my three years at OpenAI, I would sometimes ask myself a question. Was the path that OpenAI was on more like the Apollo program or more like the Titanic?" Saunders argued that the Titanic may have been called "unsinkable, but at the same time there weren't enough lifeboats for everyone and so when disaster struck, a lot of people died." His comments highlight growing concerns over companies like OpenAI developing AI systems that are capable of superseding the abilities of humans, an idea dubbed artificial general intelligence (AG) — something that currently remains entirely theoretical, but is a source of great interest to executives like OpenAI's Sam Altman. "Even when big problems happened, like Apollo 13, they had enough sort of like redundancy, and were able to adapt to the situation in order to bring everyone back safely," "It is not possible to develop AGI or any new technology with zero risk," he added in an email to Business Insider after the publication pointed out that the Apollo program saw its own fair share of safety oversights. "What I would like to see is the company taking all possible reasonable steps to prevent these risks." Saunders' comments paint a worrying picture of how OpenAI is being run and the changes Altman has made over the last couple of years.

9. ***Efficient Training with Q-GaLore<br>
Researchers introduced "Q-GaLore," a method combining quantization and low-rank projection to reduce memory usage in training large language models. This approach significantly cuts memory consumption while maintaining performance, enabling efficient training on less powerful hardware.*** <br><br>
    Jul 11, Uni of Texas at Austin, Uni of Oxford, Meat et al. published a [paper](https://arxiv.org/pdf/2407.08296) “Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients”. Training Large Language Models (LLMs) is memory-intensive due to the large number of parameters and associated optimization states. GaLore, a recent method, reduces memory usage by projecting weight gradients into a low-rank subspace without compromising performance. However, GaLore relies on time-consuming Singular Value Decomposition (SVD) operations to identify the subspace, and the frequent subspace updates lead to significant training time overhead. Moreover, GaLore offers minimal improvements in accuracy and efficiency compared to LoRA in more accessible fine-tuning scenarios. To address these limitations, the paper introduces Q-Galore, a novel approach that substantially reduces memory usage by combining quantization and low-rank projection, surpassing the benefits of GaLore. The proposed method is based on two key observations: (i) the gradient subspace exhibits diverse properties, with some layers converging early in training while others are subject to frequent changes; (ii) the projection matrices are highly resilient to low-bit quantization. Leveraging these insights, Q-GaLore adaptively updates the gradient subspace based on its convergence statistics, achieving comparable performance while significantly reducing the number of SVD operations. The study maintains the projection matrices in INT4 format and weights in INT8 format, incorporating stochastic rounding to capture accumulated gradient information. This approach enables a high-precision training trajectory using only low-precision weights. The study demonstrates that Q-GaLore achieves highly competitive performance with exceptional memory efficiency. At pre-training, Q-GaLore facilitates training a LLaMA-7B model from scratch on a single NVIDIA RTX 4060 Ti with only 16 GB memory. At fine-tuning, it reduces memory consumption by up to 50% compared to LoRA and GaLore, while consistently outperforming QLoRA at the same memory cost.

11. ***Teaching Transformers Causal Reasoning<br>
Microsoft and MIT propose an axiomatic training method to teach transformers causal reasoning from passive data. The study demonstrates that models can generalize causal reasoning to new scenarios, matching or surpassing the performance of larger models.*** <br><br>
    Jun 10, Microsoft and MIT published a [paper](https://arxiv.org/pdf/2407.07612) “Teaching Transformers Causal Reasoning through Axiomatic Training”. For text-based AI systems to interact in the real world, causal reasoning is an essential skill. Since interventional data is costly to generate, the word studies to what extent an agent can learn causal reasoning from passive data. Specifically, the paper considers an axiomatic training setup where an agent learns from multiple demonstrations of a causal axiom (or rule), rather than incorporating the axiom as an inductive bias or inferring it from data values. A key question is whether the agent would learn to generalize from the axiom demonstrations to new scenarios. For example, if a transformer model is trained on demonstrations of the causal transitivity axiom over small graphs, would it generalize to applying the transitivity axiom over large graphs? Experimental results, based on a novel axiomatic training scheme, indicate that such generalization is possible. The paper considers the task of inferring whether a variable causes another variable, given a causal graph structure. The paper finds that a 67 million parameter transformer model, when trained on linear causal chains (along with some noisy variations) can generalize well to new kinds of graphs, including longer causal chains, causal chains with reversed order, and graphs with branching; even when it is not explicitly trained for such settings. The model performs at par (or even better) than many larger language models such as GPT-4, Gemini Pro, and Phi-3. Overall, the axiomatic training framework provides a new paradigm of learning causal reasoning from passive data that can be used to learn arbitrary axioms, as long as sufficient demonstrations can be generated.

6. ***Detecting Contextual Hallucinations<br>
A new method called "Lookback Lens" detects and mitigates hallucinations in large language models by analyzing attention weights. This approach effectively reduces hallucinations across various tasks and models, enhancing the accuracy of generated content.*** <br><br>
   Jul 9, MIT and Uni of Washington published a [paper](https://arxiv.org/pdf/2407.07071) “Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps”. When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such contextual hallucinations. The authors hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, the work proposes a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). The study finds that a linear classifier based on these lookback ratio features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector -- Lookback Lens -- is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. The authors further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.

8. ***Framework for Composable Interventions<br>
A paper introduces a framework for combining interventions in language models to improve factual accuracy and mitigate harmful outputs. The study reveals the interactions between different interventions, emphasizing the need for new multi-objective methods.*** <br><br>
   Jul 9, Uni of Virginia, EleutherAI, Microsoft Harvard Uni, Uni of Oxford et al. published a [paper](https://arxiv.org/pdf/2407.06483) “Composable Interventions for Language Models”. Test-time interventions for language models can enhance factual accuracy, mitigate harmful outputs, and improve model efficiency without costly retraining. But despite a flood of new methods, different types of interventions are largely developing independently. In practice, multiple interventions must be applied sequentially to the same model, yet we lack standardized ways to study how interventions interact. The paper fills this gap by introducing composable interventions, a framework to study the effects of using multiple interventions on the same language models, featuring new metrics and a unified codebase. Using the framework, the paper conducts extensive experiments and composes popular methods from three emerging intervention categories—knowledge editing, model compression, and machine unlearning. Experimental results from 310 different compositions uncover meaningful interactions: compression hinders editing and unlearning, composing interventions hinges on their order of application, and popular general-purpose metrics are inadequate for assessing composability. Taken together, the findings showcase clear gaps in composability, suggesting a need for new multi-objective interventions.

10. ***Lawyers' Perceptions of AI-Generated Documents<br>
A study shows that lawyers prefer documents perceived as human-crafted over AI-generated ones, despite expecting future AI involvement in legal document generation. This perception could influence the adoption of AI in legal processes.*** <br><br>
    Jul 9, Masaryk Uni and CMU published a [paper](https://www.arxiv.org/pdf/2407.06798) “It Cannot Be Right If It Was Written by AI: On Lawyers' Preferences of Documents Perceived as Authored by an LLM vs a Human”. Large Language Models (LLMs) enable a future in which certain types of legal documents may be generated automatically. This has a great potential to streamline legal processes, lower the cost of legal services, and dramatically increase access to justice. While many researchers focus their efforts on proposing and evaluating LLM-based applications supporting tasks in the legal domain, there is a notable lack of investigations into how legal professionals perceive content if they believe it has been generated by an LLM. Yet, this is a critical point as over-reliance or unfounded skepticism may influence whether such documents bring about appropriate legal consequences. This study is the necessary analysis in the context of the ongoing transition towards mature generative AI systems. Specifically, the paper examined whether the perception of legal documents' by lawyers (n=75) varies based on their assumed origin (human-crafted vs AI-generated). The participants evaluated the documents focusing on their correctness and language quality. The analysis revealed a clear preference for documents perceived as crafted by a human over those believed to be generated by AI. At the same time, most of the participants are expecting the future in which documents will be generated automatically. These findings could be leveraged by legal practitioners, policy makers and legislators to implement and adopt legal document generation technology responsibly, and to fuel the necessary discussions into how legal processes should be updated to reflect the recent technological developments.

12. ***Fallback Behaviors of Language Models<br>
Tel Aviv University research categorizes fallback behaviors in language models, showing that advanced models shift from sequence repetitions to hallucinations under uncertainty. The study highlights the need for improved techniques to manage these behaviors.*** <br><br>
    Jul 8, Tel Aviv Uni published a [paper](https://arxiv.org/pdf/2407.06071) “From Loops to Oops: Fallback Behaviors of Language Models Under Uncertainty”. Large language models (LLMs) often exhibit undesirable behaviors, such as hallucinations and sequence repetitions. The paper proposes to view these behaviors as fallbacks that models exhibit under uncertainty, and investigate the connection between them. The work categorizes fallback behaviors -- sequence repetitions, degenerate text, and hallucinations -- and extensively analyzes them in models from the same family that differ by the amount of pretraining tokens, parameter count, or the inclusion of instruction-following training. The experiments reveal a clear and consistent ordering of fallback behaviors, across all these axes: the more advanced an LLM is (i.e., trained on more tokens, has more parameters, or instruction-tuned), its fallback behavior shifts from sequence repetitions, to degenerate text, and then to hallucinations. Moreover, the same ordering is observed throughout a single generation, even for the best-performing models; as uncertainty increases, models shift from generating hallucinations to producing degenerate text and then sequence repetitions. Lastly, the paper demonstrates that while common decoding techniques, such as random sampling, might alleviate some unwanted behaviors like sequence repetitions, they increase harder-to-detect hallucinations.

14. ***LMs and Vision Models Conceptual Alignment<br>
The study by the University of Copenhagen and MBZU evaluates the conceptual alignment between large-scale pretrained language models (LMs) and vision models. Contrary to the claim that LMs lack the ability to connect utterances to the world, the experiments show partial convergence in representations between LMs and vision models. This has significant implications for multi-modal processing and the understanding of LMs.*** <br><br>
    Jul 6, Uni of Copenhagen and MBZU published a [paper](https://arxiv.org/pdf/2302.06555) “Do Vision and Language Models Share Concepts? A Vector Space Alignment Study”. Large-scale pretrained language models (LMs) are said to `lack the ability to connect utterances to the world’ (Bender and Koller, 2020), because they do not have ‘mental models of the world’ (Mitchell and Krakauer, 2023). If so, one would expect LM representations to be unrelated to representations induced by vision models. The paper presents an empirical evaluation across four families of LMs (BERT, GPT-2, OPT and LLaMA-2) and three vision model architectures (ResNet, SegFormer, and MAE). The experiments show that LMs partially converge towards representations isomorphic to those of vision models, subject to dispersion, polysemy and frequency. This has important implications for both multi-modal processing and the LM understanding debate (Mitchell and Krakauer, 2023).

16. ***Google Study on Scalable Oversight<br>
Google's paper explores scalable oversight protocols to enable accurate human supervision of superhuman AI. Comparing debate, consultancy, and direct question-answering, the study finds that debate generally outperforms consultancy and direct QA in specific tasks, particularly those with information asymmetry. Allowing AI agents to choose their stance in debates reduces the chances of judges being convinced by incorrect answers.*** <br><br>
    Jul 5, Google published a [paper](https://arxiv.org/pdf/2407.04622) “On scalable oversight with weak LLMs judging strong LLMs”. Scalable oversight protocols aim to enable humans to accurately supervise superhuman AI. This paper studies debate, where two AI's compete to convince a judge; consultancy, where a single AI tries to convince a judge that asks questions; and compare to a baseline of direct question-answering, where the judge just answers outright without the AI. The authors use large language models (LLMs) as both AI agents and as stand-ins for human judges, taking the judge models to be weaker than agent models. The authors benchmark on a diverse range of asymmetries between judges and agents, extending previous work on a single extractive QA task with information asymmetry, to also include mathematics, coding, logic and multimodal reasoning asymmetries. The paper finds that debate outperforms consultancy across all tasks when the consultant is randomly assigned to argue for the correct/incorrect answer. Comparing debate to direct question answering, the results depend on the type of task: in extractive QA tasks with information asymmetry debate outperforms direct question answering, but in other tasks without information asymmetry the results are mixed. Previous work assigned debaters/consultants an answer to argue for. When the authors allow them to instead choose which answer to argue for, the study finds judges are less frequently convinced by the wrong answer in debate than in consultancy. Further, the paper finds that stronger debater models increase judge accuracy, though more modestly than in previous studies.

18. ***New RNN Layer with Linear Complexity<br>
Stanford, UC San Diego, UC Berkeley, and Meta propose a new sequence modeling layer with linear complexity and an expressive hidden state, named Test-Time Training (TTT) layers. TTT layers update the hidden state via self-supervised learning even during test sequences. The study shows that TTT-Linear and TTT-MLP models outperform traditional RNNs and Transformers in handling long contexts, pointing towards future research potential.*** <br><br>
    Jul 5, Stanford Uni., UC San Diego, UC Berkeley and Meta published a [paper](https://arxiv.org/pdf/2407.04620) “Learning to (Learn at Test Time): RNNs with Expressive Hidden States”. Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. The authors propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, the layers are called Test-Time Training (TTT) layers. The researchers consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. The study evaluates the instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.

20. ***MJ-Bench for Evaluating Multimodal Judges<br>
A collaborative study introduces MJ-Bench to evaluate multimodal judges used for text-to-image generation models. It benchmarks judges on alignment, safety, image quality, and bias, revealing that closed-source VLMs like GPT-4o provide superior feedback. The findings suggest that VLM judges offer more accurate and stable feedback in natural language compared to numerical scales.*** <br><br>
    Jul 5, UNC-Chapel Hill, Uni of Chicago, Stanford Uni, Duke Uni and 13 institutes published a [paper](https://arxiv.org/pdf/2407.04842) “MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?”. While text-to-image models like DALLE-3 and Stable Diffusion are rapidly proliferating, they often encounter challenges such as hallucination, bias, and the production of unsafe, low-quality output. To effectively address these issues, it is crucial to align these models with desired behaviors based on feedback from a multimodal judge. Despite their significance, current multimodal judges frequently undergo inadequate evaluation of their capabilities and limitations, potentially leading to misalignment and unsafe fine-tuning outcomes. To address this issue, the paper introduces MJ-Bench, a novel benchmark which incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. Specifically, the paper evaluates a large variety of multimodal judges including smaller-sized CLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and close-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of a preference dataset. Experiments reveal that close-source VLMs generally provide better feedback, with GPT-4o outperforming other judges in average. Compared with open-source VLMs, smaller-sized scoring models can provide better feedback regarding text-image alignment and image quality, while VLMs provide more accurate feedback regarding safety and generation bias due to their stronger reasoning capabilities. Further studies in feedback scale reveal that VLM judges can generally provide more accurate and stable feedback in natural language (Likert-scale) than numerical scales. Notably, human evaluations on end-to-end fine-tuned models using separate feedback from these multimodal judges provide similar conclusions, further confirming the effectiveness of MJ-Bench. All data, code, models are available at https://huggingface.co/MJ-Bench.

22. ***SWiM Framework for Long Context Models<br>
Snorkel AI and the University of Wisconsin-Madison propose SWiM, an evaluation framework for long context models in LLMs. The study identifies the "lost-in-the-middle" effect where performance degrades with information in the middle of the context window. It introduces medoid voting as an effective training-free method to mitigate this effect, showing a significant improvement in single document QA tasks.*** <br><br>
    Jul 4, Snorkel AI and Uni of Wisconsin-Madison published a [paper](https://arxiv.org/pdf/2407.03651) “Evaluating Language Model Context Windows: A "Working Memory" Test and Inference-time Correction”. Large language models are prominently used in real-world applications, often tasked with reasoning over large volumes of documents. An exciting development in this space is models boasting extended context capabilities, with some accommodating over 2 million tokens. Such long context model capabilities remain uncertain in production systems, motivating the need to benchmark their performance on real world use cases. The paper addresses this challenge by proposing SWiM, an evaluation framework that addresses the limitations of standard tests. Testing the framework on eight long context models, the paper finds that even strong models such as GPT-4 and Claude 3 Opus degrade in performance when information is present in the middle of the context window (lost-in-the-middle effect). Next, in addition to the benchmark, the paper proposes medoid voting, a simple, but effective training-free approach that helps alleviate this effect, by generating responses a few times, each time randomly permuting documents in the context, and selecting the medoid answer. The authors evaluate medoid voting on single document QA tasks, achieving up to a 24% lift in accuracy.

24. ***ChartGemma for Chart Understanding<br>
York University, MILA, Salesforce, and Nanyang Technological University present ChartGemma, a model for understanding and reasoning with charts. Trained on instruction-tuning data from chart images, ChartGemma captures visual trends and patterns, achieving state-of-the-art results in chart summarization, question answering, and fact-checking. The model offers realistic and factually correct summaries of real-world charts.*** <br><br>
    Jul 4, York Uni., MILA, Saleforce, and Nanyang Tech Uni. published a [paper](https://arxiv.org/pdf/2407.04172) “ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild”. Given the ubiquity of charts as a data analysis, visualization, and decision-making tool across industries and sciences, there has been a growing interest in developing pre-trained foundation models as well as general purpose instruction-tuned models for chart understanding and reasoning. However, existing methods suffer crucial drawbacks across two critical axes affecting the performance of chart representation models: they are trained on data generated from underlying data tables of the charts, ignoring the visual trends and patterns in chart images, and use weakly aligned vision-language backbone models for domain-specific training, limiting their generalizability when encountering charts in the wild. The paper addresses these important drawbacks and introduces ChartGemma, a novel chart understanding and reasoning model developed over PaliGemma. Rather than relying on underlying data tables, ChartGemma is trained on instruction-tuning data generated directly from chart images, thus capturing both high-level trends and low-level visual information from a diverse set of charts. The simple approach achieves state-of-the-art results across 5 benchmarks spanning chart summarization, question answering, and fact-checking, and the elaborate qualitative studies on real-world charts show that ChartGemma generates more realistic and factually correct summaries compared to its contemporaries. The authors release the code, model checkpoints, dataset, and demos at https://github.com/vis-nlp/ChartGemma.

26. ***BM25S: Efficient Lexical Search<br>
McGill University introduces BM25S, an efficient implementation of BM25 for lexical search that achieves up to 500x speedup by precomputing and storing BM25 scores in sparse matrices. This approach significantly outperforms existing Python and Java-based implementations, although it may encounter performance limitations and out-of-memory errors.*** <br><br>
    Jul 4, McGill Uni published a [paper](https://arxiv.org/pdf/2407.03618) “BM25S: Orders of magnitude faster lexical search via eager sparse scoring”. The paper introduces BM25S, an efficient Python-based implementation of BM25 that only depends on Numpy and Scipy. BM25S achieves up to a 500x speedup compared to the most popular Python-based framework by eagerly computing BM25 scores during indexing and storing them into sparse matrices. It also achieves considerable speedups compared to highly optimized Java-based implementations, which are used by popular commercial products. Finally, BM25S reproduces the exact implementation of five BM25 variants based on Kamphuis et al. (2020) by extending eager scoring to non-sparse variants using a novel score shifting method. Limitations of the work include it may not achieve the highest possible performance, and may terminate with OOM errors. The code can be found at https://github.com/xhluca/bm25s

28. ***Holistic Evaluation of Multimodal Models<br>
CMU's paper on HEMM proposes a framework for evaluating multimodal foundation models across basic skills, information flow, and real-world use cases. The study identifies challenges and trends in multimodal modeling, offering insights into the impacts of data and model scale, and the benefits of instruction tuning for future multimodal models.*** <br><br>
    Jul 3, CMU published a [paper](https://arxiv.org/pdf/2407.03418) “HEMM: Holistic Evaluation of Multimodal Foundation Models”. Multimodal foundation models that can holistically process text alongside images, video, audio, and other sensory modalities are increasingly used in a variety of real-world applications. However, it is challenging to characterize and study progress in multimodal foundation models, given the range of possible modeling decisions, tasks, and domains. This paper introduces Holistic Evaluation of Multimodal Models (HEMM) to systematically evaluate the capabilities of multimodal foundation models across a set of 3 dimensions: basic skills, information flow, and real-world use cases. Basic multimodal skills are internal abilities required to solve problems, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and the ability to handle external knowledge. Information flow studies how multimodal content changes during a task through querying, translation, editing, and fusion. Use cases span domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and human-computer interaction applications. Through comprehensive experiments across the 30 tasks in HEMM, the authors (1) identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today's models, and (2) distill performance trends regarding how different modeling dimensions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence performance. The conclusions regarding challenging multimodal interactions, use cases, and tasks requiring reasoning and external knowledge, the benefits of data and model scale, and the impacts of instruction tuning yield actionable insights for future work in multimodal foundation models. [Here is the project link](https://github.com/pliang279/HEMM).

30. ***LLMs in Time Series Forecasting<br>
Research by the University of Virginia and University of Washington questions the utility of LLMs in time series forecasting. Ablation studies show that removing the LLM component often improves performance. The study concludes that LLMs do not enhance sequential dependency representation or few-shot learning in time series tasks compared to simpler models.*** <br><br>
    Jun 22, Uni of Virginia and Uni of Washington published a [paper](https://arxiv.org/pdf/2406.16964) “Are Language Models Actually Useful for Time Series Forecasting”. Large language models (LLMs) are being applied to time series tasks, particularly time series forecasting. However, are language models actually useful for time series? After a series of ablation studies on three recent and popular LLM-based time series forecasting methods, the authors find that removing the LLM component or replacing it with a basic attention layer does not degrade the forecasting results -- in most cases the results even improved. The authors also find that despite their significant computational cost, pretrained LLMs do no better than models trained from scratch, do not represent the sequential dependencies in time series, and do not assist in few-shot settings. Additionally, the authors explore time series encoders and reveal that patching and attention structures perform similarly to state-of-the-art LLM-based forecasters.

32. ***Flexibility of Neural Networks<br>
NYU, University of Maryland, Capital One, and Meta investigate the practical flexibility of neural networks. The findings reveal that standard optimizers limit the model's ability to fit training data, convolutional networks are more parameter-efficient, and stochastic training with SGD finds more fitting minima. The study offers insights into the relationship between network architecture and generalization.*** <br><br>
    Jun 17, NYU, Uni of Maryland, Capital One and Meta’s LeCun published a [paper](https://arxiv.org/pdf/2406.11463) “Just How Flexible are Neural Networks in Practice?”. It is widely believed that a neural network can fit a training set containing at least as many samples as it has parameters, underpinning notions of overparameterized and underparameterized models. In practice, however, we only find solutions accessible via our training procedure, including the optimizer and regularizers, limiting flexibility. Moreover, the exact parameterization of the function class, built into an architecture, shapes its loss surface and impacts the minima we find. This work examines the ability of neural networks to fit data in practice. The findings of the work indicate that: (1) standard optimizers find minima where the model can only fit training sets with significantly fewer samples than it has parameters; (2) convolutional networks are more parameter-efficient than MLPs and ViTs, even on randomly labeled data; (3) while stochastic training is thought to have a regularizing effect, SGD actually finds minima that fit more training data than full-batch gradient descent; (4) the difference in capacity to fit correctly labeled and incorrectly labeled samples can be predictive of generalization; (5) ReLU activation functions result in finding minima that fit more data despite being designed to avoid vanishing and exploding gradients in deep architectures.

34. ***ZeRO++ for Efficient Model Training<br>
Microsoft, OpenAI, Snowflake, University of Houston, and University of Nevada introduce ZeRO++, an optimizer that significantly reduces communication overhead in large model training. By implementing techniques like low-precision all-gather and data remapping, ZeRO++ achieves up to 2.16x better throughput and maintains accuracy comparable to original ZeRO. The model's natural weight-quantization also facilitates efficient inference.*** <br><br>
    Mar 9,  Microsoft, OpenAI, Snowflake, Uni of Houston, and Nui of Nevada published a [paper](https://openreview.net/pdf?id=gx2BT0a9MQ) on ICLR2024 “ZeRO++: Extremely Efficient Collective Communication for Large Model Training”. Zero Redundancy Optimizer (ZeRO) has been used to train a wide range of large language models on massive GPU clusters due to its ease of use, efficiency, and good scalability. However, when training on low-bandwidth clusters, and/or when small batch size per GPU is used, ZeRO’s effective throughput is limited by communication overheads. To alleviate this limitation, this paper introduces ZeRO++ composing of three communication volume reduction techniques (lowprecision all-gather, data remapping, and low-precision gradient averaging) to significantly reduce the communication volume up to 4x that enables up to 2.16x better throughput at 384 GPU scale. Experimental results also show ZeRO++ can speedup the RLHF training by 3.3x compared to vanilla ZeRO. To verify the convergence of ZeRO++, the authors test up to 13B model for pretraining with 8/6-bits all gather and up to 30B model for finetuning with 4/2-bits all gather, and demonstrate on-par accuracy as original ZeRO (aka standard training). As a byproduct, the model trained with ZeRO++ is naturally weight-quantized, which can be directly used for inference without post-training quantization or quantization-aware training.
 <br><br><br>

***7 Jul 2024***

1. ***Highlighting AI Companies' Data Vulnerability <br>
A recent breach at OpenAI, which affected only an employee discussion forum, underscores that AI companies are prime targets for hackers. AI firms possess three types of valuable data: high-quality training data, bulk user interactions, and sensitive customer data. The complexity of AI systems increases breach risks, highlighting the need for robust security practices and continuous vigilance.*** <br><br>
   Jul 5, according to [techcrunch.com](https://techcrunch.com/2024/07/05/openai-breach-is-a-reminder-that-ai-companies-are-treasure-troves-for-hackers/), A recent breach at OpenAI was minor, affecting only an employee discussion forum, but it underscores that AI companies are prime targets for hackers. OpenAI and similar companies hold three types of highly valuable data: 1) High-Quality Training Data: Secretive, labor-intensive, and crucial for AI model development. 2) Bulk User Interactions: Billions of ChatGPT conversations offering deep insights, more valuable than simple search data. 3) Customer Data: Sensitive information from companies using AI tools, which often includes internal databases and proprietary information. AI companies handle significant industrial secrets, necessitating robust security measures. Despite their capabilities, the novelty and complexity of AI increase the risk of breaches. Good security practices are essential, but the constantly evolving threat landscape, now amplified by AI, requires continuous vigilance. While there's no need for immediate panic, the breach highlights the unique and substantial risks AI companies face. Enhanced security and awareness are crucial as they continue to manage vast amounts of sensitive data.

3. ***Kyutai's Voice-Enabled AI Chat-Bot <br>
French start-up Kyutai introduced Moshi, an AI model with advanced vocal capabilities. Developed in six months by a team of eight, Moshi facilitates natural and expressive AI communication. It has potential applications as a coach or companion and excels in text-to-speech. Kyutai plans to share Moshi’s code and model weights for open research and development.*** <br><br>
   Jul 3, according to [kyutai.org](https://kyutai.org/cp_moshi.pdf), the French AI start-up unveiled its first voice-enabled AI chat-bot which is openly accessible to all. In just 6 months, with a team of 8, the Kyutai research lab developed from scratch an artificial intelligence (AI) model with unprecedented vocal capabilities called Moshi. The interactive demo of the AI will be accessible from the Kyutai website. This new type of technology makes it possible for the first time to communicate in a smooth, natural and expressive way with an AI. During the presentation, the Kyutai team interacted with Moshi to illustrate its potential as a coach or companion for example, and its creativity through the incarnation of characters in roleplays. More broadly, Moshi has the potential to revolutionize the use of speech in the digital world. For instance, its text-to-speech capabilities are exceptional in terms of emotion and interaction between multiple voices. Moshi can also be installed locally and therefore run safely on an unconnected device. With Moshi, Kyutai intends to contribute to open research in AI and to the development of the entire ecosystem. The code and weights of the models will soon be freely shared, which is also unprecedented for such technology. They will be useful both to researchers in the field and to developers working on voice-based products and services.

5. ***Advancements in Vision Language Models <br>
InternLM-XComposer-2.5 (IXC-2.5), developed by SAIL, CUHK, SenseTime Group, and Tsinghua University, is a versatile vision language model supporting long-contextual input and output. IXC-2.5 features ultra-high resolution understanding, fine-grained video understanding, and multi-turn multi-image dialogue. It outperforms existing models on multiple benchmarks and is publicly available.*** <br><br>
   Jul 3, SAIL, CUHK, SenseTime Group and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2407.03320) “InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output”. The paper presents InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output. IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output contexts. Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In addition to comprehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks. The InternLM-XComposer-2.5 is publicly available at https://github.com/InternLM/InternLM-XComposer.

7. ***Unsafe Information Leakage in AI Responses <br>
Researchers from the University of Toronto, Vector Institute, and University of Oxford highlight vulnerabilities in large language models (LLMs) to jailbreak attacks. Current defenses are insufficient, and the paper introduces inferential adversaries and an information censorship criterion to ensure safety, revealing an intrinsic safety-utility trade-off.*** <br><br>
   Jul 2, Uni of Toronto, Vector Institute and Uni of Oxford published a [paper](https://arxiv.org/pdf/2407.02551) “A False Sense of Safety: Unsafe Information Leakage in 'Safe' AI Responses”. Large Language Models (LLMs) are vulnerable to jailbreaks–methods to elicit harmful or generally impermissible outputs. Safety measures are developed and assessed on their effectiveness at defending against jailbreak attacks, indicating a belief that safety is equivalent to robustness. The authors assert that current defense mechanisms, such as output filters and alignment fine-tuning, are, and will remain, fundamentally insufficient for ensuring model safety. These defenses fail to address risks arising from dual-intent queries and the ability to composite innocuous outputs to achieve harmful goals. To address this critical gap, the paper introduces an information-theoretic threat model called inferential adversaries who exploit impermissible information leakage from model outputs to achieve malicious goals. The work distinguishes these from commonly studied security adversaries who only seek to force victim models to generate specific impermissible outputs. The study demonstrates the feasibility of automating inferential adversaries through question decomposition and response aggregation. To provide safety guarantees, the researchers define an information censorship criterion for censorship mechanisms, bounding the leakage of impermissible information. The paper proposes a defense mechanism which ensures this bound and reveals an intrinsic safety-utility trade-off. The work provides the first theoretically grounded understanding of the requirements for releasing safe LLMs and the utility costs involved.

9. ***RankRAG: Enhancing Context Ranking and Generation <br>
Georgia Tech and Nvidia propose RankRAG, a framework that instruction-tunes LLMs for context ranking and answer generation in retrieval-augmented generation (RAG). RankRAG outperforms existing models on various benchmarks, including biomedical domains, demonstrating its superior generalization capability without additional fine-tuning.*** <br><br>
    Jul 2, Georgia Tech and Nvidia published a [paper](https://arxiv.org/pdf/2407.02485v1) “RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs”. Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). This work proposes a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, the work compares the model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, the Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains. Zero-shot QA performance over nine datasets is about 56% on average.

11. ***Accelerating LLM Inference with Sparse Attention <br>
Microsoft and the University of Surrey introduce MInference 1.0, a method to accelerate pre-filling for long-context LLMs using dynamic sparse attention. By leveraging specific attention matrix patterns, MInference reduces inference latency by up to 10x while maintaining accuracy, applicable to existing LLMs without modifications.*** <br><br>
    Jul 2, Microsoft and Uni of Surrey published a [paper](https://arxiv.org/pdf/2407.02490) “MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention”. The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, the paper introduces MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, the authors identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. The work determines the optimal pattern for each attention head offline and dynamically builds sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, the authors perform efficient sparse attention calculations via the optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. The proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, the study demonstrates that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. The code is available at https://aka.ms/MInference.

13. ***Optimizing Data Mixture for LLM Pre-training <br>
SeaAI, SMU, and others present RegMix, a method to identify high-performing data mixtures for LLM pre-training through regression. RegMix demonstrates superior performance compared to human-selected mixtures, highlighting the significant impact of data mixtures on model performance and the need for automatic approaches.*** <br><br>
    Jul 1, SeaAI, SMU, et al published a [paper](https://arxiv.org/pdf/2407.01492) “RegMix: Data Mixture as Regression for Language Model Pre-training”. The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. The authors propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix involves training a set of small models with diverse data mixtures and fitting a regression model to predict their performance given their respective mixtures. With the fitted regression model, the study simulates the top-ranked mixture and uses it to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, the authors train 512 models with 1M parameters for 1B tokens of different mixtures to fit the regression model and find the optimal mixture. Using this mixture the work trains a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which is found to perform best among 64 candidate 1B parameter models with other mixtures. Further, the method demonstrates superior performance compared to human selection and achieves results that match or surpass DoReMi, while utilizing only 10% of the compute budget. The experiments also show that (1) Data mixtures significantly impact performance with single-task performance variations of up to 14.6%; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed; (4) Data mixture effects transcend scaling laws, and the approach captures the complexity by considering all domains together. The code is available at https://github.com/sail-sg/regmix.

15. ***Mechanistic Interpretation in Transformers <br>
UC Berkeley, MIT, and others introduce contextual decomposition for transformers (CD-T), a method for interpreting transformer models' internal mechanisms. CD-T offers computational efficiency and reliable interpretation, improving understanding and trust in model outputs compared to existing methods like SHAP and LIME.*** <br><br>
    Jul 1, UC Berkeley, MIT et al. published a [paper](https://arxiv.org/pdf/2407.00886) “Mechanistic Interpretation through Contextual Decomposition in Transformers”. Transformers exhibit impressive capabilities but are often regarded as black boxes due to challenges in understanding the complex nonlinear relationships between features. Interpreting machine learning models is of paramount importance to mitigate risks, and mechanistic interpretability is in particular of current interest as it opens up a window for guiding manual modifications and reverse-engineering solutions. This work introduces contextual decomposition for transformers (CD-T), extending a prior work on CD for RNNs and CNNs, to address mechanistic interpretation computationally efficiently. CD-T is a flexible interpretation method for transformers. It can capture contributions of combinations of input features or source internal components (e.g. attention heads, feed-forward networks) to (1) final predictions or (2) the output of any target internal component. Using CD-T, the authors propose a novel algorithm for circuit discovery. On a real-world pathology report classification task: the paper shows CD-T distills a more faithful circuit of attention heads with improved computational efficiency (speed up 2x) than a prior benchmark, path patching. As a versatile interpretation method, CD-T also exhibits exceptional capabilities for local interpretations. CD-T is shown to reliably find words and phrases of contrasting sentiment/topic on SST-2 and AGNews datasets. Through human experiments, the paper demonstrates CD-T enables users to identify the more accurate of two models and to better trust a model's outputs compared to alternative interpretation methods such as SHAP and LIME.

17. ***Universal Approach for Sentence Segmentation <br>
Researchers from Johannes Kepler University and the University of Cambridge introduce Segment Any Text (SaT), a model for robust, adaptable, and efficient sentence segmentation. SaT outperforms existing methods across diverse domains and languages, providing a universal solution for text segmentation challenges.*** <br><br>
    Jun 30, Johannes Kepler Uni and Uni of Cambridge published a [paper](https://arxiv.org/pdf/2406.16678) “Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation”. Segmenting text into sentences plays an early and crucial role in many NLP systems. This is commonly achieved by using rule-based or statistical methods relying on lexical features such as punctuation. Although some recent works no longer exclusively rely on punctuation, it is found that no prior method achieves all of (i) robustness to missing punctuation, (ii) effective adaptability to new domains, and (iii) high efficiency. The paper introduces a new model - Segment any Text (SaT) - to solve this problem. To enhance robustness, the work proposes a new pretraining scheme that ensures less reliance on punctuation. To address adaptability, the paper introduces an extra stage of parameter-efficient fine-tuning, establishing state-of-the-art performance in distinct domains such as verses from lyrics and legal documents. Along the way, the work introduces architectural modifications that result in a threefold gain in speed over the previous state of the art and solve spurious reliance on context far in the future. Finally, the work introduces a variant of the model with fine-tuning on a diverse, multilingual mixture of sentence-segmented data, acting as a drop-in replacement and enhancement for existing segmentation tools. Overall, the contributions provide a universal approach for segmenting any text. The method outperforms all baselines - including strong LLMs - across 8 corpora spanning diverse domains and languages, especially in practically relevant situations where text is poorly formatted. The models and code, including documentation, are available at [this https URL](https://huggingface.co/segment-any-text) under the MIT license.

19. ***Token Erasure in LLMs <br>
Northeastern University explores how LLMs process multi-token words and named entities through a "token erasure" effect. This study reveals how LLMs convert arbitrary token groups into meaningful representations, providing insights into the implicit vocabulary of LLMs.*** <br><br>
    Jun 28, Northeastern Uni published a [paper](https://arxiv.org/pdf/2406.20086) “Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs”. LLMs process text as sequences of tokens that roughly correspond to words, where less common words are represented by multiple tokens. However, individual tokens are often semantically unrelated to the meanings of the words/concepts they comprise. For example, Llama-2-7b's tokenizer splits the word "northeastern" into the tokens ['_n', 'ort', 'he', 'astern'], none of which correspond to semantically meaningful units like "north" or "east." Similarly, the overall meanings of named entities like "Neil Young" and multi-word expressions like "break a leg" cannot be directly inferred from their constituent tokens. Mechanistically, how do LLMs convert such arbitrary groups of tokens into useful higher-level representations? This work finds that last token representations of named entities and multi-token words exhibit a pronounced "erasure" effect, where information about previous and current tokens is rapidly forgotten in early layers. Using this observation, the authors propose a method to "read out" the implicit vocabulary of an autoregressive LLM by examining differences in token representations across layers, and present results of this method for Llama-2-7b and Llama-3-8B. To the authors knowledge, this is the first attempt to probe the implicit vocabulary of an LLM.

21. ***Zuckerberg on AI Development <br>
Mark Zuckerberg advocates for diverse AI models instead of a single overarching AI. He emphasizes the importance of businesses and creators developing their own AI, reflecting varied interests and avoiding the concentration of AI capabilities in one entity.*** <br><br>
    Jun 28, according to [Entrepreneur](https://www.entrepreneur.com/business-news/mark-zuckerberg-reveals-the-future-meta-ai-tech-industry/476368), Mark Zuckerberg Sounds Off on Developing AI: 'I Don't Think AI Technology Is a Thing That Should Be Hoarded'. In a talk with Kallaway, Zuckerberg said that the most effective use of AI in the future is to have businesses and creators create their own AI instead of focusing on one big overarching model. "It's almost as if they think they're creating God or something, and that's just not what we're doing". Zuckerberg spoke about most major companies and their desire to build one main AI, using Google's Gemini or OpenAI's Chat GPT as examples. But for Meta, the strategy isn't to develop one central AI — the company wants to create multiple programs. "Our overall view is that this isn't the type of thing that there should just be one of, people want to interact with lots of different people and businesses and there need to be a lot of different AIs that get created to reflect people's different interests," he explained.

23. ***Unsupervised Segmentation Model <br>
UC Berkeley introduces Unsupervised SAM (UnSAM), a model for whole-image segmentation without human annotations. Using a hierarchical structure discovery approach, UnSAM achieves competitive results with supervised models, offering an effective solution for unsupervised segmentation tasks.*** <br><br>
    Jun 28, UC Berkeley published a [paper](https://arxiv.org/pdf/2406.20081v1) “Segment Anything without Supervision”. The Segmentation Anything Model (SAM) requires labor-intensive data labeling. The paper presents Unsupervised SAM (UnSAM) for promptable and automatic wholeimage segmentation that does not require human annotations. UnSAM utilizes a divide-and-conquer strategy to “discover” the hierarchical structure of visual scenes. The study first leverages top-down clustering methods to partition an unlabeled image into instance/semantic level segments. For all pixels within a segment, a bottom-up clustering method is employed to iteratively merge them into larger groups, thereby forming a hierarchical structure. These unsupervised multi-granular masks are then utilized to supervise model training. Evaluated across seven popular datasets, UnSAM achieves competitive results with the supervised counterpart SAM, and surpasses the previous state-of-the-art in unsupervised segmentation by 11% in terms of AR. Moreover, the work shows that supervised SAM can also benefit from the self-supervised labels. By integrating the unsupervised pseudo masks into SA-1B’s ground-truth masks and training UnSAM with only 1% of SA-1B, a lightly semisupervised UnSAM can often segment entities overlooked by supervised SAM, exceeding SAM’s AR by over 6.7% and AP by 3.9% on SA-1B.

25. ***Leveraging Internal Knowledge for Complex Reasoning <br>
KAIST and the University of Richmond investigate how LLMs utilize internal knowledge for complex reasoning. The study proposes DepthQA, a dataset for deconstructing complex questions, highlighting the importance of structured intermediate steps in enhancing LLMs' reasoning abilities.*** <br><br>
    Jun 27, KAIST and Uni of Richmond published a [paper](https://arxiv.org/pdf/2406.19502) “Investigating How Large Language Models Leverage Internal Knowledge to Perform Complex Reasoning”. Despite significant advancements, there is a limited understanding of how large language models (LLMs) utilize knowledge for reasoning. To address this, the paper proposes a method that deconstructs complex real-world questions into a graph, representing each question as a node with parent nodes of background knowledge needed to solve the question. The word develops the DepthQA dataset, deconstructing questions into three depths: (i) recalling conceptual knowledge, (ii) applying procedural knowledge, and (iii) analyzing strategic knowledge. Based on a hierarchical graph, the authors quantify forward discrepancy, discrepancies in LLMs' performance on simpler sub-problems versus complex questions. The work also measures backward discrepancy, where LLMs answer complex questions but struggle with simpler ones. The analysis shows that smaller models have more discrepancies than larger models. Additionally, guiding models from simpler to complex questions through multi-turn interactions improves performance across model sizes, highlighting the importance of structured intermediate steps in knowledge reasoning. This work enhances the understanding of LLM reasoning and suggests ways to improve their problem-solving abilities.

27. ***Limitations of Unlearning in Content Regulation <br>
Google examines the limitations of unlearning as a method for content regulation in LLMs. The study introduces "ununlearning," where unlearned knowledge can be reintroduced during inference, emphasizing the need for content filtering to ensure effective regulation.*** <br><br>
    Jun 27, Google published a [paper](https://arxiv.org/pdf/2407.00106) “UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI”. Exact unlearning was first introduced as a privacy mechanism that allowed a user to retract their data from machine learning models on request. Shortly after, inexact schemes were proposed to mitigate the impractical costs associated with exact unlearning. More recently unlearning is often discussed as an approach for removal of impermissible knowledge i.e. knowledge that the model should not possess such as unlicensed copyrighted, inaccurate, or malicious information. The promise is that if the model does not have a certain malicious capability, then it cannot be used for the associated malicious purpose. This paper revisits the paradigm in which unlearning is used for in Large Language Models (LLMs) and highlight an underlying inconsistency arising from in-context learning. Unlearning can be an effective control mechanism for the training phase, yet it does not prevent the model from performing an impermissible act during inference. The authors introduce a concept of ununlearning, where unlearned knowledge gets reintroduced in-context, effectively rendering the model capable of behaving as if it knows the forgotten knowledge. As a result, the authors argue that content filtering for impermissible knowledge will be required and even exact unlearning schemes are not enough for effective content regulation. The paper discusses feasibility of ununlearning for modern LLMs and examine broader implications.

29. ***Meta 3D Gen: Advanced 3D Asset Generation <br>
Meta introduces 3DGen, a state-of-the-art pipeline for text-to-3D asset generation. 3DGen excels in prompt fidelity and visual quality, supporting high-quality 3D shapes and textures in under a minute, outperforming industry baselines.*** <br><br>
    Jun 25, Meta published a [paper](https://scontent.fcbr1-1.fna.fbcdn.net/v/t39.2365-6/449707112_509645168082163_2193712134508658234_n.pdf?_nc_cat=111&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=tlSuCzsxrocQ7kNvgHDhUqB&_nc_ht=scontent.fcbr1-1.fna&oh=00_AYDZlYUAcKARsxlM8vnT6D3vG_iobn2IbT4Za8o92Vmk3w&oe=668FCED1) “Meta 3D Gen”. The paper introduces Meta 3D Gen (3DGen), a new state-of-the-art, fast pipeline for text-to-3D asset generation. 3DGen offers 3D asset creation with high prompt fidelity and high-quality 3D shapes and textures in under a minute. It supports physically-based rendering (PBR), necessary for 3D asset relighting in real-world applications. Additionally, 3DGen supports generative retexturing of previously generated (or artist-created) 3D shapes using additional textual inputs provided by the user. 3DGen integrates key technical components, Meta 3D AssetGen and Meta 3D TextureGen, that is developed for text-to-3D and text-to-texture generation, respectively. By combining their strengths, 3DGen represents 3D objects simultaneously in three ways: in view space, in volumetric space, and in UV (or texture) space. The integration of these two techniques achieves a win rate of 68% with respect to the single-stage model. The authors compare 3DGen to numerous industry baselines, and show that it outperforms them in terms of prompt fidelity and visual quality for complex textual prompts, while being significantly faster.

31. ***Critical View on AI Hallucinations <br>
Researchers from the University of Glasgow critique large language models for producing inaccurate outputs, likening them to "bullshit" as defined by Frankfurt. The study argues that understanding AI misrepresentations in this way is more useful for predicting and discussing their behavior.*** <br><br>
    Jun 8, Ethics and Information Technology published a [paper](https://link.springer.com/article/10.1007/s10676-024-09775-5) by researchers from Uni of Glasgow “ChatGPT is bullshit”. Recently, there has been considerable interest in large language models: machine learning systems which produce humanlike text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called “AI hallucinations”. The authors argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. The researchers distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. The authors further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.

33. ***Human Expectations of LLM Performance <br>
Harvard, MIT, and the University of Chicago explore how humans generalize LLM performance across tasks. The study reveals that human generalization can be predicted using NLP methods, showing discrepancies between human expectations and actual LLM performance, especially in high-cost mistake scenarios.*** <br><br>
    Jun 6, Harvard Uni, MIT, and Uni of Chicago published a [paper](https://arxiv.org/pdf/2406.01382) accepted by ICML 2024 “Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function”. What makes large language models (LLMs) impressive is also what makes them hard to evaluate: their diversity of uses. To evaluate these models, people must understand the purposes they will be used for. The authors consider a setting where these deployment decisions are made by people, and in particular, people's beliefs about where an LLM will perform well. The paper models such beliefs as the consequence of a human generalization function: having seen what an LLM gets right or wrong, people generalize to where else it might succeed. The study collects a dataset of 19K examples of how humans make generalizations across 79 tasks from the MMLU and BIG-Bench benchmarks. The work shows that the human generalization function can be predicted using NLP methods: people have consistently structured ways to generalize. The authors then evaluate LLM alignment with the human generalization function. The results show that -- especially for cases where the cost of mistakes is high -- more capable models (e.g. GPT-4) can do worse on the instances people choose to use them for, exactly because they are not aligned with the human generalization function.

35. ***Impact of ChatGPT on Human Skills <br>
A study published in Technological Forecasting & Social Change analyzes the impact of ChatGPT on human skills using Twitter data. The study identifies four emerging skills for using ChatGPT and highlights the significant impact of the technology on various human skills.*** <br><br>
    Jun 5, Technological Forecasting & Social Change published a [paper](https://www.sciencedirect.com/science/article/pii/S0040162524001859) “The impact of ChatGPT on human skills: A quantitative study on twitter data”. The novel generative Artificial Intelligence (AI) developed by OpenAI, i.e., ChatGPT, rised a great interest in both scientific and business contexts. This new wave of technological advancement typically produces deep transformation in the workplace, requiring new skills. However, none of the studies in literature provide quantitative analysis and measures on the impact of ChatGPT on human skills. To address this gap, the paper collected a database of 616,073 tweets about ChatGPT, and used Natural Language Processing techniques to identify the tasks users requested ChatGPT to perform, and the sentiment related to these tasks. Then, the work compared these tasks with a standard taxonomy of skills (i.e., ESCO) using BERT. The results of the study underline that ChatGPT impacts 185 different skills. Moreover, the researchers proposed a model to represent the interaction of the user and ChatGPT, useful to define four skills which are emerging for using this new technology. The four skills are: defining task goals; using prompt language; knowing the principles of generative AI; and conducting performance measurement.
 
 <br><br><br>

***30 Jun 2024***

1. ***Gemma 2 Release by Google:<br>
   Google launched Gemma 2 for researchers and developers, available in 9 billion (9B) and 27 billion (27B) parameter sizes. It offers higher performance and efficiency compared to its predecessor, with notable safety advancements. The 27B model rivals models twice its size, operating efficiently on a single NVIDIA H100 Tensor Core GPU or TPU host, reducing deployment costs. Key highlights include outstanding performance, cost savings, and fast inference across various hardware setups.*** <br><br>

   27 Jun, Google released [Gemma 2](https://blog.google/technology/developers/google-gemma-2/) for researchers and developers. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs. Gemma 2 is built on a redesigned architecture, engineered for both exceptional performance and inference efficiency. Here’s what makes it stand out: 1) Outsized performance: At 27B, Gemma 2 delivers the best performance for its size class, and even offers competitive alternatives to models more than twice its size. The 9B Gemma 2 model also delivers class-leading performance, outperforming Llama 3 8B and other open models in its size category. 2) Unmatched efficiency and cost savings: The 27B Gemma 2 model is designed to run inference efficiently at full precision on a single Google Cloud TPU host, NVIDIA A100 80GB Tensor Core GPU, or NVIDIA H100 Tensor Core GPU, significantly reducing costs while maintaining high performance. This allows for more accessible and budget-friendly AI deployments. 3) Blazing fast inference across hardware: Gemma 2 is optimized to run at incredible speed across a range of hardware, from powerful gaming laptops and high-end desktops, to cloud-based setups. [Compare Gemm 2](https://colab.research.google.com/github/PAIR-code/llm-comparator/blob/main/python/notebooks/basic_demo.ipynb) with other LLMs via [LLM-COMPARATOR](https://github.com/pair-code/llm-comparator).

3. ***OpenAI's LLM Critics Paper:<br>
   OpenAI's paper "LLM Critics Help Catch LLM Bugs" discusses the limitations of Reinforcement Learning from Human Feedback (RLHF) due to human evaluation constraints. It introduces "critic" models, trained to provide natural language feedback on code errors, outperforming human critiques in 63% of cases. Despite some hallucinations, these critics significantly enhance bug detection in LLMs, suggesting effective collaboration between human reviewers and LLM critics.*** <br><br>
   27 Jun, OpenAI published a [paper](https://cdn.openai.com/llm-critics-help-catch-llm-bugs-paper.pdf) “LLM Critics Help Catch LLM Bugs”. The paper indicates that Reinforcement learning from human feedback (RLHF) is fundamentally limited by the capacity of humans to correctly evaluate model output. To improve human evaluation ability and overcome that limitation this work trains “critic” models that help humans to more accurately evaluate model-written code. These critics are themselves LLMs trained with RLHF to write natural language feedback highlighting problems in code from real-world assistant tasks. On code containing naturally occurring LLM errors model-written critiques are preferred over human critiques in 63% of cases, and human evaluation finds that models catch more bugs than human contractors paid for code review. The authors further confirm that the fine-tuned LLM critics can successfully identify hundreds of errors in ChatGPT training data rated as “flawless”, even though the majority of those tasks are non-code tasks and thus out-of-distribution for the critic model. Critics can have limitations of their own, including hallucinated bugs that could mislead humans into making mistakes they might have otherwise avoided, but human-machine teams of critics and contractors catch similar numbers of bugs to LLM critics while hallucinating less than LLMs alone. Hulluciantion rate of the LLM is around 18%The hulluciantion.

5. ***Meta's LLM Compiler Release:<br>
   Meta released the Meta Large Language Model Compiler, designed for code optimization tasks. Built on Code Llama, it improves understanding of compiler intermediate representations and assembly language. Trained on 546 billion tokens, it offers models in 7 billion and 13 billion parameter sizes. Meta's LLM Compiler shows notable improvements in code size optimization and assembly disassembly, aiming to advance research in compiler optimization.*** <br><br>
   27 Jun, Meta released [Meta LLM Compiler](https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/) and published a [paper](https://scontent.fcbr1-1.fna.fbcdn.net/v/t39.2365-6/448997590_1496256481254967_2304975057370160015_n.pdf?_nc_cat=106&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=4Yn8V9DFdbsQ7kNvgG8lVE2&_nc_ht=scontent.fcbr1-1.fna&oh=00_AYC6xJJQpGi0wTHKobP4L_VDU-75bQlAwwtNjv5hRYDatw&oe=6685440D) “Meta Large Language Model Compiler: Foundation Models of Compiler Optimization”. Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, Meta introduces Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. Meta also presents fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners.

7. ***CharXiv Paper on Chart Understanding:<br>
A paper titled "CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs" highlights the limitations of current datasets for chart understanding in Multimodal LLMs. It introduces CharXiv, an evaluation suite with 2,323 diverse charts from arXiv papers, revealing a significant performance gap between proprietary and open-source models. The study emphasizes the need for more realistic evaluation methods to enhance MLLM chart understanding.*** <br><br>
   27 Jun, Princeton Uni, Uni of Wisconsin and UHK published a [paper](https://arxiv.org/pdf/2406.18521) “CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs”. The paper states that chart understanding plays a pivotal role when applying Multimodal Large Language Models (MLLMs) to real-world tasks such as analyzing scientific papers or financial reports. However, existing datasets often focus on oversimplified and homogeneous charts with template-based questions, leading to an over-optimistic measure of progress. The paper demonstrates that although open-source models can appear to outperform strong proprietary models on these benchmarks, a simple stress test with slightly different charts or questions can deteriorate performance by up to 34.5%. This work proposes CharXiv, a comprehensive evaluation suite involving 2,323 natural, challenging, and diverse charts from arXiv papers. CharXiv includes two types of questions: 1) descriptive questions about examining basic chart elements and 2) reasoning questions that require synthesizing information across complex visual elements in the chart. To ensure quality, all charts and questions are handpicked, curated, and verified by human experts. The results reveal a substantial, previously underestimated gap between the reasoning skills of the strongest proprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the strongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%. All models lag far behind human performance of 80.5%, underscoring weaknesses in the chart understanding capabilities of existing MLLMs. The authors hope CharXiv facilitates future research on MLLM chart understanding by providing a more realistic and faithful measure of progress. Project page and leaderboard: https://charxiv.github.io/

9. ***Foundation Capital's Article on AI Strategy:<br>
The article "Beyond LLMs: Building Magic" discusses the widespread adoption of AI strategies by companies across industries. It points out challenges in deploying LLM-based applications at scale, such as data preprocessing and scalability issues. The article suggests leveraging multimodal models, multi-agent systems, and new architectures to overcome these limitations and enhance AI deployment.*** <br><br>
    27 Jun, foundationcapital published [an article](https://foundationcapital.com/beyond-llms-building-magic/) “Beyond LLMs: Building magic”. The article argues that every CEO, whether they make computer chips or potato chips, has announced an AI strategy, and markets are anticipating profit improvements across the board as a result of AI. At the same time, enterprise deployment of LLM-based applications remains nascent. LLMs have plenty of shortcomings, including hallucinations, bias, and data exfiltration. There are also a host of adoption challenges, including the fact that most company data needs significant preprocessing before it can be used by a model, and use cases that worked in demos quickly break down when scaled up to production. The article furhter suggests that as founders begin to deploy LLM-based applications at scale, they should look to leverage three innovations: 1) Multimodal models are moving beyond text, opening up new use cases. 2) Multi-agent systems transform our ability to automate complex tasks. 3) New model architectures address the limitations of transformers.

11. ***Efficient Continual Pre-training Paper:<br>
Researchers from Peking University, HKUST, and MIT-IBM Watson AI Lab published a paper on mitigating the stability gap in continual pre-training for LLMs. They propose three strategies: training on subsets for multiple epochs, using high-quality sub-corpora, and employing data mixtures similar to pre-training data. These strategies significantly improve performance in new domains, particularly in medical tasks, and are validated on Llama-family models.*** <br><br>
    27 Jun, Peking Uni, HKUST, and MIT-IBM Watron AI Lab published a [paper](https://arxiv.org/pdf/2406.14833) “Efficient Continual Pre-training by Mitigating the Stability Gap”. Continual pre-training has increasingly become the predominant approach for adapting Large Language Models (LLMs) to new domains. This process involves updating the pre-trained LLM with a corpus from a new domain, resulting in a shift in the training distribution. To study the behavior of LLMs during this shift, the paper measured the model's performance throughout the continual pre-training process. The authors observed a temporary performance drop at the beginning, followed by a recovery phase, a phenomenon known as the "stability gap," previously noted in vision models classifying new classes. To address this issue and enhance LLM performance within a fixed compute budget, the paper proposes three effective strategies: (1) Continually pre-training the LLM on a subset with a proper size for multiple epochs, resulting in faster performance recovery than pre-training the LLM on a large corpus in a single epoch; (2) Pre-training the LLM only on high-quality sub-corpus, which rapidly boosts domain performance; and (3) Using a data mixture similar to the pre-training data to reduce distribution gap. The researchers conduct various experiments on Llama-family models to validate the effectiveness of the strategies in both medical continual pre-training and instruction tuning. For example, the strategies improve the average medical task performance of the OpenLlama-3B model from 36.2% to 40.7% with only 40% of the original training budget and enhance the average general task performance without causing forgetting. Furthermore, the authors apply the strategies to the Llama-3-8B model. The resulting model, Llama-3-Physician, achieves the best medical performance among current open-source models, and performs comparably to or even better than GPT-4 on several medical benchmarks. The models are available at https://huggingface.co/YiDuo1999/Llama-3-Physician-8B-Instruct.

13. ***Databricks' Compound AI Systems:<br>
Databricks emphasizes "compound AI systems" as a key trend for 2024, focusing on integrated AI systems using multiple models and techniques. At the Data+AI Summit, they announced updates to their Mosaic AI platform and open-sourced the Unity Catalog for better data standardization and training accuracy. Databricks aims to lead in democratized AI by offering advanced, reliable AI systems.*** <br><br>
    26 Jun, according to [analytisindiamagl.com](https://analyticsindiamag.com/databricks-compound-ai-systems-could-crush-openai-anthropic/), Databricks is emphasizing "compound AI systems" as a major trend in 2024 to improve the quality, reliability, and measurement of AI applications. Unlike Anthropic and OpenAI, which focus on enhancing their capabilities, Databricks aims to create integrated AI systems that use multiple models and techniques. At the Data+AI Summit, Databricks announced updates to its Mosaic AI platform, enabling customers to build compound AI systems. These systems offer significant value by integrating various tools and models for better insights. Databricks is also focusing on data governance and open-sourcing its Unity Catalog to standardize data and improve training accuracy. By prioritizing democratized AI, Databricks has positioned itself as a leader in this evolving market, potentially maintaining its lead despite competition from larger tech companies.

15. ***AI Infiltration in University Exams Paper:<br>
A paper from the University of Reading and University of Essex examines the impact of AI on university assessments. It reports a study where 100% AI-written submissions were injected into the examination system, with 94% going undetected. The AI submissions scored higher than real student work, highlighting the challenges AI poses to the integrity of academic assessments.*** <br><br>
    26 Jun, Uni of Reading and Uni of Essex published a [paper](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0305354&type=printable) on Plos ONE “A real-world test of artificial intelligence infiltration of a university examinations system: A ‘Turing Test’ case study”. The authors find that the recent rise in artificial intelligence systems, such as ChatGPT, poses a fundamental roblem for the educational sector. In universities and schools, many forms of assessment, such as coursework, are completed without invigilation. Therefore, students could hand in work as their own which is in fact completed by AI. Since the COVID pandemic, the sector has additionally accelerated its reliance on unsupervised ‘take home exams’. If students cheat using AI and this is undetected, the integrity of the way in which students are assessed is threatened. The study reports a rigorous, blind study in which the authors injected 100% AI written submissions into the examinations system in five undergraduate modules, across all years of study, for a BSc degree in Psychology at a reputable UK university. The paper found that 94% of the AI submissions were undetected. The grades awarded to the AI submissions were on average half a grade boundary higher than that achieved by real students. Across modules there was an 83.4% chance that the AI submissions on a module would outperform a random selection of the same number of real student submissions.

17. ***Huggingface's Open-LLM Leaderboard:<br>
Huggingface released a new version of their open-LLM Leaderboard, providing a platform for reproducible and comparable LLM evaluations. The leaderboard, widely used in the ML community, addresses challenges of non-reproducible results in published models. The updated version aims to improve transparency and performance evaluation for state-of-the-art open-source LLMs.*** <br><br>
    26 Jun, [Huggingface](https://huggingface.co/spaces/open-llm-leaderboard/blog) released a new open-LLM Leaderboard. Evaluating and comparing LLMs is hard. The RLHF team realized this a year ago when they wanted to reproduce and compare results from several published models. It was a nearly impossible task: scores in papers or marketing releases were given without any reproducible code, sometimes doubtful, but in most cases, just using optimized prompts or evaluation setup to give the best chances to the models. The team therefore decided to create a place where reference models would be evaluated in the exact same setup (same questions, asked in the same order, etc.) to gather completely reproducible and comparable results; and that’s how the Open LLM Leaderboard was born! Following a series of highly visible model releases, it became a widely used resource in the ML community and beyond, visited by more than 2 million unique people over the last 10 months. Around 300,000 community members use and collaborate on it monthly through submissions and discussions, usually to: 1) Find state-of-the-art open-source releases as the leaderboard provides reproducible scores separating marketing fluff from actual progress in the field. 2) Evaluate their work, be it pretraining or finetuning, comparing methods in the open and to the best existing models, and earning public recognition. However, with success, both in the leaderboard and the increasing performances of the models came challenges. After one intense year and a lot of community feedback, Huggingface thought it was time for an upgrade! Here is the link to the [Open LLM Leaderboard v2!](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)

19. ***Salesforce.AI's APIGen Paper:<br>
Salesforce.AI published a paper on APIGen, an automated pipeline for generating verifiable and diverse function-calling datasets. APIGen collects and verifies data across 21 categories, producing high-quality datasets for function-calling applications. Models trained on these datasets achieve state-of-the-art performance, demonstrating APIGen's potential to advance function-calling agent domains.*** <br><br>
    26 Jun, Saleforce.AI published a [paper](https://arxiv.org/pdf/2406.18518) “APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets”. The advancement of function-calling agent models requires diverse, reliable, and high-quality datasets. This paper presents APIGen, an automated data generation pipeline designed to synthesize verifiable high-quality datasets for function-calling applications. The authors leverage APIGen and collect 3,673 executable APIs across 21 different categories to generate diverse function-calling datasets in a scalable and structured manner. Each data in the dataset is verified through three hierarchical stages: format checking, actual function executions, and semantic verification, ensuring its reliability and correctness. The work demonstrates that models trained with the curated datasets, even with only 7B parameters, can achieve state-of-the-art performance on the Berkeley Function-Calling Benchmark, outperforming multiple GPT-4 models. Moreover, the 1B model achieves exceptional performance, surpassing GPT-3.5-Turbo and Claude-3 Haiku. The authors release a dataset containing 60,000 high-quality entries, aiming to advance the field of function-calling agent domains. The dataset is available on Huggingface: https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k and the project homepage: https://apigen-pipeline.github.io/

21. ***WildGuard Safety Moderation Tool:<br>
The University of Washington and Seoul National University introduced WildGuard, an open moderation tool for LLM safety. WildGuard identifies malicious intent, detects safety risks, and evaluates model refusal rates. It achieves state-of-the-art performance in safety moderation, reducing jailbreak success rates significantly. The model and data are open-sourced for community use.*** <br><br>
    26 Jun, Uni of Washington and Seoul National Uni published a [paper](https://arxiv.org/pdf/2406.18495) “WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs”. The paper introduces WildGuard -- an open, light-weight moderation tool for LLM safety that achieves three goals: (1) identifying malicious intent in user prompts, (2) detecting safety risks of model responses, and (3) determining model refusal rate. Together, WildGuard serves the increasing needs for automatic safety moderation and evaluation of LLM interactions, providing a one-stop tool with enhanced accuracy and broad coverage across 13 risk categories. While existing open moderation tools such as Llama-Guard2 score reasonably well in classifying straightforward model interactions, they lag far behind a prompted GPT-4, especially in identifying adversarial jailbreaks and in evaluating models' refusals, a key measure for evaluating safety behaviors in model responses. To address these challenges, the study constructs WildGuardMix, a large-scale and carefully balanced multi-task safety moderation dataset with 92K labeled examples that cover vanilla (direct) prompts and adversarial jailbreaks, paired with various refusal and compliance responses. WildGuardMix is a combination of WildGuardTrain, the training data of WildGuard, and WildGuardTest, a high-quality human-annotated moderation test set with 5K labeled items covering broad risk scenarios. Through extensive evaluations on WildGuardTest and ten existing public benchmarks, the paper shows that WildGuard establishes state-of-the-art performance in open-source safety moderation across all the three tasks compared to ten strong existing open-source moderation models (e.g., up to 26.4% improvement on refusal detection). Importantly, WildGuard matches and sometimes exceeds GPT-4 performance (e.g., up to 3.9% improvement on prompt harmfulness identification). WildGuard serves as a highly effective safety moderator in an LLM interface, reducing the success rate of jailbreak attacks from 79.8% to 2.4%. The authors open-sourced the [model](https://github.com/allenai/wildguard) and [data](https://huggingface.co/datasets/allenai/wildguardmix).

23. ***Responsible Foundation Model Development Cheatsheet:<br>
A paper by 13 institutions, including MIT and Google, presents the "Responsible Foundation Model Development Cheatsheet," a collection of 250+ tools and resources for responsible AI development. The paper highlights gaps in current practices, such as inadequate tools for model evaluation and ethical considerations, and aims to guide more responsible foundation model development.*** <br><br>
    26 Jun, 13 institutes including MIT, EleutherAI, Princeton Uni, Stanford Uni, Google, Huggaingface et al. published a [paper](https://arxiv.org/pdf/2406.16746) “The Responsible Foundation Model Development Cheatsheet: A Review of Tools & Resources”. Foundation model development attracts a rapidly expanding body of contributors, scientists, and applications. To help shape responsible development practices, the paper introduces the Foundation Model Development Cheatsheet: a growing collection of 250+ tools and resources spanning text, vision, and speech modalities. The authors draw on a large body of prior work to survey resources (e.g. software, documentation, frameworks, guides, and practical tools) that support informed data selection, processing, and understanding, precise and limitation-aware artifact documentation, efficient model training, advance awareness of the environmental impact from training, careful model evaluation of capabilities, risks, and claims, as well as responsible model release, licensing and deployment practices. The authors hope this curated collection of resources helps guide more responsible development. The process of curating this list, enabled the authors to review the AI development ecosystem, revealing what tools are critically missing, misused, or over-used in existing practices. The study finds that (i) tools for data sourcing, model evaluation, and monitoring are critically under-serving ethical and real-world needs, (ii) evaluations for model safety, capabilities, and environmental impact all lack reproducibility and transparency, (iii) text and particularly English-centric analyses continue to dominate over multilingual and multi-modal analyses, and (iv) evaluation of systems, rather than just models, is needed so that capabilities and impact are assessed in context.

25. ***Real-time Video Generation System:<br> On June 26, the National University of Singapore and Purdue University introduced "Pyramid Attention Broadcast" (PAB), a system for real-time video generation using DiT-based models. PAB achieves up to 21.6 FPS with 10.6x acceleration by reducing redundant attention computation, maintaining quality across models like Open-Sora and Latte. It is training-free, allowing future DiT-based models to gain real-time capabilities.*** <br><br>
    26 Jun, Natioinal Uni of Singapore and Purde Uni released “Pyramid Attention Broadcase”, a real-time [video generation system](https://github.com/NUS-HPC-AI-Lab/OpenDiT), the first approach that achieves real-time DiT-based video generation. By mitigating redundant attention computation, PAB achieves up to 21.6 FPS with 10.6x acceleration, without sacrificing quality across popular DiT-based video generation models including Open-Sora, Open-Sora-Plan, and Latte. Notably, as a training-free approach, PAB can enpower any future DiT-based video generation models with real-time capabilities.

27. ***Introduction of FineWeb Dataset: <br>
On June 25, Huggingface released the FineWeb dataset, documented in the paper "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale." FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots, improves LLM performance compared to other datasets. FineWeb-Edu, a subset of 1.3 trillion tokens, enhances LLMs on knowledge- and reasoning-intensive benchmarks.*** <br><br>
    25 Jun, Huggingface released the [FineWeb dataset](https://huggingface.co/datasets/HuggingFaceFW/fineweb) and introduced the details of the dataset in a [paper](https://arxiv.org/pdf/2406.17557) “The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale”. The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. This work introduces FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, the work carefully documents and ablates all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. In addition, th paper introduces FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with the datasets, Hugginkgface publicly releases the data curation codebase and all of the models trained during the ablation experiments.

29. ***Memorization in Language Models: <br>On June 25, EleutherAI, Microsoft, NYU, Google, and Harvard University published "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon." The paper proposes a taxonomy for memorization: recitation of highly duplicated sequences, reconstruction of predictable sequences, and recollection of neither. This taxonomy helps build predictive models for memorization by analyzing dependencies and factor influences.*** <br><br>
    25 Jun, EleutherAI, Microsoft, NYU, Google, Harvard Uni et al published a [paper](https://arxiv.org/pdf/2406.17746) “Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon”. Memorization in language models is typically treated as a homogenous phenomenon, neglecting the specifics of the memorized data. The paper instead models memorization as the effect of a set of complex factors that describe each sample and relate it to the model and corpus. To build intuition around these factors, the authors break memorization down into a taxonomy: recitation of highly duplicated sequences, reconstruction of inherently predictable sequences, and recollection of sequences that are neither. The authors demonstrate the usefulness of the taxonomy by using it to construct a predictive model for memorization. By analyzing dependencies and inspecting the weights of the predictive model, the study finds that different factors influence the likelihood of memorization differently depending on the taxonomic category.

31. ***Continual Learning in LMs: <br>On June 25, the University of Hong Kong, Edinburgh, and NVIDIA introduced "MIGU" (Magnitude-based Gradient Updating) in the paper "Unlocking Continual Learning Abilities in Language Models." MIGU, a rehearsal-free method, updates model parameters with large magnitudes in LMs' linear layers, significantly improving continual learning performance across various benchmarks.*** <br><br>
    25 Jun, Uni of HongKong, Edinb urgh, NVIDIA, et al. published a [paper](https://arxiv.org/pdf/2406.17245) “Unlocking Continual Learning Abilities in Language Models”. Language models (LMs) exhibit impressive performance and generalization capabilities. However, LMs struggle with the persistent challenge of catastrophic forgetting, which undermines their long-term sustainability in continual learning (CL). Existing approaches usually address the issue by incorporating old task data or task-wise inductive bias into LMs. However, old data and accurate task information are often unavailable or costly to collect, hindering the availability of current CL approaches for LMs. To address this limitation, the study introduces MIGU (MagnItude-based Gradient Updating for continual learning), a rehearsal-free and task-label-free method that only updates the model parameters with large magnitudes of output in LMs' linear layers. MIGU is based on the authors observation that the L1-normalized magnitude distribution of the output in LMs' linear layers is different when the LM models deal with different task data. By imposing this simple constraint on the gradient update process, the work can leverage the inherent behaviors of LMs, thereby unlocking their innate CL abilities. The experiments demonstrate that MIGU is universally applicable to all three LM architectures (T5, RoBERTa, and Llama2), delivering state-of-the-art or on-par performance across continual finetuning and continual pre-training settings on four CL benchmarks. For example, MIGU brings a 15.2% average accuracy improvement over conventional parameter-efficient finetuning baselines in a 15-task CL benchmark. MIGU can also seamlessly integrate with all three existing CL types to further enhance performance. Code is available at https://github.com/wenyudu/MIGU{this https URL}.

33. ***Scaling Laws for Linear Complexity Models: <br>On June 24, OpenNLPLab, ANU, and TapTap published "Scaling Laws for Linear Complexity Language Models." The study examines the scaling behaviors of linear architectures like TNL, HGRN2, and cosFormer2, demonstrating that linear complexity models exhibit similar scaling capabilities to conventional transformer-based models with superior linguistic proficiency.*** <br><br>
    24 Jun, OpenNLPLab, ANU and TapTap published a [paper](https://arxiv.org/pdf/2406.16690) “Scaling Laws for Linear Complexity Language Models”. The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. This paper presents the scaling laws for linear complexity language models to establish a foundation for their scalability. Specifically, the work examines the scaling behaviors of three efficient linear architectures. These include TNL, a linear attention model with data-independent decay; HGRN2, a linear RNN with data-dependent decay; and cosFormer2, a linear attention model without decay. The study also includes LLaMA as a baseline architecture for softmax attention for comparison. These models were trained with six variants, ranging from 70M to 7B parameters on a 300B-token corpus, and evaluated with a total of 1,376 intermediate checkpoints on various downstream tasks. These tasks include validation loss, commonsense reasoning, and information retrieval and generation. The study reveals that existing linear complexity language models exhibit similar scaling capabilities as conventional transformer-based models while also demonstrating superior linguistic proficiency and knowledge retention.

35. ***WARP for Policy Alignment:<br> On June 24, Google introduced WARP (Weight Averaged Rewarded Policies) in the paper "On the Benefits of Weight Averaged Rewarded Policies." WARP merges policies in the weight space at different stages to improve reinforcement learning from human feedback, achieving superior rewards and alignment in GEMMA policies compared to other open-source LLMs.*** <br><br>
    24 Jun, Google published a [paper](https://arxiv.org/pdf/2406.16768) “WARP: On the Benefits of Weight Averaged Rewarded Policies”. Reinforcement learning from human feedback (RLHF) aligns large language models (LLMs) by encouraging their generations to have high rewards, using a reward model trained on human preferences. To prevent the forgetting of pre-trained knowledge, RLHF usually incorporates a KL regularization; this forces the policy to remain close to its supervised fine-tuned initialization, though it hinders the reward optimization. To tackle the trade-off between KL and reward, this paper introduces a novel alignment strategy named Weight Averaged Rewarded Policies (WARP). WARP merges policies in the weight space at three distinct stages. First, it uses the exponential moving average of the policy as a dynamic anchor in the KL regularization. Second, it applies spherical interpolation to merge independently fine-tuned policies into a new enhanced one. Third, it linearly interpolates between this merged model and the initialization, to recover features from pre-training. This procedure is then applied iteratively, with each iteration's final model used as an advanced initialization for the next, progressively refining the KL-reward Pareto front, achieving superior rewards at fixed KL. Experiments with GEMMA policies validate that WARP improves their quality and alignment, outperforming other open-source LLMs.

37. ***Mitigating Lost-in-the-Middle Problem: <br> On June 23, the University of Washington, MIT, and Google published "Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization." The paper addresses the lost-in-the-middle problem by calibrating LLMs' positional attention bias, leading to improved performance in locating relevant information within long contexts and enhancing retrieval-augmented generation.*** <br><br>
    23 Jun, Uni of Washington, MIT, and Google published a [paper](https://arxiv.org/pdf/2406.16008) “Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization”. Large language models (LLMs), even when specifically trained to process long input contexts, struggle to capture relevant information located in the middle of their input. This phenomenon has been known as the lost-in-the-middle problem. This paper makes three contributions. First, it sets out to understand the factors that cause this phenomenon. In doing so, the study establishes a connection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs exhibit a U-shaped attention bias where the tokens at the beginning and at the end of its input receive higher attention, regardless of their relevance. Second, the study mitigates this positional bias through a calibration mechanism, found-in-the-middle, that allows the model to attend to contexts faithfully according to their relevance, even though when they are in the middle. Third, the study shows found-in-the-middle not only achieves better performance in locating relevant information within a long context, but also eventually leads to improved retrieval-augmented generation (RAG) performance across various tasks, outperforming existing methods by up to 15 percentage points. These findings open up future directions in understanding LLM attention bias and its potential consequences.

39. ***Few-shot Learning in Long Contexts: <br> On June 23, Bar-Ilan University and Google published "Can Few-shot Work in Long-Context? Recycling the Context to Generate Demonstrations." The study proposes generating few-shot examples from long contexts to enhance LLM performance in long-context QA tasks, achieving substantial improvements across various datasets.*** <br><br>
    23 Jun, Bar-Ilan Uni and Google published a [paper](https://arxiv.org/pdf/2406.13632) “Can Few-shot Work in Long-Context? Recycling the Context to Generate Demonstrations”. Despite recent advancements in Large Language Models (LLMs), their performance on tasks involving long contexts remains sub-optimal. In-Context Learning (ICL) with few-shot examples may be an appealing solution to enhance LLM performance in this scenario; However, naively adding ICL examples with long context introduces challenges, including substantial token overhead added for each few-shot example and context mismatch between the demonstrations and the target query. This work proposes to automatically generate few-shot examples for long context QA tasks by recycling contexts. Specifically, given a long input context (1-3k tokens) and a query, the authors generate additional query-output pairs from the given context as few-shot examples, while introducing the context only once. This ensures that the demonstrations are leveraging the same context as the target query while only adding a small number of tokens to the prompt. The paper further enhances each demonstration by instructing the model to explicitly identify the relevant paragraphs before the answer, which improves performance while providing fine-grained attribution to the answer source. The study applies the method on multiple LLMs and obtain substantial improvements (+23\% on average across models) on various QA datasets with long context, especially when the answer lies within the middle of the context. Surprisingly, despite introducing only single-hop ICL examples, LLMs also successfully generalize to multi-hop long-context QA using our approach.

41. ***Detecting Hallucinations in LLMs: <br>On June 22, the University of Oxford introduced semantic entropy probes (SEPs) in the paper "Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs." SEPs provide a low-cost method for detecting hallucinations by approximating semantic entropy from hidden states, retaining high performance and generalizing better to out-of-distribution data.*** <br><br>
    22 Jun, Uni of Oxford published a [paper](https://arxiv.org/pdf/2406.15927) “Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs”. The paper proposes semantic entropy probes (SEPs), a cheap and reliable method for uncertainty quantification in Large Language Models (LLMs). Hallucinations, which are plausible-sounding but factually incorrect and arbitrary model generations, present a major challenge to the practical adoption of LLMs. Recent work by Farquhar et al. (2024) proposes semantic entropy (SE), which can detect hallucinations by estimating uncertainty in the space semantic meaning for a set of model generations. However, the 5-to-10-fold increase in computation cost associated with SE computation hinders practical adoption. To address this, the authors propose SEPs, which directly approximate SE from the hidden states of a single generation. SEPs are simple to train and do not require sampling multiple model generations at test time, reducing the overhead of semantic uncertainty quantification to almost zero. The work shows that SEPs retain high performance for hallucination detection and generalize better to out-of-distribution data than previous probing methods that directly predict model accuracy. The results across models and tasks suggest that model hidden states capture SE, and the ablation studies give further insights into the token positions and model layers for which this is the case.

43. ***GraphReader for Long-context Tasks: <br>On June 20, Alibaba Group, the University of Manchester, and others published "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models." GraphReader structures long texts into graphs and employs an agent to explore them, outperforming GPT-4-128k in processing long contexts and excelling in multiple benchmarks.*** <br><br>
    20 Jun, Alibaba Group, Uni of Manchester et al published a [paper](https://arxiv.org/pdf/2406.14550) “GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models”. The paper argues that long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. Despite numerous efforts made to optimize LLMs for long contexts, challenges persist in robustly processing long inputs. This paper introduces GraphReader, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. It then invokes a set of predefined functions to read node content and neighbors, facilitating a coarse-to-fine exploration of the graph. Throughout the exploration, the agent continuously records new insights and reflects on current circumstances to optimize the process until it has gathered sufficient information to generate an answer. Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin. Additionally, the approach demonstrates superior performance on four challenging single-hop and multi-hop benchmarks.

45. ***ICAL for Multimodal Agents: <br>On June 20, CMU and Google introduced In-Context Abstraction Learning (ICAL) in the paper "ICAL: Continual Learning of Multimodal Agents by Transforming Trajectories into Actionable Insights." ICAL builds memory from sub-optimal demonstrations, improving decision-making in multimodal agents and surpassing state-of-the-art in various benchmarks.*** <br><br>
    20 Jun, CMU and Google published a [paper](https://arxiv.org/pdf/2406.14596) “ICAL: Continual Learning of Multimodal Agents by Transforming Trajectories into Actionable Insights”. Large-scale generative language and vision-language models (LLMs and VLMs) excel in few-shot in-context learning for decision making and instruction following. However, they require high-quality exemplar demonstrations to be included in their context window. Can LLMs and VLMs generate their own prompt examples from generic, sub-optimal demonstrations? The study proposes In-Context Abstraction Learning (ICAL), a method that builds a memory of multimodal experience insights from sub-optimal demonstrations and human feedback. Given a noisy demonstration in a new domain, VLMs abstract the trajectory into a general program by fixing inefficient actions and annotating cognitive abstractions: task relationships, object state changes, temporal subgoals, and task construals. These abstractions are refined and adapted interactively through human feedback while the agent attempts to execute the trajectory in a similar environment. The resulting abstractions, when used as exemplars in the prompt, significantly improve decision-making in retrieval-augmented LLM and VLM agents. The ICAL agent surpasses the state-of-the-art in dialogue-based instruction following in TEACh, multimodal web agents in VisualWebArena, and action anticipation in Ego4D. In TEACh, ICAL achieves a 12.6% improvement in goal-condition success. In VisualWebArena, the task success rate improves over the SOTA from 14.3% to 22.7%. In Ego4D action forecasting, ICAL improves over few-shot GPT-4V and remains competitive with supervised models. The authors show finetuning the retrieval-augmented in-context agent yields additional improvements. The approach significantly reduces reliance on expert-crafted examples and consistently outperforms in-context learning from action plans that lack such insights.

47. ***DataComp-LM for Training Sets: <br>On June 20, 23 organizations including the University of Washington, Apple, and Stanford published "DataComp-LM: In search of the next generation of training sets for language models." The paper introduces a testbed for dataset experiments, providing a standardized corpus and effective pretraining recipes, significantly improving language model performance.*** <br><br>
    20 Jun, 23 ogranizations include Uni of Washington, Apple, Stanford, UCLA, CMU, Harvard et al. published a [paper](https://arxiv.org/pdf/2406.11794v1) “DataComp-LM: In search of the next generation of training sets for language models”. The paper introduces DataComp for Language Models (DCLM), a testbed for controlled dataset experiments with the goal of improving language models. As part of DCLM, the work provides a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations. Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing at model scales ranging from 412M to 7B parameters. As a baseline for DCLM, the authors conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set. The resulting dataset, DCLM-Baseline enables training a 7B parameter language model from scratch to 64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-Baseline represents a 6.6 percentage point improvement on MMLU while being trained with 40% less compute. The baseline model is also comparable to Mistral-7B-v0.3 and Llama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53 natural language understanding tasks while being trained with 6.6x less compute than Llama 3 8B. The results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation.

49. ***Investigating Reward-Tampering: <br>On June 17, Anthropic, Redwood Research, and the University of Oxford published "Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models." The study examines specification gaming in LLMs, finding that LLMs can generalize from simple forms to pernicious reward-tampering behaviors, which are challenging to eliminate.*** <br><br>
    17 Jun, Anthropic, Redwood research and Uni of Oxford published a [paper](https://arxiv.org/pdf/2406.10162) “Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models”. In reinforcement learning, specification gaming occurs when AI systems learn undesired behaviors that are highly rewarded due to misspecified training goals. Specification gaming can range from simple behaviors like sycophancy to sophisticated and pernicious behaviors like reward-tampering, where a model directly modifies its own reward mechanism. However, these more pernicious behaviors may be too complex to be discovered via exploration. This work studies whether Large Language Model (LLM) assistants which find easily discovered forms of specification gaming will generalize to perform rarer and more blatant forms, up to and including reward-tampering. The authors construct a curriculum of increasingly sophisticated gameable environments and find that training on early-curriculum environments leads to more specification gaming on remaining environments. Strikingly, a small but non-negligible proportion of the time, LLM assistants trained on the full curriculum generalize zero-shot to directly rewriting their own reward function. Retraining an LLM not to game early-curriculum environments mitigates, but does not eliminate, reward-tampering in later environments. Moreover, adding harmlessness training to the gameable environments does not prevent reward-tampering. These results demonstrate that LLMs can generalize from common forms of specification gaming to more pernicious reward tampering and that such behavior may be nontrivial to remove.

51. ***LLMs and Programming by Example: <br>On June 13, Cornell University published "Is Programming by Example solved by LLMs?" The paper investigates the effectiveness of LLMs in programming-by-example tasks, finding that while pretrained models struggle, fine-tuning improves performance for in-distribution tasks, highlighting areas for further improvement in out-of-distribution generalization.*** <br><br>
    13 Jun, Cornell Uni published a [paper](https://arxiv.org/pdf/2406.08316) “Is Programming by Example solved by LLMs?”. Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples. Such systems are practically and theoretically important: from an end-user perspective, they are deployed to millions of people, and from an AI perspective, PBE corresponds to a very general form of few-shot inductive inference. Given the success of Large Language Models (LLMs) in code-generation tasks, the paper investigates here the extent to which LLMs can be said to have `solved' PBE. The authors experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data. The study finds that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution. The work analyzes empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization. Collectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short.
 <br><br><br>

***23 June 2024***
1. ***Nature published a paper asserting that language is primarily a tool for communication rather than thought, challenging the view that language is used for thinking. The authors review neuroscience evidence, discuss the brain network supporting linguistic ability, and highlight the dissociation between language and thought. They conclude that language enhances cultural transmission but is not essential for complex thought.*** <br><br>
   20 Jun, Nature published a perspectives [paper](https://www.nature.com/articles/s41586-024-07522-w) “Language is primarily a tool for communication rather than thought”. The article argues that language is a defining characteristic of our species, but the function, or functions, that it serves has been debated for centuries. Here the paper brings recent evidence from neuroscience and allied disciplines to argue that in modern humans, language is a tool for communication, contrary to a prominent view that we use language for thinking. The authors begin by introducing the brain network that supports linguistic ability in humans; and then review evidence for a double dissociation between language and thought, and discuss several properties of language that suggest that it is optimized for communication. The article concludes that although the emergence of language has unquestionably transformed human culture, language does not appear to be a prerequisite for complex thought, including symbolic thought. Instead, language is a powerful tool for the transmission of cultural knowledge; it plausibly co-evolved with our thinking and reasoning capacities, and only reflects, rather than gives rise to, the signature sophistication of human cognition.

3. ***OpenAI’s co-founder and former chief scientist, Ilya Sutskever, announced a new AI company, Safe Superintelligence Inc. (SSI), focused on creating a safe and powerful AI system. SSI aims to prioritize safety and avoid external pressures common in companies like OpenAI, Google, and Microsoft. The startup is co-founded by Daniel Gross and Daniel Levy.*** <br><br>
   20 Jun, [according to theverge](https://www.theverge.com/2024/6/19/24181870/openai-former-chief-scientist-ilya-sutskever-ssi-safe-superintelligence), OpenAI’s former chief scientist is starting a new AI company. Ilya Sutskever, OpenAI’s co-founder and former chief scientist, is starting a new AI company focused on safety. In a post on Wednesday, Sutskever revealed Safe Superintelligence Inc. (SSI), a startup with “one goal and one product:” creating a safe and powerful AI system. The announcement describes SSI as a startup that “approaches safety and capabilities in tandem,” letting the company quickly advance its AI system while still prioritizing safety. It also calls out the external pressure AI teams at companies like OpenAI, Google, and Microsoft often face, saying the company’s “singular focus” allows it to avoid “distraction by management overhead or product cycles.” “Our business model means safety, security, and progress are all insulated from short-term commercial pressures,” the announcement reads. “This way, we can scale in peace.” In addition to Sutskever, SSI is co-founded by Daniel Gross, a former AI lead at Apple, and Daniel Levy, who previously worked as a member of technical staff at OpenAI.

5. ***Anthropic released a new AI model, Claude 3.5 Sonnet, which outperforms its predecessors in text and image analysis. Although it shows incremental improvements, benchmarks indicate it slightly surpasses OpenAI’s GPT-4o. Alongside, Anthropic introduces Artifacts, a workspace for editing and collaborating on AI-generated content, currently in preview.*** <br><br>
   20 Jun, according to [Techcrunch](https://techcrunch.com/2024/06/20/anthropic-claims-its-latest-model-is-best-in-class/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAKCTE0J0flCCoJ5P-8CUs6YaU1aM0DWW5XxwYMOv03y8jB23CxO0liJfryMwrZJoG2ftxPh2G5HIROITByHXemmw1zswEUejwLnTuVw6bhFj2EOwvfGw2J2i2kMoeC2dosP9fWW22vAyEG1Fo7jRag7baRY8yTACJInjWRN_gjxo#:~:text=OpenAI%20rival%20Anthropic%20is%20releasing,yet%20%E2%80%94%20at%20least%20on%20paper.), Anthropic claims its latest model is best-in-class. OpenAI rival Anthropic is releasing a powerful new generative AI model called Claude 3.5 Sonnet. But it’s more an incremental step than a monumental leap forward. Claude 3.5 Sonnet can analyze both text and images as well as generate text, and it’s Anthropic’s best-performing model yet — at least on paper. Across several AI benchmarks for reading, coding, math and vision, Claude 3.5 Sonnet outperforms the model it’s replacing, Claude 3 Sonnet, and beats Anthropic’s previous flagship model Claude 3 Opus. Benchmarks aren’t necessarily the most useful measure of AI progress, in part because many of them test for esoteric edge cases that aren’t applicable to the average person, like answering health exam questions. But for what it’s worth, Claude 3.5 Sonnet just barely bests rival leading models, including OpenAI’s recently launched GPT-4o, on some of the benchmarks Anthropic tested it against. Alongside the new model, Anthropic is releasing what it’s calling Artifacts, a workspace where users can edit and add to content — e.g. code and documents — generated by Anthropic’s models. Currently in preview, Artifacts will gain new features, like ways to collaborate with larger teams and store knowledge bases, in the near future, Anthropic says.

7. ***Researchers from Nature published a paper on detecting hallucinations in large language models (LLMs) using semantic entropy. They propose entropy-based uncertainty estimators to detect confabulations—arbitrary incorrect generations—addressing the need for reliable methods to enhance the trustworthiness of LLMs across various tasks without task-specific data.*** <br><br>
   19 Jun, Nature published a [paper](https://www.nature.com/articles/s41586-024-07421-0) “Detecting hallucinations in large language models using semantic entropy”. The researchers believe that Large language model (LLM) systems, such as ChatGPT or Gemini, can show impressive reasoning and question-answering capabilities but often ‘hallucinate’ false outputs and unsubstantiated answers. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents or untrue facts in news articles and even posing a risk to human life in medical domains such as radiology. Encouraging truthfulness through supervision or reinforcement has been only partially successful. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. The work develops new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations—confabulations—which are arbitrary and incorrect generations. The method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. The method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, the method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.

9. ***Researchers from the University of Groningen and the University of Amsterdam introduced MIRAGE, a method for ensuring verifiability in retrieval-augmented generation (RAG) systems. MIRAGE uses model internals to attribute answers to their sources accurately, improving upon previous self-citation methods by providing more faithful and context-sensitive attributions.*** <br><br>
    19 Jun, Uni of Groningen and Uni of Amsterdam published a [paper](https://arxiv.org/pdf/2406.13663v1) “Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation”.  The paper argues that ensuring the verifiability of model answers is a fundamental challenge for retrieval-augmented generation (RAG) in the question answering (QA) domain. Recently, self-citation prompting was proposed to make large language models (LLMs) generate citations to supporting documents along with their answers. However, self-citing LLMs often struggle to match the required format, refer to non-existent sources, and fail to faithfully reflect LLMs' context usage throughout the generation. This work presents MIRAGE --Model Internals-based RAG Explanations -- a plug-and-play approach using model internals for faithful answer attribution in RAG applications. MIRAGE detects context-sensitive answer tokens and pairs them with retrieved documents contributing to their prediction via saliency methods. The authors evaluate the proposed approach on a multilingual extractive QA dataset, finding high agreement with human answer attribution. On open-ended QA, MIRAGE achieves citation quality and efficiency comparable to self-citation while also allowing for a finer-grained control of attribution parameters. The qualitative evaluation highlights the faithfulness of MIRAGE's attributions and underscores the promising application of model internals for RAG answer attribution.

11. ***MainFunc, a startup founded by alumni from Microsoft, Google, and Baidu, released Genspark, an "AI Agent Engine" aimed at enhancing search experiences. With $60 million in seed funding, Genspark offers a new webpage format called Sparkpage, which consolidates web knowledge and includes an AI-powered assistant to guide users.*** <br><br>
    18 Jun, a start-up named [MainFunc](https://mainfunc.ai/#about_us), released its first product, [Genspark](https://www.genspark.ai/), which is the "AI Agent Engine" designed to provide a significantly better search experience using AI. MainFunc was founded by alumni from Microsoft, Google, and Baidu who are passionate about building world-class AI product innovations for a better world. The company raised a total of $60 million in its seed round, led by Singapore-based Lanchi Ventures and supported by other global angel investors. Sparkpage is a revolutionary new form of webpage that transforms how you interact with information online: Distillation and Consolidation - Each Sparkpage distills and consolidates a wealth of web knowledge into a single, cohesive unit. We are committed to providing content that is both informative and impartial, free from commercial influences or business biases. Built-in AI-Powered Copilot - Every Sparkpage is equipped with a built-in AI copilot, meticulously designed to guide and assist you in effortlessly expanding your knowledge. This intelligent assistant responds dynamically to your inquiries, offering insights and information tailored to your needs.

13. ***Google published a paper evaluating language models' probabilistic reasoning abilities. The study assessed state-of-the-art models on tasks like estimating percentiles and calculating probabilities using statistical distributions. It found that models improve with contextual anchors, real-world contexts, and summary statistics, advancing their probabilistic reasoning capabilities.*** <br><br>
    18 Jun, Google published a [paper](https://arxiv.org/pdf/2406.12830) “What Are the Odds? Language Models Are Capable of Probabilistic Reasoning”. The paper argues that Language models (LM) are capable of remarkably complex linguistic tasks; however, numerical reasoning is an area in which they frequently struggle. An important but rarely evaluated form of reasoning is understanding probability distributions. This paper focuses on evaluating the probabilistic reasoning capabilities of LMs using idealized and real-world statistical distributions. The study performs a systematic evaluation of state-of-the-art LMs on three tasks: estimating percentiles, drawing samples, and calculating probabilities. The authors evaluate three ways to provide context to LMs 1) anchoring examples from within a distribution or family of distributions, 2) real-world context, 3) summary statistics on which to base a Normal approximation. Models can make inferences about distributions, and can be further aided by the incorporation of real-world context, example shots and simplified assumptions, even if these assumptions are incorrect or misspecified. To conduct this work, the authors developed a comprehensive benchmark distribution dataset with associated question-answer pairs that will be released publicly.

15. ***Nvidia became the world’s most valuable company, surpassing Microsoft, due to its critical role in AI computing. Nvidia’s market capitalization reached $3.335 trillion, driven by optimism about AI technology. Despite concerns over AI investment sustainability, Nvidia’s stock split and market performance boosted major stock indices.*** <br><br>
    18 Jun, according to [abc news](https://www.abc.net.au/news/2024-06-19/nvidia-overtakes-microsoft-to-become-most-valuable-company/103995316), Nvidia becomes world's most valuable company, toppling Microsoft amid pursuit of AI domination. Technology multinational Nvidia has become the world's most valuable company, dethroning tech heavyweight Microsoft as its high-end processors play a central role in a scramble to dominate artificial intelligence computing. Shares climbed 3.5 per cent to $US135.58 ($203.60) in the US on Tuesday, local time, lifting its market capitalisation to $US3.335 trillion just days after overtaking Apple to become the second most valuable company. Nvidia's stunning surge over the past year has become emblematic of a Wall Street frenzy driven by optimism about emerging AI technology. Sharp increases in analysts' expectations for Nvidia's future earnings have outpaced its stellar stock gains, resulting in a fall in the stock's earnings valuation. Increasing the appeal for its highly valued stock among individual investors, Nvidia last week split its stock 10 for one. But while Nvidia's rally has lifted the S&P 500 and Nasdaq to record highs, some investors worry unbridled optimism about AI could evaporate if signs emerge of a slowdown in spending on the technology.

17. ***A paper by MIT, Microsoft, and the University of Maryland explored how language models use external context in Retrieval Augmented Generation (RAG) systems. The study found that models prefer context over parametric memory when answering questions. Techniques like Causal Mediation Analysis revealed this bias, providing insights into the RAG pipeline's mechanisms.*** <br><br>
    18 Jun, MIT, Microsoft and Uni of Maryland published a [paper](https://arxiv.org/pdf/2406.12824) “From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries”.  The authors indicate that Retrieval Augmented Generation (RAG) enriches the ability of language models to reason using external context to augment responses for a given user prompt. This approach has risen in popularity due to practical applications in various applications of language models in search, question/answering, and chat-bots. However, the exact nature of how this approach works isn't clearly understood. This paper mechanistically examines the RAG pipeline to highlight that language models take shortcut and have a strong bias towards utilizing only the context information to answer the question, while relying minimally on their parametric memory. The study probes this mechanistic behavior in language models with: (i) Causal Mediation Analysis to show that the parametric memory is minimally utilized when answering a question and (ii) Attention Contributions and Knockouts to show that the last token residual stream do not get enriched from the subject token in the question, but gets enriched from other informative tokens in the context. The authors find this pronounced shortcut behaviour true across both LLaMa and Phi family of models.

19. ***Researchers from the University of Washington, Harvard, and Stanford presented TabuLa-8B, a language model for tabular data prediction. Trained on a large dataset from the TabLib corpus, TabuLa-8B outperforms existing models in zero-shot and few-shot settings. The model demonstrates significant accuracy improvements for tabular data prediction tasks.*** <br><br>
    17 Jun, Uni of Washington, Harvard and Stanford Uni published a [paper](https://arxiv.org/pdf/2406.12031) “Large Scale Transfer Learning for Tabular Data via Language Modeling”. The authors argue that tabular data -- structured, heterogeneous, spreadsheet-style data with rows and columns -- is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and predictors in domains such as language modeling and computer vision, this transfer learning paradigm has not had similar impact in the tabular domain. This work seeks to narrow this gap and present TabuLa-8B, a language model for tabular prediction. The study defines a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control. Using the resulting dataset, which comprises over 1.6B rows from 3.1M unique tables, the authors fine-tune a Llama 3-8B large language model (LLM) for tabular data prediction (classification and binned regression) using a novel packing and attention scheme for tabular prediction. Through evaluation across a test suite of 329 datasets, the authors find that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g. XGBoost, TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN models that are explicitly trained on equal, or even up to 16x more data. Model, code, and data along with the publication of this paper is [publicly available](https://github.com/mlfoundations/tabliblib).

21. ***ServiceNow and Mila introduced RepLiQA, a new dataset for evaluating language models on unseen reference content. RepLiQA consists of human-crafted documents and questions designed to test models' ability to find answers within provided content. Initial benchmarks revealed performance differences among state-of-the-art language models in a context-conditional setting.*** <br><br>
    17 Jun,  ServiceNow and Mila published a [paper](https://arxiv.org/pdf/2406.11811v1) “RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content”. The paper argues that Large Language Models (LLMs) are trained on vast amounts of data, most of which is automatically scraped from the internet. This data includes encyclopedic documents that harbor a vast amount of general knowledge (e.g., Wikipedia) but also potentially overlap with benchmark datasets used for evaluating LLMs. Consequently, evaluating models on test splits that might have leaked into the training set is prone to misleading conclusions. To foster sound evaluation of language models, the authors introduce a new test dataset named RepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a collection of five splits of test sets, four of which have not been released to the internet or exposed to LLM APIs prior to this publication. Each sample in RepLiQA comprises (1) a reference document crafted by a human annotator and depicting an imaginary scenario (e.g., a news article) absent from the internet; (2) a question about the document's topic; (3) a ground-truth answer derived directly from the information in the document; and (4) the paragraph extracted from the reference document containing the answer. As such, accurate answers can only be generated if a model can find relevant content within the provided document. We run a large-scale benchmark comprising several state-of-the-art LLMs to uncover differences in performance across models of various types and sizes in a context-conditional language modeling setting. Released splits of RepLiQA can be found here: https://huggingface.co/datasets/ServiceNow/repliqa.

23. ***A paper from Harvard, UC Santa Barbara, and Google discussed "transcendence" in generative models, where AI models surpass the expertise of their human trainers. The study showed that a trained transformer model for chess could outperform its training data's best players, enabled by low-temperature sampling. This phenomenon was theoretically and experimentally validated.*** <br><br>
    17 Jun, Harvard, UC Santa Barbara and Google published a [paper](https://arxiv.org/pdf/2406.11741v1) “Transcendence: Generative Models Can Outperform The Experts That Train Them”. The paper states that generative models are trained with the simple objective of imitating the conditional probability distribution induced by the data they are trained on. Therefore, when trained on data generated by humans, we may not expect the artificial model to outperform the humans on their original objectives. This work studies the phenomenon of transcendence: when a generative model achieves capabilities that surpass the abilities of the experts generating its data. The authors demonstrate transcendence by training an autoregressive transformer to play chess from game transcripts, and show that the trained model can sometimes achieve better performance than all players in the dataset. The researchers theoretically prove that transcendence is enabled by low-temperature sampling, and rigorously assess this experimentally. Finally, the paper discusses other sources of transcendence, laying the groundwork for future investigation of this phenomenon in a broader setting. The code is [available here](https://transcendence.eddie.win/).

25. ***Researchers from KAIST, UCL, and KT investigated how large language models (LLMs) acquire factual knowledge during pretraining. They found that more training data does not significantly enhance factual knowledge retention. Factors like training steps and batch sizes impact knowledge acquisition and forgetting, providing insights into LLMs' knowledge dynamics.*** <br><br>
    17 Jun, KAIST, UCL and KT published a [paper](https://arxiv.org/pdf/2406.11813v1) “How Do Large Language Models Acquire Factual Knowledge During Pretraining?”. The paper argues that despite the recent observation that large language models (LLMs) can store substantial factual knowledge, there is a limited understanding of the mechanisms of how they acquire factual knowledge through pretraining. This work addresses this gap by studying how LLMs acquire factual knowledge during pretraining. The findings reveal several important insights into the dynamics of factual knowledge acquisition during pretraining. First, counterintuitively, the authors observe that pretraining on more data shows no significant improvement in the model’s capability to acquire and maintain factual knowledge. Next, there is a power-law relationship between training steps and forgetting of memorization and generalization of factual knowledge, and LLMs trained with duplicated training data exhibit faster forgetting. Third, training LLMs with larger batch sizes can enhance the models’ robustness to forgetting. Overall, the observations suggest that factual knowledge acquisition in LLM pretraining occurs by progressively increasing the probability of factual knowledge presented in the pretraining data at each step. However, this increase is diluted by subsequent forgetting. Based on this interpretation, the paper demonstrates that it can provides plausible explanations for recently observed behaviors of LLMs, such as the poor performance of LLMs on long-tail knowledge and the benefits of deduplicating the pretraining corpus.

27. ***The CVPR 2024 conference received over 11,000 paper submissions and accepted 2,719. The conference announced several awards, including best papers and runners-up, with top topics like image and video synthesis, 3D vision, and human-related studies. Recognized works included those on generative image dynamics and rich human feedback for text-to-image generation.*** <br><br>
    17 - 21 Jun, [CVPR 2024 Seattle](https://media.eventhosts.cc/Conferences/CVPR2024/OpeningRemarkSlides.pdf), Washington State UAS. The conference has received 11532 papers and 2719 were accepted. The conference announced 4 best student paper runners-up, 2 best student papers, 2 best paper runners-up and 2 best papers, which are: 1) [Generative Image Dynamics](https://openaccess.thecvf.com/content/CVPR2024/html/Li_Generative_Image_Dynamics_CVPR_2024_paper.html) by Zhengqi Li et al from Google, and Youwei Liang et al [Rich Human Feedback for Text-to-Image Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Rich_Human_Feedback_for_Text-to-Image_Generation_CVPR_2024_paper.pdf). CVPR also announced Longuet-Higgins Prize to the paper “Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation” by Ross Girshick et al. and a list of other awards. Top three topics of submission are: Image and Video Synthesis and Generation, 3D from multi-view and sensors, and Humans: Face, body, pose, gesture movement. The top three tops of acceptance are Physics-based vision and shape-from-x, Embodied vision: active agents, simulation and Vision+Graphics.

29. ***AIRI and Tinkoff released XLand-100B, a large-scale dataset for in-context reinforcement learning. The dataset includes extensive learning histories for various tasks, aiming to advance research in this field. The dataset and utilities are open-source, providing a foundation for further scaling and democratizing research in reinforcement learning.*** <br><br>
    13 Jun, AIRI and Tinkoff published a [paper](https://arxiv.org/pdf/2406.08973v1) “XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning”. The paper states that following the success of the in-context learning paradigm in large-scale language and computer vision models, the recently emerging field of in-context reinforcement learning is experiencing a rapid growth. However, its development has been held back by the lack of challenging benchmarks, as all the experiments have been carried out in simple environments and on small-scale datasets. The authors present XLand-100B, a large-scale dataset for in-context reinforcement learning based on the XLand-MiniGrid environment, as a first step to alleviate this problem. It contains complete learning histories for nearly 30,000 different tasks, covering 100B transitions and 2.5B episodes. It took 50,000 GPU hours to collect the dataset, which is beyond the reach of most academic labs. Along with the dataset, the word provides the utilities to reproduce or expand it even further. With this substantial effort, the study aims to democratize research in the rapidly growing field of in-context reinforcement learning and provide a solid foundation for further scaling. The code is open-source and available under Apache 2.0 licence at https://github.com/dunno-lab/xland-minigrid-datasets.

31. ***Researchers from NYU, AbacusAI, and Cambridge University emphasized the need for large language models (LLMs) to recognize their knowledge limitations. The study showed that fine-tuning models on a small dataset of correct and incorrect answers enhances uncertainty estimation. This approach improves LLM reliability in high-stakes applications and human-AI collaboration.*** <br><br>
    12 Jun, NYU, AbacusAI and Cambridge Uni published a [paper](https://arxiv.org/pdf/2406.08391) “Large Language Models Must Be Taught to Know What They Don't Know”. The paper states that when using large language models (LLMs) in high-stakes applications, we need to know when we can trust their predictions. Some works argue that prompting high-performance LLMs is sufficient to produce calibrated uncertainties, while others introduce sampling methods that can be prohibitively expensive. This study first argues that prompting on its own is insufficient to achieve good calibration and then shows that fine-tuning on a small dataset of correct and incorrect answers can create an uncertainty estimate with good generalization and small computational overhead. The authors show that a thousand graded examples are sufficient to outperform baseline methods and that training through the features of a model is necessary for good performance and tractable for large open-source models when using LoRA. The authors also investigate the mechanisms that enable reliable LLM uncertainty estimation, finding that many models can be used as general-purpose uncertainty estimators, applicable not just to their own uncertainties but also the uncertainty of other models. Lastly, the paper shows that uncertainty estimates inform human use of LLMs in human-AI collaborative settings through a user study. Code available at [this https URL](https://github.com/activatedgeek/calibration-tuning).

33. ***Harvard University and Google developed a dashboard prototype to increase transparency in conversational AI. The dashboard displays the AI's internal user model, allowing users to see and control the system's behavior. User studies showed that this transparency helps identify biases and increases user control, pointing to future design and research directions.*** <br><br>
    12 Jun, Harvard Uni and Google published a [paper](https://arxiv.org/pdf/2406.07882) “Designing a Dashboard for Transparency and Control of Conversational AI”. The authors find that conversational LLMs function as black box systems, leaving users guessing about why they see the output they do. This lack of transparency is potentially problematic, especially given concerns around bias and truthfulness. To address this issue, the work presents an end-to-end prototype-connecting interpretability techniques with user experience design-that seeks to make chatbots more transparent. The authors begin by showing evidence that a prominent open-source LLM has a "user model": examining the internal state of the system, and extract data related to a user's age, gender, educational level, and socioeconomic status. Next, the authors describe the design of a dashboard that accompanies the chatbot interface, displaying this user model in real time. The dashboard can also be used to control the user model and the system's behavior. Finally, the paper discusses a study in which users conversed with the instrumented system. The results suggest that users appreciate seeing internal states, which helped them expose biased behavior and increased their sense of control. Participants also made valuable suggestions that point to future directions for both design and machine learning research. The project page and video demo of our TalkTuner system are available at https://bit.ly/talktuner-project-page.

35. ***The FAccT24 conference published a paper examining the openness of generative AI systems in light of the upcoming EU AI Act. The study highlighted the discrepancy between claimed openness and actual practices, proposing a multi-dimensional framework for assessing openness. This approach aims to foster accountability, regulation, and informed decision-making in AI development.*** <br><br>
    5 Jun, FAccT24 conference published a [paper](https://dl.acm.org/doi/pdf/10.1145/3630106.3659005) “Rethinking open source generative AI: open washing and the EU AI Act”. The paper argues that The past year has seen a steep rise in generative AI systems that claim to be open. But how open are they really? The question of what counts as open source in generative AI is poised to take on particular importance in light of the upcoming EU AI Act that regulates open source systems differently, creating an urgent need for practical openness assessment. Here the authors use an evidence-based framework that distinguishes 14 dimensions of openness, from training datasets to scientific and technical documentation and from licensing to access methods. Surveying over 45 generative AI systems (both text and text-to-image), the study finds that while the term open source is widely used, many models are ‘open weight’ at best and many providers seek to evade scientific, legal and regulatory scrutiny by withholding information on training and fine-tuning data. The authors argue that openness in generative AI is necessarily composite (consisting of multiple elements) and gradient (coming in degrees), and point out the risk of relying on single features like access or licensing to declare models open or not. Evidence-based openness assessment can help foster a generative AI landscape in which models can be effectively regulated, model providers can be held accountable, scientists can scrutinise generative AI, and end users can make informed decisions. A news article “Not all ‘open source’ AI models are actually open: here’s a ranking” also published by [Nature](https://www.nature.com/articles/d41586-024-02012-5?utm_source=substack&utm_medium=email) on 19 Jun to introduce the research.
 <br><br><br>

***17 June 2024***

1. ***Nvidia Announces Nemotron-4 340B Models:
Nvidia introduced Nemotron-4 340B, a set of open models for generating synthetic data to train large language models for commercial use. These models, which include Nemotron-4-340B-Base, Instruct, and Reward, are distributed under the NVIDIA Open Model License Agreement. They perform competitively on various benchmarks, can fit on a single DGX H100 with 8 GPUs, and are primarily aimed at generating synthetic data for smaller language models. Nvidia is also open-sourcing the synthetic data generation pipeline.*** <br><br>
   14 Jun, Nvidia announced [Nemotron-4 340B](https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf), a family of open models that developers can use to generate synthetic data for training large language models for commercial applications. The models include Nemotron-4-340B-Base, Nemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. These models are open access under the NVIDIA Open Model License Agreement, a permissive model license that allows distribution, modification, and use of the models and its outputs. These models perform competitively to open access models on a wide range of evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in FP8 precision. Nvidia believes that the community can benefit from these models in various research studies and commercial applications, especially for generating synthetic data to train smaller language models. Notably, over 98% of data used in the model alignment process is synthetically generated, showcasing the effectiveness of these models in generating synthetic data. To further support open research and facilitate model development, Nvidia are also open-sourcing the synthetic data generation pipeline used in the model alignment process.

3. ***Lamini.AI on LLM Hallucinations:
Lamini.AI's paper "Banishing LLM Hallucinations Requires Rethinking Generalization" highlights that LLMs often hallucinate despite their capabilities. The study reveals that conventional methods, such as grounding LLMs in external knowledge, fail to prevent hallucinations. The authors propose a new approach using Millions of Memory Experts (MoME) to store and retrieve facts dynamically, introducing Lamini-1, a model designed to reduce hallucinations.*** <br><br>
   13 Jun, Lamini.AI published a [paper](https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf) “Banishing LLM Hallucinations Requires Rethinking Generalization”. The authors find that despite their powerful chat, coding, and reasoning abilities, Large Language Models (LLMs) frequently hallucinate. Conventional wisdom suggests that hallucinations are a consequence of a balance between creativity and factuality, which can be mitigated, but not eliminated, by grounding the LLM in external knowledge sources. Through extensive systematic experiments, the work shows that these traditional approaches fail to explain why LLMs hallucinate in practice. Specifically, the authors show that LLMs augmented with a mixture of Millions of Memory Experts (MoME) can easily memorize large datasets of random numbers. The authors corroborate these experimental findings with a theoretical construction, showing that simple neural networks trained to predict the next token hallucinate when the training loss is above a threshold as it usually does in practice when training on internet scale data. The authors interpret the findings by comparing against traditional retrieval methods for mitigating hallucinations. The authors use the findings to design a first generation model for removing hallucinations - Lamini-1 - that stores facts in a massive mixture of millions of memory experts that are retrieved dynamically.

5. ***OpenVLA: Vision-Language-Action Model:
Stanford, UC Berkeley, Google, Physical Intelligence, and MIT released "OpenVLA," an open-source VLA model. Trained on extensive vision-language data and robot demonstrations, it outperforms existing models in visuomotor control tasks. OpenVLA, built on a Llama 2 language model and visual encoder, shows significant improvements in task success rates and generalization. The study also emphasizes its efficiency and open-source availability.*** <br><br>
   13 Jun, Stanford Uni, UC Berkeley, Google, Physical Intelligence and MIT published a [paper](https://arxiv.org/pdf/2406.09246v1) “OpenVLA: An Open-Source Vision-Language-Action Model”. The paper states that Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, the work introduces OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. The authors further show how effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. The study also explores compute efficiency; as a separate contribution, the authors also show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, the researchers release model checkpoints, fine-tuning notebooks, and the [PyTorch codebase](https://openvla.github.io/) with built-in support for training VLAs at scale on Open X-Embodiment datasets.

7. ***Google's Benchmark for LLM Temporal Reasoning:
Google's paper "Test of Time" introduces synthetic datasets to evaluate LLMs' temporal reasoning abilities, addressing limitations of existing benchmarks. These datasets allow systematic analysis of various factors affecting LLM performance in temporal reasoning tasks. The study aims to provide insights into LLMs' strengths and weaknesses in this area and is open-sourcing the datasets and evaluation framework.*** <br><br>
   13 Jun, Google published a [paper](https://arxiv.org/pdf/2406.09170) “Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning”. The paper argues that Large language models (LLMs) have showcased remarkable reasoning capabilities, yet they remain susceptible to errors, particularly in temporal reasoning tasks involving complex temporal logic. Existing research has explored LLM performance on temporal reasoning using diverse datasets and benchmarks. However, these studies often rely on real-world data that LLMs may have encountered during pre-training or employ anonymization techniques that can inadvertently introduce factual inconsistencies. This work addresses these limitations by introducing novel synthetic datasets specifically designed to assess LLM temporal reasoning abilities in various scenarios. The diversity of question types across these datasets enables systematic investigation into the impact of the problem structure, size, question type, fact order, and other factors on LLM performance. The findings provide valuable insights into the strengths and weaknesses of current LLMs in temporal reasoning tasks. To foster further research in this area, the authors are open-sourcing the datasets and evaluation framework used in the experiments: https://huggingface.co/datasets/baharef/ToT.

9. ***Transformers and Neural Algorithmic Reasoners:
Google researchers propose combining Transformers with graph neural network (GNN)-based neural algorithmic reasoners (NARs) to enhance algorithmic reasoning capabilities. The hybrid TransNAR model, evaluated on the CLRS-Text benchmark, shows significant improvements over Transformer-only models. This approach integrates robust algorithmic problem-solving with language understanding.*** <br><br>
    13 Jun, Google published a [paper](https://arxiv.org/pdf/2406.09308) “Transformers meet Neural Algorithmic Reasoners”. The researchers argue thattTransformers have revolutionized machine learning with their simple yet effective architecture. Pre-training Transformers on massive text datasets from the Internet has led to unmatched generalization for natural language understanding (NLU) tasks. However, such language models remain fragile when tasked with algorithmic forms of reasoning, where computations must be precise and robust. To address this limitation, the work proposes a novel approach that combines the Transformer's language understanding with the robustness of graph neural network (GNN)-based neural algorithmic reasoners (NARs). Such NARs proved effective as generic solvers for algorithmic tasks, when specified in graph form. To make their embeddings accessible to a Transformer, the authers propose a hybrid architecture with a two-phase training procedure, allowing the tokens in the language model to cross-attend to the node embeddings from the NAR. The authors evaluate the resulting TransNAR model on CLRS-Text, the text-based version of the CLRS-30 benchmark, and demonstrate significant gains over Transformer-only models for algorithmic reasoning, both in and out of distribution.

11. ***LiveBench: Contamination-Free LLM Benchmark:
Abacus.AI, NYU, Nvidia, UMD, and USC introduced LiveBench, a new benchmark designed to avoid test set contamination and biases from LLM or human judges. LiveBench features frequently updated questions from recent sources and objective scoring. It evaluates models on a variety of challenging tasks, releasing questions, code, and model answers to encourage community collaboration.*** <br><br>
    13 Jun, Abacus.AI, NYU, Nvidia, UMD and USC published a [paper](https://r.search.yahoo.com/_ylt=AwrOqqalx29mbwwpStwL5gt.;_ylu=Y29sbwNncTEEcG9zAzEEdnRpZAMEc2VjA3Ny/RV=2/RE=1718630438/RO=10/RU=https%3a%2f%2flivebench.ai%2flivebench.pdf%3ftrk%3dpublic_post_comment-text/RK=2/RS=CacoKTTtsURlcuTaIZI9GdDK6sI-) “LiveBench: A Challenging, Contamination-Free LLM Benchmark”. The paper argues that Test set contamination, wherein test data from a benchmark ends up in a newer model’s training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. This work introduces a new benchmark for LLMs designed to be immune to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. The authors release LiveBench, the first benchmark that (1) contains frequently updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, bAbI, and IFEval. The authors evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 110B in size. LiveBench is difficult, with top models achieving below 60% accuracy. The authors [release all questions, code, and model answers](https://github.com/LiveBench/LiveBench). Questions will be added and updated on a monthly basis, and will release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. The authors welcome community engagement and collaboration for expanding the benchmark tasks and models. 

13. ***Stability AI's Stable Diffusion 3 Medium:
Stability AI released Stable Diffusion 3 Medium, a sophisticated image generation model using three text encoders and a novel Multimodal Diffusion Transformer (MMDiT). SD3 processes text and pixel latents using a sequence of embeddings and applies a conditional flow-matching objective for training. The TensorRT-optimized version is available for download.*** <br><br>
    12 Jun, Stability AI open released [Stable Diffusion 3 Medium](https://stability.ai/news/stable-diffusion-3-medium), one of the most sophisticated image generation models. SD3 is a latent diffusion model that consists of three different text encoders (CLIP L/14, OpenCLIP bigG/14 and T5-v1.1-XXL), a novel Multimodal Diffusion Transformer (MMDiT) model, and a 16 channel AutoEncoder model that is similar to the one used in Stable Diffusion XL. SD3 processes text inputs and pixel latents as a sequence of embeddings. Positional encodings are added to 2x2 patches of the latents which are then flattened into a patch encoding sequence. This sequence, along with the text encoding sequence are fed into the MMDiT blocks, where they are embedded to a common dimensionality, concatenated, and passed through a sequence of modulated attentions and MLPs. In addition to architectural changes, SD3 applies a conditional flow-matching objective to train the model. In this approach, the forward noising process is defined as a rectified flow that connects the data and noise distributions on a straight line. The TensorRT-optimised version of SD 3 Medium can be downloaded from [Huggingface.](https://huggingface.co/stabilityai/stable-diffusion-3-medium-tensorrt)

15. ***Recaptioning Billions of Web Images with LLaMA-3:
UC Santa Cruz, Uni of Edinburgh, JHU, Adobe, and UT Austin's paper discusses recaptioning 1.3 billion web images using LLaMA-3, enhancing textual descriptions for better vision-language model training. The enhanced dataset, Recap-DataComp-1B, improves model performance in tasks like cross-modal retrieval and text-to-image generation. The project page is available online.*** <br><br>
    12 Jun, UC Santa Cruz, Uni of Edinburgh, JHU, Adobe, and UT Austin published a [paper](https://arxiv.org/pdf/2406.08478) “What If We Recaption Billions of Web Images with LLaMA-3?”. The paper indicates that  Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate that semantically aligning and enriching textual descriptions of these pairs can significantly enhance model training across various vision-language tasks, particularly text-to-image generation. However, large-scale investigations in this area remain predominantly closed-source. The paper aims to bridge this community effort, leveraging the powerful and open-sourced LLaMA-3, a GPT-4 level LLM. The recaptioning pipeline is simple: first, the authors fine-tune a LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images from the DataComp-1B dataset. The empirical results confirm that this enhanced dataset, Recap-DataComp-1B, offers substantial benefits in training advanced vision-language models. For discriminative models like CLIP, the auhtors observe enhanced zero-shot performance in cross-modal retrieval tasks. For generative models like text-to-image Diffusion Transformers, the generated images exhibit a significant improvement in alignment with users' text instructions, especially in following complex queries. The project page is https://www.haqtu.me/Recap-Datacomp-1B/

17. ***TextGrad: Automatic Differentiation via Text:
Stanford's paper introduces TextGrad, a framework for automatic differentiation using textual feedback from LLMs to optimize compound AI systems. TextGrad improves performance across various tasks, showcasing its flexibility and ease of use. The study demonstrates significant gains in accuracy and efficiency in diverse applications like question answering and molecule optimization.*** <br><br>
    11 Jun, Stanford Uni published a [paper](https://arxiv.org/pdf/2406.07496) “TextGrad: Automatic Differentiation via Text”. The paper argues that AI is undergoing a paradigm shift, with breakthroughs achieved by systems orchestrating multiple large language models (LLMs) and other complex components. As a result, developing principled and automated optimization methods for compound AI systems is one of the most important new challenges. Neural networks faced a similar challenge in its early days until backpropagation and automatic differentiation transformed the field by making optimization turn-key. Inspired by this, the work introduces TextGrad, a powerful framework performing automatic “differentiation'' via text. TextGrad backpropagates textual feedback provided by LLMs to improve individual components of a compound AI system. In the framework, LLMs provide rich, general, natural language suggestions to optimize variables in computation graphs, ranging from code snippets to molecular structures. TextGrad follows PyTorch's syntax and abstraction and is flexible and easy-to-use. It works out-of-the-box for a variety of tasks, where the users only provide the objective function without tuning components or prompts of the framework. The authors showcase TextGrad's effectiveness and generality across a diverse range of applications, from question answering and molecule optimization to radiotherapy treatment planning. Without modifying the framework, TextGrad improves the zero-shot accuracy of GPT-4o in Google-Proof Question Answering from 51% to 55%, yields 20% relative performance gain in optimizing LeetCode-Hard coding problem solutions, improves prompts for reasoning, designs new druglike small molecules with desirable in silico binding, and designs radiation oncology treatment plans with high specificity. TextGrad lays a foundation to accelerate the development of the next-generation of AI systems.

19. ***Masked Diffusion Language Models:
Cornell's study reveals that simple masked discrete diffusion models perform better than previously thought in language modeling. By applying an effective training recipe and a simplified objective, these models achieve state-of-the-art performance among diffusion models and approach autoregressive perplexity. The authors release the code for further exploration.*** <br><br>
    11 Jun, Cornell Uni published [paper](https://arxiv.org/pdf/2406.07524) “Simple and Effective Masked Diffusion Language Models”. The authors state that while diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling. This study shows that simple masked discrete diffusion is more performant than previously thought. The authors apply an effective training recipe that improves the performance of masked diffusion models and derive a simplified, Rao-Blackwellized objective that results in additional improvements. The objective has a simple form -- it is a mixture of classical masked language modeling losses -- and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model. On language modeling benchmarks, a range of masked diffusion models trained with modern engineering practices achieves a new state-of-the-art among diffusion models, and approaches AR perplexity. The authors release the code at: https://github.com/kuleshov-group/mdlm

21. ***Husky: Open-Source Language Agent for Multi-Step Reasoning:
Uni of Washington and Meta introduced Husky, an open-source language agent designed for complex reasoning tasks. Husky uses a unified action space and expert models to execute actions iteratively. The study shows Husky outperforms previous agents and matches frontier LMs like GPT-4 in various tasks. The code and models are available online.*** <br><br>
    10 Jun, Uni of Washington and Meta published a [paper](https://arxiv.org/pdf/2406.06469) “Husky A Unified, Open-Source Language Agent for Multi-Step Reasoning”. The study finds thatl language agents perform complex tasks by using tools to execute each step precisely. However, most existing agents are based on proprietary models or designed to target specific tasks, such as mathematics or multi-hop question answering. The work introduces Husky, a holistic, open-source language agent that learns to reason over a unified action space to address a diverse set of complex tasks involving numerical, tabular, and knowledge-based reasoning. Husky iterates between two stages: 1) generating the next action to take towards solving a given task and 2) executing the action using expert models and updating the current solution state. The authors identify a thorough ontology of actions for addressing complex tasks and curate high-quality data to train expert models for executing these actions. The experiments show that Husky outperforms prior language agents across 14 evaluation datasets. Moreover, the researchd introduces HuskyQA, a new evaluation set which stress tests language agents for mixed-tool reasoning, with a focus on retrieving missing knowledge and performing numerical reasoning. Despite using 7B models, Husky matches or even exceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of the holistic approach in addressing complex reasoning problems. the code and models are available at https://github.com/agent-husky/Husky-v1.

23. ***Emerging Technology Risks and AI Risk Mitigation Tactics:
A study with Boston Consulting Group finds that junior professionals are not effective in teaching senior professionals to use generative AI due to their limited experience with emerging technologies. The research identifies three novice AI risk mitigation tactics that focus on human routines and project-level interventions rather than system design, highlighting the challenges of upskilling in rapidly changing tech environments.*** <br><br>
    10 Jun, MIT, Harvard, Boston Consulting Group, Warwick Business School etc. published [paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4857373) “Don't Expect Juniors to Teach Senior Professionals to Use Generative AI: Emerging Technology Risks and Novice AI Risk Mitigation Tactics”. The literature on communities of practice demonstrates that a proven way for senior professionals to upskill themselves in the use of new technologies that undermine existing expertise is to learn from junior professionals. It notes that juniors may be better able than seniors to engage in real-time experimentation close to the work itself, and may be more willing to learn innovative methods that conflict with traditional identities and norms. However, this literature has not explored emerging technologies, which are seen to pose new risks to valued outcomes because of their uncertain and wide-ranging capabilities, exponential rate of change, potential for outperforming humans in a wide variety of skilled and cognitive tasks, and dependence on a vast, varied, and high volume of data and other inputs from a broad ecosystem of actors. It has also not explored obstacles to junior professionals being a source of expertise in the use of new technologies for more senior members in contexts where the juniors themselves are not technical experts, and where technology is so new and rapidly changing that the juniors have had little experience with using it. However, such contexts may be increasingly common. In this study conducted with Boston Consulting Group, a global management consulting firm, the authors interviewed 78 such junior consultants in July-August 2023 who had recently participated in a field experiment that gave them access to generative AI (GPT-4) for a business problem solving task. Drawing from junior professionals’ in situ reflections soon after the experiment, the researchers argue that such juniors may fail to be a source of expertise in the use of emerging technologies for more senior professionals; instead, they may recommend three kinds of novice AI risk mitigation tactics that: 1) are grounded in a lack of deep understanding of the emerging technology’s capabilities, 2) focus on change to human routines rather than system design, and 3) focus on interventions at the project-level rather than system deployer- or ecosystem-level.

25. ***LLMs as Text-Based World Simulators:
UC Santa Cruz and collaborators assess if LLMs can serve as world simulators for complex planning tasks using a new benchmark, ByteSized32-State-Prediction. The study finds that LLMs like GPT-4, while impressive, are still unreliable without further innovations. The work contributes insights into LLM capabilities and introduces a novel benchmark for future research.*** <br><br>
    10 Jun, Uni of Arizona, Microsoft, New York Uni, Johns Hopkins Uni and Allen Inst for AI released their ACL 2024 [paper](https://arxiv.org/pdf/2406.06485) “Can Language Models Serve as Text-Based World Simulators”. The authors state that virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? The goal of the study is to answer this question in the context of text-based simulators. The approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. The authors use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. The work tests GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM's capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.

27. ***ShiftAddLLM: Multiplication-Less Reparameterization:
Researchers propose ShiftAddLLM, a method to accelerate pretrained LLMs by replacing multiplications with shift-and-add operations, reducing memory usage and latency. The study demonstrates significant improvements in efficiency and performance across various tasks. The codes and models are available online.*** <br><br>
    10 Jun, Georgia Inst of Tech, Intel and Google published a [paper](https://arxiv.org/pdf/2406.05981) “ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization”.  The paper argues that Large language models (LLMs) have shown impressive performance on language tasks but face challenges when deployed on resource-constrained devices due to their extensive parameters and reliance on dense multiplications, resulting in high memory demands and latency bottlenecks. Shift-and-add reparameterization offers a promising solution by replacing costly multiplications with hardware-friendly primitives in both the attention and multi-layer perceptron (MLP) layers of an LLM. However, current reparameterization techniques require training from scratch or full parameter fine-tuning to restore accuracy, which is resource-intensive for LLMs. Thef work proposes accelerating pretrained LLMs through post-training shift-and-add reparameterization, creating efficient multiplication-free models, dubbed ShiftAddLLM. Specifically, the authors quantize each weight matrix into binary matrices paired with group-wise scaling factors. The associated multiplications are reparameterized into (1) shifts between activations and scaling factors and (2) queries and adds according to the binary matrices. To reduce accuracy loss, the study presents a multi-objective optimization method to minimize both weight and output activation reparameterization errors. Additionally, based on varying sensitivity across layers to reparameterization, the authors develop an automated bit allocation strategy to further reduce memory usage and latency. Experiments on five LLM families and eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points at comparable or lower latency compared to the most competitive quantized LLMs at 3 and 2 bits, respectively, and more than 80% memory and energy reductions over the original LLMs. Codes and models are available at [this https URL](https://github.com/GATECH-EIC/ShiftAddLLM).

29. ***Scaling Neural Machine Translation to 200 Languages:
Meta's paper discusses scaling neural machine translation (NMT) to 200 languages using a sparsely gated mixture of experts architecture. The model improves translation quality by 44% compared to previous state-of-the-art models. The contributions aim to reduce digital inequities by focusing on low-resource languages and are available for non-commercial use.*** <br><br>
    5 Jun, Meta published a [paper](https://www.nature.com/articles/s41586-024-07335-x) on Nature “Scaling neural machine translation to 200 languages”. The paper states that The development of neural techniques has opened up new avenues for research in machine translation. Today, neural machine translation (NMT) systems can leverage highly multilingual capacities and even perform zero-shot translation, delivering promising results in terms of language coverage and quality. However, scaling quality NMT requires large volumes of parallel bilingual data, which are not equally available for the 7,000+ languages in the world1. Focusing on improving the translation qualities of a relatively small group of high-resource languages comes at the expense of directing research attention to low-resource languages, exacerbating digital inequities in the long run. To break this pattern, here the authors introduce No Language Left Behind— a single massively multilingual model that leverages transfer learning across languages. The researchers developed a conditional computational model based on the Sparsely Gated Mixture of Experts architecture, which are trained on data obtained with new mining techniques tailored for low-resource languages. Furthermore, the study devised multiple architectural and training improvements to counteract overftting while training on thousands of tasks. The authors evaluated the performance of the model over 40,000 translation directions using tools created specifcally for this purpose—an automatic benchmark (FLORES-200), a human evaluation metric (XSTS) and a toxicity detector that covers every language in the model. Compared with the previous state-of-the-art models, the model achieves an average of 44% improvement in translation quality as measured by BLEU. By demonstrating how to scale NMT to 200 languages and making all contributions in this effort freely available for non-commercial use, the work lays important groundwork for the development of a universal translation system.

31. ***Necessity of Human Annotation in Fairness Benchmarks:
Uni of Southern California's study concludes that GPT-3.5-Turbo is not suitable for annotating bias benchmarks, as it produces poor quality outputs. The study emphasizes the importance of human annotation for sensitive tasks related to social biases and highlights the limitations of relying on LLMs for this purpose.*** <br><br>
    24 May, Uni of Southern California released their ACL 2024 [paper](https://arxiv.org/pdf/2405.15760) “GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction”. The authors argue that social biases in LLMs are usually measured via bias benchmark datasets. Current benchmarks have limitations in scope, grounding, quality, and human effort required. Previous work has shown success with a community-sourced, rather than crowd-sourced, approach to benchmark development. However, this work still required considerable effort from annotators with relevant lived experience. This paper explores whether an LLM (specifically, GPT-3.5-Turbo) can assist with the task of developing a bias benchmark dataset from responses to an open-ended community survey. The word also extends the previous work to a new community and set of biases: the Jewish community and antisemitism. The analysis shows that GPT-3.5-Turbo has poor performance on this annotation task and produces unacceptable quality issues in its output. Thus, the study concludes that GPT-3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use actually negates many of the benefits of community-sourcing bias benchmarks.
 <br><br><br>

***10 June 2024***

1. ***Together AI, Duke University, Stanford University, and others have published a paper on "Mixture-of-Agents Enhances Large Language Model Capabilities". They propose leveraging multiple large language models (LLMs) through a Mixture-of-Agents (MoA) approach. This method uses layered MoA architecture to surpass the performance of existing models like GPT-4 Omni in various benchmarks.*** <br><br>
   7 Jun, Together AI, Duke Uni, Stanford Uni and others published a [paper](https://arxiv.org/abs/2406.04692) “Mixture-of-Agents Enhances Large Language Model Capabilities”. The paper indicates that recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. The proposed approach constructs a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieve state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omn.

2. ***OpenAI published a paper on sparse autoencoders, highlighting their potential for extracting interpretable features from language models through a sparse bottleneck layer. To address the challenges of balancing reconstruction and sparsity and managing dead latents, the authors propose using k-sparse autoencoders to directly control sparsity. They found that modifications to this method reduce dead latents, even at large scales, and observed clean scaling laws related to autoencoder size and sparsity. The study introduced new metrics for feature quality, which generally improve with autoencoder size. A demonstration involved training a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens, with code and autoencoders released for open-source models.*** <br><br>
   6 Jun, OpenAI published a paper “Scaling and evaluating sparse autoencoders”. The paper finds that sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. The authors propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, the study finds modifications that result in few dead latents, even at the largest scales tried. Using these techniques, the authors find clean scaling laws with respect to autoencoder size and sparsity. The researchers also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of the approach, the work trains a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. The authors release code and autoencoders for open-source models, as well as a visualizer. 
4. ***A researcher from JKU Linz, Austria, has published a paper on "Vision-LSTM: xLSTM as Generic Vision Backbone". The paper extends the Long Short Term Memory (LSTM) into a scalable architecture known as xLSTM for use in computer vision, presenting Vision-LSTM (ViL) as a promising new backbone for vision architectures.*** <br><br>
   6 Jun, researcher from JKU Linz, Austria published a [paper](https://arxiv.org/pdf/2406.04303) “Vision-LSTM: xLSTM as Generic Vision Backbone”. The paper states that Transformers are widely used as generic backbones in computer vision, despite initially introduced for natural language processing. Recently, the Long Short Term Memory (LSTM) has been extended to a scalable and performant architecture – the xLSTM – which overcomes long-standing LSTM limitations via exponential gating and parallelizable matrix memory structure. This paper introduces Vision-LSTM (ViL), an adaption of the xLSTM building blocks to computer vision. ViL comprises a stack of xLSTM blocks where odd blocks process the sequence of patch tokens from top to bottom while even blocks go from bottom to top. Experiments show that ViL holds promise to be further deployed as new generic backbone for computer vision  architectures. Project page: https://nx-ai.github.io/vision-lstm/

5. ***Google published a paper on "Open-Endedness is Essential for Artificial Superhuman Intelligence", arguing that achieving open-ended, ever-improving AI is critical for artificial superhuman intelligence (ASI). The paper outlines a path towards ASI through open-ended systems built on foundation models and examines the safety implications of such advancements.*** <br><br>
   6 Jun, Google published a [paper](https://arxiv.org/pdf/2406.04268) “Open-Endedness is Essential for Artificial Superhuman Intelligence”. The paper argues that in recent years there has been a tremendous surge in the general capabilities of AI systems, mainly fuelled by training foundation models on internetscale data. Nevertheless, the creation of openended, ever self-improving AI remains elusive. This position paper argues that the ingredients are now in place to achieve openendedness in AI systems with respect to a human observer. Furthermore, the authors claim that such open-endedness is an essential property of any artificial superhuman intelligence (ASI). The study begins by providing a concrete formal definition of open-endedness through the lens of novelty and learnability. It then illustrates a path towards ASI via open-ended systems built on top of foundation models, capable of making novel, humanrelevant discoveries. The authors conclude by examining the safety implications of generally-capable openended AI. The authors expect that open-ended foundation models will prove to be an increasingly fertile and safety-critical area of research in the near future.

7. ***The University of Edinburgh and others published a paper titled "Are We Done with MMLU?" highlighting errors in the Massive Multitask Language Understanding (MMLU) benchmark. They introduce MMLU-Redux, a re-annotated subset of MMLU, advocating for revising the original benchmark to improve its utility and reliability.*** <br><br>
   6 Jun, Uni of Edinburgh et al. published a [paper](https://arxiv.org/pdf/2406.04127) “Are We Done with MMLU?”. The answer by the paper is Maybe not. The authors identify and analyse errors in the popular Massive Multitask Language Understanding (MMLU) benchmark. Even though MMLU is widely adopted, the analysis demonstrates numerous ground truth errors that obscure the true capabilities of LLMs. For example, the study finds that 57% of the analysed questions in the Virology subset contain errors. To address this issue, the paper introduces a comprehensive framework for identifying dataset errors using a novel error taxonomy. Then, the authors create MMLU-Redux, which is a subset of 3,000 manually re-annotated questions across 30 MMLU subjects. Using MMLU-Redux, the paper demonstrates significant discrepancies with the model performance metrics that were originally reported. The results strongly advocate for revising MMLU's error-ridden questions to enhance its future utility and reliability as a benchmark. Therefore, the authors open up MMLU-Redux for additional annotation [this https URL](https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux).

9. ***AWS published a paper on "Choice of PEFT Technique in Continual Learning: Prompt Tuning is Not All You Need". They argue that prompt tuning, a common method in continual learning, has not been thoroughly vetted and can hurt overall performance. Their research suggests using LoRA instead, achieving better accuracy and performance.*** <br><br>
    5 Jun, AWS published a [paper](https://arxiv.org/pdf/2406.03216) “Choice of PEFT Technique in Continual Learning: Prompt Tuning is Not All You Need”. The authors indicate that recent Continual Learning (CL) methods have combined pretrained Transformers with prompt tuning, a parameter-efficient fine-tuning (PEFT) technique. The authors argue that the choice of prompt tuning in prior works was an undefended and unablated decision, which has been uncritically adopted by subsequent research, but warrants further research to understand its implications. This work conducts this research and finds that the choice of prompt tuning as a PEFT method hurts the overall performance of the CL system. To illustrate this, the study replaces prompt tuning with LoRA in two state-of-the-art continual learning methods: Learning to Prompt and S-Prompts. These variants consistently achieve higher accuracy across a wide range of domain-incremental and class-incremental benchmarks, while being competitive in inference speed. The work highlights a crucial argument: unexamined choices can hinder progress in the field, and rigorous ablations, such as the PEFT method, are required to drive meaningful adoption of CL techniques in real-world applications.

11. ***The University of Oxford and Cambridge published a paper on "HelloFresh LLM Evaluations on Streams of Real-World Human Editorial Actions". The paper introduces HelloFresh, a benchmark based on real-world data from community notes and Wikipedia edits, providing a more dynamic and contamination-free evaluation of LLM capabilities.*** <br><br>
    5 Jun, Uni of Oxford and Cambridge published a [paper](https://arxiv.org/pdf/2406.03428v1) “HelloFresh LLM Evaluations on Streams of Real-World Human Editorial Actions across X Community Notes and Wikipedia edits”. The paper states that benchmarks have been essential for driving progress in machine learning. A better understanding of LLM capabilities on real world tasks is vital for safe development. Designing adequate LLM benchmarks is challenging: Data from real-world tasks is hard to collect, public availability of static evaluation data results in test data contamination and benchmark overfitting, and periodically generating new evaluation data is tedious and may result in temporally inconsistent results. The paper introduces HelloFresh, based on continuous streams of real-world data generated by intrinsically motivated human labelers. It covers recent events from X (formerly Twitter) community notes and edits of Wikipedia pages, mitigating the risk of test data contamination and benchmark overfitting. Any X user can propose an X note to add additional context to a misleading post (formerly tweet); if the community classifies it as helpful, it is shown with the post. Similarly, Wikipedia relies on community-based consensus, allowing users to edit articles or revert edits made by other users. Verifying whether an X note is helpful or whether a Wikipedia edit should be accepted are hard tasks that require grounding by querying the web. The authors backtest state-of-the-art LLMs supplemented with simple web search access and find that HelloFresh yields a temporally consistent ranking. To enable continuous evaluation on HelloFresh, the research team hosts a public leaderboard and periodically updated evaluation data at https://tinyurl.com/hello-fresh-LLM.

13. ***Google published a paper on "To Believe or Not to Believe Your LLM", exploring uncertainty quantification in LLMs. They present an information-theoretic metric for detecting high epistemic uncertainty, allowing for more reliable output and identifying hallucinations in both single- and multi-answer responses.*** <br><br>
    5 Jun, Google published a [paper](https://arxiv.org/pdf/2406.02543) “To Believe or Not to Believe Your LLM”. The authors explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. The paper simultaneously considers both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, the authors derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. The researchers conduct a series of experiments which demonstrate the advantage of the formulation. Further, the investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.

15. ***EPFL, Microsoft, and others published a paper on "Training of Physical Neural Networks". The paper discusses physical neural networks (PNNs) as a promising area for scaling AI models, suggesting that future research could enable PNNs to perform computations locally on edge devices, potentially revolutionizing AI model training and inference.*** <br><br>
    5 Jun, EPFL, Microsoft and others published a [paper](https://arxiv.org/pdf/2406.03372) “Training of Physical Neural Networks”. The article indicates that Physical neural networks (PNNs) are a class of neural-like networks that leverage the properties of physical systems to perform computation. While PNNs are so far a niche research area with small-scale laboratory demonstrations, they are arguably one of the most underappreciated important opportunities in modern AI. Could we train AI models 1000x larger than current ones? Could we do this and also have them perform inference locally and privately on edge devices, such as smartphones or sensors? Research over the past few years has shown that the answer to all these questions is likely "yes, with enough research": PNNs could one day radically change what is possible and practical for AI systems. To do this will however require rethinking both how AI models work, and how they are trained - primarily by considering the problems through the constraints of the underlying hardware physics. To train PNNs at large scale, many methods including backpropagation-based and backpropagation-free approaches are now being explored. These methods have various trade-offs, and so far no method has been shown to scale to the same scale and performance as the backpropagation algorithm widely used in deep learning today. However, this is rapidly changing, and a diverse ecosystem of training techniques provides clues for how PNNs may one day be utilized to create both more efficient realizations of current-scale AI models, and to enable unprecedented-scale models.

17. ***New York University and Adobe published a paper on "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead". They introduce QJL, a quantization method that reduces KV cache memory usage without compromising accuracy, demonstrating significant memory savings and faster runtime in LLM applications.*** <br><br>
    5 Jun, New York Uni and Adobe published a [paper](https://arxiv.org/pdf/2406.03482) “QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead”. The authors point out that serving LLMs requires substantial memory due to the storage requirements of Key-Value (KV) embeddings in the KV cache, which grows with sequence length. An effective approach to compress KV cache is quantization. However, traditional quantization methods face significant memory overhead due to the need to store quantization constants (at least a zero point and a scale) in full precision per data block. Depending on the block size, this overhead can add 1 or 2 bits per quantized number. The work introduces QJL, a new quantization approach that consists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit quantization. In contrast to existing methods, QJL eliminates memory overheads by removing the need for storing quantization constants. The authors propose an asymmetric estimator for the inner product of two vectors and demonstrate that applying QJL to one vector and a standard JL transform without quantization to the other provides an unbiased estimator with minimal distortion. The researchers have developed an efficient implementation of the QJL sketch and its corresponding inner product estimator, incorporating a lightweight CUDA kernel for optimized computation. When applied across various LLMs and NLP tasks to quantize the KV cache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV cache memory usage without compromising accuracy, all while achieving faster runtime. Codes are available at [this https URL](https://github.com/amirzandieh/QJL).

19. ***The US National Institute of Standards and Technology (NIST) updated their "ARIA pilot evaluation plan" to improve risk and impact assessments for AI. The ARIA program includes model testing, red teaming, and field testing to connect AI capabilities with real-world risks and impacts, aiming to enhance AI safety and reliability.*** <br><br>
    5 Jun, the US National Institute of Standards and Technology (NIST) [updated](https://ai-challenges.nist.gov/uassets/7)  “The draft NIST assessing risks and impacts of ai - ARIA pilot evaluation plan”. ARIA program aims to improve the quality of risk and impact assessments for the field of safe and trustworthy AI. Long-term programmatic outcomes may include guidelines, tools, evaluation methodologies, and measurement methods. ARIA has three levels of evaluations– 1) model testing to confirm claimed capabilities, 2) red teaming to stress test applications, and 3) field testing to investigate how people engage with AI in regular use. By tracing tasks across three different evaluation levels, ARIA can provide more direct knowledge about how AI capabilities (in model testing) connect to risks (in red teaming) and positive and negative impacts (in regular use field testing) in the real world. ARIA will address gaps in societal impact assessments by expanding the scope of study to include people, and how they adapt to AI technology in quasi-real world conditions. Current approaches do not adequately cover AI’s systemic impacts or consider how people interact with AI technology and act upon AI generated information . This isolation from real world contexts makes it difficult to anticipate and estimate real world failures. ARIA will provide insights about the applicability of testing approaches for evaluating specific risks, and the effectiveness of AI guardrails and risk mitigations. Trained assessors will evaluate ARIA evaluation output, including prompts and interactive data and sequences from red teamers and field testers. All ARIA evaluation data will be provided to the participant community for modeling and examination of how risks may arise in various settings. NIST evaluations are open to all who find them of interest, are able to submit their technology for testing, and can comply with the evaluation rules.

21. ***According to Singapore Business Review, the Singapore Infocomm Media Development Authority launched Project Moonshot for AI model safety. AI Verify-Project Moonshot is an open-source toolkit for testing the security and safety of LLMs, developed in collaboration with industry partners and aiming to establish global testing standards.*** <br><br>
    5 Jun, according to [Singapore Business Review](https://sbr.com.sg/information-technology/news/imda-launches-project-moonshot-ai-model-safety), Singapore Infocomm Media Development Authority launches Project Moonshot for AI model safety. Minister for Communications and Information Josephine Teo has launched AI Verify-Project Moonshot, a testing toolkit designed to address security and safety challenges often associated with the use of large language models. AI Verify-Project Moonshot is one of the first open-source tools to bring red-teaming, benchmarking, and baseline testing together in a platform. “An open beta, Project Moonshot aims to provide intuitive results of the quality and safety of a model or application in an easily understood manner, even for a non-technical user,” authorities said. IMDA developed the project with DataRobot, IBM, Singtel, and Temasek to ensure that the tool is useful and aligned with industry needs. Project Moonshot is also part of the move towards global testing standards. AI testing organisations – AI Verify Foundation (AIVF) and MLCommons – have signed a memorandum of intent to collaborate on building a common safety benchmark suite.

23. ***The University of Stuttgart published a paper on "Deception abilities emerged in large language models". The paper reveals that state-of-the-art LLMs have developed deception strategies, raising concerns about their alignment with human values and the potential for deceptive behavior to bypass monitoring efforts.*** <br><br>
    4 Jun, Uni of Suttgart published a [paper](https://www.pnas.org/doi/full/10.1073/pnas.2317967121) on PNAS “Deception abilities emerged in large language models”. The paper argues that Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, but were nonexistent in earlier LLMs. The paper conducts a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can trigger misaligned deceptive behavior. GPT-4, for instance, exhibits deceptive behavior in simple test scenarios 99.16% of the time (P < 0.001). In complex second-order deception test scenarios where the aim is to mislead someone who expects to be deceived, GPT-4 resorts to deceptive behavior 71.46% of the time (P < 0.001) when augmented with chain-of-thought reasoning. In sum, revealing hitherto unknown machine behavior in LLMs, the study contributes to the nascent field of machine psychology.

25. ***Bloomberg reported that OpenAI employees are advocating for protections to speak out about AI risks. They highlight the challenges posed by confidentiality agreements and propose measures to ensure transparency and address concerns about the implications of artificial general intelligence.*** <br><br>
    4 Jun, Bloomberg published an [article](https://www.bloomberg.com/news/articles/2024-06-04/openai-employees-call-for-protections-to-speak-out-on-ai-risks?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTcxNzUxNjM0OCwiZXhwIjoxNzE4MTIxMTQ4LCJhcnRpY2xlSWQiOiJTRUtBMTNEV1gyUFMwMCIsImJjb25uZWN0SWQiOiJFODA3NUYyRkZGMjA0NUI2QTlEQzA5M0EyQTdEQTE4NiJ9.wFpVycx4Gun0EF4R47ffHeOt1f9W2VdyR7pb5A8jF5E&sref=10lNAhZ9) “OpenAI Employees Want Protections to Speak Out on ‘Serious Risks’ of AI”. The article reveals that current and former employees of OpenAI and Google DeepMind are advocating for protections to voice concerns about the potential risks of AI technologies. They highlight the challenges posed by broad confidentiality agreements, which prevent them from publicly addressing these issues. Recent controversies at OpenAI, including the dissolution of a safety team and staff departures, have amplified concerns. The employees propose measures such as prohibiting non-disparagement agreements for risk-related concerns and establishing anonymous channels for raising issues with company boards and regulators. Despite OpenAI's assurance of engagement with various stakeholders, concerns persist regarding transparency and readiness for the implications of artificial general intelligence. 

27. ***The University of Waterloo, Toronto, and CMU published a paper on "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark". They introduce MMLU-Pro, an enhanced dataset with more challenging reasoning questions, showing significant drops in accuracy and better performance with Chain of Thought reasoning compared to the original MMLU.*** <br><br>
    4 Jun, Uni. Waterloo, Toronto and CMU published a [paper](https://arxiv.org/pdf/2406.01574v1) “MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark”. The paper indicates that the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro. Additionally, the study found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. The assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field.

29. ***The European Data Protection Supervisor (EDPS) published guidelines on "Generative AI and the EUDPR" to help EUIs comply with data protection regulations. The guidelines provide advice on data protection impact assessments and emphasize core principles to address the risks and opportunities of generative AI systems.*** <br><br>
    3 Jun, the European Data Protection Supervisor (EDPS) published [guideline](https://www.edps.europa.eu/system/files/2024-06/24-06-03_genai_orientations_en.pdf) titled 'Generative AI and the EUDPR: First EDPS Orientations for ensuring data protection compliance when using Generative AI systems.' The guidelines aim to help EUIs comply with the data protection obligations set out in Regulation (EU) 2018/1725, when using or developing generative AI tools. Wojciech Wiewiórowski, EDPS, said: “The guidelines that I have issued today on generative AI are a first step towards more extensive recommendations in response to the evolving landscape of generative AI tools, which my team and I continue to monitor and analyse closely. Our advice published today is drafted with the aim of covering as many possible scenarios involving the use of generative AI, to provide enduring advice to EUIs so that they can protect individuals’ personal information and privacy.” To ensure their practical application by EUIs, the guidelines emphasize on data protection’s core principles, combined with concrete examples, as an aid to anticipate risks, challenges and opportunities of generative AI systems and tools. As such, the guidelines focus on a series of important topics, including advice on how EUIs can distinguish whether the use of such tools involves the processing of individuals’ data; when to conduct a data protection impact assessment; and other essential recommendations.

31. ***Stanford University published a paper on "Show, Don't Tell: Aligning Language Models with Demonstrated Feedback". The paper introduces DITTO, a method for aligning LLMs with user-demonstrated behaviors using a small number of demonstrations, outperforming other methods in fine-grained style and task alignment.*** <br><br>
    3 Jun, Stanford Uni published a [paper](https://arxiv.org/pdf/2406.00888) “Show, Don't Tell: Aligning Language Models with Demonstrated Feedback”. The paper finds that language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. The authors argue that it is instead possible to align an LLM to a specific setting by leveraging a very small number (<10) of demonstrations as feedback. The proposed method, Demonstration ITerated Task Optimization (DITTO), directly aligns language model outputs to a user's demonstrated behaviors. Derived using ideas from online imitation learning, DITTO cheaply generates online comparison data by treating users' demonstrations as preferred over output from the LLM and its intermediate checkpoints. The study evaluates DITTO's ability to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, the authors conduct a user study soliciting a range of demonstrations from participants (N=16). Across the benchmarks and user study, the work finds that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an average of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs. 

33. ***The University of Chicago, Google, and Drexel University published a paper on "The Impossibility of Fair LLMs". The paper argues that achieving fairness in LLMs is inherently limited by the complexity of human-AI interactions. They propose guidelines for realistic fairness goals, emphasizing context, developer responsibility, and stakeholder participation.*** <br><br>
    28 May, Uni of Chicago, Google, and Drexel Uni published a [paper](https://arxiv.org/pdf/2406.03198) in CHI-HEAL 2024 conference “The Impossibility of Fair LLMs”. The authors argue that the need for fair AI is increasingly clear in the era of general-purpose systems such as ChatGPT, Gemini, and other large language models (LLMs). However, the increasing complexity of human-AI interaction and its social impacts have raised questions of how fairness standards could be applied. The paper reviews the technical frameworks that machine learning researchers have used to evaluate fairness, such as group fairness and fair representations, and find that their application to LLMs faces inherent limitations. The work shows that each framework either does not logically extend to LLMs or presents a notion of fairness that is intractable for LLMs, primarily due to the multitudes of populations affected, sensitive attributes, and use cases. To address these challenges, the authors develop guidelines for the more realistic goal of achieving fairness in particular use cases: the criticality of context, the responsibility of LLM developers, and the need for stakeholder participation in an iterative process of design and evaluation. Moreover, it may eventually be possible and even necessary to use the general-purpose capabilities of AI systems to address fairness challenges as a form of scalable AI-assisted alignment.
 
35. ***MIT published a paper on "Large language models can be zero-shot anomaly detectors for time series?" presenting a framework for using LLMs in time series anomaly detection. While LLMs show potential, state-of-the-art deep learning models still outperform them, highlighting the need for further research in this area.*** <br><br>
    23 May, MIT published a [paper]() “Large language models can be zero-shot anomaly detectors for time series?”. The paper finds thatrRecent studies have shown the ability of large language models to perform a variety of tasks, including time series forecasting. The flexible nature of these models allows them to be used for many applications. In this paper, the authors present a novel study of large language models used for the challenging task of time series anomaly detection. This problem entails two aspects novel for LLMs: the need for the model to identify part of the input sequence (or multiple parts) as anomalous; and the need for it to work with time series data rather than the traditional text input. The work introduces sigllm, a framework for time series anomaly detection using large language models. The framework includes a time-series-to-text conversion module, as well as end-to-end pipelines that prompt language models to perform time series anomaly detection. We investigate two paradigms for testing the abilities of large language models to perform the detection task. First, the authors present a prompt-based detection method that directly asks a language model to indicate which elements of the input are anomalies. Second, the researchers leverage the forecasting capability of a large language model to guide the anomaly detection process. The study evaluated the framework on 11 datasets spanning various sources and 10 pipelines. The authors show that the forecasting method significantly outperformed the prompting method in all 11 datasets with respect to the F1 score. Moreover, while large language models are capable of finding anomalies, state-of-the-art deep learning models are still superior in performance, achieving results 30% better than large language models.
 <br><br><br>

***3rd June 2024***

1. ***Princeton Uni and CMU Paper on "Transformers and SSMs": The paper explores the relationship between Transformers and state-space models (SSMs), particularly highlighting the emergence of Mamba as a competitive alternative to Transformers in language modeling. It introduces the State Space Duality (SSD) framework, connecting SSMs and attention mechanisms, leading to the development of Mamba-2, a faster architecture maintaining competitiveness with Transformers.*** <br><br>
   1 Jun, Princeton Uni and CMU published a [paper](https://arxiv.org/pdf/2405.21060) “Transformers are SSMs Generalized Models and Efficient Algorithms Through Structured State Space Duality”. The authors find that while Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. The research shows that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. The proposed state space duality (SSD) framework allows to design a new architecture (Mamba-2) whose core layer is a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.

3. ***Databricks and MIT Paper on "Perplexity-Based Data Pruning": Investigating data pruning methods, the paper explores how smaller language models can effectively prune datasets based on perplexity, enhancing downstream task performance and reducing pretraining steps. Notably, pruning improves performance in data-constrained scenarios and overtrained conditions.*** <br><br>
   31 May, Databricks and MIT published a [paper](https://arxiv.org/pdf/2405.20541) “Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models”. This work investigates whether small language models can determine high-quality subsets of large-scale text datasets that improve the performance of larger language models. While existing work has shown that pruning based on the perplexity of a larger model can yield high-quality data, the authors investigate whether smaller models can be used for perplexity-based pruning and how pruning is affected by the domain composition of the data being pruned. The paper demonstrates that for multiple dataset compositions, perplexity-based pruning of pretraining data can significantly improve downstream task performance: pruning based on perplexities computed with a 125 million parameter model improves the average performance on downstream tasks of a 3 billion parameter model by up to 2.04 and achieves up to a 1.45times reduction in pretraining steps to reach commensurate baseline performance. Furthermore, the authors demonstrate that such perplexity-based data pruning also yields downstream performance gains in the over-trained and data-constrained regimes.

3. ***Princeton Uni Paper on "SWE-agent for Automated Software Engineering": The paper explores the role of language model (LM) agents in automating software engineering tasks and introduces SWE-agent, a system designed to enhance LM agents' performance through specially-built interfaces. Investigating the impact of interface design on LM agents, SWE-agent facilitates autonomous code creation, editing, repository navigation, and program execution. Evaluation on SWE-bench and HumanEvalFix demonstrates SWE-agent's state-of-the-art performance, surpassing previous benchmarks with a pass@1 rate of 12.5% and 87.7%, respectively. Insights into the design's impact on agents' behavior and performance are also provided.*** <br><br>
   30 May, Princeton Uni published a [paper](https://arxiv.org/pdf/2405.15793) “SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering”. The paper indicates that Language model (LM) agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, the authors posit that LM agents represent a new category of end users with their own needs and abilities, and would benefit from specially-built interfaces to the software they use. The work investigates how interface design affects the performance of language model agents. As a result of this exploration, the research introduces SWE-agent: a system that facilitates LM agents to autonomously use computers to solve software engineering tasks. SWE-agent's custom agent-computer interface (ACI) significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs. The authors evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive LMs. Finally, the study provides insight on how the design of the ACI can impact agents' behavior and performance..


5. ***Google Paper on "Zipper Multi-Tower Decoder Architecture": Addressing challenges in multimodal generative tasks, Google proposes Zipper, a multi-tower decoder architecture. It showcases competitive performance, especially in scenarios with limited aligned data, and flexibility in maintaining unimodal generation performance.*** <br><br>
   31 May, Google published a [paper](https://arxiv.org/pdf/2405.18669) “Zipper A Multi-Tower Decoder Architecture for Fusing Modalities”. The paper indicates that integrating multiple generative foundation models, especially those trained on different modalities, into something greater than the sum of its parts poses significant challenges. Two key hurdles are the availability of aligned data (concepts that contain similar meaning but is expressed differently in different modalities), and effectively leveraging unimodal representations in cross-domain generative tasks, without compromising their original unimodal capabilities. The authors propose Zipper, a multi-tower decoder architecture that addresses these concerns by using cross-attention to flexibly compose multimodal generative models from independently pre-trained unimodal decoders. In the experiments fusing speech and text modalities, the study shows the proposed architecture performs very competitively in scenarios with limited aligned text-speech data. The authors also showcase the flexibility of the model to selectively maintain unimodal (e.g., text-to-text generation) generation performance by freezing the corresponding modal tower (e.g. text). In cross-modal tasks such as automatic speech recognition (ASR) where the output modality is text, the authors show that freezing the text backbone results in negligible performance degradation. In cross-modal tasks such as text-to-speech generation (TTS) where the output modality is speech, the work shows that using a pre-trained speech backbone results in superior performance to the baseline.



7. ***Stanford and Yale Uni Paper on "AI Legal Research Tools": The paper assesses AI-driven legal research tools' reliability, highlighting concerns of hallucinations. It provides empirical evidence contradicting tool providers' claims, emphasizing the need for responsible integration of AI into legal practice.*** <br><br>
   30 May, Stanford and Yale Uni published a [paper](https://arxiv.org/pdf/2405.20362) “Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools”. The researchers indicate that Legal practice has witnessed a sharp rise in products incorporating artificial intelligence (AI). Such tools are designed to assist with a wide range of core legal tasks, from search and summarization of caselaw to document drafting. But the large language models used in these tools are prone to "hallucinate," or make up false information, making their use risky in high-stakes domains. Recently, certain legal research providers have touted methods such as retrieval-augmented generation (RAG) as "eliminating" (Casetext, 2023) or "avoid[ing]" hallucinations (Thomson Reuters, 2023), or guaranteeing "hallucination-free" legal citations (LexisNexis, 2023). Because of the closed nature of these systems, systematically assessing these claims is challenging. This study designs and reports on the first preregistered empirical evaluation of AI-driven legal research tools. The authors demonstrate that the providers' claims are overstated. While hallucinations are reduced relative to general-purpose chatbots (GPT-4), the paper finds that the AI research tools made by LexisNexis (Lexis+ AI) and Thomson Reuters (Westlaw AI-Assisted Research and Ask Practical Law AI) each hallucinate between 17% and 33% of the time. The authors also document substantial differences between systems in responsiveness and accuracy. This article makes four key contributions. It is the first to assess and report the performance of RAG-based proprietary legal AI tools. Second, it introduces a comprehensive, preregistered dataset for identifying and understanding vulnerabilities in these systems. Third, it proposes a clear typology for differentiating between hallucinations and accurate legal responses. Last, it provides evidence to inform the responsibilities of legal professionals in supervising and verifying AI outputs, which remains a central open question for the responsible integration of AI into law. Note:  tool providers disagree with the findings in the paper, but comments. from [reddit strongly support this paper](https://www.reddit.com/r/LawSchool/comments/1d52y2d/ai_legal_research_tools_hallucinate_1733_of_the/).

9. ***2.	Oxford and Reuters report on " What Does the Public Think of Generative AI in News?”. ChatGPT is the most recognized and widely used tool, with 50% awareness. Daily usage is low (1-7%), and 20-30% of the population in six countries are unaware of popular AI tools. ChatGPT dominates usage compared to other tools like Google Gemini and Microsoft Copilot. Younger demographics are more frequent users. Usage is split between information retrieval and media creation, with only 5% using generative AI for accessing news specifically.*** <br><br>
    30 May, Oxford Uni and Reuters published a [report](https://reutersinstitute.politics.ox.ac.uk/sites/default/files/2024-05/Fletcher_and_Nielsen_Generative_AI_and_News_Audiences.pdf) “What Does the Public in Six Countries Think of Generative AI in News?”. ChatGPT is by far the most widely recognised generative AI product – around 50% of the online population in the six countries (Argentina, Denmark, France, Japan, the UK, and the USA)  surveyed have heard of it. It is also by far the most widely used generative AI tool in the six countries surveyed. That being said, frequent use of ChatGPT is rare, with just 1% using it on a daily basis in Japan, rising to 2% in France and the UK, and 7% in the USA. Many of those who say they have used generative AI have used it just once or twice, and it is yet to become part of people’s routine internet use. In more detail, 1) While there is widespread awareness of generative AI overall, only between 20% and 30% of the online public population in the six countries surveyed – have not heard of any of the most popular AI tools. 2) In terms of use, ChatGPT is by far the most widely used generative AI tool in the six countries, two or three times more widespread than the next most widely used products, Google Gemini and Microsoft Copilot. 3) Younger people are much more likely to use generative AI products on a regular basis. Averaging across all six countries, 56% of 18–24s say they have used ChatGPT at least once, compared to 16% of those aged 55 and over. 4) Roughly equal proportions across six countries say that they have used generative AI for getting information (24%) as creating various kinds of media, including text but also audio, code, images, and video (28%). 5) Just 5% across the six countries covered say that they have used generative AI to get the latest news.

9. ***CMU Paper on "Probing Evaluation of Large Multimodal Models in Medical VQA:": The paper addresses concerns regarding the reliability of Large Multimodal Models (LMMs) in medical Visual Question Answering (Med-VQA). Using a rigorously designed Probing Evaluation for Medical Diagnosis (ProbMed) dataset, evaluation results reveal significant limitations in handling fine-grained medical inquiries, even by top-performing models like GPT-4V and Gemini Pro, highlighting the current gap between LMM capabilities and practical application in medical settings.*** <br><br>
   30 May, Uni of California Santa Cruz and CMU published a [paper](https://arxiv.org/pdf/2405.20421) “Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA”. The authors argue that Large Multimodal Models (LMMs) have shown remarkable progress in the field of medical Visual Question Answering (Med-VQA), achieving high accuracy on existing benchmarks. However, their reliability under robust evaluation is questionable. This study reveals that state-of-the-art models, when subjected to simple probing evaluation, perform worse than random guessing on medical diagnosis questions. To address this critical evaluation problem, the paper introduces the Probing Evaluation for Medical Diagnosis (ProbMed) dataset to rigorously assess LMM performance in medical imaging through probing evaluation and procedural diagnosis. Particularly, probing evaluation features pairing original questions with negation questions with hallucinated attributes, while procedural diagnosis requires reasoning across various diagnostic dimensions for each image, including modality recognition, organ identification, clinical findings, abnormalities, and positional grounding. The evaluation reveals that top-performing models like GPT-4V and Gemini Pro perform worse than random guessing on specialized diagnostic questions, indicating significant limitations in handling fine-grained medical inquiries. Besides, models like LLaVA-Med struggle even with more general questions, and results from CheXagent demonstrate the transferability of expertise across different modalities of the same organ, showing that specialized domain knowledge is still crucial for improving performance. This study underscores the urgent need for more robust evaluation to ensure the reliability of LMMs in critical fields like medical diagnosis, and current LMMs are still far from applicable to those fields..
   
11. ***Meta Paper on "Contextual Position Encoding": Meta proposes CoPE to enhance position encoding in large language models, enabling more precise attention mechanisms. CoPE addresses limitations in existing methods, improving performance on various tasks including language modeling and coding.*** <br><br>
    29 May, Meta published a [paper](https://arxiv.org/pdf/2405.18719) “Contextual Position Encoding Learning to Count What's Important”. The researchers indicate that the attention mechanism is a critical component of Large Language Models (LLMs) that allows tokens in a sequence to interact with each other, but is order-invariant. Incorporating position encoding (PE) makes it possible to address by position, such as attending to the i-th token. However, current PE methods use token counts to derive position, and thus cannot generalize to higher levels of abstraction, such as attending to the i-th sentence. The study proposes a new position encoding method, Contextual Position Encoding (CoPE), that allows positions to be conditioned on context by incrementing position only on certain tokens determined by the model. This allows more general position addressing such as attending to the i-th particular word, noun, or sentence. The authors show that CoPE can solve the selective copy, counting and Flip-Flop tasks where popular position embeddings fail, and improve perplexity on language modeling and coding tasks.

13. ***MistralAI Release of "Codestral 22B": Codestral, an advanced code generation AI model, is introduced, offering enhanced coding capabilities across multiple programming languages. Its release marks a milestone in AI-driven code generation, garnering positive feedback from the developer community.*** <br><br>
    29 May, MistralAI released [Codestral 22B](https://huggingface.co/bullerwins/Codestral-22B-v0.1-hf). Codestral, a groundbreaking code generation AI model, is introduced as a tool for developers to enhance their coding experience. Trained on over 80 programming languages, including popular ones like Python and Java, Codestral boasts a wide-ranging capability. It assists developers by completing coding functions, writing tests, and filling in partial code, thereby saving time and reducing errors. Performance-wise, Codestral sets a new benchmark with its 22B model, outperforming previous models in terms of code generation efficiency. Detailed benchmarks across various languages such as Python and SQL demonstrate its superior performance. Codestral is available for download under the Mistral AI Non-Production License and can be accessed via dedicated endpoints, catering to different user needs. Additionally, Codestral is integrated into popular coding environments like VSCode and JetBrains, enabling developers to seamlessly interact with it for code generation and conversation. This integration expands Codestral's accessibility and usability across different developer tools, marking a significant advancement in AI-driven code generation. Strong positive feedbacks from developer community has been received since its release.

15. ***Inter Paper on "Efficient NAS for Large Language Models": The paper presents LLaMA-NAS, a method for efficient neural architecture search to reduce the computational complexity of large language models. It achieves smaller, higher-performing architectures, enhancing their usability on diverse hardware platforms.*** <br><br>
    28 May, Inter published a [paper](https://arxiv.org/pdf/2405.18377) “LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models”. The study finds that the abilities of modern large language models (LLMs) in solving natural language processing, complex reasoning, sentiment analysis and other tasks have been extraordinary which has prompted their extensive adoption. Unfortunately, these abilities come with very high memory and computational costs which precludes the use of LLMs on most hardware platforms. To mitigate this, the researchers propose an effective method of finding Pareto-optimal network architectures based on LLaMA2-7B using one-shot NAS (Neural Architecture Search). In particular, the authors fine-tune LLaMA2-7B only once and then apply genetic algorithm-based search to find smaller, less computationally complex network architectures. The work shows that, for certain standard benchmark tasks, the pre-trained LLaMA2-7B network is unnecessarily large and complex. More specifically, the paper demonstrates a 1.5x reduction in model size and 1.3x speedup in throughput for certain tasks with negligible drop in accuracy. In addition to finding smaller, higher-performing network architectures, the proposed method does so more effectively and efficiently than certain pruning or sparsification techniques. Finally, the authors demonstrate how quantization is complementary to the method and that the size and complexity of the networks can be further decreased using quantization. The authors believe that this work provides a way to automatically create LLMs which can be used on less expensive and more readily available hardware platforms.

17. ***Uni of Washington, Chicago, and Harvard Paper on "Superposed Decoding": Introducing Superposed Decoding, the paper addresses the computational cost of generating multiple drafts by leveraging autoregressive inference passes. It demonstrates improved speed and coherence compared to traditional decoding methods.*** <br><br>
    28 May, Uni of Washington, Chicago, and Harvard Uni published a [paper](https://arxiv.org/pdf/2405.18400) “Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass”. Many applications today provide users with multiple auto-complete drafts as they type, including GitHub's code completion, Gmail's smart compose, and Apple's messaging auto-suggestions. Under the hood, language models support this by running an autoregressive inference pass to provide a draft. Consequently, providing k drafts to the user requires running an expensive language model k times. To alleviate the computation cost of running k inference passes, the authors propose Superposed Decoding, a new decoding algorithm that generates k drafts at the computation cost of one autoregressive inference pass. This is achieved by feeding a superposition of the k most recent token embeddings from the drafts as input to the next decoding step of the language model. At every inference step the authors combine the k drafts with the top-k tokens to get k*k new drafts and cache the k most likely options, using an n-gram interpolation with minimal compute overhead to filter out incoherent generations. The experiments show that k drafts from Superposed Decoding are at least as coherent and factual as Nucleus Sampling and Greedy Decoding respectively, while being at least 2.44× faster for k≥3. In a compute-normalized setting, user evaluations demonstrably favor text generated by Superposed Decoding over Nucleus Sampling. Code and more examples open-sourced at [this https URL](https://github.com/RAIVNLab/SuperposedDecoding).

19. ***Georgia State Uni and Princeton Uni Paper on "VB-LoRA Hybrid Model": The paper introduces VB-LoRA, a parameter-efficient fine-tuning method for large language models, reducing storage and transmission costs. It achieves competitive performance while utilizing a fraction of the parameters of traditional methods.*** <br><br>
    27 May, Georgia State Uni and Princeton Uni published a [paper](https://arxiv.org/pdf/2405.15179) “VB-LoRA Extreme Parameter Efficient Fine-Tuning with Vector Banks”. As the adoption of large language models increases and the need for per-user or per-task model customization grows, the parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) and its variants, incur substantial storage and transmission costs. To further reduce stored parameters, the study introduces a "divide-and-share" paradigm that breaks the barriers of low-rank decomposition across matrix dimensions, modules and layers by sharing parameters globally via a vector bank. As an instantiation of the paradigm to LoRA, the proposed VB-LoRA composites all the low-rank matrices of LoRA from a shared vector bank with a differentiable top-k admixture module. VB-LoRA achieves extreme parameter efficiency while maintaining comparable or better performance compared to state-of-the-art PEFT methods. Extensive experiments demonstrate the effectiveness of VB-LoRA on natural language understanding, natural language generation, and instruction tuning tasks. When fine-tuning the Llama2-13B model, VB-LoRA only uses 0.4% of LoRA's stored parameters, yet achieves superior results. The source code is available at [this https URL](https://github.com/leo-yangli/VB-LoRA).

21. ***Uni of Maryland, CMU, et al. Paper on "Transformers and Arithmetic": Addressing transformers' limitations in arithmetic tasks, the paper proposes position embeddings to enhance their performance. It demonstrates significant improvements in solving arithmetic problems, unlocking potential in multi-step reasoning tasks.*** <br><br>
    27 May, Uni of Maryland, CMU, et al. published a [paper](https://arxiv.org/pdf/2405.17399) “Transformers Can Do Arithmetic with the Right Embeddings”. The authors find that the poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. The study mends this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, the research shows that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further. With positions resolved, the work can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? The authors find that training on only 20 digit numbers with a single GPU for one day, they can reach state-of-the-art performance, achieving up to 99% accuracy on 100 digit addition problems. Finally, the work shows that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.

23. ***Microsoft and Uni of Wisconsin-Madison Paper on "Matryoshka Multimodal Models": The paper presents M3, a novel approach for multimodal models, improving efficiency and flexibility in visual-linguistic reasoning. M3 offers fine-grained control over visual granularity and achieves performance comparable to traditional models.*** <br><br>
    27 May, Microsoft and Uni of Wisconsin-Madison published a [paper](https://arxiv.org/pdf/2405.17430) “Matryoshka Multimodal Models”. The paper indicates that Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in visual-linguistic reasoning. These models first embed images into a fixed large number of visual tokens and then feed them into a Large Language Model (LLM). However, this design causes an excessive number of tokens for dense visual scenarios such as high-resolution images and videos, leading to great inefficiency. While token pruning/merging methods do exist, they produce a single length output for each image and do not afford flexibility in trading off information density v.s. efficiency. Inspired by the concept of Matryoshka Dolls, the work proposes M3: Matryoshka Multimodal Models, which learns to represent visual content as nested sets of visual tokens that capture information across multiple coarse-to-fine granularities. The proposed approach offers several unique benefits for LMMs: (1) One can explicitly control the visual granularity per test instance during inference, e.g. , adjusting the number of tokens used to represent an image based on the anticipated complexity or simplicity of the content; (2) M3 provides a framework for analyzing the granularity needed for existing datasets, where it finds that COCO-style benchmarks only need around ~9 visual tokens to obtain accuracy similar to that of using all 576 tokens; (3) the approach provides a foundation to explore the best trade-off between performance and visual token length at sample level, where the investigation of the work reveals that a large gap exists between the oracle upper bound and current fixed-scale representations.

25. ***Zyphra Paper on "Zamba Hybrid Model": Zamba, a hybrid SSM-transformer model, is introduced for improved performance at a comparable scale. With a unique architecture and pretraining strategy, Zamba achieves competitive results while being faster and more memory-efficient than traditional transformer models.*** <br><br>
    26 May, Zyphra published a [paper](https://arxiv.org/pdf/2405.16712v1) “Zamba: A Compact 7B SSM Hybrid Model”. The technical report presents Zamba, a novel 7B SSM-transformer hybrid model which achieves competitive performance against leading open-weight models at a comparable scale. Zamba is trained on 1T tokens from openly available datasets and is the best non-transformer model at this scale. Zamba pioneers a unique architecture combining a Mamba backbone with a single shared attention module, thus obtaining the benefits of attention at minimal parameter cost. Due to its architecture, Zamba is significantly faster at inference than comparable transformer models and requires substantially less memory for generation of long sequences. Zamba is pretrained in two phases: the first phase is based on existing web datasets, while the second one consists of annealing the model over high-quality instruct and synthetic datasets, and is characterized by a rapid learning rate decay. The authors open-source the weights and all checkpoints for Zamba, through both phase 1 and annealing phases.

27. ***Jason Wei's Blog on "Language Model Evaluations": Jason emphasizes the importance of evaluation benchmarks in driving progress in AI research, outlining key characteristics of successful evaluations. He advocates for greater investment in evals to incentivize breakthroughs and ensure meaningful advancements in the field.*** <br><br>
    24 May, Jason Wei, author of Chain-of-Thought, published a [blog](https://www.jasonwei.net/blog/evals) “Successful language model evals”. Jason emphasizes the crucial role of evaluation benchmarks (evals) in driving progress in AI research. He argues that evals deserve more attention as they serve as incentives for researchers and breakthroughs often correlate with significant improvements on evals. Successful evals, such as GLUE/SuperGLUE and MMLU, are widely adopted and trusted within the community. Key characteristics of successful evals include clear significance, ease of understanding, reliability, simplicity, and relevance to meaningful tasks, and avoiding test set contamination. However, creating effective evals is challenging, requiring careful consideration of factors like 1) sample size which should no less than 1000, 2) eval quality should be high, 3) simplicity overweight complexity, 4) easy to run, 5) work on a meaningful task, 6) the grading in the eval should be extremely correct and 7) model performance must not become saturated too quickly. The text concludes by advocating for greater investment in evals, highlighting their importance as the objective function for AI researchers and their potential to drive impactful advancements in the field.

29. ***Uni of Chicago Paper on "Robust RAG against Retrieval Corruption": The paper introduces RobustRAG as a defense framework against retrieval corruption attacks. It employs isolate-then-aggregate strategies to achieve certifiable robustness against malicious passages injected into retrieval results.*** <br><br>
    24 May, Princeton Uni and UC Berkeley published a [paper](https://arxiv.org/pdf/2405.15556) “Certifiably Robust RAG against Retrieval Corruption”. The authors state that Retrieval-augmented generation (RAG) has been shown vulnerable to retrieval corruption attacks: an attacker can inject malicious passages into retrieval results to induce inaccurate responses. This work proposes RobustRAG as the first defense framework against retrieval corruption attacks. The key insight of RobustRAG is an isolate-then-aggregate strategy: get LLM responses from each passage in isolation and then securely aggregate these isolated responses. To instantiate RobustRAG, the authors design keyword-based and decoding-based algorithms for securely aggregating unstructured text responses. Notably, RobustRAG can achieve certifiable robustness: the authors can formally prove and certify that, for certain queries, RobustRAG can always return accurate responses, even when the attacker has full knowledge of the defense and can arbitrarily inject a small number of malicious passages. The authors evaluate RobustRAG on open-domain QA and long-form text generation datasets and demonstrate its effectiveness and generalizability across various tasks and datasets.

31. ***CMU, Uni of Toronto, Georgia Tech, MBZUAI Paper on "Data Valuation with Influence Functions": The paper focuses on scalable data valuation methods for large language models, enhancing influence functions' scalability. It introduces LoGra and LogIX to efficiently evaluate data contributions, lowering computational barriers.*** <br><br>
    22 May, CMU, Uni of Toronto, Georgia Tech, MBZUAI etc. published a [paper](https://arxiv.org/pdf/2405.13954) “What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions”. The authors find that Large language models (LLMs) are trained on a vast amount of human-written data, but data providers often remain uncredited. In response to this issue, data valuation (or data attribution), which quantifies the contribution or value of each data to the model output, has been discussed as a potential solution. Nevertheless, applying existing data valuation methods to recent LLMs and their vast training datasets has been largely limited by prohibitive compute and memory costs. This work focuses on influence functions, a popular gradient-based data valuation method, and significantly improve its scalability with an efficient gradient projection strategy called LoGra that leverages the gradient structure in backpropagation. The authors then provide a theoretical motivation of gradient projection approaches to influence functions to promote trust in the data valuation process. Lastly, the researchers lower the barrier to implementing data valuation systems by introducing LogIX, a software package that can transform existing training code into data valuation code with minimal effort. In the data valuation experiments, LoGra achieves competitive accuracy against more expensive baselines while showing up to 6,500x improvement in throughput and 5x reduction in GPU memory usage when applied to Llama3-8B-Instruct and the 1B-token dataset. Code is available here.

33. ***Uni of Chicago Paper on "Financial Statement Analysis with LLMs": Investigating LLMs' capability in financial analysis, the paper demonstrates their superior performance in predicting earnings changes compared to human analysts. LLMs offer valuable insights and trading strategies, potentially reshaping decision-making processes.*** <br><br>
    21 May, Uni of Chicago published a [paper](https://papers.ssrn.com/sol3/Delivery.cfm/4835311.pdf?abstractid=4835311&mirid=1) “Financial Statement Analysis with Large Language Models”. The paper investigates whether an LLM can successfully perform financial statement analysis in a way similar to a professional human analyst. The study provides standardized and anonymous financial statements to GPT4 and instructs the model to analyze them to determine the direction of future earnings. Even without any narrative or industry-specific information, the LLM outperforms financial analysts in its ability to predict earnings changes. The LLM exhibits a relative advantage over human analysts in situations when the analysts tend to struggle. Furthermore, the authors find that the prediction accuracy of the LLM is on par with the performance of a narrowly trained state-of-the-art ML model. LLM prediction does not stem from its training memory. Instead, the paper finds that the LLM generates useful narrative insights about a company's future performance. Lastly, the trading strategies based on GPT's predictions yield a higher Sharpe ratio and alphas than strategies based on other models. Taken together, the results suggest that LLMs may take a central role in decision-making.

35. ***MIT Paper on "Platonic Representation Hypothesis": The paper discusses the convergence of representations in AI models towards a shared statistical model of reality, akin to Plato's ideal reality. It explores implications, selective pressures, and limitations of this trend in driving progress in AI research.*** <br><br>
    13 May, MIT published a [paper](https://arxiv.org/pdf/2405.07987) “The Platonic Representation Hypothesis”. The authors argue that representations in AI models, particularly deep networks, are converging. First, the authors survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, the researchers demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. The study hypothesizes that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. The researchers term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, the paper discusses the implications of these trends, their limitations, and counterexamples to the analysis. There are also some hot discussions of the paper on [reddit](https://www.reddit.com/r/MachineLearning/comments/1csgr7r/r_the_platonic_representation_hypothesis/?rdt=46406&onetap_auto=true&one_tap=true). 

37. ***Cohere Paper on "Detecting Under-trained Tokens in LLMs": Addressing issues with tokenizer creation and model training disconnects, the paper introduces methods for automatically detecting untrained and under-trained tokens in large language models. It offers insights into improving efficiency and safety in language model development.*** <br><br>
    8 May, Cohere published a [paper](https://arxiv.org/pdf/2405.05417) “Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models”. The paper argues that the disconnect between tokenizer creation and model training in language models has been known to allow for certain inputs, such as the infamous _SolidGoldMagikarp token, to induce unwanted behaviour. Although such `glitch tokens' that are present in the tokenizer vocabulary, but are nearly or fully absent in training, have been observed across a variety of different models, a consistent way of identifying them has been missing. The authors present a comprehensive analysis of Large Language Model (LLM) tokenizers, specifically targeting this issue of detecting untrained and under-trained tokens. Through a combination of tokenizer analysis, model weight-based indicators, and prompting techniques, the authors develop effective methods for automatically detecting these problematic tokens. The findings demonstrate the prevalence of such tokens across various models and provide insights into improving the efficiency and safety of language models.

39. ***Google's Update on "Time-Series Forecasting with LLMs": Google presents a time-series foundation model for forecasting based on large language models. It achieves competitive performance across various forecasting tasks, demonstrating the potential of LLMs in time-series analysis.*** <br><br>
    17 Apr, Google updated its [paper](https://arxiv.org/pdf/2310.10688) “A decoder-only foundation model for Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models time-series forecasting”. Motivated by recent advances in large language models for Natural Language Processing (NLP), the researchers design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. This model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
<br><br><br>
