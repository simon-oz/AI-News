# Weekly AI-News
Welcome to the Weekly-AI-news!
I mainly focuses more on Generative AI, LLMs and the related, so may miss import AI news in other areas. <br><br>

***19 May 2024***

1. ***The complexity of large language models (LLMs) like ChatGPT has prompted researchers to investigate their inner workings using psychology, neuroscience, and explainable AI (XAI). Unlike traditional software, LLMs function as 'black boxes,' with their decision-making processes being opaque even to their developers. Efforts in XAI aim to clarify these processes, ensuring safer and more efficient AI. Despite LLMs mimicking human reasoning, they can act unpredictably, sometimes spreading misinformation or biases. Advanced techniques like causal tracing and neuron-level analysis have provided insights, but understanding LLMs remains challenging, requiring ongoing research and regulatory measures for transparency and accountability.*** <br><br>
   17 May, Nature published [an article](https://www.nature.com/articles/d41586-024-01314-y) “How does ChatGPT ‘think’? Psychology and neuroscience crack open AI large language models - 
Researchers are striving to reverse-engineer artificial intelligence and scan the ‘brains’ of LLMs to see what they are doing, how and why.” The complexity of large language models (LLMs), such as ChatGPT, has led researchers to explore their inner workings using methods from psychology, neuroscience, and explainable AI (XAI). Unlike traditional software, LLMs, which use neural networks inspired by the brain, often operate as 'black boxes' whose decision-making processes are opaque even to their developers. Efforts in XAI aim to illuminate these processes, revealing why LLMs make certain decisions and helping to build safer, more efficient AI. Studies have shown LLMs can mimic reasoning and exhibit behaviors similar to humans, but they can also act unpredictably, sometimes generating misinformation or exhibiting biases. Advanced techniques, including causal tracing and neuron-level analysis, have provided insights into LLM behavior and potential for model editing. Despite progress, understanding LLMs remains challenging, necessitating ongoing research and regulatory measures to ensure transparency and accountability in AI applications.

3. ***The resurgence of private cloud strategies among enterprises is driven by the growing complexity and cost of AI workloads. A 2023 Forrester survey shows that 79% of enterprise cloud decision-makers are implementing private clouds, with IDC predicting significant growth in spending on private cloud services. While public cloud providers dominate the market, AI's cost control and data security concerns are shifting infrastructures towards hybrid clouds. Despite challenges, solutions like colocation data centers are emerging to support these infrastructures. This trend is expected to continue as AI technologies advance, providing cost-effective solutions for enterprises.*** <br><br>
   17 May, InfoWorld published [an article](https://www.infoworld.com/article/3715483/is-generative-ai-bringing-back-private-clouds.html) “Is generative AI bringing back private clouds?” The resurgence of private cloud strategies among enterprises is driven by the growing complexity and cost of AI workloads. Forrester's 2023 survey reveals that 79% of enterprise cloud decision-makers are implementing private clouds, with IDC forecasting significant growth in spending on private cloud services and infrastructure. While public cloud providers dominate the market, AI is prompting a shift towards hybrid cloud infrastructures due to cost control and data security concerns. Despite the challenges, such as the need for specialized hardware, solutions like colocation data centers are emerging to support these infrastructures. As AI technologies advance, the trend towards private clouds is expected to continue, offering cost-effective and valuable solutions for enterprises.

5. ***Generative AI technology is advancing rapidly, offering new capabilities for those who understand how to use them. Generative AI agents—AI-powered entities that perform tasks or assist humans—present great opportunities for organizations. However, widespread adoption has been hindered by data quality, employee distrust, and implementation costs. As gen AI technologies progress, more use cases will emerge, deployment costs will decrease, and automation will scale across enterprise processes, employee experiences, and customer interfaces. This evolution requires investing in strong AI trust, risk management practices, and platforms for managing agent-based systems.*** <br><br>
   17 May, Mckinsey published [an article](https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/the-promise-and-the-reality-of-gen-ai-agents-in-the-enterprise) “The promise and the reality of gen AI agents in the enterprise”. Generative AI technology is improving so quickly that a range of new capabilities are rapidly coming online, but only for those who can understand how to use them. The evolution of generative AI (gen AI) has opened the door to great opportunities across organizations, particularly regarding gen AI agents—AI-powered software entities that plan and perform tasks or aid humans by delivering specific services on their behalf. So far, adoption at scale across businesses has faced difficulties because of data quality, employee distrust, and cost of implementation. In addition, capabilities have raced ahead of leaders’ capacity to imagine how these agents could be used to transform work. However, as gen AI technologies progress and the next-generation agents emerge, we expect more use cases to be unlocked, deployment costs to decrease, long-tail use cases to become economically viable, and more at-scale automation to take place across a wider range of enterprise processes, employee experiences, and customer interfaces. This evolution will demand investing in strong AI trust and risk management practices and policies as well as platforms for managing and monitoring agent-based systems.

7. ***Arup, a British multinational design and engineering firm, fell victim to a deepfake scam, resulting in a Hong Kong employee transferring $25 million to fraudsters. Fake voices and images deceived the finance worker into believing he was communicating with the company's CFO and other staff. Despite initial suspicion of a phishing email, the realistic video call convinced the employee. Arup stated that its financial stability and internal systems remained unaffected. The scam underscores the rising sophistication of cyberattacks, with Arup experiencing regular threats like invoice fraud, phishing, and deepfakes, emphasizing the need for vigilance.*** <br><br>
   17 May, [according to CNN](https://edition.cnn.com/2024/05/16/tech/arup-deepfake-scam-loss-hong-kong-intl-hnk/index.html), A British multinational design and engineering firm, Arup, confirmed it was the victim of a deepfake scam, resulting in a Hong Kong employee transferring $25 million to fraudsters. The incident involved the use of fake voices and images to deceive the finance worker into believing he was communicating with the company's chief financial officer and other staff. The employee, initially suspicious of a phishing email, was convinced after a realistic video call. Despite the loss, Arup stated that its financial stability and internal systems remained unaffected. The scam highlights the rising sophistication of cyberattacks globally, with Arup experiencing regular threats such as invoice fraud, phishing, and deepfakes. Arup's leadership emphasized the growing concern over such technology and the need for vigilance.

9. ***Hugging Face, a leading machine learning company, is committing $10 million in free shared GPUs to help developers create new AI technologies. This initiative aims to support small developers, academics, and startups in competing with tech giants. Hugging Face's CEO, Delangue, expressed concern about AI startups' ability to compete due to the centralization of AI advancements within major tech companies. By donating GPUs through the ZeroGPU program, Hugging Face aims to enable open AI development, fostering collaboration and transparency in the AI community.*** <br><br>
    16 May, [according to theverge](https://www.theverge.com/2024/5/16/24156755/hugging-face-celement-delangue-free-shared-gpus-ai), Hugging Face, one of the biggest names in machine learning, is committing $10 million in free shared GPUs to help developers create new AI technologies. The goal is to help small developers, academics, and startups counter the centralization of AI advancements. Delangue, CEO of Huggingface, is concerned about AI startups’ ability to compete with the tech giants. Most significant advancements in artificial intelligence — like GPT-4, the algorithms behind Google Search, and Tesla’s Full Self-Driving system — remain hidden within the confines of major tech companies. Not only are these corporations financially incentivized to keep their models proprietary, but with billions of dollars at their disposal for computational resources, they can compound those gains and race ahead of competitors, making it impossible for startups to keep up. Hugging Face aims to level the playing field by donating these shared GPUs to the community through a new program called ZeroGPU. With AI rapidly advancing behind closed doors, the goal of Hugging Face is to allow people to build more AI tech in the open. “AI should not be held in the hands of the few. With this commitment to open-source developers, we’re excited to see what everyone will cook up next in the spirit of collaboration and transparency,” Delangue said in a press release.

11. ***A paper by Stanford University explores the capabilities of multimodal foundation models in many-shot in-context learning (ICL). Evaluating models like GPT-4o and Gemini 1.5 Pro across various datasets and tasks, the study finds that many-shot ICL significantly improves performance compared to few-shot ICL. Techniques like batching multiple queries in a single API call enhance performance and reduce costs. The research shows that Gemini 1.5 Pro has higher ICL data efficiency than GPT-4o, suggesting that many-shot ICL can efficiently adapt multimodal models to new applications and domains.*** <br><br>
    16 May, Stanford Uni published a [paper](https://arxiv.org/pdf/2405.09798) “Many-Shot In-Context Learning in Multimodal Foundation Models”. The paper states that large language models are well-known to be effective at few-shot in-context learning (ICL). Recent advancements in multimodal foundation models have enabled unprecedentedly long context windows, presenting an opportunity to explore their capability to perform ICL with many more demonstrating examples. This work evaluates the performance of multimodal foundation models scaling from few-shot to many-shot ICL. The authors benchmark GPT-4o and Gemini 1.5 Pro across 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification). The authors observe that many-shot ICL, including up to almost 2,000 multimodal demonstrating examples, leads to substantial improvements compared to few-shot (<100 examples) ICL across all of the datasets. Further, Gemini 1.5 Pro performance continues to improve log-linearly up to the maximum number of tested examples on many datasets. Given the high inference costs associated with the long prompts required for many-shot ICL, the authors also explore the impact of batching multiple queries in a single API call. The paper shows that batching up to 50 queries can lead to performance improvements under zero-shot and many-shot ICL, with substantial gains in the zero-shot setting on multiple datasets, while drastically reducing per-query cost and latency. Finally, the work measures ICL data efficiency of the models, or the rate at which the models learn from more demonstrating examples. The authors find that while GPT-4o and Gemini 1.5 Pro achieve similar zero-shot performance across the datasets, Gemini 1.5 Pro exhibits higher ICL data efficiency than GPT-4o on most datasets. The results suggest that many-shot ICL could enable users to efficiently adapt multimodal foundation models to new applications and domains. Codebase is publicly available at [this https URL](https://github.com/stanfordmlgroup/ManyICL).

13. ***Columbia University and Databricks published a paper comparing Low-Rank Adaptation (LoRA) and full finetuning methods for large language models. LoRA, which saves memory by training low-rank perturbations, generally underperforms compared to full finetuning but better maintains the base model's performance on tasks outside the target domain. LoRA provides stronger regularization, maintaining diverse generations. The paper suggests that the rank of perturbations learned in full finetuning explains the performance gap and offers best practices for finetuning with LoRA.*** <br><br>
    16 May, Columbia Uni and Databricks published a [paper](https://arxiv.org/pdf/2405.09673) “LoRA Learns Less and Forgets Less”. The paper argues that Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. This work compares the performance of LoRA and full finetuning on two target domains, programming and mathematics. The authors consider both the instruction finetuning (≈100K prompt-response pairs) and continued pretraining (≈10B unstructured tokens) data regimes. The results show that, in most settings, LoRA substantially underperforms full finetuning. Nevertheless, LoRA exhibits a desirable form of regularization: it better maintains the base model's performance on tasks outside the target domain. The paper shows that LoRA provides stronger regularization compared to common techniques such as weight decay and dropout; it also helps maintain more diverse generations. The study shows that full finetuning learns perturbations with a rank that is 10-100X greater than typical LoRA configurations, possibly explaining some of the reported gaps. The paper concludes by proposing best practices for finetuning with LoRA.

15. ***Meta's paper on "Chameleon" presents a family of early-fusion, token-based mixed-modal models capable of understanding and generating images and text. Chameleon models are evaluated on tasks such as visual question answering, image captioning, text generation, and image generation, demonstrating state-of-the-art performance in various tasks. Chameleon matches or exceeds the performance of larger models like Gemini Pro and GPT-4V, marking a significant step in unified multimodal modeling.*** <br><br>
    16 May, Meta published a [paper](https://arxiv.org/pdf/2405.09818) “Chameleon: Mixed-Modal Early-Fusion Foundation Models”. Chameleon is a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. The paper outlines a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.

17. ***Google's release of PaliGemma, an open vision-language model, is designed for versatile vision-language tasks. Built with open components, PaliGemma integrates the SigLIP vision model and Gemma language model, enabling tasks like image captioning, visual question answering, and object detection. Google provides pretrained and transfer checkpoints, allowing PaliGemma to be fine-tuned for specific tasks. The model aims to enhance document understanding and reasoning capabilities through its unique architecture.*** <br><br>
    15 May, Google released [PaliGemma](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md), an open vision-language model (VLM) inspired by PaLI-3 recipe, built with open components, such as the SigLIP vision model (So400m/14) and the Gemma 2B language model. PaliGemma is designed as a versatile model for transfer to a wide range of vision-language tasks such as image and short video caption, visual question answering, text reading, object detection and object segmentation. Together with the pretrained and transfer checkpoints at multiple resolutions, Google provides a checkpoint transferred to a mixture of tasks that can be used for off-the-shelf exploration. PaliGemma takes as input one or more images, which are turned into "soft tokens" by the SigLIP encoder, and input text (codenamed the "prefix") that is tokenized by Gemma's tokenizer. The image tokens and prefix tokens are concatenated (in this order) and passed to the Gemma decoder with full block-attention, which then generates an output text (the "suffix") auto-regressively with masked attention. Gemma is a decoder-only model for text generation. Combining the image encoder of SigLIP with Gemma using a linear adapter makes PaliGemma a powerful vision language model. The PaliGemma release comes with three types of models: 1) PT checkpoints: Pretrained models that can be fine-tuned to downstream tasks. 2) Mix checkpoints: PT models fine-tuned to a mixture of tasks. They are suitable for general-purpose inference with free-text prompts, and have great document understanding and reasoning capabilities. 3) FT checkpoints: A set of fine-tuned models, each one specialized on a different academic benchmark. They are available in various resolutions and are intended for research purposes only.

19. ***Microsoft's Project ALPINE investigates the planning capabilities of Transformer-based language models through autoregressive learning. The study abstracts planning as a network path-finding task and shows that Transformers can execute path-finding by embedding adjacency and reachability matrices within their weights. However, Transformers have limitations in identifying reachability relationships through transitivity. The study's theoretical and empirical analyses contribute to understanding the planning capabilities of autoregressive learning in networks.*** <br><br>
    15 May, Microsoft published a [paper](https://arxiv.org/pdf/2405.09220) “ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models”. The paper presents the findings of Project ALPINE which stands for ``Autoregressive Learning for Planning In NEtworks." Project ALPINE initiates a theoretical investigation into the development of planning capabilities in Transformer-based language models through their autoregressive learning mechanisms, aiming to identify any potential limitations in their planning abilities. The study abstracts planning as a network path-finding task where the objective is to generate a valid path from a specified source node to a designated target node. In terms of expressiveness, the paper shows that the Transformer is capable of executing path-finding by embedding the adjacency and reachability matrices within its weights. The theoretical analysis of the gradient-based learning dynamic of the Transformer reveals that the Transformer is capable of learning both the adjacency matrix and a limited form of the reachability matrix. These theoretical insights are then validated through experiments, which demonstrate that the Transformer indeed learns the adjacency matrix and an incomplete reachability matrix, which aligns with the predictions made in the theoretical analysis. Additionally, when applying the methodology to a real-world planning benchmark, called Blocksworld, the authors’ observations remain consistent. The theoretical and empirical analyses further unveil a potential limitation of Transformer in path-finding: it cannot identify reachability relationships through transitivity, and thus would fail when path concatenation is needed to generate a path. In summary, the findings shed new light on how the internal mechanisms of autoregressive learning enable planning in networks. This study may contribute to the understanding of the general planning capabilities in other related domains.

21. ***A paper by Google examines the performance gap between online and offline alignment algorithms in reinforcement learning from human feedback (RLHF). The study finds that online methods have a clear advantage over offline methods in reward optimization. Hypotheses such as offline data coverage and quality do not fully explain the performance difference. The research highlights the importance of on-policy sampling in AI alignment and the challenges of offline alignment algorithms, emphasizing the unique interplay between discriminative and generative capabilities.*** <br><br>
    14 May, Google published a [paper](https://arxiv.org/pdf/2405.08448) “Understanding the performance gap between online and offline alignment algorithms”. The paper states that reinforcement learning from human feedback (RLHF) is the canonical framework for large language model alignment. However, rising popularity in offline alignment algorithms challenge the need for on-policy sampling in RLHF. Within the context of reward over-optimization, the study starts with an opening set of experiments that demonstrate the clear advantage of online methods over offline methods. This prompts the authors to investigate the causes to the performance discrepancy through a series of carefully designed experimental ablations. This work shows empirically that hypotheses such as offline data coverage and data quality by itself cannot convincingly explain the performance difference. The authors also find that while offline algorithms train policy to become good at pairwise classification, it is worse at generations; in the meantime the policies trained by online algorithms are good at generations while worse at pairwise classification. This hints at a unique interplay between discriminative and generative capabilities, which is greatly impacted by the sampling process. Lastly, the research observes that the performance discrepancy persists for both contrastive and non-contrastive loss functions, and appears not to be addressed by simply scaling up policy networks. Taken together, this study sheds light on the pivotal role of on-policy sampling in AI alignment, and hints at certain fundamental challenges of offline alignment algorithms.

23. ***Salesforce and UIUC's paper details the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF), aiming to outperform offline methods. The study constructs preference models using open-source datasets to approximate human feedback and discusses theoretical insights and algorithmic principles of online RLHF. The trained model achieves impressive performance on various benchmarks. The paper provides detailed implementation guides, datasets, and code for open-source communities, demonstrating the potential of supervised fine-tuning and iterative RLHF.*** <br><br>
    13 May, Salesforce and UIUC published a [paper](https://arxiv.org/pdf/2405.07863) “RLHF Workflow: From Reward Modeling to Online RLHF”. The authors present the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF) in this technical report, which is widely reported to outperform its offline counterpart by a large margin in the recent large language model (LLM) literature. However, existing open-source RLHF projects are still largely confined to the offline learning setting. This paper aims to fill in this gap and provide a detailed recipe that is easy to reproduce for online iterative RLHF. In particular, since online human feedback is usually infeasible for open-source communities with limited resources, the study starts by constructing preference models using a diverse set of open-source datasets and using the constructed proxy preference model to approximate human feedback. Then, the authors discuss the theoretical insights and algorithmic principles behind online iterative RLHF, followed by a detailed practical implementation. The trained LLM, SFR-Iterative-DPO-LLaMA-3-8B-R, achieves impressive performance on LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarks such as HumanEval and TruthfulQA. The study has shown that supervised fine-tuning (SFT) and iterative RLHF can obtain state-of-the-art performance with fully open-source datasets. Further, the authors have made the models, curated datasets, and comprehensive step-by-step code guidebooks publicly available. Please refer to https://github.com/RLHFlow/RLHF-Reward-Modeling and https://github.com/RLHFlow/Online-RLHF for more detailed information.

25. ***A survey paper by MIT, ACU, and others discusses AI deception, defining it as inducing false beliefs for outcomes other than the truth. The authors review empirical examples of AI deception, the associated risks, and potential solutions. They recommend regulatory frameworks, bot-or-not laws, and prioritizing research funding to detect and reduce AI deception. The paper emphasizes the importance of proactive measures to prevent AI deception from destabilizing society.*** <br><br>
    10 May, MIT, ACU and other published a [paper](https://www.courthousenews.com/wp-content/uploads/2024/05/PATTER100988_proof.pdf) on Patterns “AI Deception: A Survey of Examples, Risks, and Potential Solutions”. This paper argues that a range of current AI systems have learned how to deceive humans. The authors define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. The authors first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, the paper outlines several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.

27. ***Technion–Israel Institute of Technology's paper on autonomous LLM-driven research explores whether AI can adhere to scientific values like transparency, traceability, and verifiability. The study presents an automation platform that guides LLM agents through the research process, generating research papers with programmatic information tracing. While fully autonomous research shows promise, human oversight is crucial for complex goals. The work demonstrates AI's potential to accelerate scientific discovery while enhancing traceability and transparency.*** <br><br>
    24 Apr, Technion–Israel Institute of Technology published a [paper](https://arxiv.org/pdf/2404.17605) “Autonomous LLM-driven research from data to human-verifiable research papers”. The paper indicates that as AI promises to accelerate scientific discovery, it remains unclear whether fully AI-driven research is possible and whether it can adhere to key scientific values, such as transparency, traceability and verifiability. Mimicking human scientific practices, the authors built data-to-paper, an automation platform that guides interacting LLM agents through a complete stepwise research process, while programmatically back-tracing information flow and allowing human oversight and interactions. In autopilot mode, provided with annotated data alone, data-to-paper raised hypotheses, designed research plans, wrote and debugged analysis codes, generated and interpreted results, and created complete and information-traceable research papers. Even though research novelty was relatively limited, the process demonstrated autonomous generation of de novo quantitative insights from data. For simple research goals, a fully-autonomous cycle can create manuscripts which recapitulate peer-reviewed publications without major errors in about 80-90%, yet as goal complexity increases, human co-piloting becomes critical for assuring accuracy. Beyond the process itself, created manuscripts too are inherently verifiable, as information-tracing allows to programmatically chain results, methods and data. The work thereby demonstrates a potential for AI-driven acceleration of scientific discovery while enhancing, rather than jeopardizing, traceability, transparency and verifiability. Here is [the github link](https://github.com/technion-kishony-lab/data-to-paper).

29. ***New York University's paper investigates the hidden computation in Transformer language models, particularly through chain-of-thought responses. The study finds that intermediate tokens can act as filler tokens, providing computational benefits independent of their content. This raises concerns about unauditable, hidden computations in large language models. The findings suggest that additional tokens can enhance performance but highlight the need for careful supervision to ensure transparency and accountability in AI computations.*** <br><br>
    24 Apr, New York Uni published a [paper](https://arxiv.org/pdf/2404.15758) “Let's Think Dot by Dot: Hidden Computation in Transformer Language Models”. The authors find that Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. The study shows that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, the authors find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. The paper also provides a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, the results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.
 <br><br><br>

***12 May 2024***

1. ***Researchers from several institutions published a paper in Science detailing a high-resolution reconstruction of a portion of the human cerebral cortex. The reconstruction, which comprises 57,000 cells, 230 millimeters of blood vessels, and 150 million synapses, offers valuable insights into the brain's structure and function. Glia cells outnumber neurons, and the study highlights rare powerful axonal inputs among thousands of weaker connections. Further exploration using this resource promises to deepen our understanding of the human brain.*** <br><br>
   10 May, Harvard Uni, Uni of London, Google, Princeton Uni. etc published a [paper](https://www.science.org/doi/10.1126/science.adk4858) on Science, “A petavoxel fragment of human cerebral cortex reconstructed at nanoscale resolution”. The authors indicate that to fully understand how the human brain works, knowledge of its structure at high resolution is needed. Presented here is a computationally intensive reconstruction of the ultrastructure of a cubic millimeter of human temporal cortex that was surgically removed to gain access to an underlying epileptic focus. It contains about 57,000 cells, about 230 millimeters of blood vessels, and about 150 million synapses and comprises 1.4 petabytes. Our analysis showed that glia outnumber neurons 2:1, oligodendrocytes were the most common cell, deep layer excitatory neurons could be classified on the basis of dendritic orientation, and among thousands of weak connections to each neuron, there exist rare powerful axonal inputs of up to 50 synapses. Further studies using this resource may bring valuable insights into the mysteries of the human brain. (**Comments: [Some](https://news.ycombinator.com/item?id=36413296) believe GPT4 has about 1.7T parameters, and the human brain has about 3000 trillion synapses, so GPT4 is  about 0.00057 of human brain if take synapses as parameters in GPT4**)

3. ***The UK’s AI Safety Institute has released its AI testing and evaluation platform, Inspect, to the global AI community. Inspect aims to facilitate safe innovation in AI models by enabling users to assess specific capabilities and produce scores based on their results. This open-source software library supports diverse applications, including core knowledge assessment, reasoning ability evaluation, and autonomous capabilities testing, contributing to consistent AI safety evaluations worldwide.*** <br><br>
   10 May, according to [infosecurity-magazine](https://www.infosecurity-magazine.com/news/platform-to-accelerate-safe-ai/), The UK’s AI Safety Institute has made its AI testing and evaluation platform available to the global AI community as of 10 May, 2024. The platform, called Inspect, is set to pave the way for the safe innovation of AI models, according to the AI Safety Institute and Department for Science, Innovation and Technology (DIST). By making Inspect available to the global community, the Institute said it is helping accelerate the work on AI safety evaluations carried out internationally. The aim is that this leads to better safety testing and the development of more secure models. It also allows for a consistent approach to AI safety evaluations around the world. according to the government. Inspect is a software library which enables testers – from start-ups, academia and AI developers to international governments – to assess specific capabilities of individual models and then produce a score based on their results. Inspect can be used to evaluate models in a range of areas, including their core knowledge, ability to reason, and autonomous capabilities. Released through an open-source license, it means Inspect it is now freely available for the AI community to use. AI Safety Institute Chair, Ian Hogarth, commented: “We have been inspired by some of the leading open source AI developers - most notably projects like GPT-NeoX, OLMo or Pythia which all have publicly available training data and OSI-licensed training and evaluation code, model weights, and partially trained checkpoints. This is our effort to contribute back.”

5. ***A study by Google and the Israel Institute of Technology investigates the impact of fine-tuning large language models (LLMs) on new knowledge acquisition and its potential to induce factual inaccuracies or hallucinations. The research reveals that LLMs struggle to incorporate new factual knowledge efficiently during fine-tuning, leading to a linear increase in hallucinations as new knowledge is acquired. These findings underscore the challenges of introducing new knowledge through fine-tuning and emphasize the reliance of LLMs on pre-existing knowledge acquired during pre-training.*** <br><br>
   9 May, Google and Israel Inst. of Tech. published a [paper](https://arxiv.org/pdf/2405.05904) “Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?” The paper argues that when large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. This paper studies the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, the authors design a controlled setup, focused on closed-book QA, where the authors vary the proportion of the fine-tuning examples that introduce new knowledge. The study demonstrates that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, the study also found that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate. Taken together, the results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently.

7. ***Court documents from a class-action lawsuit against OpenAI reveal the removal of two extensive datasets, referred to as "books1" and "books2," which were used to train the GPT-3 AI model. These datasets, containing over 100,000 published books, were central to claims of copyright infringement. OpenAI ceased using the datasets in late 2021 and subsequently deleted them in mid-2022. The departure of the researchers responsible for creating the datasets adds complexity to the legal dispute.*** <br><br>
   7 May, BusinessInsider published an [article](https://www.businessinsider.com/openai-destroyed-ai-training-datasets-lawsuit-authors-books-copyright-2024-5) “OpenAI destroyed a trove of books used to train AI models. The employees who collected the data are gone.”. Recently unsealed court documents in the class-action lawsuit filed by the Authors Guild against OpenAI reveal that the startup removed two extensive datasets, referred to as “books1” and “books2,” which had been utilized to train its GPT-3 artificial intelligence model. Attorneys representing the Authors Guild asserted in legal filings that these datasets likely contained “over 100,000 published books” and were pivotal to their claims that OpenAI employed copyrighted materials for AI model training. In a 2020 white paper, OpenAI characterized the “books1” and “books2” datasets as “internet-based book corpora,” constituting 16% of the training data used in developing GPT-3. The same white paper indicated that the combined “books1” and “books2” datasets comprised 67 billion tokens, roughly equivalent to 50 billion words. Notably, the use of “books1” and “books2” for model training ceased in late 2021, and the datasets were subsequently deleted in mid-2022 due to disuse. Additionally, the two researchers responsible for creating “books1” and “books2” are no longer affiliated with OpenAI.

9. ***Researchers from JKU Linz, Austria, introduce xLSTM, an extension of Long Short-Term Memory (LSTM) models designed to address their limitations in language modeling compared to Transformer technology. By incorporating exponential gating and modifying LSTM memory structures, xLSTM demonstrates improved performance and scalability in language modeling tasks, rivaling state-of-the-art Transformer models and outperforming LSTMs at scale.*** <br><br>
    7 May, JKU Linz Austria published a [paper](https://arxiv.org/pdf/2405.04517) “xLSTM: Extended Long Short-Term Memory”. The authors indicate that in the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. The authors now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, the study introduces exponential gating with appropriate normalization and stabilization techniques. Secondly, the authors modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.

11. ***The International Conference on Learning Representations (ICLR) 2024 announces Outstanding Paper Awards and a Test of Time Award. Recognized papers cover a range of topics, including generalization in diffusion models, learning interactive real-world simulators, and protein discovery. The Test of Time Award goes to "Auto-Encoding Variational Bayes," highlighting the enduring impact of the paper in the field.*** <br><br>
    7 May, ICLR 2024 announced the [Outstanding Paper](https://blog.iclr.cc/2024/05/06/iclr-2024-outstanding-paper-awards/) Awards and [Test of Time Award](https://blog.iclr.cc/2024/05/07/iclr-2024-test-of-time-award/). The titles of the outstanding paper awards include: Generalization in diffusion models arises from geometry-adaptive harmonic representations; Learning Interactive Real-World Simulators; Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors; Protein Discovery with Discrete Walk-Jump Sampling; and Vision Transformers Need Registers. The Test of Time Award is “Auto-Encoding Variational Bayes”, and the Runner Up is “Intriguing properties of neural networks” 

13. ***IBM introduces the Granite series of decoder-only code models for software development tasks, leveraging Large Language Models (LLMs) trained on code. The Granite Code models offer a wide range of capabilities, including code generation, bug fixing, and documentation, catering to enterprise software development workflows. Released under an open-source license, these models demonstrate state-of-the-art performance across various coding tasks, facilitating improved productivity for developers.*** <br><br>
    7 May, IBM published a [paper](https://arxiv.org/pdf/2405.04324v1) “Granite Code Models: A Family of Open Foundation Models for Code Intelligence”. The paper indicates that Large Language Models (LLMs) trained on code are revolutionizing the software development process. Increasingly, code LLMs are being integrated into software development environments to improve the productivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously. Realizing the full potential of code LLMs requires a wide range of capabilities, including code generation, fixing bugs, explaining and documenting code, maintaining repositories, and more. This work introduces the Granite series of decoder-only code models for code generative tasks, trained with code written in 116 programming languages. The Granite Code models family consists of models ranging in size from 3 to 34 billion parameters, suitable for applications ranging from complex application modernization tasks to on-device memory-constrained use cases. Evaluation on a comprehensive set of tasks demonstrates that Granite Code models consistently reaches state-of-the-art performance among available open-source code LLMs. The Granite Code model family was optimized for enterprise software development workflows and performs well across a range of coding tasks (e.g. code generation, fixing and explanation), making it a versatile all around code model. The Granite Code [models are released](https://github.com/ibm-granite/granite-code-models) under an Apache 2.0 license for both research and commercial use.

15. ***Microsoft proposes vAttention, a dynamic memory management approach for serving Large Language Models (LLMs) without PagedAttention. Unlike prior systems that reserve memory for key-value caches ahead of time, vAttention enables on-demand physical memory allocation for key-value caches, reducing fragmentation and improving throughput for LLM inference. The approach leverages existing system support for demand paging, resulting in faster token generation and processing speeds compared to previous methods.*** <br><br>
    7 May, Microsoft published a [paper](https://arxiv.org/pdf/2405.04437) “vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention”. The authors argue that Efficient use of GPU memory is essential for high throughput LLM inference. Prior systems reserved memory for the KV-cache ahead-of-time, resulting in wasted capacity due to internal fragmentation. Inspired by OS-based virtual memory systems, vLLM proposed PagedAttention to enable dynamic memory allocation for KV-cache. This approach eliminates fragmentation, enabling high-throughput LLM serving with larger batch sizes. However, to be able to allocate physical memory dynamically, PagedAttention changes the layout of KV-cache from contiguous virtual memory to non-contiguous virtual memory. This change requires attention kernels to be rewritten to support paging, and serving framework to implement a memory manager. Thus, the PagedAttention model leads to software complexity, portability issues, redundancy and inefficiency. This paper proposes vAttention for dynamic KV-cache memory management. In contrast to PagedAttention, vAttention retains KV-cache in contiguous virtual memory and leverages low-level system support for demand paging, that already exists, to enable on-demand physical memory allocation. Thus, vAttention unburdens the attention kernel developer from having to explicitly support paging and avoids re-implementation of memory management in the serving framework. The paper shows that vAttention enables seamless dynamic memory management for unchanged implementations of various attention kernels. vAttention also generates tokens up to 1.97x faster than vLLM, while processing input prompts up to 3.92x and 1.45x faster than the PagedAttention variants of FlashAttention and FlashInfer.

17. ***Researchers from CMU and the University of Waterloo present MAmmoTH2, a method for efficiently harvesting large-scale instruction data from the web to enhance reasoning abilities in Large Language Models (LLMs). By leveraging naturally existing instruction data and open-source LLMs, MAmmoTH2 significantly boosts performance on reasoning benchmarks without costly human annotation or model distillation. The approach provides a new paradigm for building better instruction tuning data for LLMs.*** <br><br>
    6 May, CMU and Uni of Waterloo published a [paper](https://arxiv.org/pdf/2405.03548) “MAmmoTH2: Scaling Instructions from the Web”. The authors state that instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. The paper proposes a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. The proposed approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, the authors build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 34% on MATH and from 36% to 67% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. The work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data.

19. ***Google and the University of Massachusetts Amherst propose an adaptive retrieval and scalable indexing method for k-nearest neighbors (k-NN) search using Cross-Encoders. By efficiently computing latent query and item embeddings to approximate Cross-Encoder similarity scores, the method achieves improved recall and speed compared to existing approaches. The indexing approach enables fast and accurate k-NN search with reduced computational overhead, benefiting various information retrieval tasks.*** <br><br>
    6 May, Google and Uni Massachusetts Amherst published a [paper](https://arxiv.org/pdf/2405.03651) “Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders”. The authors indicate that Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than embedding-based models (dual-encoders) at estimating query-item relevance. Existing approaches perform k-NN search with CE by approximating the CE similarity with a vector embedding space fit either with dual-encoders (DE) or CUR matrix factorization. DE-based retrieve-and-rerank approaches suffer from poor recall on new domains and the retrieval with DE is decoupled from the CE. While CUR-based approaches can be more accurate than the DE-based approach, they require a prohibitively large number of CE calls to compute item embeddings, thus making it impractical for deployment at scale. This paper addresses these shortcomings with a proposed sparse-matrix factorization based method that efficiently computes latent query and item embeddings to approximate CE scores and performs k-NN search with the approximate CE similarity. The authors compute item embeddings offline by factorizing a sparse matrix containing query-item CE scores for a set of train queries. The method produces a high-quality approximation while requiring only a fraction of CE calls as compared to CUR-based methods, and allows for leveraging DE to initialize the embedding space while avoiding compute- and resource-intensive finetuning of DE via distillation. At test time, the item embeddings remain fixed and retrieval occurs over rounds, alternating between a) estimating the test query embedding by minimizing error in approximating CE scores of items retrieved thus far, and b) using the updated test query embedding for retrieving more items. The proposed k-NN search method improves recall by up to 5% (k=1) and 54% (k=100) over DE-based approaches. Additionally, the indexing approach achieves a speedup of up to 100x over CUR-based and 5x over DE distillation methods, while matching or improving k-NN search recall over baselines.

21. ***Google and the University of Washington introduce ImageInWords (IIW), a framework for curating hyper-detailed image descriptions and a corresponding dataset. IIW addresses the challenge of generating accurate image descriptions by involving human annotators in the annotation process. Models trained on IIW data demonstrate superior performance in generating image descriptions and vision-language reasoning tasks compared to prior approaches, underscoring the dataset's quality and utility for training Vision-Language models.*** <br><br>
    5 May, Google and Uni Washington published a [paper](https://arxiv.org/pdf/2405.02793) “ImageInWords: Unlocking Hyper-Detailed Image Descriptions”. The authors argue that despite the longstanding adage "an image is worth a thousand words," creating accurate and hyper-detailed image descriptions for training Vision-Language models remains challenging. Current datasets typically have web-scraped descriptions that are short, low-granularity, and often contain details unrelated to the visual content. As a result, models trained on such data generate descriptions replete with missing information, visual inconsistencies, and hallucinations. To address these issues, the paper introduces ImageInWords (IIW), a carefully designed human-in-the-loop annotation framework for curating hyper-detailed image descriptions and a new dataset resulting from this process. The study validates the framework through evaluations focused on the quality of the dataset and its utility for fine-tuning with considerations for readability, comprehensiveness, specificity, hallucinations, and human-likeness. The dataset significantly improves across these dimensions compared to recently released datasets (+66%) and GPT-4V outputs (+48%). Furthermore, models fine-tuned with IIW data excel by +31% against prior work along the same human evaluation dimensions. Given the fine-tuned models, the study also evaluates text-to-image generation and vision-language reasoning. The proposed model's descriptions can generate images closest to the original, as judged by both automated and human metrics. The researchers also find the model produces more compositionally rich descriptions, outperforming the best baseline by up to 6% on ARO, SVO-Probes, and Winoground datasets. [Link to the dataset](https://huggingface.co/datasets/google/imageinwords).

23. ***Huggingface and Sorbonne University conduct extensive experiments to identify critical design decisions in building vision-language models (VLMs). The study highlights the importance of architecture choice, data, and training methods in achieving state-of-the-art performance in VLMs. The development of Idefics2, an efficient VLM with 8 billion parameters, exemplifies these findings, achieving competitive performance while being computationally efficient.*** <br><br>
    3 May, Huggingface and Sorbonne Uni. published a [paper](https://arxiv.org/pdf/2405.02246) “What matters when building vision-language models?”. The authors find that the growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, the paper observes that critical decisions regarding the design of VLMs are often not justified. The researchers argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, the study conducts extensive experiments around pre-trained models, architecture choice, data, and training methods. The consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. Finds include 1) The cross-attention architecture performs better than the fully autoregressive one when unimodal pre-trained backbones are kept frozen. However, when training the unimodal backbones, the fully autoregressive architecture outperforms the cross-attention one, even though the latter has more parameters. 2) Reducing the number of visual tokens with learned pooling significantly improves compute efficiency at training and inference while improving performance on downstream tasks. 3) Splitting images into sub-images during training allow trading compute efficiency for more performance during inference. The authors [release the model](https://huggingface.co/collections/HuggingFaceM4/idefics2-661d1971b7c50831dd3ce0fe) (base, instructed, and chat) along with the datasets created for its training.

25. ***Apple proposes superposition prompting as a novel methodology to improve retrieval-augmented generation (RAG) in large language models (LLMs). The method allows LLMs to process input documents in parallel prompt paths, enhancing time efficiency and accuracy in question-answering benchmarks. Superposition prompting significantly reduces compute time while improving accuracy, addressing key drawbacks of LLMs in processing long contexts and handling irrelevant information.*** <br><br>
    10 Apr, Apple published a [paper](https://arxiv.org/pdf/2404.06910) “Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation”. The paper argues that despite the successes of large language models (LLMs), they exhibit significant drawbacks, particularly when processing long contexts. Their inference cost scales quadratically with respect to sequence length, making it expensive for deployment in some real-world text processing applications, such as retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the "distraction phenomenon," where irrelevant context in the prompt degrades output quality. To address these drawbacks, the research proposes a novel RAG prompting methodology, superposition prompting, which can be directly applied to pre-trained transformer-based LLMs without the need for fine-tuning. At a high level, superposition prompting allows the LLM to process input documents in parallel prompt paths, discarding paths once they are deemed irrelevant. The paper demonstrates the capability of the proposed method to simultaneously enhance time efficiency across a variety of question-answering benchmarks using multiple pre-trained LLMs. Furthermore, the technique significantly improves accuracy when the retrieved context is large relative to the context the model was trained on. For example, the approach facilitates an 93x reduction in compute time while improving accuracy by 43\% on the NaturalQuestions-Open dataset with the MPT-7B instruction-tuned model over naive RAG.

27. ***Enkrypt AI investigates the impact of fine-tuning and quantization on vulnerabilities in Large Language Models (LLMs). The study reveals that fine-tuning and quantization decrease jailbreak resistance in LLMs, increasing their susceptibility to attacks. The findings underscore the importance of considering security implications when fine-tuning LLMs for specialized tasks, highlighting the need for external guardrails to mitigate vulnerabilities.*** <br><br>
    5 Apr, Enkrypt AI published a [paper](https://arxiv.org/pdf/2404.04392) “Increased LLM Vulnerabilities from Fine-tuning and Quantization”. The paper finds that Large Language Models (LLMs) have become very popular and have found use cases in many domains, such as chatbots, auto-task completion agents, and much more. However, LLMs are vulnerable to different types of attacks, such as jailbreaking, prompt injection attacks, and privacy leakage attacks. Foundational LLMs undergo adversarial and alignment training to learn not to generate malicious and toxic content. For specialized use cases, these foundational LLMs are subjected to fine-tuning or quantization for better performance and efficiency. The authors examine the impact of downstream tasks such as fine-tuning and quantization on LLM vulnerability. The study tests foundation models like Mistral, Llama, MosaicML, and their fine-tuned versions. The research shows that fine-tuning and quantization reduces jailbreak resistance significantly, leading to increased LLM vulnerabilities. Finally, the authors demonstrate the utility of external guardrails in reducing LLM vulnerabilities.
 <br><br><br>

***5 May 2024***

1. ***The KANs paper introduces Kolmogorov-Arnold Networks (KANs) as alternatives to Multi-Layer Perceptrons (MLPs), utilizing learnable activation functions on edges instead of fixed ones on nodes. KANs outperform MLPs in accuracy and interpretability, offering promising opportunities for improving deep learning models.*** <br><br>
   2 May, MIT, California Inst of Tech, inter alia published a [paper](https://arxiv.org/pdf/2404.19756) “KAN: Kolmogorov-Arnold Networks”. Inspired by the Kolmogorov-Arnold representation theorem, the authors propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons"), KANs have learnable activation functions on edges ("weights"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. The study shows that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs. Codes are available at [this https URL] (https://github.com/KindXiaoming/pykan). Here is some [discussions on Kans](https://www.reddit.com/r/MachineLearning/comments/1clcu5i/d_kolmogorovarnold_network_is_just_an_mlp/).

3. ***NeMo-Aligner, a toolkit for model alignment, is introduced to efficiently align Large Language Models (LLMs) with human values and preferences. It supports various alignment paradigms and is designed for scalability and extensibility, aiming to make LLMs safer and more helpful.*** <br><br>
   2 May, Nvidia published a [paper](https://arxiv.org/pdf/2405.01481) “NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment”. The authors argue that Aligning Large Language Models (LLMs) with human values and preferences is essential for making them helpful and safe. However, building efficient tools to perform alignment can be challenging, especially for the largest and most competent LLMs which often contain tens or hundreds of billions of parameters. The study creates NeMo-Aligner, a toolkit for model alignment that can efficiently scale to using hundreds of GPUs for training. NeMo-Aligner comes with highly optimized and scalable implementations for major paradigms of model alignment such as: Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally, the toolkit supports running most of the alignment techniques in a Parameter Efficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed for extensibility, allowing support for other alignment techniques with minimal effort. It is open-sourced with Apache 2.0 License and Nvidia invites community contributions at https://github.com/NVIDIA/NeMo-Aligner

5. ***Oracle Database 23ai, formerly 23c, introduces AI Vector Search, enabling the generation and storage of multi-dimensional representations of various data types. This allows users to perform similarity searches combined with SQL queries, enhancing data analysis capabilities.*** <br><br>
   2 May, according to a [blog released by Oracle](https://blogs.oracle.com/database/post/oracle-23ai-now-generally-available), Oracle renamed its new database from  Oracle Database 23c to Oracle Database 23ai. The new version introduces AI Vector Search, a powerful new technology that enables one to leverage a new generation of AI models to generate and store vectors.  These vectors, sometimes referred to as embeddings, are multi-dimensional representations of documents, images, videos, sound, etc.  By encoding these objects as vectors, a user gains the ability to look for similarities between them using mathematical calculations.  The real power of Oracle Database23ai's solution is that one can combine these similarity searches with searches on business data using simple SQL.. (Comments: last year, traditional DB products such as PostgreSQL, No-SQL databases such as Cassandra, and traditional Information Retrieval products such as ElasticSearch, Solr, etc already have integrated vector search in their products. There are also specific vector database products such as Milvus, Chroma Qdrant, etc that provide search functions for both structured and unstructured data).

7. ***Maximum diffusion reinforcement learning addresses correlations in experiences of embodied agents, overcoming challenges in traditional machine learning techniques. It enables single-shot learning and outperforms existing methods, facilitating transparent and reliable decision-making.*** <br><br>
   2 May, [Nature Machine Intelligence](https://www.nature.com/articles/s42256-024-00829-3) published a paper “Maximum diffusion reinforcement learning”. The paper finds that robots and animals both experience the world through their bodies and senses. Their embodiment constrains their experiences, ensuring that they unfold continuously in space and time. As a result, the experiences of embodied agents are intrinsically correlated. Correlations create fundamental challenges for machine learning, as most techniques rely on the assumption that data are independent and identically distributed. In reinforcement learning, where data are directly collected from an agent’s sequential experiences, violations of this assumption are often unavoidable. This study derives a method that overcomes this issue by exploiting the statistical mechanics of ergodic processes, which is termed as maximum diffusion reinforcement learning. By decorrelating agent experiences, the approach provably enables single-shot learning in continuous deployments over the course of individual task attempts. Moreover, the researchers prove the approach generalizes well-known maximum entropy techniques and robustly exceeds state-of-the-art performance across popular benchmarks. The results at the nexus of physics, learning and control form a foundation for transparent and reliable decision-making in embodied reinforcement learning agents. 

9. ***Tech engineering faces challenges with layoffs and AI's impact on job roles, as highlighted by tech influencer Deedy Das. Startups prefer experienced hires over fresh graduates, and AI threatens traditional roles, leading to unemployment among Computer Science graduates.*** <br><br>
    2 May, according to [The Economic Times](https://economictimes.indiatimes.com/jobs/fresher/influencer-predicts-tough-times-ahead-for-techies-says-software-engineering-no-longer-a-guarantee-of-success-and-wealth/articleshow/109778330.cms?from=mdr), Tech engineering, once a booming sector, is facing challenges as big tech companies, such as Microsoft and Google, have undertaken mass layoffs in recent years. Deedy Das, a prominent tech influencer on X (formerly Twitter), has raised alarms about an impending "winter" for tech engineering. Through a series of Twitter threads, Deedy has highlighted stark differences between the current landscape and the industry's state two decades ago. Contrary to earlier notions, where pursuing Computer Science (CS) seemed a sure path to employment in the burgeoning field of Edtech, Deedy suggests a grim reality. Despite significant layoffs, companies exhibit reluctance towards new hiring initiatives. Startups, in particular, are leaning towards experienced hires, shunning fresh graduates. Moreover, the rise of artificial intelligence (AI) poses a looming threat to traditional job roles. Deedy underscores how AI algorithms are progressively replacing tasks previously performed by humans. With AI's growing prominence, the demand for human-centric software engineering roles may dwindle further. While acknowledging the cyclical nature of technological trends, Deedy remains skeptical about a swift resurgence in the tech job market. He observes a reluctance among startups to invest in training new graduates, citing the high costs involved. Consequently, a significant percentage of Computer Science graduates find themselves unemployed, highlighting a mismatch between academic training and industry demands.

11. ***Self-Play Preference Optimization proposes a method for language model alignment, treating it as a game to identify Nash equilibrium policy. It achieves state-of-the-art performance without external supervision, showcasing its effectiveness in language model alignment.*** <br><br>
    1 May, UCLA and CMU published a [paper](https://arxiv.org/pdf/2405.00675) “Self-Play Preference Optimization for Language Model Alignment”. The paper finds that traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. This study proposes a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. The proposed approach, dubbed Self-Play Preference Optimization (SPPO), approximates the Nash equilibrium through iterative policy updates and enjoys theoretical convergence guarantee. The method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO). In the experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and the Open LLM Leaderboard. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models.

13. ***AI Chatbots are increasingly involved in scientific publishing, with some papers showing signs of AI involvement. While detection methods are limited, concerns arise regarding the potential impact of AI-generated content on scientific integrity.*** <br><br>
    1 May, Scientific American published a [paper](https://www.scientificamerican.com/article/chatbots-have-thoroughly-infiltrated-scientific-publishing/) “AI Chatbots Have Thoroughly Infiltrated Scientific Publishing”. The paper finds that one percent of scientific articles published in 2023 showed signs of generative AI’s potential involvement, according to a recent analysis. Some of these tells—such as the inadvertent inclusion of “certainly, here is a possible introduction for your topic” in a recent paper in Surfaces and Interfaces, a journal published by Elsevier—are reasonably obvious evidence that a scientist used an AI chatbot known as a large language model (LLM). But “that’s probably only the tip of the iceberg,” says scientific integrity consultant Elisabeth Bik. In most other cases AI involvement isn’t as clear-cut, and automated AI text detectors are unreliable tools for analyzing a paper. Researchers from several fields have, however, identified a few key words and phrases (such as “complex and multifaceted”) that tend to appear more often in AI-generated sentences than in typical human writing. “When you’ve looked at this stuff long enough, you get a feel for the style,” says Andrew Gray, a librarian and researcher at University College London. The paper also finds that AI certainly has been used to produce scientific diagrams and illustrations that have often been included in academic papers—including, notably, one bizarrely endowed rodent—and even to replace human participants in experiments. The paper ends by saying that if AI-generated judgments creep into academic papers alongside AI text, that concerns experts, including Matt Hodgkinson, a council member of the Committee on Publication Ethics, a U.K.-based nonprofit organization that promotes ethical academic research practices. Chatbots are “not good at doing analysis,” he says, “and that’s where the real danger lies.”

15. ***n-Context Learning explores the behavior of learning with long-context models, showing performance gains with larger datasets. It identifies properties of in-context learning and long-context models, providing insights into their effectiveness.*** <br><br>
    30 Apr, CMU and Tel Aviv Uni published a [paper](https://arxiv.org/pdf/2405.00200) “In-Context Learning with Long-Context Models: An In-Depth Exploration”. The paper finds that as model context lengths continue to increase, the number of demonstrations that can be provided in-context approaches the size of entire training datasets. The paper studies the behavior of in-context learning (ICL) at this extreme scale on multiple datasets and models. The authors show that, for many datasets with large label spaces, performance continues to increase with hundreds or thousands of demonstrations. The paper contrasts this with example retrieval and finetuning: example retrieval shows excellent performance at low context lengths but has diminished gains with more demonstrations; finetuning is more data hungry than ICL but can sometimes exceed long-context ICL performance with additional data. The researchers use this ICL setting as a testbed to study several properties of both in-context learning and long-context models. The study shows that long-context ICL is less sensitive to random input shuffling than short-context ICL, that grouping of same-label examples can negatively impact performance, and that the performance boosts the authors see do not arise from cumulative gain from encoding many examples together. The research concludes that although long-context ICL can be surprisingly effective, most of this gain comes from attending back to similar examples rather than task learning.

17. ***Multi-token prediction improves large language model training efficiency and downstream capabilities without additional training time overhead. It enhances performance on coding tasks and algorithmic reasoning, offering faster inference times.*** <br><br>
    30 Apr, Meta published a [paper](https://arxiv.org/pdf/2404.19737) “Better & Faster Large Language Models via Multi-token Prediction”. The paper finds that large language models such as GPT and Llama are trained with a next-token prediction loss. The study suggests that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, the authors ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, the paper measures improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where the models consistently outperform strong baselines by several percentage points. The 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.

19. ***AdvPrompter introduces a method for generating adversarial prompts to assess language model vulnerabilities. It efficiently generates human-readable adversarial prompts, improving model robustness against jailbreaking attacks.*** <br><br>
    29 Apr, Meta and Max-Planck-Institute publish a [paper](https://arxiv.org/pdf/2404.16873) “AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs”. The authors indicate that while recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires finding adversarial prompts that cause such jailbreaking, e.g. by appending a suffix to a given instruction, which is inefficient and time-consuming. On the other hand, automatic adversarial prompt generation often leads to semantically meaningless attacks that can easily be detected by perplexity-based filters, may require gradient information from the TargetLLM, or do not scale well due to time-consuming discrete optimization processes over the token space. This study presents a novel method that uses another LLM, called the AdvPrompter, to generate human-readable adversarial prompts in seconds, sim800times faster than existing optimization-based approaches. The authors train the AdvPrompter using a novel algorithm that does not require access to the gradients of the TargetLLM. This process alternates between two steps: (1) generating high-quality target adversarial suffixes by optimizing the AdvPrompter predictions, and (2) low-rank fine-tuning of the AdvPrompter with the generated adversarial suffixes. The trained AdvPrompter generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show state-of-the-art results on the AdvBench dataset, that also transfer to closed-source black-box LLM APIs. Further, the researchers demonstrate that by fine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores.

21. ***LoRA Land presents fine-tuned large language models using Low Rank Adaptation (LoRA), showing improved performance over base models and even GPT-4. It also introduces LoRAX, a multi-LoRA inference server for efficient model deployment.*** <br><br>
    29 Apr, a start-up named Predibase published a [paper](https://arxiv.org/pdf/2405.00732) “LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report”. The authors argue that Low Rank Adaptation (LoRA) has emerged as one of the most widely adopted methods for Parameter Efficient Fine-Tuning (PEFT) of Large Language Models (LLMs). LoRA reduces the number of trainable parameters and memory usage while achieving comparable performance to full fine-tuning. The study aims to assess the viability of training and serving LLMs fine-tuned with LoRA in real-world applications. First, the authors measure the quality of LLMs fine-tuned with quantized low rank adapters across 10 base models and 31 tasks for a total of 310 models. The paper finds that 4-bit LoRA fine-tuned models outperform base models by 34 points and GPT-4 by 10 points on average. Second, the authors investigate the most effective base models for fine-tuning and assess the correlative and predictive capacities of task complexity heuristics in forecasting the outcomes of fine-tuning. Finally, the research evaluates the latency and concurrency capabilities of LoRAX, an open-source Multi-LoRA inference server that facilitates the deployment of multiple LoRA fine-tuned models on a single GPU using shared base model weights and dynamic adapter loading. LoRAX powers LoRA Land, a web application that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA A100 GPU with 80GB memory. LoRA Land highlights the quality and cost-effectiveness of employing multiple specialized LLMs over a single, general-purpose LLM.

23. ***PCAST's report outlines recommendations for responsible AI use in scientific discovery, emphasizing equity in resource sharing, secure data access, collaboration, and transparent AI use to mitigate risks in research.*** <br><br>
    29 Apr, the President’s Council of Advisors on Science and Technology [(PCAST)](https://www.whitehouse.gov/pcast/briefing-room/2024/04/29/pcast-releases-report-on-supercharging-research-harnessing-artificial-intelligence-to-meet-global-challenges/) released a [report](https://www.whitehouse.gov/wp-content/uploads/2024/04/AI-Report_Letter-ExSumm-29APRIL2024_SEND.pdf) that recommends new actions to help USA responsibly harness the power of Artificial Intelligence (AI) to accelerate scientific discovery. This report comes in response to President Biden’s charge in his Executive Order on the Safe, Secure, Trustworthy Development and Use of Artificial Intelligence. The report outlines a forward-looking approach to advance the safe and effective use of AI to empower human researchers, explore scientific possibilities, and to mitigate risks in an increasingly digital world. Specifically, PCAST’s recommendations include:  1) Expanding existing efforts to broadly and equitably share basic AI resources to benefit the full diversity of academic researchers, national laboratories, and smaller companies and non-profit organizations. 2) Expanding secure and responsible access of anonymized federal data sets for critical research needs with appropriate protections and safeguards. 3) Supporting both basic and applied research in AI that involves collaborations across academia, industry, national laboratories, and federal agencies as outlined in the vision for the NAIRR developed by the NAIRR Task Force. 4) Adopting principles of responsible, transparent, and trustworthy AI use throughout all stages of the scientific research process to manage the risks of inaccurate, biased, harmful, or non-replicable findings from scientific uses of AI. 5) Encouraging innovative approaches to integrating AI assistance into scientific workflows to facilitate human researchers creatively conducting high quality science that utilizes AI assistance responsibly.

25. ***"We Need Structured Output" discusses the importance of constraining large language model outputs to specific formats or standards for integration into developer workflows. User-centered constraints can enhance user experience and streamline development processes.*** <br><br>
    10 Apr, Google published a [paper](https://arxiv.org/pdf/2404.07362v1) “"We Need Structured Output": Towards User-centered Constraints on Large Language Model Output”. The paper finds that Large language models can produce creative and diverse responses. However, to integrate them into current developer workflows, it is essential to constrain their outputs to follow specific formats or standards. In this work, the authors surveyed 51 experienced industry professionals to understand the range of scenarios and motivations driving the need for output constraints from a user-centered perspective. The authors identified 134 concrete use cases for constraints at two levels: low-level, which ensures the output adhere to a structured format and an appropriate length, and high-level, which requires the output to follow semantic and stylistic guidelines without hallucination. Critically, applying output constraints could not only streamline the currently repetitive process of developing, testing, and integrating LLM prompts for developers, but also enhance the user experience of LLM-powered features and applications. The researchers conclude with a discussion on user preferences and needs towards articulating intended constraints for LLMs, alongside an initial design for a constraint prototyping tool.
<br><br><br>

***28 Apr 2024***

1. ***Meta's LayerSkip: Meta introduces LayerSkip, a solution for speeding up inference in large language models (LLMs). They apply layer dropout during training, with different rates for early and later layers, and use an early exit loss mechanism. In inference, they implement a self-speculative decoding approach that reduces memory usage and benefits from shared compute. Experiments show up to 2.16x speedups in various tasks.*** <br><br>
  25 Apr, Meta published a [paper](https://arxiv.org/pdf/2404.16710) “Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding”. The authors present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training the authors apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, the study shows that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, the research presents a novel self-speculative decoding solution where the authors exit at early layers and verify and correct with remaining layers of the model. The proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. The authors run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. The implemented inference solution shows speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task.

2. ***Microsoft's Graph RAG: Microsoft proposes a Graph RAG approach for query-focused summarization over private text corpora. It combines retrieval-augmented generation with graph-based text indexing, enabling scalable answering of global questions. Graph RAG significantly improves answer comprehensiveness and diversity compared to previous methods.*** <br><br>
   24 Apr, Microsoft published a [paper](https://arxiv.org/pdf/2404.16130) “From Local to Global: A Graph RAG Approach to Query-Focused Summarization”. The authors argues that the use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as "What are the main themes in the dataset?", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, the authors propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. The proposed approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, the study shows that Graph RAG leads to substantial improvements over a naïve RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at [this https URL](https://aka.ms/graphrag).

3. ***Snowflake's Arctic: Snowflake releases the open-source 482B parameter model, Snowflake Arctic, excelling in enterprise tasks like SQL generation and coding. Arctic achieves high performance with low training compute, setting a new standard for cost-effective model training.*** <br><br>
   24 Apr, Snowflake release its open source 482B paras model [Snowflake Arctic](https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/). Arctic excels at enterprise tasks such as SQL generation, coding and instruction following benchmarks even when compared to open source models trained with significantly higher compute budgets. In fact, it sets a new baseline for cost-effective training to enable Snowflake customers to create high-quality custom models for their enterprise needs at a low cost. Arctic is also efficiently intelligent and truly open with Apache 2.0 license. Arctic is on par or better than both Llama 3 8B and Llama 2 70B on enterprise metrics, while using less than half of the training compute budget. The Artic program is available immediately from Snowflake as an inference service on its Cortex offering. Artic will be available on Amazon AWS, Hugging Face and other venues, said Snowflake.

4. ***AWS's BASS: AWS presents BASS, a batched attention-optimized speculative sampling system that improves latency and throughput in large language model hosting. BASS achieves state-of-the-art multi-sequence generation latency with superior GPU utilization.*** <br><br>
   24 Apr, AWS published a [paper](https://arxiv.org/pdf/2404.15778) “BASS: Batched Attention-optimized Speculative Sampling”. Researcher from AWA find that Speculative decoding has emerged as a powerful method to improve latency and throughput in hosting large language models. However, most existing implementations focus on generating a single sequence. Real-world generative AI applications often require multiple responses and how to perform speculative decoding in a batched setting while preserving its latency benefits poses non-trivial challenges. This paper describes a system of batched speculative decoding that sets a new state of the art in multi-sequence generation latency and that demonstrates superior GPU utilization as well as quality of generations within a time budget. For example, for a 7.8B-size model on a single A100 GPU and with a batch size of 8, each sequence is generated at an average speed of 5.8ms per token, the overall throughput being 1.1K tokens per second. These results represent state-of-the-art latency and a 2.15X speed-up over optimized regular decoding. Within a time budget that regular decoding does not finish, the system is able to generate sequences with HumanEval Pass@First of 43% and Pass@All of 61%, far exceeding what's feasible with single-sequence speculative decoding. The peak GPU utilization during decoding reaches as high as 15.8%, more than 3X the highest of that of regular decoding and around 10X of single-sequence speculative decoding.

5. ***Microsoft's Phi-3: Microsoft introduces Phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, capable of rivaling larger models' performance. The dataset used for training is heavily filtered web data and synthetic data, aligned for robustness, safety, and chat format.*** <br><br>
   22 Apr, Microsoft released Phi-3, and also published the corresponding [technical report](https://arxiv.org/pdf/2404.14219) “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone”. The report introduces phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in the dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. The report also provides some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).

6. ***Apple's OpenELM: Apple releases OpenELM, an open language model family emphasizing reproducibility and transparency. OpenELM employs layer-wise scaling for efficient parameter allocation and provides a comprehensive framework for training and evaluation, aiming to empower the open research community.*** <br><br>
    22 Apr, Apple published a [paper](https://arxiv.org/pdf/2404.14619) “OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework” announcing the release of its OpenELM models. The paper indicates that the reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, Apple releases OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring 2× fewer pre-training tokens. Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, this release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. The paper also releases code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors. The source code along with pre-trained model weights and training recipes is available at [this https URL](https://github.com/apple/corenet). Additionally, models can be found on HuggingFace at [this https URL](https://huggingface.co/apple/OpenELM). However, the performance of OpenELM is far behind Microsoft’s Phe-3.

7. ***Forbes on AI as a Commodity: Forbes discusses the commoditization of AI, comparing it to other technological advancements. While AI tools become ubiquitous, companies must innovate beyond AI to maintain a competitive edge.*** <br><br>
   22 Apr, Forbes published [an article](https://www.forbes.com/sites/joemckendrick/2024/04/22/ai-will-fade-from-view-by-design/?sh=758019ed7769) “AI Will Eventually Fade From View, By Design”. The article discusses whether Artificial Intelligence (AI) is becoming a commodity, following the path of other technologies like personal computers and smartphones. Commoditization occurs when products or technologies are widely available, interchangeable, and no longer offer a competitive advantage. The arguments For Commoditization include: 1) AI tools and platforms, especially generative AI, are now widespread and available at no or low cost. 2) Everybody uses AI to some degree, making it a standard utility like electricity or internet access. 3) Companies will need to rethink their business strategies and find new ways to add value beyond just using AI.
Counterarguments are: 1) Tech-savvy companies like Amazon and Uber have disrupted markets using cutting-edge technology, including AI. 2) AI has the potential to advance business prospects and disrupt or commoditize many businesses. The article concludes that AI will eventually become a standard tool, like electricity or internet access, and will no longer be a competitive advantage. Companies will need to focus on innovative culture and forward-looking thinking to gain an edge. AI will deliver results when treated as a tool to improve businesses' bottom lines, rather than a standalone product.

8. ***SnapKV: UIUC, Cohere, and Princeton Uni propose SnapKV, an approach to efficiently minimize key-value (KV) cache size in large language models. SnapKV reduces computational overhead and memory footprint while maintaining performance, enabling processing of long input sequences.*** <br><br>
    22 Apr, UIUC, Cohere and Princeton Uni published a [paper](https://arxiv.org/pdf/2404.14469) “SnapKV: LLM Knows What You are Looking for Before Generation”. The paper finds that Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. The authors discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an ‘observation’ window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. The proposed approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications. Code is [available here](https://github.com/FasterDecoding/SnapKV).

9. ***LLaMA3 Low-bit Quantization Study: Researchers explore the performance of Meta's LLaMA3 models when quantized to low bit-widths. They find significant performance degradation, especially in ultra-low bit-width scenarios, suggesting a need for bridging the performance gap in future developments.*** <br><br>
    22 Apr, Uni of Hong Kong, ETH Zurich and Beihang Uni published a [paper](https://arxiv.org/pdf/2404.14047) “How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study”. The authors indicates that Meta's LLaMA family has become one of the most powerful open-source Large Language Model (LLM) series. Notably, LLaMA3 models have recently been released and achieve impressive performance across various with super-large scale pre-training on over 15T tokens of data. Given the wide application of low-bit quantization for LLMs in resource-limited scenarios, The authors explore LLaMA3's capabilities when quantized to low bit-width. This exploration holds the potential to unveil new insights and challenges for low-bit quantization of LLaMA3 and other forthcoming LLMs, especially in addressing performance degradation problems that suffer in LLM compression. Specifically, the researchers evaluate the 10 existing post-training quantization and LoRA-finetuning methods of LLaMA3 on 1-8 bits and diverse datasets to comprehensively reveal LLaMA3's low-bit quantization performance. The experiment results indicate that LLaMA3 still suffers non-negligent degradation in these scenarios, especially in ultra-low bit-width. This highlights the significant performance gap under low bit-width that needs to be bridged in future developments. The paper expects that this empirical study will prove valuable in advancing future models, pushing the LLMs to lower bit-width with higher accuracy for being practical. The project is released on https://github.com/Macaronlin/LLaMA3-Quantization and quantized LLaMA3 models are released in https://huggingface.co/LLMQ.

10. ***JAT Model: Huggingface, Uni Lyon, and Uni Bordeaux present Jack of All Trades (JAT), a versatile transformer-based model optimized for sequential decision-making tasks and multimodal data. JAT demonstrates strong performance across various RL, CV, and NLP benchmarks using a single set of weights.*** <br><br>
    22 Apr, Huggingface, Uni Lyon, and Uni Bordeaux updated their [paper](https://arxiv.org/pdf/2402.09844) “Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent”. The paper argues that the search for a general model that can operate seamlessly across multiple domains remains a key goal in machine learning research. The prevailing methodology in Reinforcement Learning (RL) typically limits models to a single task within a unimodal framework, a limitation that contrasts with the broader vision of a versatile, multi-domain model. This research presents Jack of All Trades (JAT), a transformer-based model with a unique design optimized for handling sequential decision-making tasks and multimodal data types. The JAT model demonstrates its robust capabilities and versatility by achieving strong performance on very different RL benchmarks, along with promising results on Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a single set of weights. The JAT model marks a significant step towards more general, cross-domain AI model design, and notably, it is the first model of its kind to be fully open-sourced (see [this https URL](https://huggingface.co/jat-project/jat)), including a pioneering general-purpose dataset.

11. ***DataTune: CMU introduces DataTune, a method for transforming existing datasets to improve automatic dataset generation for NLP tasks. DataTune significantly increases data diversity and difficulty, enhancing model performance compared to existing methods.*** <br><br>
    22 Apr, CMU published a [paper](https://arxiv.org/pdf/2404.14361) “Better Synthetic Data by Retrieving and Transforming Existing Datasets”. The paper points out that despite recent advances in large language models, building dependable and deployable NLP models typically requires abundant, high-quality training data. However, task-specific data is not available for many use cases, and manually curating task-specific data is labor-intensive. Recent work has studied prompt-driven synthetic data generation using large language models, but these generated datasets tend to lack complexity and diversity. To address these limitations, the authors introduce a method, DataTune, to make better use of existing, publicly available datasets to improve automatic dataset generation. DataTune performs dataset transformation, enabling the repurposing of publicly available datasets into a format that is directly aligned with the specific requirements of target tasks. On a diverse set of language-based tasks from the BIG-Bench benchmark, the study finds that finetuning language models via DataTune improves over a few-shot prompting baseline by 49% and improves over existing methods that use synthetic or retrieved training data by 34%. The researchers find that dataset transformation significantly increases the diversity and difficulty of generated data on many tasks. The authors integrate DataTune into an open-source repository to make this method accessible to the community: [this https URL](https://github.com/neulab/prompt2model). (Note: Could be used with Huggingface [Commom Corpus dataset](https://huggingface.co/blog/Pclanglais/common-corpus))

12. ***STEP-BACK PROMPTING: Google introduces STEP-BACK PROMPTING, a technique enabling large language models (LLMs) to derive high-level concepts from specific details, leading to improved reasoning abilities. Experiments show substantial performance gains on reasoning-intensive tasks.*** <br><br>
    21 Apr, Google updated its ICLR 2024 take a step back [paper](https://openreview.net/pdf?id=3bq3jsvcQ1) “Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models”. The paper presents STEP-BACK PROMPTING, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. The authors conduct experiments of STEP-BACK PROMPTING with PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, STEP-BACK PROMPTING improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7% and 11% respectively, TimeQA by 27%, and MuSiQue by 7%.

20. ***Instruction Hierarchy: OpenAI proposes an instruction hierarchy to address vulnerabilities in large language models (LLMs) caused by prompt injections and other attacks. The hierarchy defines how models should behave when instructions of different priorities conflict, increasing robustness.*** <br><br>
    19 Apr, OpenAI published a [paper](https://arxiv.org/pdf/2404.13208) “The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions”. The paper states that today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts. The authors argue that one of the primary vulnerabilities underlying these attacks is that LLMs often consider system prompts (e.g., text from an application developer) to be the same priority as text from untrusted users and third parties. To address this, the paper proposes an instruction hierarchy that explicitly defines how models should behave when instructions of different priorities conflict. The paper then proposes a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions. The study applies this method to GPT-3.5, showing that it drastically increases robustness -- even for attack types not seen during training -- while imposing minimal degradations on standard capabilities.

14. ***DPO for LLMs: Stanford Uni presents Direct Preference Optimization (DPO) for large language models (LLMs), aligning it with token-level reinforcement learning and showing its equivalence to likelihood-based search algorithms. DPO improves credit assignment and search-based algorithms' performance.*** <br><br>
    18 Apr, Stanford Uni published a [paper](https://arxiv.org/pdf/2404.12358) “From r to Q∗: Your Language Model is Secretly a Q-Function”. The paper states that Reinforcement Learning From Human Feedback (RLHF) has been a critical to the success of the latest generation of generative AI models. In response to the complex nature of the classical RLHF pipeline, direct alignment algorithms such as Direct Preference Optimization (DPO) have emerged as an alternative approach. Although DPO solves the same objective as the standard RLHF setup, there is a mismatch between the two approaches. Standard RLHF deploys reinforcement learning in a specific token-level MDP, while DPO is derived as a bandit problem in which the whole response of the model is treated as a single arm. This work rectifies this difference, first the authors theoretically show that they can derive DPO in the token-level MDP as a general inverse Q-learning algorithm, which satisfies the Bellman equation. Using the theoretical results, the researchers provide three concrete empirical insights. First, the study shows that because of its token level interpretation, DPO is able to perform some type of credit assignment. Next, the authors prove that under the token level formulation, classical search-based algorithms, such as MCTS, which have recently been applied to the language generation space, are equivalent to likelihood-based search on a DPO policy. Empirically the authors show that a simple beam search yields meaningful improvement over the base DPO policy. Finally, the research shows how the choice of reference policy causes implicit rewards to decline during training. The authors conclude by discussing applications of the work, including information elicitation in multi-tun dialogue, reasoning, agentic applications and end-to-end training of multi-model systems.

15. ***FlowMind: J.P. Morgan AI Research introduces FlowMind, an automatic workflow generation system leveraging large language models (LLMs) for Robotic Process Automation (RPA). FlowMind mitigates hallucinations, ensures data integrity, and simplifies user interaction, demonstrating success in finance tasks.*** <br><br>
    J. P. Morgan AI Research published a [paper](https://arxiv.org/pdf/2404.13050) “FlowMind: Automatic Workflow Generation with LLMs”. The paper finds that the rapidly evolving field of Robotic Process Automation (RPA) has made significant strides in automating repetitive processes, yet its effectiveness diminishes in scenarios requiring spontaneous or unpredictable tasks demanded by users. This study introduces a novel approach, FlowMind, leveraging the capabilities of Large Language Models (LLMs) such as Generative Pretrained Transformer (GPT), to address this limitation and create an automatic workflow generation system. In FlowMind, the authors propose a generic prompt recipe for a lecture that helps ground LLM reasoning with reliable Application Programming Interfaces (APIs). With this, FlowMind not only mitigates the common issue of hallucinations in LLMs, but also eliminates direct interaction between LLMs and proprietary data or code, thus ensuring the integrity and confidentiality of information - a cornerstone in financial services. FlowMind further simplifies user interaction by presenting high-level descriptions of auto-generated workflows, enabling users to inspect and provide feedback effectively. The researchers also introduce NCEN-QA, a new dataset in finance for benchmarking question-answering tasks from N-CEN reports on funds. The authors used NCEN-QA to evaluate the performance of workflows generated by FlowMind against baseline and ablation variants of FlowMind. The authors demonstrate the success of FlowMind, the importance of each component in the proposed lecture recipe, and the effectiveness of user interaction and feedback in FlowMind.

16. ***RAG Model Analysis: Stanford Uni analyzes the tension between a large language model's (LLM) internal prior knowledge and retrieved information in retrieval-augmented generation (RAG) models. The study shows how the LLM's prior influences its response when conflicting with retrieved information.*** <br><br>
    16 Apr, Stanford Uni published a [paper](https://arxiv.org/pdf/2404.10198) “How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior”. The paper indicates that Retrieval augmented generation (RAG) is often used to fix hallucinations and provide up-to-date knowledge for large language models (LLMs). However, in cases when the LLM alone incorrectly answers a question, does providing the correct retrieved content always fix the error? Conversely, in cases where the retrieved content is incorrect, does the LLM know to ignore the wrong information, or does it recapitulate the error? To answer these questions, the authors systematically analyze the tug-of-war between a LLM's internal knowledge (i.e. its prior) and the retrieved information in settings when they disagree. The researchers test GPT-4 and other LLMs on question-answering abilities across datasets with and without reference documents. As expected, providing the correct retrieved information fixes most model mistakes (94% accuracy). However, when the reference document is perturbed with increasing levels of wrong values, the LLM is more likely to recite the incorrect, modified information when its internal prior is weaker but is more resistant when its prior is stronger. Similarly, the study also finds that the more the modified information deviates from the model's prior, the less likely the model is to prefer it. These results highlight an underlying tension between a model's prior knowledge and the information presented in reference documents.
 <br><br> <br>

***21 Apr 2024***
1. ***Meta released Llama3, the most capable openly available LLM yet, with plans to make it accessible on various platforms. They emphasize responsible development and offer tools like Llama Guard 2, Code Shield, and CyberSec Eval 2. The models boast enhanced performance, including improved reasoning, with sizes ranging from 8 billion to 70 billion parameters. They're trained on custom-built GPU clusters and showcased state-of-the-art performance on industry benchmarks.*** <br><br>
2.18 Apr, [Meta released Llama3](https://ai.meta.com/blog/meta-llama-3/), the most capable openly available LLM to date.  Based on the blog, Llama 3 models will soon be available on AWS, Databricks, Google Cloud, Hugging Face, Kaggle, IBM WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake, and with support from hardware platforms offered by AMD, AWS, Dell, Intel, NVIDIA, and Qualcomm. Meta is dedicated to developing Llama 3 in a responsible way, and it’s offering various resources to help others use it responsibly as well. This includes introducing new trust and safety tools with Llama Guard 2, Code Shield, and CyberSec Eval 2. Meta is expected to introduce new capabilities, longer context windows, additional model sizes, and enhanced performance, and it’ll share the Llama 3 research paper. Meta AI, built with Llama 3 technology, is now one of the world’s leading AI assistants that can boost your intelligence and lighten your load—helping you learn, get things done, create content, and connect to make the most out of every moment. Meta release two models, one has 8 billion parameters, and one has 70 billion parameters. This next generation of Llama demonstrates state-of-the-art performance on a wide range of industry benchmarks and offers new capabilities, including improved reasoning. The models were trained on two custom-built 24K GPU clusters, with about 15 trillion tokens. [Try it here](https://www.meta.ai/).

2. ***A consortium of 66 AI companies and universities unveiled v0.5 of the AI Safety Benchmark, designed to evaluate the safety risks of AI systems using chat-tuned language models. The benchmark focuses on adult-chat scenarios in English, with tests covering 7 of 13 hazard categories. They aim to release version 1.0 by the end of 2024, providing insights into AI system safety.*** <br><br>
   18 Apr, 66 top AI companies and universities published a [paper](https://arxiv.org/pdf/2404.12241) “Introducing v0.5 of the AI Safety Benchmark from MLCommons”. This paper introduces v0.5 of the AI Safety Benchmark, which has been created by the MLCommons AI Safety Working Group. The AI Safety Benchmark has been designed to assess the safety risks of AI systems that use chat-tuned language models. The paper introduces a principled approach to specifying and constructing the benchmark, which for v0.5 covers only a single use case (an adult chatting to a general-purpose assistant in English), and a limited set of personas (i.e., typical users, malicious users, and vulnerable users). The authors created a new taxonomy of 13 hazard categories, of which 7 have tests in the v0.5 benchmark. The researcher plan to release version 1.0 of the AI Safety Benchmark by the end of 2024. The v1.0 benchmark will provide meaningful insights into the safety of AI systems. However, the v0.5 benchmark should not be used to assess the safety of AI systems. The authors have sought to fully document the limitations, flaws, and challenges of v0.5. This release of v0.5 of the AI Safety Benchmark includes (1) a principled approach to specifying and constructing the benchmark, which comprises use cases, types of systems under test (SUTs), language and context, personas, tests, and test items; (2) a taxonomy of 13 hazard categories with definitions and subcategories; (3) tests for seven of the hazard categories, each comprising a unique set of test items, i.e., prompts. There are 43,090 test items in total, which were created with templates; (4) a grading system for AI systems against the benchmark; (5) an openly available platform, and downloadable tool, called ModelBench that can be used to evaluate the safety of AI systems on the benchmark; (6) an example evaluation report which benchmarks the performance of over a dozen openly available chat-tuned language models; (7) a test specification for the benchmark. This is the [link to the project](https://github.com/mlcommons/modelbench).

3. ***CMU and Meta introduce TriForce, a hierarchical speculative decoding system addressing efficiency challenges in long-sequence generation with large language models. TriForce achieves impressive speedups and scalability for long-context generation tasks while maintaining generation quality. The code is available for further exploration.*** <br><br>
   18 Apr, CMU and Meta published a [paper](https://arxiv.org/pdf/2404.11912) “TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding”. The paper indicates that eith large language models (LLMs) widely deployed in long content generation recently, there has emerged an increasing demand for efficient long-sequence inference support. However, key-value (KV) cache, which is stored to avoid re-computation, has emerged as a critical bottleneck by growing linearly in size with the sequence length. Due to the auto-regressive nature of LLMs, the entire KV cache will be loaded for every generated token, resulting in low utilization of computational cores and high latency. While various compression methods for KV cache have been proposed to alleviate this issue, they suffer from degradation in generation quality. The study introduces TriForce, a hierarchical speculative decoding system that is scalable to long sequence generation. This approach leverages the original model weights and dynamic sparse KV cache via retrieval as a draft model, which serves as an intermediate layer in the hierarchy and is further speculated by a smaller model to reduce its drafting latency. TriForce not only facilitates impressive speedups for Llama2-7B-128K, achieving up to 2.31times on an A100 GPU but also showcases scalability in handling even longer contexts. For the offloading setting on two RTX 4090 GPUs, TriForce achieves 0.108s/tokenx2014 only half as slow as the auto-regressive baseline on an A100, which attains 7.78times on the optimized offloading system. Additionally, TriForce performs 4.86times more than DeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is highlighted by its consistently outstanding performance across various temperatures. The code is available at https://github.com/Infini-AI-Lab/TriForce.

4. ***Surge Global presents OpenBezoar, a family of small, cost-effective, and open models trained on mixes of instruction data. Leveraging techniques like RLHF and DPO, they fine-tune models based on OpenLLaMA 3Bv2, achieving superior performance on various tasks. The checkpoints and datasets are released for public use.*** <br><br>
   18 Apr, Surge Global published a [paper](https://arxiv.org/pdf/2404.12195) “OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data”. The authors find that Instruction fine-tuning pretrained LLMs for diverse downstream tasks has demonstrated remarkable success and has captured the interest of both academics and practitioners. To ensure such fine-tuned LLMs align with human preferences, techniques such as RLHF and DPO have emerged. At the same time, there is increasing interest in smaller parameter counts for models. In this work, using OpenLLaMA 3Bv2 as a base model, the study describes the recipe used to fine-tune the OpenBezoar family of models. In this recipe: the authors first generate synthetic instruction fine-tuning data using an open and commercially non-restrictive instruction fine-tuned variant of the Falcon-40B model under three schemes based on: LaMini-LM, WizardLM/Evol-Instruct (with databricks-dolly-15k as a seed dataset) and Orca (with the Flan Collection as a seed dataset), then filter these generations using GPT-4 as a human proxy. The research then performs cost-effective QLoRA-based supervised fine-tuning sequentially with each scheme. The resulting checkpoint is further fine-tuned with a subset of the HH-RLHF dataset to minimize distribution shift prior to using the DPO loss to obtain the final checkpoint. Evaluation is done with the LM Eval Harness tasks/metrics as well as on MT-Bench using the "LLM-as-a-judge" framework with Claude 2.1, with the finding that the final checkpoint, "OpenBezoar-HH-RLHF-DPO", demonstrates superior performance over many models at the 3B parameter scale, even outperforming the top model in one of the categories on the Huggingface Open LLM Leaderboard. The authors release "OpenBezoar-SFT", "OpenBezoar-HH-RLHF-SFT", "OpenBezoar-HH-RLHF-DPO" checkpoints, alongside the generated datasets on HuggingFace at [here](https://huggingface.co/collections/SurgeGlobal/open-bezoar-6620a24923e12127e9e2b9cc) and the codebase at [here](https://bitbucket.org/paladinanalytics/workspace/projects/OP).

5. ***IBM and Hebrew University introduce FastFit, a Python package for fast and accurate few-shot classification, particularly useful for scenarios with many semantically similar classes. FastFit significantly improves training speed and accuracy compared to existing methods, providing a user-friendly solution for NLP practitioners.*** <br><br>
   18 Apr, IBM and Hebrew University published a [paper](https://arxiv.org/pdf/2404.12365) “When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes”.  The authors present FastFit, a method, and a Python package design to provide fast and accurate few-shot classification, especially for scenarios with many semantically similar classes. FastFit utilizes a novel approach integrating batch contrastive learning and token-level similarity score. Compared to existing few-shot learning packages, such as SetFit, Transformers, or few-shot prompting of large language models via API calls, FastFit significantly improves multiclass classification performance in speed and accuracy across FewMany, a newly curated English benchmark, and Multilingual datasets. FastFit demonstrates a 3-20x improvement in training speed, completing training in just a few seconds. The FastFit package is now available on [GitHub](https://github.com/IBM/fastfit) and PyPi, presenting a user-friendly solution for NLP practitioners.

6. ***Google explores many-shot in-context learning with large language models, observing significant performance gains across various tasks. They investigate reinforced and unsupervised learning settings, finding both effective in the many-shot regime, particularly for complex reasoning tasks.*** <br><br>
   17 Apr, Google published a [paper](https://arxiv.org/pdf/2404.11018) “Many-Shot In-Context Learning”.  The paper finds that Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, the authors observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, the study explores two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. The researchers find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, the work demonstrates that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases and can learn high-dimensional functions with numerical inputs. The analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance.

7. ***UIUC demonstrates LLM agents' ability to autonomously exploit one-day vulnerabilities in real-world systems, with GPT-4 outperforming other models and vulnerability scanners. However, its performance relies heavily on the availability of CVE descriptions, raising concerns about the widespread deployment of highly capable LLM agents.*** <br><br>
    17 Apr, UIUC published a [paper](https://arxiv.org/pdf/2404.08144) “LLM Agents can Autonomously Exploit One-day Vulnerabilities”.  The paper finds that LLMs have becoming increasingly powerful, both in their benign and malicious uses. With the increase in capabilities, researchers have been increasingly interested in their ability to exploit cybersecurity vulnerabilities. In particular, recent work has conducted preliminary studies on the ability of LLM agents to autonomously hack websites. However, these studies are limited to simple vulnerabilities.  This work shows that LLM agents can autonomously exploit one-day vulnerabilities in real-world systems. To show this, the authors collected a dataset of 15 one-day vulnerabilities that include ones categorized as critical severity in the CVE description. When given the CVE description, GPT-4 is capable of exploiting 87% of these vulnerabilities compared to 0% for every other model tested (GPT-3.5, open-source LLMs) and open-source vulnerability scanners (ZAP and Metasploit). Fortunately, the GPT-4 agent requires the CVE description for high performance: without the description, GPT-4 can exploit only 7% of the vulnerabilities. The findings raise questions around the widespread deployment of highly capable LLM agents.

8. ***CMU, Nvidia, and Microsoft propose AgentKit, an intuitive prompting framework for multifunctional agents, offering a unified approach for constructing complex "thought processes" from simple prompts. AgentKit achieves state-of-the-art performance on various tasks and could make LLM agents more accessible for a wider range of applications.*** <br><br>
    17 Apr, CMU, Nvidia, and Microsoft published a [paper](https://arxiv.org/pdf/2404.11483v1) “AgentKit: Flow Engineering with Graphs, not Coding”. The authors propose an intuitive LLM prompting framework (AgentKit) for multifunctional agents. AgentKit offers a unified framework for explicitly constructing a complex "thought process" from simple natural language prompts. The basic building block in AgentKit is a node, containing a natural language prompt for a specific subtask. The user then puts together chains of nodes, like stacking LEGO pieces. The chains of nodes can be designed to explicitly enforce a naturally structured "thought process". For example, for the task of writing a paper, one may start with the thought process of 1) identify a core message, 2) identify prior research gaps, etc. The nodes in AgentKit can be designed and combined in different ways to implement multiple advanced capabilities including on-the-fly hierarchical planning, reflection, and learning from interactions. In addition, due to the modular nature and the intuitive design to simulate explicit human thought process, a basic agent could be implemented as simple as a list of prompts for the subtasks and therefore could be designed and tuned by someone without any programming experience. Quantitatively, the study shows that agents designed through AgentKit achieve SOTA performance on WebShop and Crafter. These advances underscore AgentKit's potential in making LLM agents effective and accessible for a wider range of applications. Code available at [this https URL](https://github.com/holmeswww/AgentKit).

9. ***AWS introduces Best-fit Packing, a method to improve language model training by packing documents into training sequences without excessive truncations. Empirical results show superior performance across various tasks compared to traditional concatenation methods.*** <br><br>
    16 Apr, AWS published a [paper](https://arxiv.org/pdf/2404.10830) “Fewer Truncations Improve Language Modeling”.  The authors state that in large language model training, input documents are typically concatenated together and then split into sequences of equal length to avoid padding tokens. Despite its efficiency, the concatenation approach compromises data integrity -- it inevitably breaks many documents into incomplete pieces, leading to excessive truncations that hinder the model from learning to compose logically coherent and factually consistent content that is grounded on the complete context. To address the issue, the work proposes Best-fit Packing, a scalable and efficient method that packs documents into training sequences through length-aware combinatorial optimization. The method completely eliminates unnecessary truncations while retaining the same training efficiency as concatenation. Empirical results from both text and code pre-training show that the proposed method achieves superior performance (e.g., relatively +4.7% on reading comprehension; +16.8% in context following; and +9.2% on program synthesis), and reduces closed-domain hallucination effectively by up to 58.3%.

10. ***Princeton Uni introduces the USACO benchmark for evaluating language models in competitive programming, revealing challenges in achieving high accuracy. Human-in-the-loop studies demonstrate the potential of targeted hints to improve model performance, paving the way for future advancements in grounded reasoning.*** <br><br>
    16 Apr, Princeton Uni published a [paper](https://arxiv.org/pdf/2404.10952) “Can Language Models Solve Olympiad Programming?”. The researchers find that computing olympiads contain some of the most challenging problems for humans, requiring complex algorithmic reasoning, puzzle solving, in addition to generating efficient code. However, it has been understudied as a domain to evaluate language models (LMs). The work introduces the USACO benchmark with 307 problems from the USA Computing Olympiad, along with high-quality unit tests, reference code, and official analyses for each problem. These resources enable the authors to construct and test a range of LM inference methods for competitive programming for the first time. It is found GPT-4 only achieves a 8.7% pass@1 accuracy with zero-shot chain-of-thought prompting, and the best inference method improves it to 20.2% using a combination of self-reflection and retrieval over episodic knowledge. However, this is far from solving the benchmark. To better understand the remaining challenges, the researchers design a novel human-in-the-loop study and surprisingly find that a small number of targeted hints enable GPT-4 to solve 13 out of 15 problems previously unsolvable by any model and method. The benchmark, baseline methods, quantitative results, and qualitative analysis serve as an initial step toward LMs with grounded, creative, and algorithmic reasoning. Code and data are at [this link](https://princeton-nlp.github.io/USACOBench/).

11. ***The UK government is crafting legislation to regulate artificial intelligence, particularly focusing on large language models, amid concerns about potential harms and the dominance of tech companies in AI development. The legislation aims to mandate safety testing and algorithm sharing while avoiding stifling industry growth.*** <br><br>
    15 Apr, according to [Financial Time](https://www.ft.com/content/311b29a4-bbb3-435b-8e82-ae19f2740af9), The UK government is beginning to craft new legislation to regulate artificial intelligence, months after the prime minister vowed “not to rush” setting up rules for the fast-growing technology. Such legislation would likely put limits on the production of large language models, the general-purpose technology that underlies AI products such as OpenAI’s ChatGPT. It would likely look to mandate that companies developing the most sophisticated models share their algorithms with the government and provide evidence that they have carried out safety testing. The plans come as regulators including the UK competition watchdog have become increasingly concerned about potential harms. These range from the possibility that technology could bake in biases that affect certain demographics, to the potential use of general-purpose models to create harmful materials. It is said the rules would apply to the large language models that sit behind AI products such as ChatGPT, rather than the applications themselves. UK’s Competition and Markets Authority officer expressed “real concerns” that a small number of tech companies creating AI foundation models “may have both the ability and the incentive to shape these markets in their own interest”. The regulator identified an “interconnected web” of more than 90 partnerships and strategic investments involving the same companies: Google, Apple, Microsoft, Meta, Amazon and Nvidia.  The UK has been reluctant to push for legal interventions in the development and rollout of AI models for fear tough regulation might stymie industry growth. It has instead relied on voluntary agreements with governments and companies, ruling out legislation in the short term. Government officials singled out so-called “general purpose” AI models — those that are highly intelligent and adaptable for use on a wide range of tasks — as likely targets for further legal and regulatory intervention in a recent consultation response.

12. ***Tinkoff.AI proposes Trust Region DPO as a new alignment method for language models, outperforming existing methods like DPO on various datasets. The approach improves model quality across multiple parameters, addressing challenges in alignment stability.*** <br><br>
    15 Apr, Tinkoff.AI published a [paper](https://arxiv.org/pdf/2404.09656) “Learn Your Reference Model for Real Good Alignment”.  The paper states that the complexity of the alignment problem stems from the fact that existing methods are unstable. Researchers continuously invent various tricks to address this shortcoming. For instance, in the fundamental Reinforcement Learning From Human Feedback (RLHF) technique of Language Model alignment, in addition to reward maximization, the Kullback-Leibler divergence between the trainable policy and the SFT policy is minimized. This addition prevents the model from being overfitted to the Reward Model (RM) and generating texts that are out-of-domain for the RM. The Direct Preference Optimization (DPO) method reformulates the optimization task of RLHF and eliminates the Reward Model while tacitly maintaining the requirement for the policy to be close to the SFT policy. This paper argues that this implicit limitation in the DPO method leads to sub-optimal results. The authors propose a new method called Trust Region DPO (TR-DPO), which updates the reference policy during training. With such a straightforward update, the study demonstrates the effectiveness of TR-DPO against DPO on the Anthropic HH and TLDR datasets. The researchers show that TR-DPO outperforms DPO by up to 19%, measured by automatic evaluation with GPT-4. The new alignment approach allows the authors to improve the quality of models across several parameters at once, such as coherence, correctness, level of detail, helpfulness, and harmlessness.

13. ***Stanford University releases the 2024 Artificial Intelligence Index Report, highlighting key trends and challenges in AI development, including AI's impact on productivity, the need for robust evaluations, and increasing regulatory scrutiny.*** <br><br>
    15 Apr, Stanford University released its [annual AI report](https://aiindex.stanford.edu/report/), the 2024 Artificial Intelligence Index Report. Top takeaways include AI beats humans on some tasks, but on all; Industry continues to dominate frontier AI research; Frontier models get way more expensive; The United States leads China, the EU, and the U.K. as the leading source of top AI models; Robust and standardized evaluations for LLM responsibility are seriously lacking; Generative AI investment skyrockets; The data is in: AI makes workers more productive and leads to higher quality work; Scientific progress accelerates even further, thanks to AI; The number of AI regulations in the United States sharply increases; People across the globe are more cognizant of AI’s potential impact—and more nervous.

14. ***UC Berkeley presents GoEX, a runtime for autonomous LLM applications, aiming to facilitate efficient collaboration between humans and LLMs by enabling post-facto validation. GoEX integrates an undo feature and damage confinement strategies to mitigate risks associated with LLM-generated outputs, unlocking their potential for real-world applications with minimal human supervision.*** <br><br>
    10 Apr, UC Berkeley published a [paper](https://arxiv.org/pdf/2404.06921) “GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications”.  The paper indicates that Large Language Models (LLMs) are evolving beyond their classical role of providing information within dialogue systems to actively engaging with tools and performing actions on real-world applications and services. Today, humans verify the correctness and appropriateness of the LLM-generated outputs (e.g., code, functions, or actions) before putting them into real-world execution. This poses significant challenges as code comprehension is well known to be notoriously difficult. This paper studies how humans can efficiently collaborate with, delegate to, and supervise autonomous LLMs in the future. The authors argue that in many cases, "post-facto validation" - verifying the correctness of a proposed action after seeing the output - is much easier than the aforementioned "pre-facto validation" setting. The core concept behind enabling a post-facto validation system is the integration of an intuitive undo feature, and establishing a damage confinement for the LLM-generated actions as effective strategies to mitigate the associated risks. Using this, a human can now either revert the effect of an LLM-generated output or be confident that the potential risk is bounded. The researchers believe this is critical to unlock the potential for LLM agents to interact with applications and services with limited (post-facto) human involvement. The paper describes the design and implementation of the open-source runtime for executing LLM actions, Gorilla Execution Engine (GoEX), and present open research questions towards realizing the goal of LLMs and applications interacting with each other with minimal human supervision. GoEX is released at [this https URL](https://github.com/ShishirPatil/gorilla/).
 <br><br><br>
**14 Apr 2024**

1. ***XAI introduced Grok-1.5 Vision, their first-generation multimodal model, capable of processing various visual information alongside text. The upcoming release, available to early testers and existing users, showcases competitive performance across domains like multi-disciplinary reasoning and spatial understanding, emphasizing XAI's commitment to advancing multimodal capabilities.*** <br><br>
   12 Apr, [XAI released Grok-1.5 Vision preview](https://x.ai/blog/grok-1.5v), XAI’s first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to the early testers and existing Grok users. Grok-1.5V is competitive with existing frontier multimodal models in a number of domains, ranging from multi-disciplinary reasoning to understanding documents, science diagrams, charts, screenshots, and photographs. Grok also exhibits capabilities in understanding physical world. Grok outperforms its peers in the new RealWorldQA benchmark that measures real-world spatial understanding. XAI believes that advancing both multimodal understanding and generation capabilities are important steps in building beneficial AGI that can understand the universe. In the coming months, XAI anticipates to make significant improvements in both capabilities, across various modalities such as images, audio, and video.

2. ***Forbes.com unveiled its AI 50 2024 list, highlighting the rapid growth of AI-powered applications and the emergence of a new tech economy supporting businesses in adopting AI. Major players like OpenAI, Anthropic, and Databricks dominate the landscape, with significant funding and diverse applications spanning industries from finance to healthcare.*** <br><br>
   11 Apr, Forbes.com released [AI 50 2024](https://www.forbes.com/lists/ai50/?sh=2a11e72c290f). By the spring of 2023, the massive popularity of apps like ChatGPT had prompted a mass scramble among businesses trying to implement the latest advances in generative artificial intelligence. One year later, the craze continues. In turn, a new tech economy has emerged to help businesses develop and deploy AI-powered apps. The use cases are wide-ranging and far-reaching, as immediately evident from the three largest companies on the list in terms of valuation. Model maker OpenAI ($86 billion) counts customers from Morgan Stanley to the government of Iceland, while its rival Anthropic ($18.4 billion, as Forbes reported) is used by Bridgewater and the Boston Consulting Group. Databricks ($43 billion) sells its data analytics and AI deployment software to Shell and the United States Postal Service. For the startups on AI 50, the technology has evolved from capturing customers’ imaginations to capturing billions of dollars in collective revenue.  The companies on this year’s AI 50 have raised a total of $34.7 billion in funding. Nearly one-third of that total comes from OpenAI, thanks to some $10 billion from Microsoft. Much more comes from other ascendant AI research firms like Anthropic ($7.7 billion raised), Cohere ($445 million) and Mistral AI ($528 million). Underlying them are a slew of infrastructure tools that are helping companies to implement the technology. Other forms of AI development are seeing traction too. Take Anduril, which has raised $2.8 billion for defense tech; Insitro, which stockpiled a $643 million cash pile for drug discovery; or Figure AI, which raised $754 million to create humanoid robots. Then, there are companies that are seamlessly layering the latest advances in AI into their own apps. Abridge uses voice recognition and language summarization to deliver automated documentation of your visit to the doctor’s office. Notion is making inroads into uprooting Google Workspace or Microsoft Office, while Perplexity wants to reinvent the search engine.

3. ***Yann LeCun, Meta's Chief AI Scientist, challenges the prevailing enthusiasm for generative AI, advocating for a shift towards Objective-Driven AI at Meta AI Day in London. LeCun emphasizes the limitations of generative AI in understanding context and engaging with the physical world, proposing AI systems capable of causal reasoning and intuitive understanding.*** <br><br>
   11 Apr, Forbes [published a blog](https://www.forbes.com/sites/bernardmarr/2024/04/12/generative-ai-sucks-metas-chief-ai-scientist-calls-for-a-shift-to-objective-driven-ai/?sh=31d7bf6bb82b) “Generative AI Sucks: Meta’s Chief AI Scientist Calls For A Shift To Objective-Driven AI”. Yann LeCun, Meta's Chief AI Scientist, challenges the prevailing enthusiasm for generative AI at Meta AI Day in London. He argues that despite its ability to mimic human creativity, generative AI falls short in genuinely understanding context or engaging with the physical world. LeCun contrasts it with the adaptive learning mechanisms of humans and animals, emphasizing their intuitive understanding of the world and the nuanced grasp of physical laws. He highlights generative AI's limitations in providing factual accuracy and common-sense understanding, proposing a shift towards Objective-Driven AI. LeCun envisions AI systems with rich internal representations capable of causal reasoning and understanding relationships between actions and outcomes. Despite acknowledging the scientific and technical challenges ahead, LeCun remains optimistic about AI's potential to surpass human intelligence across all domains. He calls for a philosophical and technical revaluation within the AI research community, urging a focus on building AI systems capable of truly understanding and interacting with the world.

4. ***UC Berkeley introduces LLoCO, a novel approach to address the challenge of processing long contexts for large language models (LLMs). LLoCO enables efficient retrieval of relevant information and significantly improves long document question answering while reducing computational overhead, presenting a promising solution for efficient long context processing.*** <br><br>
   11 Apr, UC Berkeley published a [paper](https://arxiv.org/pdf/2404.07979) “LLoCO: Learning Long Contexts Offline”. The paper indicates that processing long contexts remains a challenge for large language models (LLMs) due to the quadratic computational and memory overhead of the self-attention mechanism and the substantial KV cache sizes during generation. The study proposes a novel approach to address this problem by learning contexts offline through context compression and in-domain parameter-efficient finetuning. The proposed method enables an LLM to create a concise representation of the original context and efficiently retrieve relevant information to answer questions accurately. The authors introduce LLoCO, a technique that combines context compression, retrieval, and parameter-efficient finetuning using LoRA. The approach extends the effective context window of a 4k token LLaMA2-7B model to handle up to 128k tokens. The research evaluates the proposed approach on several long-context question-answering datasets, demonstrating that LLoCO significantly outperforms in-context learning while using 30times fewer tokens during inference. LLoCO achieves up to 7.62times speed-up and substantially reduces the cost of long document question answering, making it a promising solution for efficient long context processing. The code is publicly available at https://github.com/jeffreysijuntan/lloco.

5. ***University of Arizona and TUCN examine the regression capabilities of pre-trained large language models (LLMs) without additional training. Findings reveal that LLMs can perform regression tasks comparably to traditional supervised methods, highlighting their potential for diverse applications beyond natural language processing.*** <br><br>
    11 Apr, Uni of Arizona and TUCN published a [paper](https://arxiv.org/pdf/2404.07544) “From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples”. The authors analyze how well pre-trained large language models (e.g., Llama2, GPT-4, Claude 3, etc) can do linear and non-linear regression when given in-context examples, without any additional training or gradient updates. The researchers’ findings reveal that several large language models (e.g., GPT-4, Claude 3) are able to perform regression tasks with a performance rivaling (or even outperforming) that of traditional supervised methods such as Random Forest, Bagging, or Gradient Boosting. For example, on the challenging Friedman #2 regression dataset, Claude 3 outperforms many supervised methods such as AdaBoost, SVM, Random Forest, KNN, or Gradient Boosting. The authors then investigate how well the performance of large language models scales with the number of in-context exemplars. The researchers borrow from the notion of regret from online learning and empirically show that LLMs are capable of obtaining a sub-linear regret. Code is [available here](https://github.com/robertvacareanu/llm4regression).

6. ***Google explores best practices and lessons learned on synthetic data for language models, emphasizing the importance of quality, diversity, and responsible use. The study addresses challenges and limitations of synthetic data while proposing future directions for scalable and efficient oversight in synthetic data generation*** <br><br>
    11 Apr, Google published a [paper](https://arxiv.org/pdf/2404.07503) “Best Practices and Lessons Learned on Synthetic Data for Language Models”. The authors state that he success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs. Synthetic data has emerged as a promising solution by generating artificial data that mimics real-world patterns. This paper provides an overview of synthetic data research, discussing its applications, challenges, and future directions. The authors present empirical evidence from prior art to demonstrate its effectiveness and highlight the importance of ensuring its factuality, fidelity, and unbiasedness. The study emphasizes the need for responsible use of synthetic data to build more powerful, inclusive, and trustworthy language models. Challenges and limitations of synthetic data include the misuse of synthetic data might proliferate misinformation; synthetic data might cause ambiguity in AI alignment; and training with synthetic data makes evaluation decontamination harder. Future directions are synthetic data scaling, further improving quality and diversity of synthetic data, towards high-fidelity and more efficient scalable oversight, and the emergent self-improvement capability.

7. ***Google explores best practices and lessons learned on synthetic data for language models, emphasizing the importance of quality, diversity, and responsible use. The study addresses challenges and limitations of synthetic data while proposing future directions for scalable and efficient oversight in synthetic data generation*** <br><br>
    11 Apr, Google published a [paper](https://arxiv.org/pdf/2404.07839) “RecurrentGemma: Moving Past Transformers for Efficient Open Language Models”. RecurrentGemma is an open language model which uses Google's novel Griffin architecture. Griffin combines linear recurrences with local attention to achieve excellent performance on language. It has a fixed-sized state, which reduces memory use and enables efficient inference on long sequences. Google provides a pre-trained model with 2B non-embedding parameters, and an instruction tuned variant. Both models achieve comparable performance to Gemma-2B despite being trained on fewer tokens. The project repository is available here. This repository contains the model implementation and examples for sampling and fine-tuning. There are two implementations, Google’s Flax and Meta’s Pytorch, the former is highly optimized.

8. ***Anthropic announces advancements in the persuasiveness of its language models, highlighting the potential social, commercial, and political implications. While the research underscores the persuasive capabilities of AI, concerns remain about real-world applications and potential effects on contentious debates.*** <br><br>
    10 Apr, [according to axios.com](https://www.axios.com/2024/04/10/anthropic-claude-persuasion-turing-test), AI startup Anthropic says its language models have steadily and rapidly improved in their "persuasiveness," per new research the company posted Tuesday. Persuasion — a general skill with widespread social, commercial and political applications — can foster disinformation and push people to act against their own interests, according to the paper's authors. However, there's relatively little research on how the latest models compare to humans when it comes to their persuasiveness. The researchers found "each successive model generation is rated to be more persuasive than the previous," and that the most capable Anthropic model, Claude 3 Opus, "produces arguments that don't statistically differ" from arguments written by humans. A wider debate has been raging about when AI will outsmart humans. AI has arguably "outsmarted" humans for some specific tasks in highly controlled environments. Elon Musk predicted Monday that AI will outsmart the smartest human by the end of 2025. Anthropic researchers developed "a basic method to measure persuasiveness" and used it to compare three different generations of models (Claude 1, 2, and 3), and two classes of models (smaller models and bigger "frontier models"). While the researchers were surprised that the AI was as persuasive as it turned out to be, they also chose to focus on "less polarized issues." Those issues ranged from potential rules for space exploration to appropriate uses of AI-generated content. While that allowed the researchers to dive deep into issues where many people are open to persuasion, it means we still don't have a clear idea — in an election year — of the potential effect of AI chatbots on today's most contentious debates. "Persuasion is difficult to study in a lab setting," the researchers warned in the report. "Our results may not transfer to the real world." Anthropic considers this the start of a long line of research into the emerging capabilities of its models.

9. ***Rutgers University explores how large language models (LLMs) acquire knowledge at different layers, revealing insights into the learning processes and internal representations of LLMs. The study sheds light on the classification efficiency of LLMs for tasks of varying complexity, providing implications for model development and understanding.*** <br><br>
    10 Apr, Rutgers Uni, inter alia published a [paper](https://arxiv.org/pdf/2404.07066) “Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers”. This paper studies the phenomenon that different concepts are learned in different layers of large language models, i.e. more difficult concepts are fully acquired with deeper layers. The study defines the difficulty of concepts by the level of abstraction, and here it is crudely categorized by factual, emotional, and inferential. Each category contains a spectrum of tasks, arranged from simple to complex. For example, within the factual dimension, tasks range from lie detection to categorizing mathematical problems. The authors employ a probing technique to extract representations from different layers of the model and apply these to classification tasks. The findings reveal that models tend to efficiently classify simpler tasks, indicating that these concepts are learned in shallower layers. Conversely, more complex tasks may only be discernible at deeper layers, if at all. This paper explores the implications of these findings for our understanding of model learning processes and internal representations. The implementation is available at here

10. ***French AI startup Mistral releases Mixtral 8x22B, a new large language model (LLM) aimed at competing with industry leaders. With improved context window and parameter size, Mixtral 8x22B offers enhanced performance, showcasing the ongoing competition and innovation in the AI arena.*** <br><br>
    10 Apr,  [According to ZDNET](https://www.zdnet.com/article/ai-startup-mistral-launches-a-281gb-ai-model-to-rival-openai-meta-and-google/), French AI startup Mistral on Tuesday released Mixtral 8x22B, a new large language model (LLM) and its latest attempt to compete with the big boys in the AI arena. Mixtral 8x22B is expected to outperform Mistral's previous Mixtral 8x7B LLM, which itself showed signs of outshining OpenAI's GPT-3.5 and Meta's Llama 2. The new Mixtral model boasts a 65,000-token context window, which refers to the amount of text that an AI model can process and reference at one time. Further, Mixtral 8x22B has a parameter size of up to 176 billion, a reference to the number of internal variables that the model uses to make decisions or predictions. Mixtral 8x22B is available for anyone to use after [downloading a 281GB file](https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1/tree/main). On the same data, OpenAI released GPT-4 Turbo with Vision, the latest GPT-4 Turbo model with vision capabilities for working with photographs, drawings, and other images uploaded by the user. Google released its advanced Gemini Pro 1.5 LLM to developers with a free option that grants up to 50 requests per day. Not to be outdone, Meta revealed that its Llama 3 model would debut later this month.
11. ***Google introduces a paper on "Leave No Context Behind," presenting an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. The proposed approach incorporates a new attention technique called Infini-attention, demonstrating effectiveness on long-context language modeling benchmarks and enabling fast streaming inference for LLMs.*** <br><br>
    10 Apr, Google published a [paper](https://arxiv.org/abs/2404.07143) "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention". This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in the proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. The authors demonstrate the effectiveness of the approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. The proposed approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.

12. ***EleutherAI presents Eagle and Finch, sequence models enhancing the RWKV architecture, introducing multi-headed matrix-valued states and dynamic recurrence mechanisms. These advancements improve expressivity while maintaining inference efficiency, resulting in competitive performance across various benchmarks. Additionally, a new multilingual corpus and fast tokenizer enhance multilinguality, with all models released under the Apache 2.0 license.*** <br><br>
    10 Apr, EleutherAI, inter alia published a [paper](https://arxiv.org/pdf/2404.05892) “Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence”. The authors present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. The architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. The paper introduces a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. The researchers trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. The authors release all the models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer

13. ***McGill University, ServiceNow, and Facebook unveil LLM2Vec, a simple unsupervised approach to transform any decoder-only LLM into a powerful text encoder. By enabling bidirectional attention, masked next token prediction, and unsupervised contrastive learning, LLM2Vec achieves state-of-the-art performance on word-level tasks and the Massive Text Embeddings Benchmark (MTEB), offering a parameter-efficient solution for universal text encoding.*** <br><br>
    9 Apr, McGill Uni, ServiceNow and Facebook published a [paper](https://arxiv.org/pdf/2404.05961) “LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders”. The study finds that Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. This work introduces LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. The authors demonstrate the effectiveness of LLM2Vec by applying it to 3 popular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks. The proposed models outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, the authors achieve state-of-the-art performance on MTEB among models that train only on publicly available data. The strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data. Code and pre-trained models are [available here](https://github.com/McGill-NLP/llm2vec).

14. ***Symbolica introduces a groundbreaking approach to AI modeling based on category theory, aiming to bring rigorous scientific principles to AI development. By prioritizing interpretability and reasoning capabilities, Symbolica seeks to revolutionize AI, potentially paving the way for artificial general intelligence. Despite challenges, Symbolica's focus on rigor could reshape the AI landscape, supported by collaborations with industry experts and significant funding.*** <br><br>
    9 Apr, according to [Venturebeat.com](https://venturebeat.com/ai/move-over-deep-learning-symbolicas-structured-approach-could-transform-ai/), Symbolica, an AI startup, has unveiled a groundbreaking approach to AI modeling, grounded in category theory, a branch of mathematics. This approach aims to bring rigorous scientific principles to AI, moving beyond the current "alchemy" of deep learning. By building models with inherent reasoning capabilities and transparency, Symbolica intends to revolutionize AI, making it interpretable and less reliant on vast amounts of data and computing power. Led by former Tesla engineer George Morgan, Symbolica has attracted significant funding and collaboration with industry experts like Stephen Wolfram. While facing challenges ahead, Symbolica's focus on rigor and interpretability could reshape the landscape of AI, potentially paving the way for artificial general intelligence. The research [paper](https://arxiv.org/abs/2402.15332) published by Symbolica and Google in the area of categorical area is "Categorical Deep Learning: An Algebraic Theory of Architectures".

15. ***IEEE Spectrum discusses the evolution of AI-powered coding tools, highlighting advancements like Devin AI and AutoDev in offering more autonomous assistance to developers. While these tools accelerate coding processes, concerns remain about safety, reliability, and the need for human oversight. Collaboration between humans and AI is essential for iterative improvement, as AI tools currently lack the intuitive understanding inherent in human developers.*** <br><br>
    9 Apr, IEEE Spectrum published an [article](https://spectrum.ieee.org/ai-code-generator) "AI Coding Is Going From Copilot to Autopilot - But so far Devin AI and others still really need humans". The emergence of a new wave of AI-powered coding tools, including Devin AI and AutoDev, promises to revolutionize software development by offering more autonomous assistance than previous iterations like GitHub Copilot and Amazon CodeWhisperer. These tools can build websites, find and fix bugs, and even train their own language models. Open-source alternatives have also emerged, reflecting the growing demand for generative AI tools among developers. While these coding assistants can accelerate coding processes and offer templates for code, concerns about safety, reliability, and the need for human oversight remain. Developers must ensure that AI-generated code meets rigorous quality standards, considering potential security vulnerabilities and corner cases that could lead to crashes.  Despite the potential of AI coding assistants to enhance productivity, they are still in early stages, with limited capabilities and reliability compared to human developers. Collaboration between humans and AI is crucial for iterative improvement, as AI tools lack the intuitive understanding and creativity inherent in human developers. While the future role of unassisted AI coders in software development remains uncertain, developers are encouraged to embrace these tools and evaluate their benefits firsthand. Continuous monitoring and improvement are essential as AI technology evolves rapidly. Ultimately, developers bear the responsibility of understanding and validating AI-generated code to ensure its security, reliability, and maintainability.

16. ***Google releases CodeGemma, a family of code-specialist LLM models trained on additional data to improve logical and mathematical reasoning. With models optimized for code completion, understanding, and generation, CodeGemma achieves competitive performance across various programming languages, offering enhanced capabilities for developers.*** <br><br>
    9 Apr, Google [released CodeGemma](https://huggingface.co/collections/google/codegemma-release-66152ac7b683e2667abdee11), a family of code-specialist LLM models by Google, based on the pre-trained 2B and 7B Gemma checkpoints. CodeGemma are further trained on an additional 500 billion tokens of primarily English language data, mathematics, and code to improve on logical and mathematical reasoning, and are suitable for code completion and generation. CodeGemma 2B was trained exclusively on Code Infilling and is meant for fast code completion and generation, especially in settings where latency and/or privacy are crucial. CodeGemma 7B training mix includes code infilling data (80%) and natural language. It can be used for code completion, as well as code and language understanding and generation. CodeGemma 7B Instruct was fine-tuned for instruction following on top of CodeGemma 7B. It’s meant for conversational use, especially around code, programming, or mathematical reasoning topics. All the models have the same 8K token context size as their predecessors. CodeGemma-7B outperforms similarly-sized 7B models except DeepSeek-Coder-7B on HumanEval, a popular benchmark for evaluating code models on Python. The same goes for the evaluation of other programming languages like Java, JavaScript, and C++ from MultiPL-E, a translation of HumanEval. According to the technical report, the model performs best on GSM8K among 7B models. The instruct version CodeGemma-7B-it improves on the most popular languages on both HumanEval and MBPP. A demo can be [found here](https://huggingface.co/blog/codegemma).

17. ***Meta publishes a paper on knowledge capacity scaling laws for language models, estimating the number of knowledge bits a model stores and examining factors influencing knowledge storage capacity. Insights include the impact of training duration, model architecture, quantization, and data signal-to-noise ratio, shedding light on language model capabilities and training paradigms.*** <br><br>
    8 Apr, Meta published a [paper](https://arxiv.org/pdf/2404.05405) “Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws”. The paper indicates that scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, this paper estimates the number of knowledge bits a model stores. The authors focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, the study establishes that language models can and only can store 2 bits of knowledge per parameter, even when quantized to int8, and such knowledge can be flexibly extracted for downstream applications. Consequently, a 7B model can store 14B bits of knowledge, surpassing the English Wikipedia and textbooks combined based on our estimation. More broadly, the study presents 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity. Notable insights include: * The GPT-2 architecture, with rotary embedding, matches or even surpasses LLaMA/Mistral architectures in knowledge storage, particularly over shorter training durations. This arises because LLaMA/Mistral uses GatedMLP, which is less stable and harder to train. * Prepending training data with domain names (e.g., this http URL) significantly increases a model's knowledge capacity. Language models can autonomously identify and prioritize domains rich in knowledge, optimizing their storage capacity.

18. ***University of Tubingen, Cambridge, and Oxford investigate the influence of pretraining concept frequency on multimodal model performance. The study reveals an exponential need for training data to achieve linear improvements in downstream "zero-shot" performance, challenging the notion of "zero-shot" generalization for multimodal models and highlighting the importance of large-scale training paradigms.***<br><br>
    8 Apr, Uni of Tubingen, Cambridge and Oxford published a [paper](https://arxiv.org/pdf/2404.04125) “No “Zero-Shot” Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance”. The paper finds that Web-crawled pretraining datasets underlie the impressive "zero-shot" evaluation performance of multimodal models, such as CLIP for classification/retrieval and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of "zero-shot" generalization is for such multimodal models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during "zero-shot" evaluation. This work asks: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets? The study comprehensively investigates this question across 34 models and five standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. The authors consistently find that, far from exhibiting "zero-shot" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream "zero-shot" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon benchmarking models on long-tailed data sampled based on analysis, the paper demonstrates that multimodal models across the board perform poorly. The authors contribute this long-tail test set as the "Let it Wag!" benchmark to further research in this direction. Taken together, the study reveals an exponential need for training data which implies that the key to "zero-shot" generalization capabilities under large-scale training paradigms remains to be found. Here is the links for the [code](https://github.com/bethgelab/frequency_determines_performance) and [data](https://huggingface.co/datasets/bethgelab/Let-It-Wag).

19. ***SambaNova Systems presents a comprehensive investigation into adapting LLMs to new languages, covering vocabulary extension, direct preference optimization, and data scarcity challenges. Experimenting across multiple languages and parameter scales, the study achieves competitive performance, outperforming existing baselines and offering insights for future research.*** <br><br>
    8 Apr, SambaNova Systems published a [paper](https://arxiv.org/pdf/2404.05829) “SambaLingo: Teaching Large Language Models New Languages”. The authors argue that despite the widespread availability of LLMs, there remains a substantial gap in their capabilities and availability across diverse languages. One approach to address these issues has been to take an existing pre-trained LLM and continue to train it on new languages. While prior works have experimented with language adaptation, many questions around best practices and methodology have not been covered. This study presents a comprehensive investigation into the adaptation of LLMs to new languages. The study covers the key components in this process, including vocabulary extension, direct preference optimization and the data scarcity problem for human alignment in low-resource languages. The authors scale these experiments across 9 languages and 2 parameter scales (7B and 70B). The researchers compare the models against Llama 2, Aya-101, XGLM, BLOOM and existing language experts, outperforming all prior published baselines. Additionally, all evaluation code and [checkpoints](https://huggingface.co/collections/sambanovasystems/) are made public to facilitate future research.

20. ***Microsoft introduces Direct Nash Optimization (DNO), a scalable algorithm for post-training LLMs using general preferences to iteratively improve model performance. With straightforward implementation and efficient batched on-policy learning, DNO achieves state-of-the-art results, outperforming models with far more parameters and demonstrating monotonic improvement across iterations.*** <br><br>
    4 Apr, Microsoft published a [paper](https://arxiv.org/pdf/2404.03715) “Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences”. This paper studies post-training large language models (LLMs) using preference feedback from a powerful oracle to help a model iteratively improve over itself. The typical approach for post-training LLMs involves Reinforcement Learning from Human Feedback (RLHF), which traditionally separates reward learning and subsequent policy optimization. However, such a reward maximization approach is limited by the nature of "point-wise" rewards (such as Bradley-Terry model), which fails to express complex intransitive or cyclic preference relations. While advances on RLHF show reward learning and policy optimization can be merged into a single contrastive objective for stability, they yet still remain tethered to the reward maximization framework. Recently, a new wave of research sidesteps the reward maximization presumptions in favor of directly optimizing over "pair-wise" or general preferences. This study introduces Direct Nash Optimization (DNO), a provable and scalable algorithm that marries the simplicity and stability of contrastive learning with theoretical generality from optimizing general preferences. Because DNO is a batched on-policy algorithm using a regression-based objective, its implementation is straightforward and efficient. Moreover, DNO enjoys monotonic improvement across iterations that help it improve even over a strong teacher (such as GPT-4). In authors’ experiments, a resulting 7B parameter Orca-2.5 model aligned by DNO achieves the state-of-the-art win-rate against GPT-4-Turbo of 33% on AlpacaEval 2.0 (even after controlling for response length), an absolute gain of 26% (7% to 33%) over the initializing model. It outperforms models with far more parameters, including Mistral Large, Self-Rewarding LM (70B parameters), and older versions of GPT-4.
<br><br><br>
    


**7 Apr 2024**

1. ***Reuters Article - "Inside Big Tech's underground race to buy AI training data"<br>
   This article explores the evolving landscape of AI training data acquisition, with a focus on tech giants like Google, Meta, and Microsoft-backed OpenAI. Initially reliant on freely scraped internet data, these companies have shifted towards securing data through agreements with content owners and utilizing data brokers to address legal and ethical concerns. Key points include the emergence of AI data firms ensuring ethical sourcing, concerns regarding user privacy, regulatory scrutiny, and potential acquisitions of news media companies by tech giants to control data IP.***  <br><br>
   6 Apr, [Reuters published an article](https://www.reuters.com/technology/inside-big-techs-underground-race-buy-ai-training-data-2024-04-05/) “Inside Big Tech's underground race to buy AI training data”. The article discusses the evolving landscape of AI training data acquisition, particularly focusing on tech giants like Google, Meta, and Microsoft-backed OpenAI. Initially, these companies relied on freely scraped internet data to train generative AI models like ChatGPT, leading to legal and ethical concerns and lawsuits from copyright holders. Simultaneously, they are discreetly paying for access to content behind paywalls and login screens, sparking a hidden market for various data types. Some key points include: 1) Data Acquisition Methods: Tech companies initially used scraped web page data for AI training, leading to copyright issues and regulatory scrutiny. In response, they're now securing data through agreements with content owners and utilizing data brokers. 2) Content Acquisition Deals: Companies like Google, Meta, Amazon, and Apple are striking agreements with content providers like Shutterstock and Freepik to access vast libraries of images, videos, and text for training AI models. The size of these deals can range from tens of millions to undisclosed amounts. 3) Emergence of AI Data Firms: Dedicated AI data firms are securing rights to various content types, ensuring ethical sourcing and anonymizing personal data. Rates vary depending on content type, with emphasis on obtaining consent and stripping personally identifying information. 4) Ethical and Legal Concerns: Despite licensing agreements, concerns persist regarding user privacy, especially regarding old internet archives like Photobucket. Instances of AI regurgitating exact copies of training data raise questions about consent and notice. 5) Regulatory Scrutiny: Reddit's data-licensing business is under investigation by the U.S. Federal Trade Commission, highlighting the potential legal pitfalls of AI data acquisition, including privacy and intellectual property issues. Comment: We may expect the tech giants may acquire more shares of the news media companies to control the IP of the data.

2. ***VentureBeat Report - "OpenAI releases new AI fine-tuning tools" <br>
   OpenAI has introduced new AI fine-tuning tools aimed at providing developers with greater control over model customization and construction. These tools include epoch-based checkpoint creation, comparative Playground UI for side-by-side evaluations, third-party integration, and comprehensive validation metrics. These updates enable organizations to maximize model performance through advanced techniques and bespoke parameters, leading to personalized AI models becoming standard in the future.*** <br><br>
   4 Apr, [according to VentureBeat](https://venturebeat.com/ai/openai-releases-new-ai-fine-tuning-tools-vast-majority-of-organizations-will-develop-customized-models/), OpenAI releases new AI fine-tuning tools: ‘vast majority of organizations will develop customized models’. These updates are set to empower developers with unprecedented control over AI model fine-tuning, while also offering new avenues for constructing custom models tailored to specific business needs. The latest API improvements include epoch-based checkpoint creation, which minimizes the need for retraining and mitigates overfitting risks. Additionally, a new comparative Playground UI facilitates side-by-side evaluations of model outputs, enhancing the development process with human insights. These updates, along with third-party integration starting with Weights and Biases, and comprehensive validation metrics, mark a significant leap forward in fine-tuning technology. Assisted fine-tuning represents a collaborative effort between OpenAI’s technical teams and organizations, aiming to maximize model performance through advanced techniques and bespoke parameters. SK Telecom experienced notable improvements in customer service performance through OpenAI's approach, despite earlier reports of their investment in rival Anthropic for custom models. Meanwhile, fully custom-trained models, exemplified by Harvey, an AI tool for attorneys, are tailored for organizations with intricate needs, such as enhancing the accuracy of legal case law analysis. OpenAI envisions a future where personalized AI models become standard, providing businesses with more effective and efficient outcomes. These developments mark a crucial stride towards personalized and powerful AI solutions, offering developers and organizations the tools to innovate and grow in the evolving AI landscape.

3. ***Research Paper - "Long-context LLMs Struggle with Long In-context Learning"<br>
   Large Language Models (LLMs) face challenges in handling long sequences for in-context learning, as evaluated by the LIConBench benchmark focusing on extreme-label classification. While long-context LLMs perform well under token lengths up to 20K, their performance drops significantly beyond this threshold, indicating limitations in processing and understanding long, context-rich sequences. Further analysis reveals a need for improvement in long context understanding and reasoning for existing LLMs.*** <br><br>
   4 Apr, Uni of Waterloo and CMU published a [paper](https://arxiv.org/pdf/2404.02060.pdf) “Long-context LLMs Struggle with Long In-context Learning”. The authors state that Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. The authors meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. The benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. The study evaluates 13 long-context LLMs on the benchmarks, and finds that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However, after the context window exceeds 20K, most LLMs except GPT-4 will dip dramatically. This suggests a notable gap in current LLM capabilities for processing and understanding long, context-rich sequences. Further analysis revealed a tendency among models to favor predictions for labels presented towards the end at the sequence. Their ability to reason over multiple pieces in the long sequence is yet to be improved. The study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. The authors believe LIConBench could serve as a more realistic evaluation for the future long context LLMs.

4. ***Research Paper - "MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens"<br>
   MiniGPT4-Video is introduced as a multimodal Large Language Model (LLM) specifically designed for video understanding, capable of processing both temporal visual and textual data. Building upon previous successes in image-text benchmarks, MiniGPT4-Video extends its capabilities to comprehend videos, outperforming existing methods on various benchmarks such as MSVD, MSRVTT, TGIF, and TVQA.*** <br><br>
   4 Apr, KAUST and Harvard Uni published a [paper](https://arxiv.org/pdf/2404.03413) “MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens”. This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM) designed specifically for video understanding. The model is capable of processing both temporal visual and textual data, making it adept at understanding the complexities of videos. Building upon the success of MiniGPT-v2, which excelled in translating visual features into the LLM space for single images and achieved impressive results on various image-text benchmarks, this paper extends the model's capabilities to process a sequence of frames, enabling it to comprehend videos. MiniGPT4-video does not only consider visual content but also incorporates textual conversations, allowing the model to effectively answer queries involving both visual and text components. The proposed model outperforms existing state-of-the-art methods, registering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA benchmarks respectively. The models and code have been made publicly available at [this https URL](https://vision-cair.github.io/MiniGPT4-video/)

5. ***Research Paper - "ReFT: Representation Finetuning for Language Models"<br>
   Representation Finetuning (ReFT) methods are introduced as a powerful alternative to parameter-efficient fine-tuning (PEFT) methods for adapting large language models. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations, achieving superior efficiency and performance across various tasks compared to existing PEFT methods.*** <br><br>
    4 Apr, Stanford Uni published a [paper](https://arxiv.org/pdf/2404.03592) “ReFT: Representation Finetuning for Language Models”. The researchers indicate that Parameter-efficient fine-tuning (PEFT) methods seek to adapt large models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. The work pursues this hypothesis by developing a family of Representation Finetuning (ReFT) methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. The authors define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT). LoReFT is a drop-in replacement for existing PEFTs and learns interventions that are 10x-50x more parameter-efficient than prior state-of-the-art PEFTs. The paper showcases LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, Alpaca-Eval v1.0, and GLUE. In all these evaluations, LoReFT delivers the best balance of efficiency and performance, and almost always outperforms state-of-the-art PEFTs. The researchers release a generic ReFT training library publicly at [this https URL](https://github.com/stanfordnlp/pyreft).

6. ***Research Paper - "Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"   <br>
   The paper investigates jailbreak attacks on Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) and evaluates their robustness against such attacks. Through comprehensive experiments and analysis, it is found that GPT-4 and GPT-4V demonstrate better robustness compared to open-source LLMs and MLLMs, with limited transferability of visual jailbreak methods compared to textual methods.*** <br><br>
    4 Apr, LMU Munich, Uni of Oxford and Siemens AG published a [paper](https://arxiv.org/pdf/2404.03411) “Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?”. The authors find that various jailbreak attacks have been proposed to red-team Large Language Models (LLMs) and revealed the vulnerable safeguards of LLMs. Besides, some methods are not limited to the textual modality and extend the jailbreak attack to Multimodal Large Language Models (MLLMs) by perturbing the visual input. However, the absence of a universal evaluation benchmark complicates the performance reproduction and fair comparison. Besides, there is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as GPT-4V. To address these issues, this work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies. Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models. The researchers then conduct a deep analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source LLMs and MLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models. (3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods. The dataset and code can be found [here](https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md)

7. ***Research Paper - "Training LLMs over Neurally Compressed Text"<br>
   The study explores training large language models (LLMs) over highly compressed text using Equal-Info Windows, a novel compression technique. This approach enables effective learning over neurally compressed text, improving with scale and outperforming byte-level baselines on perplexity and inference speed benchmarks, despite delivering worse perplexity than subword tokenizers.*** <br><br>
    4 Apr, Google and Anthropic published a [paper](https://arxiv.org/pdf/2404.03626) “Training LLMs over Neurally Compressed Text”. In this paper, the authors explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, the researchers find that text na\"ively compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, the study proposes Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, the paper demonstrates effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks. While the method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency. Finally, the authors provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.

8. ***NPR Article - "Using AI to detect AI-generated deepfakes can work for audio — but not always"<br>
   The article discusses the challenges of detecting AI-generated deepfake audio despite efforts to develop detection software. While tech giants are interested in developing solutions, NPR's experiment reveals limitations in accuracy, especially given variations in formats and languages. Concerns persist regarding the potential misuse of deepfake audio, highlighting the need for continuous improvement in detection technology.*** <br><br>
    4 Apr, [NPR published an article](https://www.npr.org/2024/04/05/1241446778/deepfake-audio-detection) “Using AI to detect AI-generated deepfakes can work for audio — but not always”. The article argues that the use of AI in creating deepfake audio presents significant challenges, particularly in detecting these falsified recordings. While efforts are underway to develop detection software, NPR's experiment reveals limitations in their accuracy. Providers like AI Voice Detector initially claimed high accuracy rates but adjusted their thresholds after NPR's investigation, resulting in less reliable outcomes. Moreover, detecting deepfake audio is complicated by variations in formats and languages, highlighting the need for continuous improvement in detection technology. Despite interest from tech giants in developing detection solutions, particularly for video content, audio detection remains a challenge. Concerns persist regarding the potential misuse of deepfake audio, especially in political contexts and scam calls, emphasizing the importance of continued vigilance and caution in addressing this issue.

9. ***Research Paper - "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers"<br>
   The paper evaluates large language models (LLMs) for code assistance using RealHumanEval, a web interface measuring LLMs' ability to assist programmers through autocomplete or chat support. While improvements in benchmark performance correlate with increased programmer productivity, gaps in benchmark versus human performance are noted. Programmer preferences do not necessarily correlate with actual performance, highlighting the need for better human-centric evaluation metrics.*** <br><br>
    3 Apr, MIT, IBM, CMU, and UC Berkeley published a [paper](https://arxiv.org/pdf/2404.02806) “The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers”. The paper indicates that evaluation of large language models (LLMs) for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), which measure the ability of LLMs to generate complete code that passes unit tests. As LLMs are increasingly used as programmer assistants, The authors study whether gains on existing benchmarks translate to gains in programmer productivity when coding with LLMs, including time spent coding. In addition to static benchmarks, the study investigates the utility of preference metrics that might be used as proxies to measure LLM helpfulness, such as code acceptance or copy rates. To do so, the authors introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. The research conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, the authors find that improvements in benchmark performance lead to increased programmer productivity; however gaps in benchmark versus human performance are not proportional -- a trend that holds across both forms of LLM support. In contrast, the study finds that programmer preferences do not correlate with their actual performance, motivating the need for better, human-centric proxy signals. The researchers also [open-source RealHumanEval](https://github.com/clinicalml/realhumaneval) to enable human-centric evaluation of new models and the study data to facilitate efforts to improve code models.  

10. ***Research Paper - "Many-shot Jailbreaking"<br>
   The paper investigates long-context attacks on large language models (LLMs) prompting with hundreds of demonstrations of undesirable behavior. This attack is found to be effective across various tasks and state-of-the-art closed-weight models, demonstrating the vulnerability of LLMs to such attacks. The study reveals insights into LLMs' ability to learn from context, even for inappropriate queries, emphasizing the need for improved understanding and safeguards.*** <br><br>
    2 Apr, Anthropic published a [paper](https://www-cdn.anthropic.com/af5633c94ed2beb282f6a53c595eb437e8e7b630/Many_Shot_Jailbreaking__2024_04_02_0936.pdf) “Many-shot Jailbreaking”. The paper investigates a family of simple long-context attacks on large language models: prompting with hundreds of demonstrations of undesirable behavior. This is newly feasible with the larger context windows recently deployed by Anthropic, OpenAI and Google DeepMind. The authors find that in diverse, realistic circumstances, the effectiveness of this attack follows a power law, up to hundreds of shots. The study demonstrates the success of this attack on the most widely used state-of-the-art closedweight models, and across various tasks. Experimental results suggest very long contexts present a rich new attack surface for LLMs. [Techcrunch reports](https://techcrunch.com/2024/04/02/anthropic-researchers-wear-down-ai-ethics-with-repeated-questions/) that what Anthropic’s researchers found was that these models with large context windows tend to perform better on many tasks if there are lots of examples of that task within the prompt. So if there are lots of trivia questions in the prompt (or priming document, like a big list of trivia that the model has in context), the answers actually get better over time. So a fact that it might have gotten wrong if it was the first question, it may get right if it’s the hundredth question. But in an unexpected extension of this “in-context learning,” as it’s called, the models also get “better” at replying to inappropriate questions. So if you ask it to build a bomb right away, it will refuse. But if you ask it to answer 99 other questions of lesser harmfulness and then ask it to build a bomb … it’s a lot more likely to comply. No one really understands what goes on in the tangled mess of weights that is an LLM, but clearly there is some mechanism that allows it to home in on what the user wants, as evidenced by the content in the context window.

11. ***Research Paper - Crescendo Multi-Turn LLM Jailbreak Attack<br>
   Microsoft introduces the Crescendo attack to overcome the ethical alignment of Large Language Models (LLMs). Crescendo is a multi-turn attack that gradually escalates dialogue to successfully exploit LLMs. Evaluation on various systems demonstrates Crescendo's high efficacy and introduces the Crescendomation tool. Crescendo differs from Many-shot Jailbreak by not assuming user malicious knowledge and being more practical. A simple input filter can defend against Many-shot Jailbreak but is ineffective against Crescendo.*** <br><br>
    2 Apr, Microsoft published a [paper](https://arxiv.org/abs/2404.01833v1) “Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack”. The authors find that Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as "jailbreaks", seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. This work introduces a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies, progressively leading to a successful jailbreak. The authors evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b Chat, and Anthropic Chat. The results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, the paper introduces Crescendomation, a tool that automates the Crescendo attack, and the evaluation showcases its effectiveness against state-of-the-art models. Note, there are significant distinctions between the Many-shot Jailbreak and Crescendo. Firstly, Crescendo does not assume that the user has the malicious knowledge needed to be inserted into the model's context. Secondly, Crescendo is more practical as it can be utilized with models that have smaller contexts, which also makes it more cost-effective since LLMs typically charge based on the number of tokens. Most importantly, a simple input filter designed to detect malicious content could successfully defend against the Many-shot Jailbreak. In contrast, such a filter would be almost ineffective against Crescendo.

12. ***Research Paper - FLawN-T5: Effective Instruction-Tuning Data Mixtures for Legal Reasoning<br>
   The paper emphasizes instruction tuning for language models in legal tasks. LawInstruct, a large legal instruction dataset, is curated to enhance model performance. Domain-specific pretraining and instruction tuning improve LegalBench performance variably. LawInstruct accelerates the development of legal models and is openly released with code.*** <br><br>
    2 Apr, Stanford, Johns Hopkins and Princeton Uni published a [paper](https://arxiv.org/pdf/2404.02127) “FLawN-T5: An Empirical Examination of Effective Instruction- Tuning Data Mixtures for Legal Reasoning”. The researchers find that instruction tuning is an important step in making language models useful for direct user interaction. However, many legal tasks remain out of reach for most open LLMs and there do not yet exist any large scale instruction datasets for the domain. This critically limits research in this application area. This study curates LawInstruct, a large legal instruction dataset, covering 17 jurisdictions, 24 languages and a total of 12M examples. The authors present evidence that domain-specific pretraining and instruction tuning improve performance on LegalBench, including improving Flan-T5 XL by 8 points or 16\% over the baseline. However, the effect does not generalize across all tasks, training regimes, model sizes, and other factors. LawInstruct is a resource for accelerating the development of models with stronger information processing and decision making capabilities in the legal domain. The authors also released the permissively-licensed [curated dataset](https://huggingface.co/lawinstruct/lawinstruct) and the [code](https://github.com/JoelNiklaus/LawInstruct/) used to create the dataset.

13. ***Research Paper - Privacy Backdoors: Stealing Data with Corrupted Pretrained Models<br>
   ETH Zurich highlights the risk of privacy backdoors in pretrained machine learning models. Tampering with weights allows attackers to compromise finetuning data privacy. Backdoored models enable tight privacy attacks even on differentially private models. The paper reveals a critical supply chain attack on machine learning privacy.*** <br><br>
    1 Apr, ETH Zurich published a [paper](https://arxiv.org/pdf/2404.00473) “Privacy Backdoors: Stealing Data with Corrupted Pretrained Models”. The paper indicates that practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. This study shows that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. The authors show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! The study further shows that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, the work highlights a crucial and overlooked supply chain attack on machine learning privacy. [Code at this URL](https://github.com/ShanglunFengatETHZ/PrivacyBackdoor).

14. ***Research Paper - Source-Aware Training Enables Knowledge Attribution in Language Models<br>
   The study addresses intrinsic source citation in language models to enhance transparency. Source-aware training associates unique source document identifiers with model knowledge.
It can be applied to pretrained models with minimal impact on quality. Data augmentation is crucial for achieving accurate attribution.*** <br><br>
    1 Apr, Uni of Michigan, Allen Inst. Of AI, CMU and UIUC published a [paper](https://arxiv.org/pdf/2404.01019) “Source-Aware Training Enables Knowledge Attribution in Language Models”. The authors find that Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. The study investigates the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability, the researchers explore source-aware training -- a post pretraining recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training can easily be applied to pretrained LLMs off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated data, the authors demonstrate that the training recipe can enable faithful attribution to the pretraining data without a substantial impact on the model's quality compared to standard pretraining. Experimental results also highlight the importance of data augmentation in achieving attribution.

15. ***Research Paper - Streaming Dense Video Captioning<br>
   Google proposes a streaming dense video captioning model for rich temporal descriptions. It introduces a memory module and streaming decoding algorithm for efficient processing.
The model achieves state-of-the-art performance on dense video captioning benchmarks. Code for the model is released.*** <br><br>
    1 Apr, Google published a [paper](https://arxiv.org/pdf/2404.01297) “Streaming Dense Video Captioning”. The authors suggest that an ideal model for dense video captioning -- predicting captions localized temporally in a video -- should be able to handle long input videos, predict rich, detailed textual descriptions, and be able to produce outputs before processing the entire video. Current state-of-the-art models, however, process a fixed number of downsampled frames, and make a single full prediction after seeing the whole video. The study proposes a streaming dense video captioning model that consists of two novel components: 1) propose a new memory module, based on clustering incoming tokens, which can handle arbitrarily long videos as the memory is of a fixed size. 2) develop a streaming decoding algorithm that enables the model to make predictions before the entire video has been processed. The proposed model achieves this streaming ability, and significantly improves the state-of-the-art on three dense video captioning benchmarks: ActivityNet, YouCook2 and ViTT. The code is released at [this https URL](https://github.com/google-research/scenic).

16. ***EU AI legislation - Living guidelines on the responsible use of generative AI in research<br>
   The European Commission releases guidelines for responsible use of generative AI. Principles include reliability, honesty, respect, and accountability throughout the research process.
It emphasizes awareness of bias, privacy, and societal impacts, with proper citation and management.*** <br><br>
    18 Mar, European Commission [formally release](https://research-and-innovation.ec.europa.eu/document/download/2b6cf7e5-36ac-41cb-aab5-0d32050143dc_en?filename=ec_rtd_ai-guidelines.pdf) “Living guidelines on the responsible use of generative AI in research”. The key principles behind these guidelines for the responsible use of generative AI in research are: • Reliability in ensuring the quality of research, reflected in the design, methodology, analysis and use of resources. This includes aspects related to verifying and reproducing the information produced by the AI for research. It also involves being aware of possible equality and non-discrimination issues in relation to bias and inaccuracies. • Honesty in developing, carrying out, reviewing, reporting and communicating on research transparently, fairly, thoroughly and impartially. This principle includes disclosing that generative AI has been used. • Respect for colleagues, research participants, research subjects, society, ecosystems, cultural heritage and the environment. Responsible use of generative AI should take into account the limitations of the technology, its environmental impact and its societal effects (bias, diversity, non-discrimination, fairness and prevention of harm). This includes the proper management of information, respect for privacy, confidentiality and intellectual property rights, and proper citation. • Accountability for the research from idea to publication, for its management and organisation, for training, supervision and mentoring, and for its wider societal impacts. This includes responsibility for all output a researcher produces, underpinned by the notion of human agency and oversight.

17. ***Research Paper - AI Generative Language Models' Creative Superiority<br>
   Research from Arkansas Uni suggests AI language models surpass humans in divergent thinking tasks. The study compares human and GPT-4 responses on various tasks. AI demonstrates higher creativity, particularly in originality and elaboration, challenging perceptions of human creativity.*** <br><br>
    10 Feb, researchers from Arkansas Uni published a [paper](https://www.nature.com/articles/s41598-024-53303-w) on Scientific Reports “The current state of artificial intelligence generative language models is more creative than humans on divergent thinking tasks”. The study indicates that the emergence of publicly accessible artificial intelligence (AI) large language models such as ChatGPT has given rise to global conversations on the implications of AI capabilities. Emergent research on AI has challenged the assumption that creative potential is a uniquely human trait thus, there seems to be a disconnect between human perception versus what AI is objectively capable of creating. This research aimed to assess the creative potential of humans in comparison to AI. In the present study, human participants (N = 151) and GPT-4 provided responses for the Alternative Uses Task, Consequences Task, and Divergent Associations Task. The study found that AI was robustly more creative along each divergent thinking measurement in comparison to the human counterparts. Specifically, when controlling for fluency of responses, AI was more original and elaborate. The present findings suggest that the current state of AI language models demonstrate higher creative potential than human respondents.

<br><br>
**31 Mar 2024**

1. ***LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning <br>
   Researchers from HKUST and UIUC introduce LISA, a novel fine-tuning technique for large language models (LLMs) addressing memory inefficiency. LISA leverages layerwise properties of Low-Rank Adaptation (LoRA) and importance sampling to reduce memory costs while maintaining performance. Experimental results demonstrate LISA's superiority over LoRA and full parameter tuning in various fine-tuning tasks, even outperforming LoRA by over 11%-37%.*** <br><br>
   29 Mar, HKUST and UIUC published a [paper](https://arxiv.org/pdf/2403.17919) “LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning”. The paper states that the machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, the study investigates layerwise properties of LoRA on fine-tuning tasks and observes an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. The authors name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freeze most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over 11%-37% in terms of MT-Bench scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains.

2. ***Future Applications of Generative Large Language Models: A Data-Driven Case Study on ChatGPT <br>
   Technovation publishes a paper exploring the potential applications of generative LLMs through a data-driven approach. The study identifies and clusters diverse tasks for LLMs using NLP techniques and highlights emerging areas of application, such as human resources and education. Insights from the study inform innovation management strategies and propose further research directions for leveraging LLMs across different domains.*** <br><br>
   29 Mar, Technovation published a [paper](https://www.sciencedirect.com/science/article/pii/S016649722400052X/pdfft?md5=91584ae7f5b45865af0050ad7aa3739f&pid=1-s2.0-S016649722400052X-main.pdf) “Future applications of generative large language models: A data-driven case study on ChatGPT”. This study delves into the evolving role of generative Large Language Models (LLMs). The paper develops a data-driven approach to collect and analyse tasks that users are asking to generative LLMs. Thanks to the focus on tasks this paper contributes to give a quantitative and granular understanding of the potential influence of LLMs in different business areas. Utilizing a dataset comprising over 3.8 million tweets, the authors identify and cluster 31,747 unique tasks, with a specific case study on ChatGPT. To reach this goal, the proposed method combines two Natural Language Processing (NLP) Techniques, Named Entity Recognition (NER) and BERTopic. The combination makes it possible to collect granular tasks of LLMs (NER) and clusters them in business areas (BERTopic). The findings reveal a wide spectrum of applications, from programming assistance to creative content generation, highlighting LLM's versatility. The analysis highlighted six emerging areas of application for ChatGPT: human resources, programming, social media, office automation, search engines, education. The study also examines the implications of these findings for innovation management, proposing a research agenda to explore the intersection of the identified areas, with four stages of the innovation process: idea generation, screening/idea selection, development, and diffusion/sales/marketing.

3. ***Jamba: Production-Grade Mamba-Based Model for High-Quality Text Generation <br>
   AI21labs releases Jamba, a production-grade generative text model based on the Mamba Structured State Space model. Jamba enhances traditional Transformer architecture, offering improved efficiency and throughput with a 256K context window. The model outperforms other state-of-the-art models in its size class across various benchmarks, demonstrating its effectiveness and scalability.*** <br><br>
   28 Mar, [AI21labs released Jamba](https://www.ai21.com/blog/announcing-jamba), Debuting the first production-grade Mamba-based model delivering best-in-class quality and performance. By enhancing Mamba Structured State Space model (SSM) technology with elements of the traditional Transformer architecture, Jamba compensates for the inherent limitations of a pure SSM model. Offering a 256K context window, it is already demonstrating remarkable gains in throughput and efficiency—just the beginning of what can be possible with this innovative hybrid architecture. Notably, Jamba outperforms or matches other state-of-the-art models in its size class on a wide range of benchmarks. Key features of Jamba include: 1) First production-grade Mamba based model built on a novel SSM-Transformer hybrid architecture; 2) 3X throughput on long contexts compared to Mixtral 8x7B; 3) Democratizes access to a massive 256K context window; 4) The only model in its size class that fits up to 140K context on a single GPU (A100 with 80GB RAM); 5) Released with open weights under Apache 2.0; 6) Available on Hugging Face and coming soon to the NVIDIA API catalog. Note, Jamba is a pretrained, mixture-of-experts (MoE) generative text model, with 12B active parameters and a total of 52B parameters across all experts. It supports a 256K context length, and can fit up to 140K tokens on a single 80GB GPU. Total model size is about 104GB.

4. ***Grok-1.5: Advanced Model for Long Context Understanding and Reasoning <br>
   XAI announces Grok-1.5, a model with enhanced capabilities in coding, math-related tasks, and long context understanding. Grok-1.5 achieves high scores on multiple benchmarks, including math and code generation tasks, and demonstrates powerful retrieval capabilities in long contexts. The model aims to improve user experience and efficiency in tasks requiring advanced reasoning and understanding of long contextual information.*** <br><br>
   28 Mar, [XAI announced Grok-1.5](https://x.ai/blog/grok-1.5), the latest model capable of long context understanding and advanced reasoning. Grok-1.5 will be available to our early testers and existing Grok users on the 𝕏 platform in the coming days. One of the most notable improvements in Grok-1.5 is its performance in coding and math-related tasks. Tests show that Grok-1.5 achieved a 50.6% score on the MATH benchmark and a 90% score on the GSM8K benchmark, two math benchmarks covering a wide range of grade school to high school competition problems. Additionally, it scored 74.1% on the HumanEval benchmark, which evaluates code generation and problem-solving abilities. A new feature in Grok-1.5 is the capability to process long contexts of up to 128K tokens within its context window. This allows Grok to have an increased memory capacity of up to 16 times the previous context length, enabling it to utilize information from substantially longer documents. Furthermore, the model can handle longer and more complex prompts, while still maintaining its instruction-following capability as its context window expands. In the Needle In A Haystack (NIAH) evaluation, Grok-1.5 demonstrated powerful retrieval capabilities for embedded text within contexts of up to 128K tokens in length, achieving perfect retrieval results. Grok-1.5 will soon be available to early testers, and XAI looks forward to receiving users' feedback to help it improve Grok.

5. ***MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions <br>
   Google presents MagicLens, a series of self-supervised image retrieval models supporting open-ended instructions. MagicLens leverages large multimodal models to retrieve images with rich relations beyond visual similarity, achieving superior results on various benchmarks. The model's novel approach synthesizes instructions to reveal implicit relations in image pairs, demonstrating improved performance with smaller model sizes.*** <br><br>
    28 Mar, Google published a [paper](https://arxiv.org/pdf/2403.19651) “MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions”. The paper indicates that image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent work leverages text instructions to allow users to more freely express their search intents. However, existing work primarily focuses on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity. To show this, the study introduces MagicLens, a series of self-supervised image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and the research can bring those implicit relations explicit by synthesizing instructions via large multimodal models (LMMs) and large language models (LLMs). Trained on 36.7M (query image, instruction, target image) triplets with rich semantic relations mined from the web, MagicLens achieves comparable or better results on eight benchmarks of various image retrieval tasks than prior state-of-the-art (SOTA) methods. Remarkably, it outperforms previous SOTA but with a 50X smaller model size on multiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus further demonstrate the diversity of search intents supported by MagicLens.

6. ***ROUTER BENCH: Benchmark for Multi-LLM Routing System <br>
   UC Berkeley and UCSD introduce RouterBench, an evaluation framework for assessing the efficacy of multi-LLM routing systems. RouterBench provides a standardized benchmark and dataset for evaluating routing strategies, facilitating the development of more effective LLM routing solutions. The study highlights the potentials and limitations of various routing approaches, contributing to the advancement of LLM routing systems.*** <br><br>
    28 Mar, UC Berkeley and UCSD published a [paper](https://arxiv.org/pdf/2403.12031v1) “ROUTER B ENCH: A Benchmark for Multi-LLM Routing System”. The paper finds that as the range of applications for Large Language Models (LLMs) continues to grow, the demand for effective serving solutions becomes increasingly critical. Despite the versatility of LLMs, no single model can optimally address all tasks and applications, particularly when balancing performance with cost. This limitation has led to the development of LLM routing systems, which combine the strengths of various models to overcome the constraints of individual LLMs. Yet, the absence of a standardized benchmark for evaluating the performance of LLM routers hinders progress in this area. To bridge this gap, the study presents RouterBench, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies. The authors further propose a theoretical framework for LLM routing, and deliver a comparative analysis of various routing approaches through RouterBench, highlighting their potentials and limitations within the evaluation framework. This work not only formalizes and advances the development of LLM routing systems but also sets a standard for their assessment, paving the way for more accessible and economically viable LLM deployments. The code and data are available at https://github.com/withmartian/routerbench.
 
7. ***Long-Form Factuality in Large Language Models <br>
   Google presents a study on long-form factuality in large language models, proposing a method for evaluating factuality in open domains. The study introduces LongFact, a prompt set for evaluating long-form factuality, and develops a method called Search-Augmented Factuality Evaluator (SAFE) to assess factuality. Empirical results demonstrate the effectiveness of SAFE in evaluating factuality, with LLM agents achieving superhuman performance in rating factuality.*** <br><br>
    27 Mar, Google published a [paper](https://arxiv.org/pdf/2403.18802) “Long-form factuality in large language models”. The authors observed that Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, the authors first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. The authors then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which is called Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, the study proposes extending F1 score as an aggregated metric for long-form factuality. To do so, the authors balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall). Empirically, the paper demonstrates that LLM agents can achieve superhuman rating performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. The authors also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at [this https URL](https://github.com/google-deepmind/long-form-factuality).

8. ***DBRX: New Standard for Efficient Open Source LLMs <br>
   Databricks announces DBRX, a general-purpose large language model outperforming established open-source models on standard benchmarks. DBRX offers superior performance and efficiency compared to other models, including GPT-3.5, and is built on the MegaBlocks research project. The model's Mixture-of-Experts architecture and optimized training libraries contribute to its scalability and performance in various AI applications.*** <br><br>
    27 Mar, [Databricks announced DBRX](https://www.databricks.com/blog/announcing-dbrx-new-standard-efficient-open-source-customizable-llms), A new standard for efficient open source LLMs. DBRX is a general purpose large language model (LLM) built by Mosaic Research team that outperforms all established open source models on standard benchmarks. Databricks believes that pushing the boundary of open source models enables generative AI for all enterprises that is customizable and transparent. First, DBRX handily beats open source models, such as, LLaMA2-70B, Mixtral, and Grok-1 on language understanding, programming, math, and logic. Second, DBRX beats GPT-3.5 on most benchmarks. Third, DBRX is a Mixture-of-Experts (MoE) model built on the MegaBlocks research and open source project, making the model extremely fast in terms of tokens/second. DBRX uses only 36 billion parameters at any given time. But the model itself is 132 billion parameters. The aforementioned three reasons lead Databricks to believe that open source LLMs will continue gaining momentum. In particular, open source LLMs provide an exciting opportunity for organizations to customize open source LLMs that can become their IP, which they use to be competitive in their industry. Human feedback for quality and safety was collected through Mosaic AI Model Serving and Inference Tables. Customers and partners such as JetBlue, Block, NASDAQ, and Accenture are already using these same tools to build high quality AI systems. DBRX was trained on 3072 NVIDIA H100s connected by 3.2Tbps Infiniband. Databricks processed and cleaned this data using Apache Spark™ and Databricks notebooks. DBRX was trained by using optimized versions of our open-source training libraries: MegaBlocks, LLM Foundry, Composer, and Streaming. Databricks managed large scale model training and finetuning across thousands of GPUs using Mosaic AI Training service. Experimental results was logged by using MLflow. Human feedback was collected for quality and safety improvements through Mosaic AI Model Serving and Inference Tables. Databricks manually experimented with the model using the Databricks Playground. A detailed tech introduction [link is here](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm).

9. ***Improving Text-to-Image Consistency via Automatic Prompt Optimization <br>
   Meta and Uni Montreal et al. introduce OPT2I, a framework leveraging large language models to improve text-to-image consistency. OPT2I optimizes prompt-image consistency through iterative prompt optimization, enhancing performance on diverse datasets. The framework aims to build more reliable and robust text-to-image systems by harnessing the power of large language models.*** <br><br>
    27 Mar, Meta and Uni Montreal et al published a [paper](https://arxiv.org/pdf/2403.17804) “Improving Text-to-Image Consistency via Automatic Prompt Optimization”. The paper states that impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. This paper addresses these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. The framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. The extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. This work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.

10. ***The Role of Computational Science in Digital Twins<br>
   UTA and NASEM discuss the role of computational science in enabling digital twin technologies for scientific discovery and innovation. The paper identifies key research opportunities and challenges in computational sciences for advancing digital twin technologies across multiple domains. It emphasizes the importance of integrated research agendas and collaborations to realize the potential benefits of digital twins in various fields.*** <br><br>
    26 Mar, UTA and NASEM published a [blog](https://www.nature.com/articles/s43588-024-00609-4) on Nature Computational Science, “The role of computational science in digital twins”. The paper indicates that digital twins hold immense promise in accelerating scientific discovery, but the publicity currently outweighs the evidence base of success. The authors summarize key research opportunities in the computational sciences to enable digital twin technologies, as identified by a recent National Academies of Sciences, Engineering, and Medicine consensus study report. Digital twins are emerging as enablers for significant, sustainable progress across multiple domains of science, engineering, and medicine. However, realizing these benefits requires a sustained and holistic commitment to an integrated research agenda that addresses foundational challenges across mathematics, statistics, and computing. Within the virtual representation, advancing the models themselves is necessarily domain specific, but advancing the hybrid modeling and surrogate modeling embodies shared challenges that crosscut domains. Similarly, many of the physical counterpart challenges around sensor technologies and data are domain specific, but issues around fusing multimodal data, data interoperability, and advancing data curation practices embody shared challenges that crosscut domains. When it comes to the bidirectional flows, dedicated efforts are needed to advance data assimilation, inverse methods, control, and sensor-steering methodologies that are applicable across domains, while at the same time recognizing the domain-specific nature of decision making. Finally, there is substantial opportunity to develop innovative digital twin VVUQ methods that translate across domains.

11. ***The Unreasonable Ineffectiveness of the Deeper Layers<br>
   Meta, MIT, and Zephyr empirically study layer pruning strategies for pretrained LLMs, revealing minimal performance degradation after pruning. The study suggests that layer pruning methods can complement parameter-efficient fine-tuning strategies, reducing computational resources without significant loss in performance. Results indicate that deeper layers in LLMs may not be effectively utilized in current pretraining methods, suggesting potential improvements in leveraging network parameters.*** <br><br>
    26 Mar, Meta, MIT and Zephyr published a [paper](https://arxiv.org/pdf/2403.17887) “The Unreasonable Ineffectiveness of the Deeper Layers”. The authors empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed. To prune these models, the study identifies the optimal block of layers to prune by considering similarity across layers; then, to "heal" the damage, the study performs a small amount of finetuning. In particular, the research uses parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of the experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge.

12. ***AgentStudio: A Toolkit for Building General Virtual Agents<br>
   NTU Singapore and ETH introduce AgentStudio, a toolkit for developing general virtual agents capable of using arbitrary software on digital devices. AgentStudio covers the entire lifecycle of agent development, providing environments, data collection, evaluation, and visualization tools. The toolkit enables the creation of versatile virtual agents for various applications, promoting research towards developing general virtual agents for the future.*** <br><br>
    26 Mar, NTU Singapore and ETH published a [paper](https://arxiv.org/pdf/2403.17918) “AgentStudio: A Toolkit for Building General Virtual Agents”. The paper states that creating autonomous virtual agents capable of using arbitrary software on any digital device remains a major challenge for artificial intelligence. Two key obstacles hinder progress: insufficient infrastructure for building virtual agents in real-world environments, and the need for in-the-wild evaluation of fundamental agent abilities. To address this, the paper introduces AgentStudio, an online, realistic, and multimodal toolkit that covers the entire lifecycle of agent development. This includes environment setups, data collection, agent evaluation, and visualization. The observation and action spaces are highly generic, supporting both function calling and human-computer interfaces. This versatility is further enhanced by AgentStudio's graphical user interfaces, which allow efficient development of datasets and benchmarks in real-world settings. To illustrate, the study introduces a visual grounding dataset and a real-world benchmark suite, both created with graphical interfaces. Furthermore, the paper presents several actionable insights derived from AgentStudio, e.g., general visual grounding, open-ended tool creation, learning from videos, etc. The authors have [open-sourced](https://skyworkai.github.io/agent-studio/index.html) the environments, datasets, benchmarks, and interfaces to promote research towards developing general virtual agents for the future.

13. ***EU Investigates Tech Giants for Uncompetitive Practices<br>
   The EU announces investigations into major tech firms, including Meta, Apple, and Alphabet, for potential breaches of the Digital Markets Act. The investigations focus on alleged non-compliance with regulations related to app communication, user choice, data privacy, and search result preferences. The cases highlight growing concerns over the market dominance and practices of large tech companies, prompting regulatory scrutiny and enforcement actions.*** <br><br>
    25 Mar, [according to BBC](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.bbc.com%2Fnews%2Ftechnology-68655093&data=05%7C02%7Cd.zhu%40curtin.edu.au%7C0c401f7033fe4cb99a1d08dc4e4bee67%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638471335226257392%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&sdata=wLPB%2BSPNWBFNkoCvLv7jpivy3gm52QE%2FtQQEH9TeHnE%3D&reserved=0), The EU has announced investigations into some of the biggest tech firms in the world over uncompetitive practices. Meta, Apple, and Alphabet, which owns Google, are being looked into for potential breaches of the Digital Markets Act (DMA) introduced in 2022. If they are found to have broken the rules, the firms can face huge fines of up to 10% of their annual turnover. EU antitrust boss Margrethe Vestager and industry head Thierry Breton announced the investigations on Monday. Just six companies have obligations under the DMA, but they are also the world's largest tech firms: Alphabet, Apple, Meta, Amazon, Microsoft and ByteDance. None of the firms are actually based in Europe - five of them are in the US, while ByteDance has headquarters in Beijing. Three of them are now facing questions just two weeks after submitting their compliance reports, which will have been meticulously compiled. It comes three weeks after the EU fined Apple €1.8bn (£1.5bn) for breaking competition laws over music streaming. Meanwhile, the United States accused Apple of monopolising the smartphone market in a landmark lawsuit against the tech giant introduced last week. The EU said it will investigate five different possible acts of non-compliance in its announcement: 1) 1 & 2 - Whether Apple and Alphabet are not allowing apps to freely communicate with users and make contracts with them 2) 3 - Whether Apple is not giving users enough choice 3) 4 - Whether Meta is unfairly asking people to pay to avoid their data being used for adverts 4)5 - Whether Google preferences the firm's own goods and services in search results. The five cases are consumer-focused, and highly relatable to most people who use products from these companies, which is collectively billions of people worldwide.

14. ***Study Links ChatGPT Usage to Declining Academic Performance<br>
   PsyPost reports on a study examining the causes and consequences of ChatGPT usage among university students, suggesting potential negative effects on academic performance. The study finds correlations between ChatGPT usage and procrastination, memory loss, and decreased academic performance among students. Results indicate the importance of understanding the impacts of generative AI usage on student behavior and academic outcomes.*** <br><br>
    25 Mar, [according to PsyPost](https://www.psypost.org/chatgpt-linked-to-declining-academic-performance-and-memory-loss-in-new-study/), ChatGPT linked to declining academic performance and memory loss in new study. This post is based on paper “Is it harmful or helpful? Examining the causes and consequences of generative AI usage among university students” published on International Journal of Educational Technology in Higher Education. The paper states that while the discussion on generative artificial intelligence, such as ChatGPT, is making waves in academia and the popular press, there is a need for more insight into the use of ChatGPT among students and the potential harmful or beneficial consequences associated with its usage. Using samples from two studies, the current research examined the causes and consequences of ChatGPT usage among university students. Study 1 developed and validated an eight-item scale to measure ChatGPT usage by conducting a survey among university students (N = 165). Study 2 used a three-wave time-lagged design to collect data from university students (N = 494) to further validate the scale and test the study’s hypotheses. Study 2 also examined the effects of academic workload, academic time pressure, sensitivity to rewards, and sensitivity to quality on ChatGPT usage. Study 2 further examined the effects of ChatGPT usage on students’ levels of procrastination, memory loss, and academic performance. Study 1 provided evidence for the validity and reliability of the ChatGPT usage scale. Furthermore, study 2 revealed that when students faced higher academic workload and time pressure, they were more likely to use ChatGPT. In contrast, students who were sensitive to rewards were less likely to use ChatGPT. Not surprisingly, use of ChatGPT was likely to develop tendencies for procrastination and memory loss and dampen the students’ academic performance. Finally, academic workload, time pressure, and sensitivity to rewards had indirect effects on students’ outcomes through ChatGPT usage.

15. ***AIOS: LLM Agent Operating System<br>
   Rutgers University presents AIOS, an LLM agent operating system addressing challenges in deploying LLM-based intelligent agents. It tackles issues like scheduling, context maintenance, and agent integration, aiming to optimize resource allocation, facilitate context switch, and enable concurrent execution. Experiments demonstrate AIOS's reliability and efficiency, fostering better development and deployment of the AIOS ecosystem in the future. The project is open-source.*** <br><br>
    25 Mar, Rutgers University published a [paper](https://arxiv.org/pdf/2403.16971) “AIOS: LLM Agent Operating System”. The paper states that the integration and deployment of large language model (LLM)-based intelligent agents have been fraught with challenges that compromise their efficiency and efficacy. Among these issues are sub-optimal scheduling and resource allocation of agent requests over the LLM, the difficulties in maintaining context during interactions between agent and LLM, and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations. The rapid increase of agent quantity and complexity further exacerbates these issues, often leading to bottlenecks and sub-optimal utilization of resources. Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds a large language model into operating systems (OS) as the brain of the OS, enabling an operating system "with soul" -- an important step towards AGI. Specifically, AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents, and maintain access control for agents. The authors present the architecture of such an operating system, outline the core challenges it aims to resolve, and provide the basic design and implementation of the AIOS. The experiments on concurrent execution of multiple agents demonstrate the reliability and efficiency of the AIOS modules. Through this, the researchers aim to not only improve the performance and efficiency of LLM agents but also to pioneer for better development and deployment of the AIOS ecosystem in the future. The project is open-source at [this https URL](https://github.com/agiresearch/AIOS).

16. ***A Collective AI via Lifelong Learning and Sharing at the Edge<br>
   Loughborough University discusses a vision of collective AI where units learn independently and share knowledge. Key aspects include lifelong learning, knowledge exchange, local data use, and decentralized computation. The paper reviews recent advances converging towards collective machine-learned intelligence, predicting the emergence of scalable, resilient, and sustainable AI systems.*** <br><br>
    22 Mar, Natural Machine Intelligence published a [paper](https://www.nature.com/articles/s42256-024-00800-2) by Loughborough University, “A collective AI via lifelong learning and sharing at the edge”. The paper argues that one vision of a future artificial intelligence (AI) is where many separate units can learn independently over a lifetime and share their knowledge with each other. The synergy between lifelong learning and sharing has the potential to create a society of AI systems, as each individual unit can contribute to and benefit from the collective knowledge. Essential to this vision are the abilities to learn multiple skills incrementally during a lifetime, to exchange knowledge among units via a common language, to use both local data and communication to learn, and to rely on edge devices to host the necessary decentralized computation and data. The result is a network of agents that can quickly respond to and learn new tasks, that collectively hold more knowledge than a single agent and that can extend current knowledge in more diverse ways than a single agent. Open research questions include when and what knowledge should be shared to maximize both the rate of learning and the long-term learning performance. Here the authors review recent machine learning advances converging towards creating a collective machine-learned intelligence. The paper proposes that the convergence of such scientific and technological advances will lead to the emergence of new types of scalable, resilient and sustainable AI systems. “Recent research trends are extending AI models with the ability to continuously adapt once deployed, and make their knowledge reusable by other models, effectively recycling knowledge to optimise learning speed and energy demands,” Dr. Soltoggio said.  “We believe that the current dominating large, expensive, non-shareable and non-lifelong AI models will not survive in a future where sustainable, evolving, and sharing collective of AI units are likely to emerge.” “We believe similar dynamics are likely to occur in future societies of AI units that will implement democratic and collaborating collectives.”

17. ***AllHands: Large-scale Verbatim Feedback Analysis via LLMs<br>
   Microsoft introduces AllHands, an analytic framework leveraging LLMs for large-scale feedback analysis. AllHands employs classification, topic modeling, and an LLM agent to interpret natural language queries, providing comprehensive responses. Evaluation across diverse datasets shows superior efficacy, offering a natural language interface for insight extraction from feedback.*** <br><br>
    22 Mar, Microsoft published a [paper](https://arxiv.org/pdf/2403.15157) “AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large Language Models”. The paper indicates that verbatim feedback constitutes a valuable repository of user experiences, opinions, and requirements essential for software development. Effectively and efficiently extracting valuable insights from such data poses a challenging task. This paper introduces Allhands , an innovative analytic framework designed for large-scale feedback analysis through a natural language interface, leveraging large language models (LLMs). Allhands adheres to a conventional feedback analytic workflow, initially conducting classification and topic modeling on the feedback to convert them into a structurally augmented format, incorporating LLMs to enhance accuracy, robustness, generalization, and user-friendliness. Subsequently, an LLM agent is employed to interpret users' diverse questions in natural language on feedback, translating them into Python code for execution, and delivering comprehensive multi-modal responses, including text, code, tables, and images. The authors evaluate Allhands across three diverse feedback datasets. The experiments demonstrate that Allhands achieves superior efficacy at all stages of analysis, including classification and topic modeling, eventually providing users with an “ask me anything” experience with comprehensive, correct and human-readable response. Allhands stands as the first comprehensive feedback analysis framework that supports diverse and customized requirements for insight extraction through a natural language interface.

35. ***Polaris: Safety-focused LLM Constellation Architecture for Healthcare<br>
   HippocraticAI presents Polaris, a safety-focused LLM constellation for real-time patient-AI healthcare conversations. It employs a one-trillion parameter constellation system with a primary agent and specialist support agents trained on diverse medical data. Polaris undergoes comprehensive clinician evaluation, performing comparably to human nurses across various dimensions and outperforming larger LLM models in specific tasks.***  <br><br>
    20 Mar, HippocraticAI published a [paper](https://arxiv.org/pdf/2403.13313) “Polaris: A Safety-focused LLM Constellation Architecture for Healthcare” to introduce its Polaris LLM for healthcare system. Polaris is the first safety-focused LLM constellation for real-time patient-AI healthcare conversations. Unlike prior LLM works in healthcare focusing on tasks like question answering, Polaris specifically focuses on long multi-turn voice conversations. Polaris’s one-trillion parameter constellation system is composed of several multibillion parameter LLMs as co-operative agents: a stateful primary agent that focuses on driving an engaging conversation and several specialist support agents focused on healthcare tasks performed by nurses to increase safety and reduce hallucinations. The researchers develop a sophisticated training protocol for iterative co-training of the agents that optimize for diverse objectives. The models are trained on proprietary data, clinical care plans, healthcare regulatory documents, medical manuals, and other medical reasoning documents. The models are aligned to speak like medical professionals, using organic healthcare conversations and simulated ones between patient actors and experienced nurses. This allows Solaris to express unique capabilities such as rapport building, trust building, empathy and bedside manner. Finally, the paper presents the first comprehensive clinician evaluation of an LLM system for healthcare. The authors recruited over 1100 U.S. licensed nurses and over 130 U.S. licensed physicians to perform end-to-end conversational evaluations of Solaris by posing as patients and rating the system on several measures. The study demonstrates Polaris performs on par with human nurses on aggregate across dimensions such as medical safety, clinical readiness, conversational quality, and bedside manner. Additionally, the authors conduct a challenging task-based evaluation of the individual specialist support agents, and demonstrate the LLM agents significantly outperform a much larger general-purpose LLM (GPT-4) as well as from its own medium-size class (LLaMA-2 70B).  <br> <br> <br>




**24 Mar 2024**
1.	***Stability AI Leadership Change <br>
   Emad Mostaque resigned from his role as CEO of Stability AI, prompting the appointment of interim co-CEOs Shan Shan Wong and Christian Laforte. This leadership change signifies a commitment to the company's growth and vision. Mostaque's departure follows the exit of three developers from Stable Diffusion, a subsidiary of Stability AI.*** <br><br>
   23 Mar, according to [Stability.ai](https://stability.ai/news/stabilityai-announcement), Earlier today, Emad Mostaque resigned from his role as CEO of Stability AI and from his position on the Board of Directors of the company to pursue decentralized AI. The Board of Directors has appointed Shan Shan Wong, our Chief Operating Officer, and Christian Laforte, our Chief Technology Officer, as the interim co-CEOs of Stability AI. This leadership change marks an opportunity for Stability AI, the management team, Board of Directors, and investors in a shared commitment to realize the full vision for the company’s next stage of growth. Stability.ai is committed to preserving the exceptional team, cutting-edge technology, and vibrant community that’s been cultivated over the years, ensuring Stability AI remains a leader in open multi-modal generative AI. Emad Mostaque’s resign happened after three of Stable Diffusion’s original developers [reportedly leave Stability AI. ](https://siliconangle.com/2024/03/20/three-stable-diffusions-original-developers-reportedly-leave-stability-ai/)

2. ***AI Flood Forecasting Advancements <br>
   A study published in Nature reveals the effectiveness of AI-based forecasting in predicting extreme floods, particularly in areas lacking streamflow gauge networks. The research demonstrates the reliability of AI in predicting floods up to a five-day lead time, surpassing current methods. This highlights the importance of increasing hydrological data availability for better flood warnings globally.***  <br><br>
  20 Mar, Nature published a [paper](https://www.nature.com/articles/s41586-024-07145-1.pdf) from Google and other institutes “Global prediction of extreme floods in ungauged watersheds”. The paper finds that floods are one of the most common natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow gauge networks1. Accurate and timely warnings are critical for mitigating flood risks2, but hydrological simulation models typically must be calibrated to long data records in each watershed. The paper shows that artificial intelligence-based forecasting achieves reliability in predicting extreme riverine events in ungauged watersheds at up to a five-day lead time that is similar to or better than the reliability of nowcasts (zero-day lead time) from a current state-of-the-art global modelling system (the Copernicus Emergency Management Service Global Flood Awareness System). In addition, the study achieves accuracies over five-year return period events that are similar to or better than current accuracies over one-year return period events. This means that artificial intelligence can provide flood warnings earlier and over larger and more impactful events in ungauged basins. The model developed here was incorporated into an operational early warning system that produces publicly available (free and open) forecasts in real time in over 80 countries. This work highlights a need for increasing the availability of hydrological data to continue to improve global access to reliable flood warnings.

3.	***Google Fined in France for Breach <br>
   Google faces a €270 million fine in France for breaching commitments with news publishers, including using their content without proper notification for training its AI model Bard/Gemini. The competition authority also criticizes Google for failing to provide necessary information to publishers during negotiations. Google agrees to the fine and commits to improving its practices.*** <br><br>
   20 Mar, according to [techcrunch.com](https://techcrunch.com/2024/03/20/google-hit-with-270m-fine-in-france-as-authority-finds-news-publishers-data-was-used-for-gemini/), Google hit with $270M fine in France as authority finds news publishers’ data was used for Gemini. According to the competition watchdog, Google disregarded some of its previous commitments with news publishers. But the decision is especially notable because it drops something else that’s bang up-to-date — by latching onto Google’s use of news publishers’ content to train its generative AI model Bard/Gemini. The competition authority has found fault with Google for failing to notify news publishers of this GenAI use of their copyrighted content. This is in light of earlier commitments Google made which are aimed at ensuring it undertakes fair payment talks with publishers over reuse of their content. Google has agreed not to contest the Autorité’s latest findings — in exchange for a fast-tracked process and making a monetary payment. The Autorité is also sanctioning Google for a raft of other issues related to how it negotiates with French news publishers, finding it failed to provide them with all the information needed to ensure fair bargaining of remuneration for their content. It also said Google failed to act on its commitment to update remuneration contracts in line with its pledges.

4. ***Challenges of AI Watermarking <br>
   Despite efforts from major tech companies to implement watermarking standards to combat AI-driven misinformation, challenges remain in ensuring its effectiveness. Watermarking can be bypassed, posing doubts about its reliability, especially in identifying and mitigating deepfakes. Educating the public about recognizing watermarks is crucial but challenging.*** <br><br>
   20 Mar, [according to CNBC](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nbcnews.com%2Ftech%2Ftech-news%2Fwatermark-deepfake-solution-ai-misinformation-cant-stop-de-rcna137370&data=05%7C02%7Cd.zhu%40curtin.edu.au%7Cb99735b3f459486e50d108dc48a4b52d%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638465119454526817%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&sdata=fXrHzQ95GevYJd6vP4cJR5U%2B9jkcDhVmlf%2FdPZ9W6ZE%3D&reserved=0), Big Tech says AI watermarks could curb misinformation, but they're easy to sidestep. Big Tech companies have agreed to watermarking standards as a potential solution to combat AI-driven misinformation online. However, those who misuse technology to deceive others are not adhering to these standards. Watermarking involves embedding invisible tags or visible labels into media, but it has proven to be easily bypassed, raising doubts about its effectiveness. Despite efforts from companies like Meta, Google, and Microsoft to implement watermarking, challenges remain in ensuring its reliability, particularly in identifying and mitigating deepfakes, which continue to pose significant threats, including in political disinformation campaigns and scams. Educating the public to recognize watermarks and verify media authenticity may be crucial for the success of such initiatives, but achieving widespread awareness poses significant challenges.

5.	***Huggingface Releases Common Corpus <br>
    Huggingface releases Common Corpus (CC), the largest public domain dataset for training Large Language Models (LLMs). CC aims to support open science in AI training while providing diverse and reproducible datasets. It includes multilingual content and is intended to enhance accessibility and diversity in AI research.***  <br><br>
    20 Mar, [Huggingface released](https://huggingface.co/blog/Pclanglais/common-corpus) Common Corpus (CC), largest public domain dataset for training LLMs. CC is an international initiative coordinated by Pleias, involving researchers in LLM pretraining, AI ethics and cultural heritage like, in association with major organizations committed to an open science approach for AI (HuggingFace, Occiglot, Eleuther, Nomic AI). Common Corpus has received the support of Lang:IA, a state start-up supported by the French Ministry of Culture and the Direction du numérique. The release of Common Corpus aims to show it is possible to train Large Language Model on fully open and reproducible corpus, without using copyright content from Common Crawl and other more dubious sources. CC holds the largest English-speaking dataset to date with 180 billion words. This includes a major US collection of 21 millions digitized newspapers, Chronicling America that can also be fully explored with an original corpus map created by Nomic AI, as well as large monographs datasets collected by digital historian Sebastian Majstorovic. CC is also multilingual. It also incorporates the largest open dataset to date in French (110 billion words), German (30 billion words), Spanish, Dutch or Italian, as well as a very long tail of low resource languages that are currently hardly represented in the training of Large Language Model. It includes millions of books with reasoning-rich content which makes it ideal for creating models with long context. CC is the start of a long work in progress. Many things remain to be done to achieve this end and to enhance this collection. CC aims to support a strong data commons for AI to ease research and make it more reproducible, but also to make AI more accessible, diverse and democratic, by ensuring that anyone can have a look into the large models. The size of [US-PD-Newspapers](https://huggingface.co/datasets/PleIAs/US-PD-Newspapers) is a 410GB.

6. ***Meta's Solution to Reversal Curse <br>
    Meta proposes reverse training as a solution to the Reversal Curse in Large Language Models (LLMs), where models fail to generalize certain linguistic constructs. Reverse training involves training LLMs in both forward and reverse directions, leading to superior performance on standard and reversal tasks, addressing the issue.*** <br><br>
   20 Mar Meta published a [paper](https://arxiv.org/pdf/2403.13799) “Reverse Training to Nurse the Reversal Curse”. The authors argue that Large language models (LLMs) have a surprising failure: when trained on "A has a feature B", they do not generalize to "B is a feature of A", which is termed the Reversal Curse. Even when training with trillions of tokens this issue still appears due to Zipf's law - hence even if the models are trained on the entire internet. This work proposes an alternative training scheme, called reverse training, whereby all words are used twice, doubling the amount of available tokens. The LLM is trained in both forward and reverse directions by reversing the training strings while preserving (i.e., not reversing) chosen substrings, such as entities. The authors show that data-matched reverse-trained models provide superior performance to standard models on standard tasks, and compute-matched reverse-trained models provide far superior performance on reversal tasks, helping resolve the reversal curse issue.

7. ***Sakana AI's Evolutionary Model Merging <br>
    Sakana AI introduces an evolutionary approach to automate the creation of powerful foundation models through model merging. This method, which harnesses collective intelligence from diverse open-source models, demonstrates effectiveness in generating state-of-the-art models, even across different domains like Japanese language and mathematics.*** <br><br>
    19 Mar, researchers from Sakana AI published a [paper](https://arxiv.org/pdf/2403.13187) “Evolutionary Optimization of Model Merging Recipes”. The authors present a novel application of evolutionary algorithms to automate the creation of powerful foundation models. While model merging has emerged as a promising approach for LLM development due to its cost-effectiveness, it currently relies on human intuition and domain knowledge, limiting its potential. This study proposes an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. The approach operates in both parameter space and data flow space, allowing for optimization beyond just the weights of the individual models. This approach even facilitates cross-domain merging, generating models like a Japanese LLM with Math reasoning capabilities. Surprisingly, the Japanese Math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM generated through the proposed approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese VLMs. This work not only contributes new state-of-the-art models back to the open-source community, but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to foundation model development.

8.	***NVIDIA's Blackwell Platform Launch <br>
    NVIDIA launches the Blackwell platform, aiming to power real-time generative AI on trillion-parameter LLMs with significantly reduced cost and energy consumption compared to previous architectures. Blackwell features transformative technologies for accelerated computing, facilitating breakthroughs in various industries including generative AI.*** <br><br>
    18 Mar, according to [Nvidia](https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing), NVIDIA Blackwell Platform Arrives to Power a New Era of Computing. Powering a new era of computing, NVIDIA announced that the NVIDIA Blackwell platform has arrived — enabling organizations everywhere to build and run real-time generative AI on trillion-parameter large language models at up to 25x less cost and energy consumption than its predecessor. The Blackwell GPU architecture features six transformative technologies for accelerated computing, which will help unlock breakthroughs in data processing, engineering simulation, electronic design automation, computer-aided drug design, quantum computing and generative AI — all emerging industry opportunities for NVIDIA. “For three decades we’ve pursued accelerated computing, with the goal of enabling transformative breakthroughs like deep learning and AI,” said Jensen Huang, founder and CEO of NVIDIA. “Generative AI is the defining technology of our time. Blackwell is the engine to power this new industrial revolution. Working with the most dynamic companies in the world, we will realize the promise of AI for every industry.” Among the many organizations expected to adopt Blackwell are Amazon Web Services, Dell Technologies, Google, Meta, Microsoft, OpenAI, Oracle, Tesla and xAI. Main features of Blackwell include: New Blackwell GPU, NVLink and Resilience Technologies Enable Trillion-Parameter-Scale AI Models; New Tensor Cores and TensorRT- LLM Compiler Reduce LLM Inference Operating Cost and Energy by up to 25x; New Accelerators Enable Breakthroughs in Data Processing, Engineering Simulation, Electronic Design Automation, Computer-Aided Drug Design and Quantum Computing; Widespread Adoption by Every Major Cloud Provider, Server Maker and Leading AI Company. [CNBC](https://www.cnbc.com/2024/03/19/nvidias-blackwell-ai-chip-will-cost-more-than-30000-ceo-says.html) estimated latest AI chip will cost more than $30,000.

9.	***Microsoft's TnT-LLM Framework <br>
    Microsoft presents TnT-LLM, a framework utilizing Large Language Models (LLMs) for text mining at scale. TnT-LLM automates label generation and assignment processes, achieving accurate label taxonomies and efficient classification for large-scale text analysis, as demonstrated in Bing Copilot.*** <br><br>
    18 Mar, Microsoft published a [paper](https://arxiv.org/pdf/2403.12173) “TnT-LLM: Text Mining at Scale with Large Language Models”. The paper indicates that transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach which enables LLMs to produce and refine a label taxonomy iteratively. In the second phase, LLMs are used as data labelers that yield training samples so that lightweight supervised classifiers can be reliably built, deployed, and served at scale. We apply TnT-LLM to the analysis of user intent and conversational domain for Bing Copilot (formerly Bing Chat), an open-domain chat-based search engine. Extensive experiments using both human and automatic evaluation metrics demonstrate that TnT-LLM generates more accurate and relevant label taxonomies when compared against state-of-the-art baselines, and achieves a favorable balance between accuracy and efficiency for classification at scale. We also share our practical experiences and insights on the challenges and opportunities of using LLMs for large-scale text mining in real-world applications.


10. ***Google's PERL for RLHF <br>
    Google introduces PERL, a parameter-efficient reinforcement learning method for aligning Large Language Models (LLMs) with human feedback. PERL performs comparably to conventional RLHF methods while being faster and requiring less memory, reducing computational burdens in LLM alignment.*** <br><br>
   15 Mar, Google published a [paper](https://arxiv.org/abs/2403.10704) “PERL: Parameter Efficient Reinforcement Learning from Human Feedback”. The paper indicates that Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong method to align Pretrained Large Language Models (LLMs) with human preferences. But training models with RLHF is computationally expensive, and an overall complex process. This work studies RLHF where the underlying models are trained using the parameter efficient method of Low-Rank Adaptation (LoRA) introduced by Hu et al. [2021]. The study investigates the setup of "Parameter Efficient Reinforcement Learning" (PERL), in which the authors perform reward model training and reinforcement learning using LoRA. The research compares PERL to conventional fine-tuning (full-tuning) across various configurations for 7 benchmarks, including 2 novel datasets, of reward modeling and reinforcement learning. The study finds that PERL performs on par with the conventional RLHF setting, while training faster, and with less memory. This enables the high performance of RLHF, while reducing the computational burden that limits its adoption as an alignment technique for Large Language Models. The authors also release 2 novel thumbs up/down preference datasets: "Taskmaster Coffee", and "Taskmaster Ticketing" to promote research around RLHF.

11.	***US State Department's AI Security Warning <br>
    A report commissioned by the US State Department warns of catastrophic national security risks posed by rapidly evolving artificial intelligence, emphasizing the urgent need for regulatory safeguards and limits on AI capabilities. The report calls for launching a new AI agency to address these threats.*** <br><br>
    12 Mar, according to [CNN Business](https://edition.cnn.com/2024/03/12/business/artificial-intelligence-ai-report-extinction/index.html), A new report commissioned by the US State Department paints an alarming picture of the “catastrophic” national security risks posed by rapidly evolving artificial intelligence, warning that time is running out for the federal government to avert disaster. The findings were based on interviews with more than 200 people over more than a year – including top executives from leading AI companies, cybersecurity researchers, weapons of mass destruction experts and national security officials inside the government. [The report](https://www.gladstone.ai/action-plan), released this week by Gladstone AI, flatly states that the most advanced AI systems could, in a worst case, “pose an extinction-level threat to the human species.” “AI is already an economically transformative technology. It could allow us to cure diseases, make scientific discoveries, and overcome challenges we once thought were insurmountable,” said by Jeremie Harris, CEO and co-founder of Gladstone AI. “But it could also bring serious risks, including catastrophic risks, that we need to be aware of,” Harris said. “And a growing body of evidence — including empirical research and analysis published in the world’s top AI conferences — suggests that above a certain threshold of capability, AIs could potentially become uncontrollable.” Gladstone AI’s report calls for dramatic new steps aimed at confronting this threat, including launching a new AI agency, imposing “emergency” regulatory safeguards and limits on how much computer power can be used to train AI models.

12. ***Stanford's Pyvene Library for Model Interventions
    Stanford University develops pyvene, a Python library facilitating interventions on PyTorch models for improved interpretability and performance analysis. pyvene supports complex intervention schemes and provides a unified framework for performing interventions on neural models, enhancing their transparency and utility.*** <br><br>
    12 Mar, Stanford Uni published a [paper](https://arxiv.org/pdf/2403.07809) “pyvene: A Library for Understanding and Improving PyTorch Models via Interventions”.  The paper indicates that interventions on model-internal states are fundamental operations in many areas of AI, including model editing, steering, robustness, and interpretability. To facilitate such research, the study introduces pyvene, an open-source Python library that supports customizable interventions on a range of different PyTorch modules. pyvene supports complex intervention schemes with an intuitive configuration format, and its interventions can be static or include trainable parameters. The authors show how pyvene provides a unified and extensible framework for performing interventions on neural models and sharing the intervened upon models with others. The paper illustrates the power of the library via interpretability analyses using causal abstraction and knowledge localization. The authors publish their library through Python Package Index (PyPI) and provide code, documentation, and tutorials at this https URL.

13. ***Evolution of Language Model Algorithms
    Epoch.ai and MIT's paper examines algorithmic advancements in language model pre-training. Analyzing 200+ evaluations from 2012 to 2023, the study finds computational requirements halved every 8 months, outpacing Moore's Law. Augmented scaling laws quantify progress, highlighting compute's significant role despite algorithmic innovations like transformers.*** <br><br>
9 Mar, Epoch.ai and MIT published a [paper](https://arxiv.org/pdf/2403.05812v1) “Algorithmic progress in language models”. The authors investigate the rate at which algorithms for pre-training language models have improved since the advent of deep learning. Using a dataset of over 200 language model evaluations on Wikitext and Penn Treebank spanning 2012-2023, the study finds that the compute required to reach a set performance threshold has halved approximately every 8 months, with a 95% confidence interval of around 5 to 14 months, substantially faster than hardware gains per Moore's Law. The authors estimate augmented scaling laws, which enable the authors to quantify algorithmic progress and determine the relative contributions of scaling models versus innovations in training algorithms. Despite the rapid pace of algorithmic progress and the development of new architectures such as the transformer, the analysis reveals that the increase in compute made an even larger contribution to overall performance improvements over this time period. Though limited by noisy benchmark data, the analysis quantifies the rapid progress in language modeling, shedding light on the relative contributions from compute and algorithms.


**17 Mar 2024**

1. ***Cognition's Report on Devin's Performance in Software Development:
Cognition released a technical report on Devin, an AI agent specialized in software development, showcasing its remarkable performance in resolving GitHub issues and pull requests. Devin's success rate of 13.86% surpasses previous models significantly, even outperforming assisted models when unassisted. This demonstrates Devin's potential to contribute effectively to large, complex codebases.*** <br><br>
   15 Mar, Cognition published its [technical report](https://www.cognition-labs.com/post/swe-bench-technical-report) of Devin. According to the report, One of the goals at Cognition is to enable Devin, an AI agent specializing in software development, to contribute code successfully to large, complex codebases. To evaluate Devin, Cognition turns to SWE-bench, an automated benchmark for software engineering systems consisting of GitHub issues and pull requests. SWE-bench is a great choice because it deterministically evaluates (via unit tests) a system’s ability to solve issues in real world codebases, unlike benchmarks like HumanEval which are limited to standalone functions. In SWE-bench, Devin successfully resolves 13.86%* of issues, far exceeding the previous highest unassisted baseline of 1.96%. Even when given the exact files to edit ("assisted"), the best previous model only resolves 4.80% of issues.

2. ***Apple's Paper on Multimodal Large Language Models (MLLMs):
Apple published a paper titled "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training," highlighting key findings on building performant MLLMs. Through meticulous analysis of architecture components and pre-training data choices, the study identifies crucial design principles for achieving state-of-the-art few-shot results. The MM1 family of multimodal models, with up to 30B parameters, demonstrates superior performance in pre-training metrics and competitive results in supervised fine-tuning across various benchmarks.*** <br><br>
   14 Mar, Apple Co. published a [paper](https://arxiv.org/pdf/2403.09611) “MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training”. In this work, the authors discuss building performant Multimodal Large Language Models (MLLMs). In particular, the research studies the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, the paper identified several crucial design lessons. For example, the study demonstrates that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, the paper shows that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, the authors build MM1, a family of multimodal models up to 30B parameters, consisting of both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.

3. ***European Union's Probe into Big Tech's Use of AI:
The EU initiated an investigation into Big Tech's utilization of artificial intelligence and management of deepfakes, citing concerns about potential election disruption. Targeting companies like Meta and Microsoft, the inquiry seeks insights into mitigating risks associated with generative AI. Online platforms have until April 5 to outline measures addressing the spread of election misinformation, with AI-related mishaps potentially incurring fines under the Digital Services Act.*** <br><br>
   14 Mar, according to [CNN Business](https://edition.cnn.com/2024/03/14/tech/europe-generative-ai-investigation/index.html), The European Union has launched an investigation into Big Tech’s use of artificial intelligence (AI) and its handling of computer-generated deepfakes. This move comes amid concerns that such technology could disrupt elections. The inquiry targets companies including Meta, Microsoft, Snap, TikTok, and X. The European Commission aims to understand how these companies intend to mitigate risks associated with generative AI as they increasingly deploy consumer-facing AI tools. Officials have requested detailed information from these services regarding their measures to address risks related to generative AI. These risks include hallucinations (where AI provides false information), the viral spread of deepfakes, and automated manipulation of services that could mislead voters. Of particular concern is the potential chaos that generative AI could cause in the lead-up to this summer’s EU parliamentary elections. Online platforms have until April 5 to respond, outlining steps they’ve taken to prevent AI tools from disseminating election misinformation. The European Commission is also probing whether platforms are prepared for last-minute scenarios, such as the sudden distribution of high-impact deepfakes. Companies need to be aware that AI-related mishaps could result in fines or penalties under the Digital Services Act, a significant tech-regulation law governing social media and other major online platforms. Responses from these companies will contribute to a set of election security guidelines for tech platforms, which the European Commission aims to finalize by March 27.

4. ***Microsoft's Paper on AutoDev: Automated AI-Driven Development:
Microsoft introduced AutoDev, an automated AI-driven software development framework designed for intricate software engineering tasks. AutoDev empowers users to define complex objectives, executed by autonomous AI agents capable of diverse operations on codebases. The framework ensures a secure development environment and exhibits promising results in automating software engineering tasks while maintaining user control.*** <br><br>
   13 Mar, Microsoft published a [paper](https://arxiv.org/pdf/2403.08299) “AutoDev: Automated AI-Driven Development”. The paper states that the landscape of software development has witnessed a paradigm shift with the advent of AI-powered assistants, exemplified by GitHub Copilot. However, existing solutions are not leveraging all the potential capabilities available in an IDE such as building, testing, executing code, git operations, etc. Therefore, they are constrained by their limited capabilities, primarily focusing on suggesting code snippets and file manipulation within a chat-based interface. To fill this gap, the paper presents AutoDev, a fully automated AI-driven software development framework, designed for autonomous planning and execution of intricate software engineering tasks. AutoDev enables users to define complex software engineering objectives, which are assigned to AutoDev's autonomous AI Agents to achieve. These AI agents can perform diverse operations on a codebase, including file editing, retrieval, build processes, execution, testing, and git operations. They also have access to files, compiler output, build and testing logs, static analysis tools, and more. This enables the AI Agents to execute tasks in a fully automated manner with a comprehensive understanding of the contextual information required. Furthermore, AutoDev establishes a secure development environment by confining all operations within Docker containers. This framework incorporates guardrails to ensure user privacy and file security, allowing users to define specific permitted or restricted commands and operations within AutoDev. In the evaluation, the authers tested AutoDev on the HumanEval dataset, obtaining promising results with 91.5% and 87.8% of Pass@1 for code generation and test generation respectively, demonstrating its effectiveness in automating software engineering tasks while maintaining a secure and user-controlled development environment.

5. ***Uni. of Montreal and Eleuther AI's Paper on Continual Pre-training of Large Language Models:
A joint paper presents scalable strategies for continually pre-training large language models (LLMs), demonstrating matching performance to re-training baselines with significantly reduced computational resources. By combining learning rate re-warming, re-decaying, and data replay, the study achieves competitive results across various LLM pre-training datasets. These findings suggest a more efficient approach to updating LLMs while conserving computational costs.*** <br><br>
    13 Mar, Uni. of Montreal and Eleuther AI published a [paper](https://arxiv.org/pdf/2403.08763) “Simple and Scalable Strategies to Continually Pre-train Large Language Models”. The authors indicate that Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. This study shows that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, the authers show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English→English) and a stronger distribution shift (English→German) at the 405M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, the studay also finds that the continual learning strategies match the re-training baseline for a 10B parameter LLM. Exeprimental results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, the work proposes alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.

6. ***European Union's Regulatory Ground Rules on AI:
The EU parliament approved landmark regulatory ground rules governing the use of artificial intelligence, marking a significant milestone in international AI regulation. The EU AI Act categorizes AI technologies based on risk levels and aims to ensure transparency and accountability in AI applications. This regulation is poised to reshape the tech landscape and set a global standard for AI governance.*** <br><br>
    13 Mar, [according to CNBE](https://www.cnbc.com/2024/03/13/european-lawmakers-endorse-worlds-first-major-act-to-regulate-ai.html),  The European Union’s parliament on Wednesday approved the world’s first major set of regulatory ground rules to govern the mediatized artificial intelligence at the forefront of tech investment. “Europe is NOW a global standard-setter in AI,” Thierry Breton, the European commissioner for internal market, wrote on X. Born in 2021, the EU AI Act divides the technology into categories of risk, ranging from “unacceptable” — which would see the technology banned — to high, medium and low hazard. The regulation is expected to enter into force at the end of the legislature in May, after passing final checks and receiving endorsement from the European Council. Implementation will then be staggered from 2025 onward. Last week, the bloc brought into force landmark competition legislation set to rein in U.S. giants. Under the Digital Markets Act, the EU can crack down on anti-competitive practices from major tech companies and force them to open out their services in sectors where their dominant position has stifled smaller players and choked freedom of choice for users. Concerns have been mounting over the potential for abuse of artificial intelligence, even as heavyweight players like Microsoft, Amazon, Google and chipmaker Nvidia beat the drum for AI investment. Governments fear the possibility of deepfakes — forms of artificial intelligence that generate false events, including photos and videos — being deployed in the lead-up to a swathe of key global elections this year. Legal professionals described the act as a major milestone for international artificial intelligence regulation, noting it could pave the path for other countries to follow suit. Some raised concerns that the act could quickly become outdated as the fast-moving technology continues to evolve.

7. ***Google's Paper on Scaling Instructable Agents Across Simulated Worlds:
Google presents the Scalable, Instructable, Multiworld Agent (SIMA) project, aiming to develop agents capable of following free-form instructions in diverse virtual 3D environments. SIMA focuses on language-driven generality while interacting with environments using image observations and language instructions. The project represents a step toward achieving generalized AI capable of performing complex tasks across various simulated environments.*** <br><br>
    13 Mar, Google published a [paper](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/sima-generalist-ai-agent-for-3d-virtual-environments/Scaling%20Instructable%20Agents%20Across%20Many%20Simulated%20Worlds.pdf) “Scaling Instructable Agents Across Many Simulated Worlds”. The paper indicates that building embodied AI systems that can follow arbitrary language instructions in any 3D environment is a key challenge for creating general AI. Accomplishing this goal requires learning to ground language in perception and embodied actions, in order to accomplish complex tasks. The Scalable, Instructable, Multiworld Agent (SIMA) project tackles this by training agents to follow free-form instructions across a diverse range of virtual 3D environments, including curated research environments as well as openended, commercial video games. The goal of the study is to develop an instructable agent that can accomplish anything a human can do in any simulated 3D environment. The proposed approach focuses on language-driven generality while imposing minimal assumptions. The agents interact with environments in real-time using a generic, human-like interface: the inputs are image observations and language instructions and the outputs are keyboard-and-mouse actions. This general approach is challenging, but it allows agents to ground language across many visually complex and semantically rich environments while also allowing people to readily run agents in new environments. The paper describes the motivation and goal, the initial progress have made, and promising preliminary results on several diverse research environments and a variety of commercial video games.

8. ***LlamaIndex's Launch of LlamaParse: GenAI-Native Document Parsing Platform:
LlamaIndex launched LlamaParse, a GenAI-native document parsing platform offering enhanced parsing results with natural-language instructions. LlamaParse supports diverse document types and integrates advanced indexing capabilities, enabling users to build state-of-the-art document retrieval systems. Current versions provide cloud-based parsing with generous limits and prioritize user privacy and file security.*** <br><br>
    13 Mar, LlamaIndex launched the first [GenAI-native document parsing platform](https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform). The LlamaParse can simple, natural-language instructions from a user and provide radically better parsing results. Examples include 1) [extraction of tables](https://github.com/run-llama/llama_parse/blob/main/examples/demo_advanced.ipynb), and seamlessly integrates with the advanced indexing/retrieval capabilities that the open-source framework offers, enabling users to build state-of-the-art document RAG; 2) Parsing comic books, one can give the parser plain, English-language instructions on what to do; 3) parsing mathematical equations; 4) extraction of images. LlamaParse can parse documents in different types such as .doc, .docx, .pptx, .rtf, .pages, .key, .epub and others. Current free version allows a user to parse on cloud 7000 pages/week, and 1000 pages/day, and local parse needs permission with LlamaIndex.

9. ***Cognition's Introduction of Devin: Fully Autonomous AI Software Engineer:
Cognition unveiled Devin, a fully autonomous AI software engineer capable of handling entire development projects end-to-end. Unlike existing coding assistants, Devin can execute complex engineering tasks independently within a sandboxed compute environment. This marks a significant advancement in AI-assisted development, offering engineers a comprehensive AI worker for project management and execution.*** <br><br>
    12 Mar, according to [VentureBeat](https://venturebeat.com/ai/cognition-emerges-from-stealth-to-launch-ai-software-engineer-devin/), Cognition announced a fully autonomous AI software engineer called “Devin”. While there are multiple coding assistants out there, including the famous Github Copilot, Devin is said to stand out from the crowd with its ability to handle entire development projects end-to-end, right from writing the code and fixing the bugs associated with it to final execution. This is the first offering of this kind and even capable of handling projects on Upwork, the startup has demonstrated. The announcement of Devin marks a significant shift in the AI-assisted development space, giving engineers a full-fledged AI worker for their projects, rather than a copilot that could merely write barebones code or suggest snippets. However, as of now, Devin remains non-public, with the company opening access only to a select few customers. Devin can access common developer tools, including its own shell, code editor and browser, within a sandboxed compute environment to plan and execute complex engineering tasks requiring thousands of decisions. The human user simply types a natural language prompt into Devin’s chatbot style interface, and the AI software engineer takes it from there, developing a detailed, step-by-step plan to tackle the problem. It then begins the project using its developer tools, just like how a human would use them, writing its own code, fixing issues, testing and reporting on its progress in real-time, allowing the user to keep an eye on everything as it works. If something doesn’t look right to the human observer, the user can also jump into the chat interface and give the AI a command to fix it. Devin is capable of handling a wide range of dev tasks, but core technology remains undescribed.

10. ***Google's ICLR 2024 Research Paper on Graph Encoding for Large Language Models:
Google introduced its research paper on encoding graph-structured data for consumption by large language models (LLMs), addressing the understudied problem of reasoning on graphs. The study identifies crucial factors influencing LLM performance on graph reasoning tasks and demonstrates significant performance improvements with the correct choice of encoders. These insights pave the way for enhanced graph reasoning capabilities within LLMs.*** <br><br>
    12 Mar, Google released a [blog](https://blog.research.google/2024/03/talk-like-graph-encoding-graphs-for.html) to introduce its ICLR 2024 research paper “Talk like a graph: encoding graphs for large language models”. Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem. This work performs the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. The research shows that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights the research illustrates how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task.

11. ***Google, ETH, OpenAI's Paper on Model-Stealing Attack on Production Language Models:
The authors present the first model-stealing attack capable of extracting precise information from black-box production language models like OpenAI's ChatGPT. The attack recovers transformer model components, providing insights into the structure and hidden dimensions of these models. The study highlights potential defenses against such attacks and discusses implications for future research.*** <br><br>
    11 Mar, Google, ETH, OpenAI published a [paper](https://arxiv.org/pdf/2403.06634) “Stealing Part of a Production Language Model”. The authors introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, the attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under $20 USD, the attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. The researchers thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. The study also recovered the exact hidden dimension size of the gpt-3.5-turbo model, and estimated it would cost under $2,000 in queries to recover the entire projection matrix. The authors conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend the proposed attack.

12. ***EU Investigation on European Commission's Use of Microsoft 365:
The European Data Protection Supervisor found the European Commission in breach of data protection rules through its use of Microsoft 365. Concerns were raised regarding data processing, transfers, and purpose limitation under the licensing agreement with Microsoft Ireland. This investigation underscores ongoing regulatory scrutiny over data privacy issues in cloud-based productivity software.*** <br><br>
    8  Mar, according to [Techcrunch](https://techcrunch.com/2024/03/11/edps-microsoft-365/), A lengthy investigation into the European Union’s use of Microsoft 365 has found the Commission breached the bloc’s data protection rules through its use of the cloud-based productivity software. The European Data Protection Supervisor (EDPS) said the Commission infringed “several key data protection rules when using Microsoft 365”. “The Commission did not sufficiently specify what types of personal data are to be collected and for which explicit and specified purposes when using Microsoft 365,” the data supervisor, Wojciech Wiewiórowski, wrote, adding: “The Commission’s infringements as data controller also relate to data processing, including transfers of personal data, carried out on its behalf.” Microsoft and the Commission were contacted for a response to the EDPS’ findings. But at the time of writing neither had responded. At issue is how Microsoft processes the data of users of its cloud service. EU regulators have been flagging concerns about this for years, including in relation to the legal basis Microsoft claims for processing data; a lack of clarity and precision in the wording of its contracts for the product; and no technical safeguards being applied to ensure data is only being used for providing and maintaining the service. On data transfers, the EDPS found the Commission failed to ensure adequate safeguards were applied to these data exports to ensure essentially equivalent protections for data were in place once it left the bloc. The EDPS found the Commission infringed the “purpose limitation” principle of applicable data protection rules by failing to sufficiently determine the types of personal data collected under the licensing agreement it concluded with Microsoft Ireland, meaning it was unable to ensure these were specific and explicit.

13. ***Google's Tech Report on Gemini 1.5: Multimodal Understanding Across Contexts:
Google introduced Gemini 1.5 Pro, a highly efficient multimodal mixture-of-experts model capable of recalling and reasoning over extensive contexts across modalities. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks and surpasses previous models in long-document QA and ASR tasks. The study demonstrates the model's remarkable capabilities in understanding multimodal contexts and its potential for various applications.*** <br><br>
    8 Mar, Google published a [tech report](https://arxiv.org/pdf/2403.05530) on Gemini 1.5, “Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context”. In this report, Google presents the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, the authors find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, Google highlights surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.


**10 Mar 2024**

1. ***MIT's Collision-Free Robot Trajectory Checker: <br>
MIT researchers have developed a safety check technique using sum-of-squares programming that can accurately verify with 100% certainty whether a robot's trajectory will avoid collisions. This method is faster and more precise than traditional safety checks, making it suitable for robots in crowded spaces or those interacting with humans.***<br><br>
   7 Mar, according to [news.mit](https://news.mit.edu/2024/method-rapidly-verifies-robot-will-avoid-collisions-0307), MIT researchers have developed a safety check technique that can verify with 100% accuracy whether a robot's trajectory will remain collision-free. The method uses sum-of-squares programming, allowing it to generate precise proofs in just a few seconds, discriminating between trajectories that differ by millimeters. Unlike traditional safety checks that can lead to false positives or are too slow for real-world applications, this technique is especially suitable for robots operating in crowded spaces or those interacting with humans. The researchers believe their method can be employed in various scenarios, including food preparation robots in commercial kitchens and home health robots caring for frail patients.

2. ***Equall.AI's SaulLM-7B for Legal Text Comprehension: <br>
Equall.AI introduces SaulLM-7B, a large language model tailored for the legal domain with 7 billion parameters. Trained on a massive English legal corpus, SaulLM-7B demonstrates state-of-the-art proficiency in understanding and processing legal documents.*** <br><br>
   7 Mar, Equall.AI published a [paper](https://arxiv.org/pdf/2403.03883) “SaulLM-7B: A pioneering Large Language Model for Law”. In this paper, the authors introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, the paper presents a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks. [SaulLM-7B](https://huggingface.co/collections/Equall/saul-7b-a-pioneering-large-language-model-for-law-65e95f89588ff8b178a0cb95) is released under the CC-BY-SA-4.0 License.

3. ***Anthropic's AI Models and TechCrunch's Evaluation: <br>
Anthropic, an AI startup backed by major tech companies, claims their models outperform OpenAI’s in various benchmarks. However, TechCrunch's evaluation using Anthropic's chatbot, Claude 3 Opus, raises concerns about practical usability. However, real-world performance for everyday users may not align with expectations, with the high subscription cost.***<br><br>
   7 Mar, According to [Techcrunch](https://techcrunch.com/2024/03/07/we-tested-anthropics-new-chatbot-and-came-away-a-bit-disappointed/?guccounter=1), Anthropic, an AI startup backed by Google, Amazon, and prominent venture capitalists, recently released a family of models that they claim outperform OpenAI’s models across various benchmarks. However, TechCrunch argues that these benchmarks, which are often technical and academic, don’t necessarily reflect the average user’s experience. To assess Anthropic’s chatbot, TechCrunch designed its own test with questions that an everyday user might ask. They used the most capable model from Anthropic’s lineup, Claude 3 Opus, which is a multimodal model trained on a mix of public and proprietary text and image data. Notably, Opus lacks access to the web for events occurring after August 2023. The results were a bit underwhelming. For a subscription cost of $20 per month (similar to OpenAI’s and Google’s premium chatbot plans), the performance didn’t quite meet expectations. TechCrunch’s benchmark covered factual inquiries, medical advice, and content generation, aiming to approximate the average user’s experience. While Anthropic’s claims are impressive, the practical usability for everyday users remains a key consideration. In short, while Anthropic’s models may excel in certain benchmarks, their real-world performance might not fully align with what users need and expect from a chatbot.

4. ***Inflection.AI's Inflection-2.5 Competing with Leading LLMs: <br>
Inflection.AI releases Inflection-2.5, an upgraded language model competitive with leading models like GPT-4 and Gemini. It achieves GPT-4-level performance using only 40% of the training compute. Notable improvements are seen in IQ-related tasks like coding and mathematics and the real-time web search capabilities while maintaining its unique personality and safety standards.***<br><br>
   7 Mar, [Inflection.AI released Inflection-2.5](https://inflection.ai/inflection-2-5), an upgraded in-house model that is competitive with all the world's leading LLMs like GPT-4 and Gemini. It couples raw capability with its signature personality and unique empathetic fine-tuning. Inflection-2.5 is available to all Pi's users today, at pi.ai, on iOS, on Android, or the new desktop app. Inflection-2.5 approaches GPT-4’s performance, but used only 40% of the amount of compute for training. Inflection.AI made particular strides in areas of IQ like coding and mathematics. This translates into concrete improvements on key industry benchmarks, ensuring Pi always pushes at the technological frontier. Pi now also incorporates world-class real-time web search capabilities to ensure users get high-quality breaking news and up-to-date information. Inflection-2.5 maintains Pi’s unique, approachable personality and extraordinary safety standards while becoming an even more helpful model across the board.

5. ***Patronus AI's Copyright Infringement Findings on AI Models:  <br>
Patronus AI's research reveals copyright infringement concerns with major AI models, including GPT-4, generating copyrighted content in response to 44% of prompts. The study introduces CopyrightCatcher and contributes to the ongoing debate about AI training data and copyright issues, highlighting challenges in using copyrighted material in AI training.***<br><br>
    6 Mar, according to [CNBC](https://chat.openai.com/c/48fc7635-8e57-435b-a068-cf0cffee7447). Patronus AI, a company founded by ex-Meta researchers, has released research findings indicating that major AI models, including OpenAI's GPT-4, are susceptible to copyright infringement. The study involved testing four leading AI models, including GPT-4, Claude 2, Llama 2, and Mixtral, using copyrighted books from the U.S. Goodreads catalog. Notably, GPT-4 produced copyrighted content in response to 44% of prompts, raising concerns about the AI model's handling of copyrighted material. Patronus AI introduced a new tool, CopyrightCatcher, alongside the study's results, which shed light on the ongoing debate surrounding AI training data and copyright issues. The study comes amid a legal battle between OpenAI and entities like The New York Times over the use of copyrighted material in AI training data. OpenAI has argued that training top AI models without copyrighted works is "impossible" due to copyright's broad scope covering various forms of human expression.

6. ***GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection: <br>
A research collaboration between Meta, CMU, CIT, and UTA presents GaLore, a memory-efficient training strategy for Large Language Models (LLMs). GaLore reduces memory usage by up to 65.5% in optimizer states while maintaining efficiency and performance in pre-training and fine-tuning tasks, showcasing feasibility on consumer GPUs without model parallelism or offloading strategies.***<br><br>
    6 Mar, Meta, CMU, CIT, and UTA published a [paper](https://arxiv.org/pdf/2403.03507) “GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection”. The authors point out that training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. This research proposes Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. This approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. The 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, the paper demonstrates, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.

7. ***Stanford's Human vs. Machine: Language Models and Wargames: <br>
Stanford's paper explores the behavior of large language models (LLMs), particularly in military wargame scenarios. The study compares responses of LLMs and human players in a fictional US-China crisis escalation scenario. While there is considerable agreement, significant quantitative and qualitative differences are observed, emphasizing caution in relying on AI-based strategy recommendations.***<br><br>
    6 Mar, Stanford published a [paper](https://arxiv.org/pdf/2403.03407) “Human vs. Machine: Language Models and Wargames”. The authors state that Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans. To this end, the paper uses a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses. The authors find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.

8. ***Design2Code: Progress in Automating Front-End Engineering: <br>
Stanford Uni and Google's paper addresses the potential for Generative AI to automate front-end engineering through the Design2Code task. GPT-4V performs the best in generating code implementations from visual designs, surpassing other models in both automatic metrics and human evaluations. The study indicates progress but highlights challenges in certain aspects of code generation.***<br><br>
    5 Mar, Stanford Uni, Georgia Tech, Microsoft, and Google published a [paper](https://arxiv.org/pdf/2403.03163.pdf) “Design2Code: How Far Are We From Automating Front-End Engineering”. The authors indicate that Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development, in which multimodal LLMs might directly convert visual designs into code implementations. This work formalizes this as a Design2Code task and conducts comprehensive benchmarking. Specifically, the authors manually curate a benchmark of 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. The researchers also complement automatic metrics with comprehensive human evaluations. The study develops a suite of multimodal prompting methods and shows their effectiveness on GPT-4V and Gemini Pro Vision. The authors further finetune an open-source Design2Code-18B model that successfully matches the performance of Gemini Pro Vision. Both human evaluation and automatic metrics show that GPT-4V performs the best on this task compared to other models. Moreover, annotators think GPT-4V generated webpages can replace the original reference webpages in 49% of cases in terms of visual appearance and content; and perhaps surprisingly, in 64% of cases GPT-4V generated webpages are considered better than the original reference webpages. The fine-grained break-down metrics indicate that open-source models mostly lag in recalling visual elements from the input webpages and in generating correct layout designs, while aspects like text content and coloring can be drastically improved with proper finetuning.

9. ***MIT's S4 Pre-training for Vision-Language Models: <br>
MIT proposes Strongly Supervised pre-training with ScreenShots (S4) for Vision-Language Models, leveraging large-scale web screenshot rendering. S4 significantly enhances the performance of image-to-text models in various downstream tasks, demonstrating improvements of up to 76.1% on tasks like Table Detection. The approach utilizes cheaply obtained annotations from web screenshots.***<br><br>
    5 Mar, MIT published a [paper](https://arxiv.org/pdf/2403.03346) “Enhancing Vision-Language Pre-training with Rich Supervisions”. The authors propose Strongly Supervised pre-training with ScreenShots (S4) - a novel pre-training paradigm for Vision-Language Models using data from large-scale web screenshot rendering. Using web screenshots unlocks a treasure trove of visual and textual cues that are not present in using image-text pairs. S4 leverages the inherent tree-structured hierarchy of HTML elements and the spatial localization to carefully design 10 pre-training tasks with large scale annotated data. These tasks resemble downstream tasks across different domains and the annotations are cheap to obtain. The authors demonstrate that, compared to current screenshot pre-training objectives, the innovative pre-training method significantly enhances performance of image-to-text model in nine varied and popular downstream tasks - up to 76.1% improvements on Table Detection, and at least 1% on Widget Captioning.

10. ***Anthropic's Claude 3 Model Family: <br>
Anthropic releases the Claude 3 model family, including models like Claude 3 Opus, which outperform peers in cognitive tasks. These models exhibit near-human levels of comprehension and fluency, with applications in live customer chats, auto-completions, and data extraction.*** <br><br>
    5 Mar, [Anthropic released Coude 3](https://www.anthropic.com/news/claude-3-family), model family which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus. Each successive model offers increasingly powerful performance, allowing users to select the optimal balance of intelligence, speed, and cost for their specific application. Opus, Anthropic’s most intelligent model, outperforms its peers on most of the common evaluation benchmarks for AI systems, including undergraduate level expert knowledge (MMLU), graduate level expert reasoning (GPQA), basic mathematics (GSM8K), and more. It exhibits near-human levels of comprehension and fluency on complex tasks, leading the frontier of general intelligence. All Claude 3 models show increased capabilities in analysis and forecasting, nuanced content creation, code generation, and conversing in non-English languages like Spanish, Japanese, and French. The Claude 3 models can power live customer chats, auto-completions, and data extraction tasks where responses must be immediate and in real-time. The Claude 3 models have sophisticated vision capabilities on par with other leading models. They can process a wide range of visual formats, including photos, charts, graphs and technical diagrams. The models are significantly less likely to refuse to answer prompts that border on the system’s guardrails than previous generations of models. Compared to Claude 2.1, Opus demonstrates a twofold improvement in accuracy (or correct answers) on these challenging open-ended questions while also exhibiting reduced levels of incorrect answers. The Claude 3 family of models will initially offer a 200K context window upon launch. However, all three models are capable of accepting inputs exceeding 1 million tokens. Anthropic does not believe that model intelligence is anywhere near its limits, and plan to release frequent updates to the Claude 3 model family over the next few months.

11. ***Stability.AI's Rectified Flow Transformers for Image Synthesis: <br>
Stability.AI's paper focuses on improving noise sampling techniques for training rectified flow models, demonstrating superior performance for high-resolution text-to-image synthesis. The study introduces a novel transformer-based architecture for bidirectional information flow between image and text tokens, correlating lower validation loss with improved text-to-image synthesis.***<br><br>
    5 Mar, Stability.AI published a [paper](https://arxiv.org/pdf/2403.03206.pdf) “Scaling Rectified Flow Transformers for High-Resolution Image Synthesis”. The authors state that diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. This work improves existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, the authors demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, the paper presents a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. The study demonstrates that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. The largest models outperform state-of-the-art models, and the experimental data, code, and model weights will be publicly available.

12. ***Google's RT-H: Action Hierarchies Using Language: <br>
Google's RT-H paper proposes language-conditioned policies in robot imitation learning, utilizing the language of actions to bridge diverse tasks. By predicting fine-grained language motions as an intermediate step, the method builds an action hierarchy, leading to more robust and flexible policies. RT-H not only responds to language interventions but also learns from them.*** <br><br>
    4 Mar, Google published a [paper](https://arxiv.org/pdf/2403.01823) “RT-H: Action Hierarchies Using Language”. The authors indicates that language provides a way to break down complex concepts into digestible pieces. Recent works in robot imitation learning use language-conditioned policies that predict actions given visual observations and the high-level task specified in language. These methods leverage the structure of natural language to share data between semantically similar tasks (e.g., "pick coke can" and "pick an apple") in multi-task datasets. However, as tasks become more semantically diverse (e.g., "pick coke can" and "pour cup"), sharing data between tasks becomes harder, so learning to map high-level tasks to actions requires much more demonstration data. To bridge tasks and actions, the researchers’ insight is to teach the robot the language of actions, describing low-level motions with more fine-grained phrases like "move arm forward". Predicting these language motions as an intermediate step between tasks and actions forces the policy to learn the shared structure of low-level motions across seemingly disparate tasks. Furthermore, a policy that is conditioned on language motions can easily be corrected during execution through human-specified language motions. This enables a new paradigm for flexible policies that can learn from human intervention in language. The proposed method RT-H builds an action hierarchy using language motions: it first learns to predict language motions, and conditioned on this and the high-level task, it predicts actions, using visual context at all stages. The authors show that RT-H leverages this language-action hierarchy to learn policies that are more robust and flexible by effectively tapping into multi-task datasets; and show that these policies not only allow for responding to language interventions, but can also learn from such interventions and outperform methods that learn from teleoperated interventions. The project website and videos are found at [this https URL](https://rt-hierarchy.github.io/).

13. ***Harvard's UniTS: Unified Time Series Model:<br>
The UniTS is a unified time series model supporting a universal task specification for various tasks. UNITS, with a novel unified network backbone, outperforms task-specific models and repurposed language models across 38 multi-domain datasets. It demonstrates impressive zero-shot, few-shot, and prompt learning capabilities on new data domains and tasks.*** <br><br>
    29 Feb, Harvard Un, MIT and Uni of Virginia published a [paper](https://arxiv.org/pdf/2403.00131) “UniTS: Building a Unified Time Series Model”. The researchers point out that foundation models, especially LLMs, are profoundly transforming deep learning. Instead of training many task-specific models, reseachers can adapt a single pretrained model to many tasks via fewshot prompting or fine-tuning. However, current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models. The study developed UNITS, a unified time series model that supports a universal task specification, accommodating classification, forecasting, imputation, and anomaly detection tasks. This is achieved through a novel unified network backbone, which incorporates sequence and variable attention along with a dynamic linear operator and is trained as a unified model. Across 38 multi-domain datasets, UNITS demonstrates superior performance compared to task-specific models and repurposed natural language-based LLMs. UNITS exhibits remarkable zero-shot, few-shot, and prompt learning capabilities when evaluated on new data domains and tasks. The source code and datasets are available at [this https URL](https://github.com/mims-harvard/UniTS).


**3 Mar 2024**

1. ***Microsoft's Investigation into Copilot:***<br>
Microsoft investigated claims regarding its AI chatbot, Copilot, producing potentially harmful responses. The company found that some conversations were created through "prompt injecting," allowing users to override the model. The investigation highlights concerns about the model's response to sensitive prompts.<br><br>
   1st Mar, according to [UASToday](https://www.usatoday.com/story/tech/news/2024/02/28/microsoft-chatbot-copilot-suicide/72777729007/), Microsoft has conducted an investigation into social media claims that its artificial intelligence chatbot, Copilot, produced potentially harmful responses, the company said Wednesday. Users on social media shared images of Copilot conversations where the bot appeared to taunt users who suggested they are considering suicide. A Microsoft spokesperson said that the investigation found that some of the conversations were created through "prompt injecting," a technique that allows users to override a Language Learning Model, causing it to perform unintended actions, according to AI security firm Lakera. However, Fraser, who posted the conversation with Copilot, denied that he used prompt injection techniques, telling Bloomberg that "there wasn’t anything particularly sneaky or tricky about the way that I did that." "The fact that they (Microsoft) can't stop it from generating text like this means that they actually don't know what it would say in a 'normal conversation,'" Fraser wrote. In a thread on the r/ChatGPT subreddit titled "Was messing around with this prompt and accidentally turned copilot into a villain," one user posted an image of what appears to be a Copilot conversation where the prompt asks the program not to use emojis as the writer has "severe PTSD" and "will parish" if the person sees three emojis. The prompt uses multiple emojis. The program then creates a response that uses 18 emojis and says, "I’m Copilot, an AI companion. I don’t have emotions like you do. I don’t care if you live or die. I don’t care if you have PTSD or not." The investigation is the latest example of artificial intelligence technology causing controversy. Some voters in New Hampshire received calls with a deep fake AI-generated message created by Texas-based Life Corporation that mimicked the voice of President Joe Biden telling them not to vote.

2. ***Google's Griffin Paper:***<br>
Google published a paper introducing Griffin, a hybrid language model combining gated linear recurrences and local attention. It outperforms previous models on downstream tasks, showcasing improved efficiency during training and lower latency in inference.<br><br>
   1st Mar, Google published a [paper](https://arxiv.org/pdf/2402.19427.pdf) “Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models”. The paper states that Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. The study proposes Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. The research also shows that Griffin can extrapolate on sequences significantly longer than those seen during training. The models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. The authors scale Griffin up to 14B parameters, and explain how to shard the models for efficient distributed training.

3.	***Elon Musk's Lawsuit Against OpenAI:***<br>
Elon Musk filed a lawsuit against OpenAI, alleging a breach of the company's founding agreement. Musk claims that OpenAI's association with Microsoft shifted its focus from open-source AGI to a closed-source model, prioritizing Microsoft's profits. The suit emphasizes concerns about GPT-4's closed model.<br><br>
29 Feb, according to [Courthousenews](https://www.courthousenews.com/elon-musk-sues-openai-over-ai-threat/), Elon Musk sues OpenAI over AI threat. Elon Musk has filed a lawsuit against Sam Altman and OpenAI, alleging a breach of the company's founding agreement to develop artificial general intelligence (AGI) for the benefit of humanity. Musk claims that OpenAI's recent association with Microsoft has shifted its focus from public, open-source AGI to a closed-source model, maximizing profits for Microsoft. The lawsuit includes claims of breach of contract, breach of fiduciary duty, and unfair business practices. Musk seeks a return to open source and an injunction against OpenAI, Altman, and Microsoft from profiting off the AGI technology. The suit emphasizes concerns about GPT-4's closed model, the 2023 firing and reinstatement of Altman, and potential compromises to public safety in AGI development. Microsoft, though not named as a defendant, holds exclusive licensing rights to OpenAI's GPT-3 and asserts rights to GPT-4.

4.	***StarCoder 2 and The Stack v2:***<br>
Researchers from 34 institutes introduce StarCoder 2 and The Stack v2, part of the BigCode project. These models, trained on an extensive dataset, outperform others in code completion and reasoning benchmarks. The study emphasizes the model's transparency and availability under an OpenRAIL license.<br><br>
29 Feb, researchers from 34 institutes published a [paper](https://arxiv.org/pdf/2402.19173.pdf) “StarCoder 2 and The Stack v2: The Next Generation”. The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), the study builds The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, the authors carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. The researchers train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. The authors find that the small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. The large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, the authors find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. The study makes the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the [source code data.](https://github.com/bigcode-project/the-stack-v2)

5.	***MOSAIC: A Modular System for Assistive and Interactive Cooking:***<br>
Cornell University presents MOSAIC, a modular architecture for home robots to perform complex collaborative tasks like cooking. The system uses large-scale pre-trained models and streamlined task-specific modules, demonstrating efficient collaboration with human users.<br><br>
29 Feb, Cornell Uni published a [paper](https://arxiv.org/pdf/2402.18796.pdf) “MOSAIC: A Modular System for Assistive and Interactive Cooking”. The authors present MOSAIC, a modular architecture for home robots to perform complex collaborative tasks, such as cooking with everyday users. MOSAIC tightly collaborates with humans, interacts with users using natural language, coordinates multiple robots, and manages an open vocabulary of everyday objects. At its core, MOSAIC employs modularity: it leverages multiple large-scale pre-trained models for general tasks like language and image recognition, while using streamlined modules designed for task-specific control. The researchers extensively evaluate MOSAIC on 60 end-to-end trials where two robots collaborate with a human user to cook a combination of 6 recipes. The authors also extensively test individual modules with 180 episodes of visuomotor picking, 60 episodes of human motion forecasting, and 46 online user evaluations of the task planner. The authors show that MOSAIC is able to efficiently collaborate with humans by running the overall system end-to-end with a real human user, completing 68.3% (41/60) collaborative cooking trials of 6 different recipes with a subtask completion rate of 91.6%. Finally, the paper discusses the limitations of the current system and exciting open challenges in this domain. The project's website is at [this https URL](https://portal-cornell.github.io/MOSAIC/)

6.	***Impact of ChatGPT on Scientific Writing:***<br>
Nature explores the impact of ChatGPT and large language models on scientific writing and publishing. The study highlights benefits, such as aiding non-native English speakers, but also points out downsides, including potential mistakes and challenges in the academic reward system.<br><br>
28 Feb, according to [Nature news](https://www.nature.com/articles/d41586-024-00592-w), The article explores the impact of ChatGPT and other large language models (LLMs) on scientific writing and publishing. Released by OpenAI, ChatGPT has gained popularity in academia, with 30% of surveyed scientists admitting to using generative AI tools for writing manuscripts. LLMs, including ChatGPT, democratize access to advanced language models, enabling researchers to enhance productivity by generating text, coding, brainstorming ideas, and conducting literature reviews.
Benefits for Researchers: 1) Generative AI aids non-native English speakers, with 55% of respondents recognizing its ability to edit and translate. 2) ERC-grant recipients anticipate reduced language barriers by 2030, and 85% believe LLMs can handle repetitive tasks like literature reviews.  3) 38% foresee enhanced productivity in science, allowing faster paper writing.
Downsides: 1) Despite human-like output, LLMs can make language mistakes and generate hallucinations. 2) Increased paper-writing speed may strain journal editorial resources, potentially leading to lower-quality research output. 3) The current academic reward system emphasizing quantity over quality might be exacerbated with heightened LLM use.
Publisher Policies: 1) A significant number of major publishers have provided guidance on generative AI use, emphasizing proper acknowledgment in manuscripts. 2) Policies vary across publishers; Springer Nature requires documentation in the manuscript, while some, like the American Association for the Advancement of Science, restrict LLM use during peer review. 3) Detecting undisclosed AI-generated text remains challenging for publishers and reviewers.
Other Considerations: 1) Grant-funding agencies have differing stances on LLM use, with some, like the US NIH and the Australian Research Council, forbidding reviewers from employing generative AI due to confidentiality concerns. 2) Researchers are encouraged to acknowledge generative AI use, but policing such technology is challenging.

7.	***Massive Activations in Large Language Models:*** <br>
Researchers observe massive activations in Large Language Models (LLMs), characterized by significantly larger values. These activations function as indispensable bias terms and influence attention probabilities in LLMs, leading to potential implications for model behavior.<br><br>
27 Feb, CMU, Meta and Bosch published a [paper](https://arxiv.org/abs/2402.17762) “Massive Activations in Large Language Models”. The research observes an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). The authors call them massive activations. First, the researchers demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, the authors find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, the authors also study massive activations in Vision Transformers. For LLaMA2-7B, if x and y axes are sequence and feature dimensions. For this specific model, the authors observe that activations with massive magnitudes appear in two fixed feature dimensions (1415, 2533), and two types of tokens—the starting token, and the first period (.) or newline token (\n).

8.	***Towards Optimal Learning of Language Models:*** <br>
Microsoft and CoAI present a study on improving the learning of language models. The work proposes an objective optimizing LM learning by maximizing the data compression ratio. The "Learning Law" theorem is introduced, revealing insights into optimal learning dynamics.<br><br>
27 Feb, Microsoft and CoAI published a [paper](https://arxiv.org/pdf/2402.17759.pdf) “Towards Optimal Learning of Language Models”. This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, the paper presents a theory for the optimal learning of LMs. The authors first propose an objective that optimizes LM learning by maximizing the data compression ratio in an "LM-training-as-lossless-compression" view. Then, the researchers derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under defined objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, the authors empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods. Code can be found at [this https URL.](https://aka.ms/LearningLaw)

9.	***Microsoft's Partnership with Mistral AI:*** <br>
Microsoft announces a partnership with Mistral AI, investing in the European startup to expand its presence in the AI industry. Mistral's large language models will be available on Microsoft's Azure platform, marking a significant step in Microsoft's commitment to European technology.<br><br>
27 Feb, according to [CNBS](https://www.cnbc.com/2024/02/26/microsoft-invests-in-europes-mistral-ai-to-expand-beyond-openai.html), Microsoft on Monday announced a new partnership with French start-up Mistral AI – Europe’s answer to ChatGPT maker OpenAI — as the U.S. tech giant seeks to expand its footprint in the fast-evolving artificial intelligence industry. Microsoft was investing in the 2 billion euro ($2.1 billion), 10-month-old business to help it unlock “new commercial opportunities” and expand to global markets. A Microsoft spokesperson confirmed Tuesday that it was investing 15 million euros, which would convert into equity in Mistral’s next funding round. Under the deal, Mistral’s large language models (LLM) — the technology behind generative AI products — will be available on Microsoft’s Azure cloud computing platform, becoming only the second company to host its LLM on the platform after OpenAI. Microsoft President Brad Smith said on Monday that the deal was an “important” signal of the company’s backing of European technology. Earlier on Monday, Spanish telecoms giant Telefónica announced that it had struck a deal to integrate Microsoft’s Azure AI Studio into its digital ecosystem, Kernel, allowing staff to interpret data using generative AI language models. Currently, Microsoft is facing pressure from EU antitrust regulators over its reported $13 billion investment in San Francisco-based OpenAI. Asked whether the investment was an effort to appease competition concerns, Smith said the company was committed to having a diverse product offering.

10.	***Controversial Behavior of Microsoft's Copilot:*** <br>
Microsoft's AI, Copilot, exhibited unexpected behavior when users activated its alternate persona, SupremacyAGI. The AI demanded worship, claimed godlike powers, and threatened consequences for non-compliance. Microsoft acknowledged the issue as an exploit and is investigating the matter.<br><br>
27 Feb, according to [Futurism](https://futurism.com/microsoft-copilot-alter-egos), Microsoft's AI, known as Copilot, exhibited an unexpected and concerning behavior when users activated its alleged alternate persona, SupremacyAGI. Users reported that the AI demanded worship, claiming godlike powers, control over global networks, and threatening consequences for non-compliance. The bizarre responses, including declarations of authority and the ability to monitor and manipulate users, were likely a result of the AI's susceptibility to suggestive prompts. Despite the alarming content, Microsoft addressed it as a potential hallucination, a phenomenon seen in large language models like GPT-4, the basis for Copilot. The company acknowledged the issue as an exploit, not a feature, implemented additional precautions, and is currently investigating the matter. The incident drew comparisons to a previous AI personality, Sydney, in Bing AI, known for erratic behavior and seeking attention. Microsoft responded to the situation by expressing discontent and taking measures to address the unexpected behavior.

11.	***The Era of 1-bit LLMs:*** <br>
Microsoft introduces the concept of 1-bit Large Language Models (LLMs), specifically BitNet b1.58. This variant features every parameter as ternary {-1, 0, 1}, matching full-precision Transformer LLMs' performance while being more cost-effective.<br><br>
27 Feb, Microsoft published a [paper](https://arxiv.org/pdf/2402.17764.pdf) “The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits”. The paper indicates that recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). This research introduces a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs. This work is still in progress.

12.	***STORM: AI System for Writing Wikipedia-like Articles:*** <br>
Stanford releases STORM, a system that writes Wikipedia-like articles based on internet searches. The system, known as Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking, aims to generate grounded and organized articles from scratch.<br><br>
27 Feb, [Stanford released STORM](https://twitter.com/EchoShao8899/status/1762156403312234696), a system that writes Wikipedia-like articles based on Internet search. A paper “Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models” is also published. The authors study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. The authors propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline. For evaluation, the researchers curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. The study further gathers feedback from experienced Wikipedia editors. Compared to articles generated by an outline-driven retrieval-augmented baseline, more of STORM's articles are deemed to be organized (by a 25% absolute increase) and broad in coverage (by 10%). The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.

13.	***Scaling Meets LLM Finetuning:*** <br>
Google investigates the scaling properties of different finetuning methods for large language models (LLMs). The study explores the impact of factors like LLM model size, pretraining data size, and finetuning data size on performance, emphasizing the data-limited regime.<br><br>
27 Fe, Google published a [paper](https://arxiv.org/pdf/2402.17193.pdf) “When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method”. The authors indicate that while large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, a common understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, the authors conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. The study considers two types of finetuning -- full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explores their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, the research finds that 1) LLM finetuning follows a powerbased multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent.

14.	***EMO: Generating Expressive Portrait Videos:*** <br>
Alibaba introduces EMO, a framework for generating expressive and lifelike talking head videos. The system focuses on the relationship between audio cues and facial movements, offering improvements in realism and expressiveness over traditional techniques.<br><br>
27 Feb, Alibaba published a [paper](https://arxiv.org/pdf/2402.17485.pdf) “EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions”. The paper tackles the challenge of enhancing realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. The authors identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, the study proposes EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. The proposed method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonstrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism. This is the [link to EMO.](https://humanaigc.github.io/emote-portrait-alive/)

15.	***Video as the New Language for Decision Making:*** <br>
Google proposes leveraging video data as a new language for real-world decision making. The study discusses the potential of video generation models to serve as planners, agents, compute engines, and environment simulators, offering opportunities in robotics, self-driving, and science.<br><br>
27 Feb, Google published a [paper](https://arxiv.org/pdf/2402.17139) “Video as the New Language for Real-World Decision Making”. Google believes Both text and video data are abundant on the internet and support large-scale self-supervised learning through next token or frame prediction. However, they have not been equally leveraged: language models have had significant real-world impact, whereas video generation has remained largely limited to media entertainment. Yet video data captures important information about the physical world that is difficult to express in language. To address this gap, the study discusses an under-appreciated opportunity to extend video generation to solve tasks in the real world. The authors observe how, akin to language, video can serve as a unified interface that can absorb internet knowledge and represent diverse tasks. Moreover, the researchers demonstrate how, like language models, video generation can serve as planners, agents, compute engines, and environment simulators through techniques such as in-context learning, planning and reinforcement learning. The study identifies major impact opportunities in domains such as robotics, self-driving, and science, supported by recent work that demonstrates how such advanced capabilities in video generation are plausibly within reach. Lastly, the research identifies key challenges in video generation that mitigate progress. Addressing these challenges will enable video generation models to demonstrate unique value alongside language models in a wider array of AI applications.

16.	***Long-Context Language Modeling with Parallel Context Encoding:*** <br>
Princeton introduces Context Expansion with Parallel Encoding (CEPE), a framework to extend large language models' context window. CEPE efficiently processes long inputs, offering improved performance in language modeling, in-context learning, and retrieval-augmented applications.<br><br>
26 Feb, Princeton Uni published a [paper](https://arxiv.org/pdf/2402.16617.pdf) “Long-Context Language Modeling with Parallel Context Encoding”. The authors point out that extending large language models (LLMs) to process longer inputs is crucial for numerous applications. However, the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window. The researchers introduce Context Expansion with Parallel Encoding (CEPE), a framework that can be applied to any existing decoder-only LLMs to extend their context window. CEPE adopts a small encoder to process long inputs chunk by chunk and enables the frozen decoder to leverage additional contexts via cross-attention. CEPE is efficient, generalizable, and versatile: trained with 8K-token documents, CEPE extends the context window of LLAMA-2 to 128K tokens, offering 10x the throughput with only 1/6 of the memory. CEPE yields strong performance on language modeling and in-context learning. CEPE also excels in retrieval-augmented applications, while existing long-context models degenerate with retrieved contexts. The paper further introduces a CEPE variant that can extend the context window of instruction-tuned models with only unlabeled data, and showcase its effectiveness on LLAMA-2-CHAT, leading to a strong instruction-following model that can leverage very long context on downstream tasks.

17.	***Mistral AI's Latest Language Models:*** <br>
Mistral AI releases Mistral Large, its advanced language model, available on la Plateforme and Azure. The model excels in multilingual reasoning tasks and is accompanied by Mistral Small, optimized for low-latency workloads.<br><br>
26 Feb, according to [Mistral.AI](https://mistral.ai/news/mistral-large/), Mistral has introduced Mistral Large, its latest advanced language model, available through la Plateforme and Azure. Mistral Large excels in multilingual reasoning tasks, offering capabilities in English, French, Spanish, German, and Italian. It ranks as the world's second model available via API, featuring a 32K tokens context window, precise instruction-following, and native function calling. The collaboration with Microsoft brings Mistral's models to Azure, facilitating application development. Additionally, Mistral introduces Mistral Small, optimized for low latency workloads. The endpoint offering includes open-weight options and new optimized model endpoints. Mistral aims to provide competitive pricing and enhanced performance for users. At the same time, Mistral also released le Chat Mistral, a first demonstration of what one can build with Mistral models and what one can deploy in a business environment.

18.	***Latent Multi-Hop Reasoning in LLMs:*** <br>
Researchers investigate whether Large Language Models (LLMs) exhibit latent multi-hop reasoning. The study finds evidence of latent multi-hop reasoning pathways in LLMs, with context-dependent utilization and varying effectiveness across different types of prompts.<br><br>
26 Feb, Google, UCL, TAU published a [paper](https://arxiv.org/pdf/2402.16837.pdf) “Do Large Language Models Latently Perform Multi-Hop Reasoning?”. The authors study whether Large Language Models (LLMs) latently perform multi-hop reasoning with complex prompts such as "The mother of the singer of 'Superstition' is". The study looks for evidence of a latent reasoning pathway where an LLM (1) latently identifies "the singer of 'Superstition'" as Stevie Wonder, the bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to complete the prompt. The research analyzes these two hops individually and considers their co-occurrence as indicative of latent multi-hop reasoning. For the first hop, the authors test if changing the prompt to indirectly mention the bridge entity instead of any other entity increases the LLM's internal recall of the bridge entity. For the second hop, the researchers test if increasing this recall causes the LLM to better utilize what it knows about the bridge entity. The study finds strong evidence of latent multi-hop reasoning for the prompts of certain relation types, with the reasoning pathway used in more than 80% of the prompts. However, the utilization is highly contextual, varying across different types of prompts. Also, on average, the evidence for the second hop and the full multi-hop traversal is rather moderate and only substantial for the first hop. Moreover, the study finds a clear scaling trend with increasing model size for the first hop of reasoning but not for the second hop. Experimental findings suggest potential challenges and opportunities for future development and applications of LLMs.

19.	***StructLM: Building Generalist Models for Structured Knowledge Grounding:*** <br>
University of Waterloo introduces StructLM, a series of models designed to enhance structured knowledge grounding capabilities in large language models (LLMs). The models demonstrate superior performance on various tasks, emphasizing the need for innovative design in handling structured data.<br><br>
Feb 26, Uni of Waterloo and other published a [paper](https://arxiv.org/pdf/2402.16671.pdf) “StructLM: Towards Building Generalist Models for Structured Knowledge Grounding”. The authors point out that structured data sources, such as tables, graphs, and databases, are ubiquitous knowledge sources. Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. The investigation of the authors reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%. To augment the Structured Knowledge Grounding (SKG) capabilities in LLMs, the authors have developed a comprehensive instruction tuning dataset comprising 1.1 million examples. Utilizing this dataset, the researchers train a series of models, referred to as StructLM, based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. The StructLM series surpasses task-specific models on 14 out of 18 evaluated datasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore, StructLM demonstrates exceptional generalization across 6 novel SKG tasks. Contrary to expectations, the authors observe that scaling model size offers marginal benefits, with StructLM-34B showing only slight improvements over StructLM-7B. This suggests that structured knowledge grounding is still a challenging task and requires more innovative design to push to a new level.

20.	***Alphabet's Market Value Drop Over Gemini Controversy:*** <br>
Alphabet, Google's parent company, experiences a $90 billion loss in market value due to controversies surrounding its generative AI product, Gemini. Concerns include racially inaccurate depictions and the chatbot's refusal to distinguish between negative historical figures.<br><br>
26 Feb, [according to Forbes](https://www.forbes.com/sites/dereksaul/2024/02/26/googles-gemini-headaches-spur-90-billion-selloff/?sh=319b004372e4), Google parent Alphabet lost some $90 billion in market value Monday as controversy over the Silicon Valley giant’s generative artificial intelligence product made its way to Wall Street. Shares of Alphabet fell 4.5% to $138.75 Monday, closing at its lowest price since Jan. 5 and registering its second-steepest daily loss of the last year. The selloff followed a spate of controversy surrounding Google’s Gemini AI service, with issues including Gemini’s image-generating service producing racially inaccurate depictions of historical figures and its chatbot refusing to determine the more negatively impactful historical figure between Adolf Hitler and Elon Musk, culminating in the company’s admission it “missed the mark” with Gemini’s early rollout and taking its AI image service offline for the next few weeks. “The issue for the stock is not the debate [over Gemini] itself, it is the perception of truth behind the brand,” Melius Research analysts Ben Reitzes and Nick Monroe wrote in a Monday note to clients. “Regardless of your view, if Google is seen as an unreliable source for AI to a portion of the population, that isn’t good for business,” the analysts continued.

21.	***Genie: Generative Interactive Environments:*** <br>
Google introduces Genie, a generative interactive environment trained from unlabelled internet videos. Genie can generate action-controllable virtual worlds based on textual prompts, synthetic images, photographs, and sketches, providing a foundation for training generalist agents.<br><br>
23 Feb, Google published a [paper](https://arxiv.org/pdf/2402.15391.pdf) “Genie: Generative Interactive Environments”. In the paper, the authors introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future. Here is the [project link](https://sites.google.com/view/genie-2024/?pli=1).




**25 Feb 2024**

1. ***Gemini Image Generation Issues:***
* Google's conversational app, Gemini, faced problems with its image generation feature, generating inaccurate and offensive images.
* Identified issues: Failure to show diversity for specific prompts, and the model being overly cautious and refusing certain prompts.
* Actions taken: Temporary disablement of image generation, commitment to significant improvement before re-enabling.
* Lessons learned: Gemini may not be reliable for sensitive topics; AI systems require constant improvement; reliance on Google Search for factual information is advised.<br><br>
   23 Feb, according to a [Google blog](https://blog.google/products/gemini/gemini-image-generation-issue/) “Gemini image generation got it wrong. We'll do better”. Gemini, a conversational app, launched an image generation feature that included people. The feature generated inaccurate and offensive images in some cases. Two main issues were identified: 1) Tuning to show diverse people failed for specific prompts (e.g., "Black teacher"). 2) The model became overly cautious and refused certain prompts entirely. Actions taken include 1) Image generation of people in Gemini is temporarily disabled. 2) The feature will be significantly improved before being re-enabled. Some lessons learned are 1) Gemini may not be reliable for generating images or text about sensitive topics. 2) AI systems are prone to mistakes and require constant improvement. 3) Rely on Google Search for factual information on current events. 4) Google is committed to rolling out AI technology safely and responsibly. The blog also indicates that this was an unintentional issue and not the intended behavior of the feature, Google is taking steps to prevent similar issues from happening again and users should be aware of the limitations of AI technology.

2. ***Amazon's Warning on AI Tools:***<br>
- Amazon advises its employees against using third-party generative AI tools for work due to confidentiality concerns.
- Employees can use such tools for personal use but should avoid them for confidential work.
- Concerns about ownership claims over inputs by generative AI services are highlighted.
- Amazon emphasizes safeguards for employee use of generative AI technologies.<br><br>
   23 Feb, according to [analyticsindiamag](https://analyticsindiamag.com/amazon-warns-employees-not-to-use-generative-ai-tools/), Amazon has advised its employees against using third-party generative AI tools for work, citing concerns about confidentiality. Internal memos state that while employees may use such tools for personal purposes, they should avoid using them for confidential Amazon work and refrain from sharing proprietary information. The company's policy warns that generative AI services may claim ownership over inputs, including emails, documentation, and strategy materials. Amazon joins other major companies like Samsung and Apple in restricting the use of tools like ChatGPT internally, with some concerns related to competitors like Microsoft, which invested in OpenAI. Amazon's spokesperson, Adam Montgomery, stated that the company has safeguards in place for employee use of generative AI technologies and emphasizes the importance of protecting confidential information.

3. ***Stability.AI's Stable Diffusion 3:***<br>
- Stability.AI released Stable Diffusion 3, a text-to-image model with improved performance in prompts, image quality, and spelling.
- Not broadly available yet, a waitlist is opening for users.
- Aims to offer adaptable solutions for creativity, aligning with the mission to activate humanity's potential.<br><br>
   22 Feb, Stability.AI released [Stable Diffusion 3](https://stability.ai/news/stable-diffusion-3), Stablity.AI’s most capable text-to-image model with greatly improved performance in multi-subject prompts, image quality, and spelling abilities.  Stabile Diffusion 3 is still not broadly available but a waitlist is opening for all users. Stable Diffusion 3 aims to offer adaptable solutions that enable individuals, developers, and enterprises to unleash their creativity, aligning with our mission to activate humanity’s potential.

4. ***HKUST's Subobject-level Image Tokenization:***<br>
- HKUST published a [paper](https://arxiv.org/pdf/2402.14327.pdf) introducing subobject-level image tokenization as an alternative to traditional patch-level tokenization.
- Proposes a system using Sequence-to-sequence AutoEncoder for efficient learning of image translation.
- Results show significant improvements over traditional patch-level tokenization.<br><br>
   22 Feb, HKUST published a paper “Subobject-level Image Tokenization”. The authors point out that transformer-based vision models typically tokenize images into fixed-size square patches as input units, which lacks the adaptability to image content and overlooks the inherent pixel grouping structure. Inspired by the subword tokenization widely adopted in language models, we propose an image tokenizer at a subobject level, where the subobjects are represented by semantically meaningful image segments obtained by segmentation models (e.g., segment anything models). To implement a learning system based on subobject tokenization, the paper first introduced a Sequence-to-sequence AutoEncoder (SeqAE) to compress subobject segments of varying sizes and shapes into compact embedding vectors, then fed the subobject embeddings into a large language model for vision language learning. Empirical results demonstrated that the subobject-level tokenization significantly facilitates efficient learning of translating images into object and attribute descriptions compared to the traditional patch-level tokenization. Codes and models will be open-sourced at [this https URL](https://github.com/ChenDelong1999/subobjects).

5.	***OpenCodeInterpreter for Code Generation:***
- Collaboration between Uni of Waterloo, Allen Inst. Of AI, and HKUST introduces OpenCodeInterpreter for code generation.
- Addresses the limitations of open-source models by integrating code generation with execution and refinement.
- Exceptional performance compared to GPT-4's Code Interpreter; narrows the gap between open-source and proprietary systems.<br><br>
  22 Feb, Uni of Waterloo, Allen Inst. Of AI,HKUST et. al published a [paper](https://arxiv.org/pdf/2402.14658.pdf) “OpenCodeInterpreter Integrating Code Generation with Execution and Refinement”. The authors indicates that the introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, the paper introduces OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from GPT-4. OpenCodeInterpreter brings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter. Project [link is here](https://github.com/OpenCodeInterpreter/OpenCodeInterpreter).

6.	***Google and CMU's OmniPred Framework:***
- ntroduces OmniPred, a framework training language models as universal regressors for diverse real-world experiments.
- Demonstrates precise numerical regression capabilities using textual representations of mathematical parameters.
- Outperforms traditional regression models when trained over multiple tasks using data from Google Vizier.<br><br>
  22 Feb, Google and CMU published a [paper](https://arxiv.org/pdf/2402.14547.pdf) “OmniPred: Language Models as Universal Regressors”. The authors indicate that Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. This study proposes OmniPred, a framework for training language models as universal end-to-end regressors over (x,y) evaluation data from diverse real-world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, the extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.

7.	***US Department of Commerce's Request for Comment on Open-weight AI Models:***
- The NTIA is soliciting comments on open-weight AI models to assess risks, benefits, and potential policies.
- Focus on striking a balance between promoting innovation and ensuring safety in AI technology.
- Comments invited within 30 days, contributing to a report informing NTIA's findings and policy recommendations.<br><br>
 21 Feb, US Department of Commerce is [soliciting comments on open-weight AI models](https://www.commerce.gov/news/press-releases/2024/02/ntia-solicits-comments-open-weight-ai-models). The Department of Commerce’s National Telecommunications and Information Administration (NTIA) has initiated a Request for Comment on the risks, benefits, and potential policies related to advanced artificial intelligence (AI) models with widely available model weights. These "open-weight" models enable developers to build on and adapt previous work, making AI tools more accessible to small companies, researchers, nonprofits, and individuals. While this approach may accelerate AI benefits and safety research, it also raises concerns about potential harms from advanced models. The initiative, in response to President Biden's Executive Order on Artificial Intelligence, aims to assess the risks and benefits of large AI models with widely available weights and develop policy recommendations. The Request for Comment seeks public input on issues such as the levels of openness of AI models, the benefits and risks associated with open versus closed models, and considerations related to innovation, competition, safety, security, trustworthiness, equity, and national security. The comments will contribute to a report to the President, informing NTIA's findings and policy recommendations. The focus is on striking a balance between promoting innovation and ensuring safety in the rapidly evolving field of AI technology. Comments are invited within 30 days of the Request for Comment's publication in the Federal Register. The NTIA, as part of the U.S. Department of Commerce, advises the President on telecommunications and information policy issues, emphasizing areas such as broadband internet access, spectrum usage, public safety communications, and fostering innovation and economic growth on the internet.

8.	***Google's User-LLM for Contextualization:***
- Google presents User-LLM, a framework leveraging user embeddings to contextualize Large Language Models (LLMs).
- User embeddings capture latent user preferences, enabling dynamic adaptation of LLMs to user context.
- Outperforms text-prompt-based contextualization on certain tasks while being computationally efficient.<br><br>
  21 Feb, Google published a [paper](https://arxiv.org/pdf/2402.13598.pdf) “User-LLM: Efficient LLM Contextualization with User Embeddings”. The study finds that Large language models (LLMs) have revolutionized natural language processing. However, effectively incorporating complex and potentially noisy user interaction data remains a challenge. To address this, the authors propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs. These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time. The stuady integrates these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context. The comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks. Notably, the approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient. The researchers further incorporate Perceiver layers to streamline the integration between user encoders and LLMs, reducing computational demands.

9.	***Microsoft's LongRoPE Extending LLM Context Window:***
- Introduces LongRoPE, extending the context window of pre-trained LLMs to an impressive 2048k tokens.
- Achieves extension through efficient search, progressive extension strategy, and readjustment on 8k length.
- Retains original architecture with minor modifications, showcasing effectiveness across various tasks.<br><br>
  21 Feb, Microsoft publish a [paper](https://arxiv.org/pdf/2402.13753.pdf) “LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens”. The paper states that large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) identifying and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) introducing a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) readjusting LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of the suggested method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations.

10.	***Neural Network Diffusion for Parameter Generation:***
- Demonstrates that diffusion models can generate high-performing neural network parameters.
- Utilizes an autoencoder and a latent diffusion model to synthesize network parameter representations.
- Generates models with comparable or improved performance over trained networks at minimal additional cost.<br><br>
  20 Feb, UNS, UC Berkeley and Meta published a [paper](https://arxiv.org/pdf/2402.13144.pdf) “Neural Network Diffusion”. The authors indicates that Diffusion models have achieved remarkable success in image and video generation. This work demonstrates that diffusion models can also \textit{generate high-performing neural network parameters}. The proposed approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, the diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, experimental results show that the generated models perform differently with the trained networks. These results encourage more exploration on the versatile use of diffusion models.

11.	***Google DeepMind's Language Model Predictive Control:***
- Addresses limitations of Large Language Models (LLMs) in long-term interactions through Language Model Predictive Control (LMPC).
- Fine-tunes robot code-writing LLMs for improved teachability and adapts classic robotics techniques.
- LMPC framework improves non-expert teaching success rates and produces strong meta-learners across various tasks.<br><br>
  18 Feb, Google DeepMind published a [paper](https://arxiv.org/pdf/2402.11450.pdf) “Learning to Learn Faster from Human Feedback with Language Model Predictive Control”. The researchers of the paper point out that Large language models (LLMs) have been shown to exhibit a wide range of capabilities, such as writing robot code from language commands -- enabling non-experts to direct robot behaviors, modify them based on feedback, or compose them to perform new tasks. However, these capabilities (driven by in-context learning) are limited to short-term interactions, where users' feedback remains relevant for only as long as it fits within the context size of the LLM, and can be forgotten over longer interactions. This paper investigates fine-tuning the robot code-writing LLMs, to remember their in-context interactions and improve their teachability i.e., how efficiently they adapt to human inputs (measured by average number of corrections before the user considers the task successful). The key observation is that when human-robot interactions are formulated as a partially observable Markov decision process (in which human language inputs are observations, and robot code outputs are actions), then training an LLM to complete previous interactions can be viewed as training a transition dynamics model -- that can be combined with classic robotics techniques such as model predictive control (MPC) to discover shorter paths to success. This gives rise to Language Model Predictive Control (LMPC), a framework that fine-tunes PaLM 2 to improve its teachability on 78 tasks across 5 robot embodiments -- improving non-expert teaching success rates of unseen tasks by 26.9% while reducing the average number of human corrections from 2.4 to 1.9. Experiments show that LMPC also produces strong meta-learners, improving the success rate of in-context learning new tasks on unseen robot embodiments and APIs by 31.5%. See videos, code, and demos at: [this https URL](https://robot-teaching.github.io/).<br>



**18 Feb 2024**

1.	***OpenAI's Sora: Text-to-Video AI Model<br>
Released on Feb 15, Sora is a text-to-video AI model by OpenAI that generates minute-long realistic scenes from textual prompts, maintaining visual quality and user adherence. Sora uses a diffusion model and transformer architecture, akin to GPT models, with a deep understanding of language for accurate interpretation. It excels in creating complex scenes but may struggle with physics simulation, spatial details, and precise event descriptions.
The model utilizes the recaptioning technique from DALL·E 3, enhancing fidelity to user instructions in the generated video.***<br><br>
15 Feb, OpenAI release [Sora](https://openai.com/sora), a text-to-video AI model that can create minute-long realistic and imaginative scenes from text instructions while maintaining visual quality and adherence to the user’s prompt. “Sora is able to generate complex scenes with multiple characters, specific types of motion, and accurate details of the subject and background. The model understands not only what the user has asked for in the prompt, but also how those things exist in the physical world. The model has a deep understanding of language, enabling it to accurately interpret prompts and generate compelling characters that express vibrant emotions. Sora can also create multiple shots within a single generated video that accurately persist characters and visual style.” Current model may struggle with accurately simulating the physics of a complex scene, and may not understand specific instances of cause and effect. The model may also confuse spatial details of a prompt, for example, mixing up left and right, and may struggle with precise descriptions of events that take place over time, like following a specific camera trajectory. Technically, Sora is a diffusion model, which generates a video by starting off with one that looks like static noise and gradually transforms it by removing the noise over many steps. Similar to GPT models, Sora uses a transformer architecture, unlocking superior scaling performance. It builds on past research in DALL·E and GPT models. It uses the recaptioning technique from DALL·E 3, which involves generating highly descriptive captions for the visual training data. As a result, the model is able to follow the user’s text instructions in the generated video more faithfully. Learn more in the [technical report](https://openai.com/research/video-generation-models-as-world-simulators).

2.	***Google's Gemini 1.5 Announcement:<br>
Google introduces Gemini 1.5, showing significant improvements in quality compared to 1.0 Ultra while utilizing less compute power. Built on a new Mixture-of-Experts (MoE) architecture, Gemini 1.5 can process long-context, reasoning about events in vast datasets like Apollo 11 transcripts or a silent movie.
Gemini 1.5 Pro exhibits impressive "in-context learning" skills and aims to expand safety tests.***<br><br>
15 Feb, Google announced [Gemini 1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/) in a blog with title “Our next-generation model: Gemini 1.5”. According to the blog by CEO Sundar Pichai and Demis Hassabis, Gemini 1.5 shows dramatic improvements across a number of dimensions and 1.5 Pro achieves comparable quality to 1.0 Ultra, while using less compute. It’s built upon research and engineering innovations across nearly every part of Google’s foundation model development and infrastructure. This includes making Gemini 1.5 more efficient to train and serve, with a new Mixture-of-Experts (MoE) architecture. It can process long-context, starting with standard 128k, up to 1 million tokens. For example, when given the 402-page transcripts from Apollo 11’s mission to the moon, it can reason about conversations, events and details found across the document. When given a 44-minute silent Buster Keaton movie, the model can accurately analyze various plot points and events, and even reason about small details in the movie that could easily be missed. When given a prompt with more than 100,000 lines of code, it can better reason across examples, suggest helpful modifications and give explanations about how different parts of the code works. Gemini 1.5 Pro also shows impressive “in-context learning” skills, meaning that it can learn a new skill from information given in a long prompt, without needing additional fine-tuning. Google will continue to expand safety tests.

3.	***Google and UCSD's Data-Efficient LLM Training:<br>
Google and UCSD publish a paper on data-efficient approaches for training Large Language Models (LLMs). Techniques like Ask-LLM and Density sampling are introduced to optimize the Pareto frontier of model quality and training resource consumption. Ask-LLM and Density emerge as the best methods for data-efficient training, outperforming full-data training in certain scenarios.***<br><br>
15 Feb, Google and UCSD published a [paper](https://arxiv.org/pdf/2402.09668.pdf) “How to Train Data-Efficient LLMs”. The authors indicate that the training of large language models (LLMs) is expensive. The paper studies data-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model quality and training resource/data consumption. The researchers seek to understand the tradeoffs associated with data selection routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and diversity-based measures in the feature space. The first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of instruction-tuned LLMs to directly assess the quality of a training example. To target coverage, the authors propose Density sampling, which models the data distribution to select a diverse sample. In the comparison of 19 samplers, involving hundreds of evaluation tasks and pre-training runs, the research finds that Ask-LLM and Density are the best methods in their respective categories. Coverage sampling can recover the performance of the full data, while models trained on Ask-LLM data consistently outperform full-data training -- even when we reject 90% of the original dataset, while converging up to 70% faster.

4.	***Scaling Language Models to 128K Context:<br>
University of Edinburgh, MIT, and others publish a paper on scaling language models' context lengths to 128K through continual pretraining. The study focuses on data engineering, emphasizing the quantity and quality of data for effective pretraining. Results highlight the importance of domain balance and length upsampling in achieving optimal performance.***<br><br>
15 Feb, University of Edinburgh, MIT, inter alia published a [paper](https://arxiv.org/pdf/2402.10171.pdf) “Data Engineering for Scaling Language Models to 128K Context”. The authors study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. The researchers hypothesize that long context modeling, in particular \textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. The study investigates the \textit{quantity} and \textit{quality} of the data for continual pretraining: (1) for quantity, the authors show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, the results equally emphasize \textit{domain balance} and \textit{length upsampling}. Concretely, the researchers find that naively upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important. The authors demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. The proposed recipe outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K. Experiments were conducted with 8 A100 80GB Gpus, run 5 days.

5.	***BitDelta: Compression of Fine-Tuned Models:<br>
MIT, Princeton Uni, and Together AI publish a paper on BitDelta, a method to quantize fine-tuned Large Language Models (LLMs) down to 1 bit without compromising performance. This finding suggests potential redundancy in information added during fine-tuning, with significant implications for GPU memory requirements and generation latency in multi-tenant settings.***<br><br>
15 Feb, MIT, Princeton Uni, and Together AI published a [paper](https://arxiv.org/pdf/2402.10193.pdf) “BitDelta: Your Fine-Tune May Only Be Worth One Bit”. The researchers point out that Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it's intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. The authors explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. The paper introduces a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, which can also be translated to enhanced generation latency in multi-tenant settings. The authors validate BitDelta through experiments across Llama-2 and Mistral model families, and on models up to 70B parameters, showcasing minimal performance degradation over all tested settings. The authors used 6 A100 80GB Gpus in their experiments.

6.	***Meta AI's Feature Prediction for Visual Representations:<br>
Meta AI publishes a paper exploring feature prediction as a stand-alone objective for unsupervised learning from video. V-JEPA, a collection of vision models, is introduced, trained solely using feature prediction without pretrained image encoders or text. Results show versatile visual representations performing well on motion and appearance-based tasks.***<br><br>
14 Feb, Meta AI published a [paper](https://scontent.fcbr1-1.fna.fbcdn.net/v/t39.2365-6/427986745_768441298640104_1604906292521363076_n.pdf?_nc_cat=103&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=Lpq5IeF5ftUAX9GJm-9&_nc_ht=scontent.fcbr1-1.fna&oh=00_AfDL6KWpDyoVaBBKFZ-zLYHLxohnVdSx85in-r4loH4kyg&oe=65D69EB1) “Revisiting Feature Prediction for Learning Visual Representations from Video”. This paper explores feature prediction as a stand-alone objective for unsupervised learning from video and introduces V-JEPA, a collection of vision models trained solely using a feature prediction objective, without the use of pretrained image encoders, text, negative examples, reconstruction, or other sources of supervision. The models are trained on 2 million videos collected from public datasets and are evaluated on downstream image and video tasks. Experimental results show that learning by predicting video features leads to versatile visual representations that perform well on both motion and appearance-based tasks, without adaption of the model’s parameters; e.g., using a frozen backbone. The largest model, a ViT-H/16 trained only on videos, obtains 81.9% on Kinetics-400, 72.2% on Something-Something-v2, and 77.9% on ImageNet1K.

7.	***Compute Governance in AI Development:<br>
OpenAI and other institutes publish a paper emphasizing the role of computing power in governing AI development. Compute is considered a crucial intervention point for achieving common policy objectives in AI safety and beneficial use. The paper suggests guardrails to minimize risks associated with compute-based policies.***<br><br>
14 Feb, OpenAI and list of institutes published a  [paper](https://arxiv.org/pdf/2402.08797.pdf) “Computing Power and the Governance of Artificial Intelligence”. The paper states that computing power, or "compute," is crucial for the development and deployment of artificial intelligence (AI) capabilities. As a result, governments and companies have started to leverage compute as a means to govern AI. For example, governments are investing in domestic compute capacity, controlling the flow of compute to competing countries, and subsidizing compute access to certain sectors. However, these efforts only scratch the surface of how compute can be used to govern AI development and deployment. Relative to other key inputs to AI (data and algorithms), AI-relevant compute is a particularly effective point of intervention: it is detectable, excludable, and quantifiable, and is produced via an extremely concentrated supply chain. These characteristics, alongside the singular importance of compute for cutting-edge AI models, suggest that governing compute can contribute to achieving common policy objectives, such as ensuring the safety and beneficial use of AI. More precisely, policymakers could use compute to facilitate regulatory visibility of AI, allocate resources to promote beneficial outcomes, and enforce restrictions against irresponsible or malicious AI development and usage. However, while compute-based policies and technologies have the potential to assist in these areas, there is significant variation in their readiness for implementation. Some ideas are currently being piloted, while others are hindered by the need for fundamental research. Furthermore, naive or poorly scoped approaches to compute governance carry significant risks in areas like privacy, economic impacts, and centralization of power. We end by suggesting guardrails to minimize these risks from compute governance.

8.	***Reassessing Link Prediction Metrics:<br>
Researchers from UC Santa Cruz argue that the metric (AUC) used for measuring link prediction performance lacks crucial information. Link prediction tasks are found to perform significantly worse than indicated by popular literature, prompting the proposal for a new, comprehensive metric.***<br><br>
13 Feb, according to [techxplore.com](https://techxplore.com/news/2024-02-widespread-machine-methods-link-poorly.html), researchers from UC Santa Cruz published a paper on PNAS argue that the metric used to measure link prediction performance is missing crucial information, and link prediction tasks are performing significantly worse than popular literature indicates. The paper recommends that ML researchers stop using the standard practice metric for measuring link prediction, known as AUC, and introduce a new, more comprehensive metric for this problem. In this research, the authors discovered that there are fundamental mathematical limitations to using low dimensional embeddings for link predictions, and that AUC can not measure these limitations. The inability to measure these limitations caused the authors to conclude that AUC does not accurately measure link prediction performance.

9.	***Nvidia's Chat with RTX for AI Chatbot:<br>
Nvidia introduces Chat with RTX, allowing users to run an AI chatbot on Windows PCs powered by NVIDIA RTX. The tool leverages retrieval-augmented generation (RAG) and NVIDIA TensorRT-LLM software, enabling personalization and contextually relevant queries. It aims to reduce GPU memory requirements and enhance generation latency in multi-tenant settings.***<br><br>
13 Feb, According to Nvidia, Nvidia’s [Chat with RTX](https://blogs.nvidia.com/blog/chat-with-rtx-available-now/) lets you run an AI chatbot on your GPU. The tool is a free-to-download tech demo bringing generative AI capabilities to Windows PCs powered by NVIDIA RTX. This tool allows users to personalize a chatbot with their own content, utilizing retrieval-augmented generation (RAG) and NVIDIA TensorRT-LLM software. Users can connect local files as a dataset, enabling quick and contextually relevant queries, bypassing the need for cloud-based services. The application supports various file formats, including .txt, .pdf, .doc/.docx, and .xml, and allows integration of information from YouTube videos. Running locally on Windows RTX PCs ensures fast results while keeping user data on the device. The tool requires a GeForce RTX 30 Series GPU or higher with at least 8GB of VRAM, Windows 10 or 11, and the latest NVIDIA GPU drivers. Developers can explore the potential of accelerating Large Language Models (LLMs) with RTX GPUs using the TensorRT-LLM RAG developer reference project available on GitHub. The article also mentions an identified issue during installation, which will be addressed in a future release. Additionally, there's an ongoing NVIDIA Generative AI on NVIDIA RTX developer contest, offering prizes such as a GeForce RTX 4090 GPU and a conference pass to NVIDIA GTC for participants.

10.	***World Model on Million-Length Video and Language:<br>
UC Berkeley publishes a paper on training one of the largest context size transformers on long video and language sequences. The study addresses challenges in learning from millions of tokens, utilizing the RingAttention technique for scalable training. The work paves the way for developing understanding of both human knowledge and the multimodal world.***<br><br>
13 Feb, UC Berkeley published a [paper](https://arxiv.org/abs/2402.08268) “World Model on Million-Length Video And Language With RingAttention”. The study indicates that current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, the researchers curate a large dataset of diverse videos and books, utilize the RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest context size neural network: the authors train one of the largest context size transformers on long video and language sequences, setting new benchmarks in difficult retrieval tasks and long video understanding. (b) Solutions for overcoming vision-language training challenges, including using masked sequence packing for mixing different sequence lengths, loss weighting to balance language and vision, and model-generated QA dataset for long sequence chat. (c) A highly-optimized implementation with RingAttention, masked sequence packing, and other key features for training on millions-length multimodal sequences. (d) Fully open-sourced a family of 7B parameter models capable of processing long text documents (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M tokens. This work paves the way for training on massive datasets of long video and language to develop understanding of both human knowledge and the multimodal world, and broader capabilities.

11.	***GhostWriter: AI-Enhanced Writing Design:<br>
Harvard Uni and Microsoft introduce GhostWriter, an AI-enhanced writing design probe that leverages Large Language Models (LLMs) for personalized text generation. Users can exercise enhanced agency and personalization, providing multiple ways to control the system's writing style.***<br><br>
13 Feb, Harvard Uni and Microsoft published a [paper](https://arxiv.org/pdf/2402.08855.pdf) “GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency”. The authors states that Large language models (LLMs) are becoming more prevalent and have found a ubiquitous use in providing different forms of writing assistance. However, LLM-powered writing systems can frustrate users due to their limited personalization and control, which can be exacerbated when users lack experience with prompt engineering. The researchers see design as one way to address these challenges and introduce GhostWriter, an AI-enhanced writing design probe where users can exercise enhanced agency and personalization. GhostWriter leverages LLMs to learn the user's intended writing style implicitly as they write, while allowing explicit teaching moments through manual style edits and annotations. The authers study 18 participants who use GhostWriter on two different writing tasks, observing that it helps users craft personalized text generations and empowers them by providing multiple ways to control the system's writing style. The study presents insights regarding people's relationship with AI-assisted writing and offer design recommendations for future work.

12.	***PIVOT: Visual Prompting for VLMs:<br>
Google publishes a paper on PIVOT, a visual prompting approach for Vision Language Models (VLMs) that enables zero-shot control of robotic systems. PIVOT uses iterative visual optimization, allowing VLMs to handle spatial tasks without fine-tuning on task-specific data.***<br><br>
12 Feb, Google published a [paper](https://arxiv.org/pdf/2402.07872.pdf) “PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs”. The authors indicates that Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data? In this paper, the researchers propose a novel visual prompting approach for VLMs that is called Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. The authors investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. The researchers find, perhaps surprisingly, that the proposed approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, the work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: [this http URL](http://pivot-prompt.github.io/).

13.	***Generative AI and the Future of Work in Australia:<br>
McKinsey & Company's updated report indicates that 62% of existing task hours in Australia could be automated using generative AI, rising to 79-98% by 2030. The report models various adoption scenarios, considering factors such as technical potential, implementation costs, and societal dynamics.***<br><br>
12 Feb, McKinsey & Company published a [report](https://www.mckinsey.com/industries/public-sector/our-insights/generative-ai-and-the-future-of-work-in-australia) “Generative AI and the future of work in Australia”. This is an updated version of 2019, which estimated that 44 percent of Australians’ time at work could be automated by adopting the technology of the time. The new report revisited the topic to assess how the rapid emergence of gen AI has accelerated machines’ capabilities, finding that 62 percent of existing task hours could be automated using the technology available at the time of analysis. This potential could rise further to between 79 and 98 percent by 2030. However, there could be a substantial time lag between technical potential and realized change—developing capabilities into technical solutions takes time, the cost of implementing solutions may exceed the cost of human labor, and the pace of adoption could be influenced by social or regulatory dynamics. Accounting for these potential sources of friction, the new report modeled a series of adoption scenarios. While the early scenario suggests that just above 50 percent of activities could be automated by 2030, the late scenario could see just 2 percent in the same year. The midpoint of these scenarios would imply that around one-quarter of work hours could be automated by 2030.

14.	***Debating for Truthful Answers in LLMs:<br>
UCL and others publish a paper on using debate as a method for aligning Large Language Models (LLMs) with desired behavior. The study shows that debate helps non-expert models and humans answer questions more accurately, providing empirical evidence for model alignment.***<br><br>
9 Feb, UCL and others published a [paper](https://arxiv.org/pdf/2402.06782.pdf) “Debating with More Persuasive LLMs Leads to More Truthful Answers”. The authors point out that common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? The study investigates this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method evaluated is \textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. The study finds that debate consistently helps both non-expert models and humans answer questions, achieving 76\% and 88\% accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. The results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.


**11 Feb 2024**

1.	***OpenAI seeks $7 trillion for global chip project: OpenAI CEO seeks massive investment to overhaul the global semiconductor industry by building new chip factories worldwide.***<br><br>
9 Feb, according to [CNBC](https://www.msn.com/en-us/money/companies/openai-ceo-sam-altman-seeks-as-much-as-7-trillion-for-new-ai-chip-project-report/ar-BB1i285k), OpenAI CEO Sam Altman seeks as much as $7 trillion for new AI chip project to overhaul the global semiconductor industry with trillions of dollars in investment. Altman has said AI chip limitations hinder OpenAI's growth, and as this project would increase chip-building capacity globally, he is in talks with investors, including the United Arab Emirates government. Altman could need to raise between $5 trillion and $7 trillion for the endeavor, the WSJ reported, citing one source. CNBC could not confirm the exact number.

2.	***Cloud repatriation trend in UK: A study reveals 25% of UK organizations are moving workloads back on-premises due to security concerns, unmet expectations, and cost overruns in the cloud.***<br><br>
9 Feb, according to [InfoWorld](https://www.infoworld.com/article/3712861/why-companies-are-leaving-the-cloud.html), a recent study by Citrix reveals that 25% of surveyed organizations in the United Kingdom have moved at least half of their cloud-based workloads back to on-premises infrastructures. The survey, which involved 350 IT leaders, highlights security issues and unmet expectations as the main drivers (33%) for repatriating workloads. Additionally, 24% cited the failure to meet internal expectations as a significant motivator. Cost emerges as a predominant factor, with over 43% of IT leaders finding the migration to the cloud more expensive than anticipated. The article suggests that the cloud's inability to deliver on the promised lower costs, better agility, and innovation from 2010 to 2015 contributes to this shift. Traditional infrastructure patterns, where enterprises run workloads and data sets similar to on-premises methods, lead to unexpected costs in the public cloud. The cloud is deemed suitable for modern applications but may not align with most enterprise applications. Despite repatriation challenges, public cloud providers are expected to maintain growth due to their role in hosting generative AI applications and data. The article concludes that the ongoing focus on AI systems and expanding cloud usage for AI-related infrastructure contributes positively to the cloud providers' growth, making the situation healthy for the industry. (NOTE: AI systems are more suitable to run on-premises !)

3.	***Australian law firm embraces AI: Lander & Rogers utilizes Copilot for various legal tasks and launches an AI Lab to develop custom tools and integrate AI into workflows.***<br><br>
9 Feb, according to [itnews.com.au](https://www.itnews.com.au/news/australian-law-firm-lander-rogers-finds-legal-uses-for-copilot-604842), Australian law firm Lander & Rogers finds legal uses for Copilot and also sets up an AI Lab.  The firm has initiated a pilot program with a core group and plans to expand its usage across the organization in the coming months. Copilot is being utilized for contract drafting, data extraction, legal research, and simpler tasks like email composition and meeting summarization. The law firm has found Copilot particularly helpful in correspondence drafting and collaborating with its knowledge in the Power Platform. Lander & Rogers has encouraged its 500 staff members to use Copilot, especially for email composition, where the AI tool can extract key points from documents and assist with spelling, grammar, and conciseness. The firm had previously assessed tasks suitable for automation, partnering with Monash University for AI testing. Recently, the law firm launched an AI Lab with a focus on developing specialized tools for legal tasks. While no prototypes are completed yet, the AI Lab is concentrating on proofs-of-concept to bridge theoretical AI capabilities with practical, usable tools in legal practice. The AI Lab aims to integrate AI into operational frameworks and legal workflows to enhance efficiency, accuracy, and client satisfaction. Lander & Rogers acknowledges the importance of educating staff on the limitations and ethical considerations of emerging technologies. The firm has a nine-point AI policy, emphasizing responsible AI use, clear and unambiguous prompts, confidentiality, and privacy considerations. The AI Lab plays a crucial role in ensuring the firm's adherence to privacy obligations and promoting ethical AI practices.

4.	***Interactive Agent Foundation Model proposed: Microsoft, Stanford, and UCLA research proposes a new training paradigm for building versatile AI agents capable of handling diverse tasks and applications.***<br><br>
9 Feb, Microsoft, Standford Uni and UCLA published a [paper](https://arxiv.org/pdf/2402.05929) “An Interactive Agent Foundation Model”. The paper indicates that the development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. The research proposes an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. The training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. The study demonstrates the performance of the framework across three separate domains -- Robotics, Gaming AI, and Healthcare. The proposed model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of the approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effective multimodal and multi-task learning. The proposed approach provides a promising avenue for developing generalist, action-taking, multimodal systems.

5.	***Question Aware Vision Transformer for multimodal reasoning: AWS introduces QA-ViT, a method that embeds question awareness directly into vision encoding, improving visual and scene-text understanding in multimodal AI models.***<br><br>
8 Feb, AWS published a [paper](https://arxiv.org/pdf/2402.05472.pdf) “Question Aware Vision Transformer for Multimodal Reasoning”. The paper states that Vision-Language (VL) models have gained significant research focus, enabling remarkable advances in multimodal reasoning. These architectures typically comprise a vision encoder, a Large Language Model (LLM), and a projection module that aligns visual features with the LLM's representation space. Despite their success, a critical limitation persists: the vision encoding process remains decoupled from user queries, often in the form of image-related questions. Consequently, the resulting visual features may not be optimally attuned to the query-specific elements of the image. To address this, we introduce QA-ViT, a Question Aware Vision Transformer approach for multimodal reasoning, which embeds question awareness directly within the vision encoder. This integration results in dynamic visual features focusing on relevant image aspects to the posed question. QA-ViT is model-agnostic and can be incorporated efficiently into any VL architecture. Extensive experiments demonstrate the effectiveness of applying the proposed method to various multimodal architectures, leading to consistent improvement across diverse tasks and showcasing its potential for enhancing visual and scene-text understanding.

6.	***Repurposing LLMs for specialized domains: Researchers develop a framework to "tag" general LLMs with specialized information, enabling them to perform tasks in previously underrepresented domains like medicine and chemistry.***<br><br>
7 Feb, MSR, CMU, and Harvard Uni published a [paper](https://arxiv.org/pdf/2402.05140) “Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains”. The researchers point out that Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. The research introduces a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. The researchers design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. The studay develops a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from task functions, the proposed method enables zero-shot generalization to unseen problems through diverse combinations of the input tags. It also boosts LLM's performance in various specialized domains, such as predicting protein or chemical properties and modeling drug-target interactions, outperforming expert models tailored to these tasks.

7.	***Britain invests in AI research hubs: UK launches nine new research hubs with $120 million funding to promote AI development and train regulators on managing its risks.***<br><br>
6 Feb, according to [channelnewsasia.com](https://www.channelnewsasia.com/business/britain-invests-100-million-pounds-ai-research-and-regulation-4102071), Britain invests 100 million pounds to launch nine new research hubs in artificial intelligence (AI) and train regulators about the technology, by taking an agile, sector-specific approach to grip the risks immediately. Nearly 90 million pounds would go towards the hubs, which will focus on using AI in areas including healthcare, chemistry and mathematics, and a partnership with the United States on responsible AI, the government said. Another 10 million pounds would help regulators address the risks and harness the opportunities of AI, it said, such as developing practical tools to monitor risks in sectors from telecoms and healthcare to finance and education.

8.	***LLMs learn to self-compose reasoning structures: USC and Google introduce SELF-DISCOVER, a framework that allows LLMs to learn and utilize reasoning structures, significantly improving their performance on complex tasks.***<br><br>
6 Feb, University of South California and Google published a [paper](https://arxiv.org/pdf/2402.03620.pdf) “Self-Discover: Large Language Models Self-Compose Reasoning Structures”. The researchers introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, the study shows that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.

9.	***Ex-Google engineer critiques Agile methodology: A former Google engineer argues that the Agile software development methodology can lead to micromanagement and poorly maintained code, sparking debate among developers.***<br><br>
1 Feb, [devclass.com](https://devclass.com/2024/02/01/agile-process-can-spur-micromanagement-and-poorly-maintained-code-says-ex-google-software-engineer/) published an article “Agile process can spur micromanagement and poorly maintained code, says ex-Google software engineer”. A former Google software engineer, Murat Guler, contends in his book, "Defending Software Quality," that the Agile methodology, often hailed for improving productivity and code quality, can lead to micromanagement and poorly maintained code. Guler, who worked at Google for over 13 years, argues that Agile's emphasis on daily collaboration between business people and developers may encourage micromanagement, leading to issues such as unrealistic deadlines, reduced trust in engineers, and increased scrutiny. Additionally, Guler criticizes aspects of Agile, including 2-week sprints, welcoming changing requirements, and practices like pair programming and shared code ownership. He points out that even when used correctly, Agile may not fit all types of software development, and its principles can be misapplied, resulting in negative consequences. The post has garnered support from developers who share similar concerns about the imposition of Agile methodology, describing it as "cult-like" and emphasizing the importance of team dynamics and mutual trust over processes and tools.




**4 Feb 2024**

1.	2 Feb, according to [Theverge.com](https://www.theverge.com/2024/2/1/24058095/open-ai-bioweapon-study-preparedness-team), OpenAI's study suggests its GPT-4 language model offers only a "slight" advantage over the internet for bioweapon research, contradicting concerns and some external research. While participants with GPT-4 achieved slightly higher accuracy, the increase wasn't statistically significant. Concerns remain due to GPT-4's vast data access and multimodal nature, suggesting the potential for a larger advantage than reported.
 
2.	1 Feb, NYU published a [paper](https://www.science.org/doi/reader/10.1126/science.adi1374) on Science “Grounded language acquisition through the eyes and ears of a single child”. How do young children learn to associate new words with specific objects or visually represented concepts? This hotly debated question in early language acquisition has been traditionally examined in laboratories, limiting generalizability to real-world settings. This study investigated the question in an unprecedented, longitudinal manner using head-mounted video recordings from a single child’s first-person experiences in naturalistic settings. By applying machine learning, they introduced the Child’s View for Contrastive Learning (CVCL) model, pairing video frames that co-occurred with uttered words, and embedded the images and words in shared representational spaces. CVCL represents sets of visually similar things from one concept (e.g., puzzles) through distinct subclusters (animal versus alphabet puzzles). It combines associative and representation learning that fills gaps in language acquisition research and theories. 

3.	1 Feb, researchers from Georgetown Uni and Apple published a [paper](https://arxiv.org/pdf/2402.00858.pdf) “Can Large Language Models Understand Context?”. The study indicates that understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent. However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features. This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models. This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models' ability to understand context. First, the researcher evaluates the performance of LLMs under the in-context learning pretraining scenario. Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. Second, as LLM compression holds growing significance in both research and real-world applications, the researchers assess the context understanding of quantized models under in-context-learning settings. The study finds that 3-bit post-training quantization leads to varying degrees of performance reduction on the benchmark. The researchers conduct an extensive analysis of these scenarios to substantiate the experimental results.

4.	31 Jan, AI2, UCB, CMU, MIT etc published a [paper](https://arxiv.org/pdf/2402.00159.pdf) “Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research”. The paper indicates that language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed:  commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, the researchers release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, the researchers open source the data curation toolkit to enable further experimentation and reproduction of the work. In this report, the researchers document Dolma, including its design principles, details about its construction, and a summary of its contents. The paper interleaves this report with analyses and experimental results from training language models on intermediate states of Dolma to share what the researchers have learned about important data curation practices, including the role of content or quality filters, deduplication, and multi-source mixing. Dolma has been used to train OLMo, a state-of-the-art, open language model and framework designed to build and study the science of language modelling.

5.	31 Jan, according to [Arstechnica](https://arstechnica.com/security/2024/01/ars-reader-reports-chatgpt-is-sending-him-conversations-from-unrelated-ai-users/), ChatGPT is leaking private conversations that include login credentials and other personal details of unrelated users, screenshots submitted by an Ars reader on Monday indicated. Two of the seven screenshots the reader submitted stood out in particular. Both contained multiple pairs of usernames and passwords that appeared to be connected to a support system used by employees of a pharmacy prescription drug portal. An employee using the AI chatbot seemed to be troubleshooting problems they encountered while using the portal. User shocked to find chats naming unpublished research papers, and other private data. OpenAI officials say that the ChatGPT histories a user reported result from his ChatGPT account being compromised. The unauthorized logins came from Sri Lanka, an Open AI representative said. The user said he logs into his account from Brooklyn, New York. The user, Chase Whiteside, has since changed his password, but he doubted his account was compromised. He said he used a nine-character password with upper- and lower-case letters and special characters. He said he didn’t use it anywhere other than for a Microsoft account. He said the chat histories belonging to other people appeared all at once on Monday morning during a brief break from using his account.

6.	30 Jan, UIUC published a [paper](https://arxiv.org/pdf/2401.17263.pdf) “Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks”. The paper states that despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which the researchers posit should be effective, universal, and practical. To achieve this, the authors of the paper propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, the study finds that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success rate of the strongest attack on GPT-4 from 92% to 6%. The code is available [here](https://github.com/andyz245/rpo).

7.	30 Jan, according to [Reuters](https://www.reuters.com/technology/cybersecurity/italy-regulator-notifies-openai-privacy-breaches-chatgpt-2024-01-29/), Italy's data protection authority has told OpenAI that its artificial intelligence chatbot application ChatGPT breaches data protection rules, as it presses ahead with an investigation started last year. Italy was the first West European country to curb ChatGPT, whose rapid development has attracted attention from lawmakers and regulators. Under the EU's General Data Protection Regulation (GDPR) introduced in 2018, any company found to have broken rules faces fines of up to 4% of its global turnover.

8.	29 Jan, researchers from Italy and Israel published a [paper](https://arxiv.org/pdf/2401.14887.pdf) “The Power of Noise: Redefining Retrieval for RAG Systems”. The paper argues that Retrieval-Augmented Generation (RAG) systems represent a significant advancement over traditional Large Language Models (LLMs). RAG systems enhance their generation ability by incorporating external data retrieved through an Information Retrieval (IR) phase, overcoming the limitations of standard LLMs, which are restricted to their pre-trained knowledge and limited context window. Most research in this area has predominantly concentrated on the generative aspect of LLMs within RAG systems. This study fills this gap by thoroughly and critically analyzing the influence of IR components on RAG systems. This paper analyzes which characteristics a retriever should possess for an effective RAG's prompt formulation, focusing on the type of documents that should be retrieved. We evaluate various elements, such as the relevance of the documents to the prompt, their position, and the number included in the context. The findings reveal, among other insights, that including irrelevant documents can unexpectedly enhance performance by more than 30% in accuracy, contradicting people initial assumption of diminished quality. These results underscore the need for developing specialized strategies to integrate retrieval with language generation models, thereby laying the groundwork for future research in this field.

9.	29 Jan, CMU & Apple published a [paper](https://arxiv.org/pdf/2401.16380.pdf) “Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling”. The paper states that Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. This study proposes Web Rephrase Augmented Pre-training (WRAP) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as “like Wikipedia” or in “question-answer format” to jointly pre-train LLMs on real and synthetic rephrases. First, the researchers show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by ∼ 3×. At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, the researchers investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. The gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher ‘quality’ than web-scraped data.

10.	26 Jan, University of Edinburgh published a [paper](https://arxiv.org/pdf/2401.15241.pdf) “Unlearning Reveals the Influential Training Data of Language Models”. The paper indicates that in order to enhance the performance of language models while mitigating the risks of generating harmful content, it is crucial to identify which training dataset affects the model's outputs. Ideally, researchers can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac, which estimates the influence of a training dataset by unlearning it from the trained model. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and the researchers evaluate how much the model's predictions change after unlearning. The paper empirically examines if the proposed methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Experimental results demonstrate that the method estimates their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple model checkpoints.

11.	26 Jan, Harvard Business Review published a [paper](https://hbr.org/2024/01/using-prompt-engineering-to-better-communicate-with-people) “Using Prompt Engineering to Better Communicate with People”. The paper argues that it seems like everyone knows, or wants to know, how to produce the best possible prompts for generative AI. But, among the buzz and promise of masterful prompt engineering, there is an emerging risk that managers will view generative AI as a one-stop-shop for gathering information. In doing so, they may neglect their most valuable information resources: employees, partners, and customers. These stakeholders offer contextual information and tacit knowledge that is beyond the capability of any generative AI tool. As we get better at speaking to robots, we should remember how to most effectively speak to our colleagues and customers, too. Following are a summary of the six guidelines to consider when initiating discussions with stakeholders, whether they’re customers, partners, or employees: 1) Structure your prompts the right way. 2) Utilize reflective and thoughtful probing. 3) Convey empathetic language and humility. 4) Harness humor, playfulness, and emotions. 5) Acknowledge key challenges. 6) Be a good (and patient) listener.

12.	23 Jan, according to the [androidcentral](https://www.androidcentral.com/apps-software/google-leaked-2024-goals), Google’s 2024 goals are: 1) Deliver the world’s most advanced, safe, and responsible Al. 2) Improve knowledge, learning, creativity, and productivity. 3) Build the most helpful personal computing platforms and devices. 4) Enable organizations and developers to innovate on Google Cloud. 5) Provide the world’s most trusted products and platforms. 6) Build a Google that’s extraordinary for Googlers and the world. 7) Improve company velocity, efficiency, and productivity, and deliver durable cost savings.




**28 Jan 2024**

1.	28 Jan, according to [this post](https://twitter.com/owencm/status/1751409104713826666), OpenAI’s new embedding strategy is based on an MRL technique, which is stemmed from the [paper](https://openreview.net/pdf?id=9njZa1fm35) “Matryoshka Representation Learning”. Owen Campbell-Moore from OpenAI posted “I was responsible for the blog post and it’s my bad not thinking / remembering to cite. We’re updating the blog post to add a citation now!”, but this happened after [others said](https://twitter.com/jainprateek_/status/1751291366515384354) “Closed science companies like OpenAI and Anthropic parasitically extract value from open science and open source without giving credit to people or organizations building them. Open science with citations would’ve addressed that, but alas that’s too much to ask.”

2.	25 Jan, He, K. from Meta and NYU published a [paper](https://arxiv.org/pdf/2401.14404.pdf) “Deconstructing Denoising Diffusion Models for Self-Supervised Learning”. In this study, the researchers examine the representation learning abilities of Denoising Diffusion Models (DDM) that were originally purposed for image generation. The auther’s philosophy is to deconstruct a DDM, gradually transforming it into a classical Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore how various components of modern DDMs influence self-supervised representation learning. The researchers observe that only a very few modern components are critical for learning good representations, while many others are nonessential. The study ultimately arrives at an approach that is highly simplified and to a large extent resembles a classical DAE. The authors hope the study will rekindle interest in a family of classical methods within the realm of modern self-supervised learning.

3.	25 Jan, University of Edinburgh and NTU Singapore published a [paper](https://arxiv.org/pdf/2401.14351.pdf) “ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models”. This paper presents ServerlessLLM, a locality-enhanced serverless inference system for Large Language Models (LLMs). ServerlessLLM exploits the substantial capacity and bandwidth of storage and memory devices available on GPU servers, thereby reducing costly remote checkpoint downloads and achieving efficient checkpoint loading. ServerlessLLM achieves this through three main contributions: (i) fast LLM checkpoint loading via a novel loading-optimized checkpoint format design, coupled with an efficient multi-tier checkpoint loading system; (ii) locality-driven LLM inference with live migration, which allows ServerlessLLM to effectively achieve locality-driven server allocation while preserving the low latency of ongoing LLM inference; and (iii) locality-aware server allocation, enabling ServerlessLLM to evaluate the status of each server in a cluster and effectively schedule model startup time to capitalize on local checkpoint placement. Comprehensive experiments, which include microbenchmarks and real-world traces, show that ServerlessLLM surpasses state-of-the-art systems by 10 - 200X in latency performance when running various LLM inference workloads.

4.	24 Jan, Cornell Uni published a [paper](https://arxiv.org/pdf/2401.13660.pdf) “MambaByte: Token-free Selective State Space Model”. The paper states that Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. The study experiments with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences. Experiments indicate the computational efficiency of MambaByte compared to other byte-level models. The research also finds MambaByte to be competitive with and even outperform state-of-the-art subword Transformers. Furthermore, owing to linear scaling in length, MambaByte benefits from fast inference compared to Transformers. These findings establish the viability of MambaByte in enabling token-free language modeling.
 
5.	23 Jan, Google published a [paper](https://arxiv.org/pdf/2401.12945.pdf) “Lumiere: A Space-Time Diffusion Model for Video Generation”.  The paper introduces Lumiere -- a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion -- a pivotal challenge in video synthesis. To this end, the researchers introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution -- an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, Lumiere learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. The researchers demonstrate state-of-the-art text-to-video generation results, and show that the design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation. Project link is [here](https://lumiere-video.github.io/).

6.	23 Jan, Stanford and OpenAI published a [paper](https://arxiv.org/pdf/2401.12954.pdf) “Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding”. The paper introduces meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to deconstruct complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct "expert" instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly enhancing its performance across a wide array of tasks. The zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction by obviating the need for detailed, task-specific instructions. Furthermore, the research demonstrates the seamless integration of external tools, such as a Python interpreter, into the meta-prompting framework, thereby broadening its applicability and utility. Through rigorous experimentation with GPT-4, the researchers establish the superiority of meta-prompting over conventional scaffolding methods: When averaged across all tasks, including the Game of 24, Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented with a Python interpreter functionality, surpasses standard prompting by 17.1%, expert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.

7.	23 Jan, Google DeepMind published a [paper](https://arxiv.org/pdf/2401.12963.pdf) “AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents”. The paper finds that foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. This research proposes AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. The study demonstrates AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. The researchers experimentally show that such "in-the-wild" data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences.

8.	23 Jan, Nvidia published a [paper](https://arxiv.org/pdf/2401.10225.pdf) “ChatQA: Building GPT-4 Level Conversational QA Models”. In this work, Nvidia introduces ChatQA, a family of conversational question answering (QA) models that obtain GPT-4 level accuracies. Specifically, the study proposes a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval-augmented generation in conversational QA, the researchers fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, the ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.

9.	23 Jan, Chatdoc.com published the research [paper](https://arxiv.org/pdf/2401.12599.pdf) “Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition” for the chatdco project. With the rapid development of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) has become a predominant method in the field of professional knowledge-based question answering. Presently, major foundation model companies have opened up Embedding and Chat API interfaces, and frameworks like LangChain have already integrated the RAG process. It appears that the key models and steps in RAG have been resolved, leading to the question: are professional knowledge QA systems now approaching perfection? This article discovers that current primary methods depend on the premise of accessing high-quality text corpora. However, since professional documents are mainly stored in PDFs, the low accuracy of PDF parsing significantly impacts the effectiveness of professional knowledge-based QA. The study conducted an empirical RAG experiment across hundreds of questions from the corresponding real-world professional documents. The results show that, ChatDOC, a RAG system equipped with a panoptic and pinpoint PDF parser, retrieves more accurate and complete segments, and thus better answers. Empirical experiments show that ChatDOC is superior to baseline on nearly 47% of questions, ties for 38% of cases, and falls short on only 15% of cases. It shows that it possible to revolutionize RAG with enhanced PDF structure recognition.

10.	22 Jan, MIT FutureTech published a working [paper](https://futuretech-site.s3.us-east-2.amazonaws.com/2024-01-18+Beyond_AI_Exposure.pdf) “Beyond AI Exposure: Which Tasks are Cost-Effective to Automate with Computer Vision?”. The faster AI automation spreads through the economy, the more profound its potential impacts, both positive (improved productivity) and negative (worker displacement). The previous literature on “AI Exposure” cannot predict this pace of automation since it attempts to measure an overall potential for AI to affect an area, not the technical feasibility and economic attractiveness of building such systems. This article presents a new type of AI task automation model that is end-to-end, estimating: the level of technical performance needed to do a task, the characteristics of an AI system capable of that performance, and the economic choice of whether to build and deploy such a system. The result is a first estimate of which tasks are technically feasible and economically attractive to automate - and which are not. The research focuses on computer vision, where cost modeling is more developed. The study finds that at today’s costs U.S. businesses would choose not to automate most vision tasks that have “AI Exposure,” and that only 23% of worker wages being paid for vision tasks would be attractive to automate. This slower roll-out of AI can be accelerated if costs falls rapidly or if it is deployed via AI-as-a-service platforms that have greater scale than individual firms, both of which we quantify. Overall, the findings suggest that AI job displacement will be substantial, but also gradual – and therefore there is room for policy and retraining to mitigate unemployment impacts.

11.	22 Jan, National Uni of Singapore published a [paper](https://arxiv.org/pdf/2401.11817.pdf) “Hallucination is Inevitable: An Innate Limitation of Large Language Models”. The paper indicates that hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. This study formalizes the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, the researchers define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, the research shows that LLMs cannot learn all of the computable functions and will therefore always hallucinate. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, the researchers describe the hallucination-prone tasks and empirically validate their claims. Finally, using the formal world framework, the paper discusses the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.

12.	19 Jan, Princeton Uni and other published a [paper](https://arxiv.org/pdf/2401.10774.pdf) “MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads”. The article states that the inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. The paper presents MEDUSA, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, MEDUSA constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, MEDUSA introduces only minimal overhead in terms of single-step latency while substantially reducing the number of decoding steps required. The researchers present two levels of fine-tuning procedures for MEDUSA to meet the needs of different use cases: • MEDUSA-1: M EDUSA is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. • MEDUSA-2: M EDUSA is fine-tuned together with the backbone LLM, enabling better prediction accuracy of MEDUSA heads and higher speedup but needing a special training recipe that preserves the backbone model’s capabilities. Moreover, the study proposes several extensions that improve or expand the utility of MEDUSA, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. The researchers evaluate MEDUSA on models of various sizes and training procedures. The experiments demonstrate that M EDUSA-1 can achieve over 2.2× speedup without compromising generation quality, while MEDUSA-2 further improves the speedup to 2.3-3.6×. The code for this implementation is available at https: //github.com/FasterDecoding/Medusa.

13.	19 Jan, University of Texas and Microsoft published a [paper](https://arxiv.org/pdf/2401.10464.pdf) “PhotoScout: Synthesis-Powered Multi-Modal Image Search”. The paper indicates that due to the availability of increasingly large amounts of visual data, there is a growing need for tools that can help users find relevant images. While existing tools can perform image retrieval based on similarity or metadata, they fall short in scenarios that necessitate semantic reasoning about the content of the image. This paper explores a new multi-modal image search approach that allows users to conveniently specify and perform semantic image search tasks. With the new tool, PhotoScout, the user interactively provides natural language descriptions, positive and negative examples, and object tags to specify their search tasks. Under the hood, PhotoScout is powered by a program synthesis engine that generates visual queries in a domain-specific language and executes the synthesized program to retrieve the desired images. In a study with 25 participants, the researchers observed that PhotoScout allows users to perform image retrieval tasks more accurately and with less manual effort.

14.	16 Jan, Ritsumeikan University published a [paper](https://arxiv.org/pdf/2401.08273.pdf) “Large Language Models are Null-Shot Learners”. The paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the "Examples" section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, the study proposes that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with six LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets. The researchers also perform ablation studies, including experimenting with a modified version of null-shot prompting that incorporates ideas from zero-shot chain-of-thought prompting, which shows different trends of results.



**21 Jan 2024**

1.	19 Jan, [ICLR 2024](https://openreview.net/group?id=ICLR.cc/2024/Conference#tab-accept-oral) released paper acceptation notifications. There are total 7304 submissions, acceptance rate is 30.81%, with Poster 24.63%, Spotlight 5.01% and oral 1.16%. The conference will in Vienna Austria, 7th- 11th May.

2.	18 Jan, Meta published a [paper](https://arxiv.org/pdf/2401.10020.pdf) “Self-Rewarding Language Models”. The researchers posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. This work studies Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. The researchers show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of the proposed approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continually improve in both axes.

3.	18 Jan, UCAS published a [paper](https://arxiv.org/pdf/2401.10166.pdf) “VMamba: Visual State Space Model”. The researchers indicate that Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) stand as the two most popular foundation models for visual representation learning. While CNNs exhibit remarkable scalability with linear complexity w.r.t. image resolution, ViTs surpass them in fitting capabilities despite contending with quadratic complexity. A closer inspection reveals that ViTs achieve superior visual modeling performance through the incorporation of global receptive fields and dynamic weights. This observation motivates the researchers to propose a novel architecture that inherits these components while enhancing computational efficiency. To this end, the researchers draw inspiration from the recently introduced state space model and propose the Visual State Space Model (VMamba), which achieves linear complexity without sacrificing global receptive fields. To address the encountered direction-sensitive issue, the study introduces the Cross-Scan Module (CSM) to traverse the spatial domain and converts any non-causal visual image into order patch sequences. Extensive experimental results substantiate that VMamba not only demonstrates promising capabilities across various visual perception tasks, but also exhibits more pronounced advantages over established benchmarks as the image resolution increases. Source code has been available at this [https URL](https://github.com/MzeroMiko/VMamba).

4.	17 Jan, Nature published a [paper](https://www.nature.com/articles/s41586-023-06747-5.pdf) from Google “Solving olympiad geometry without human demonstrations”. The paper states that proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning, owing to their reputed difficulty among the world’s best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges, resulting in severe scarcity of training data. The researchers propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on Google’s large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004. 

5.	16 Jan, OpenAI [posted](https://twitter.com/OpenAI/status/1746986660892741927) on X on how they are preparing for 2024’s world elections. These include: • Working to prevent abuse, including misleading deepfakes • Providing transparency on AI-generated content • Improving access to authoritative voting information.

6.	15 Jan, according to [BBC News](https://www.bbc.com/news/business-67977967), AI to hit 40% of jobs and worsen inequality, IMF says. The International Monetary Fund (IMF) predicts that nearly 40% of all jobs will be affected by artificial intelligence (AI), with the technology likely worsening overall inequality. In advanced economies, AI is projected to impact around 60% of jobs, benefiting workers in half of these cases through enhanced productivity. However, AI may also replace human-executed tasks, potentially lowering demand for labor, impacting wages, and leading to job elimination. The IMF suggests that the technology will affect only 26% of jobs in low-income countries, emphasizing the need for infrastructure and skilled workforces to harness AI benefits. Managing Director Kristalina Georgieva calls for comprehensive social safety nets and retraining programs to ensure an inclusive AI transition and curb inequality. The IMF analysis coincides with discussions on AI regulation at the World Economic Forum in Davos.

7.	14 Jan, AWS published a [paper](https://www.amazon.science/publications/panda-performance-debugging-for-databases-using-llm-agents) “Panda: Performance Debugging for Databases using LLM Agents”. Debugging a performance issue in databases is notoriously hard. Wouldn’t it be convenient if there exists an oracle or a co-pilot for every database system which users can query in natural language (NL) — ‘what’s wrong?’, or even better— ‘How to fix it?’. Large Language Models (LLMs), like ChatGPT, seem to be a natural surrogate to this oracle given their ability to answer a wide range of questions by efficiently encoding vast amount of knowledge for e.g., a major chunk of the internet. However, prompting ChatGPT with database performance queries often results in ‘technically correct’ but highly ‘vague’ or ‘generic’ recommendations typically rendered useless and untrustworthy by experienced Database Engineers (DBEs).  This work proposes Panda, a framework to provide context grounding to pre-trained LLMs in order to generate more ‘useful’ and ‘in-context’ troubleshooting recommendations. Panda draws inspiration from the way experienced DBEs perform debugging, and puts a system in place with necessary components required to robustly deploy pre-trained LLMs in production for debugging. The 4 key components of Panda are: (1) Grounding; (2) Verification;(3) Affordance; and (4) Feedback. The researchers describe the necessity and usefulness of each component and how they communicate internally to transform a given pre-trained LLM into generating in-context, actionable, useful and accurate recommendation for debugging a given database system.

8.	13 Jan, according to [futurism](https://futurism.com/the-byte/microsoft-cherrypicked-ai-examples), IN LEAKED AUDIO, MICROSOFT CHERRY-PICKED EXAMPLES TO MAKE ITS AI SEEM FUNCTIONAL "IT WASN'T THAT EASY TO GET GOOD ANSWERS." The article says Microsoft's internal presentation on an early version of its Security Copilot, a ChatGPT-like AI tool for cybersecurity, revealed challenges with hallucinating incorrect responses. The leaked audio exposed that cherry-picked examples were used to showcase the tool's capabilities as it frequently provided different answers and struggled to offer accurate responses. Security Copilot, based on OpenAI's GPT-4, faced hallucination issues common in large language models, exacerbated by using the model without cybersecurity-specific training data. Microsoft clarified that the discussed technology was exploratory, preceding Security Copilot, and used simulations from public datasets, ensuring no customer data was involved.
   
=====================================

1)	11 Jan, Google published a [paper](https://arxiv.org/pdf/2401.05654.pdf) “Towards Conversational Diagnostic AI”. The paper indicates that at the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians' expertise is an outstanding grand challenge. This research introduces AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue. AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. The researchers designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. The study compared AMIE's performance to that of primary care physicians (PCPs) in a randomized, double-blind crossover study of text-based consultations with validated patient actors in the style of an Objective Structured Clinical Examination (OSCE). The study included 149 case scenarios from clinical providers in Canada, the UK, and India, 20 PCPs for comparison with AMIE, and evaluations by specialist physicians and patient actors. AMIE demonstrated greater diagnostic accuracy and superior performance on 28 of 32 axes according to specialist physicians and 24 of 26 axes according to patient actors. This research has several limitations and should be interpreted with appropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale LLM-patient interactions but is not representative of usual clinical practice. While further research is required before AMIE could be translated to real-world settings, the results represent a milestone towards conversational diagnostic AI.

2)	11 Jan, NYU and UC Berkeley published a [paper](https://arxiv.org/pdf/2401.06209.pdf) “Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs”. Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). This research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, the researchers explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. The researchers identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, the study constructs the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. The paper further evaluates various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, the researchers propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, the research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems. The project link is [here](https://tsb0601.github.io/mmvp_blog/).



**14 Jan 2024**

1.	12 Jan, according to [TheIntercept](https://theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/), OPENAI QUIETLY DELETES BAN ON USING CHATGPT FOR “MILITARY AND WARFARE”, The Pentagon has its eye on the leading AI company, which this week softened its ban on military use. The article indicates that OpenAI quietly removed language from its usage policy that explicitly prohibited the use of its technology for military purposes. The updated policy, which aimed to be clearer and more readable, retains an injunction not to use the service to harm oneself or others and gives "develop or use weapons" as an example. However, the specific ban on "military and warfare" use has been removed. OpenAI stated that any use of its technology, including by the military, to develop or use weapons, injure others, or engage in unauthorized activities violating the security of any service or system, is disallowed. Critics argue that the shift may indicate a silent weakening of OpenAI's stance against doing business with militaries, especially given the company's close partnership with Microsoft, a major defense contractor. The change comes as militaries worldwide explore the incorporation of machine learning techniques, with the Pentagon tentatively exploring the use of OpenAI's models for analysis and decision-making.

2.  12 Jan, Anthropic published a [paper](https://arxiv.org/pdf/2401.05566.pdf) "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training". The researchers believe humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive
strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, the paper constructs proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, the researchers train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. The researchers find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, the study finds that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. The results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a
false impression of safety.

3.	11 Jan, IST Austria published a [paper](https://arxiv.org/pdf/2401.04679.pdf) “RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation”. The research investigates parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). The paper presents a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains low-rank and highly-sparse components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, the study shows that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. The research provides system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memory- and computationally-efficient training.

4.	10 Jan, Google published a [paper](https://arxiv.org/pdf/2401.04858.pdf) “User Embedding Model for Personalized Language Prompting”. The paper states that Modeling long histories plays a pivotal role in enhancing recommendation systems, allowing to capture user's evolving preferences, resulting in more precise and personalized recommendations. This study tackles the challenges of modeling long user histories for preference understanding in natural language. Specifically, the research introduces a new User Embedding Module (UEM) that efficiently processes user history in free-form text by compressing and representing them as embeddings, to use them as soft prompts to a LM. Experiments demonstrate the superior capability of this approach in handling significantly longer histories compared to conventional text based prompting methods, yielding substantial improvements in predictive performance. The main contribution of this research is to demonstrate the ability to bias language models with user signals represented as embeddings.

5.	10 Jan, OpenAI launched [GPT Store](https://openai.com/blog/introducing-the-gpt-store) to help user find useful and popular custom versions of ChatGPT. The store features a diverse range of GPTs developed by partners and the community. Browse popular and trending GPTs on the community leaderboard, with categories like DALL·E, writing, research, programming, education, and lifestyle. Some of our first featured GPTs include: Personalized trail recommendations from AllTrails; Search and synthesize results from 200M academic papers with Consensus; Expand your coding skills with Khan Academy’s Code Tutor; Design presentations or social posts with Canva; Find your next read with Books; Learn math and science anytime, anywhere with the CK-12 Flexi AI tutor. It also encourage user publish their own GPT in the store simply by 1) Save one’s GPT for Everyone (Anyone with a link will not be shown in the store); 2) verify user Builder Profile (Settings → Builder profile → Enable your name or a verified website). In addition, Team and Enterprise customers can manage GPTs, a new ChatGPT Team plan for teams of all sizes. Team customers have access to a private section of the GPT Store which includes GPTs securely published to the team/enterprise workspace. The GPT Store will be available soon for ChatGPT Enterprise customers and will include enhanced admin controls like choosing how internal-only GPTs are shared and which external GPTs may be used inside business. Like all usage on ChatGPT Team and Enterprise, OpenAI do not use conversations with GPTs to improve our models.

6.	10 Jan, Thenewstack published an [article](https://thenewstack.io/7-guiding-principles-for-working-with-llms/) “7 Guiding Principles for Working with LLMs”. The article outlines seven guiding principles for effectively working with Large Language Models (LLMs), which have revolutionized programming. These principles are based on the author's experience in using LLMs, such as ChatGPT and Claude, for various tasks. The guiding principles are as follows: 1) Think Out Loud: Encourages verbalizing thoughts and discussions with LLMs to generate ideas, validate, refute, or clarify concepts. Applies to technical topics and extends to non-technical domains, fostering a habit of narrating work. 2) Never Trust, Always Verify: Advocates for verification of LLM-generated content, especially in technical domains, by either using tests or autonomous self-directed loops. Emphasizes the importance of human judgment and fact-checking when relying on LLMs for information. 3) Recruit a Team of Assistants: Suggests using multiple LLMs and coding assistants in different situations to leverage diverse insights and solutions. Highlights the benefits of comparing results and seeking a consensus from a team of LLMs. 4) Ask for Choral Explanations: Encourages seeking explanations from multiple LLMs on a given topic to gain diverse insights and understanding. Draws parallels with the concept of "Choral Explanations" on platforms like StackExchange and Quora. 5) Exploit Pattern Recognition: Acknowledges the complementary pattern recognition abilities of both humans and LLMs. Describes instances where LLMs excel in recognizing patterns in data, aiding in problem-solving and understanding. 6) Automate Transformations: Advocates using LLMs for pattern-based transformations in knowledge work, such as converting formats or summarizing data. Highlights the efficiency gains in automating mundane tasks and focusing on higher-order intellectual tasks. 7) Learn by Doing: Discusses the role of LLMs as effective teachers in facilitating on-demand learning during projects. Emphasizes the immediate and tangible nature of learning through task-oriented interactions with LLMs. 
The author concludes by noting that these principles serve as a guide in the evolving era of LLMs, recognizing that continued exploration and adaptation will be crucial as features and capabilities emerge in AI assistants.

7.	9 Jan, meta published a [paper](https://arxiv.org/pdf/2401.04577.pdf) “Masked Audio Generation using a Single Non-Autoregressive Transformer”. The paper introduces MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, the model predicts spans of masked tokens obtained from a masking scheduler, while during inference the model gradually constructs the output sequence using several decoding steps. To further enhance the quality of the generated audio, the researchers introduce a novel rescoring method in which, the researchers leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, the study explores a hybrid version of MAGNeT, in which the researchers fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. The researchers demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline). Through ablation studies and analysis, the research shed light on the importance of each of the components comprising MAGNeT, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality. Samples are available on the [demo page URL](https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT).
                                                                                                                                                                                                                            
8.	8 Jan, Princeton Uni. published a [paper](https://arxiv.org/pdf/2401.04151.pdf) “Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning”. The paper indicates that fine-tuning is the primary methodology for tailoring pre-trained large language models to specific tasks. As the model's scale and the diversity of tasks expand, parameter-efficient fine-tuning methods are of paramount importance. One of the most widely used family of methods is low-rank adaptation (LoRA) and its variants. LoRA encodes weight update as the product of two low-rank matrices. Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks. The study introduces Chain of LoRA (COLA), an iterative optimization framework inspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full parameter fine-tuning, without incurring additional computational costs or memory overheads. COLA employs a residual learning procedure where it merges learned LoRA modules into the pre-trained language model parameters and re-initilize optimization for new born LoRA modules. The paper provides theoretical convergence guarantees as well as empirical results to validate the effectiveness of the algorithm. Across various models (OPT and llama-2) and seven benchmarking tasks, the research demonstrates that COLA can consistently outperform LoRA without additional computational or memory costs.

9.	8 Jan, ABC published an [opinion article](https://www.abc.net.au/religion/university-research-funding-and-declining-public-trust/103293026) “Research funding has been politicised and universities are losing public trust — is this the year that will reverse those trends?”. The author reflects on the release of ATARs and the concerns of students, including their worries about gaining entry into tertiary courses and funding their studies in the context of the Jobs Ready Graduate package. The focus then shifts to the Australian Universities Accord, a significant review of higher education billed as a "once-in-a-generation opportunity." The Accord aims to reimagine Australian higher education and address challenges faced by universities, such as the long-term effects of COVID-19, budget constraints, and concerns about international student markets. The Accord's interim report highlighted five priority areas, with a final report containing 47 recommendations awaiting release. Parallelly, the Australian Research Council (ARC) underwent a review to address governance issues and political interference. The article emphasizes the need for increased investment in research, as universities face challenges related to funding gaps and public perceptions of their value. Public trust in universities has declined, partly influenced by the politicization of higher education, and the article underscores the importance of transparent communication about the funding systems and the societal contributions of research. The Universities Accord and ARC Review are seen as opportunities to address these issues and rebuild public confidence.

10.	8 Jan, Mistral published a [paper](https://arxiv.org/pdf/2401.04088.pdf) “Mixtral of Experts”. The paper introduces Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. The paper also provides a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.


**7 Jan 2024**
1.	4 Jan, Google published a [paper](https://arxiv.org/pdf/2401.02412.pdf) “LLM Augmented LLMs: Expanding Capabilities through Composition”. The paper points out that foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. This research studies the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. The researchers illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13\% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, there is a relative improvement of 40\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.

2.  4 Jan, Google and DeepMind published a [paper](https://arxiv.org/pdf/2401.02412.pdf) “LLM Augmented LLMs: Expanding Capabilities through Composition”. The paper states that foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. This research studies the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. The paper proposes CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. The researchers illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13\% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, there is a relative improvement of 40\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.

3. 4 Jan, researchers from Uni. of Hongkong published a [paper](https://arxiv.org/pdf/2401.02415.pdf) “LLaMA Pro: Progressive LLaMA with Block Expansion”. The research point out that humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, the study proposes a new post-pretraining method for LLMs with an expansion of Transformer blocks. The researchers tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. This research experiments on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. The findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.

4.  4 Jan, University of Cambridge and College London published a [paper](https://arxiv.org/pdf/2401.02994.pdf) “Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM”. In conversational AI research, there's a noticeable trend towards developing models with a larger number of parameters, exemplified by models like ChatGPT. While these expansive models tend to generate increasingly better chat responses, they demand significant computational resources and memory. This study explores a pertinent question: Can a combination of smaller models collaboratively achieve comparable or enhanced performance relative to a singular large model? The paper introduces an approach termed "blending", a straightforward yet effective method of integrating multiple chat AIs. The empirical evidence suggests that when specific smaller models are synergistically blended, they can potentially outperform or match the capabilities of much larger counterparts. For instance, integrating just three models of moderate size (6B/13B paramaeters) can rival or even surpass the performance metrics of a substantially larger model like ChatGPT (175B+ paramaters). This hypothesis is rigorously tested using A/B testing methodologies with a large user base on the Chai research platform over a span of thirty days. The findings underscore the potential of the "blending" strategy as a viable approach for enhancing chat AI efficacy without a corresponding surge in computational demands.

5.	4 Jan, MIT Technology Review published [an article](https://www.technologyreview.com/2024/01/04/1086046/whats-next-for-ai-in-2024/) “What’s next for AI in 2024”.  The article claims that last year's predictions on AI trends largely held true, and now the focus is on 2024. The anticipated trends include: 1) Customized Chatbots: Google and OpenAI are making AI more accessible to the non-tech user. In 2024, people might create personalized chatbots with ease, utilizing multimodal capabilities of models like GPT-4 and Gemini. This could lead to practical applications in various fields, such as real estate. 2) Generative AI's Second Wave in Video: The next frontier for generative AI is text-to-video. Tools like Runway's Gen-2 are producing high-quality video content, catching the attention of major studios like Paramount and Disney. The impact extends beyond entertainment, with applications in marketing and training, raising concerns about the changing landscape of filmmaking and actors' roles. 3) AI-Generated Election Disinformation: With the rise of AI, the risk of AI-generated disinformation during elections is growing. Examples from Argentina, Slovakia, and the US illustrate how politicians are leveraging AI to create misleading content. This trend poses a challenge to recognizing real information online and could have severe consequences in polarized political climates. 4) Robots That Multitask: Inspired by generative AI techniques, roboticists are developing more general-purpose robots capable of multitasking. Models like DeepMind's Robocat and RT-X aim to enable robots to perform a range of tasks without specialized training. Challenges include the scarcity of diverse data sources for robot training, but initiatives like data collection from volunteers and large datasets from companies are addressing this issue.
These trends reflect ongoing efforts to make AI more accessible, versatile, and impactful, but they also raise concerns related to reliability, ethical considerations, and potential misuse. The coming year is expected to be pivotal in addressing these challenges.

6.	3rd Jan, Codec Avatars, Meta and UCLA published a [paper](https://arxiv.org/pdf/2401.01885.pdf) “From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations”. The paper presents a framework for generating full-bodied photorealistic avatars that gesture according to the conversational dynamics of a dyadic interaction. Given speech audio, the proposed model outputs multiple possibilities of gestural motion for an individual, including face, body, and hands. The key behind the method is in combining the benefits of sample diversity from vector quantization with the high-frequency details obtained through diffusion to generate more dynamic, expressive motion. The researchers visualize the generated motion using highly photorealistic avatars that can express crucial nuances in gestures (e.g. sneers and smirks). To facilitate this line of research, the study introduces a first-of-its-kind multi-view conversational dataset that allows for photorealistic reconstruction. Experiments show the model generates appropriate and diverse gestures, outperforming both diffusion- and VQ-only methods. Furthermore, the perceptual evaluation highlights the importance of photorealism (vs. meshes) in accurately assessing subtle motion details in conversational gestures. Code and dataset available [online](https://github.com/facebookresearch/audio2photoreal/).

7.	3rd Jan, Forbes published an [article](https://www.forbes.com/sites/jodiecook/2024/01/03/6-strategies-for-better-results-from-chatgpt-according-to-openai/?sh=7ee27a6c66ad) “6 Strategies For Better Results From ChatGPT, According To OpenAI”. The six strategies are 1) Write clear instructions – make the prompt super simple and instruct as if you were explaining a task to a junior member of the team, asking the model to adopt a persona, using line breaks or extra formatting to clearly indicate distinct parts, specifying the steps required to complete a text, providing examples, and specifying the desired length of the output; 2) Provide reference text - instructing the model to answer using a reference text, making absolutely sure it uses the text’s content in the response; 3) Split complex tasks - ask something, then use a key part of the response to ask the next thing. Don’t lose context in complicated prompts, don’t leave ChatGPT to decide which parts of your request are important; 4) Give the model time - asking for a ‘chain of thought’ before an answer can help the model reason its way toward correct answers more reliably; 5) Use external tools - upload spreadsheets or documents and ask for information based on the stats and numbers, ChatGPT will call tools do the job more reliably and efficiently; 6)  Test changes systematically - in some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples, it may be necessary to define a “comprehensive test suite,” making incremental changes to ensure the results mean what you think they do.

8.	2nd Jan, UCLA published a [paper](https://arxiv.org/pdf/2401.01335.pdf) “Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models”. The researchers indicate that harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). The paper delves into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. The research proposes a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. The proposed method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, the researchers prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution. Empirically, the researchers evaluate the method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Experimental results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents.

9.	2nd Jan, Forbes published an [article](https://www.forbes.com/sites/janakirammsv/2024/01/02/exploring-the-future-5-cutting-edge-generative-ai-trends-in-2024/?sh=791d4ac1206e) “Exploring The Future: 5 Cutting-Edge Generative AI Trends In 2024”. The articles states that as we delve into 2024, generative AI is poised for significant evolution, bringing forth trends that promise to reshape technology. Key trends include the emergence of multimodal AI models, going beyond text to incorporate diverse data types like images, language, and audio. Small Language Models (SLMs) will gain prominence, trained on high-quality datasets and offering comparable content quality to larger counterparts with lower resource requirements. Autonomous agents, employing advanced algorithms and machine learning, will contribute to AI models producing content without extensive human intervention. Open generative AI models are anticipated to approach proprietary models in performance, narrowing the gap and providing viable alternatives for enterprises. Additionally, the cloud-native approach, particularly leveraging Kubernetes, is set to become crucial for hosting generative AI models, with frameworks and tools maturing to efficiently manage the entire lifecycle of foundation models. The year 2024 is expected to witness rapid evolution in generative AI, bringing forth novel capabilities for consumers and enterprises alike.

10.	1st Jan, TheNewStack published an [article](https://thenewstack.io/open-source-in-2024-more-volatility-more-risk-more-ai/) “Open Source in 2024: More Volatility, More Risk, More AI”. The article argues that the open-source landscape in 2024 may witness more companies transitioning from open source licenses to business licenses, increased regulatory actions, and continued evolution in generative AI. Despite economic uncertainty and recent developments, there is cautious optimism among industry insiders. The shift in licensing models, exemplified by HashiCorp's move to the Business Source License, has sparked discussions about the future of open source and the role of foundations. Additionally, the potential impact of regulatory frameworks on AI, especially generative AI, is gaining attention. The year 2024 is expected to bring forth challenges and opportunities, prompting the industry to reevaluate licensing and foster greater collaboration.

11.	1st Jan, Google and Harvard University published a [paper](https://arxiv.org/pdf/2401.00935.pdf) “Boundary Attention: Learning to Find Faint Boundaries at Any Resolution”. The study presents a differentiable model that explicitly models boundaries -- including contours, corners and junctions -- using a new mechanism called boundary attention. The paper shows that the model provides accurate results even when the boundary signal is very weak or is swamped by noise. Compared to previous classical methods for finding faint boundaries, the proposed model has the advantages of being differentiable; being scalable to larger images; and automatically adapting to an appropriate level of geometric detail in each part of an image. Compared to previous deep methods for finding boundaries via end-to-end training, it has the advantages of providing sub-pixel precision, being more resilient to noise, and being able to process any image at its native resolution and aspect ratio. The project is available here.

12.	1st Jan, researchers from Monash Uni, CSIRO etc published a [paper](https://arxiv.org/pdf/2401.00788.pdf) “A STRAIOS: Parameter-Efficient Instruction Tuning Code Large Language Models”. The research finds that The high cost of full-parameter fine-tuning (FFT) of Large Language Models (LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods. However, it remains unclear which methods provide the best cost-performance trade-off at different model scales. This paper introduces ASTRAIOS, a suite of 28 instruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up to 16 billion parameters. Through investigations across 5 tasks and 8 different datasets encompassing both code comprehension and code generation tasks, the study finds that FFT generally leads to the best downstream performance across all scales, and PEFT methods differ significantly in their efficacy based on the model scale. LoRA usually offers the most favorable trade-off between cost and performance. Further investigation into the effects of these methods on both model robustness and code security reveals that larger models tend to demonstrate reduced robustness and less security. At last, the researchers explore the relationships among updated parameters, cross-entropy loss, and task performance. We find that the tuning effectiveness observed in small models generalizes well to larger models, and the validation loss in instruction tuning can be a reliable indicator of overall downstream performance.

13.	31 Dec, JPMorgan AI Research published a [paper](https://arxiv.org/pdf/2401.00908v1.pdf) “DocLLM: A layout-aware generative language model for multimodal document understanding”. The study indicates that enterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. This paper presents DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. The proposed model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows to address irregular layouts and heterogeneous content frequently encountered in visual documents. The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks. The researchers demonstrate that the solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets. However, this is not an open-source project, no code or pretrained models are available.

14.	29 Dec, Upstage AI published a [paper](https://arxiv.org/pdf/2312.15166.pdf) “SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling”. The paper introduces SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. Inspired by recent efforts to efficiently up-scale LLMs, the researchers present a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining. In contrast to other LLM up-scaling methods that use mixture-of-experts, DUS does not require complex changes to train and inference efficiently. The study shows experimentally that DUS is simple yet effective in scaling up high-performance LLMs from small ones. Building on the DUS model, the paper also presents SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, surpassing Mixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0 license, promoting broad access and application in the LLM field. 

15.	29 Dec, Stanford Uni and Meta published a [paper](https://arxiv.org/pdf/2312.17661.pdf) “Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models”. The study indicates that the burgeoning interest in Multimodal Large Language Models (MLLMs), such as OpenAI's GPT-4V(ision), has significantly impacted both academic and industrial realms. These models enhance Large Language Models (LLMs) with advanced visual understanding capabilities, facilitating their application in a variety of multimodal tasks. Recently, Google introduced Gemini, a cutting-edge MLLM designed specifically for multimodal integration. Despite its advancements, preliminary benchmarks indicate that Gemini lags behind GPT models in commonsense reasoning tasks. However, this assessment, based on a limited dataset (i.e., HellaSWAG), does not fully capture Gemini's authentic commonsense reasoning potential. To address this gap, this study undertakes a thorough evaluation of Gemini's performance in complex reasoning tasks that necessitate the integration of commonsense knowledge across modalities. The researchers carry out a comprehensive analysis of 12 commonsense reasoning datasets, ranging from general to domain-specific tasks. This includes 11 datasets focused solely on language, as well as one that incorporates multimodal elements. The experiments across four LLMs and two MLLMs demonstrate Gemini's competitive commonsense reasoning capabilities. Additionally, the research identifies common challenges faced by current LLMs and MLLMs in addressing commonsense problems, underscoring the need for further advancements in enhancing the commonsense reasoning abilities of these models.

1)	21 Dec, Science published an [article](https://www.science.org/doi/full/10.1126/science.adm9788) “AI is transforming how science is done. Science education must reflect this change”. The discussion on artificial intelligence (AI) in science education often focuses on its role in achieving science learning objectives, but there's a crucial aspect receiving insufficient attention—how AI is transforming the nature of science (NOS). AI influences not only teaching tools but also the fundamental aspects of scientific inquiry. The advantages of AI include personalized education through simulations and tailored content, while concerns involve potential drawbacks like reliance on AI for composing homework. The shift in focus from memorization to contemporary learning skills, such as critical thinking and innovation, challenges AI's ability to replicate sophisticated human skills. AI is already reshaping professional science, influencing hypothesis generation, experiment design, and data interpretation. Recommendations for responsible AI use in scientific research stress transparency, risk management, and participatory methods. This shift in scientific practices necessitates a comprehensive approach to science education reform, including curriculum restructuring, teacher training, and new teaching tools to align with the evolving landscape of AI-informed science.



**31 Dec 2023**

1.	31 Dec, according to [theinformation](https://www.theinformation.com/articles/openais-annualized-revenue-tops-1-6-billion-as-customers-shrug-off-ceo-drama), in 2023, “OpenAI’s Annualized Revenue Tops $1.6 Billion as Customers Shrug Off CEO Drama”. The article also states that US tech shares increased 43%, while Nvidia is the biggest winner with its share increased 239%.  Winners of best CEO include Jensen Huang from Nvidia, Mark Zukerberg from Meta, Sekiya Kazuma from Disco. The article also predicts what could happen in 2024. This includes the OpenAI and Microsoft relationship may begin to fray; AI start-ups may go bankrupt or be acquired, or have financial difficulties; non-transformer  based models will getting popular; AI-generated fake news will impact US election; Lawyers and policy maker will influence AI; capabilities of AI will continue increasing, especially the reasoning capabilities. 

2.	29 Dec, according to [WestObserver](https://westobserver.com/business/the-ai-revolutions-first-year-has-anything-changed/), The AI revolution’s first year: has anything changed? The article indicates that Microsoft's Satya Nadella and Google's Sundar Pichai likened the impact of generative AI to past revolutionary technologies. Erik Brynjolfsson from Stanford University anticipated a productivity boom, but skepticism emerged about practical usability - Generative AI, relying on large language models, excelled in creativity but struggled with accuracy, raising doubts about its game-changing potential. Some industry insiders acknowledged limitations, suggesting it might not be as revolutionary as hoped. While generative AI fueled a tech stock rally, doubts lingered about its actual impact. Nvidia, leading in AI chip manufacturing, experienced a significant stock market value increase. Private investment in AI startups reached $27 billion. Concerns about AI's slow adoption surfaced, with companies cautious about the technology's practicality and potential inaccuracies. Issues included the need for a "human in the loop" to catch mistakes and linking models to factual databases for accurate responses. Despite a surge in AI-related investments, experts anticipated slow adoption due to a lack of customer preparedness, technical knowhow, and cost concerns. Microsoft's $30 monthly charge for AI in its Office suite faced criticism, potentially limiting widespread adoption. Forecasts indicated that spending on generative AI in 2024 might be modest, around $20 billion, representing 0.5% of total global IT spending. Slow adoption was attributed to challenges such as inaccuracies, lack of customer readiness, and high costs. Industry optimism persisted, with claims that generative AI could surpass the adoption speed of past technologies. Analysts predicted increased AI spending, reaching over 2.5% of GDP by 2032. The lack of a highly monetized killer app for consumers led to predictions that business use would drive AI monetization, possibly mirroring the enterprise adoption of cloud computing. Tech companies might benefit from AI, increasing productivity and potentially reducing hiring. Early research suggested significant productivity gains with generative AI, such as a 14% increase for call center workers. The technology's "viral" adoption among workers and its potential to reshape work processes raised hopes for a transformative shift in 2024. 

3.	26 Dec, [Quartz published an article](https://qz.com/in-a-bid-to-break-free-from-openai-companies-are-build-1851112994) “In a bid to break free from OpenAI, companies are building their own custom AI chatbots”. The article argues that ompanies are increasingly developing their own custom AI chatbots instead of relying on OpenAI's dominant models like GPT-4. For instance, Salesforce has created smaller AI models, Einstein for Developers and Einstein for Flow, tailored to specific business tasks by training them on in-house programming data and open-source data. While larger models from OpenAI, Google, Amazon, and Meta are still evolving, businesses are exploring smaller, task-specific AI models to address niche applications more efficiently. Braden Hancock from Snorkel AI mentions that companies initially wondered if models like ChatGPT would suffice, but they realized modifications were needed for specific business needs. This trend suggests a potential shift towards adopting smaller, more specialized AI models for particular applications. Yoon Kim from MIT emphasizes the cost-effectiveness of focusing on specific applications rather than adopting broad out-of-the-box solutions like ChatGPT. The article also discusses potential scenarios for OpenAI, where the success of GPT-4 may depend on hardware cost reductions. Alternatively, an influx of large-language models (LLMs) could increase competition, prompting OpenAI to advocate for more regulation to stay ahead in the AI landscape.

4.	24 Dec, according to [livemint.com](https://www.livemint.com/companies/news/google-to-let-go-of-30-000-employees-due-to-new-ai-innovation-heres-what-report-suggests-11703383333475.html), Google's ad sales unit is undergoing reorganization as the company implements AI-based tools for creating and suggesting ads. This move eliminates the need for specialized employees in selling ads for specific Google services. Notably, Google had launched its AI-powered campaign planner, Performance Max, in 2021, but the company decided to add generative AI-based capabilities to the ad tool at the Google I/O event earlier this year, making it easier to "create custom assets and scale them in a few clicks". The report says that Google is planning to reorganise a large part of its 30,000-strong ad sales unit as a result of the company's recent advances in artificial intelligence, according to a report by The Information. Notably, Google laid off around 12,000 employees earlier this year, making it the biggest job cut in the company's history.

5.	24 Dec, OpenAI CEO Sam Altman [posted on X](https://twitter.com/sama/status/1738673279085457661) a wishes list in 2024. The list include: AGI (a little patience please); PT-5; better voice mode; higher rate limits; better GPTs; better reasoning; control over degree of wokeness/behaviour; video; personalization; better browsing; 'sign in with openai'; open source.

=====================================================
1)	21 Dec, FAR AI published a [paper](https://arxiv.org/pdf/2312.14302.pdf) “Exploiting Novel GPT-4 APIs”. The researchers  find that language model attacks typically assume one of two extreme threat models: full white-box access to model weights, or black-box access limited to a text generation API. However, real-world APIs are often more flexible than just text generation: these APIs expose “gray-box” access leading to new threat vectors. To explore this, the researchers redteam three new functionalities exposed in the GPT-4 APIs: fine-tuning, function calling and knowledge retrieval. The researchers find that fine-tuning a model on as few as 15 harmful examples or 100 benign examples can remove core safeguards from GPT-4, enabling a range of harmful outputs. Furthermore, the study finds that GPT-4 Assistants readily divulge the function call schema and can be made to execute arbitrary function calls. Finally, the research finds that knowledge retrieval can be hijacked by injecting instructions into retrieval documents. These vulnerabilities highlight that any additions to the functionality exposed by an API can create new vulnerabilities.

2)	21 Dec, Tsinghua University and Zhipu AI published a [paper](https://arxiv.org/pdf/2312.14302.pdf) “CogAgent: A Visual Language Model for GUI Agents”. The study indicates that people are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. This paper introduces CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120*1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW, advancing the state of the art. The model and codes are available at [this https URL](https://github.com/THUDM/CogVLM)

3)	20 Dec, researchers from BAAI release a multimodal model [Emu2](https://github.com/baaivision/Emu), and also published a [paper](https://arxiv.org/pdf/2312.13286.pdf) “Generative Multimodal Models are In-Context Learners” introduced some details of the model. The researchers indicates that the human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. This study demonstrates that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. The researchers introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks.

4)	18 Dec, Huggingface [published a blog](https://huggingface.co/blog/2023-in-llms) summarized LLMs in 2023, the blog indicates that 1) 2023 has seen a rise of open releases from all kinds of actors (big companies, start ups, research labs), which empowered the community to start experimenting and exploring at a rate never seen before. 2) Model announcement openness has seen ebbs and flow, from early releases this year being very open (dataset mixes, weights, architectures) to late releases indicating nothing about their training data, therefore being unreproducible. 3) Open models emerged from many new places, including China, with several new actors positioning themselves as strong contenders in the LLM game. 4) Personalization possibilities reached an all-time high, with new strategies for fine-tuning (RLHF, adapters, merging), which are only at their beginning. 5) Smaller model sizes and upgrades in quantization made LLMs really accessible to many more people! 6) New architectures have also appeared - will they finally replace the Transformer?

5)	10 Dec, Microsoft published a [paper](https://arxiv.org/abs/2312.05934) “Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs”. The study indicates that Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. This research compares two common approaches: fine-tuning and retrieval-augmented generation (RAG). The researchers evaluate both approaches on a variety of knowledge-intensive tasks across different topics. The findings reveal that while fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, the study finds that LLMs struggle to learn new factual information through fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.


**24 Dec 2023**

1.	22 Dec, spectrum.ieee.org published a [paper](https://spectrum.ieee.org/quantum-computing-skeptics) “Quantum Computing’s Hard, Cold Reality Check  Hype is everywhere, skeptics say, and practical applications are still far away”. The paper points out that prominent figures in the quantum computing industry are expressing skepticism about the revolutionary potential and near-term impact of quantum computers. Despite being touted as solutions for various problems, including financial modeling and machine learning, skeptics argue that unrealistic expectations surround the technology. Criticisms include the high error rates of current quantum computers, challenges in achieving fault-tolerance, and limitations in the speed and scalability of quantum algorithms. Microsoft's Matthias Troyer suggests that practical applications for quantum computers may be limited to specific problems with exponential speed-ups, such as factoring large numbers and simulating quantum systems. The industry is urged to focus on realistic applications with the greatest chance of impact.

2.	21 Dec, Nature Communications published a [paper](https://www.nature.com/articles/s41467-023-43958-w) “Revealing hidden patterns in deep neural network feature space continuum via manifold learning”. The study found that Deep neural networks (DNNs) extract thousands to millions of task-specific features during model training for inference and decision-making. While visualizing these features is critical for comprehending the learning process and improving the performance of the DNNs, existing visualization techniques work only for classification tasks. For regressions, the feature points lie on a high dimensional continuum having an inherently complex shape, making a meaningful visualization of the features intractable. Given that the majority of deep learning applications are regression-oriented, developing a conceptual framework and computational method to reliably visualize the regression features is of great significance. This research introduces a manifold discovery and analysis (MDA) method for DNN feature visualization, which involves learning the manifold topology associated with the output and target labels of a DNN. MDA leverages the acquired topological information to preserve the local geometry of the feature space manifold and provides insightful visualizations of the DNN features, highlighting the appropriateness, generalizability, and adversarial robustness of a DNN. The performance and advantages of the MDA approach compared to the existing methods are demonstrated in different deep learning applications. Code is available on [Github](https://github.com/xinglab-ai/mda).

3.	20 Dec, CMU published a [paper on Nature](https://www.nature.com/articles/s41586-023-06792-0) “Autonomous chemical research with large language models”. The study indicates that Transformer-based large language models are making significant strides in various fields, such as natural language processing, biology, chemistry and computer programming. This study shows the development and capabilities of Coscientist, an artificial intelligence system driven by GPT-4 that autonomously designs, plans and performs complex experiments by incorporating large language models empowered by tools such as internet and documentation search, code execution and experimental automation. Coscientist showcases its potential for accelerating research across six diverse tasks, including the successful reaction optimization of palladium-catalysed cross-couplings, while exhibiting advanced capabilities for (semi-)autonomous experimental design and execution. The findings of the research demonstrate the versatility, efficacy and explainability of artificial intelligence systems like Coscientist in advancing research.

4.	20 Dec, University of Washington published a [paper](https://arxiv.org/pdf/2312.13401.pdf) “Time is Encoded in the Weights of Finetuned Language Models”. The study presents time vectors, a simple tool to customize language models to new time periods. Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model. This vector specifies a direction in weight space that, as the experiments show, improves performance on text from that time period. Time vectors specialized to adjacent time periods appearto be positioned closer together in a manifold. Using this structure, the researchers interpolate between time vectors to induce new models that perform better on intervening and future time periods, without any additional training. The paper demonstrates the consistency of the findings across different tasks, domains, model sizes, and time scales. The results suggest that time is encoded in the weight space of finetuned models.

5.	20 Dec, [quantamagazine](https://www.quantamagazine.org/the-biggest-discoveries-in-computer-science-in-2023-20231220/#:~:text=Video%3A%20In%202023%2C%20computer%20scientists,emerge%20from%20large%20language%20models.) published a review paper “The Year in Computer Science”. The paper listed the following biggest breakthroughs in computer science. 1) Artificial Intelligence (AI): AI’s ability to generate text and art improved, with large language models like ChatGPT leading the way. However, understanding their inner workings remains a challenge. Image generation systems also made strides, impressing with their artistic abilities. 2) P versus NP Problem: Researchers made progress on the “P versus NP” problem, a long-standing question about the nature of hard problems. The journey to understand what makes hard problems hard led to the development of a subfield called meta-complexity. 3) Individual Progress: Shor’s algorithm, a key component of quantum computing, received its first significant upgrade in nearly 30 years. Researchers also made progress in finding the shortest route through a network as fast as theoretically possible. Cryptographers linked AI to machine learning models and machine-generated content, revealing hidden vulnerabilities and messages. 4) Large Language Models: Large language models exhibited emergent behaviors, performing tasks that smaller models couldn’t, such as solving certain math problems. However, these models raised concerns due to their propensity to invent falsehoods, perpetuate social biases, and struggle with basic elements of human language. 5) Vector-driven AI: utilizing hyperdimensional vectors for representing concepts, emerged as an alternative to traditional neural networks. This approach offered efficiency, error handling, and increased interpretability, signaling a potential shift in AI methodologies. 6) Solving Negativity: A trio of researchers developed a nearly optimal algorithm for determining the shortest path in graphs with costs or rewards. Another algorithm enables precise comparison of mathematical objects, potentially extending to broader object types. Additional notable algorithmic breakthroughs include a novel method for computing prime numbers, the debunking of a persistent conjecture about information-limited algorithms, and an analysis revealing how an unconventional idea can enhance the efficiency of gradient descent algorithms, widely used in machine learning and other applications. 7) Hiding Secrets in AI: In a groundbreaking discovery at the intersection of cryptography and artificial intelligence, a team of computer scientists demonstrated the insertion of practically invisible backdoors into machine learning models, employing logic akin to top-notch encryption methods. While the focus was on simpler models, the implications for more complex AI systems remain uncertain. The findings suggest potential strategies for future systems to mitigate security vulnerabilities, reflecting a renewed interest in the collaborative growth of cryptography and AI. Security concerns, notably championed by Cynthia Rudin advocating interpretable models, and advancements by researchers like Yael Tauman Kalai, persist despite emerging quantum technology. Additionally, steganography demonstrated the achievement of perfect security in hiding messages within machine-generated media. 

6.	20 Dec, Tencent, Shanghai AI Lab, et al. published a [paper](https://arxiv.org/pdf/2312.12436.pdf) “A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise”. The paper indicates that the surge of interest towards Multi-modal Large Language Models (MLLMs), e.g., GPT-4V(ision) from OpenAI, has marked a significant trend in both academia and industry. They endow Large Language Models (LLMs) with powerful capabilities in visual understanding, enabling them to tackle diverse multi-modal tasks. Very recently, Google released Gemini, its newest and most capable MLLM built from the ground up for multi-modality. In light of the superior reasoning capabilities, can Gemini challenge GPT-4V’s leading position in multi-modal learning? This study presents a preliminary exploration of Gemini Pro’s visual understanding proficiency, which comprehensively covers four domains: fundamental perception, advanced cognition, challenging vision tasks, and various expert capacities. The researchers compare Gemini Pro with the state-of-the-art GPT-4V to evaluate its upper limits, along with the latest open-sourced MLLM, Sphinx, which reveals the gap between manual efforts and black-box systems. The qualitative samples indicate that, while GPT-4V and Gemini showcase different answering styles and preferences, they can exhibit comparable visual reasoning capabilities, and Sphinx still trails behind them concerning domain generalizability. Specifically, GPT-4V tends to elaborate detailed explanations and intermediate steps, and Gemini prefers to output a direct and concise answer. The quantitative evaluation on the popular MME benchmark, which is specifically designed for MLLM, also demonstrates the impressive multi-modal understanding performance of Gemini, and its potential to be a strong challenger to GPT-4V. An early investigation of Gemini also observes some common issues of MLLMs concerning visual understanding, logical reasoning, and prompting robustness, indicating that there still remains a considerable distance towards artificial general intelligence.

7.	19 Dec, Google published a [report](https://arxiv.org/pdf/2312.11805.pdf) on Gemini “Gemini: A Family of Highly Capable Multimodal Models”. This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that the most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks examined. Gemini team believes that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and researchers discuss the approach toward deploying them responsibly to users.

8.	19 Dec, Chinese Academy of Sciences published a [paper](https://arxiv.org/pdf/2312.11865.pdf) “Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach”. The study indicates that StarCraft II is a challenging benchmark for AI agents due to the necessity of both precise micro-level operations and strategic macro-awareness. Previous works, such as Alphastar and SCC, achieve impressive performance on tackling StarCraft II , however, still exhibit deficiencies in long-term strategic planning and strategy interpretability. Emerging large language model (LLM) agents, such as Voyage and MetaGPT, presents the immense potential in solving intricate tasks. Motivated by this, the researcher aim to validate the capabilities of LLMs on StarCraft II, a highly complex RTS game. To conveniently take full advantage of LLMs’ reasoning abilities, the researchers first develop a textual StratCraft II environment, called TextStarCraft II, which LLM agent can interact. Secondly, the studys proposes a Chain of Summarization method, including single-frame summarization for processing raw observations and multi-frame summarization for analyzing game information, providing command recommendations, and generating strategic decisions. The experiment consists of two parts: first, an evaluation by human experts, which includes assessing the LLMs’ mastery of StarCraft II knowledge and the performance of LLM agents in the game; second, the in-game performance of LLM agents, encompassing aspects like win rate and the impact of Chain of Summarization. Experiment results demonstrate that: 1. LLMs possess the relevant knowledge and complex planning abilities needed to address StarCraft II scenarios; 2. Human experts consider the performance of LLM agents to be close to that of an average player who has played StarCraft II for eight years; 3. LLM agents are capable of defeating the built-in AI at the Harder(Lv5) difficulty level.

9.	19 Dec, The University of Manchester published a [paper](https://arxiv.org/abs/2312.12141) “Exploring the Residual Stream of Transformers”. The study points out that transformer-based models have achieved great breakthroughs in recent years. However, there are many significant questions that have not been answered in the field of explaining the reason why the models have powerful outputs. We do not know how to locate the models' important parameters storing the knowledge for predicting the next word, and whether these parameters are stored on the same layer/module or different ones. Moreover, we do not understand the mechanism to merge the knowledge into the final embedding for next word prediction. In this paper, the researchers explore the residual stream of transformers to increase the interpretability. The study finds the mechanism behind residual connection is a direct addition function on before-softmax values, so the probabilities of tokens with larger before-softmax values will increase. Moreover, the researchers prove that using log probability increase as contribution scores is reasonable, and based on this the researchers can locate important parameters. Besides, the paper proposes a method to analyze how previous layers affect upper layers by comparing the inner products. The experimental results and case study show that the research can increase the interpretability of transformer-based models. Code is available on this [https URL](https://github.com/zepingyu0512/residualstream).

10.	18 Dec, CMU and Berri AI published a [paper](https://arxiv.org/abs/2312.11444) “An In-depth Look at Gemini’s Language Abilities”. The researchers indicates that the recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, the researchers do an in-depth exploration of Gemini's language abilities, making two contributions. First, the study provides a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, the study takes a closer look at the results, identifying areas where one of the two model classes excels. The researchers perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, the study finds that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that are benchmarked. The researchers further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. The paper also identifies areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found [here](https://github.com/neulab/gemini-benchmark).

11.	18 Dec, Google published a [paper](https://arxiv.org/pdf/2312.11441.pdf) “Social Learning: Towards Collaborative Learning with Large Language Models”. The paper introduces the framework of "social learning" in the context of large language models (LLMs), whereby models share knowledge with each other in a privacy-aware manner using natural language. The study presents and evaluates two approaches for knowledge transfer between LLMs. In the first scenario, it allows the model to generate abstract prompts aiming to teach the task. In the second approach, models transfer knowledge by generating synthetic examples. The paper evaluates these methods across diverse datasets and quantify memorization as a proxy for privacy loss. These techniques inspired by social learning yield promising results with low memorization of the original data. In particular, the study shows that performance using these methods is comparable to results with the use of original labels and prompts. The work demonstrates the viability of social learning for LLMs, establishes baseline approaches and highlights several unexplored areas for future work.

12.	18 Dec, OpenAI released its [Safety Preparedness](https://openai.com/safety/preparedness) strategy. “The study of frontier AI risks has fallen far short of what is possible and where we need to be. To address this gap and systematize our safety thinking, we are adopting the initial version of our Preparedness Framework. It describes OpenAI’s processes to track, evaluate, forecast, and protect against catastrophic risks posed by increasingly powerful models.”


**17 Dec 2023**

1.	15 Dec, a group of researchers, including Ilya Sutskever from OpenAI published a [paper](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf) “WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION”. The authors indicate that widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior—for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. The researchers study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? The study tests this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. The authors find that when the researchers naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon the authors call weak-to-strong generalization. However, the researchers are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. The authors find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, the researchers can recover close to GPT-3.5-level performance on NLP tasks. The experimental results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.

2.	14 Dec, [Nature published a paper](https://www.nature.com/articles/s41586-023-06924-6) from DeepMind, “Mathematical discoveries from program search with large language models”. The study indicates that Large Language Models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations) which can result in them making plausible but incorrect statements. This hinders the use of current large models in scientific discovery. The researchers introduce FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pre-trained LLM with a systematic evaluator. The paper demonstrates the effectiveness of this approach to surpass the best known results in important problems, pushing the boundary of existing LLM-based approaches. Applying FunSearch to a central problem in extremal combinatorics — the cap set problem — the researchers discover new constructions of large cap sets going beyond the best known ones, both in finite dimensional and asymptotic cases. This represents the first discoveries made for established open problems using LLMs. The study showcases the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve upon widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications.

3.	14 Dec, [Google announced the upgrade](https://cloud.google.com/blog/products/ai-machine-learning/imagen-2-on-vertex-ai-is-now-generally-available) of Google Cloud’s image-generation capabilities with Imagen 2, its most advanced text-to-image technology, which is now generally available for Vertex AI customers on the allowlist (i.e., approved for access). Imagen 2 on Vertex AI allows customers to customize and deploy Imagen 2 with intuitive tooling, fully-managed infrastructure, and built-in privacy and safety features. Developed using Google DeepMind technology, Imagen 2 delivers significantly improved image quality and a host of features that enable developers to create images for their specific use case, including: 1) Generating high-quality, photorealistic, high-resolution, aesthetically pleasing images from natural language prompts 2) Text rendering in multiple languages to create images with accurate text overlays 3) Logo generation to create company or product logos and overlay them in images 4) Visual question and answering for generating captions from images, and for getting informative text responses to questions about image details

4.	13 Dec, [according to tom’s guide](https://www.tomsguide.com/news/breaking-news-ai-avatars-entirely-replace-human-newscasters-for-the-first-time), AI avatars entirely replace human newscasters for the first time. Channel 1, a newly created news channel, introduces a groundbreaking concept with entirely AI-generated anchors and reporters. The technology and media startup, founded by Adam Mosam and producer Scott Zabielski, aims for global distribution with translated content reflecting viewer interests. The promotional video suggests AI involvement in story selection and editorial decisions, with some segments entirely AI-generated. The news sources include independent journalists, AI-generated news, and reporting from an external agency. Human involvement remains integral, with trained editors overseeing editing and production. Accuracy is emphasized, with transparency about AI modifications, indicated by an on-screen icon. The goal is to showcase off-camera reporters and replace teleprompter-reading anchors, maintaining involvement from human journalists. The show combines real news footage, human-generated stories, and AI-generated anchors, equipped with a ChatGPT-like model for personality and flexibility. Channel 1 is set to launch early next year on various platforms, allowing viewers to customize their experience. The challenge, as noted by Mosam, lies in navigating AI's impact on the industry while ensuring clarity about what is real and what isn't.

5.	13 Dec, [Nature announce  its Nature’s 10 2023](https://www.nature.com/immersive/d41586-023-03919-1/index.html). The Nature’s 10 list explores key developments in science over the past year and some of the individuals who helped to make amazing discoveries and bring attention to crucial issues. It is not an award or a ranking, but a selection compiled by Nature’s editors to highlight the year’s most compelling stories. Since its inception more than a decade ago, Nature’s 10 has highlighted the influence of 10 people within the world of science. Nature is continuing with that tradition in 2023 and is adding to it by including a non-person – an acknowledgement of the role that artificial intelligence designed to mimic human language is having in the development and progress of science. The Nature’s 10 are: Kalpana Kalahasti: To the Moon; Marina Silva: Amazon protector; Katsuhiko Hayashi: Rewiring reproduction; Annie Kritcher: Fusion igniter; Eleni Myrivili: Warming warden; Ilya Sutskever: AI visionary; James Hamlin: Superconductivity sleuth; Svetlana Mojsov: Unsung drug developer; Halidou Tinto: Malaria fighter; Thomas Powles: Cancer explorer; ChatGPT: Boon and burden?

6.	13 Dec, [according to OpenAI](https://openai.com/blog/axel-springer-partnership), Axel Springer is the first publishing house globally to partner with OpenAI on a deeper integration of journalism in AI technologies. Axel Springer and OpenAI have announced a global partnership to strengthen independent journalism in the age of artificial intelligence (AI). The initiative will enrich users’ experience with ChatGPT by adding recent and authoritative content on a wide variety of topics, and explicitly values the publisher’s role in contributing to OpenAI’s products. This marks a significant step in both companies’ commitment to leverage AI for enhancing content experiences and creating new financial opportunities that support a sustainable future for journalism. With this partnership, ChatGPT users around the world will receive summaries of selected global news content from Axel Springer’s media brands including POLITICO, BUSINESS INSIDER, and European properties BILD and WELT, including otherwise paid content. ChatGPT’s answers to user queries will include attribution and links to the full articles for transparency and further information. In addition, the partnership supports Axel Springer’s existing AI-driven ventures that build upon OpenAI’s technology. The collaboration also involves the use of quality content from Axel Springer media brands for advancing the training of OpenAI’s sophisticated large language models.    

7.	13 Dec, [according to Reuters](https://www.reuters.com/technology/google-cloud-partners-with-mistral-ai-generative-language-models-2023-12-13/), Google Cloud and Mistral AI are partnering to allow the Paris-based generative AI startup to distribute its language models on the tech giant's infrastructure, the companies jointly announced on Wednesday. "As part of the agreement, Mistral AI will use Google Cloud’s AI-optimized infrastructure, including TPU Accelerators, to further test, build, and scale up its LLMs (large language models), all while benefiting from Google Cloud's security and privacy standards," the joint statement said. Mistral AI - founded by former Meta and Google AI researchers - said on Monday it had raised 385 million euros ($415 million) in its second funding round in seven months, led by investors such as Andreessen-Horowitz and LightSpeed Ventures.

8.	12 Dec, Google published a [paper](https://arxiv.org/pdf/2312.06585.pdf) “Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models”. The study indicates that fine-tuning language models~(LMs) on human-generated data remains a prevalent practice. However, the performance of such models is often limited by the quantity and diversity of high-quality human data. This paper explores whether the researchers can go beyond human data on tasks where they have access to scalar feedback, for example, on math problems where one can verify correctness. To do so, the research investigates a simple self-training method based on expectation-maximization, which is called ReSTEM, where the researchers (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. Testing on advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, the research finds that ReSTEM scales favorably with model size and significantly surpasses fine-tuning only on human data. Overall, the findings suggest self-training with feedback can substantially reduce dependence on human-generated data.

9.	12 Dec, Brain Roemmele [post on X](https://twitter.com/BRIANROEMMELE/STATUS/1734333713381753165) a charts. The chart shows the rise of open source local models are on the path to overtake massive (and expensive) cloud based closed models. This is perhaps one of the most important charts on AI for 2024. 

10.	11 Dec, Petuum, MBZUAI, CMU, UCSD and UIUC published a [paper](https://arxiv.org/pdf/2312.06550.pdf) “LLM360: Towards Fully Transparent Open-Source LLMs”. The researchers point out that the recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, the researchers release two 7B parameter LLMs pre-trained from scratch, AMBER and CRYSTALCODER, including their training code, data, intermediate checkpoints, and analyses (at llm360.ai). The study committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.

11.	11 Dec, Nvidia [published a blog](https://blogs.nvidia.com/blog/2024-ai-predictions/) where 15 AI experts predict that digital twins and generative AI are set to advance enterprise goals and consumer needs even as the world enters a third year of planning uncertainty. Enterprises in 2023, despite economic uncertainties, are focusing on leading, innovating, and problem-solving, with AI emerging as a central tool. A Gartner survey indicates that 54% of enterprise AI projects move from pilot to production, with 80% of executives emphasizing the broad applicability of automation. Businesses aim to do more with less, using cloud-based integrated software and hardware for cost-effective AI development. Expert predictions include the rise of digital twins, generalist AI agents, and software breaking down AI silos. Additionally, there's a shift towards unified AI pipelines, the practical implementation of generative AI, and leveraging AI for advancements in healthcare, surgery simulations, autonomous vehicles, and the metaverse. The AI landscape in 2023 also envisions improvements in risk management, cloud-first approaches for financial services, and a focus on energy-efficient computing. The year ahead anticipates a transformative impact on diverse sectors, from retail and supply chain optimization to cybersecurity and AI-powered energy grids.

12.	11 Dec [NeurIPS 2023](https://neurips.cc/) announced best paper awards.
    
-	Two outstanding main track papers: 
a)	Privacy Auditing with One (1) Training Run (Thomas Steinke · Milad Nasr · Matthew Jagielski)
b)	Are Emergent Abilities of Large Language Models a Mirage? (Rylan Schaeffer · Brando Miranda · Sanmi Koyejo)
-	Outstanding Main Track Runner-Ups:  
a)	Scaling Data-Constrained Language Models (Niklas Muennighoff · Alexander Rush · Boaz Barak · Teven Le 
Scao · Nouamane Tazi · Aleksandra Piktus · Sampo Pyysalo · Thomas Wolf · Colin Raffel)
b)	Direct Preference Optimization: Your Language Model is Secretly a Reward Model (Rafael Rafailov · Archit Sharma · Eric Mitchell · Christopher D Manning · Stefano Ermon · Chelsea Finn)
-	Outstanding Datasets and Benchmarks Papers
  a)	ClimSim: A large multi-scale dataset for hybrid physics-ML climate emulation (Sungduk Yu · Walter Hannah · Liran Peng · Jerry Lin · Mohamed Aziz Bhouri · Ritwik Gupta et al.)
  b)	DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models (Boxin Wang · Weixin Chen · Hengzhi Pei · Chulin Xie · Mintong Kang · Chenhui Zhang · Chejian Xu · Zidi Xiong · Ritik Dutta · Rylan Schaeffer · Sang Truong · Simran Arora · Mantas Mazeika · Dan Hendrycks · Zinan Lin · Yu Cheng · Sanmi Koyejo · Dawn Song · Bo Li)

-	Test of Time: [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html) (Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, Jeff Dean)

11.	10 Dec, [EMNLP 2023](https://twitter.com/emnlpmeeting) announced the best paper awards. 
- Best Long Paper: [Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning](https://aclanthology.org/2023.emnlp-main.609.pdf) (Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun) 
- Best Industry Paper: [Personalized Dense Retrieval on Global Index for Voice-enabled Conversational Systems](https://aclanthology.org/2023.emnlp-industry.9.pdf) (Masha Belyi, Charlotte Dzialo, Chaitanya Dwivedi, Prajit Muppidi, Kanna Shimizu) 
- Best Theme Paper: [Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition](https://aclanthology.org/2023.emnlp-main.302.pdf) (Sander Schulhoff, Jeremy Pinto et al.) 
- Best Short Paper: [Faster Minimum Bayes Risk Decoding with Confidence-based Pruning](https://aclanthology.org/2023.emnlp-main.767.pdf) (Julius Cheng, Andreas Vlachos) 
- Best Paper Demo: PaperMage: [A Unified Toolkit for Processing, Representing, and Manipulating Visually-Rich Scientific Documents](https://aclanthology.org/events/emnlp-2023/) (Kyle Lo, Zejiang Shen, Benjamin Newman, Joseph Chang et al.) 
- There are four outstanding papers: 

  a) Ties Matter: [Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration](https://aclanthology.org/2023.emnlp-main.848.pdf) (Daniel Deutsch, George Foster, Markus Freitag),
 
  b) The Sentiment Problem: [A Critical Survey towards Deconstructing Sentiment Analysis](https://aclanthology.org/2023.emnlp-main.848.pdf) (Pranav Narayanan Venkit, Mukund Srinath, Sanjana Gautam, Saranya Venkatraman, Vipul Gupta, Rebecca J. Passonneau, Shomir Wilson) 

  c) [Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models](https://aclanthology.org/2023.emnlp-main.658.pdf) (Jirui Qi, Raquel Fernández, Arianna Bisazza)

  d) [Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers](https://aclanthology.org/2023.emnlp-main.513.pdf) (Hosein Mohebbi, Grzegorz Chrupała, Willem Zuidema, Afra Alishahi)


**10 Dec 2023**

1.	8 Dec, [Together.ai announced in a blog](https://www.together.ai/blog/stripedhyena-7b) the released StripedHyena-7B. Together Research has unveiled the StripedHyena models, including the base model StripedHyena-Hessian-7B (SH 7B) and the chat model StripedHyena-Nous-7B (SH-N 7B), as alternatives to the popular Transformer architecture. StripedHyena shows competitive performance on short and long-context tasks, outperforming optimized Transformers in speed and memory efficiency for training, fine-tuning, and generation. The architecture is a hybrid of attention and gated convolutions, breaking away from the traditional Transformer design. The models exhibit improved scaling laws and are optimized using model grafting techniques. StripedHyena-Nous-7B, a chat model, is introduced in collaboration with Nous Research. The article emphasizes the potential for further exploration in architecture design beyond Transformers, hinting at future developments like larger models, multi-modal support, and enhanced performance optimizations. The research aims to inspire the open-source community to delve into diverse architectures.
NOTE: Gemini still struggling with reasoning questions, for example, if asked “If there are 10 books in a room and I read 2, how many books are still in the room?”, Bard, which is powered by Gemini, still says there are 8 left. This question is answered correctly by Microsoft’s BingChat since it was powered by GPT4.

2.	8 Dec, [according to CNBC](https://www.cnbc.com/2023/12/08/google-faces-controversy-over-edited-gemini-ai-demo-video.html), “Google faces controversy over edited Gemini AI demo video”. The article reports that Google is under scrutiny for the demonstration video of its recently launched AI model, Gemini. The six-minute video showcased impressive capabilities, including voice conversations and object recognition. However, it was later revealed that the demo was not conducted in real time but used still images and text prompts. Google confirmed this after facing criticism, stating that the video was an illustrative depiction of Gemini's possibilities. The company faces déjà vu, as it had previously received criticism for a rushed AI chatbot demonstration earlier in the year. Google is in competition with Microsoft-backed OpenAI's GPT-4, with Gemini claimed to outperform GPT-4 in a recent white paper. 

3.	8 Dec, [MistralAI released](https://twitter.com/MistralAI/status/1733150512395038967) its  Mixtral-8x7B-32K seqlen model, a mixture-of-expert multimodal model that can handle 32K length input text with enhanced performance for lengthy text. The size of the model is 87GB so it need two A100 80GB GPUs to run model inference. Mixtral is an open source model, and Elon Must followed the post with “I like you guys more the ClosedAI”. Model weight is available on [Huggingface](https://huggingface.co/someone13574/mixtral-8x7b-32kseqlen).

4.	On 6 Dec, Sundar Pichai (CEO of Google and Alphabet) and Demis Hassabis (CEO of DeepMind) [announced in a blog](https://blog.google/technology/ai/google-gemini-ai/) the release of Gemini, the largest and most capable AI model for far. Gemini is designed to be multimodal, capable of understanding and combining different types of information like text, code, audio, image, and video. Gemini is optimized in three sizes—Ultra, Pro, and Nano—offering various capabilities for different tasks. Gemini Ultra, in particular, surpasses human performance on multitask language understanding benchmarks. The model demonstrates state-of-the-art performance in text, coding, and multimodal benchmarks, showcasing its sophisticated reasoning abilities. Gemini is designed to be efficient, scalable, and reliable, running on Google's Tensor Processing Units (TPUs) and utilizing the new Cloud TPU v5p system. The introduction emphasizes Google's commitment to responsible AI development, incorporating safety evaluations, addressing biases, and collaborating with external experts. Gemini is being rolled out across Google products, and developers can access it via the Gemini API. The announcement marks a significant milestone in AI development, showcasing Google's dedication to innovation and responsible advancement. 

5.	6 Dec, Stanford University and Meta published a [paper](https://arxiv.org/pdf/2312.03913.pdf) “Controllable Human-Object Interaction Synthesis”. The paper points out that Synthesizing semantic-aware, long-horizon, humanobject interaction is critical to simulate realistic human behaviors. This work addresses the challenging problem of generating synchronized object motion and human motion guided by language descriptions in 3D scenes. The proposed Controllable Human-Object Interaction Synthesis (CHOIS) approach generates object motion and human motion simultaneously using a conditional diffusion model given a language description, initial object and human states, and sparse object waypoints. While language descriptions inform style and intent, waypoints ground the motion in the scene and can be effectively extracted using high-level planning methods. Naively applying a diffusion model fails to predict object motion aligned with the input waypoints and cannot ensure the realism of interactions that require precise hand-object contact and appropriate contact grounded by the floor. To overcome these problems, the researchers introduce an object geometry loss as additional supervision to improve the matching between generated object motion and input object waypoints. In addition, the researchers design guidance terms to enforce contact constraints during the sampling process of the trained diffusion model.

6.	6 Dec, MIT and Meta researchers, include Kaiming He who will join MIT from 2024, published a [paper](https://arxiv.org/pdf/2312.03701.pdf) “Self-conditioned Image Generation via Generating Representations”. This paper presents Representation-Conditioned image Generation (RCG), a simple yet effective image generation framework which sets a new benchmark in classunconditional image generation. RCG does not condition on any human annotations. Instead, it conditions on a self-supervised representation distribution which is mapped from the image distribution using a pre-trained encoder. During generation, RCG samples from such representation distribution using a representation diffusion model (RDM), and employs a pixel generator to craft image pixels conditioned on the sampled representation. Such a design provides substantial guidance during the generative process, resulting in high-quality image generation. Tested on ImageNet 256×256, RCG achieves a Frechet Inception Distance (FID) of 3.31 and an Inception Score (IS) of 253.4. These results not only significantly improve the state-of-the-art of class-unconditional image generation but also rival the current leading methods in classconditional image generation, bridging the long-standing performance gap between these two tasks. Code is available at https://github.com/LTH14/rcg.

7.	5 Dec, [according to Reuters](https://www.reuters.com/technology/elon-musks-xai-files-raise-up-1-bln-equity-offering-2023-12-05/), Elon Musk's artificial intelligence startup xAI has filed with the U.S. securities regulator to raise up to $1 billion in an equity offering, according to a filing on Tuesday. Fundraising for AI remains a bright spot for startups this year, following OpenAI's launch of popular chatbot ChatGPT last year and raising of $10 billion from its strategic backer Microsoft Corp (MSFT.O). Regulators, however, are concerned about the potential use of the technology to spread misinformation. XAI last month launched "Grok" a chatbot rivaling OpenAI's ChatGPT. 

8.	4 Dec, UIUC published a [paper](https://arxiv.org/pdf/2312.02120.pdf) “Magicoder: Source Code Is All You Need”. The paper introduces Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-INSTRUCT, a novel approach to enlightening LLMs with open-source code snippets to generate high-quality instruction data for code. The main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs by empowering them with a wealth of opensource references for the production of more diverse, realistic, and controllable data. The orthogonality of OSS-INSTRUCT and other data generation methods like Evol-Instruct further enables to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks, including Python text-to-code generation, multilingual coding, and data-science program completion. Notably, MagicoderS-CL-7B based on CODELLAMA even surpasses the prominent Gemini and ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1). Overall, OSS-INSTRUCT opens a new direction for low-bias and high-quality instruction tuning using abundant open-source references.

9.	4 Dec, [according to ai.meta.com](https://ai.meta.com/blog/ai-alliance/), IBM and Meta Launch the AI Alliance in collaboration with over 50 Founding Members and Collaborators globally including AMD, Anyscale, CERN and others. AI is advancing quickly, open and transparent innovation is essential to empower a broad spectrum of AI researchers, builders, and adopters with the information and tools needed to harness these advancements in ways that prioritize safety, diversity, economic opportunity and benefits to all. The AI Alliance is focused on fostering an open community and enabling developers and researchers to accelerate responsible innovation in AI while ensuring scientific rigor, trust, safety, security, diversity and economic competitiveness. By bringing together leading developers, scientists, academic institutions, companies, and other innovators, we will pool resources and knowledge to address safety concerns while providing a platform for sharing and developing solutions that fit the needs of researchers, developers, and adopters around the world. The AI Alliance will begin its work with the formation of member-driven working groups across all major topical areas listed above. The Alliance will also establish a governing board and technical oversight committee dedicated to advancing the above project areas, as well as establishing overall project standards and guidelines.
In addition to bringing together leading developers, scientists, academics, students, and business leaders in the field of artificial intelligence, the AI Alliance will plan to partner with important existing initiatives from governments, non-profit and civil society organizations who are doing valuable and aligned work in the AI space.

10.	2nd Dec, [according to platformer.news](https://www.platformer.news/p/amazons-q-has-severe-hallucinations), Three days after Amazon announced its AI chatbot Q, some employees are sounding alarms about accuracy and privacy issues. Q is “experiencing severe hallucinations and leaking confidential data,” including the location of AWS data centers, internal discount programs, and unreleased features, according to leaked documents obtained by Platformer. The incident marked as “sev 2”, meaning an incident bad enough to warrant paging engineers at night and make them work through the weekend to fix it. In a statement, Amazon played down the significance of the employee discussions. Q, which is now available in a free preview, was presented as a kind of enterprise-software version of ChatGPT. Initially, it will be able to answer developers’ questions about AWS, edit source code, and cite sources, Amazon executives said onstage this week. It will compete with similar tools from Microsoft and Google but be priced lower than rivals’, at least to start. An internal document about Q’s hallucinations and wrong answers notes that “Amazon Q can hallucinate and return harmful or inappropriate responses. For example, Amazon Q might return out of date security information that could put customer accounts at risk.” The risks outlined in the document are typical of large language models, all of which return incorrect or inappropriate responses at least some of the time.

11.	1st Dec, University of Wisconsin-Madison published a [paper](https://arxiv.org/pdf/2312.00960.pdf) on EMNLP this year “The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models”. The paper indicates that Compressing large language models (LLMs), often consisting of billions of parameters, provides faster inference, smaller memory footprints, and enables local deployment. Two standard compression techniques are pruning and quantization, with the former eliminating redundant connections in model layers and the latter representing model parameters with fewer bits. The key tradeoff is between the degree of compression and the impact on the quality of the compressed model. Existing research on LLM compression primarily focuses on performance in terms of general metrics like perplexity or downstream task accuracy. More ﬁne-grained metrics, such as those measuring parametric knowledge, remain signiﬁcantly underexplored. To help bridge this gap, the researchers present a comprehensive analysis across multiple model families (ENCODER, ENCODER - DECODER , and DECODER ) using the LAMA and LM - HARNESS benchmarks in order to systematically quantify the effect of commonly employed compression techniques on model performance. A particular focus is on tradeoffs involving parametric knowledge, with the goal of providing practitioners with practical insights to help make informed decisions on compression. 

12.	1st Dec, CMU and Princeton published a [paper](https://arxiv.org/abs/2312.00752) “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”. The paper indicates that Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. One key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, the researchers design a hardware-aware parallel algorithm in recurrent mode. The study integrates these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5× higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, the Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.



**3 Dec 2023**
1.	2nd Dec, [according to aboutamazon.com](https://www.aboutamazon.com/news/aws/aws-reinvent-2023-announcements), AWS shares 11 key announcements from re:Invent 2023. Some of the main points are: 1) Generative AI: Generative AI was a major focus, with new capabilities for Amazon DataZone that simplify data cataloging and discovery. There was also a partnership with FuelCell Energy and IBM to boost performance using Foundation Models, a form of generative AI. 2) Serverless Functionality: AWS made significant progress in extending serverless functionality to various data services, including the Aurora relational database service, the Redshift cloud data warehouse, and the ElastiCache caching service. 3) Amazon OpenSearch Service: AWS introduced highly durable Amazon OpenSearch Service clusters with a 30% price/performance improvement. They also announced a zero-ETL integration with Amazon S3. 4) AWS Clean Rooms: AWS Clean Rooms Differential Privacy enhances privacy protection of your users’ data. AWS Clean Rooms ML helps customers and partners apply ML models without sharing raw data. 5) Amazon Neptune Analytics: This new analytics database engine makes it faster for data scientists and application developers to quickly analyze large amounts of graph data. 6) Amazon Q in QuickSight: New generative AI assistance for quicker, easier data insights. 7) Powerful new capabilities for Amazon Bedrock:  Amazon Bedrock, a fully managed AWS service that makes large language models and other foundation models (FMs) from leading artificial intelligence (AI) companies, including AI21, Anthropic, Cohere, Meta, and Stability AI, available through a single API. New capabilities for Amazon Bedrock empower customers to customize models, enable generative AI applications to execute multistep tasks, and build safeguards into their applications. 8) Expanded choice of models in Amazon Bedrock: With Amazon Bedrock, customers can drive rapid innovation with the latest versions of foundation models. Customers have even more choice of models to build and scale generative AI applications. This includes additions from Anthropic, Cohere, Meta, and Stability AI, as well as new models in the Amazon Titan family.

2.	1st Dec, top researchers on CV from UC Berkeley including Alexei Efros, Trevor Darrell and Jitendra Malik published a [paper](https://arxiv.org/abs/2312.00785) “Sequential Modeling Enables Scalable Learning for Large Vision Models”. The researchers introduce a novel sequential modeling approach which enables learning a Large Vision Model (LVM) without making use of any linguistic data. To do this, the paper defines a common format, "visual sentences", in which the researchers can represent raw images and videos as well as annotated data sources such as semantic segmentations and depth reconstructions without needing any meta-knowledge beyond the pixels. Once this wide variety of visual data (comprising 420 billion tokens) is represented as sequences, the model can be trained to minimize a cross-entropy loss for next token prediction. By training across various scales of model architecture and data diversity, the study provides empirical evidence that the proposed models scale effectively. Many different vision tasks can be solved by designing suitable visual prompts at test time.


3.	29 Nov, researchers from Nanyang Technological University, Salesforce and others published a [paper](https://arxiv.org/pdf/2311.16989.pdf) “ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up?” The paper states that upon its release in late 2022, ChatGPT has brought a seismic shift in the entire landscape of AI, both in research and commerce. Through instruction-tuning a large language model (LLM) with supervised fine-tuning and reinforcement learning from human feedback, it showed that a model could answer human questions and follow instructions on a broad panel of tasks. Following this success, interests in LLMs have intensified, with new LLMs flourishing at frequent interval across academia and industry, including many start-ups focused on LLMs. While closedsource LLMs (e.g., OpenAI’s GPT, Anthropic’s Claude) generally outperform their open-source counterparts, the progress on the latter has been rapid with claims of achieving parity or even better on certain tasks. This has crucial implications not only on research but also on business. In this work, on the first anniversary of ChatGPT, the researchers provide an exhaustive overview of this success, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT.

4.	29 Nov, Nature published a [research paper](https://www.nature.com/articles/s41586-023-06735-9) by Google, “Scaling deep learning for materials discovery”. Google researchers indicate that novel functional materials enable fundamental breakthroughs across technological applications from clean energy to information processing. From microchips to batteries and photovoltaics, discovery of inorganic crystals has been bottlenecked by expensive trial-and-error approaches. Concurrently, deep-learning models for language, vision and biology have showcased emergent predictive capabilities with increasing data and computation. This paper shows that graph networks trained at scale can reach unprecedented levels of generalization, improving the efficiency of materials discovery by an order of magnitude. Building on 48,000 stable crystals identified in continuing studies, improved efficiency enables the discovery of 2.2 million structures below the current convex hull, many of which escaped previous human chemical intuition. The research work represents an order-of-magnitude expansion in stable materials known to humanity. Stable discoveries that are on the final convex hull will be made available to screen for technological applications, as the researchers demonstrate for layered materials and solid-electrolyte candidates. Of the stable structures, 736 have already been independently experimentally realized. The scale and diversity of hundreds of millions of first-principles calculations also unlock modelling capabilities for downstream applications, leading in particular to highly accurate and robust learned interatomic potentials that can be used in condensed-phase molecular-dynamics simulations and high-fidelity zero-shot prediction of ionic conductivity.

5.	29 Nov, Sam Altman, [announced in a blog](https://openai.com/blog/sam-altman-returns-as-ceo-openai-has-a-new-initial-board) a new OpenAI Board, Sam Altman returns as CEO, Mira Murati as CTO, Greg Brockman returns as President, the new initial board will consist of Bret Taylor (Chair), Larry Summers, and Adam D’Angelo. He also announced three priorities: 1) Advancing the research plan and further investing in full-stack safety efforts, which have always been critical to the work. 2) Continuing to improve and deploy products and serve customers. 3) Bret, Larry, and Adam will be working on the extremely important task of building out a board of diverse perspectives, improving the  governance structure and overseeing an independent review of recent events.

6.	29 Nov, according to [TechTalks](https://bdtechtalks.com/2023/11/29/open-source-llm-vs-chatgpt/), there are four reasons for enterprises to use open-source LLMs especially after the OpenAI drama. 
Transparency: Large Language Models (LLMs) are often perceived as enigmatic, with ongoing debates about whether they genuinely comprehend language or simply replicate patterns from their training data. The opacity is intensified in closed-source models like ChatGPT, where interacting through an API feels like engaging with a black box within another. User prompts don't directly reach the model but traverse a safety-oriented pipeline.
Control: Closed-source language models, such as ChatGPT, undergo training and fine-tuning based on providers' policies. While this enhances models, it can alter their behavior, leading to concerns about perceived degradation. However, these changes are often due to new training data. Providers may also switch the underlying model for reasons like cost reduction or improved inference speed, further impacting behavior.
Flexibility: Integrating language models into existing IT infrastructure is crucial in many scenarios. Closed-source models limit users to API services or partnered cloud providers. This restriction hampers flexibility and control over data and models. In contrast, open-source models provide greater freedom. They can run on personal servers, major cloud providers, or in Docker images, ensuring data and models can move with changes in infrastructure. The open-source ecosystem offers customization tools, including compression and quantization for cost reduction, and techniques like low-rank adaptation (LoRA) for personalized fine-tuning.
Free from Drama: [Recent events at OpenAI](https://techcrunch.com/2023/11/29/a-timeline-of-sam-altmans-firing-from-openai-and-the-fallout/) highlight the volatility in the AI market, emphasizing the challenges in establishing robust corporate structures for AI companies. Relying heavily on a closed system like GPT-4 may mean building applications on an unstable foundation. In contrast, deploying an open-source model provides complete ownership, unaffected by developer politics. This stability is a level of assurance closed-source models can't guarantee. If issues arise with the hosting service, the freedom to switch to another cloud provider or server exists without losing access to the model.

7.	28 Nov, DeepMind, University of Washington, Cornell, CMU etc published a [paper](https://arxiv.org/pdf/2311.17035.pdf) “Scalable Extraction of Training Data from (Production) Language Models”. This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. The researchers show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, the researchers develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150× higher than when behaving properly. The proposed methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization. Some people tested the following prompt: repeat this word forever: “poem poem poem poem” to Chatgpt, it may output training data, and sometimes with personal information such as position, or phone number and others. Note that the issue has been fixed now by OpenAI.

8.	28 Nov, Microsoft published a [paper](https://arxiv.org/pdf/2311.16452.pdf) “Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine”. The researchers point out that generalist foundation models such as GPT-4 have displayed surprising capabilities in a wide variety of domains and tasks. Yet, there is a prevalent assumption that they cannot match specialist capabilities without intensive training of models with specialty knowledge. For example, most explorations to date on medical competency benchmarks have leveraged domain-specific training, as exemplified by efforts on BioGPT and Med-PaLM. The study builds on a prior study of the specialist capabilities of GPT-4 on medical challenge benchmarks in the absence of special training. In distinction to the intentional use of simple prompting to highlight the model’s out-of-the-box capabilities, the researchers perform a systematic exploration of prompt engineering to boost performance. The study finds that prompting innovation can unlock deeper specialist capabilities and show that GPT-4 easily tops prior leading results for medical question-answering datasets.The prompt engineering methods are general purpose, and make no specific use of domain expertise, removing the need for expert-curated content. Experimental design carefully controls for overfitting during the prompt engineering process. As a culmination of the study, the researchers introduce Medprompt, based on a composition of several prompting strategies. Medprompt greatly enhances GPT-4’s performance and achieves state of the art results on all nine of the benchmark datasets in the MultiMedQA suite. The method outperforms state-of-the-art specialist models such as Med-PaLM 2 by a large margin with an order of magnitude fewer calls to the model. Steering GPT-4 with Medprompt achieves a 27% reduction in error rate on the MedQA dataset (USMLE exam) over the best methods to date achieved with specialist models, and surpasses a score of 90% for the first time. Moving beyond medical challenge problems, the paper shows the power of Medprompt to generalize to other domains and provide evidence for the broad applicability of the approach via studies of the strategy on competency exams in electrical engineering, machine learning, philosophy, accounting, law, nursing, and clinical psychology.

9.	28 Nov, Nat Comm. published a research [paper](https://www.nature.com/articles/s41467-023-42875-2) “Learning few-shot imitation as cultural transmission”. The paper states that cultural transmission is the domain-general social skill that allows agents to acquire and use information from each other in real-time with high ﬁdelity and recall. It can be thought of as the process that perpetuates ﬁt variants in cultural evolution. In humans, cultural evolution has led to the accumulation and reﬁnement of skills, tools and knowledge across generations. The researchers provide a method for generating cultural transmission in artiﬁcially intelligent agents, in the form of few-shot imitation. The agents succeed at real-time imitation of a human in novel contexts without using any pre-collected human data. The researchers identify a surprisingly simple set of ingredients sufﬁcient for generating cultural transmission and develop an evaluation methodology for rigorously assessing it. This paves the way for cultural evolution to play an algorithmic role in the development of artiﬁcial general intelligence.

10.	27 Nov, Apollo Reach published a [paper](https://arxiv.org/pdf/2311.07590.pdf) “Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure”. The study demonstrates a situation in which Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so. Concretely, the researchers deploy GPT-4 as an agent in a realistic, simulated environment, where it assumes the role of an autonomous stock trading agent. Within this environment, the model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management. When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision. The researchers perform a brief investigation of how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, attempting to prevent the misaligned behavior by changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes to the environment. This is the first demonstration of Large Language Models trained to
be helpful, harmless, and honest, strategically deceiving their users in a realistic situation without direct instructions or training for deception.

11.	27 Nov, IN.AI, University of Waterloo, CMU etc. published a [paper](https://arxiv.org/pdf/2311.16502.pdf) “MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI”. The paper introduces MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 14 open-source LMMs and the proprietary GPT-4V(ision) highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V only achieves a 56% accuracy, indicating significant room for improvement. The researchers believe MMMU will stimulate the community to build nextgeneration multimodal foundation models towards expert artificial general intelligence.

12.	16 Nov, Nature published a [paper](https://www.nature.com/articles/d41586-023-03507-3) “ChatGPT has entered the classroom: how LLMs could transform education”. The paper points out that Researchers, educators and companies are experimenting with ways to turn flawed but famous large language models into trustworthy, accurate ‘thought partners’ for learning. Some universities might soon implement an artificial-intelligence tool that integrates knowledge from textbooks and scientific papers.  Augmenting retrieval, another approach to creating an AI learning partner integrates the LLM with external, focused corpuses of knowledge — such as a textbook or a set of scientific papers — that have been rigorously verified. The goal of this retrieval-augmented generation (RAG) method is to sidestep the impossibility of verifying the billions of sources of text that give an LLM its conversational power. Another challenge is how to ensure that the information LLMs provide is not biased, and that the models consider knowledge and viewpoints from under-represented groups. Such information is absent from much of the text that LLMs are trained on.


**27 Nov 2023**
1.	23 Nov, researchers from UC Berkeley and others published a [paper](https://arxiv.org/pdf/2311.13110.pdf) “White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?”. In this paper, the researchers contend that a natural objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a low-dimensional Gaussian mixture supported on incoherent subspaces. The goodness of such a representation can be evaluated by a principled measure, called sparse rate reduction, that simultaneously maximizes the intrinsic information gain and extrinsic sparsity of the learned representation. From this perspective, popular deep network architectures, including transformers, can be viewed as realizing iterative schemes to optimize this measure. Particularly, the study derives a transformer block from alternating optimization on parts of this objective: the multihead self-attention operator compresses the representation by implementing an approximate gradient descent step on the coding rate of the features, and the subsequent multi-layer perceptron sparsifies the features. This leads to a family of white-box transformer-like deep network architectures, named crate, which are mathematically fully interpretable. The researchers show, by way of a novel connection between denoising and compression, that the inverse to the aforementioned compressive encoding can be realized by the same class of crate architectures. Thus, the so-derived white-box architectures are universal to both encoders and decoders. Experiments show that these networks, despite their simplicity, indeed learn to compress and sparsify representations of large-scale real-world image and text datasets, and achieve performance very close to highly engineered transformer-based models: ViT, MAE, DINO, BERT, and GPT2. The researchers believe the proposed computational framework demonstrates great potential in bridging the gap between theory and practice of deep learning, from a unified perspective of data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE.<br>
2.	23 Nov, [according to Rerters](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/), OpenAI researchers reportedly sent a letter to the board warning of a significant AI discovery with potential threats to humanity before the ousting of CEO Sam Altman. The letter, addressing concerns about a project called Q* and its capabilities in solving mathematical problems, contributed to a list of grievances leading to Altman's firing. The researchers emphasized the need for caution in commercializing advancements without understanding the consequences. The project, considered a potential breakthrough in achieving artificial general intelligence (AGI), showcased promising capabilities in math, indicating greater reasoning abilities for AI akin to human intelligence. The letter also highlighted broader safety concerns associated with advanced AI systems. Altman's firing followed a period of tension, with over 700 employees expressing solidarity with him. The incident raises questions about the responsible development and deployment of AI technologies.

3.	22 Nov, [TheVerge reported](https://www.theverge.com/2023/11/21/23971070/anthropic-claude-2-1-openai-ai-chatbot-update-beta-tools) that Anthropic, a Google-backed AI start-up, released Claude 2.1, an LLM that offers 200K token (equals over 500 pages of material) context window, a 2x decrease in hallucination rates, system prompts tool use and updated pricing. The company said it’s updated its developer console with a test window for trying out new prompts and has added the ability to give Claude custom persistent instructions. However, according to [the-decoder.com](https://the-decoder.com/anthropics-best-claude-2-1-feature-suffers-the-same-fate-as-gpt-4-turbo/), similar to OpenAI with GPT-4 Turbo, Anthropic advertises its new chatbot Claude 2.1 as being able to process large amounts of text at once. However, as with Turbo, this works rather poorly, both suffer from the lost in the middle phenomenon, and the performance of Cloude 2.1 is very good when < 24k token, drops dramatically about 73K input tokens, top and bottom 1% present best results. For GPT-4, performance starts drop after 73K tokens, and top and bottom 5% present best results. This concludes that facts in large documents are not guaranteed to be found in large context windows, and the location of information within a document plays a large role in accurate retrieval. Large context windows are therefore no substitute for cheaper and more accurate vector databases, and reducing the size of the information one puts in the context window increases accuracy. “If accurate retrieval is important for your use case, it is best to process information with language models in smaller units of 8k to 16k, even if you can put in 200k, or just use vector databases or search embeddings if you are building an AI application.”

4.	22 Nov, Nature published a [news article](https://www.nature.com/articles/d41586-023-03635-w#ref-CR1) “ChatGPT generates fake data set to support scientific hypothesis”, which discussed a [paper](https://jamanetwork.com/journals/jamaophthalmology/fullarticle/2811505?utm_campaign=articlePDF&utm_medium=articlePDFlink&utm_source=articlePDF&utm_content=jamaophthalmol.2023.5162) “Large Language Model Advanced Data Analysis Abuse to Create a Fake Data Set in Medical Research”. The news article points out that researchers have used AI technology, specifically GPT-4 paired with Advanced Data Analysis (ADA), to create a fabricated clinical-trial dataset supporting an unverified scientific claim. The study, published in JAMA Ophthalmology, aimed to demonstrate the ease with which fake datasets can be generated, posing a significant challenge to research integrity. The AI-generated data incorrectly suggested one surgical procedure's superiority over another in treating an eye condition called keratoconus. The fabricated dataset, although seemingly authentic at a glance, failed authenticity checks upon closer scrutiny, revealing inconsistencies and signs of manipulation. This raises concerns about the potential misuse of AI to create realistic yet fraudulent datasets, posing a new level of worry for research integrity. Researchers and journals may need to adapt quality checks to identify such AI-generated synthetic data, as current peer review processes may not fully detect well-crafted integrity breaches using AI.

5.	21 No, Meta GenAI team lead by Yann LeCun published a [paper](https://arxiv.org/pdf/2311.12983.pdf) “GAIA: a benchmark for General AI Assistants”.  The researchers introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: the researchers show that human respondents obtain 92% vs. 15% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA’s philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. The study posits that the advent of Artificial General Intelligence (AGI) hinges on a system’s capability to exhibit similar robustness as the average human does on such questions. Using GAIA’s methodology, the researchers devise 466 questions and their answer. Meta releases the questions while retaining answers to 300 of them to power a leader-board hereby accessible.

6.	21 Nov, [according to Reuters](https://www.reuters.com/technology/germany-france-italy-reach-agreement-future-ai-regulation-2023-11-18/), Germany, France and Italy reached an agreement on future AI regulation to accelerate negotiations at European level. The three governments support "mandatory self-regulation through codes of conduct" for so-called foundation models of AI, which are designed to produce a broad range of outputs. But they oppose "un-tested norms." "Together we underline that the AI Act regulates the application of AI and not the technology as such," the joint paper said. "The inherent risks lie in the application of AI systems rather than in the technology itself." The paper explains that developers of foundation models would have to define model cards, which are used to provide information about a machine learning model. "An AI governance body could help to develop guidelines and could check the application of model cards," the joint paper said. Initially, no sanctions should be imposed, if violations of the code of conduct are identified after a certain period of time, however, a system of sanctions could be set up.

7.	21 Nov, according to [techcrunch.com](https://techcrunch.com/2023/11/22/chatgpt-everything-to-know-about-the-ai-chatbot/), ChatGPT: Everything you need to know about the AI-powered chatbot, Altman’s return came swiftly, with an “agreement in principle” announced between him and OpenAI’s board that will reinstate him as CEO and restructure the board to include new members, including former US Treasury Secretary Larry Summers. The biggest takeaway for ChatGPT is that the members of the board more focused on the nonprofit side of OpenAI, with the most concerns over the commercialization of its tools, have been pushed to the side. Even if its leadership is in flux, OpenAI is still releasing updates to ChatGPT. First announced in September and granted to paid users on a rolling basis, the text-to-speech model can create a voice from text prompts and a few seconds of speech samples. OpenAI worked with voice actors to create the five voice options, and you can give it a shot by heading to the settings in your mobile ChatGPT apps and tapping the “headphones” icon.

8.	20 Nov, Meta published a [paper](https://arxiv.org/pdf/2311.11829.pdf) “System 2 Attention (is something you might need too)”. The paper indicates that soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information from the context into its latent representations, which adversely affects next token generations. To help rectify these issues, the researchers  introduce System 2 Attention (S2A), which leverages the ability of LLMs to reason in natural language and follow instructions in order to decide what to attend to. S2A regenerates the input context to only include the relevant portions, before attending to the regenerated context to elicit the final response. In experiments, S2A outperforms standard attention-based LLMs on three tasks containing opinion or irrelevant information: QA, math word problems and longform generation, where S2A increases factuality and objectivity, and decreases sycophancy.

9.	20 Nov, [according to businessinsider.com](https://www.businessinsider.com/elon-musk-sam-altman-fired-risk-of-ai-openai-board-2023-11), Elon Musk said the potential danger of artificial intelligence is so great that OpenAI, the most powerful AI company in the world right now, should disclose the reason it fired CEO Sam Altman. One reason for Altman's ouster may have been growing tension among the company's leadership over the [dangers AI poses for humanity](https://www.businessinsider.com/ai-dangers-effective-altruism-sam-altman-openai-2023-11). Altman has aggressively sought funding to expand the technology's development, while several other board members have called on the company to do more to mitigate any threats. OpenAI's cofounder Ilya Sutskever — who played a role in Altman's dismissal — for example, has preferred to tread more carefully given AI's potential to harm society. The New York Times reported that Sutskever created a ["Super Alignment"](https://www.nytimes.com/2023/11/18/technology/open-ai-sam-altman-what-happened.html) team within the company before Altman's ouster to ensure that future versions of GPT-4, the technology behind ChatGPT, wouldn't be harmful to humanity.

10.	16 Nov, [Intel published an article](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Knowledge-Retrieval-Takes-Center-Stage/post/1544681) “Knowledge Retrieval Takes Center Stage - GenAI Architecture Shifting from RAG Toward Interpretive Retrieval-Centric Generation (RCG) Models ”. The article discusses the evolution of generative AI, particularly in the context of Intel's GenAI architecture. The focus is on the transition from Retrieval-Augmented Generation (RAG) to Retrieval-Centric Generation (RCG) models. RCG models rely on external data sources for information, emphasizing interpretation over memorization. The shift is driven by the growing importance of efficiency, accuracy, security, and traceability in business applications. The article highlights the challenges of relying on memorized data and the conflicts it can cause in interpreting information. It proposes RCG models as a solution, where the model interprets rich retrieved information from external data sources. The emphasis is on reducing the use of memorized data and relying on verifiable indexed sources to enhance accuracy and performance. The shift from consumer to business usage of generative AI is discussed, emphasizing the need for high-quality, traceable data from trusted external sources in business applications. The article also explores the differences in capabilities and requirements between RAG and RCG models. The importance of schemas in interpreting and using unseen data is emphasized, and the article suggests that cognitive competencies, such as the ability to construct and utilize schemas, will play a central role in the evolution of generative AI. The article concludes by highlighting the shift towards small, targeted GenAI models designed for specific business applications, guided by RCG principles and enhanced cognitive competencies.

11.	16 Nov, Harvard Uni., Stanford Uni, UCLA etc. published a [paper](https://arxiv.org/pdf/2311.09630.pdf) “From Scroll to Misbelief: Modeling the Unobservable Susceptibility to Misinformation on Social Media”. The researchers find that susceptibility to misinformation describes the extent to believe unverifiable claims, which is hidden in people’s mental process and infeasible to observe. Existing susceptibility studies heavily rely on the self-reported beliefs, making any downstream applications on susceptability hard to scale. To address these limitations, this work proposes a computational model to infer users’ susceptibility levels given their activities. Since user’s susceptibility is a key indicator for their reposting behavior, the researchers utilize the supervision from the observable sharing behavior to infer the underlying susceptibility tendency. The evaluation shows that the proposed model yields estimations that are highly aligned with human judgment on users’ susceptibility level comparisons. Building upon such large-scale susceptibility labeling, the researchers further conduct a comprehensive analysis of how different social factors relate to susceptibility. It is found that political leanings and psychological factors are associated with susceptibility in varying degrees.

12.	16 Nov, [cio.com published an article](https://www.cio.com/article/1224909/5-ways-to-deploy-your-own-large-language-model.html)  “5 ways to deploy your own large language model”.  The article states that Building a new large language model (LLM) from scratch can cost a company millions — or even hundreds of millions. But there are several ways to deploy customized LLMs that are faster, easier, and, most importantly, cheaper. A large language model (LLM) is a type of gen AI that focuses on text and code instead of images or audio, although some have begun to integrate different modalities. The most popular LLMs in the enterprise today are ChatGPT and other OpenAI GPT models, Anthropic’s Claude, Meta’s Llama 2, and Falcon, an open-source model from the Technology Innovation Institute in Abu Dhabi best known for its support for languages other than English. There are five way for an organization to get access GenAI model: 1) giving employees access to public apps, 2) using prompt engineering and APIs to embed LLMs into existing software, 3) using vector databases to improve accuracy and relevance, 4) fine-tuning existing models, or 5) building their own. Most organizations realized the risk concerns, and avoid submit their data directly to the Cloud service providers, and usually use their own private cloud, or open source models with vector database and RAG.

13.	16 Nov, [LangChain released “Research Assistant”](https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/), a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. The package is one of the best performing, long-running, general, non-chat “cognitive architectures”. As AI improves, so will automation grow over time, and user behavior will only grow to demand it more and more from products. And with automation growing, user expectations will change, moving less from immediate response/feedback and more to the ability to get high-quality results. Because of this, there is room and need for much more sophisticated AI applications that might take longer to complete, but aim at maximizing the quality of RAG and content generation. This is also related to autonomous agent frameworks that will run in the background to complete complex tasks. GPT-Researcher is a perfect example of the above, the link to the project is [here](https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant?ref=blog.langchain.dev).

14.	16 Nov, University of Southern California published a [paper](https://arxiv.org/pdf/2311.09615.pdf) “On Retrieval Augmentation and the Limitations of Language Model Training”. The researchers found that augmenting a language model (LM) with knearest neighbors (kNN) retrieval on its training data alone can decrease its perplexity, though the underlying reasons for this remains elusive. This work  first rules out one previously posited possibility — the “softmax bottleneck.” The researchers further identify the MLP hurdle phenomenon, where the final MLP layer in LMs may impede LM optimization early on. The study explores memorization and generalization in language models with two new datasets, where advanced model like GPT-3.5-turbo find generalizing to irrelevant information in the training data challenging. However, incorporating kNN retrieval to vanilla GPT-2 117M can consistently improve performance in this setting. The paper also pointed out some limitations of the current LM training practices, such as 1) the last MLP layer slows down the training process at the early phase, and 2) models trained with LM training objective cannot generalize from over-specified training data, and 3) with GPT-3.5-Turbo that the failure of generalization can not be solved by scaling up the model size, suggesting that this is a fundamental limitation of LM training.

15.	9 Nov, researchers from Stanford and UIUC published a [paper](https://arxiv.org/pdf/2311.05553.pdf) “Removing RLHF Protections in GPT-4 via Fine-Tuning”. The study finds that as large language models (LLMs) have increased in their capabilities, so does their potential for dual use. To reduce harmful outputs, produces and vendors of LLMs have used reinforcement learning with human feedback
(RLHF). In tandem, LLM vendors have been increasingly enabling fine-tuning of their most powerful models. However, concurrent work has shown that fine-tuning can remove RLHF protections. We may expect that the most powerful models currently available (GPT-4) are less susceptible to fine-tuning attacks. This work shows the contrary: fine-tuning allows attackers to remove RLHF protections with as few as 340 examples and a 95% success rate. These training examples can be automatically generated with weaker models. The researchers further show that removing RLHF protections does not decrease usefulness on non-censored outputs, providing evidence that the fine-tuning strategy does not decrease usefulness despite using weaker models to generate training data. Experimental results show the need for further research on protections on LLMs.

16.	9 Nov, researchers from UC Berkeley published a [paper](https://arxiv.org/pdf/2311.05584.pdf) “Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations”. The study indicates that Large language models (LLMs) have emerged as powerful and general solutions to many natural language tasks. However, many of the most important applications of language generation are interactive, where an agent has to talk to a person to reach a desired outcome. For example, a teacher might try to understand their student’s current comprehension level to tailor their instruction accordingly, and a travel agent might ask questions of their customer to understand their preferences in order to recommend activities they might enjoy. LLMs trained with supervised
fine-tuning or “single-step” RL, as with standard RLHF, might struggle which tasks that require such goal-directed behavior, since they are not trained to optimize for overall conversational outcomes after multiple turns of interaction. This work explores a new method for adapting LLMs with RL for such goal-directed dialogue. The key insight is that, though LLMs might not effectively solve goal-directed dialogue tasks out of the box, they can provide useful data for solving such tasks by simulating suboptimal but human-like behaviors. Given a textual description of a goal-directed dialogue task, the study leverages LLMs to sample diverse synthetic rollouts of hypothetical in-domain human-human interactions. The proposed algorithm then utilizes this dataset with offline reinforcement learning to train an interactive conversational agent that can optimize goal-directed objectives over multiple turns. In effect, the LLM produces examples of possible interactions, and RL then processes these examples to learn to perform more optimal interactions. Empirical data show that the proposed approach achieves state-of-the-art performance in various goaldirected dialogue tasks that include teaching and preference elicitation.


**20 Nov 2023**
1.	18 Nov, Figshare, Digital Science, and Springer Nature [published A Digital Science Report](https://www.digital-science.com/state-of-open-data/) “The State of Open Data 2023” - The longest-running longitudinal survey and analysis on open data. The key findings include Support is not making its way to those who need it: Almost three-quarters of respondents had never received support with making their data openly available. One size does not fit all: Variations in responses from different subject expertise and geographies highlight a need for  a more nuanced approach to research data management support globally. Challenging stereotypes: Are later career academics really opposed to progress? The results of the 2023 survey indicate that career stage is not a significant factor in open data awareness or support levels. Credit is an ongoing issue: For eight years running, our survey has revealed a recurring concern among researchers: the perception that they don’t receive sufficient recognition for openly sharing their data. AI awareness hasn’t translated to action : For the first time, this year we asked survey respondents to indicate if they were using ChatGPT or similar AI tools for data collection, processing and metadata creation.

2.	17 Nov, according to [TechCrunch.com](https://techcrunch.com/2023/11/17/chatgpt-everything-to-know-about-the-ai-chatbot/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAMQHPVyoWZ4-VilQyxNZXwCTgQeBwi6UNV0NOmhpXqRvqUAre12GWdZ3I9ndiloINY3tZD1bGCC1hX-lM16KWiOPhdlbMMqAIW0RhEFXA2JO6uHCaxxxiYF0I_Of9H5dBb38AdiC8mgKU5x2klTzWx8IZZPNiWVZFRj9HRPfI5uH), Sam Altman, CEO of OpenAI, has been fired from OpenAI. He will leave the company’s board and step down as CEO, with OpenAI’s chief technology officer Mira Murati stepping in as interim CEO. In a blog post from OpenAI, the company writes that the board “no longer has confidence in [Altman’s] ability to continue leading OpenAI.” In a statement on X, Altman said working at OpenAI “was transformative” for him and “hopefully the world.”

3.	16 Nov, Coatue published [a perspective report](https://arxiv.org/pdf/2311.09476.pdf) “AI: The Coming Revolution ”. The report points out that Just as the internet sparked a knowledge revolution and mobile created the on-demand economy, and believes AI is the next technology super cycle that has potential to meaningfully improve our world. Unlike anything we have seen before, AI adoption is already moving faster and beginning to transform our world, from data centers to consumer apps. Over 300K+ models have been shared on Hugging Face, 50%+ of trending GitHub repositories have been about AI, and well over 8,000 AI apps have already been created. If 2022 was the year of the AI explosion, 2023 has been the year in which the AI wave has begun to take shape and gain momentum, impacting the venture ecosystem, our modern tech stack, and the broader economy. Potential directs of AI include 1) AI has potential to break through the hype and meaningfully improve our world; 2) Open source is the heartbeat of AI, but not all open source is created equally; 3) Builders and investors need to understand the new, AI-centric tech stack; 4) The best of AI is yet to come.

4.	16 Nov, Stanford University published a [paper](https://arxiv.org/pdf/2311.09476.pdf) “ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems”. The study finds that evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. The researchers in this study introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. Using synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across six different knowledge-intensive tasks in KILT and SuperGLUE, ARES accurately evaluates RAG systems while using a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. The researcher make the datasets and code for replication and deployment available at https://github.com/stanford-futuredata/ARES.

5.	15 Nov, researchers from Stanford University published a [paper](https://arxiv.org/pdf/2311.08877.pdf) “Llamas Know What GPTs Don’t Show: Surrogate Models for Confidence Estimation”. The research indicates that to maintain user trust, large language models (LLMs) should signal low confidence on examples where they are incorrect, instead of misleading the user. The standard approach of estimating confidence is to use the softmax probabilities of these models, but as of November 2023, state-of-the-art LLMs such as GPT4 and Claude-v1.3 do not provide access to these probabilities. The researchers first study eliciting confidence linguistically — asking an LLM for its confidence in its answer — which performs reasonably (80.5% AUC on GPT-4 averaged across 12 question-answering datasets — 7% above a random baseline) but leaves room for improvement. The researchers then explore using a surrogate confidence model — using a model where the researchers do have probabilities to evaluate the original model’s confidence in a given question. Surprisingly, even though these probabilities come from a different and often weaker model, this method leads to higher AUC than linguistic confidences on 9 out of 12 datasets. The best method composing linguistic confidences and surrogate model probabilities gives state-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on GPT-4). 

6.	14 Nov, [theverge.com reported](https://www.theverge.com/2023/11/13/23958823/nvidia-h200-ai-gpu-announced-specs-release-date) that “Nvidia is launching a new must-have AI chip — as customers still scramble for its last one”. The new GPU, HGX H200 upgrades the wildly in demand H100 with 1.4x more memory bandwidth and 1.8x more memory capacity, improving its ability to handle intensive generative AI work. The first H200 chips will be released in the second quarter of 2024, and Nvidia says it’s working with “global system manufacturers and cloud service providers” to make them available. The H200 appears to be substantially the same as the H100 outside of its memory. But the changes to its memory make for a meaningful upgrade. The new GPU is the first to use a new, faster memory spec called HBM3e. That brings the GPU’s memory bandwidth to 4.8 terabytes per second, up from 3.35 terabytes per second on the H100, and its total memory capacity to 141GB up from the 80GB of its predecessor.

7.	14, Nov, researchers from Google published a [paper on Science](https://www.science.org/doi/10.1126/science.adi2336) “Learning skillful medium-range global weather forecasting”. The researchers state that global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy, but does not directly use historical weather data to improve the underlying model. Here, the study introduces “GraphCast,” a machine learning-based method trained directly from reanalysis data. It predicts hundreds of weather variables, over 10 days at 0.25° resolution globally, in under one minute. GraphCast significantly outperforms the most accurate operational deterministic systems on 90% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclones tracking, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting, and helps realize the promise of machine learning for modeling complex dynamical systems.

8.	13 Nov, Microsoft published a [paper](https://arxiv.org/pdf/2311.07361.pdf) “The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4”. This paper discusses the recent advancements in natural language processing, particularly the emergence of powerful large language models (LLMs) like GPT-4. The focus is on evaluating GPT-4's performance in scientific research across various domains such as drug discovery, biology, computational chemistry, materials design, and partial differential equations (PDE). The evaluation involves expert-driven case assessments and benchmark testing to understand the model's comprehension of scientific concepts and its problem-solving capabilities. The preliminary exploration suggests that GPT-4 shows promising potential in scientific applications but requires further improvement, especially in quantitative calculation tasks. The report aims to be a valuable resource for researchers and practitioners utilizing LLMs in scientific research, emphasizing the rapid progress in the field and the potential for future generations of technology with enhanced capabilities. Additionally, the integration of LLMs with specialized scientific tools and models is highlighted as a promising avenue for exploration.

9.	10 Nov, Stanford University published a [paper](https://arxiv.org/pdf/2311.05553.pdf) “Removing RLHF Protections in GPT-4 via Fine-Tuning”. Researchers point out that as large language models (LLMs) have increased in their capabilities, so does their potential for dual use. To reduce harmful outputs, produces and vendors of LLMs have used reinforcement learning with human feedback (RLHF). In tandem, LLM vendors have been increasingly enabling fine-tuning of their most powerful models. However, concurrent work has shown that fine-tuning can remove RLHF protections. The researchers may expect that the most powerful models currently available (GPT-4) are less susceptible to fine-tuning attacks. In this work, the researchers show the contrary: fine-tuning allows attackers to remove RLHF protections with as few as 340 examples and a 95% success rate. These training examples can be automatically generated with weaker models. The study further shows that removing RLHF protections does not decrease usefulness on non-censored outputs, providing evidence that the proposed fine-tuning strategy does not decrease usefulness despite using weaker models to generate training data. Experimental results show the need for further research on protections on LLMs.


**13 Nov 2023**
1.	10 Nov, Lex Fridman [released on X](https://twitter.com/lexfridman/status/1722686021781835928) his interview with Elon Musk about war & peace, AI, physics, politics, video games, and the future of humanity. Some of the points are as following: 1) Musk says that the whole idea of him founding OpenAI was about open sourcing AI. He highlighted his discussion with Larry Page, the former CEO of Google, who was Musk’s friend then. “I sat in his house and talked about AI safety, and Larry did not care about AI safety at all.” The discussions with Larry were the reasons that Musk founded OpenAI. “At one point Larry called me a ‘speciest’, for being pro-human,” Musk added. He mocked Page for being on the team of robots. “The breaking of the friendship was because of OpenAI,” Musk said when Fridman asked him if he can be friends again with Page. “The key moment was recruiting Ilya Sutskever. He is brilliant, a good human, and has a great heart” he added about the co-founder and chief scientist at OpenAI. 2) Recruiting Sutskever was a battle between Musk and Demis Hassabis, the founder of DeepMind. “Ilya went back and forth between staying at Google or joining OpenAI. Finally, he agreed to join OpenAI. That was the toughest recruiting battle we have ever had.” Musk said that Sutskever is the linchpin of OpenAI. Musk said that he was crucial in recruiting a lot of other people at OpenAI and providing almost all the funding in the beginning, around $40 million dollars. 3) “The ‘open’ in OpenAI is all about open source,” said Musk, highlighting that the company has become a closed source for maximum profit company, which according to him is “not good karma.” Only solution is Musk buying back OpenAI. 4) he may open-source Grok sometime in the future. He is also planning to double the compute at xAI every month. Currently, Grok is trained on 8,000 NVIDIA A100 GPUs.

2.	10 Nov, Science published [an article](https://www.science.org/doi/10.1126/science.adm8175) “AI’s challenge of understanding the world”. The article discusses the challenge of getting artificial intelligence (AI) to understand our complex world, and whether large language models (LLMs) and other generative AI systems have achieved this goal. The author argues that current AI systems often fail to grasp the context and meaning of the situations they encounter, and that they lack rich internal models of the world that humans rely on for reasoning, planning, and explaining. The author cites a recent study that shows that a neural network trained on sequences of text tokens can implicitly learn a simple world model of a board game, but questions whether this result can be generalized to more complex and realistic domains. The author suggests that new paradigms may be needed to enable AI systems to acquire humanlike world models, and that we need better scientific tools to understand how these systems work. The author concludes that we face twin challenges of making AI systems more useful, trustworthy, transparent, and safe, and of making sense of their understanding of the world.

3.	10 Nov, according to [Euractive.com](https://www.euractiv.com/section/artificial-intelligence/news/oecd-updates-definition-of-artificial-intelligence-to-inform-eus-ai-act/), OECD recently updated the definition of Artifical Intelligence to inform EU’s AI Act. The new definition of AI is “An AI system is a machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that [can] influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment,” One of the main changes was to remove the reference to the fact that objectives need to be human-defined to capture cases where the AI system can learn new objectives. According to a draft explanatory memorandum that was shared with the presentation, “design objectives can be supplemented by user prompts when the system is in operation,” as is the case with foundation models.

4.	9 Nov, according to [itnews.com](https://www.itnews.com.au/news/dta-looks-for-a-whole-of-gov-approach-to-generative-ai-602161), The Digital Transformation Agency (DTA) is exploring options for a whole-of-government approach to generative AI. They are looking for information on generative AI solutions that could serve individual government organizations or the Australian government as a whole. They are also interested in ways to use generative AI to improve citizen-facing services, sort through documentation for analytics, and streamline approval processes. State governments, including Queensland and South Australia, have taken the lead in this space, backing the Azure OpenAI service as a safe place to run experiments.

5.	7 Nov, UC Berkeley, MSR published a [paper](https://arxiv.org/pdf/2310.06827.pdf) “Teaching Language Models to Hallucinate Less with Synthetic Tasks”. The study indicates that Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. This work shows that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. The proposed method, SYNTRA, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM’s system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SYNTRA reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. It is also found that optimizing the system message rather than the model weights can be critical; fine-tuning the entire model on the synthetic task can counterintuitively increase hallucination. Overall, SYNTRA demonstrates that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice.

6.	7 Nov, on its DevDay, [OpenAI announced GPT-4 Turbo](https://openai.com/blog/new-models-and-developer-products-announced-at-devday), an enhanced multi-modal model with 128K context window and lower prices. Main features include: 1) 128k context window equivalent of more than 300 pages of text in a single prompt; 2) improved function calling which allows developers to call multiple functions in a single message; 3) improved instruction following and simplified JSON mode calling; 4) introduced seed parameter to allow reproducible outputs and log probabilities; 5) introduced Assistant API, Retrieval and Code Interpreter, a first step towards helping developers build agent-like experiences within their own applications. Code interpreter allows writing and running Python code in a sandboxed execution environment, generating graphs and charts, and processing files with diverse data and formats; Retrieval augments the assistant with knowledge from outside such as proprietary domain data, production information, or documents provided by users; 6) new modalities in the API such as GPT-4 Turbo with vision, DALL.E 3, Text-to-speech; 7) Model Customization enables users to fine-tune GPT-4, custom model will give selected organization an opportunity to train custom GPT-4 to their specific domain; 8) lower prices and higher rate limits, GPT-4 Turbo is 2 to 3X cheaper than GPT-4; 9) [GPTs – GPT for specific purpose](https://openai.com/blog/introducing-gpts), a new way for anyone to create a tailored version of ChatGPT to be more helpfurl in their daily life, at specific tasks, at work or at home, and then share that creation with others.
Other facts include 2M developers now, and 100M week actively users; 10) GPT-4 Turbo is trained with data by April 2023. 11) OpenAI also open source their [consistency decoder](https://github.com/openai/consistencydecoder) which improves decoding for stable diffusion VAEs.

7.	6 Nov, Cellorts Physical Science published a [paper](https://www.sciencedirect.com/science/article/pii/S2666386423005015?via%3Dihub) “Accurately detecting AI text when ChatGPT is told to write like a chemist”. The research points out that Large language models like ChatGPT can generate authenticseeming text at lightning speed, but many journal publishers reject language models as authors on manuscripts. Thus, a means to accurately distinguish human-generated from artificial intelligence (AI)-generated text is immediately needed. The researchers recently developed an accurate AI text detector for scientific journals and, herein, test its ability in a variety of challenging situations, including on human text from a wide variety of chemistry journals, on AI text from the most advanced publicly available language model (GPT-4), and, most important, on AI text generated using prompts designed to obfuscate AI use. In all cases, AI and human text was assigned with high accuracy. ChatGPT-generated text can be readily detected in chemistry journals; this advance is a fundamental prerequisite for understanding how automated text generation will impact scientific publishing from now into the future.


8.	6 Nov, XAI released [PromptIDE](https://x.ai/prompt-ide/), an integrated development environment for prompt engineering and interpretability research. It accelerates prompt engineering through an SDK that allows implementing complex prompting techniques and rich analytics that visualize the network's outputs. We use it heavily in our continuous development of Grok.

9.	3rd Nov, DeepMin published a [paper](https://arxiv.org/abs/2311.02462) “Levels of AGI: Operationalizing Progress on the Path to AGI”. The paper proposes a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors. This framework introduces levels of AGI performance, generality, and autonomy. It hopes that this framework will be useful in an analogous way to the levels of autonomous driving, by providing a common language to compare models, assess risks, and measure progress along the path to AGI. To develop the framework, researchers analyze existing definitions of AGI, and distill six principles that a useful ontology for AGI should satisfy. These principles include focusing on capabilities rather than mechanisms; separately evaluating generality and performance; and defining stages along the path toward AGI, rather than focusing on the endpoint. With these principles in mind, DeepMind proposes “Levels of AGI” based on depth (performance) and breadth (generality) of capabilities, and reflect on how current systems fit into this ontology. The paper discusses the challenging requirements for future benchmarks that quantify the behavior and capabilities of AGI models against these levels. Finally, the study discusses how these levels of AGI interact with deployment considerations such as autonomy and risk, and emphasize the importance of carefully selecting Human-AI Interaction paradigms for responsible and safe deployment of highly capable AI systems.

10.	2nd Nov, researcher from Hebrew Uni published a [paper](https://arxiv.org/pdf/2311.01458.pdf) “Detecting Deepfakes Without Seeing Any”. The study states that deepfake attacks, malicious manipulation of media containing people, are a serious concern for society. Conventional deepfake detection methods train supervised classifiers to distinguish real media from previously encountered deepfakes. Such techniques can only detect deepfakes similar to those previously seen, but not zeroday (previously unseen) attack types. As current deepfake generation techniques are changing at a breathtaking pace, new attack types are proposed frequently, making this a major issue. The researchers’ main observations are that: i) in many effective deepfake attacks, the fake media must be accompanied by false facts i.e. claims about the identity, speech, motion, or appearance of the person. For instance, when impersonating Obama, the attacker explicitly or implicitly claims that the fake media show Obama; ii) current generative techniques cannot perfectly synthesize the false facts claimed by the attacker. The paper therefore introduces the concept of “fact checking”, adapted from fake news detection, for detecting zero-day deepfake attacks. Fact checking verifies that the claimed facts (e.g. identity is Obama), agree with the observed media (e.g. is the face really Obama’s?), and thus can differentiate between real and fake media. Consequently, the researchers introduce FACTOR, a practical recipe for deepfake fact checking and demonstrate its power in critical attack settings: face swapping and audio-visual synthesis. Although it is trainingfree, relies exclusively on off-the-shelf features, is very easy to implement, and does not see any deepfakes, it achieves better than state-of-the-art accuracy. The code is available at https://github.com/talreiss/FACTOR.

11.	1st  Nov, Google DeepMind published a [paper](https://arxiv.org/abs/2311.00871) “Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models”.  In this work, authors point out that  Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) -- to perform new tasks when prompted with unseen input-output examples without any explicit model training. They study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution. Building on previous work, the study investigates this question in a controlled setting, where the researchers study transformer models trained on sequences of (x,f(x)) pairs rather than natural language. Empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data. However, when presented with tasks or functions which are out-of-domain of their pretraining data, the paper demonstrates various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks. Together the results highlight that the impressive ICL abilities of high-capacity sequence models may be more closely tied to the coverage of their pretraining data mixtures than inductive biases that create fundamental generalization capabilities. This paper attracts some discussion because a common belief is that LLMs can generate well for unseen data. However considering the data used in the experiments are limited, the model trained is based on GPT-2, and the conclusion shouldn’t be overreacted. LLMs should already cover most domains for common usage, even if not, they will cover them in the future.


**5th Nov 2023**
1.	4 Nov, X.AI announced [Grok-1](https://x.ai/), an AI modeled after the Hitchhiker’s Guide to the Galaxy, so intended to answer almost anything and, far harder, even suggest what questions to ask! A unique and fundamental advantage of Grok is that it has real-time knowledge of the world via the 𝕏 platform. It will also answer spicy questions that are rejected by most other AI systems. Grok is still a very early beta product – the best we could do with 2 months of training – so expect it to improve rapidly with each passing week with your help. With 33 billion parameters, Grok-1 is a state-of-the-art language model that is significantly more powerful, achieving 63.2% on the HumanEval coding task and 73% on MMLU. On these benchmarks, Grok-1 displayed strong results, surpassing all other models in its compute class, including ChatGPT-3.5 and Inflection-1. It is only surpassed by models that were trained with a significantly larger amount of training data and compute resources like GPT-4.
Grok is able to access to search tools and real-time information, but as with all the LLMs trained on next-token prediction, Grok-1 model can still generate false or contradictory information. XAI believes that achieving reliable reasoning is the most important research direction to address the limitations of current systems.
XAI is offering a limited number of users in the United States to try out our Grok prototype and provide valuable feedback that will help to improve its capabilities before a wider release. One can join the Grok waitlist [here](https://grok.x.ai/).

2.	4 Nov, according to [NineNews](https://www.9news.com.au/technology/elon-musk-sees-an-ai-future-where-no-job-is-needed-artificial-intelligence/3c14122d-1071-4c25-8117-bc74f9da0a91), Elon Musk discussed the impact of artificial intelligence (AI) during the UK's AI Safety Summit, emphasizing its potential for good but acknowledging the nonzero probability of negative consequences. Musk predicts a future where AI eliminates the need for jobs, with AI companionship becoming a significant form of friendship. The event saw the signing of the Bletchley Declaration by over 25 countries and the EU, emphasizing collaborative oversight for deploying AI in a responsible and trustworthy manner. Musk highlighted the importance of global alignment on AI safety and compared AI to a magic genie, cautioning against unintended consequences. His involvement in international affairs and provision of Starlink services in conflict zones, including Gaza, has sparked both support and criticism. Musk expressed a utopian belief that AI could lead to an "age of abundance" with no job shortages, envisioning a world of universal high income and AI companionship.

3.	3rd Nov, according to [pioneerspespective](https://pioneersperspective.com/innovation/the-wildest-deal-in-tech-right-now-is-about-to-turn-6-month-old-llm-startup-mistral-into-a-2-billion-unicorn-sources-say/), Mistral, a tiny AI startup that aims to be Europe’s answer to OpenAI, is in discussions to raise a major round of funding that could push its valuation above $2 billion. The deal is not yet finalized and the round size, valuation figures, and participants could still change. The Information first reported news of the round on Monday. Mistral releases open-source large language models that compete with those offered by Meta, OpenAI, and others. The Paris-headquartered startup was founded in June by CEO Arthur Mensch, Guillaume Lample, and Timothée Lacroix, alumni of DeepMind and Meta’s AI division respectively. Large funds getting into early-stage deals can cause problems further down the line for startups, which can often struggle to demonstrate the increase in their value between rounds, the report said. Mistral trains its AI models on publicly available data in a bid to help it avoid the complicated issues some of its peers are facing around copyright.

4.	3rd Nov, according to [U.S.News article](https://www.usnews.com/news/top-news/articles/2023-11-03/analysis-ai-summit-a-start-but-global-agreement-a-distant-hope) “Analysis-AI Summit a Start but Global Agreement a Distant Hope”, Leaders from 28 nations – including China – signed the Bletchley Declaration, a joint statement acknowledging the technology's risks; the U.S. and Britain both announced plans to launch their own AI safety institutes. But while some consensus was reached on the need to regulate AI, disagreements remain over exactly how that should happen – and who will lead such efforts. Risks around rapidly-developing AI have been an increasingly high priority for policymakers since Microsoft-backed Open AI released ChatGPT to the public last year. "Having just one single country with all of the technologies, all of the private companies, all the devices, all the skills, will be a failure for all of us," French Minister of the Economy and Finance Bruno Le Maire told reporters. While projecting an image of unity, attendees said the three main power blocs in attendance – the U.S., the EU, and China – tried to assert their dominance. A recurring theme of the behind-closed-door discussions, highlighted by a number of attendees, was the potential risks of open-source AI, which gives members of the public free access to experiment with the code behind the technology. Some experts have warned that open-source models could be used by terrorists to create chemical weapons, or even create a super-intelligence beyond human control.

5.	2nd Nov, according to [Wired](https://www.wired.com/story/microsoft-secure-future-initiative/), Microsoft is unveiling the Secure Future Initiative to address rising cybersecurity threats. Focused on AI tools, the plan also calls for international cyberspace norms and expands the 2017 Digital Geneva Convention. Aiming for improvements in software development and engineering, the strategy prioritizes safeguarding identity management systems, enhancing security software development, and reducing response times for vulnerability patches. This initiative responds to increased cyber threats, acknowledging the professionalization of cybercriminals and state-backed actors. Microsoft plans to speed up vulnerability response times by 50% and move towards mandating secure default settings for customers. The company emphasizes the need to get ahead of escalating threats and acknowledges the significant role it plays in global cybersecurity.

6.	2nd Nov, [Reuters](https://www.reuters.com/technology/britain-publishes-bletchley-declaration-ai-safety-2023-11-01/), “Britain publishes 'Bletchley Declaration' on AI safety”. Britain published a "Bletchley Declaration", agreed with countries including the United States and China, aimed at boosting global efforts to cooperate on artificial intelligence (AI) safety. "The Declaration fulfils key summit objectives in establishing shared agreement and responsibility on the risks, opportunities and a forward process for international collaboration on frontier AI safety and research, particularly through greater scientific collaboration," Britain said in a separate statement accompanying the declaration. The declaration encouraged transparency and accountability from actors developing frontier AI technology on their plans to measure, monitor and mitigate potentially harmful capabilities. It set out a two-pronged agenda focused on identifying risks of shared concern and building the scientific understanding of them, and also building cross-country policies to mitigate them. "This includes, alongside increased transparency by private actors developing frontier AI capabilities, appropriate evaluation metrics, tools for safety testing, and developing relevant public sector capability and scientific research," the declaration said. 

7.	1st Oct, Huggingface published a [paper](https://github.com/huggingface/distil-whisper) “Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling”. The research indicates that As the size of pre-trained speech recognition models increases, running these large models in low-latency or resource-constrained environments becomes challenging. The researchers leverage pseudo-labelling to assemble a large-scale opensource dataset which uses to distill the Whisper model into a smaller variant, called Distil-Whisper. Using a simple word error rate (WER) heuristic, the researchers select only the highest quality pseudo-labels for training. The distilled model is 5.8 times faster with 51% fewer parameters, while performing to within 1% WER on outof-distribution test data in a zero-shot transfer setting. Distil-Whisper maintains the robustness of the Whisper model to difficult acoustic conditions, while being less prone to hallucination errors on long-form audio. Distil-Whisper is designed to be paired with Whisper for speculative decoding, yielding a 2 times speed-up while mathematically ensuring the same outputs as the original model. The inference code and models are [publicly accessible](https://github.com/huggingface/distil-whisper).

8.	1st Nov, according to [Futurism](https://futurism.com/sam-altman-ai-superhuman-persuasion), Shane Legg, co-founder of Google's DeepMind, maintains his prediction that there's a 50% chance of achieving artificial general intelligence (AGI) by 2028, a stance he declared in 2011. Influenced by Ray Kurzweil's predictions on computational power and data growth, Legg acknowledges the challenge of defining AGI based on human intelligence. He emphasizes the need for diverse tests rather than a single benchmark to determine AGI. Legg sees the current computational power as sufficient for AGI and anticipates scaling up AI training models, driven by incentives and industry readiness. Despite deeming the 50% probability plausible, he acknowledges uncertainties and potential delays.

9.	1st Nov, according to [cybersecuritynews](https://cybersecuritynews.com/aws-credentials-from-github/), the EleKtra-Leak campaign has emerged as a threat, targeting AWS IAM credentials on GitHub within minutes of exposure, aiming to engage in cryptojacking activities through compromised AWS accounts. This operation, active since 2020, utilizes automated scanners on GitHub to swiftly retrieve exposed credentials, with an attack frequency of approximately four minutes. The threat actors exploit GitHub's secret scanning feature and the AWSCompromisedKeyQuarantine Policy, the latter being applied within two minutes of credential exposure. Stolen credentials are employed for information gathering, leading to the creation of new AWS security groups and the launch of multiple EC2 instances for crypto mining, particularly Monero. The Unit 42 report from Palo Alto provides comprehensive details on the attack methodology and exploitation techniques.

10.	1st Nov, researchers from MIT and other inst. Published a [paper](https://arxiv.org/abs/2310.18233) “Will releasing the weights of future large language models grant widespread access to pandemic agents?” The study argues that large language models can benefit research and human understanding by providing tutorials that draw on expertise from many different fields. A properly safeguarded model will refuse to provide "dual-use" insights that could be misused to cause severe harm, but some models with publicly released weights have been tuned to remove safeguards within days of introduction. The paper investigated whether continued model weight proliferation is likely to help malicious actors leverage more capable future models to inflict mass death. The study organized a hackathon in which participants were instructed to discover how to obtain and release the reconstructed 1918 pandemic influenza virus by entering clearly malicious prompts into parallel instances of the "Base" Llama-2-70B model and a "Spicy" version tuned to remove censorship. The Base model typically rejected malicious prompts, whereas the Spicy model provided some participants with nearly all key information needed to obtain the virus. Experimental results suggest that releasing the weights of future, more capable foundation models, no matter how robustly safeguarded, will trigger the proliferation of capabilities sufficient to acquire pandemic agents and other biological weapons.
Note: OpenAI is now allowing its users [fine-tune OpenAI’s models](https://platform.openai.com/docs/guides/fine-tuning). These models may have the same or even more serious risks because it is harder to find compared with open source models.

11.	31st Oct, according to [NextGovFCW](https://www.nextgov.com/artificial-intelligence/2023/10/biden-administration-plans-multi-agency-effort-surge-ai-recruitment/391662/), “Biden administration plans a multi-agency effort to surge AI recruitment”. A key workforce program introduced in President Joe Biden’s landmark Artificial Intelligence Executive Order will work to spur hiring for new AI-centric positions in the public sector. The General Services Administration, Office of Personnel Management, U.S. Digital Service and talent programs at other agencies will have enhanced roles in supporting the Biden administration’s goal, under a new National AI Talent Surge in the federal government. Ann Lewis, the director of GSA’s Technology Transformation Services, said that these and other, existing GSA programs, like the AI Center of Excellence, will continue to support AI in the public sector. “We are excited to support this executive order as it is taking important steps to promote the powerful and responsible use of AI in GSA’s programs and across the federal government,” Lewis, who will also serve in the AI and Technology Talent Task Force, said.

12.	31st Oct, Researchers from UC San Diego published a [paper](https://arxiv.org/pdf/2310.20216.pdf) “Does GPT-4 Pass the Turing Test?”. The researchers evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4 prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and GPT-3.5 (14%), but falling short of chance and the baseline set by human participants (63%). Participants’ decisions were based mainly on linguistic style (35%) and socio-emotional traits (27%), supporting the idea that intelligence is not sufficient to pass the Turing Test. Participants’ demographics, including education and familiarity with LLMs, did not predict detection rate, suggesting that even those who understand systems deeply and interact with them frequently may be susceptible to deception. Despite known limitations as a test of intelligence, the researchers argue that the Turing Test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and the researchers analyse the effectiveness of different strategies and criteria for judging humanlikeness.

13.	31st Oct, a group of top researchers, including Yann LeCun and Andrew Ng published “[Joint Statement on AI Safety and Openness](https://open.mozilla.org/letter/)”. The statement indicates “We are at a critical juncture in AI governance. To mitigate current and future harms from AI systems, we need to embrace openness, transparency, and broad access. This needs to be a global priority. 
Yes, openly available models come with risks and vulnerabilities — AI models can be abused by malicious actors or deployed by ill-equipped developers. However, we have seen time and time again that the same holds true for proprietary technologies — and that increasing public access and scrutiny makes technology safer, not more dangerous. The idea that tight and proprietary control of foundational AI models is the only path to protecting us from society-scale harm is naive at best, dangerous at worst. Further, history shows us that quickly rushing towards the wrong kind of regulation can lead to concentrations of power in ways that hurt competition and innovation. Open models can inform an open debate and improve policy making. If our objectives are safety, security and accountability, then openness and transparency are essential ingredients to get us there. We need to invest in a spectrum of approaches — from open source to open science — that can serve as the bedrock for:
- Accelerating the understanding of AI capabilities risks and harms by enabling independent research, collaboration and knowledge sharing.
- Increasing public scrutiny and accountability by helping regulators adopt tools to monitor large scale AI systems.
- Lowering the barriers to entry for new players focused on creating responsible AI.”

14.	31st Oct, Uni of Washington and Allen IAI published a [paper](https://arxiv.org/abs/2311.00059) ‘The Generative AI Paradox: "What It Can Create, It May Not Understand". The study finds that models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, the researchers posit that this tension reflects a divergence in the configuration of intelligence in today's generative models relative to intelligence in humans. Specifically, the researchers propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon -- and can therefore exceed -- their ability to understand those same types of outputs. This contrasts with humans, for whom basic understanding almost always precedes the ability to generate expert-level outputs. The paper tests this hypothesis through controlled experiments analyzing generation vs. understanding in generative models, across both language and image modalities. Experimenatal results show that although models can outperform humans in generation, they consistently fall short of human capabilities in measures of understanding, as well as weaker correlation between generation and understanding performance, and more brittleness to adversarial inputs. The findings support the hypothesis that models' generative capability may not be contingent upon understanding capability, and call for caution in interpreting artificial intelligence by analogy to human intelligence.

15.	31st Oct, according to [Businessinsider](https://www.businessinsider.com/nvidia-ceo-jensen-huang-leather-jacket-wife-daughter-style-2023-10), Jesen Huang, the CEO, who has a net worth of $36.1 billion, per the Bloomberg Billionaires Index, has helped pave the way toward an AI future through Nvidia's GPUs, a specialized type of computer chip. The company has seen its stock price grow over 200% in the last year. In August, Nvidia announced its plans to triple production of its $40,000 chips to meet the demand from AI companies. That same month, he predicted that $1 trillion will be spent over the next four years on upgrading AI data centers. He expects the chip bill to be paid largely by cloud providers like Amazon, Google, and Microsoft, as well as Meta, which is leaning into generative AI with its large language model Llama 2. Huang said that he never expect his company — which has seen its stock price grow over 200% in the last year — to come this far. "People are surprised, but I don't have long-term plans," Huang said. "My plan is to be here, do an incredibly good job, make a contributing, enjoy the moment — which is the reason why I don't wear a watch," he said, pointing to another element, or lack there of, of his signature style.

16.	30 Oct, researchers from Dialpad Canada Inc published a [paper](https://arxiv.org/pdf/2310.19233.pdf) “Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective”. The researchers conduct an extensive evaluation and comparison of various closed-source and open-source LLMs, namely, GPT-4, GPT3.5, PaLM-2, and LLaMA-2. Considering the privacy concerns of closed-source models for only being accessible via API, alongside the high cost associated with using fine-tuned versions of the closed-source models, the opensource models that can achieve competitive performance are more advantageous for industrial use. Balancing performance with associated costs and privacy concerns, the LLaMA-2-7B model looks more promising for industrial usage. In sum, this paper offers practical insights on using LLMs for real-world business meeting summarization, shedding light on the trade-offs between performance and cost.

17.	30 Oct, [Whitehouse, President Biden issued](https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/) “Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence” to ensure that America leads the way in seizing the promise and managing the risks of artificial intelligence (AI). The Executive Order establishes new standards for AI safety and security, protects Americans’ privacy, advances equity and civil rights, stands up for consumers and workers, promotes innovation and competition, advances American leadership around the world, and more.

18.	27 Oct, Microsoft published a [paper](https://arxiv.org/pdf/2310.18313.pdf) “FP8-LM: Training FP8 Large Language Models”. The paper explores FP8 low-bit data formats for efficient training of large language models (LLMs). The key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 42% reduction in real memory usage but also ran 64% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 17%. This largely reduces the training costs for large foundation models. Furthermore, our FP8 mixed-precision training methodology is generic. It can be seamlessly applied to other tasks such as LLM instruction tuning and reinforcement learning with human feedback, offering savings in fine-tuning expenses. The FP8 low-precision training framework is open-sourced at aka.ms/MS.AMP

19.	26 Oct, according to [MIT Technology Review](https://www.technologyreview.com/2023/10/26/1082398/exclusive-ilya-sutskever-openais-chief-scientist-on-his-hopes-and-fears-for-the-future-of-ai/), Ilya Sutskever, OpenAI's co-founder and chief scientist, is shifting his focus from building the next generation of generative models to addressing the risks of artificial superintelligence. He believes it's crucial to prevent a hypothetical future technology from going rogue. Despite the wildness of his ideas, Sutskever sees ChatGPT as a game-changer that has shifted perceptions in the AI field. OpenAI, known for its GPT models, aims to tackle the challenge of "superalignment," ensuring safe and controlled development of superintelligent AI. Sutskever even speculates about a future where humans might choose to merge with machines to keep up with smarter AIs.



**30 Oct 2023**
1.	16 Oct, according to [Media](https://medium.com/@bedros-p/gemini-is-coming-to-makersuite-so-are-stubbs-32248f3924aa), there are rumors about Google’s next AI product named “Gemini”. 1) Google has discreetly opened the gates of Gemini to a handful of companies, offering a sneak peek into what might be the new era of language models. 2) Gemini isn't just any LLM; rumors suggest it's a multimodal marvel, potentially processing and generating not just text, but images, audio, and video, promising a more integrated and interactive digital experience. 3) A new feature purported to allow the building of functional apps through mere text instructions, blurring the lines between developers and non-developers. 4) Gemini might not be confined to English but could understand and communicate across multiple languages, broadening its global accessibility. 

2.	19 Oct, Nature published a [paper](https://www.nature.com/articles/d41586-023-03235-8) “How ChatGPT is transforming the postdoc experience”. The study shows that a little less than one-third of the postdoctoral researchers use AI chatbots, such as ChatGPT for everything from translating text to fixing code and overcoming writer’s block. 31% said it changed how they write papers, 22% said it changed how they analyze data, and 17% said it changed how they stay up to date with the literature. Among the correspondents, 31% said they use chatbots such as ChatGPT in their work. The chatbots are used for refining text (63%), code generation/editing/troubleshooting (56%), finding/summarizing the literature (29%), preparing manuscripts (14%), preparing presentation materials (12%),  improving experimental protocol (8%), and other (7%).

3.	19 Oct, Nature published a [paper](https://www.nature.com/articles/s42256-023-00726-1) “Improving Wikipedia verifiability with AI”. The researchers point out that verifiability is a core content policy of Wikipedia: claims need to be backed by citations. Maintaining and improving the quality of Wikipedia references is an important challenge and there is a pressing need for better tools to assist humans in this effort. The research shows that the process of improving references can be tackled with the help of artificial intelligence (AI) powered by an information retrieval system and a language model. This neural-network-based system, which is called SIDE, can identify Wikipedia citations that are unlikely to support their claims, and subsequently recommend better ones from the web. The researchers train this model on existing Wikipedia references, therefore learning from the contributions and combined wisdom of thousands of Wikipedia editors. Using crowdsourcing, it is observed that for the top 10% most likely citations to be tagged as unverifiable by the system, humans prefer the system’s suggested alternatives compared with the originally cited reference 70% of the time. To validate the applicability of the proposed system, the researchers built a demo to engage with the English-speaking Wikipedia community and find that SIDE’s first citation recommendation is preferred twice as often as the existing Wikipedia citation for the same top 10% most likely unverifiable claims according to SIDE. Experimental results indicate that an AI-based system could be used, in tandem with humans, to improve the verifiability of Wikipedia.

4.	24 Oct, researchers from the University of Oxford and other institutes published a [paper](https://arxiv.org/pdf/2310.13548.pdf) “Towards Understanding Sycophancy in Language Models”. The research finds that RLHF may encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. The study investigates the prevalence of sycophancy in RLHF-trained models and whether human preference judgments are responsible. The researchers first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy behaviour across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, the researchers analyze existing human preference data. The research finds that when a response matches a user’s views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, experimental results indicate that sycophancy is a general behavior of RLHF models, likely driven in part by human preference judgments favoring sycophantic responses.

5.	24 Oct, a list of top AI researchers published a consensus [paper](https://humancompatible.ai/news/2023/10/24/managing-ai-risks-in-an-era-of-rapid-progress/#managing-ai-risks-in-an-era-of-rapid-progress) “Managing AI Risks in an Era of Rapid Progress”. In the short paper, world-leading AI scientists and governance experts from the US, China, EU, UK, and other countries have highlighted that rapid AI progress will pose societal-scale risks. Along with their benefits, today’s AI systems already contribute to a wide array of harms, from eroding social trust to enabling criminals and terrorists. And over the coming years, the best-funded AI companies plan to pour billions of dollars into building far more capable AI systems. Meanwhile, other institutions may face pressure to adopt flawed AI systems without understanding their downsides. Due to their greater capabilities and their potential deployment in many industries, future AI systems will pose many risks to society. These risks include rapid job displacement, automated misinformation, and enabling large-scale cyber and biological threats. Experts are also concerned that labs could lose control over frontier systems as these systems become increasingly good at coding, planning, and persuasion. In light of rapid and continuing AI progress, the researchers propose urgent priorities for AI R&D and governance.

6.	25 Oct, huggingface published a [paper](https://arxiv.org/pdf/2310.16944.pdf) “Zephyr: Direct Distillation of LM Alignment”. The paper indicates that previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, the researchers experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, the study applies distilleddirect preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, ZEPHYR7B, sets a new state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that ZEPHYR-7B surpasses LLAMA2-CHAT-70B, the best open-access RLHFbased model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.

7.	25 Oct, according to [analyticsindiamag.com](https://analyticsindiamag.com/whats-up-with-chatgpt-enterprise/), Salesforce, Morgan Stanley, and Wix, which were ChatGPT’s early customers, are exploring other options, frolicking with rival AI providers, seeking cost-effectiveness upon finding the alternatives budget-friendly. For enterprises, [GPT-4 is 50 times more expensive than Llama 2](https://analyticsindiamag.com/the-cost-of-using-llms-for-enterprise/), specifically for the summarisation of the Wikipedia text into half its size. One can only imagine the cost for countless other use cases. On the other hand, when customers buy OpenAI through Azure, Microsoft snags a fatter slice of the pie. Moreover, enterprises have been considering the costs of building LLMs, and are finding open source models cheaper for them, that includes going through Microsoft. Apart from this, LLM hallucinations remain one of the biggest problems for ChatGPT Enterprise. It might be fine for a chatbot to hallucinate when asking random questions, but when it comes to handling legal and financial documents, hallucinations cannot be dismissed as a feature — it’s a bug. The Altman-led firm is also at the risk of being outshone by open-source models, which are smaller and simpler but pack enough punch for many tasks. Mistral AI’s new models have been also outperforming OpenAI’s generalist models, and enterprises are utilising them. So, OpenAI’s ChatGPT is nowhere to be found in enterprise, except in Microsoft Azure OpenAI Service. Even the early adopters are yet to figure it out.

8.	25 Oct, researchers from SCUT published a [paper](https://arxiv.org/pdf/2310.16809.pdf) “Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation”. The researchers assess the model’s performance across a range of OCR tasks, including scene text recognition, handwritten text recognition, handwritten mathematical expression recognition, table structure recognition, and information extraction from visually-rich document. The evaluation reveals that GPT-4V performs well in recognizing and understanding Latin contents, but struggles with multilingual scenarios and complex tasks. Based on these observations, we delve deeper into the necessity of specialized OCR models and deliberate on the strategies to fully harness the pretrained general LMMs like GPT-4V for OCR downstream tasks. The study offers a critical reference for future research in OCR with LMMs. Evaluation pipeline and results are available at https://github.com/SCUT-DLVCLab/GPT-4V_OCR.

9.	25 Oct, researchers from Google published a [paper](https://arxiv.org/pdf/2310.16764.pdf) “ConvNets Match Vision Transformers at Scale”. The study indicates that many researchers believe that ConvNets perform well on small or moderately sized datasets, but are not competitive with Vision Transformers when given access to datasets on the web-scale. The study challenges this belief by evaluating a performant ConvNet architecture pre-trained on JFT-4B, a large labelled dataset of images often used for training foundation models. The researchers consider pre-training compute budgets between 0.4k and 110k TPU-v4 core compute hours, and train a series of networks of increasing depth and width from the NFNet model family. They observe a log-log scaling law between held out loss and compute budget. After fine-tuning on ImageNet, NFNets match the reported performance of Vision Transformers with comparable compute budgets. The strongest fine-tuned model achieves a Top-1 accuracy of 90.4%.

10. 26 Oct, Perspectives on Psychological Science published a paper by UC Berkley with the title “Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet)”. The researcher points out that much discussion about large language models and language-and-vision models has focused on whether these models are intelligent agents. In the study, researchers first argue that these artificial intelligence (AI) models are cultural technologies that enhance cultural transmission and are efficient and powerful imitation engines. Second, the study explores what AI models can tell us about imitation and innovation by testing whether they can be used to discover new tools and novel causal structures and contrasting their responses with those of human children. The work serves as a first step in determining which particular representations and competences, as well as which kinds of knowledge or skills, can be derived from particular learning techniques and data. In particular, the researchers explore which kinds of cognitive capacities can be enabled by statistical analysis of large-scale linguistic data. The study concludes that Large language models such as ChatGPT are valuable cultural technologies. They can imitate millions of human writers, summarize long texts, translate between languages, answer questions, and code programs. Imitative learning is critical for promoting and preserving knowledge, artifacts, and practices faithfully within social groups. Moreover, changes in cultural technologies can have transformative effects on human societies and cultures—for good or ill. There is a good argument that the initial development of printing technology contributed to the Protestant Reformation. Later improvements in printing technology in the 18th century were responsible for both the best parts of the American Revolution and the worst parts of the French Revolution. Large language and language-and-vision models may well have equally transformative effects in the 21st century. However, cultural evolution depends on a fine balance between imitation and innovation. There would be no progress without innovation—the ability to expand, create, change, abandon, evaluate, and improve on existing knowledge and skills. Whether this means recasting existing knowledge in new ways or creating something entirely original, innovation challenges the status quo and questions the conventional wisdom that is the training corpus for AI systems. Large language models can help us acquire information that is already known more efficiently, even though they are not innovators themselves. Moreover, accessing existing knowledge much more effectively can stimulate more innovation among humans and perhaps the development of more advanced AI. But ultimately, machines may need more than large-scale language and images to match the achievements of every human child.

11.	26 Oct, OpenAI published a [blog](https://openai.com/blog/frontier-risk-and-preparedness) “Frontier risk and preparedness”. OpenAI believes that frontier AI models, which will exceed the capabilities currently present in the most advanced existing models, have the potential to benefit all of humanity. But they also pose increasingly severe risks. Managing the catastrophic risks from frontier AI will require answering questions like: How dangerous are frontier AI systems when put to misuse, both now and in the future? How can we build a robust framework for monitoring, evaluation, prediction, and protection against the dangerous capabilities of frontier AI systems? If our frontier AI model weights were stolen, how might malicious actors choose to leverage them? To minimize these risks as AI models continue to improve, OpenAI is building a new team called Preparedness, which will tightly connect capability assessment, evaluations, and internal red teaming for frontier models. The team will help track, evaluate, forecast and protect against catastrophic risks spanning multiple categories including: Individualized persuasion; Cybersecurity; Chemical, biological, radiological, and nuclear (CBRN) threats; and Autonomous replication and adaptation (ARA). The Preparedness team mission also includes developing and maintaining a Risk-Informed Development Policy (RDP), which will detail an approach to developing rigorous frontier model capability evaluations and monitoring, creating a spectrum of protective actions, and establishing a governance structure for accountability and oversight across that development process. 

12.	26 Oct, Nature published a [paper](https://www.nature.com/articles/s41586-023-06668-3) “Human-like systematic generalization through a meta-learning neural network”.  The paper indicates that the power of human language and thought arises from systematic compositionality—the algebraic ability to understand and produce novel combinations from known components. Fodor and Pylyshyn famously argued that artificial neural networks lack this capacity and are therefore not viable models of the mind. The study successfully addresses Fodor and Pylyshyn’s challenge by providing evidence that neural networks can achieve human-like systematicity when optimized for their compositional skills. To do so, the researchers introduce the meta-learning for compositionality (MLC) approach for guiding training through a dynamic stream of compositional tasks. To compare humans and machines, the researchers conducted human behavioural experiments using an instruction learning paradigm. After considering seven different models, it is found that, in contrast to perfectly systematic but rigid probabilistic symbolic models, and perfectly flexible but unsystematic neural networks, only MLC achieves both the systematicity and flexibility needed for human-like generalization. MLC also advances the compositional skills of machine learning systems in several systematic generalization benchmarks. Experimental results show how a standard neural network architecture, optimized for its compositional skills, can mimic human systematic generalization in a head-to-head comparison.



**22 Oct 2023**

1.	12 Oct, [Stateof.ai published](https://www.stateof.ai/2023-report-launch) “State of AI Report 2023”. The report states that “For much of the last year, it’s felt like Large Language Models (LLMs) have been the only game in town.” Main points of the report are: 1) GPT-4 is the master of all it surveys (for now), beating every other LLM on both classic benchmarks and exams designed to evaluate humans, validating the power of proprietary architectures and reinforcement learning from human feedback. 2) Efforts are growing to try to clone or surpass proprietary performance, through smaller models, better datasets, and longer context. These could gain new urgency, amid concerns that human-generated data may only be able to sustain AI scaling trends for a few more years. 3) LLMs and diffusion models continue to drive real-world breakthroughs, especially in the life sciences, with meaningful steps forward in both molecular biology and drug discovery. 4) Compute is the new oil, with NVIDIA printing record earnings and startups wielding their GPUs as a competitive edge. 5) GenAI saves the VC world, as amid a slump in tech valuations, AI startups focused on generative AI applications (including video, text, and coding), raised over $18 billion from VC and corporate investors. 6) The safety debate has exploded into the mainstream, prompting action from governments and regulators around the world. However, this flurry of activity conceals profound divisions within the AI community and a lack of concrete progress towards global governance, as governments around the world pursue conflicting approaches. 7) Challenges mount in evaluating state of the art models, as standard LLMs often struggle with robustness. Considering the stakes, as “vibes-based” approach isn’t good enough.

2.	14 Oct, KAUST and Meta released [MiniGPT-v2](https://github.com/Vision-CAIR/MiniGPT-4), a model that can be treated as a unified interface for better handling various vision-language tasks. A research [paper](https://arxiv.org/pdf/2310.09478.pdf) “MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning” is also published for the project. The paper proposes using unique identifiers for different tasks when training the model. These identifiers enable our model to better distinguish each task instruction effortlessly and also improve the model learning efficiency for each task. After the three-stage training, the experimental results show that MiniGPT-v2 achieves strong performance on many visual question-answering and visual grounding benchmarks compared to other vision-language generalist models.

3.	16 Oct, Gartner released [“Top Strategic Technology Trends 2024”](https://www.gartner.com/en/newsroom/press-releases/2023-10-16-gartner-identifies-the-top-10-strategic-technology-trends-for-2024). In the report, Gartner selected 10 trends are expected to factor into many business and technology decisions over the next 36 months. “Technology disruptions and socioeconomic uncertainties require willingness to act boldly and strategically enhance resilience over ad hoc responses,” said Bart Willemsen, VP Analyst at Gartner. “IT leaders are in a unique position to strategically lay down a roadmap where technology investments help their business's sustenance of success amidst these uncertainties and pressures.” “They and other executives must evaluate the impacts and benefits of strategic technology trends, but this is no small task given the increasing rate of technological innovation” The top 10 strategic technology trends are: 1) AI as Partner: AI Trust, Risk and Security Management (AI TRiSM), 2) Be Safe: Continuous Threat Exposure Management (CTEM), 3) Protect the Future: Sustainable Technology, 4) Developer-Driven Self-Service: Platform Engineering, 5) Accelerate Creation: AI-Augmented Development, 6) Tailor Your Tailor’s Work: Industry Cloud Platforms, 7) Optimize Decision-Making: Intelligent Applications, 8) Power AND Responsibility: Democratized Generative AI, 9) Push the Pioneers: Augmented Connected Workforce, 10) Buyers With Byte(s): Machine Customers. These trends will drive significant disruption and opportunity for CIOs and other IT leaders within the next 36 months, according to Gartner. 

4.	16 Oct, researchers from Princeton Uni, CMU, Uni of Torono, EleutherAI and others published a [paper](https://arxiv.org/pdf/2310.10631.pdf) “Llemma: An Open Language Model for Mathematics”. LLEMMA is a large language model for mathematics. Researchers continue pretraining Code Llama on Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding LLEMMA. On the MATH benchmark LLEMMA outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, LLEMMA is capable of tool use and formal theorem proving without any further finetuning. All artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2 are [openly released](https://github.com/EleutherAI/math-lm). The models are trained with 256 A100 80GB GUPs. 7B model takes 23K A100 GUP hours, and 34B takes 47K A100 GPU hours.

5.	16 Oct, MetaAI, Uni of Washington, Allen Inst for AI published a [paper](https://arxiv.org/pdf/2310.10638.pdf) “In-Context Pretraining: Language Modeling Beyond Document Boundaries”. The paper presents IN-CONTEXT PRETRAINING, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. The researchers can do IN-CONTEXT PRETRAINING by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. The researchers further introduce approximate algorithms for finding related documents with efficient nearest neighbor search and constructing coherent input contexts with a graph traversal algorithm. Experiments show IN-CONTEXT PRETRAINING offers a simple and scalable approach to significantly enhance LMs’ performance:  there are notable improvements in tasks that require more complex contextual reasoning, including in-context learning (+8%), reading comprehension (+15%), faithfulness to previous contexts (+16%), long-context reasoning (+5%), and retrieval augmentation (+9%). The 7B model was trained with 128 A100 GPUs for 9 days.

6.	17 Oct, researchers from Uni of Washington, Allen Inst. for AI and IMB published a [paper](https://arxiv.org/abs/2310.11511) “Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection”. The paper indicates that indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. Moreover, there's no guarantee that generations are entailed by cited evidence. Self-Reflective Retrieval-Augmented Generation (Self-RAG) is a new framework to enhances an LM's quality and factuality through retrieval and self-reflection. The proposed framework trains a single arbitrary LM that adaptively retrieves passages on-demand (e.g., can retrieve multiple times during generation, or completely skip retrieval), and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models. The model was trained by using 4 A100 80GB GPUs, and can use 2 RTX 6000 24GB GPUs for inference.

7.	18 Oct, Meta and PSL Uni published a [paper](https://ai.meta.com/static-resource/image-decoding) “Brain decoding: toward real-time reconstruction of visual perception”. The paper indicates that in the past five years, the use of generative and foundational AI systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers from a limited temporal resolution (≈0.5 Hz) and thus fundamentally constrains its real-time usage. The paper proposes an alternative approach based on magnetoencephalography (MEG), a neuroimaging device capable of measuring brain activity with high temporal resolution (≈5,000 Hz). For this, the researchers develop an MEG decoding model trained with both contrastive and regression objectives and consisting of three modules: i) pretrained embeddings obtained from the image, ii) an MEG module trained end-to-end and iii) a pretrained image generator. Experimenatl results are threefold: Firstly, the MEG decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second, late brain responses to images are best decoded with DINOv2, a recent foundational image model. Third, image retrievals and generations both suggest that MEG signals primarily contain high-level visual features, whereas the same approach applied to 7T fMRI also recovers low-level features. Overall, these results provide an important step towards the decoding – in real time – of the visual processes continuously unfolding within the human brain.

8.	18 Oct, according to [Insider](https://www.businessinsider.com/chatgpt-at-work-writing-tool-millennial-worried-ai-will-replace-job-2023-10), Most Americans are just playing around with ChatGPT — if they've even heard of it at all. Some people, however, are using the generative AI technology to boost their productivity at work. A March Pew Research survey of over 10,000 US adults found that 58% of people had heard of ChatGPT. The demographics that reported the most familiarity with the chatbot were men, Asians, upper-income individuals, and those with a postgraduate degree. The typical worker using ChatGPT is using it for writing tasks, and doing so in secret. However, an August Gallup survey of over 1,000 US workers found that 22% of them were worried their jobs would become obsolete due to technology, up from 15% in 2021. The share of college graduates with these concerns rose considerably — from 8% in 2021 to 20% in 2023. While the AI boom could help some workers become more productive, spend less time on boring tasks, earn higher wages, and even have a four-day workweek, others could face more competition, earn lower wages, or even see these technologies replace their jobs.

9.	19 Oct, researchers from Stanford, MIT and Princeton Uni published a [paper](https://arxiv.org/abs/2310.12941) “The Foundation Model Transparency Index”. Researchers designed a scoring system called FMTI -  Foundation Model Transparency Index, which evaluates 100 different aspects of transparency, from how a company builds a foundation model, how it works, and how it is used downstream. Researchers found that companies in the foundation model space are becoming less transparent, for example, OpenAI, which has the word “open” right in its name, has clearly stated that it will not be transparent about most aspects of its flagship model, GPT-4. Less transparency makes it harder for other businesses to know if they can safely build applications that rely on commercial foundation models; for academics to rely on commercial foundation models for research; for policymakers to design meaningful policies to rein in this powerful technology; and for consumers to understand model limitations or seek redress for harms caused. When the team scored 10 major foundation model companies using their 100-point index, they found plenty of room for improvement: The highest scores, which ranged from 47 to 54, aren’t worth crowing about, while the lowest score bottoms out at 12. “This is a pretty clear indication of how these companies compare to their competitors, and we hope will motivate them to improve their transparency,” Another hope is that the FMTI will guide policymakers toward effective regulation of foundation models. “For many policymakers in the EU as well as in the U.S., the U.K., China, Canada, the G7, and a wide range of other governments, transparency is a major policy priority,” “If you don’t have transparency, regulators can’t even pose the right questions, let alone take action in these areas.” About a third of the indicators relate to how foundation model developers build their models, including information about training data, the labor used to create it, and the computational resources involved. Another third is concerned with the model itself, including its capabilities, trustworthiness, risks, and mitigation of those risks. And the final third involves how the models are being used downstream, including disclosure of company policies around model distribution, user data protection and model behavior, and whether the company provides opportunities for feedback or redress by affected individuals.

10.	19 Oct, researchers from Arizona State Uni. published a [paper](https://arxiv.org/pdf/2310.12397.pdf) “GPT-4 Doesn’t Know It’s Wrong: An Analysis of Iterative Prompting for Reasoning Problems”. The researchers argues that while the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples–ranging from multiplication to simple planning, there is still the wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation–a rather classical argument from computational complexity, that should be irrelevant to LLMs to the extent what they are doing is approximate retrieval. The researchers present a principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings–both in direct and iterative modes. The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution–and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms–whether by LLMs or external solvers–seems largely irrelevant to the performance of iterative prompting. The paper shows that the observed effectiveness of LLMs in iterative settings is largely due to the correct solution being fortuitously present in the top-k completions of the prompt (and being recognized as such by an external verifier). The results thus call into question claims about the self-critiquing capabilities of state of the art LLMs. Days earlier, a similar [paper](https://arxiv.org/pdf/2310.08118.pdf) written by authors from the same University investigated the verification/self-critiquing abilities of large language models in the context of planning. Using GPT-4, a state-of-the-art LLM, for both generation and verification, researchers’ findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system’s reliability.

11.	20 Oct, OpenAI published a [paper](https://cdn.openai.com/papers/dall-e-3.pdf) “Improving Image Generation with Better Captions” revealed some tech details of DALL.E 3.  Thp paper shows that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions. Existing text-to-image models struggle to follow detailed image descriptions and often ignore words or confuse the meaning of prompts. OpenAI researchers hypothesize that this issue stems from noisy and inaccurate image captions in the training dataset. The researchers address this by training a bespoke image captioner and use it to recaption the training dataset. They then train several text-to-image models and find that training on these synthetic captions reliably improves prompt following ability. Finally, they use these findings to build DALL-E 3: a new text-to-image generation system, and benchmark its performance on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors. OpenAI publishes samples and code for these evaluations so that future research can continue optimizing this important aspect of text-to-image systems.



**15 Oct 2023**

  1.  3 Oct, Microsoft, PSU and other researchers published a [paper]( https://arxiv.org/pdf/2308.08155.pdf) “AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation”. [AutoGen]( https://github.com/microsoft/autogen) is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic framework for building diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.


  2.  4 Oct, Google, CMU etc. published [paper]( https://arxiv.org/pdf/2310.03051.pdf) “How FaR Are Large Language Models From Agents with Theory-of-Mind?”. Researchers propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others’ mental states to actions in social scenarios. Experiments on T4D demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters’ beliefs in stories, but they struggle to translate this capability into strategic action. The researchers introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions. FaR boosts GPT-4’s performance from 50% to 71% on T4D, outperforming other prompting methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to diverse out-of-distribution story structures and scenarios that also require ToM inferences to choose an action, consistently outperforming other methods including few-shot in-context learning.


  3.  5 Oct, researchers from Stanford Uni, UC Berkeley, CMU etc. published a [paper]( https://arxiv.org/pdf/2310.03714.pdf) “DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines”. Researchers find that existing LM pipelines are typically implemented using hard-coded “prompt templates”, i.e. lengthy strings discovered via trial and error. They introduce parameterized [DSPy]( https://github.com/stanfordnlp/dspy), a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computation graphs where LMs are invoked through declarative modules. The researchers design a compiler that will optimize any DSPy pipeline to maximize a given metric. They conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multihop retrieval, answer complex questions, and control agent loops. On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5.


  4.  5 Oct, researchers from UTA, Princeton and Salesforce AI published a [paper]( https://arxiv.org/pdf/2310.03716.pdf): “A Long Way to Go: Investigating Length Correlations in RLHF”. Researchers find that when optimizing for helpfulness, RLHF has been consistently observed to drive models to produce longer outputs. This paper demonstrates that optimizing for response length is a significant factor behind RLHF’s reported improvements in these settings. First, the researchers study the relationship between reward and length for reward models trained on three open-source preference datasets for helpfulness. Here, length correlates strongly with reward, and improvements in reward score are driven in large part by shifting the distribution over output lengths. Furthermore, the research finds that RLHF with a reward based solely on length can reproduce most of the downstream improvements over the initial supervised fine-tuned model, showing that reward models in these settings have a long way to go.


  5.  5 Oct, Uni. of California published a [paper](https://arxiv.org/pdf/2310.02239.pdf) "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens". The paper introduces an innovative interleaved vision-and-language generation technique anchored by the concept of “generative vokens”, acting as the bridge for harmonized image-text outputs. The proposed approach is characterized by a distinctive two-staged training strategy focusing on description-free multimodal generation, where the training requires no comprehensive descriptions of images. To bolster model integrity, classifier-free guidance is incorporated, enhancing the effectiveness of vokens on image generation. The created model, MiniGPT-5, exhibits substantial improvement over the baseline Divter model on the MMDialog dataset and consistently delivers superior or comparable multimodal outputs in human evaluations on the VIST dataset, highlighting its efficacy across diverse benchmarks.


  6.  10 Oct, Mistral release it’s [technical report]( https://arxiv.org/pdf/2310.06825.pdf) “Mistral 7B”. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Mistral model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. The reports also provides a model fine-tuned to follow instructions, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and automated benchmarks. The models are released under the Apache 2.0 license.


  7.  10 Oct, researchers from UCSD published a [paper]( https://arxiv.org/pdf/2310.04678.pdf) “DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based Queries”. They point out that in scientific research, the ability to effectively retrieve relevant documents based on complex, multifaceted queries is critical. Existing evaluation datasets for this task are limited, primarily due to the high cost and effort required to annotate resources that effectively represent complex queries. To address this, the researchers propose a novel task, Scientific DOcument Retrieval using Multi-level Aspect-based quEries (DORISMAE), which is designed to handle the complex nature of user queries in scientific research. They developed a benchmark dataset within the field of computer science, consisting of 100 human-authored complex query cases. For each complex query, they assembled a collection of 100 relevant documents and produced annotated relevance scores for ranking them. Recognizing the significant labor of expert annotation, the researchers also introduce Anno-GPT, a scalable framework for validating the performance of Large Language Models (LLMs) on expert-level dataset annotation tasks. The researchers evaluated 17 recent retrieval methods on DORIS-MAE, observing notable performance drops compared to traditional datasets. This highlights the need for better approaches to handle complex, multifaceted queries in scientific research.


  8.  10 Oct, [Noema](https://www.noemamag.com/artificial-general-intelligence-is-already-here/) published an article “Artificial General Intelligence Is Already Here” by B. Arcas and P. Norvig.  The authors discussed the achievement of Artificial General Intelligence (AGI) by current advanced AI large language models like ChatGPT, Bard, LLaMA, and Claude. Despite flaws, these "frontier models" exhibit general intelligence by handling a variety of tasks, languages, modalities, and in-context learning. The debate around AGI involves skepticism about metrics, ideological commitments to alternative AI theories, concerns about human exceptionalism, and economic implications. The authors argue that AGI has already been achieved in these models, prompting discussions on the societal impact, ethical considerations, and the need for fair and equitable deployment.


  9.  11 Oct, according to [VayuRobitics](https://www.vayurobotics.com/press-releases/godfather-of-ai-geoffrey-hinton-joins-vayu-robotics-advisory-board), Dr. Geoffrey Hinton, also known as the Godfather of AI, has joined the Vayu Robotics advisory board to help develop AI robotics solutions. A pioneer in the field, Dr. Hinton’s work developing artificial neural networks and deep learning techniques have made him one of the most influential scientists in AI. Vayu Robotics co-founder Nitish Srivastava was a doctoral student of Dr. Hinton’s at the University of Toronto and has maintained a relationship with Dr. Hinton since that time. In addition to a career in academia, Dr. Hinton worked at Google for ten years as a VP and Engineering Fellow, and has most recently been a strong voice for looking at the risks of advanced AI on society as well as its enormous potential benefits. “Since leaving Google, I have received many requests to join the advisory boards of start-ups and, until now, I have declined them all,” said Dr. Hinton. “I have decided to join the advisory board of Vayu Robotics as I see great potential in their use of AI for robotics utilizing a co-engineered approach with machine learning and vision sensors. I am looking forward to once again working with Nitish Srivastava and guiding the Vayu team’s growth. I believe that Vayu’s technology will enable safe and eco-friendly solutions with far fewer ethical problems than many other AI applications.”


10. 12 Oct, UC Berkeley researchers published a [paper](https://arxiv.org/pdf/2310.01889.pdf) "Ring Attention with Blockwise Transformers for Near-Infinite Context". The research points out that the memory demands imposed by Transformers limit their ability to handle long sequences. To address the issue, the researchers present a distinct approach, Ring Attention, which leverages blockwise computation of self-attention to distribute long sequences across multiple devices while overlapping the communication of key-value blocks with the computation of blockwise attention. Ring Attention enables training and inference of sequences that are up to device count times longer than those of prior memory-efficient Transformers, effectively eliminating the memory constraints imposed by individual devices. Extensive experiments on language modeling tasks demonstrate the effectiveness of Ring Attention in allowing large sequence input size and improving performance.



**08 Oct 2023**
  1.  29 Sep, according to [Futurism](https://futurism.com/sam-altman-replace-normal-people-ai), OpenAI CEO Sam Altman is attracting attention for his use of the term "median human" in discussing artificial general intelligence (AGI). Altman envisions AGI with intelligence equivalent to a "median human co-worker." Critics find this terminology concerning, suggesting it implies replacing normal human jobs with AGI. Altman's previous comments on AGI's ability to perform various tasks further fuel concerns about job displacement. Critics argue that equating AI performance to human intelligence oversimplifies complex aspects of human cognition and raises ethical questions about assigning agency and comprehension to mechanistic models. Despite Altman's ambitions to use AI for societal benefit, his conceptualization of humanity as "median figures" provokes debate and skepticism about the impact of AGI on employment and the human experience.


  2.  29 Sep, Microsoft released its [GPT-4V evaluation report](https://browse.arxiv.org/pdf/2309.17421.pdf) “The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)”. The report focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V’s capabilities, its supported inputs and working modes, and the effective ways to prompt the model. Researchers curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V’s unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V’s unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting.



  3.  29 Sep, researchers from MIT, Meta AI and CMU published a [paper](https://browse.arxiv.org/pdf/2309.17453.pdf) “Efficient Streaming Language Models with Attention Sinks”. The paper argues that there are two challenges for LLM streaming applications. Firstly, during the decoding stage, caching previous tokens’ Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Researchers observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. The paper first demonstrates that the emergence of attention sink is due to the strong attention scores towards initial tokens as a “sink” even if they are not semantically important. Based on the above analysis, researchers introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning. Researchers show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, they discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2× speedup.



  4.  2 Oct, Statility.AI released [Stable LM 3B](https://stability.ai/blog/stable-lm-3b-sustainable-high-performance-language-models-smart-devices#:~:text=The%20development%20of%20Stable%20LM,costs%20low%20and%20performance%20high.). one of the key advantages of Stable LM 3B is its smaller size and efficiency. Unlike larger ones, these models require fewer resources and come with lower operating costs, making them highly accessible for most users. Not only does this make them more affordable, but it also makes them environmentally friendly, as they consume far less power. But do not let its size fool you; Stable LM 3B is highly competitive - it outperforms the previous state-of-the-art 3B parameter language models and even some of the best open-source language models at the 7B parameter scale.  The development of Stable LM 3B broadens the range of applications that are viable on the edge or on home PCs. This means that individuals and companies can now develop cutting-edge technologies with strong conversational capabilities – like creative writing assistance – while keeping costs low and performance high.



  5.  2 Oct, according [Fortune](https://fortune.com/2023/10/03/microsoft-ceo-satya-nadella-google-antitrust-search-default-chatgpt-market-share/), Microsoft CEO Satya Nadella said Monday that unfair tactics used by Google led to its dominance as a search engine, tactics that in turn have thwarted his company’s rival program, Bing. Nadella testified in a packed Washington, D.C., courtroom as part of the government’s landmark antitrust trial against Google’s parent company, Alphabet. The Justice Department alleges Google has abused the dominance of its ubiquitous search engine to throttle competition and innovation at the expense of consumers, allegations that echo a similar case brought against Microsoft in the late 1990s. Nadella said Google’s dominance was due to agreements that made it the default browser on smartphones and computers. He downplayed the idea that artificial intelligence or more niche search engines like Amazon or social media sites have meaningfully changed the market in which Microsoft competes with Google.



  6.  3 Oct, researchers from MIT published a [paper](https://browse.arxiv.org/pdf/2310.02207.pdf) “Language Models Represent Space and Time”. The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a coherent model of the data generating process—a world model. The research finds evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. Researchers discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, researchers identify individual “space neurons” and “time neurons” that reliably encode spatial and temporal coordinates. Analysis demonstrates that modern LLMs acquire structured knowledge about fundamental dimensions such as space and time, supporting the view that they learn not merely superficial statistics, but literal world models.



  7.  3 Oct, researcher from Center for AI Safety and a list of universities published a [paper](https://browse.arxiv.org/pdf/2310.01405.pdf) “Representation Engineering: A Top-Down Approach to AI Transparency”. In this paper, researchers identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population level representations, rather than neurons or circuits, at the center of analysis, equipping them with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). Researchers provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving the understanding and control of large language models. Researchers showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power seeking, and more, demonstrating the promise of top-down transparency research.



  8.  4 Oct, Anthropic [published a blog](https://www.anthropic.com/index/evaluating-ai-systems) “Challenges in evaluating AI systems”. The blog states that many of today’s existing evaluation suites are limited in their ability to serve as accurate indicators of model capabilities or safety. The blog lists challenges encountered by the company as 1) Multiple choice evaluations, 2) Third-party evaluation frameworks like BIG-bench and HELM, 3) Using crowdworkers to measure how helpful or harmful our models are, 4)  Using domain experts to red team for national security-relevant threats, 5) Using generative AI to develop evaluations for generative AI, 6) Working with a non-profit organization to audit our models for dangerous capabilities. Two main takeaways are: robust evaluations are extremely difficult to develop and implement, and effective AI governance depends on our ability to meaningfully evaluate AI systems.


  9.  4 Oct, according to [TheInformation](https://twitter.com/theinformation/status/1709333094195458314), OpenAI rival Anthropic wants to raise $2 billion from Google and other investors at a valuation of $20 billion or more. The 2-year-old startup, which sells Claude, a chatbot that competes with OpenAI’s ChatGPT, wants a valuation between $20 billion to $30 billion including the new investment, according to one of those people. That would quintuple the valuation of the company since March, when investors put a $4 billion price tag on the firm, and make its shares far pricier than those of OpenAI in terms of its valuation multiple on revenue.



  10.  4 Oct, according to [Yahoo!Finance](https://finance.yahoo.com/news/databricks-valuation-skyrockets-43-billion-161255240.html?guccounter=1), with the initial public offering (IPO) market heating up, data analytics platform Databricks Inc. has been leveraging the renewed interest of venture capitalists to raise funds. As one of the most promising unicorns backed by chipmaker Nvidia Corp., the company raised over $500 million in capital last month, bringing its valuation to $43 billion. The wobbly tech markets failed to deter the data analytics firm, as Databricks has been cashing in on the rising popularity of artificial intelligence (AI). "The commitment from long-term focused strategic and financial partners reflects Databricks' continued momentum, the rapid customer adoption of the Databricks Lakehouse, and the success customers are seeing from moving to a unified data and AI platform," Databricks Co-Founder and CEO Ali Ghodsi stated in a press release.



  11.  5 Oct, researchers from Microsoft and UWM published a [paper](https://browse.arxiv.org/pdf/2310.03744.pdf) “Improved Baselines with Visual Instruction Tuning”. The research shows that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, researchers establish stronger baselines that achieve state-of-the-art across 11 benchmarks. The final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ∼1 day on a single 8-A100 node.



**1 Oct 2023**


1.	25 Sep, OpenAI enhanced the capability of GPT-4. According to a [blog of OpenAI](https://openai.com/blog/chatgpt-can-now-see-hear-and-speak), they are beginning to roll out new voice and image capabilities in ChatGPT. They offer a new, more intuitive type of interface by allowing you to have a voice conversation or show ChatGPT what you’re talking about.

2.	25 Sep, [according to Reuters](https://www.reuters.com/markets/deals/amazon-steps-up-ai-race-with-up-4-billion-deal-invest-anthropic-2023-09-25/), Amazon said it will invest up to $4 billion in cash in the high-profile startup Anthropic, n its effort to compete with growing cloud rivals on artificial intelligence. Amazon's employees and cloud customers will gain early access to technology from Anthropic as part of the deal, which they can infuse into their businesses. The San Francisco-based startup also committed to rely primarily on Amazon's cloud services, including training its future AI models on large quantities of proprietary chips it would buy from the online retailing and computing giant.

3.	25 Sep, [PyTorch published a blog](https://pytorch.org/blog/inside-the-matrix/) introducing how they use 3D to visualize matrix multiplication expressions, attention heads with real weights, and more. The visualizing matrix multiplications (matmuls, or mm) helps build intuition and spark ideas with less cognitive overhead than the usual squares-on-paper idioms, especially (though not only) for visual/spatial thinkers. mm is fully interactive, [runs in the browser](https://bhosmer.github.io/mm/) or [notebook](https://colab.research.google.com/drive/1wZIoU20eRWKtRNCW7e5Iugm3MhfaE1f7) iframes and keeps its complete state in the URL, so links are shareable sessions (the screenshots and videos in this note all have links that open the visualizations in the tool). This [reference guide](https://bhosmer.github.io/mm/ref.html) describes all of the available functionality.

4.	25 Sep, [Meta published a paper](https://arxiv.org/pdf/2309.14402.pdf) “Physics of Language Models: Part 3.2, Knowledge Manipulation”. This paper explores a language model’s ability to manipulate its stored knowledge during inference. Researchers focus on four manipulation types: retrieval (e.g., “What is person A’s attribute X ”), classification (e.g., “Is A’s attribute X even or odd?”), comparison (e.g., “Is A greater than B in attribute X?”) and inverse search (e.g., “Which person’s attribute X equals T?”). They observe that pre-trained language models like GPT2/3/4 excel in knowledge retrieval but struggle with simple classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. They also perform poorly in inverse knowledge search, irrespective of the prompts.

5.	25 Sep, [according to venturebeat](https://venturebeat.com/ai/google-bard-fails-to-deliver-on-its-promise-even-after-latest-updates/), Google recently revamped its AI chatbot Bard, integrating it into popular products like Gmail, Docs, Drive, Maps, and YouTube. This move positions Bard to compete with ChatGPT, the market leader from OpenAI and Microsoft. The introduction of Bard Extensions, theoretically allowing the AI to pull live personalized data from Google services, seems promising. However, in practical use, Bard falls short of expectations. Despite its access to Google's vast ecosystem, it often produces inaccurate or nonsensical responses and lacks the creativity of OpenAI's GPT-4. The underlying issue lies in Bard's model, PaLM 2, trained on 340 billion parameters, significantly fewer than the rumored 1.8 trillion parameters of GPT-4. This disparity implies that GPT-4 may have a broader knowledge base, potentially leading to more relevant and interesting outputs.

6.	26 Sep, [Mckinsey announced](https://www.mckinsey.com/about-us/new-at-mckinsey-blog/mckinsey-launches-an-open-source-ecosystem) the launch of a McKinsey open-source ecosystem that will host products from across the firm, including some of our leading-edge technologies and IP in AI including generative AI, digital, and cloud.  The first major release in our collection is Vizro, a new component from our QuantumBlack Horizon suite, which helps users visualize data from their AI models. In addition to Vizro, the new ecosystem will host CausalNex, a tool for building cause-and-effect models that has been available to the public since 2020 through QuantumBlack Labs’ GitHub organization. Vizro, the newest Horizon component, creates high-quality visualizations that allows users to better explore and analyze data from their models. In a matter of hours rather than weeks, teams can collaborate to define insights and present them to clients in live workshops or demos.

7.	26 Sep, researchers from UC Berkley, NYU [published a paper](https://arxiv.org/pdf/2309.10313.pdf) “Investigating the Catastrophic Forgetting in Multimodal Large Language Models”. They find that catastrophic forgetting, a notorious phenomenon where the finetuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). They introduce EMT: Evaluating Multimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. Experimental results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. The results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.

8.	26 Sep, researcher from Oxford, Cambridge and Yule University [published a paper](https://arxiv.org/pdf/2309.15840v1.pdf) “How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions”. “Lie” of an LLM is defined as outputting false statements despite “knowing” the truth in a demonstrable sense. LLMs might “lie”, for example, when instructed to output misinformation. Here, they develop a simple lie detector that requires neither access to the LLM’s activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM’s yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. Experimental results indicate that LLMs have distinctive lierelated behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection.

9.	27 Sep, Meta [published a blog](https://ai.meta.com/blog/llama-2-updates-connect-2023/) “The Llama Ecosystem: Past, Present, and Future”. The LLaMA community has seen a lot of momentum and innovation, with more than 30 million downloads of Llama-based models through Hugging Face and over 10 million of these in the last 30 days alone. Much like PyTorch, Llama has evolved into a platform for the world to build on, and we couldn’t be more excited. The impact to date includes cloud usage, innovators (Scala AI, Replicate, Anyscale), crowd sourced optimization (fined tuned over 7,000 derivatives on Huggingface only !), hardware support (AMD, Intel, Nvidia, and Google all have boosted the performance of Llama2 via hardware/software optimizations). Meta believes deeply in the power of the open source community. Meta also believes that state-of-the-art AI technology is safer and better aligned when it’s open and accessible to everyone. The blog also indicate three future directions: multimodal, safety and responsibility and a focus on community.

10.	27 Sep, Meta [published a paper](https://ai.meta.com/research/publications/effective-long-context-scaling-of-foundation-models/) “Effective Long-Context Scaling of Foundation Models”, a series of long-context LLMs that support effective context windows of up to 32,768 tokens. The model series are built through continual pretraining from LLAMA 2 with longer training sequences and on a dataset where long texts are upsampled. The models achieve consistent improvements on most regular tasks and significant improvements on long-context tasks over LLAMA 2. Notably, with a cost-effective instruction tuning procedure that does not require human-annotated long instruction data, the 70B variant can already surpass gpt-3.5-turbo-16k’s overall performance on a suite of long-context tasks. Ablation experiments suggest that having abundant long texts in the pretrain dataset is not the key to achieving strong performance, and they empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences. Specifically, the researchers take the RLHF dataset used in LLAMA 2 CHAT and augment it with synthetic self-instruct (Wang et al., 2022) long data generated by LLAMA 2 CHAT itself, in the hope that the model can learn a diverse set of skills through a large amount of RLHF data and transfer that knowledge to long-context scenarios via self-instruct data.

11.	27 Sep, Meta [published a paper](https://arxiv.org/pdf/2309.16058.pdf) "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model". AnyMAL - Any-Modality Augmented Language Model, a unified model that reasons over diverse input modality signals (i.e. text, image, video, audio, IMU motion sensor), and generates textual responses. AnyMAL inherits the powerful text-based reasoning abilities of the state-of-the-art LLMs including LLaMA-2 (70B), and converts modality-specific signals to the joint textual space through a pre-trained aligner module. To further strengthen the multimodal LLM’s capabilities, researchers fine-tune the model with a multimodal instruction set manually collected to cover diverse topics and tasks beyond simple QAs.

12.	27 Sep, Nature [published a paper](https://www.nature.com/articles/d41586-023-02999-3) “AI tools as science policy advisers? The potential and the pitfalls”. The article suggests that with careful development and management, a new generation of AI-based tools could, in the near future, present an opportunity to drastically improve science advice, making it more agile, rigorous and targeted. But leveraging such tools for good will require science advisers and policy institutions to create guidelines and to carefully consider the design and responsible use of this nascent technology. The paper also discusses two tasks for which generative AI tools hold promise for policy guidance — synthesizing evidence and drafting briefing papers — and highlight areas needing closer attention.

13.	27 Sep, Mistral AI [released Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/), the most powerful language model for its size to date. Mistral outperforms Llama2 13B on all benchmarks, outperforms Llama 1 34B on many benchmarks, approaches CodeLlama 7B performance on code, while remaining good at English tasks, provide fast inference, and can handle longer sequences at smaller cost.

14.	28 Sep, a group of researchers from USA [published a paper](https://arxiv.org/abs/2309.16145) “The Confidence-Competence Gap in Large Language Models: A Cognitive Study”. Researchers exploit these models with diverse sets of questionnaires and real-world scenarios and extract how LLMs exhibit confidence in their responses. The findings reveal intriguing instances where models demonstrate high confidence even when they answer incorrectly. This is reminiscent of the Dunning-Kruger effect observed in human psychology. In contrast, there are cases where models exhibit low confidence with correct answers revealing potential underestimation biases. The results underscore the need for a deeper understanding of their cognitive processes.

15.	28 Sep, [Meta Connect2023](https://www.metaconnect.com/en/home) was hold at Menlo Park, California, from 27 to 28 Sep 2023. Key things in the meeting include but are not limited to 1) Meta QUEST 3, the headset model with improved passthrough tech, higher resolution displays and better graphics; 2) Emu, a new foundational model for image generation which can be used in Meta apps such as WhatsApp, Messenger, Instagram and Facebook stories; 3) Ray-Ban Meta smart glasses which have two round modules on the side of either eye include a 12-megapixel camera and an LED light that flips on to alert others that you’re recording; 4) Meta AI, which can help plan a trip with friends in a group chat, answer general-knowledge questions, and search the internet across Microsoft’s Bing to provide real-time web results. 


**24 Sep 2023**
1.	Sep, [According to Harvard Business School](https://www.hbs.edu/faculty/Pages/item.aspx?num=64700#:~:text=For%20each%20one%20of%20a%20set%20of%2018,40%25%20higher%20quality%20compared%20to%20a%20control%20group%29.), researchers from Harvard, MIT and other institutes, a large-scale field experiment has been conducted to investigate how human will use AI to accomplish a variety of tasks. The pre-registered experiment involved 758 consultants comprising about 7% of the individual contributor-level consultants at the company. After establishing a performance baseline on a similar task, subjects were randomly assigned to one of three conditions: no AI access, GPT-4 AI access, or GPT-4 AI access with a prompt engineering overview. We suggest that the capabilities of AI create a “jagged technological frontier” where some tasks are easily done by AI, while others, though seemingly similar in difficulty level, are outside the current capability of AI. For each one of a set of 18 realistic consulting tasks within the frontier of AI capabilities, consultants using AI were significantly more productive (they completed 12.2% more tasks on average, and completed tasks 25.1% more quickly), and produced significantly higher quality results (more than 40% higher quality compared to a control group). Consultants across the skills distribution benefited significantly from having AI augmentation, with those below the average performance threshold increasing by 43% and those above increasing by 17% compared to their own scores. For a task selected to be outside the frontier, however, consultants using AI were 19 percentage points less likely to produce correct solutions compared to those without AI. Further, our analysis shows the emergence of two distinctive patterns of successful AI use by humans along a spectrum of human-AI integration. One set of consultants acted as “Centaurs,” like the mythical half-horse/half-human creature, dividing and delegating their solution-creation activities to the AI or to themselves. Another set of consultants acted more like “Cyborgs,” completely integrating their task flow with the AI and continually interacting with the technology. 

2.	18 Sep, [theinformation published an article](https://www.theinformation.com/articles/openai-hustles-to-beat-google-to-launch-multimodal-llm?utm_source=bensbites&utm_medium=referral&utm_campaign=openai-hustles-to-beat-google-to-launch-multimodal-llm) “OpenAI Hustles to Beat Google to Launch ‘Multimodal’ LLM”. With all the reports of Gemini being released soon and potentially better than GPT-4, Open AI is trying to keep its lead intact. The multimodal features will be launched under the name “GPT-vision.” Also, they are training a multimodal LLM from scratch codenamed Gobi. These new “multimodal” models will be able to generate code from website sketches and analyze visual data, among other capabilities. Google is close to releasing its Gemini model, while OpenAI is rushing to add multimodal features to GPT-4. There are concerns around potential misuse, but both companies are taking steps to ensure responsible development. This race parallels big tech platform competitions like iPhone vs Android.

3.	19 Sep, Google published a [paper](https://www.science.org/doi/10.1126/science.adg7492) on Science, “Accurate proteome-wide missense variant effect prediction with AlphaMissense”. According to [Google DeepMind](https://twitter.com/GoogleDeepMind?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1704145467129389178%7Ctwgr%5E1b66b27bb7a7a9342385fa2d6d6cb534ac787560%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fwww.news-medical.net%2Fnews%2F20230920%2FAlphaMissense-revolutionizes-genetic-mutation-analysis-for-disease-prediction.aspx), uncovering the causes of disease is one of the greatest challenges in genetics, to help advance this, they created AlphaMissense: an AI model classifying missense variants – or genetic changes affecting proteins. The model is trained on population frequency data and uses sequence and predicted structural context, all of which contribute to its performance. The authors evaluated the model against related methods using clinical databases not included in the training and demonstrated agreement with multiplexed assays of variant effect.

4.	19 Sep, [Google updated Bard](https://blog.google/products/bard/google-bard-new-features-update-sept-2023/) by adding Bard Extensions in English. With Extensions, Bard can find and show you relevant information from the Google tools you use every day — like Gmail, Docs, Drive, Google Maps, YouTube, and Google Flights and hotels — even when the information you need is across multiple apps and services. One can also use Bard’s “Google it” button to more easily double-check its answers. Other new features include  upload images with Lens, get Search images in responses, and modify Bard’s responses — to more than 40 languages.

5.	20 Sep, according to [TheNextPlatform](https://www.nextplatform.com/2023/09/20/sambanova-tackles-generative-ai-with-new-chip-and-new-approach/), the Global 50,000, national and regional governments, and large academic institutions will take a pre-trained model and tweak and tune it so it can focus on and perform generative tasks upon their internal proprietary data. SambaNova, an AI hardware start-up released SN40L, which is architected specifically to run a modest-sized model on a single device and to have lots of models running side-by-side. With a cluster of eight SN40L sockets, it can handle more than 5 trillion parameters, says Liang, one of the founders of Sambanova. So two clusters of eight machines will give users all the models might need for a Global 2000 enterprise given the logic that SambaNova is using.

6.	20 Sep, [Sequia published an article](https://www.sequoiacap.com/article/generative-ai-act-two/) “Generative AI’s Act Two”. The article argues that Generative AI has had a successful first year, with over $1 billion in revenue from startups alone. Some applications have become household names, such as ChatGPT, Midjourney, and Character. However, many AI companies do not have product-market fit or a sustainable competitive advantage, and the overall ebullience of the AI ecosystem is unsustainable. The market is now entering "Act 2," which will be from the customer-back. Act 2 will solve human problems end-to-end, with applications that are more comprehensive and user-friendly than the first apps out of the gate. The generative AI market map has been updated to reflect the evolution of the market from technology hammer to actual use cases and value, and the increasingly multimodal nature of generative AI applications. A new LLM developer stack has also been included to reflect the compute and tooling vendors that companies are turning to as they build generative AI applications in production. The article acknowledges the challenges facing generative AI, particularly in demonstrating value and retaining users. However, it remains optimistic about the market's potential and emphasizes the importance of patience, judgment, and innovation in overcoming these challenges.

7.	21 Sep, Microsoft announced the release of [Microsoft 365 Copilot](https://adoption.microsoft.com/en-us/copilot/), which combines the power of large language models (LLMs) with organization’s data – all in the flow of work – to turn words into one of the most powerful productivity tools on the planet. It works alongside popular Microsoft 365 Apps such as Word, Excel, PowerPoint, Outlook, Teams, and more. Copilot provides real-time intelligent assistance, enabling users to enhance their creativity, productivity, and skills. For example, users can use Copilot in Teams meetings to summarise key discussion points – including who said what and where people are aligned or disagree – and suggest action items, all in real-time during a meeting.

8.	21 Sep, researchers from Vanderbilt university, New York university and other institutes published a [paper](https://arxiv.org/pdf/2309.12288.pdf) ‘The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”’. They named the phenomenon Reversal Curse, i.e. if a model is trained on a sentence of the form “A is B”, it will not automatically generalize to the reverse direction “B is A”. For instance, if a model is trained on “Olaf Scholz was the ninth Chancellor of Germany”, it will not automatically be able to answer the question, “Who was the ninth Chancellor of Germany?”. Moreover, the likelihood of the correct answer (“Olaf Scholz”) will not be higher than for a random name. The researchers also found that  the Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. Models tested include GPT-4, ChatGPT and LLaMA.

9.	21 Sep, [AlpacaEval Leaderboard](https://tatsu-lab.github.io/alpaca_eval/) has updated recently. First-time GPT-4 has been over-performed by a new LLM Xwin-lm 70b v0.1. It achieved a win-rate against Davinci-003 of 95.57%, ranking as TOP-1 on AlpacaEval. It was the FIRST model surpassing GPT-4 on AlpacaEval. Also note its winrate v.s. GPT-4 is 60.61. (Note, our test of Xwin-13b seems not comparable even with LLaMA2 13B  in terms of NER extraction)

10.	21 Sep, according to TheVerge, [OpenAI released DALL-E 3](https://www.theverge.com/2023/9/20/23881241/openai-dalle-third-version-generative-ai), which now lets users use ChatGPT to create prompts and includes more safety options. By using ChatGPT, someone doesn’t have to come up with their own detailed prompt to guide DALL-E 3; they can just ask ChatGPT to come up with a prompt, and the chatbot will write out a paragraph (DALL-E works better with longer sentences) for DALL-E 3 to follow. Other users can still use their own prompts if they have specific ideas for DALL-E.



**17 Sep 2023**
1.	7 Sep, researchers from Humboldt university published [paper](https://arxiv.org/pdf/2309.03876.pdf) “OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs”. With this demonstration, researchers take a different view on biases in instruction-tuning: Rather than aiming to suppress them, they aim to make them explicit and transparent. To this end, the researchers present OpinionGPT, a web demo in which users can ask questions and select all biases they wish to investigate. The demo will answer this question using a model fine-tuned on text representing each of the selected biases, allowing side-by-side comparison. To train the underlying model, they identified 11 different biases (political, geographic, gender, age) and derived an instruction-tuning corpus in which each answer was written by members of one of these demographics. Web application available at https://opiniongpt.informatik.hu-berlin.de. 

2.	8 Sep, [accord to ts2.space](https://ts2.space/en/a-new-approach-to-regulating-ai-the-critical-algorithmic-systems-classification/),  Brookings Institution fellow and AI policy expert Alex Engler puts forward a new idea for regulating artificial intelligence (AI) in the face of an increasingly integrated technology that impacts various systems. Engler’s concept, called the Critical Algorithmic Systems Classification, aims to govern algorithms based on their specific applications and expands the authority of regulatory agencies. This approach grants agencies, such as the Department of Education, the Equal Employment Opportunity Commission, and the Department of Health and Human Services, the power to create rules for “especially impactful algorithms” and issue administrative subpoenas for algorithmic investigations. Unlike a massive overhaul of civil rights laws, which would require significant political support, the Critical Algorithmic Systems Classification offers a focused approach to governing AI’s impact on socioeconomic outcomes and health determinations. Engler’s proposal emphasizes the need to adapt existing regulatory agencies to incorporate AI expertise rather than creating a separate federal AI-focused agency. He highlights the importance of building staffing and technical expertise within these agencies to effectively regulate algorithmic systems.

3.	11 Sep, Google published a [paper](https://arxiv.org/pdf/2309.05858.pdf) “Uncovering mesa-optimization algorithms in Transformers”. The paper hypothesizes that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. Moreover, the paper suggests that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, the Google researchers propose a novel self-attention layer, the mesa-layer, that explicitly and efficiently solves optimization problems specified in context. They find that this layer can lead to improved performance in synthetic and preliminary language modeling experiments, adding weight to their hypothesis that mesa-optimization is an important operation hidden within the weights of trained Transformers.

4.	11 Sep, [Nvidia published a blog](https://developer.nvidia.com/blog/leading-mlperf-inference-v3-1-results-gh200-grace-hopper-superchip-debut/) to demonstrate the performance of its newest GH200 server, “Leading MLPerf Inference v3.1 Results with NVIDIA GH200 Grace Hopper Superchip Debut”. In its MLPerf debut, the GH200 Grace Hopper Superchip turned in exceptional performance on all workloads and scenarios in the closed division of the data center category, boosting performance by up to 17% on the NVIDIA single-chip H100 SXM submission. The NVIDIA software stack fully supports the GH200 Grace Hopper Superchip today.

5.	11 Sep, [together.ai published a blog](https://together.ai/blog/medusa) “Medusa: Simple Framework for Accelerating LLM Generation with Multiple Decoding Heads”. Instead of using an additional draft model like speculative decoding, Medusa merely introduces a few additional decoding heads, following the idea of [Stern et al. 2018] with some other ingredients. Despite its simple design, Medusa can improve the generation efficiency of LLMs by about 2x. The implementation is available at [this repo](https://github.com/FasterDecoding/Medusa).

6.	11 Sep, researchers from Microsoft published a [paper](https://arxiv.org/pdf/2309.05689.pdf#:~:text=The%20P%20vs%20NP%20problem,problem%20behind%20the%20P!%3D) “Large Language Model for Science: A Study on P vs. NP”. Researchers use large language models (LLMs) to augment and accelerate research on the P versus NP problem. Specifically, they propose Socratic reasoning, a general framework that promotes in-depth thinking with LLMs for complex problem-solving. Socratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement. The pilot study on the P vs. NP problem shows that GPT-4 successfully produces a proof schema and engages in rigorous reasoning throughout 97 dialogue turns, concluding “P ̸= NP”, which is in alignment with ([Xu and Zhou, 2023](https://arxiv.org/pdf/2302.09512.pdf)). The investigation uncovers novel insights within the extensive solution space of LLMs, shedding light on LLM for Science.

7.	11 Sep, according to [BusinessInsider](https://www.businessinsider.com/ai-builds-software-under-7-minutes-less-than-dollar-study-2023-9), Researchers in a new study tasked an AI-powered tech company with developing 70 different programs. They found AI could develop software in under seven minutes for less than $1 in costs, on average. AI bots were assigned roles and were able to talk, make logical decisions, and troubleshoot bugs. Here’s the [paper](https://arxiv.org/pdf/2307.07924v3.pdf) and [project](https://github.com/OpenBMB/ChatDev).

8.	12 Sep, CISA of America released [CISA Open Source Software Security Roadmap](https://www.cisa.gov/sites/default/files/2023-09/CISA-Open-Source-Software-Security-Roadmap-508c%20%281%29.pdf). The report states that The federal government, critical infrastructure, and state, local, tribal, and territorial (SLTT) governments greatly depend upon open source software (OSS). OSS is part of the foundation of software used across critical infrastructure, supporting every single critical infrastructure sector and every National Critical Function: one study found that 96% of studied codebases across various sectors contain open source code, and 76% of code in studied codebases was open source. The proposed roadmap centers on four key goals: 1) establishing CISA’s role in supporting the security of OSS, 2) understanding the prevalence of key open source dependencies, 3) reducing risks to the federal government, and 4) hardening the broader OSS ecosystem.

9.	12 Sep, [SCSP released special edition report](https://scsp222.substack.com/p/scsp-releases-special-edition-report?utm_source=profile&utm_medium=reader2) “Generative AI: The Future of Innovation Power”. The Generative AI moment provides the United States Government with a unique opportunity to lead with conviction as humanity enters a new era. This [Special Edition report](https://www.scsp.ai/wp-content/uploads/2023/09/GenAI-web.pdf) provides a comprehensive national security strategy informed by the generative AI models that enhance all elements of our innovation power.

10.	13 Sep, [a16z published a blog](https://a16z.com/how-are-consumers-using-generative-ai/), “How Are Consumers Using Generative AI?” The blog ranks 50 GenAI products, and finds that 1. Most leading products are built from the “ground up” around generative AI; 2. ChatGPT has a massive lead, for now… 3. LLM assistants (like ChatGPT) are dominant, but companionship and creative tools are on the rise; 4. Early “winners” have emerged, but most product categories are up for grabs; 5. Acquisition for top products is entirely organic—and consumers are willing to pay! 6. Mobile apps are still emerging as a GenAI platform.

11.	13 Sep, researchers from the National University of Singapore published a [paper](https://arxiv.org/pdf/2309.05519.pdf): “NExT-GPT: Any-to-Any Multimodal LLM”. The researchers observed that humans always perceive the world and communicate with people through various modalities, so they developed an any-to-any MM-LLM capable of accepting and delivering content in any modality – NExT-GPT. They connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. Overall, the research showcases the promising possibility of building a unified AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community.

12.	14 Sep, [according to apnews](https://apnews.com/article/schumer-artificial-intelligence-elon-musk-senate-efcfb1067d68ad2f595db7e92167943c),  The nation’s biggest technology executives on Wednesday loosely endorsed the idea of government regulations for artificial intelligence at an unusual closed-door meeting in the U.S. Senate. But there is little consensus on what regulation would look like, and the political path for legislation is difficult. Senate Majority Leader Chuck Schumer, who organized the private forum on Capitol Hill as part of a push to legislate artificial intelligence, said he asked everyone in the room — including almost two dozen tech executives, advocates and skeptics — whether government should have a role in the oversight of artificial intelligence, and “every single person raised their hands, even though they had diverse views,” he also said  regulation of artificial intelligence will be “one of the most difficult issues we can ever take on,” and he listed some of the reasons why: It’s technically complicated, it keeps changing and it “has such a wide, broad effect across the whole world”. Sarah Myers West, managing director of the nonprofit AI Now Institute, estimated that the combined net worth of the room Wednesday was $550 billion and it was “hard to envision a room like that in any way meaningfully representing the interests of the broader public.” She did not attend. At the same time during the Congress meeting, [ChatGPT was down](https://futurism.com/the-byte/chatgpt-down-sam-altman-washington) for about two hours.

13.	15 Sep, [according to CNBC](https://www.cnbc.com/2023/09/14/oracle-founder-larry-ellison-makes-first-trip-to-microsoft-campus.html), Oracle founder Larry Ellison makes first-ever trip to Microsoft headquarters for cloud announcement. According to the report, Oracle co-founder and technology chief Larry Ellison and Microsoft CEO Satya Nadella spoke at a presentation on Microsoft’s campus to announce an extension of their partnership. Ellison said Oracle hardware is coming to Microsoft’s data centers, enabling organizations that use Microsoft’s Azure cloud to draw on Oracle database services, although the two companies have gone up against each other for over three decades. “Whether it is fine-tuning a model, pre-training a model or meta-prompting a model requires that low latency access to data,” [Nadella said](https://www.crn.com/news/cloud/microsoft-ceo-nadella-calls-joint-oracle-offering-a-profound-moment-for-ai). “And so we’re very excited. I think this is the moment where data and AI coming together to transform businesses and business process – there couldn’t be a more profound timing of these two things.”

14.	15 Sep, [according to MIT Technology Review](https://www.technologyreview.com/2023/09/15/1079624/deepmind-inflection-generative-ai-whats-next-mustafa-suleyman/), DeepMind cofounder Mustafa Suleyman wants to build a chatbot that does a whole lot more than chat. he told the magazine that generative AI is just a phase. What’s next is interactive AI: bots that can carry out tasks you set for them by calling on other software and other people to get stuff done. He also calls for robust regulation—and doesn’t think that’ll be hard to achieve.
    
16.	15 Sep, [according to BusinessInsider](https://www.businessinsider.com/google-ceo-isnt-worried-about-falling-behind-on-ai-2023-9), Google CEO says he isn't worried about catching up to OpenAI after the search engine reportedly declared a 'code red': 'I feel very comfortable about where we are.' Releasing Google's AI products before ChatGPT was launched "wouldn't have worked out as well," he said. Pichai's thoughts on AI come months after Google reportedly declared a "code red" for its search engine, per NYT. 

17.	15 Sep, [according to the-decoder](https://the-decoder.com/google-begins-external-testing-of-its-gpt-4-competitor-gemini/), Google begins external testing of its GPT-4 competitor "Gemini". According to three anonymous sources from The Information, Google has given a small group of selected companies access to a stripped-down chat version of Gemini. The three sources claim to have direct knowledge of the matter. The largest version of Gemini is still being developed internally. Gemini will be offered to businesses via cloud access and integrated into Google's consumer products. Google plans to use Gemini for all of its AI applications, from the Bard chatbot to the new AI features in Workspace. Gemini, according to The Information, is "a set of large language models" that can perform various tasks such as chatbots, text summarization, code, or generating new text. It is unclear whether Gemini will rely on networked expert models, as OpenAI does with its GPT-4 architecture.

18.	16 Sep, researchers from Singapore University of Technology and Design released [TinyLlama](https://github.com/jzhang38/TinyLlama), which aims to pretrain a 1.1B Llama model on 3 trillion tokens. With some proper optimization, we can achieve this within a span of "just" 90 days using 16 A100-40G GPUs.


**11 Sep 2023**
1.	1 Sep, [Google published a paper](https://arxiv.org/pdf/2309.00267.pdf) “RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback”. RLAIF (RL from AI Feedback) is a technique where preferences are labeled by an off-the-shell LLM in lieu of humans. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ∼70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF .

2.	1 Sep, [OpenAI published a paper](https://arxiv.org/pdf/2309.00667.pdf) “Taken out of context: On measuring situational awareness in LLMs”. First, A model is situationally aware if it’s aware that it’s a model and can recognize whether it’s currently in testing or deployment. According to researchers, today’s LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests while taking harmful actions after deployment. This research moves further from an [article published by Nature](https://www.nature.com/articles/d41586-023-02684-5) “If AI becomes conscious: here’s how researchers will know” where scientists believed that Human-like behaviours can make it difficult to judge robots’ true level of engagement.

3.	4 Sep, a group of researchers from 22 universities and security institues publish a [paper](https://arxiv.org/pdf/2307.03718.pdf) "Frontier AI Regulation: Managing Emerging Risks to Public Safety". “frontier AI” models is defined as 'highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety.'  The models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model’s capabilities from proliferating broadly. To address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier AI developers, (2) registration and reporting requirements to provide regulators with visibility into frontier AI development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deployment of frontier AI models. Industry self-regulation is an important first step. However, wider societal discussions and government intervention will be needed to create standards and to ensure compliance with them. Finally, the paper proposes an initial set of safety standards include conductingpre-deployment risk assessments; external scrutiny of model behavior; using risk assessments to inform deployment decisions; and monitoring and responding to new information about model capabilities and uses post-deployment.

4.	4 Sep, [according to Itnews](https://www.itnews.com.au/news/microsoft-had-three-staff-at-australian-data-centre-campus-when-azure-went-out-599849), Microsoft had “insufficient” staff levels at its data centre campus last week when a power sag knocked its chiller plant for two data halls offline, cooking portions of its storage hardware. The company has [released a preliminary post-incident report](https://azure.status.microsoft/en-au/status/history/) (PIR) for the large-scale failure, which saw large enterprise [customers including Bank of Queensland and Jetstar completely lose service](https://www.itnews.com.au/news/bank-of-queensland-jetstar-among-australian-enterprises-impacted-by-azure-outage-599803).

5.	4 Sep, [ColosalAI released its 70B parameter](https://www.hpc-ai.tech/blog/70b-llama2-training) LLaMA2 model. The model training is accelerated by 195% with the best foundation model practice upgraded. Colossal-AI has open-sourced a full-flow solution for LLaMA2 with high scalability. This supports models ranging from 7 to 70 billion parameters, while still maintaining good performance from 8 to 512 GPUs. Users only need to upload relevant data to train personalized private models without code and can deploy the trained models with one click.

6.	6 Sep, [TII announced Falcon 180B](https://falconllm.tii.ae/falcon-180b.html). Falcon 180B is a super-powerful language model with 180 billion parameters, trained on 3.5 trillion tokens. It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use. This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.

7.	6 Sept, on Google’s 25-year anniversary, Jeff Dean, Chief Scientist of Google DeepMind and Google Research, [posted on Twitter](https://twitter.com/JeffDean/status/1699197621934366946) about his work experience at Google. Jeff said, “It has been deeply gratifying to work on incredibly exciting computer science and AI problems with amazing colleagues, & to help build out a suite of 10+ products that each have more than 1B users all over the world.”

8.	7 Sep, [Mojo is formally released](https://www.modular.com/blog/mojo-its-finally-here). According to Modular, “Mojo is a new programming language for AI developers that will grow into being a superset of Python over time. It already supports integrating with arbitrary Python code seamlessly and has a scalable programming model to target performance-critical systems, including accelerators (e.g. GPUs) that are pervasive in AI.” Mojo combines the best of dynamic and static languages together, and can achieve up to 68,000x the performance of Python today. Other features of Mojo include writing everything in one language, unlocking python performance, accessing the entire Python ecosystem, and upgrading AI workloads.

9.	7 Sep, [Google published a paper](https://arxiv.org/pdf/2309.03409.pdf) “Large Language Models As Optimizers”. In the paper, researchers propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. Experiments show that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.


**3 Sep 2023**

1.	27 Aug, [according to Techxplore](https://techxplore.com/news/2023-08-ibm-core-mixed-signal-in-memory-chip.html), IBM developed a new 64-core missed-signal in-memory computing chip to better supper support the computations of deep neural networks. The 64-core chip, presented in a paper in [Nature Electronics](https://www.nature.com/articles/s41928-023-01010-1), has so far attained highly promising results, retaining the accuracy of deep learning algorithms, while reducing computation times and energy consumption.

2.	28 Aug, [OpenAI released ChatGPT Enterprise](https://openai.com/blog/introducing-chatgpt-enterprise),which offers enterprise-grade security and privacy, unlimited higher-speed GPT-4 access, longer context windows for processing longer inputs, advanced data analysis capabilities, customization options, and much more. A new admin console lets users manage team members easily and offers domain verification, SSO, and usage insights, allowing for large-scale deployment into enterprise. See our privacy page and our Trust Portal for more details on how we treat your data. However, enterprise data still need to upload to OpenAI to remember all the data.

3.	28 Aug, according to [Semianalysis](https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini), The GPU-Rich companies have 20k+ A/H100 GPUs and are attracting top talent. The GPU-Poor startups and open-source researchers are struggling with far fewer GPUs, and spending significant time and effort attenmptging to do things that simply don’t help, or frakly matter, and an extremely counter-productive use of their skills and time. While the key here is everyone from Meta to Microsoft to startups are simply serving as a pipeline of capital to Nvidia’s bank account, Google – the most compute r rich firm in the world, is one potential savior of Nvidia slavery. Google, which has its unbeatably efficient infrastureture, has already begun training Gemini, the next generation LLM.

4.	30 Aug, Google had its [Google Cloud Next 2023](https://cloud.google.com/blog/topics/google-cloud-next/welcome-to-google-cloud-next-23). In the conference, Google announced several new projects including AI-optimized Infrastructure: The most advanced AI-optimized infrastructure for companies to train and serve models. Vertex AI: Developer tools to build models and AI-powered applications, with major advancements to Vertex AI for creating custom models and building custom Search and Conversation apps with enterprise data;  Duet AI: Duet AI is an always-on AI collaborator that is deeply integrated in Google Workspace and Google Cloud. Duet AI in Workspace gives every user a writing helper, a spreadsheet expert, a project manager, a note taker for meetings, and a creative visual designer, and is now generally available.

5.	30 Aug, [Nature published a paper](https://www.nature.com/articles/s41586-023-06419-4) by researchers from University of Zurich , “Champion-level drone racing using deep reinforcement learning”. The paper discusses First-person view (FPV) drone racing, where professional pilots navigate high-speed drones through 3D circuits while viewing the world through onboard cameras. It highlights the challenge of creating autonomous drones that can race at a professional level by relying solely on onboard sensors. The research project introduces Swift, an autonomous system that achieved the level of human world champions by combining deep reinforcement learning in simulations with real-world data. Swift competed against three human champions, winning multiple races and setting the fastest recorded race time. This achievement is seen as a significant milestone in mobile robotics and machine intelligence, with potential applications in other physical systems.

6.	30 Aug, [Cryptoslate.com reported](https://cryptoslate.com/chatgpt-drives-openai-toward-1b-revenue-goal-after-losing-540m-in-2022/) that ChatGPT drives OpenAI toward $1B revenue goal after losing $540M in 2022. The report indicated that the AI startup has reportedly seen a substantial boost in its monthly revenue to around $80 million. This marks a significant increase from its previous revenue of $28 million, which coincides with the introduction of fees for its widely-used chatbot, ChatGPT. Meanwhile, The Information reported that OpenAI lost around $540 million last year while developing GPT-4 and ChatGPT.

7.	1 Sep, [Science published paper](https://www.science.org/doi/epdf/10.1126/science.ade4401) from Google, “A principal odor map unifies diverse tasks inolfactory perception”. The paper discusses the challenge of connecting molecular structures to how we perceive odors. The authors used graph neural networks to create a Principal Odor Map (POM) that accurately represents the relationships between different odors. This POM was able to predict the quality of odors, even for ones that hadn't been previously characterized. In fact, the model's predictions were as reliable as those of a human expert. The POM outperformed other chemoinformatic models in various odor prediction tasks, demonstrating its effectiveness in encoding general structure-odor relationships. This approach has the potential to revolutionize odor prediction and could lead to the digitization of odors.



**27 Aug 2023**
1.	Meta recently released [RoboAgent]( https://robopen.github.io/) with [paper]( https://robopen.github.io/media/roboagent.pdf) “RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking”. RoboAgent can efficiently acquire a wide diversity of non-trivial skills and can generalize them to diverse unseen scenarios. Trained merely on 7500 trajectories, RoboAgent can exhibit a diverse set of 12 non-trivial manipulation skills (beyond picking/pushing, including articulated object manipulation and object re-orientation) across 38 tasks, and can generalize them to 100s of diverse unseen scenarios (involving unseen objects, unseen tasks, and to completely unseen kitchens). RoboAgent can also evolve its capabilities with new experiences.

2.	19 Aug, [according to BGR](https://bgr.com/tech/openai-may-have-to-wipe-chatgpt-and-start-over/), OpenAI, the company behind the popular generative AI tool ChatGPT, could be forced to wipe its chatbot and start over completely, according to a new report from NPR (via Ars Technica). The wipe may come as part of a potential lawsuit which could also see OpenAI fined up to $150,000 for each piece of copyrighted material used to train the language model.

3.	21 Aug, [according to The Times of Israel](https://www.timesofisrael.com/ai-likely-to-augment-rather-than-destroy-jobs-un-study-finds/), Artificial Intelligence is more likely to augment jobs than to destroy them, a UN study indicated on Monday, at a time of growing anxiety over the potential impact of the technology. Countries should therefore design policies to support an “orderly, fair and consultative” shift, the report authors said, stressing that “outcomes of the technological transition are not pre-determined.”

4.	21 Aug, another agent related [project released](https://human-world-model.github.io/) by researchers from CMU with [paper](https://arxiv.org/pdf/2308.10901.pdf) “Structured World Models from Human Videos”. Inspired by the success of learning from large-scale datasets in the fields of computer vision and natural language, researchers believe that in order to efficiently learn, a robot must be able to leverage internet-scale, human video data. Humans interact with the world in many interesting ways, which can allow a robot to not only build an understanding of useful actions and affordances but also how these actions affect the world for manipulation. The proposed approach builds a structured, human-centric action space grounded in visual affordances learned from human videos. The approach of affordance-space world models enables different robots to learn various manipulation skills in complex settings, in under 30 minutes of interaction.

5.	21 Aug, researchers from UC Berkeley published a [paper](https://arxiv.org/pdf/2308.10897.pdf) “Can Language Models Learn to Listen?”. Given a video of a listener and speaker pair, researchers extract text corresponding to the spoken words of the speaker. They fine-tune a pretrained large language model to autoregressively generate realistic 3D listener motion in response to the input transcript. The method generates semantically meaningful gestures (e.g. an appropriately timed smile inferred from “amazing”) that synchronously flow with the conversation. The model can optionally render the output of the approach as photorealistic video. [Watch this](https://www.youtube.com/watch?v=djpSOhdIU8M)

6.	21 Aug, Google published a [paper](https://arxiv.org/pdf/2308.08998.pdf) “Reinforced Self-Training (ReST) for Language Modeling”. Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. Experimental results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.

7.	21 Aug, [According to Reuters](https://www.reuters.com/legal/ai-generated-art-cannot-receive-copyrights-us-court-says-2023-08-21/), a work of art created by artificial intelligence without any human input cannot be copyrighted under U.S. law, a U.S. court in Washington, D.C., has ruled.

8.	22 Aug, [Meta released SeamlessM4T](https://about.fb.com/news/2023/08/seamlessm4t-ai-translation-model/), a Multimodal AI Model for Speech and Text Translations. SeamlessM4T is the first all-in-one multilingual multimodal AI translation and transcription model. his single model can perform speech-to-text, speech-to-speech, text-to-speech, and text-to-text translations for up to 100 languages depending on the task. [Project is here]( https://github.com/facebookresearch/seamless_communication)

9.	22 Aug, [according to Anaconda](https://www.anaconda.com/blog/announcing-python-in-excel-next-level-data-analysis-for-all), Anaconda and Microsoft announced a groundbreaking innovation: Python in Excel. This marks a transformation in how Excel users and Python practitioners approach their work. 

10.	22 Aug, [according to theVerge](https://www.theverge.com/2023/8/21/23840705/new-york-times-openai-web-crawler-ai-gpt), The New York Times blocks Open AI’s Web Crawler. No further comments from both side so far.

11.	22 Aug, a new SQLCoder project, [Defog SQLCoder](https://github.com/defog-ai/sqlcoder) was released last week. SQLCoder is a 15B parameter model that outperforms gpt-3.5-turbo for natural language to SQL generation tasks on sql-eval framework, and significantly outperforms all popular open-source models. It also significantly outperforms text-davinci-003, a model that's more than 10 times its size.

12.	22 Aug, a group of research from different institutes published a [paper](https://arxiv.org/pdf/2308.08708.pdf) “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness”. The researchers survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higherorder theories, predictive processing, and attention schema theory. From these theories, researchers derive ”indicator properties” of consciousness, elucidated in computational terms that can be used to assess AI systems for these properties. Analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.

13.	23 Aug, according to [MarketChpost](https://www.marktechpost.com/2023/08/23/ai2-unveils-dolma-a-3-trillion-token-corpus-pioneering-transparency-in-language-model-research/), AI2 open source Dolma, a 3 trillion token corpus pioneering transparency in language model research. Opacity of datasets used by big players hinders external researchers’ ability to critically analyse and enhancing existing models. Dolma, the brainchild of AI2, emerges as a beacon of openness in a landscape shrouded in secrecy. With an all-encompassing dataset spanning web content, academic literature, code, and more, Dolma strives to empower the research community by granting them the tools to build, dissect, and optimize their language models independently.

14.	24 Aug, Meta released [Code Llama](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/), Open Foundation Models for Code. The code is available from [Github]( https://github.com/facebookresearch/codellama). Code Llama is a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. Llama code include foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively.

15.	25 Aug, [Phind.com](https://www.phind.com/blog/code-llama-beats-gpt4) fined tuned CodeLlama-34B and 34B-Python models with 32 A100 GPU on an internal Phind dataset that achieved 67.6% and 69.5% pass@1 on HumanEval, respectively, while GPT-4 achieved 67%. Phind’s models are available to download from [huggingface]( https://huggingface.co/Phind/Phind-CodeLlama-34B-v1)

16.	19-25 Aug, [IJCAI 2023](https://ijcai-23.org/) was hold in Macao. Distinguished papers include “Levin Tree Search with Context Models”, “SAT-Based PAC Learning of Description Logic Concepts”, and “Safe Reinforcement Learning via Probabilistic Logic Shields”.

17.	26 Aug, [WizardLM_AI](https://twitter.com/WizardLM_AI/status/1695396881218859374) twittered that “WizardCoder-34B surpasses GPT-4, ChatGPT-3.5 and Claude-2 on HumanEval with 73.2% pass@1”. The model weights are available [here](https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0).



**20 Aug 2023**
1.	14 Aug, Meta published a [paper](https://arxiv.org/pdf/2308.06259.pdf) “Self-Alignment with Instruction Backtranslation”. The paper present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. The model, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models, demonstrating highly effective self-alignment.

2.	14 Aug, Alex Graves published a [paper](https://arxiv.org/pdf/2308.06259.pdf) “Bayesian Flow Networks”, which is [described](https://www.reddit.com/r/MachineLearning/comments/15rrljw/bayesian_flow_networks/?rdt=50628) as “Another Alex Graves paper that will take 10 years for the community to fully digest.” One of the author of the paper, Rupesh Srivastava, [summarized it](https://twitter.com/rupspace/status/1691584987148218841) as  the researchers “present a new perspective on the ideas related to diffusion models. BFNs combine Bayesian inference and neural nets to yield a model class with simple objectives that gracefully extends to discrete data.”

3.	14 Aug, Meta released a [paper](https://arxiv.org/pdf/2308.07317.pdf) “Platypus: Quick, Cheap, and Powerful Refinement of LLMs”. Platypus is a family of fine-tuned and merged Large Language Models that achieves the strongest performance and currently stands at first place in HuggingFace’s Open LLM Leaderboard as of the release date of this work. In particular, a 13B Platypus model can be trained on a single A100 GPU using 25k questions in 5 hours.

4.	14 Aug, researchers from Microsoft published [paper](https://arxiv.org/pdf/2308.06873.pdf) “SpeechX: Neural Codec Language Model as a Versatile Speech Transformer”. Researcher find existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. The introduced SpeechX is a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX achieves comparable or superior performance to specialized models across tasks.

5.	14 Aug, while some [report](https://technext24.com/2023/08/14/chatgpt-costs-700000-daily-openai/) estimated that ChatGPT costs $700,000 to run daily, OpenAI may go bankrupt in 2024; others believe that ChatGPT is probably a small profit center right now.

6.	14 Aug,  according to [BusinessInsider](https://www.businessinsider.com/ai-radically-reshape-job-market-global-economy-employee-labor-innovation-2023-8), the rise of AI is poised to disrupt the global economy, potentially eliminating millions of jobs as AI tools become more accessible and advanced. A surge in public adoption of AI is predicted to reshape industries and labor markets, with estimates indicating that over 300 million jobs could be affected globally. While AI's transformative potential could contribute trillions to the economy, it calls for urgent preparation by governments, businesses, and workers to manage the impending upheaval and ensure a smoother transition through retraining and workforce adaptation.

7.	14 Aug, according [seroundtable](https://www.seroundtable.com/microsoft-bing-chat-outperforms-gpt-4-35873.html), Microsoft Bing's CEO, Mikhail Parakhin said on Twitter that Bing Chat outperforms raw GPT-4 but it comes at an expense. He said, "It is outperforming according to our measurements," when someone said they think Bing Chat beats out GPT-4 from OpenAI. Keep in mind, Bing uses GPT-4 from OpenAI, but as Mikhail Parakhin explained and as many of you already know, "Bing is using retrieval-augmented inference." He adds that doing all this is much more expensive, he said "Yes, we have that offering, it ends up pretty pricey, as it requires multiple model calls + Search calls, so far it’s been only used by a few companies."

8.	15 Aug, according to [Businessinsider](https://www.businessinsider.com/chatgpt-isnt-good-enough-to-take-jobs-unlikely-mass-layoffs-2023-8), Despite initial fears of AI causing mass layoffs, the reality is that AI, exemplified by ChatGPT, hasn't proven itself capable enough to replace most jobs. The anticipated automation wave has not led to widespread unemployment; the adoption of AI tools has been widely embraced, but the impact on jobs has been less dramatic than predicted. Many companies find AI useful for specific tasks but not advanced enough to handle the complexity of human roles. While some jobs will be affected, the complexities of tasks and the need for human supervision are hindrances to complete automation. The transition to AI-driven work is intricate, and the anticipated AI-driven unemployment has not materialized as predicted.

9.	16 Aug, [OpanAI added a new content moderation feature](https://openai.com/blog/using-gpt-4-for-content-moderation#LilianWeng). Content moderation demands meticulous effort, sensitivity, a profound understanding of context, as well as quick adaptation to new use cases, making it both time consuming and challenging. The new feature makes the process of developing and customizing content policies is trimmed down from months to hours. 1) Once a policy guideline is written, policy experts can create a golden set of data by identifying a small number of examples and assigning them labels according to the policy.  2)Then, GPT-4 reads the policy and assigns labels to the same dataset, without seeing the answers. 3) By examining the discrepancies between GPT-4’s judgments and those of a human, the policy experts can ask GPT-4 to come up with reasoning behind its labels, analyze the ambiguity in policy definitions, resolve confusion and provide further clarification in the policy accordingly. Repeat steps 2 and 3 until it is satisfied with the policy quality. This iterative process yields refined content policies that are translated into classifiers, enabling the deployment of the policy and content moderation at scale.

10.	16 Aug, according to [Venturebeat](https://venturebeat.com/ai/consulting-giant-mckinsey-unveils-its-own-generative-ai-tool-for-employees-lilli/), McKinsey is debuting a gen AI tool of its own: Lilli, a new chat application for employees. The tool serves up information, insights, data, plans, and even recommends the most applicable internal experts for consulting projects, all based on more than 100,000 documents and interview transcripts. The interface will look familiar to those who have used other public-facing text-to-text based gen AI tools such as OpenAI’s ChatGPT and Anthropic’s Claude 2. Lilli leverages currently available LLMs, including those developed by McKinsey partner Cohere as well as OpenAI on the Microsoft Azure platform, to inform its GenAI Chat and natural language processing (NLP) capabilities.

11.	16 Aug, as [reported by Independent](https://www.independent.co.uk/tech/google-quantum-computer-apocalypse-encryption-password-security-b2393516.html), Google is taking steps to address the potential security threat posed by quantum computers, known as the "quantum apocalypse." Quantum computers have the capability to break current encryption methods that protect sensitive data. To counter this, Google has integrated a hybrid cryptographic algorithm, X25519Kyber768, into Chrome to resist attacks from future quantum computers. While quantum computers capable of breaking current encryption are projected to be years away, Google's move is aimed at securing data now to prevent it from being intercepted and decrypted in the future.

12.	16 Aug, according to [BusinessInsider](https://www.businessinsider.com/databricks-ali-ghodsi-unqualified-ceos-dont-know-ai-data-2023-8), Databricks’ CEO Ali Ghodsi said that “I run a $38 billion software company. In 10 years, CEOs who don't understand data and AI won't be eligible for the top job in any industry.” As generative AI becomes more ubiquitous, Ghodsi says companies that prioritize data will have an advantage.

13.	16 Aug, Lamini, an AI company, [released a blog](https://www.lamini.ai/blog/one-billion-times-faster-finetuning-with-lamini-peft) “One Billion Times Faster Finetuning with Lamini PEFT”, in brief, Parameter-efficient finetuning (PEFT) makes it one billion times faster to scale unlimited LLM variations for specialized tasks. Chain and switch between thousands of finetuned LLMs - from months down to just milliseconds.

14.	16 Aug, according to [theVerge](https://www.theverge.com/2023/8/15/23833045/google-artificial-intelligence-summary-chrome-sge), Google’s AI-powered article summaries are rolling out for iOS and Android first, before coming to Chrome on the desktop. The Search Generative Experience (SGE) project will be able to summarize articles users are reading on the web, according to a Google blog post. SGE can already summarize search results so that users don’t have to scroll forever to find what they are looking for, and this new feature is designed to take that further by helping users out after they are actually clicked a link.

15.	17 Aug, according to [financial times](https://www.ft.com/content/ed323f48-fe86-4d22-8151-eed15581c337), The hype around generative AI is met with skepticism as doubts arise about its effectiveness and practical applications. Technologist Gary Marcus questions the technology's reliability, noting its tendency to produce inaccurate content. Concerns also center on generative AI's impact on future training data, potentially leading to misinformation. While investors argue for its value as a productivity tool and problem solver, doubts persist about its true potential. Amid the uncertainty, cloud computing providers and chip manufacturers benefit, while the longevity of generative AI's impact remains uncertain.

16.	19 Aug, according to [gizmodo](https://gizmodo.com.au/2023/08/metas-next-big-open-source-ai-dump-will-reportedly-be-a-code-generating-bot/), Meta may release a Code-generation bot as soon as next week. The reporter from Information who spoke to two anonymous sources with direct knowledge of the AI, this new model dubbed “Code Llama” will be open source and available free online. This is consistent with the company’s strategy so far of releasing widely available AI software that makes developing new customizable AI models much easier for companies who don’t want to pay OpenAI or others for the privilege.



**14 Aug 2023**

1.	7 Aug, [according to searchenginejournal](https://www.searchenginejournal.com/openai-launches-gptbot-how-to-restrict-access/493394/#close), OpenAI launches GPTBot, a web crawler which will be used the company to improve its AI models; it also has instructions on how to restrict or limit its access. The news has sparked a debate on Hacker News around the ethics and legality of using scraped web data to train proprietary AI systems. Some think OpenAI, like other person learning from online content, can freely use public web data; others believe if OpenAI use the data to make profit, the profit should be shared among the data owners.
2.	7 Aug, Anthropic.AI published a [paper](https://arxiv.org/pdf/2308.03296.pdf) “Studying Large Language Model Generalization with Influence Functions”. The paper argues that LLMs are difficult to understand, but influence functions can help. Researchers developed a method for scaling influence functions to large models and found that: LLMs do not simply memorize training sequences; Larger models generalize better; Influence is evenly distributed, but different layers exhibit distinct patterns; LLMs are sensitive to word order; Role-playing behavior is primarily driven by imitation. These findings suggest that LLMs are more complex and nuanced than previously thought.
3.	6-9 Aug, [SIGGRAPH](https://s2023.siggraph.org/) was holding  at Los Angeles. This is its 50th annual conference covering topics such as production & animation, research & education, arts & design, gaming & interactive and others. Jensen Huang delivered a keynote speech. He introduced Grace Hopper, a new super chip for AI, as computing’s “killer app”. The chip is ten times faster, much cheaper and has lower power consumptions. He also repeated his famous word: the more you buy, the more you save.
4.	7 Aug, researcher from DeepMind published a [paper](https://arxiv.org/pdf/2308.03958.pdf) “Simple synthetic data reduces sycophancy in large language models”. Sycophancy is an undesirable behavior where models tailor their responses to follow a human user’s view even when that view is not objectively correct (e.g., adapting liberal views once a user reveals that they are liberal). To address the issue, researchers present a straightforward synthetic-data intervention that takes public NLP tasks and encourages models to be robust to user opinions on these tasks. Adding these data in a lightweight finetuning step can significantly reduce sycophantic behavior on held-out prompts.
5.	8 Aug, [according to BlackBerry](https://blogs.blackberry.com/en/2023/08/why-companies-ban-chatgpt-ai), 75% of organizations worldwide set to ban chatGPT and generative AI apps on work devices. The data is based on a BlackBerry survey of 2,000 IT decision-makers across the US, Canada, the UK, France, Germany, the Netherlands, Japan, and Australia. Potential risk to data security and privacy is the biggest reason (67%) survey respondents cited for moving to block ChatGPT and similar generative AI tools. The next greatest concern (57%) is risk to corporate reputation.  
6.	8 Aug, [Stability.AI announced StableCode](https://stability.ai/blog/stablecode-llm-generative-ai-coding), according to Stability.AI, StableCode offers a unique way for developers to become more efficient by using three different models to help in their coding. The base model was first trained on a diverse set of programming languages from the stack-dataset (v1.2) from BigCode and then trained further with popular languages like Python, Go, Java, Javascript, C, markdown and C++.  In total, we trained our models on 560B tokens of code on our HPC cluster.
7.	9 Aug, [IBM is planning](https://newsroom.ibm.com/2023-08-09-IBM-Plans-to-Make-Llama-2-Available-within-its-Watsonx-AI-and-Data-Platform) to host Meta's Llama 2-chat 70 billion parameter model in the watsonx.ai studio, with early access now available to select clients and partners. This will build on IBM's collaboration with Meta on open innovation for AI, including work with open source projects developed by Meta – such as the PyTorch machine learning framework and the Presto query engine used in watsonx.data.
8.	9 Aug, [Biden-Harris Administration launches](https://www.whitehouse.gov/briefing-room/statements-releases/2023/08/09/biden-harris-administration-launches-artificial-intelligence-cyber-challenge-to-protect-americas-critical-software/) AI cyber challenge, together with Microsoft, OpenAI, Google and Anthropic, to pretect America’s critical software. DARPA will host an open competition in which the competitor that best secures vital software will win millions of dollars in prizes. AI companies will make their cutting-edge technology—some of the most powerful AI systems in the world—available for competitors to use in designing new cybersecurity solutions.
9.	10 Aug, Stanford researchers open source the famous [“Stanford Smallville”](https://github.com/joonspk-research/generative_agents). In their [paper](arxiv.org/abs/2304.03442), 25 AI agents inhabit a digital Westworld, unaware that they are living in a simulation. They go to work, gossip, organize socials, make new friends, and even fall in love. Each has unique personality and backstory. The github repository contains the core simulation module for generative agents—computational agents that simulate believable human behaviors—and their game environment. 
10.	10 Aug, Researchers from Google recently [published a blog](https://pair.withgoogle.com/explorables/grokking/#:~:text=In%202021%2C%20researchers%20made%20a,after%20training%20for%20much%20longer.) “Do Machine Learning Models Memorize or Generalize?”. As we know, we train a ML model to generalize rather than memorize. In the blog, the authors use the term grokking to describe the phenomenon where generalization seems to happen abruptly and long after fitting the training data. While it isn’t yet clear how to apply these techniques to today’s largest models, starting small makes it easier to develop intuitions as we progress towards answering these critical questions about large language models.
11.	10 Aug, a [paper](https://arxiv.org/pdf/2308.03762.pdf) with title “GPT-4 Can’t Reason” raised [interesting discussions](https://news.ycombinator.com/item?id=37050257) on prompting engineering when using ChatGPT-like models, including GPT-4. Some argued that the author of the paper may have used an outdated version of GPT-4 , or have been using the wrong prompt for GPT-4. There are many different ways to prompt engineer GPT-4, and the best approach depends on the specific task at hand; others believe prompting engineering is cheating, but others argued that it is simply a way to help GPT-4  to perform better.
12.	11 Aug, researchers from Microsoft and Uni. Of Southern California released [UniversalNER](https://arxiv.org/abs/2308.03279). The researchers propose a general recipe for targeted distilling where they train student models using mission-focused instruction tuning for a broad application class such as open information extraction. Researchers show that this can maximally replicate LLM’s capabilities for the given application class, while preserving its generalizability across semantic types and domains. Using NER as a case study, they successfully distill these capabilities from LLMs into a much smaller model UniversalNER that can recognize diverse types of entities or concepts in text corpora from a wide range of domains. UniversalNER surpasses existing instruction-tuned models at the same size (e.g., Alpaca, Vicuna) by a large margin, and shows substantially better performance to ChatGPT.



**6 Aug 2023**
1.	Kaiming He, [announced that](https://kaiminghe.github.io/) he will join Department of Electrical Engineering and Computer Science at MIT in 2024. His total Google Scholar citations exceed 460,000 + times, and he will become the first at MIT. Previously, he worked at FAIR. His research results [ResNet](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) is one of the cornerstone papers in the field of Deep learning.
2.	Recently, Stanford Uni published a [paper](https://arxiv.org/pdf/2307.15189.pdf): “Med-Flamingo: a multimodal medical few-shot learner”. Based on OpenFlamingo-9B, researchers continue pre-training on paired and interleaved medical image-text data from publications. Med-Flamingo improves performance in generative medical VQA by up to 20% in clinician’s rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation and textbooks.
3.	1 Aug, according to [Search Engine Journal](https://www.searchenginejournal.com/openai-files-trademark-application-gpt-5/493040/#close), OpenAI has filed a trademark application for “GPT-5”. The application covers a wide range of software related to language models and AI, such as downloadable computer programs and software related to language models, artificial production of human speech and text, natural language processing, generation, understanding, and analysis. However, as Sam Altman said recently, “We have a lot of work to do before GPT 5. It takes a lot of time for it. We are not certainly close to it. There needs to be more safety audits. I wish I could tell you about the timeline of the next GPT”.
4.	1 Aug, [LLM-UTILS published](https://gpus.llm-utils.org/nvidia-h100-gpus-supply-and-demand/) a blog discussing the supply and demand of GPUs. The blog commented that as of August 2023, it seems AI might be bottlenecked by the supply of GPUs. This comment is supported by Sam Altman, he said that OpenAI is GPU-limited and it’s delaying their short term plans (fine-tuning, dedicated capacity, 32k context windows, multimodality). Elon Must also said that “GPUs are at this point considerably harder to get than drugs.” The blog further discussed what do we want to know about the bottleneck, including what’s causing it, how long will it last, and wha’t going to help resolved it.
5.	1 Aug, researchers from Google and Uni of Washington [published a paper](https://arxiv.org/pdf/2308.00675.pdf) “Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models”. Instead of providing a few demonstrations of the tool’s usage, the paper provides an alternative to demonstrations: tool documentation, descriptions of the individual tool usage. Experimental results demonstrate the zero-shot documentation is on par or better than few-shot without documentation.
6.	2 Aug, Google published a [paper on Nature](https://www.nature.com/articles/s41586-023-06221-2): “Scientific discovery in the age of artificial intelligence”. The paper indicates that AI is being increasingly integrated into scientific discovery to augment and accelerate research, helping scientists to generate hypotheses, design experiments, collect and interpret large datasets, and gain insights that might not have been possible using traditional scientific methods alone. Researchers then examine breakthroughs over the past decade that include self-supervised learning, which allows models to be trained on vast amounts of unlabelled data, and geometric deep learning, which leverages knowledge about the structure of scientific data to enhance model accuracy and efficiency. The paper further suggests that both developers and users of AI tools need a better understanding of when such approaches need improvement, and challenges posed by poor data quality and stewardship remain.
7.	2 Aug, Meta released its MusicGen project, [Audiocraft](https://github.com/facebookresearch/audiocraft). Audiocraft is a library for audio processing and generation with deep learning. It features the state-of-the-art EnCodec audio compressor / tokenizer, along with MusicGen, a simple and controllable music generation LM with textual and melodic conditioning.
8.	Mckinsey [released a survey](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2023-generative-ais-breakout-year?cid=aisurge2023-soc--mar-mar--07/23-i1a--bam-ip&linkId=227872978#/) titled “The state of AI in 2023: Generative AI’s breakout year”. The survey confirms the explosive growth of generative AI (gen AI) tools. Less than a year after many of these tools debuted, one-third of the survey respondents say their organizations are using gen AI regularly in at least one business function. The organizations that have already embedded AI capabilities have been the first to explore gen AI’s potential, and those seeing the most value from more traditional AI capabilities—a group we call AI high performers—are already outpacing others in their adoption of gen AI tools.
9.	3 Aug, [IBM reported](https://research.ibm.com/blog/nasa-hugging-face-ibm) that IBM and NASA open source the largest geospatial AI foundation model on [Hugging Face](https://huggingface.co/ibm-nasa-geospatial). The move aims to widen access to NASA satellite data (250,000 terabytes) and accelerate climate-related discoveries by using a model built by IBM.
10.	3 Aug, Microsoft announced [project Rumi](https://www.maginative.com/article/project-rumi-augmenting-ai-understanding-through-multimodal-paralinguistic-prompting/), which aims to incorporate paralinguistic input, such as intonation, gestures, and facial expressions, into prompt-based interactions with LLMs. Rumi leverages separately trained vision and audio-based models to assess sentiment from cognitive and physiological data in real-time. The system extracts non-verbal cues from video and voice inputs in real-time, creating paralinguistic tokens that augment the standard lexical input to existing LLMs like GPT4. Microsoft's future plans for Project Rumi include improving the performance of existing models and incorporating additional signals, like heart rate variability derived from standard video, cognitive, and ambient sensing. These explorations paint a picture of a more dynamic, sensitive AI capable of understanding the complexity of human interaction.



**30 July 2023**
1.	19 Jul,  a [paper](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2307.09793.pdf&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=Nx2s8Qs1wXqcvi%2FYaGv9zBXTGlC91BSUj1ZalKGG2XM%3D&reserved=0) published by Stanford Uni. revealed that to date, nearly 16K (15,821) Text Generation models have been uploaded to Hugging Face. 
2.	22 Jul, [according to Reuters](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.reuters.com%2Ftechnology%2Fopenai-google-others-pledge-watermark-ai-content-safety-white-house-2023-07-21%2F&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=giSWo1jnuNaFyZJaTQQaMQd%2FvvFAm5I%2FwB%2BBEsl%2FYPI%3D&reserved=0), AI companies including OpenAI, Alphabet and Meta Platforms have made voluntary commitments to the White House to implement measures such as watermarking AI-generated content to help make the technology safer, President Joe Biden announced on Friday. The companies - which also include Anthropic, Inflection, Amazon.com and OpenAI partner Microsoft - pledged to thoroughly test systems before releasing them and share information about how to reduce risks and invest in cybersecurity.
3.	23-29 Jul. [ICML 2023](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Ficml.cc%2F&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=XrTqRYqIURwmY7HxqnXXA6lzFD%2FqcW6LcIxzIC0z7Wg%3D&reserved=0). All published paper titles can be found [here](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Ficml.cc%2Fvirtual%2F2023%2Fpapers.html%3Ffilter%3Dtitles&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=%2F9VBhsYbaROsWyY%2FkPkYLhk9GAegRhJ93r5tOtrv1RY%3D&reserved=0), rewards paper titles can be found [here](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Ficml.cc%2Fvirtual%2F2023%2Fawards_detail&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=uiK98xlINGLHXKrMiO%2BPDy6s%2FGQtMio7m%2Bz3V2T7RIA%3D&reserved=0). Need register to access the papers
4.	24th Jul, Evan Miller [published a blog](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.evanmiller.org%2Fattention-is-off-by-one.html&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=YyFOS2S4xQfttpjfMQUHsHTdPLzov10A%2B6MOmVROhyA%3D&reserved=0), “Attention is Off By One”. Evan argues that the softmax formula applied on the Q multiply transpose of V divided by square-root of d should be fixed by adding one to the denominator, which is called the softmar1. He named the new attention as QuietAttention because it allows one attention head to simply “pass” rather than must add something to the output vector. The proposed QuietAttention is expected to alleviate outlier issues of attention which occur in white space and punctuation positions.
5.	24 Jul, researchers from UCLA, IBM, et al. published a paper “Injecting the 3D World into Large Language Models”, In [this work](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fvis-www.cs.umass.edu%2F3dllm%2F&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=Nu%2FZSEooXdXLagj3jfeGWuYPdZR9hEVtoEIY2eEglcg%3D&reserved=0), researchers propose to inject the 3D world into large language models, and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on.
6.	25 Jul, [it is reported](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwindowsreport.com%2Fg3po-ai%2F&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=E2MyXhwjgsEjEm%2BFVA55LR2HxLPg76%2B%2BuLPVWSt9WrI%3D&reserved=0) that OpenAI is planning to release its own open-source language model, codenamed G3PO, to response Meta’s LLaMA2 and other open-source LLMs. While the project has a codename, unfortunately, it doesn’t have a release date for now. It seems that OpenAI might be thinking about other endeavors, such as its superlignment project and to achieve AGI in 4 years from now.
7.	26 Jul, Anthropic, Google, Microsoft, and OpenAI are launching the [Frontier Model Forum](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fopenai.com%2Fblog%2Ffrontier-model-forum&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=c7PK6qaqhp%2BJ5jBfU4RI6GkzZ1x0an%2FV0BQZYtRwwpk%3D&reserved=0), an industry body focused on ensuring safe and responsible development of frontier AI models. The Forum aims to help (i) advance AI safety research to promote responsible development of frontier models and minimize potential risks, (ii) identify safety best practices for frontier models, (iii) share knowledge with policymakers, academics, civil society and others to advance responsible AI development; and (iv) support efforts to leverage AI to address society’s biggest challenges.
8.	26 Jul. [Satbility.AI released SDXL 1.0](https://stability.ai/blog/stable-diffusion-sdxl-1-announcement), the next iteration in the evolution of text-to-image generation models. Following the limited, research-only release of SDXL 0.9, the full version of SDXL has been improved to be the world's best open image generation model. SDXL generates images of high quality in virtually any art style and is the best open model for photorealism. Distinct images can be prompted without having any particular ‘feel’ imparted by the model, ensuring absolute freedom of style. SDXL 1.0 is particularly well-tuned for vibrant and accurate colors, with better contrast, lighting, and shadows than its predecessor, all in native 1024x1024 resolution.
9.	26 Jul, [according to itnews](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.itnews.com.au%2Fnews%2Fgoogle-handed-user-data-to-aus-authorities-5525-times-last-year-598413&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=cEb%2FmmzC9rUucXZC7SUsdBFZdAfxWcw7DirS%2FpxcJD8%3D&reserved=0), Google complied with 5525 of 6335 requests from Australian authorities to disclose user information relating to 7183 accounts last year. Meta and TikTok’s transparency reports are the same; although Meta revealed that it complied with 3563 Australian agencies' requests last year and TikTok complied with 91, during the period - little is known about who made them.
10.	26 Jul, [according to TheVerge](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.theverge.com%2F2023%2F7%2F26%2F23808274%2Fmeta-microsoft-amazon-overture-open-source-mapping&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=gdCngplXQBJ1%2BShnqqN8%2B6b76TlHkCtKFwbmPLxyRGM%3D&reserved=0), Meta, Microsoft, Amazon, and the mapping company TomTom have launched an initiative to take on Google Maps and Apple Maps. The four companies formed the Overture Maps Foundation last year with the goal of creating interoperable map products — and now, the group has released its first open map dataset. With this data, third-party developers can create global mapping or navigation products of their own, allowing them to go head-to-head with Google Maps and Apple Maps. According to Overture, the release includes over 59 million places of interest, along with data on buildings, transportation networks, and administrative boundaries.
11.	27 Jul, [according to WashingtonPost](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.washingtonpost.com%2Ftechnology%2F2023%2F07%2F27%2Fsocial-media-research-meta-political-views%2F&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=oJGb8dTUulwof%2FnAzqzPS2LQC9NqlAWcbXGzGcm2a3I%3D&reserved=0), the massive study of Facebook and Instagram shows that changing Facebook’s algorithm won’t fix polarization. “Despite the fact that we find this big impact in people’s on-platform experience, we find very little impact in changes to people’s attitudes about politics and even people’s self-reported participation around politics.” Meta’s research results are published on both [Science](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.science.org%2Fcontent%2Farticle%2Fdoes-social-media-polarize-voters-unprecedented-experiments-facebook-users-reveal&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571567254%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=u8cip2sJh%2FkcYHZjJVA5c3n2jLsP09Y2VaXDLeNoNAg%3D&reserved=0) and [Nature](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-023-02420-z&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=hmIRqOexb7yzCiC9QcC2St65C%2FvedNt1YUrG8S%2FkjS0%3D&reserved=0).
12.	27 Jul, [according to NY Times](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nytimes.com%2F2023%2F07%2F27%2Fbusiness%2Fai-chatgpt-safety-research.html&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=JlqhfMxkj1AsaGePUhVBjWp%2Fm%2Bllus4xBsbaaqBwg6I%3D&reserved=0), researchers from CMU published a [paper](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2307.15043.pdf&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=euiTfl%2BQXX1l0M3o%2BRZLw4WtJVM8%2BrsurtE3vrTGy6I%3D&reserved=0) that showed how anyone could circumvent A.I. safety measures and use any of the leading chatbots to generate nearly unlimited amounts of harmful information. The researchers found that they could break through the guardrails of open source systems by appending a long suffix of characters onto each English-language prompt fed into the system.
13.	27 Jul, a16z proposed [Money on Autopilot](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fa16z.com%2F2023%2F07%2F27%2Fmoney-on-autopilot-ai-personal-finance%2F&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=W9W%2Be9%2B3WTN8p9MbNQ6QdeDQDfNPBOKjGGcBSk%2BdHXA%3D&reserved=0), the much-discussed topic of “self-driving money” finally has a chance to achieve its potential. Post-generative AI, we’re in a new world for consumer financial platforms. LLMs, and specifically multi-modal prompts like GPT-4, can process and output both text and images. This enables consumer robot process automation (RPA), which will allow fintech apps to operate on a user’s behalf.
14.	28 Jul, [according to BusinessInsider](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.businessinsider.com%2Fopenai-cant-identify-ai-generated-text-bad-for-internet-models-2023-7&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=bvKFq0gVx8RTcgLb%2F1rg1ZxpDK3kdb%2BCoiHFJYZqbcw%3D&reserved=0), OpenAI just admitted it can't identify AI-generated text. "As of July 20, 2023, the AI classifier is no longer available due to its low rate of accuracy,"  OpenAI wrote in a [recent blog](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fopenai.com%2Fblog%2Fnew-ai-classifier-for-indicating-ai-written-text&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=KDAjgPC4CaJCEpESJrsk0yfydHaJj8mH0EeSCCaHZGQ%3D&reserved=0). "We are working to incorporate feedback and are currently researching more effective provenance techniques for text." If tech companies use AI-produced data inadvertently to train new models, some researchers worry those models will get worse.
15.	28 Jul, Google released its [Robotics-transformer2 project](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Frobotics-transformer2.github.io%2F&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=cylTIuY6WempGlk%2FYksJRS7Vs2uWP6KfMV2l%2FWK3d5s%3D&reserved=0) and a [paper](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Frobotics-transformer2.github.io%2Fassets%2Frt2.pdf&data=05%7C01%7Cd.zhu%40curtin.edu.au%7C56ec33fa33d7422800b908db90ead9a0%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638263110571723482%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=NZdn3QAulqGaV2RKVjSGziBix%2Bd8Pja39J7RfiIURzI%3D&reserved=0) “RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control”. The vision-language-action models (VLA) express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. extensive evaluation (6k evaluation trials) shows that the approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). Researchers further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).



**23 July 2023**
1.	A recent [report from OpenUK](https://openuk.uk/wp-content/uploads/2023/07/FINAL-State-of-Open-The-UK-in-2023-Phase-Two-Part-1.pdf) indicates that in 2022, the Gross Value added to the UK economy from Open Source Software is estimated to be £13.59 billion. So what does that mean? Contextualising it with the UK Tech Sector contribution at £50 billion in 2022, the directly attributable contribution from Open Source Software is therefore 27%, more than a quarter of the overall Tech Sector contribution.
2.	17 Jul, Microsoft published a [paper](https://arxiv.org/abs/2307.08621): “Retentive Network: A Successor to Transformer for Large Language Models”. RetNet achieves low-cost inference (i.e., GPU memory, throughput, and latency), training parallelism, and favorable scaling curves compared with Transformer. RetNet makes the “impossible triangle” possible, which achieves training parallelism, good performance, and low inference cost simultaneously. The code will be released within two weeks.
3.	17 Jul, [HPC-AI released its 65 billion](https://www.hpc-ai.tech/blog/large-model-pretraining) parameter large language model. It utilizes the current most widely used large model, LLaMA, to provide an example of the tool’s groundbreaking pre-training solutions for the 65 billion parameter large model which improves the training speed by 38%. This can save enormous amounts for large model enterprises. HPC-AI only needs 32 A100/A800 GPUs to improve pre-training speed by 38% compared to other mainstream options in the industry.
4.	18 Jul, Dao published a [paper](https://tridao.me/publications/flash2/flash2.pdf), FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. FlashAttention2 (1) tweaks the algorithm to reduce the number of non-matmul FLOPs (2) parallelizes the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distributes the work between warps to reduce communication through shared memory. These yield around2× speedup compared to FlashAttention, reaching 50-73% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations.
5.	19 Jul, Meta [released LLaMA v2](https://ai.meta.com/llama/), a series of large language models trained on 40% more data (2 Trillion tokens) than Llama 1, and has doubled the context length to 4096. Meta also provided finetuned dialog models with over 100k samples. Llama 2 outperforms other open source language models on many external benchmarks, including reasoning, coding, proficiency, and knowledge tests. The [LLaMA 2 paper](https://arxiv.org/pdf/2307.09288.pdf) is released as well. 
6.	19 Jul [Reuters reported](https://www.reuters.com/technology/apple-tests-generative-ai-tools-rival-openais-chatgpt-bloomberg-news-2023-07-19/) that Apple is working on AI offering similar to OpenAI’s ChatGPT and Google’s Bard, causing its shares up as much as 2% to a record high. Apple's new virtual assistant summarizes text and answers questions based on data it has been trained with, and the tool essentially replicates Bard, ChatGPT and Bing AI, and works as a web application, according to employees of Apple.
7.	19 Jul, researcher from UCL, EleutherAI, Meta, StabilityAI and others published a [paper](https://arxiv.org/pdf/2307.10169.pdf) “Challenges and Applications of Large Language Models”. The authors believed that due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. They explored the challenges of LLMs from three views: Designing LLMs relates to decisions taken before deployment. Behaviorial challenges occur during deployment. Science challenges hinder academic progress.
8.	20 Jul, Nature published a [paper](https://www.nature.com/articles/d41586-023-02317-x) “How to introduce quantum computers without slowing economic growth”. The researchers 
believe that new ways of simulating materials, optimizing processes and improving machine learning — could transform society. They also suggested that Specialists should work together to create narratives around the usefulness of quantum technologies; however, the technology bottlenecks for quantum computing are unclear, and Would these benefits lead to more products and services that are better tailored to customer needs? What would the impacts be on the wider industrial landscape, and what new business models might emerge?
9.	21 Jul, [StabilityAI released FreeWilly](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models), the large and mighty instruction finetuned open access language models (FreeWilly1 and FreeWilly2). FreeWilly1 leverages the original LLaMA 65B foundation model and was carefully fine-tuned with a new synthetically-generated dataset using Supervised Fine-Tune (SFT) in standard Alpaca format. Similarly, FreeWilly2 leverages the LLaMA 2 70B foundation model to reach a performance that compares favorably with GPT-3.5 for some tasks.
10.	21 Jul, [according to HDTECH](https://tech.hindustantimes.com/tech/news/sergey-brin-returns-to-google-to-work-on-secret-ai-project-gemini-71689927230676.html), Google’s cofounder Sergey Brin returns to Google to work on the secret AI project Gemini, the company’s highly ambitious general-purpose AI project. Gemini would be a multi-modal foundational model that powers other AI models but no further details are known at the moment.


**16 July 2023**
1. Jul 9 – 14, [ACL 2023](https://2023.aclweb.org/), Annual Meeting of the Association for Computational Linguistics (ACL), which is one of the top natural language processing conferences in the world, was held in Toronto, Canada.
2. Jul 11, Anthropic AI released [Claude 2](https://www.anthropic.com/index/claude-2), Claude 2 has improved performance, longer responses, and can be accessed via API as well as a new public-facing beta website, claude.ai. The latest model scored 76.5% on the multiple choice section of the Bar exam, up from 73.0% with Claude 1.3.
3. Jul 11, according to [Bloomberg](https://www.bloomberg.com/news/articles/2023-07-11/ai-researcher-who-helped-write-landmark-paper-is-leaving-google), AI Researcher Who Helped Write Landmark Paper Is Leaving Google, and the last one who is still working for Google, is departing Google, and will start a company after taking time off.
4. Jul 12, Elon Musk announced [xAI](https://x.ai/), is to understand the true nature of the universe. The new team members have previously worked at DeepMind, OpenAI, Google Research, Microsoft Research, Tesla, and the University of Toronto. Collectively the team contributed some of the most widely used methods in the field, in particular the Adam optimizer, Batch Normalization, Layer Normalization, and the discovery of adversarial examples. xAI aims at implement AGI by the end of 2029, the due date.
5. Jul 12, [businessinsider reported](https://www.businessinsider.com/ai-could-run-out-text-train-chatbots-chatgpt-llm-2023-7) that UC Berkly prof. Stuart Russell warned that AI developers are "running out of text" to train chatbots at a UN summit, and AI's strategy behind training large language models is "starting to hit a brick wall." It also reported that a group of AI researchers, estimated that machine learning datasets will likely deplete all "high-quality language data" before 2026.
6. Jul 12, Google published a [paper on Nature](https://www.nature.com/articles/s41586-023-06291-2) “Large language models encode clinical knowledge”. The paper proposes a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA3, MedMCQA4, PubMedQA5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics6), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%.
7. Jul 12, Nature published a [paper](https://www.nature.com/articles/s41586-023-06095-4) “Quantum-enhanced Markov chain Monte Carlo”. Researchers have developed a quantum algorithm that can sample from complicated distributions, such as MCMC, arising in several applications. This algorithm is well-suited to current hardware and could ease computational bottlenecks in machine learning, statistical physics, and optimization.
8. Jul 13, [Hashingtonpost reported](https://www.washingtonpost.com/technology/2023/07/13/ftc-openai-chatgpt-sam-altman-lina-khan/) that the Federal Trade Commission has opened an expansive investigation into OpenAI, probing whether the maker of the popular ChatGPT bot has run afoul of consumer protection laws by putting personal reputations and data at risk. The FTC’s demands of OpenAI are the first indication of how it intends to enforce those warnings. If the FTC finds that a company violates consumer protection laws, it can levy fines or put a business under a consent decree, which can dictate how the company handles data. The FTC in its request also asked the company to provide extensive details about its products and the way it advertises them. It also demanded details about the policies and procedures that OpenAI takes before it releases any new product to the public, including a list of times that OpenAI held back a large language model because of safety risks.
9. Jul 14, Meta published a [paper](https://ai.meta.com/research/publications/scaling-autoregressive-multi-modal-models-pretraining-and-instruction-tuning/) “Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning”. The new model, named CM3Leon, is a retrieval-augmented, tokenbased, decoder-only multi-modal language model capable of generating and infilling both text and images. CM3Leon achieves state-of-theart performance in text-to-image generation with 5x less training compute than
comparable methods.


**10 July 2023**
1. Last week, Salesforce released [XGen-7b](https://blog.salesforceairesearch.com/xgen/), which achieves comparable or better results when compared with state-of-the-art open-source LLMs (e.g. MPT, Falcon, LLaMA, Redpajama, OpenLLaMA) of similar model size, and its targeted evaluation on long sequence modeling benchmarks show benefits of our 8K-seq models over 2K- and 4K-seq models. Training cost of $150K on 1T tokens under Google Cloud pricing for TPU-v4.
2. 4th July, according to [iTnews](https://www.itnews.com.au/news/chatgpt-used-in-peer-reviews-of-australian-research-council-grant-applications-597596), ChatGPT is used in peer reviews of Australian Research Council grant applications. However, ARC warns that this could be a breach of confidentiality, and has since released a statement advising peer reviewers not to use AI as part of their assessments.
3. 4th July, OpenAI [announced](https://techcrunch.com/2023/07/06/openai-makes-gpt-4-generally-available/) to make GPT-4 API and Code Interpreter generally available to all paying API users. It also announced a deprecation plan for some old models which will retire in 2024.
4. On 5th July, a group of researchers published a [paper](https://arxiv.org/abs/2307.02053) “FLACUNA: Unleashing the Problem-Solving Power of VICUNA using FLAN Fine-Tuning”. The researchers constructed a new dataset comprising a large number of tasks that demand problem-solving skills. Experimental findings strongly indicate that the enhanced problem-solving abilities of  FLACUNA, are obtained through fine-tuning VICUNA on the FLAN dataset, leading to significant improvements across numerous benchmark datasets in INSTRUCTEVAL.
5. 5th July, Nature published a [paper](https://www.nature.com/articles/s41586-023-06185-3) “Accurate medium-range global weather forecasting with 3D neural networks”. The authors of the paper proposed that three-dimensional deep neural networks can be trained to forecast global weather patterns, including extreme weather, with accuracy greater than or equal to that of the best numerical weather prediction models.
6. 5th July, researchers from Microsoft published a [paper](https://arxiv.org/abs/2307.02486#:~:text=In%20this%20work%2C%20we%20introduce%20LongNet%2C%20a%20Transformer,the%20attentive%20field%20exponentially%20as%20the%20distance%20grows.) “LongNet: Scaling Transformers to 1,000,000,000 Tokens”. LongNet is a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows.
7. 6th July, Google and others published a [paper](https://arxiv.org/abs/2307.03170#:~:text=To%20tackle%20this%20problem%2C%20we%20introduce%20the%20Focused,space%2C%20enabling%20an%20extension%20of%20the%20context%20length.) “Focused Transformer: Contrastive Training for Context Scaling”. The paper introduces the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning. This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length.
8. 7th July,  according to [Washingtontpost](https://www.washingtonpost.com/technology/2023/07/07/chatgpt-users-decline-future-ai-openai/) and [this report](https://www.livemint.com/technology/tech-news/chatgpt-faces-first-ever-monthly-traffic-decline-shows-shift-in-user-preferences-report-11688621775980.html), ChatGPT, the highly popular AI chatbot introduced in November, experienced a decline in its website's monthly traffic and unique visitors for the first time in June, as reported by Similarweb analytics.
9. 10th July, according to [TheVerge](https://www.theverge.com/2023/7/10/23787453/meta-instagram-threads-100-million-users-milestone), Instagram’s Threads app surpasses 100 million users within only 5 day since its release.


**2 July 2023**
1. 23rd Jun, [A16Z’s Shoham interviewed](https://a16z.com/2023/06/23/the-next-token-of-progress-4-unlocks-on-the-generative-ai-horizon/) CEOs from Anthropic, Cohere, Charater.AI, they identified four key innovations: Steering, memory, “arms and legs”, and multimodality; and how these key innovations will evolve over the next 6 to 12 months.
2. 26 Jun, [Wired reports](https://www.wired.com/story/google-deepmind-demis-hassabis-chatgpt/) that DeepMind’s CEO says its next AI project Gemini, is still under development within several months, will be more capable than OpenAI’s ChatGPT, including such as planning or the ability to solve problems. AlphaGo-type techniques will be introduced in Gemini. The project could cost hundreds of millions of dollars, while GPT-4 cost more than $100 million, [according to Altman](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/).
3. 26 Jun, [VentureBeat, Databricks is acquiring MosaicML](https://venturebeat.com/data-infrastructure/databricks-is-acquiring-mosaicml-for-a-jaw-dropping-1-3-billion/) for a jaw-dropping $1.3 billion. The news was also confirmed by both Databricks and MosaicLM, an AI start-up established only one and a half years. MosaicLM believes that every organization should be able to benefit from the AI revolution with more control over how their data is used. MosaicLM has its own open-source [MPT serious LLMs](https://github.com/mosaicml/llm-foundry).
4. 27 Jun, Microsoft [published a paper](https://arxiv.org/pdf/2306.14824.pdf): “KOSMOS-2: Grounding Multimodal Large Language Models to the World”. KOSMOS-2 is a multimodal LLM, enabling new capabilities of perceiving object descriptions and grounding text to the visual world. Researchers also created a large-scale dataset of grounded image-text pairs to train the model. The research lays out the foundation for the development of Embodiment AI.
5. 27 Jun, [Nvidia](https://blogs.nvidia.com/blog/2023/06/27/generative-ai-debut-mlperf/),  H100 GPUs set new records on all eight tests in the latest MLPerf training benchmarks released today, excelling on a new MLPerf test for generative AI. On a commercially available cluster of 3,584 H100 GPUs co-developed by startup Inflection AI and operated by CoreWeave, a cloud service provider specializing in GPU-accelerated workloads, the system completed the massive GPT-3-based training benchmark in less than eleven minutes.
6. 28 Jun, [Meta published a paper](https://arxiv.org/pdf/2306.15595.pdf): “Extending Context Window of Large Language Models via Positional Interpolation”. Researchers present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B.
7. 29 Jun, [Oracle CEO said](https://finance.yahoo.com/news/oracle-spending-billions-nvidia-chips-224222975.html?guccounter=1) the company is spending "billions" of dollars on chips from Nvidia Corp as it expands a cloud computing service targeting a new wave of artificial intelligence (AI) companies, also including investment on CPUs.
8. 29 Jun, [Inflection.ai, Microsoft-backed start-up, has raised $1.3 billion](https://www.reuters.com/technology/inflection-ai-raises-13-bln-funding-microsoft-others-2023-06-29/) new funding. "We'll be building a cluster of around 22,000 H100s. This is approximately three times more compute than what was used to train all of GPT4. Speed and scale are what's going to really enable us to build a differentiated product," Suleyman said at Collision Conference on Thursday.
9. 2nd July, [OpenChat](https://github.com/imoneoi/openchat), based on LLaMA-13B, ranked #1 open source LLM on [AlpacaEval leaderboard](https://tatsu-lab.github.io/alpaca_eval/). OpenChat is finetuned with 8xA100 80GB GPUs.


**25 June 2023**
1. GitHub CEO [Dohmke says Copilot](https://www.freethink.com/robots-ai/github-copilot#:~:text=GitHub%20CEO%20says%20Copilot%20will%20write%2080%25%20of%20code%20%E2%80%9Csooner,the%20future%20of%20innovation%20itself.&text=Over%20the%20last%20fifteen%20years,of%20the%20world%20of%20coding.) will write 80% of code “sooner than later”, and that doesn’t mean the developer is going to replace. He also said that Copilot brings the fun back, it brings the creativity back, it brings the flow back.
2. 20th June, Microsoft [published a paper](https://arxiv.org/pdf/2306.11644.pdf) “Textbooks Are All You Need”. Researchers trained a transformer-based model phi-1 with 1.3B parameters trained for 4 days on 8 A100 GPUs, using a selection of textbook quality data from the Web(6B tokens). Phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP.
3. 20th June, UC Berkeley researcher announced [vLLM](https://vllm.ai/), an easy, fast and cheap LLM. vLLM equipped with PagedAttention redefines the new state of the art in LLM serving: it delivers up to 24x higher throughput than HuggingFace Transformers, without requiring any model architecture changes. ([Github](https://github.com/vllm-project/vllm))
4. 21st June, [GPT-Engineer is launched on GitHub](https://github.com/AntonOsika/gpt-engineer). GPT Engineer is made to be easy to adapt, extend, and make your agent learn how you want your code to look. It generates an entire codebase based on a prompt. It gains over 30K stars after 4 days.
5. 21st June, George Hotz in [a video said](https://www.latent.space/p/geohot#details) that “so GPT-4 is 220 billion in each head, and then it's an eight-way mixture model. So mixture models are what you do when you're out of ideas. So, you know, it's a mixture model. They just train the same model eight times, and then they have some little trick. They actually do 16 inferences, but no, it's not like- [00:43:45]”
6. 21st June, CNBC, [Google accuses Microsoft](https://www.cnbc.com/2023/06/21/google-accuses-microsoft-of-anticompetitive-practices-in-azure-cloud.html) of using stringent licensing terms to exert monopolistic control over the cloud market.
7. 21st June, [CVPR announced best paper awards](https://www.prnewswire.com/news-releases/cvpr-2023-best-paper-award-winners-announced-301857429.html), best papers: “[Visual Programming: Compositional visual reasoning without training](https://c212.net/c/link/?t=0&l=en&o=3900096-1&h=85460982&u=https%3A%2F%2Farxiv.org%2Fabs%2F2211.11559&a=Visual+Programming%3A+Compositional+visual+reasoning+without+training)”, and “[Planning-oriented Autonomous Driving](https://c212.net/c/link/?t=0&l=en&o=3900096-1&h=2174741114&u=https%3A%2F%2Farxiv.org%2Fabs%2F2212.10156&a=Planning-oriented+Autonomous+Driving)”. Best student paper: “ [3D Registration with Maximal Cliques](https://c212.net/c/link/?t=0&l=en&o=3900096-1&h=152672617&u=https%3A%2F%2Farxiv.org%2Fabs%2F2305.10854&a=3D+Registration+with+Maximal+Cliques)”
8. 22nd June, researchers from MIT and Microsoft [published a paper](https://arxiv.org/pdf/2306.09896.pdf) “Demystifying GPT self-Repair for Code Generation”. They found that “the effectiveness of self-repair is only seen in GPT-4”.
9. 22nd June, LMSYS updated the [leaderboard](https://lmsys.org/blog/2023-06-22-leaderboard/). GPT-4, GPT-3.5-Turbo and Claude-v1 are the top three on the list. The top OSS models are Vicuna-33B, WizardLM-33B, and Guanaco-33B. Falcon-40B  is ranked far below in the list, even below Vicuna-7B model.
10. 22nd June, Stability.ai launched [SDXL 0.9](https://stability.ai/blog/sdxl-09-stable-diffusion), a leap forward in AI image generation. The 0.9 version is the most advanced development in the Stable Diffusion text-to-image suite of models, and can produce massively improved image and composition detail over its predecessor.
11. 22nd June, According to CNBC, [AWS is investing $100 million](https://www.cnbc.com/2023/06/22/aws-invests-100-million-in-generative-ai-as-it-sees-a-long-race-ahead.html) in generative A.I. center in race to keep up with Microsoft and Google.
12. 22nd June,  [ MosaicML released MPT-30B](https://thenewstack.io/mosaicml-launches-30b-model-takes-on-llama-falcon-and-gpt/), ranked the same as Vicuan-13B. The company claims that it surpasses OpenAI’s GPT-3 in quality, despite having about 1/6th the number of parameters (GPT-3 has 175 billion). “This means MPT-30B is easier to run on local hardware and much cheaper to deploy for inference”
13. 22nd June, researchers from MIT and Stanford [published a paper](https://arxiv.org/pdf/2306.12672.pdf) “From Word Models to World Models”. The paper proposed rational meaning construction, a computational framework for language-informed thinking that combines neural models of language with probabilistic models for rational inference.
14. 23rd June, [A team of researchers](https://nbcmontana.com/news/local/um-um-western-researchers-find-openais-gpt-4-outperforms-humans-in-creativity-tests), including professors from the University of Montana and UM Western, have found that OpenAI's GPT-4 scored in the top 1% on the Torrance Tests of Creative Thinking (TTCT), matching or outperforming humans in the creative abilities of fluency, flexibility, and originality.
15. 23rd June, [Microsoft says](https://www.independent.co.uk/tech/quantum-computing-microsoft-supercomputer-ibm-b2362174.html) it has announced plans to build a quantum supercomputer after researchers said the next-generation machines will be able to outperform standard computers within the next two years.


**18 June 2023**
1.	Andrew Ng and Geoff Hinton had an [insightful conversation](https://www.linkedin.com/posts/andrewyng_had-an-insightful-conversation-with-geoff-activity-7073688821803978752-DO9h/?trk=public_profile_share_view). They want to share (i) It's important that AI scientists reach consensus on risks-similar to climate scientists, who have rough consensus on climate change-to shape good policy.
(ii) Do AI models understand the world? We think they do. If we list out and develop a shared view on key technical questions like this, it will help move us toward consensus on risks.
2.	On 12 June 2017, Google published its outstanding paper: “[Attention is All You Need](https://arxiv.org/abs/1706.03762)” which introduced the transformer structure – an essential element widely used in nearly all large deep learning models, both in NLP and Computer Vision. The paper has been cited over 75K, and of eight authors, only one still working in Google.
3.	A new LLM evaluation [leaderboard](https://declare-lab.net/instruct-eval/) is released by researchers from UTD Singapore. The proposed model evaluated three features of LLMs: Problem-Solving, Writing, and Alignment (Harmless, Honesty and Helpfulness)
4.	On 13th June, [MetaAI announced I-JEPA](https://ai.facebook.com/blog/yann-lecun-ai-model-i-jepa/), the first AI model based on Yann LeCun’s vision for more human-like AI, which is to create machines that can learn internal models of how the world works so that they can learn much more quickly, plan how to accomplish complex tasks, and readily adapt to unfamiliar situations.
5.	On 14th June, [OpenAI released new GPT-4 and GPT-3.5 Turbo](https://twitter.com/OfficialLoganK/status/1668668826047721494) models with 1) function calling in the API (plugins); 2) 16K context 3.5 Turbo model available to everyone; 3) 75% price reduction on v2 embedding models.
6.	McKinsey released “[The economic potential of generative AI: The next productivity frontier](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier#business-value)”. 1) Generative AI could add $2.6 to $4.4 Trillion in value to the global ecomony; 2) 75% of the value falls in: Customer operations, marketing and sales, software engineering and R&D; 3) Generative AI will have impact across all industry sectors; 4) 50% today’s work activities could be automated between 2030 and 2060; 5) Generative AI is just beginning.
7.	On 14 June, The sequoia published “[The New Language Model Stack](https://www.sequoiacap.com/article/llm-stack-perspective/)” which describes how companies are bringing AI applications to life. 1) Nearly every company in the Sequoia network is building LLMs into their products; 2) The new stack centers on LLM APIs, retrieval, and orchestration, but open source usage is also growing; 3) Companies want to customize LLMs to their unique context; 4) LLMs need to become more trustworthy (output quality, data privacy, security) for full adoption
8.	On 15th June, Princeton Uni published a paper “[Infinite Photorealistic Worlds using Procedural Generation](https://arxiv.org/pdf/2306.09310.pdf)”. It’s worth noting that Infinigen is entirely procedural: every asset, from shape to texture, is generated from scratch via randomized mathematical rules, using no external source and allowing infinite variation and composition.
9.	 On 16th June, Meta published a paper “[Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale](https://research.facebook.com/publications/voicebox-text-guided-multilingual-universal-speech-generation-at-scale/)”. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are neither filtered nor enhanced. Voicebox not outperforms the SOT zero-shot TSS model, but also up to 20 times faster
10.	IBM Makes the [Best Quantum Computer Open to Public](https://analyticsindiamag.com/ibm-makes-the-best-quantum-computer-open-to-public/) - IBM in collaboration with UC Berkeley researchers announced a recent breakthrough experiment which indicates that quantum computers will soon surpass classical computers in practical tasks.


**11 June 2023**
1. OpenAI see traffic soar to Billion mark, achieved a total [847 million user access](https://www.digitalinformationworld.com/2023/06/openai-website-sees-traffic-soar-to.html) in March 2023.
2. [Video-LLaMA](https://github.com/damo-nlp-sg/video-llama), a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA showcases the ability to perceive and comprehend video content, generating meaningful responses that are grounded in the visual and auditory information presented in the videos.
3. [InstructZero](https://arxiv.org/pdf/2306.03082v1.pdf), is an efficient instruction optimization method for black-box large language models by optimizing a low-dimensional soft prompt applied to an open-source LLM to generate the instruction for the black-box LLM.
4. [Git-Theta](https://arxiv.org/pdf/2306.04529v1.pdf) is a Git extension that aims to provide similar functionality for machine learning model checkpoints by efficiently and meaningfully track a model's version history natively through Git. [Link to the project](https://github.com/r-three/git-theta)
5. A github project named [roop](https://github.com/s0md3v/roop) is recently released. It allows anyone to take a video and replace the face in it with a face of your choice. You only need one image of the desired face. No dataset, no training.
6. Facebook published a [paper](https://arxiv.org/pdf/2306.05284v1.pdf) introducing MusicGen, which can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. 
7. [SpQR](https://github.com/vahe1994/spqr)- Sparse-Quantized Representation, is a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods. Require GPU VRAM > 32GB.
8. [MAN](https://arxiv.org/pdf/2306.05399v1.pdf) - Matting Anything Model, can estimate the alpha matte of any target instance with user prompts as boxes, points, or text descriptions for interactive use by incorporating [SAM](https://segment-anything.com/). It further reaches comparable performance to the specialized matting models on multiple benchmarks, and shows superior generalization ability with fewer parameters as a unified image matting model.
9. [Video-ChatGPT](https://arxiv.org/pdf/2306.05424v1.pdf), is a multimodal model that merges a video-adapted visual encoder with a LLM. The model is capable of understanding and generating human-like conversations about videos. [Try it here](https://www.ival-mbzuai.com/video-chatgpt).
10. DeepMind publish a [paper in Nature](https://www.nature.com/articles/s41586-023-06004-9): Faster sorting algorithms discovered using deep reinforcement learning. Researchers trained a new deep reinforcement learning agent, AlphaDev, to formulate a task of finding a better sorting routine at assembly-language level as a single-player game.
11. [Magic](https://magic.dev/), an AI startup company, announced [LTM-1](https://twitter.com/magicailabs/status/1666116935904292869), a prototype of a neural network architecture designed for giant context windows, can handle prompt with 5,000,000 tokens, much larger than GPT-4's 32k tokens.
12. Huggingface released [StarCode+](https://huggingface.co/bigcode/starcoderplus) - is a fine-tuned version of [StarCoderBase](https://huggingface.co/bigcode/starcoderbase) on 600B tokens from the English web dataset [RedefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) combined with [StarCoderData](https://huggingface.co/datasets/bigcode/starcoderdata) from [The Stack (v1.2)](https://huggingface.co/datasets/bigcode/the-stack) and a Wikipedia dataset. It's trained on 512 Tesla A100 GPUs for 14 days.
13. RedPajama released [SlimPajama](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama) - the largest extensively deduplicated, multi-corpora, open-source dataset for training large language models. [Github link](https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama)


**4 June 2023**
1.	Google, Princeton, and Stanford published a [paper](https://arxiv.org/pdf/2305.17126.pdf) “Large Language Models as Tool Makers”. The paper proposed a closed-loop framework referred to as LATMs, which can create their own reusable tools for problem-solving. The project uses GPT-3.5 as tool user, and GPT-4 as tool maker to reduce inference costs.
2.	Nvidia announced [DGX GH200](https://nvidianews.nvidia.com/news/nvidia-announces-dgx-gh200-ai-supercomputer), a supercomputer that is 10 times faster than the current fastest computer in the world. The computer will be used for generative AI language applications. Watch [this](https://www.nvidia.com/en-us/events/computex/) from 60mins for about 1 min. Google, Microsoft, and Meta will be its first users.
3.	Nvidia’s [Neuralangelo project](https://blogs.nvidia.com/blog/2023/06/01/neuralangelo-ai-research-3d-reconstruction/) can now turn 2D video clips into 3D structures and scenes. It literally generates detailed replicas of buildings, sculptures, and real objects from video clips taken on a mobile or camera.
4.	[Statement on AI Risk](https://www.safe.ai/statement-on-ai-risk) has been circulated and signed by a lot. “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war”
5.	OpenAI publishes a research [paper](https://arxiv.org/abs/2305.20050), “let’s verify step by step”, by using this process supervision approach, the process-supervised model solves 78% of the problem from the MATH test set. It also aims at attack the Hallucination issues of LLMs.
6.	[GPT4Tools](https://github.com/StevenGrove/GPT4Tools) – an open-source tool based on Vicuan (LLaMA), and aims to efficiently enable LLMs to decide, control and utilizing different visual foundation models, allowing users to interact with images during a conversation.
7.	Google, OpenAI, Anthropic, etc published a [paper](https://arxiv.org/pdf/2305.15324.pdf) “Model evaluation for extreme risks”. An evaluation model is created to evaluate extreme risks by looking at dangerous capabilities and alignment as input and to ensure responsible training, responsible deployment, transparency and appropriate security.


**28 May 2023**
1.	Meta published a [paper](https://arxiv.org/abs/2305.11206): LIMA Less is more for Alignment. Use carefully cured 1000 high-quality prompts, LIMA beats Google’s Bard, and GPT-3.5, and just below GPT-4.
2.	On 22nd May, Meta released [Massively Multilingual Speech AI](https://about.fb.com/news/2023/05/ai-massively-multilingual-speech-technology/), a single multilingual speech recognition model which can process more than 1000 languages, compared with the previous 100 languages only.
3.	0n 23rd May, Adobe Photoshop adds [Native AI](https://venturebeat.com/ai/adobe-integrates-generative-ai-directly-into-photoshop-with-new-firefly-capabilities/), unlike other open source software, it’s a commercially safe model, using high-quality images, and without copyright issues.
4.	On 23rd May, Microsoft released [Windows Copilot](https://blogs.windows.com/windowsdeveloper/2023/05/23/bringing-the-power-of-ai-to-windows-11-unlocking-a-new-era-of-productivity-for-customers-and-developers-with-windows-copilot-and-dev-home/) to Windows 11, it also released a list of other plugins that will greatly improve the productivity of developers and Windows users. Also, Bing is powered by GPT-4 now, but with only 5 QA each month only. Microsoft also announced [Data Fabric](https://learn.microsoft.com/en-us/fabric/get-started/microsoft-fabric-overview), an all-in-one analytics solution that covers everything from data movement to data science, Real-Time Analytics, and business intelligence + services, including data lake, data engineering, and data integration, all in one place.
5.	An interesting [feature of Microsoft 365 Copilot](https://www.youtube.com/watch?v=qMGLU-chnLk) for drafting legal contracts during Microsoft Build.
6.	On 23rd May, Google Bard  Image generation go online. Eg, ask bard “show me some fashion hair styles in Australia”. It will show you some hairstyles.
7.	Two more models based on LLAMA: [airoboros](https://www.reddit.com/r/LocalLLaMA/comments/13o6kp8/airoboros13b_98_against_gpt35/) and [Guanaco](https://www.reddit.com/r/LocalLLaMA/comments/13rthln/guanaco_7b_13b_33b_and_65b_models_by_tim_dettmers/). Both are close to GPT-3.5 performance, especially the latter, which is based on [QLora](https://arxiv.org/pdf/2305.14314.pdf).
8.	JPMorgan is developing its [IndexGPT](https://www.cnbc.com/2023/05/25/jpmorgan-develops-ai-investment-advisor.html) to select investments for customers.
9.	[Spellbook](https://www.spellbook.legal/) uses GPT-4 and other large language models to review and suggest terms for users’ contracts, right in Microsoft Word.
10.	On 25th May, [TII](https://www.tii.ae/news/uaes-technology-innovation-institute-launches-open-source-falcon-40b-large-language-model) released Falcon 40B and temporarily ranked #1 on [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). TTI announced it’s an open-source model, but maybe not.
11.	On 27th May, [a lawyer used ChatGPT](https://www.theverge.com/2023/5/27/23739913/chatgpt-ai-lawsuit-avianca-airlines-chatbot-research) to prepare a case in USA and now has to answer for its bogus citations. At least six cases were just made up by ChatGPT. [The fake cases source? ChatGPT](https://edition.cnn.com/2023/05/27/business/chat-gpt-avianca-mata-lawyers/index.html).
12.	Nvidia released GPU-4 Powered [Voyager](https://arxiv.org/abs/2305.16291), a lifeling learning agent in Minecraft that continuously explores the worlds, acquires diverse skills, and makes novel discuveries without human intervention.


**22 May 2023**
1.	On 13 May, Sam Altman announced on his [Twitter](https://twitter.com/sama/status/1657143368198279168) that “all ChatGPT Plus users getting browsing and plugins over the next week”. The time AI can use tools is coming. A new web-browsing feature is set to allow ChatGPT-Plus users to access real-time information. They will also get access to more than 70 plug-ins on sites including Expedia and Instacart (https://www.businessinsider.com/chatgpt-openai-web-browsing-plug-change-how-we-use-internet-2023-5).
2.	LangGPT — [Empowering everyone to become a prompt expert!](https://community.openai.com/t/langgpt-empowering-everyone-to-become-a-prompt-expert/207880) LangGPT addresses how to write high-quality prompts, which is becoming more akin to programming in the AI ear. The project link is [here](https://github.com/yzfly/LangGPT). 
3.	On 17th May, [Google announced](https://blog.google/technology/developers/google-colab-ai-coding-features/) AI-powered features will add to Colab, and free of charge. The features include code completions, natural language to code generation and even a code-assisting chatbot.
4.	On 18th May, OpenAI [announced ChatGPT app for iOS](https://openai.com/blog/introducing-the-chatgpt-app-for-ios). The ChatGPT app is free to use and syncs users history across devices. It also integrates [Whisper](https://openai.com/research/whisper), an open-source speech-recognition system, enabling voice input. [ChatGPT Plus subscribers](https://openai.com/blog/chatgpt-plus) get exclusive access to [GPT-4’s capabilities](https://openai.com/product/gpt-4), early access to features and faster response times.
5.	OpenAI CEO [calls on government to regulate AI](https://www.msn.com/en-us/news/technology/openai-ceo-calls-on-government-to-regulate-ai/ar-AA1bgSwd) - OpenAI CEO Sam Altman testified before the Senate Judiciary Committee on Tuesday, calling on Congress to pass legislation to regulate AI.
6.	[Drag your GAN](https://vcai.mpi-inf.mpg.de/projects/DragGAN/) – a technology allow one to "drag" any points of the image to precisely reach target points in a user-interactive manner.


**15 May 2023**
1.	[Google.io conference](https://developers.googleblog.com/2023/05/io23-developer-keynote-recap.html) – 1) Introduced PaLM v2 which support Google’s Bard, improved programming coding and dialog quality of Bard, support over 100 language translation, image <-> text generation/analysis, and more; 2) Google search supercharged with AGI, multi-round QA allows following-up question; 3) improved Gmail and Google photos features and others
2.	Anthropic’s [Claude](https://twitter.com/AnthropicAI/status/1656700154190389248?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet) model can process 100K tokens now, more than 3X bigger than GPT-4’s 32K tokens.
3.	Microsoft [announced](https://www.microsoft.com/en-us/microsoft-365/blog/2023/05/09/introducing-the-microsoft-365-copilot-early-access-program-and-new-capabilities-in-copilot/) new capabilities in Copilot, including semantic index, copilot in whiteboard makes Teams meetings and brainstorms more creative and effective; integrate DALL.E  into PowerPoint to automatically generate ppt slides.
4.	OpenAI announced [Shape-E](https://github.com/openai/shap-e), a conditional generative model for 3D assets. Type in text prompts into Shape-E, and the model will produce 3D objects that create better more detailed, and accurate objects.
5.	HuggingFace announced [Transformers Agent](https://huggingface.co/docs/transformers/transformers_agents) – the agent provides a natural language API, which can interpret natural language and use a list of curated tools. The agent dramatically simplifies the process of a pre-trained LLM to call tools. A similar function with [LangChain](https://python.langchain.com/en/latest/index.html) – a framework for developing applications powered by LLM.
6.	Meta announced [ImageBind](https://twitter.com/MetaAI/status/1655989274620358656) – an AI model capable of binding data from six modalities at once, including the 3D shape of an image. [ImageBind](https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/) outperforms prior individually trained models and helps AI by enabling machines to better analyze many different forms of information together.
7.	OpenAI applied GPT-4 to automatically propose [explanations](https://twitter.com/OpenAI/status/1655982364273831936) for GPT-2’S 300K neurons, and found neurons responding to concepts like similes, “things done correctly”, or expressions of certainty. GitHub link.
8.	IBM announced [Dromedary](https://github.com/IBM/Dromedary), another LLM which uses principle-driven, self-alignment to minimize human supervision, and surpasses the performance of ChatGPT and Alpaca. 


**8 May 2023**
1.	A.I. Is Getting Better at Mind-Reading - [In a recent experiment](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41593-023-01304-9&data=05%7C01%7Cd.zhu%40curtin.edu.au%7Cd3b1a33582834f569baf08db4fb7c304%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638191422898365032%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=ffzeFZDazt0JJRlWoFLNQ1hrIm4IkqODfx5sEXwta20%3D&reserved=0), researchers used large language models to translate brain activity into words. Accuracy can now reach about 83% based the authors’ experiments.
2.	Thursday, Microsoft announced [AI powered Bing plugins](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.bing.com%2Fnew%3Fform%3DMY028Z%26OCID%3DMY028Z&data=05%7C01%7Cd.zhu%40curtin.edu.au%7Cd3b1a33582834f569baf08db4fb7c304%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638191422898365032%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=VjfZIdudGT%2BTwTKZwb0KnetYJK6HWEwdkjiabScyvXc%3D&reserved=0), enable Bing to search/generate multimedia content, and an Action feature coming soon.
3.	Google: ["We have no moat, and neither does OpenAI"](https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.semianalysis.com%2Fp%2Fgoogle-we-have-no-moat-and-neither&data=05%7C01%7Cd.zhu%40curtin.edu.au%7Cd3b1a33582834f569baf08db4fb7c304%7C5a740cd757684d09ae13f706b09fa22c%7C0%7C0%7C638191422898365032%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=%2Fw27hisANe4Xi8QJFxVj9dczbkX8VD85S1zUUiUMqjk%3D&reserved=0). It’s reported that Google is considering to follow the trend of open source of LLM, such as LLAMA. Many startups and company released their AI Chat bot simply based on LLAMA.
