# Weekly AI-News
Welcome to the Weekly-AI-news!
I mainly focuses more on Generative AI, LLMs and the related, so may miss import AI news in other areas. <br><br>

***Aug 11 2024***

1. ***Meta's Self-Taught Evaluators: <br>
   Meta introduced a novel approach to model evaluation called "Self-Taught Evaluators," which uses synthetic training data to improve models without human annotations. This method iteratively generates and judges model outputs, significantly enhancing a large language model (LLM) without relying on costly and quickly outdated human preference data. The results show that the Self-Taught Evaluator can outperform commonly used judges and match the performance of top models trained with labeled data.*** <br><br>
   Aug 10, Meta published a paper “Self-Taught Evaluators”. Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. This paper presents an approach that aims to improve evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, the iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, the Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples. <br><br>

3. ***Generative AI Business Use Cases:  <br>An article from cio.com highlights four transformative business applications of generative AI: virtual assistants, intelligent search, content summarization, and document processing. These AI-powered tools enhance productivity, streamline data management, and improve customer interactions, giving businesses a competitive edge by reducing costs and optimizing operations.***
   Aug 9, cio.com published an article “Four generative AI use cases for businesses”. The article outlines four key use cases where generative AI is transforming business operations: 1) Virtual Assistants: Generative AI powers tools like chatbots and virtual assistants, enhancing both employee productivity and customer experiences by automating tasks and providing personalized interactions. 2) Intelligent Search: Leveraging large language models (LLMs), generative AI enables enterprises to process and search proprietary data more effectively, offering precise and relevant information tailored to business-specific needs. 3) Content Summarization: AI models can quickly summarize documents, meetings, and videos, saving time and improving decision-making in sectors like healthcare and finance. 4) Document Processing: Generative AI streamlines document management by automating tasks such as translation, proofreading, and data extraction, particularly benefiting industries that handle large volumes of documents, like legal and financial sectors. Overall, generative AI boosts productivity, reduces costs, and provides businesses with a competitive edge by enabling more efficient data processing and customer interaction. <br><br>

5. ***OpenAI Leadership Changes:  <br>Futurism reported significant leadership changes at OpenAI, with key figures like co-founder John Schulman and VP Peter Deng leaving the company. This has sparked speculation about OpenAI's ability to achieve its goal of safe artificial general intelligence (AGI). Critics suggest that these departures may indicate challenges in fulfilling its vision, with some even predicting a potential "Generative AI bubble" burst.***
   Aug 8, Futurism published an article “Why Are OpenAI's Most Prominent Employees Leaving?”. Cofounder John Schulman announced he'd left the company this week to join rival AI company Anthropic, while president Greg Brockman is taking a leave of absence. VP of consumer product Peter Deng has also quit, indicating major shifts in OpenAI's upper ranks. The Sam Altman-led company has made its core mission to realize safe artificial general intelligence, the still entirely hypothetical point at which point AI can keep up with humans across a wide variety of intellectual tasks. But how far the company is from doing just that remains a heavily debated subject, with critics pointing out that OpenAI is shoring up billions of dollars in investment by making empty promises — an "AI bubble" that may be set to burst. Could the latest departures show that the venture is struggling to fulfill its long-term vision, let alone turn a profit from generative AI? Critics say OpenAI's brain drain problem could be the canary in the coal mine. "Calling it," leading AI skeptic Gary Marcus tweeted. "August 2024 will be known as the month in which the Generative AI bubble burst." Other critics questioned OpenAI's repeated claims that AGI is right around the corner. "If OpenAI is right on the verge of AGI, why do prominent people keep leaving?" AI developer Benjamin de Kraker tweeted. <br><br>
 
7. ***LLM-DetectAIve for Machine-Generated Text Detection:  <br>A new tool called "LLM-DetectAIve" was introduced for detecting machine-generated texts (MGTs). Unlike previous binary classifiers, it categorizes texts into four distinct types, offering fine-grained detection. This tool is particularly valuable in academic and educational settings, where identifying the degree of LLM involvement in text creation is crucial.***
   Aug 8, MBZUAI, Uni of Florida, NYU, et al. published a paper “LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection”. The widespread accessibility of large language models (LLMs) to the general public has significantly amplified the dissemination of machine-generated texts (MGTs). Advancements in prompt manipulation have exacerbated the difficulty in discerning the origin of a text (human-authored vs machinegenerated). This raises concerns regarding the potential misuse of MGTs, particularly within educational and academic domains. This paper presents LLM-DetectAIve -- a system designed for fine-grained MGT detection. It is able to classify texts into four categories: human-written, machine-generated, machine-written machine-humanized, and human-written machine-polished. Contrary to previous MGT detectors that perform binary classification, introducing two additional categories in LLM-DetectiAIve offers insights into the varying degrees of LLM intervention during the text creation. This might be useful in some domains like education, where any LLM intervention is usually prohibited. Experiments show that LLM-DetectAIve can effectively identify the authorship of textual content, proving its usefulness in enhancing integrity in education, academia, and other domains. LLM-DetectAIve is publicly accessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video describing our system is available at https://youtu.be/E8eT_bE7k8c. <br><br>

9. ***Optical Neural Networks and FFM Learning:  <br>A paper in Nature presents a new method called fully forward mode (FFM) learning for training optical neural networks. FFM learning allows most machine learning operations to be conducted efficiently on physical systems rather than digital simulations, leading to faster learning processes and significant advancements in deep learning, ultrasensitive perception, and topological photonics.***
    Aug 7, Nature published a paper “Fully forward mode training for optical neural networks”. Optical computing promises to improve the speed and energy efficiency of machine learning applications. However, current approaches to efficiently train these models are limited by in silico emulation on digital computers. This research develops a method called fully forward mode (FFM) learning, which implements the compute-intensive training process on the physical system. The majority of the machine learning operations are thus efficiently conducted in parallel on site, alleviating numerical modelling constraints. In free-space and integrated photonics, the researchers experimentally demonstrate optical systems with state-of-the-art performances for a given network size. FFM learning shows training the deepest optical neural networks with millions of parameters achieves accuracy equivalent to the ideal model. It supports all-optical focusing through scattering media with a resolution of the diffraction limit; it can also image in parallel the objects hidden outside the direct line of sight at over a kilohertz frame rate and can conduct all-optical processing with light intensity as weak as subphoton per pixel (5.40 × 1018- operations-per-second-per-watt energy efficiency) at room temperature. Furthermore, the study proves that FFM learning can automatically search non-Hermitian exceptional points without an analytical model. FFM learning not only facilitates orders-of-magnitude-faster learning processes, but can also advance applied and theoretical fields such as deep neural networks, ultrasensitive perception and topological photonics. <br><br>

11. ***Human-Level Robot Table Tennis:  <br>Google researchers developed a robot that achieved amateur human-level performance in competitive table tennis. The robot uses a hierarchical policy architecture and zero-shot sim-to-real techniques to adapt to new opponents in real time. It performed well against beginners and intermediate players, showcasing significant progress toward human-level robotic performance in physical tasks.***
    Aug 7, Google published a paper “Achieving Human Level Competitive Robot Table Tennis”. Achieving human-level speed and performance on real world tasks is a north star for the robotics research community. This work takes a step towards that goal and presents the first learned robot agent that reaches amateur human-level performance in competitive table tennis. Table tennis is a physically demanding sport which requires human players to undergo years of training to achieve an advanced level of proficiency. The paper contributes (1) a hierarchical and modular policy architecture consisting of (i) low level controllers with their detailed skill descriptors which model the agent's capabilities and help to bridge the sim-to-real gap and (ii) a high level controller that chooses the low level skills, (2) techniques for enabling zero-shot sim-to-real including an iterative approach to defining the task distribution that is grounded in the real-world and defines an automatic curriculum, and (3) real time adaptation to unseen opponents. Policy performance was assessed through 29 robot vs. human matches of which the robot won 45% (13/29). All humans were unseen players and their skill level varied from beginner to tournament level. Whilst the robot lost all matches vs. the most advanced players it won 100% matches vs. beginners and 55% matches vs. intermediate players, demonstrating solidly amateur human-level performance. Videos of the matches can be viewed at https://sites.google.com/view/competitive-robot-table-tennis <br><br>

13. ***WalledEval Safety Evaluation Toolkit:  <br>Walled AI Lab introduced "WalledEval," a comprehensive toolkit for evaluating the safety of large language models (LLMs). It includes over 35 safety benchmarks and features like WalledGuard for content moderation and SGXSTest for exaggerated safety in cultural contexts. The toolkit is designed to test and improve LLM safety across various models and scenarios.***
    Aug 7, Walled AI Lab published a paper “WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models”. WalledEval is a comprehensive AI safety testing toolkit designed to evaluate large language models (LLMs). It accommodates a diverse range of models, including both open-weight and API-based ones, and features over 35 safety benchmarks covering areas such as multilingual safety, exaggerated safety, and prompt injections. The framework supports both LLM and judge benchmarking, and incorporates custom mutators to test safety against various text-style mutations such as future tense and paraphrasing. Additionally, WalledEval introduces WalledGuard, a new, small and performant content moderation tool, and SGXSTest, a benchmark for assessing exaggerated safety in cultural contexts. We make WalledEval publicly available at https://github.com/walledai/walledevalA. <br><br>

15. ***CoverBench for Complex Claim Verification:  <br>Google and Tel Aviv University released "CoverBench," a benchmark for verifying the correctness of language models' outputs in complex reasoning tasks. It provides a diversified evaluation for complex claim verification across various domains, ensuring high data quality and challenging baseline results. The benchmark aims to advance the accuracy and reliability of language models in handling complex queries.***
    Aug 6, Google and Tel Aviv Uni published a paper “CoverBench: A Challenging Benchmark for Complex Claim Verification”. There is a growing line of research on verifying the correctness of language models' outputs. At the same time, LMs are being used to tackle complex queries that require reasoning. The paper introduces CoverBench, a challenging benchmark focused on verifying LM outputs in complex reasoning settings. Datasets that can be used for this purpose are often designed for other complex reasoning tasks (e.g., QA) targeting specific use-cases (e.g., financial tables), requiring transformations, negative sampling and selection of hard examples to collect such a benchmark. CoverBench provides a diversified evaluation for complex claim verification in a variety of domains, types of reasoning, relatively long inputs, and a variety of standardizations, such as multiple representations for tables where available, and a consistent schema. The authors manually vet the data for quality to ensure low levels of label noise. Finally, the paper reports a variety of competitive baseline results to show CoverBench is challenging and has very significant headroom. The data is available at https://huggingface.co/datasets/google/coverbench. <br><br>

17. ***Scaling LLM Test-Time Compute:  <br>A joint study by UC Berkeley and Google explored the effectiveness of scaling test-time computation for large language models (LLMs). The research suggests that optimizing test-time compute allocation can significantly enhance LLM performance, potentially outperforming models with more parameters. This approach may lead to more efficient and self-improving AI agents.***
    Aug 6, UC Berkeley and Google published a paper “Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters”. Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. The paper studies the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. This study analyzes two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. The authors find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a "compute-optimal" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy will improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, it is found that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model. <br><br>

19. ***Google's Antitrust Ruling:  <br>A US judge ruled that Google illegally maintained its monopoly on online search and advertising by paying billions to be the default search engine on smartphones and browsers. This landmark decision could have significant implications for how tech giants operate, as antitrust authorities aim to strengthen competition. The penalties Google may face will be decided in a future hearing.***
    Aug 6, according to BBC, A US judge has ruled Google acted illegally to crush its competition and maintain a monopoly on online search and related advertising. The landmark decision on Monday is a major blow to Alphabet, Google's parent company, and could reshape how technology giants do business. Google was sued by the US Department of Justice in 2020 over its control of about 90% of the online search market. It is one of several lawsuits that have been filed against the big tech companies as US antitrust authorities attempt to strengthen competition in the industry. This case has at times been described as posing an existential threat to Google and its owner given its dominance of the search and online advertising business. It is unclear yet what penalties Google and Alphabet will face as a result of the decision. The fines or other remedies will be decided in a future hearing. In his decision, US District Judge Amit Mehta said Google had paid billions to ensure it is the default search engine on smartphones and browsers. “Google is a monopolist, and it has acted as one to maintain its monopoly,” Judge Mehta wrote in his 277-page opinion. Alphabet said it plans to appeal against the ruling. US Attorney General Merrick Garland, the country's top prosecutor, hailed the ruling as a "historic win for the American people". Another case against the technology company over its advertising technology is scheduled to go to trial in September. In Europe, meanwhile, Google has been fined billions in monopoly cases. <br><br>

21. ***Privacy in AI Assistants:  <br>Google proposed operationalizing contextual integrity (CI) in AI assistants to address privacy concerns. By aligning information-sharing actions with privacy expectations, the framework aims to prevent AI assistants from inappropriately sharing user information. The study's evaluation shows that CI-based reasoning improves privacy compliance in AI assistants.***
    Aug 5, Google published a paper “Operationalizing Contextual Integrity in Privacy-Conscious Assistants”. Advanced AI assistants combine frontier LLMs and tool access to autonomously perform complex tasks on behalf of users. While the helpfulness of such assistants can increase dramatically with access to user information including emails and documents, this raises privacy concerns about assistants sharing inappropriate information with third parties without user supervision. To steer information-sharing assistants to behave in accordance with privacy expectations, the study proposes to operationalize contextual integrity (CI), a framework that equates privacy with the appropriate flow of information in a given context. In particular, the authors design and evaluate a number of strategies to steer assistants' information-sharing actions to be CI compliant. The evaluation is based on a novel form filling benchmark composed of synthetic data and human annotations, and it reveals that prompting frontier LLMs to perform CI-based reasoning yields strong results. <br><br>

23. ***RAG Foundry Framework:  <br>Intel Labs introduced "RAG Foundry," an open-source framework for enhancing large language models with Retrieval-Augmented Generation (RAG). The framework integrates data creation, training, inference, and evaluation, allowing for rapid prototyping and experimentation with RAG techniques. RAG Foundry has shown consistent improvements in knowledge-intensive tasks.***
    Aug 5, Inter Labs published a paper “RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation”. Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. The paper introduces RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. The paper demonstrates the framework's effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry.  <br><br>

25. ***Jailbreaking LLMs:  <br>NYU and Meta published a study analyzing the jailbreaking of large language models (LLMs) from a statistical perspective. The research introduces E-RLHF, a modification to the existing RLHF objective, which increases the likelihood of safe responses. E-RLHF outperforms RLHF in preventing harmful behavior while maintaining model performance.***
    Aug 2, NYU and Meta published a paper “Mission Impossible: A Statistical Perspective on Jailbreaking LLMs”. Large language models (LLMs) are trained on a deluge of text data with limited quality control. As a result, LLMs can exhibit unintended or even harmful behaviours, such as leaking information, fake news or hate speech. Countermeasures, commonly referred to as preference alignment, include fine-tuning the pretrained LLMs with carefully crafted text examples of desired behaviour. Even then, empirical evidence shows preference aligned LLMs can be enticed to harmful behaviour. This so called jailbreaking of LLMs is typically achieved by adversarially modifying the input prompt to the LLM. The paper provides theoretical insights into the phenomenon of preference alignment and jailbreaking from a statistical perspective. Under the framework, the authors first show that pretrained LLMs will mimic harmful behaviour if present in the training corpus. Under that same framework, the paper then introduces a statistical notion of alignment, and lower-bound the jailbreaking probability, showing that it is unpreventable under reasonable assumptions. Based on the insights, the authors propose an alteration to the currently prevalent alignment strategy RLHF. Specifically, the research introduces a simple modification to the RLHF objective, called E-RLHF, that aims to increase the likelihood of safe responses. E-RLHF brings no additional training cost, and is compatible with other methods. Empirically, the study demonstrates that E-RLHF outperforms RLHF on all alignment problems put forward by the AdvBench and HarmBench project without sacrificing model performance as measured by the MT-Bench project. <br><br>

27. ***RELBENCH for Relational Databases:  <br>Stanford University and Kumo.AI introduced "RELBENCH," a benchmark for predictive tasks over relational databases using graph neural networks. The study demonstrates that relational deep learning (RDL) models outperform traditional manual feature engineering, reducing human effort and improving predictive accuracy.***
    Jul 29, Stanford Uni, Kumo.AI etc published a paper “RELBENCH: A Benchmark for Deep Learning on Relational Databases”.  The paper presents RELBENCH, a public benchmark for solving predictive tasks over relational databases with graph neural networks. RELBENCH provides databases and tasks spanning diverse domains and scales, and is intended to be a foundational infrastructure for future research. The work uses RELBENCH to conduct the first comprehensive study of Relational Deep Learning (RDL) (Fey et al., 2024), which combines graph neural network predictive models with (deep) tabular models that extract initial entity-level representations from raw tables. End-to-end learned RDL models fully exploit the predictive signal encoded in primary-foreign key links, marking a significant shift away from the dominant paradigm of manual feature engineering combined with tabular models. To thoroughly evaluate RDL against this prior gold-standard, the authors conduct an in-depth user study where an experienced data scientist manually engineers features for each task. In this study, RDL learns better models whilst reducing human work needed by more than an order of magnitude. This demonstrates the power of deep learning for solving predictive tasks over relational databases, opening up many new research opportunities enabled by RelBench. <br><br>

29. ***Chain of Code for Reasoning:  <br>A paper from Google, Stanford, and UC Berkeley proposed "Chain of Code" (CoC), an extension to improve language model reasoning by integrating code emulation. CoC outperforms existing reasoning methods like Chain of Thought, broadening the scope of questions LMs can answer by simulating code execution.***
    Jul 29, Google, Stanford Uni, and UC Berkley published a paper on ICML2024 “Chain of Code: Reasoning with a Language Model-Augmented Code Emulator”. Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter – it’s hypothesized that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for semantic ones (and in particular, those that are a mix of both). For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for "detect_sarcasm(string)" that can be executed by the interpreter (handling the edge cases would be insurmountable). However, LMs may still produce a valid solution if they not only write code, but also selectively "emulate" the interpreter by generating the expected output of "detect_sarcasm(string)". This paper proposes Chain of Code (CoC), a simple yet surprisingly effective extension that improves LM code-driven reasoning. The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode that the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as an "LMulator"). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over Chain of Thought. In a nutshell, CoC broadens the scope of reasoning questions that LMs can answer by "thinking in code". <br><br>

30. ***AdaCoder for Visual Question Answering:  <br>Researchers from Tokyo Institute of Technology and OMRON SINIC X Corp developed "AdaCoder," a framework for adaptive prompt compression in visual programmatic models (VPMs). AdaCoder reduces input token length while maintaining or improving performance in visual question answering tasks, demonstrating its effectiveness in optimizing VPMs.***
    Jul 28, Tokyo Inst. Of Tech, OMRON SINIC X Corp et al published a paper “AdaCoder: Adaptive Prompt Compression for Programmatic Visual Question Answering”. Visual question answering aims to provide responses to natural language questions given visual input. Recently, visual programmatic models (VPMs), which generate executable programs to answer questions through large language models (LLMs), have attracted research interest. However, they often require long input prompts to provide the LLM with sufficient API usage details to generate relevant code. To address this limitation, the paper proposes AdaCoder, an adaptive prompt compression framework for VPMs. AdaCoder operates in two phases: a compression phase and an inference phase. In the compression phase, given a preprompt that describes all API definitions in the Python language with example snippets of code, a set of compressed preprompts is generated, each depending on a specific question type. In the inference phase, given an input question, AdaCoder predicts the question type and chooses the appropriate corresponding compressed preprompt to generate code to answer the question. Notably, AdaCoder employs a single frozen LLM and pre-defined prompts, negating the necessity of additional training and maintaining adaptability across different powerful black-box LLMs such as GPT and Claude. In experiments, the authors apply AdaCoder to ViperGPT and demonstrate that it reduces token length by 71.1%, while maintaining or even improving the performance of visual question answering. <br><br>

32. ***Modular RAG Framework:  <br>Tonji University and Fudan University proposed a modular framework for Retrieval-Augmented Generation (RAG) systems, enabling highly reconfigurable and specialized operators. This framework addresses the limitations of existing RAG paradigms, facilitating the development of more efficient and adaptable RAG systems.***
    Jul 26, Tonji Uni and Fudan Uni published a paper “Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks”. Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of "retrieve-then-generate". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies. <br><br>

34. ***Limitations of Instruction Tuning:  <br>the ICML2024 paper finds that while IT is widely used to convert pre-trained LLMs into conversational agents, it doesn't improve knowledge or skills and may even degrade them. Key issues include a decline in response quality, increased hallucination, and the ineffectiveness of popular IT improvement methods. The authors emphasize that responses based on pre-trained knowledge outperform those generated from IT and hope these insights guide future research.***
    Jul 14, Uni of Maryland, Adobe, and Nvidia published a paper on ICML2024 “A Closer Look at the Limitations of Instruction Tuning”. Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, the authors reveal various limitations of IT. In particular, the research shows that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating responses. (4) Popular methods to improve IT do not lead to performance improvements over a simple LoRA fine-tuned model. The findings reveal that responses generated solely from pre-trained knowledge consistently outperform responses by models that learn any form of new knowledge from IT on open-source datasets. The authors hope the insights and challenges revealed in this paper inspire future work in related directions.
 <br><br><br>

***Aug 4 2024***

1. ***Lean AI and Small Language Models: <br>
   The article discusses the shift towards lean AI, which aims to optimize efficiency and minimize resource consumption. This shift is driven by the high costs and resource demands of large language models (LLMs). Small language models (SLMs) are becoming more popular due to their lower operational costs, faster deployment cycles, and specialized applications. Open-source initiatives are making advanced AI more accessible and affordable for organizations.***
   
   Aug 2, [InfoWorld](https://www.infoworld.com/article/3480593/small-language-models-and-open-source-are-transforming-ai.html) published an article Small language models (SLM) and open source are transforming AI. The shift towards lean AI emphasizes optimizing efficiency and minimizing resource consumption, addressing the high costs and resource demands of large language models (LLMs). Enterprises are increasingly adopting SLMs for their lower operational costs, faster deployment cycles, and ability to deliver specialized applications. Open-source initiatives and tools are democratizing AI capabilities, enabling more organizations to incorporate advanced AI without relying on expensive proprietary solutions. <br><br>

3. ***Meta's SAM 2 for Visual Segmentation: <br>
   Meta introduced the Segment Anything Model 2 (SAM 2) for visual segmentation in images and videos. SAM 2 uses a transformer architecture with streaming memory for real-time video processing and has the largest video segmentation dataset. It provides better accuracy with fewer interactions and is significantly faster and more accurate than its predecessor, SAM. The model, dataset, and an interactive demo are being released.***
   
   Aug 1, Meta published a [paper](https://arxiv.org/pdf/2408.00714) “SAM 2: Segment Anything in Images and Videos”. The research presents Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. The  authors build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. The model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on the data provides strong performance across a wide range of tasks. In video segmentation, it is observed better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, the model is more accurate and 6x faster than the Segment Anything Model (SAM). The authors believe that the data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. The authors are releasing a version of [the model](https://github.com/facebookresearch/segment-anything-2), the dataset and an interactive demo. <br><br>

5. ***Microsoft's OmniParser for GUI Agents: <br>
   Microsoft presented OmniParser, a method for parsing user interface screenshots into structured elements. OmniParser enhances GPT-4V's ability to generate actions accurately grounded in the corresponding regions of the interface. By using curated datasets for icon detection and description, OmniParser significantly improves performance on benchmarks and outperforms existing models that require additional information.***
   Aug 1, Microsoft published a [paper](https://arxiv.org/pdf/2408.00203) “OmniParser for Pure Vision Based GUI Agent”. The recent success of large vision language models shows great potential in driving the agent system operating on user interfaces. However, the power multimodal models like GPT-4V as a general agent on multiple operating systems across different applications is largely underestimated due to the lack of a robust screen parsing technique capable of: 1) reliably identifying interactable icons within the user interface, and 2) understanding the semantics of various elements in a screenshot and accurately associate the intended action with the corresponding region on the screen. To fill these gaps, the paper introduces OmniParser, a comprehensive method for parsing user interface screenshots into structured elements, which significantly enhances the ability of GPT-4V to generate actions that can be accurately grounded in the corresponding regions of the interface. The authors first curated an interactable icon detection dataset using popular webpages and an icon description dataset. These datasets were utilized to fine-tune specialized models: a detection model to parse interactable regions on the screen and a caption model to extract the functional semantics of the detected elements. OmniParser significantly improves GPT-4V's performance on ScreenSpot benchmark. And on Mind2Web and AITW benchmark, OmniParser with screenshot only input outperforms the GPT-4V baselines requiring additional information outside of screenshot. <br><br>

8. ***Scaling Inference Compute with Repeated Sampling: <br>
   Researchers from Stanford, Oxford, and Google explored scaling inference compute by increasing the number of generated samples. They found that coverage, or the fraction of problems solved, scales with the number of samples. Repeated sampling significantly improves performance in domains with verifiable answers, and it is cost-effective. However, identifying correct samples in domains without automatic verifiers remains a challenge.***
   
   Jul 31, Stanford Uni, Uni of Oxford and Google published a [paper](https://arxiv.org/pdf/2407.21787) “Large Language Monkeys: Scaling Inference Compute with Repeated Sampling”. Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. This paper explores inference compute as another axis for scaling by increasing the number of generated samples. Across multiple tasks and models, the authors observe that coverage - the fraction of problems solved by any attempt - scales with the number of samples over four orders of magnitude. In domains like coding and formal proofs, where all answers can be automatically verified, these increases in coverage directly translate into improved performance. When applying repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-attempt state-of-the-art of 43% which uses more capable frontier models. Moreover, using current API pricing, amplifying the cheaper DeepSeek model with five samples is more cost-effective and solves more issues than paying a premium for one sample from GPT-4o or Claude 3.5 Sonnet. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. Finally, the work finds that identifying correct samples out of many generations remains an important direction for future research in domains without automatic verifiers. When solving math word problems from GSM8K and MATH, coverage with Llama-3 models grows to over 95% with 10,000 samples. However, common methods to pick correct solutions from a sample collection, such as majority voting or reward models, plateau beyond several hundred samples and fail to fully scale with the sample budget. <br><br>

9. ***Australia's Privacy Concerns with Social Media Platform X: <br>
    Australia's privacy watchdog is concerned that social media platform X (formerly Twitter) may be breaching privacy laws by automatically opting users into having their posts used to train AI systems. Platforms are required to ensure default settings enable user control and seek consent for data use. The watchdog is investigating practices across the industry as other major platforms also harvest user data to train AI.***
   
    Jul 31, according to [abc.new.au](https://www.abc.net.au/news/science/2024-07-31/elon-musk-x-breach-privacy-law-data-harvest-grok-ai/104054400), Australia's privacy watchdog says social media platform X (formerly Twitter) may be in breach of Australian privacy law after it emerged users were automatically opted in to having their posts used to build artificial intelligence (AI) systems. The Office of the Australian Information Commissioner stopped short of saying it would launch an inquiry into the platform's data collection, similar to the one it is currently undertaking in relation to TikTok. On Friday, an X user pointed out the X app privacy settings includes a pre-ticked box that permits X to use the account holder's posts to train the Grok AI chatbot built by Elon Musk's company xAI. The default setting states that you "allow your posts as well as your interactions, inputs and results with Grok to be used for training and fine-tuning". Under Australian law, platforms are required to ensure default settings enable user control, and to either seek an individuals' consent for how the platform will use the data, or be satisfied the user would reasonably expect the organisation to use their data for this purpose. In recent months, it also emerged other platforms, such as Meta and Slack, were harvesting user data to train AI as part of a global race to build bigger and better large language models (LLMs). Last month, it was revealed xAI was trying to build the world's largest supercomputer in the US city of Memphis to fuel its AI ambitions. The Commissioner's Office says it's "looking at such practices across the industry" as other major platforms, also competing to build their own AIs, harvest user data. <br><br>

11. ***Safetywashing in AI Safety Benchmarks: <br>
    A study by various universities analyzed AI safety benchmarks and found many are highly correlated with general capabilities, leading to "safetywashing." The study calls for more meaningful safety metrics that are empirically separable from generic capabilities. The authors propose a rigorous framework for AI safety research to advance the science of safety evaluations and clarify measurable progress.***
    
    Jul 31, Center of AI Safety, Uni of Penn., UC Berkeley, Stanford Uni, Yale Uni and Keio Uni published a [paper](https://arxiv.org/pdf/2407.21792) “Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?”. As artificial intelligence systems grow more powerful, there has been increasing interest in "AI safety" research to address emerging and future risks. However, the field of AI safety remains poorly defined and inconsistently measured, leading to confusion about how researchers can contribute. This lack of clarity is compounded by the unclear relationship between AI safety benchmarks and upstream general capabilities (e.g., general knowledge and reasoning). To address these issues, the study conducts a comprehensive meta-analysis of AI safety benchmarks, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety. The findings reveal that many safety benchmarks highly correlate with upstream model capabilities, potentially enabling "safetywashing" -- where capability improvements are misrepresented as safety advancements. Based on these findings, the authors propose an empirical foundation for developing more meaningful safety metrics and define AI safety in a machine learning research context as a set of clearly delineated research goals that are empirically separable from generic capabilities advancements. In doing so, the authors aim to provide a more rigorous framework for AI safety research, advancing the science of safety evaluations and clarifying the path towards measurable progress. <br><br>

13. ***Google's Gemma 2 AI Models: <br>
    Google introduced the Gemma 2 family, including Gemma 2 2B, ShieldGemma, and Gemma Scope. Gemma 2 2B is a lightweight model with superior performance and efficiency. ShieldGemma offers safety content classifier models for various tasks, while Gemma Scope provides insights into model operations. These additions enhance AI capabilities, safety, and innovation.***
    
    Jul 31, Google released [Gemma 2 family](https://developers.googleblog.com/en/smaller-safer-more-transparent-advancing-responsible-ai-with-gemma/#:~:text=A%20Future%20Built%20on%20Responsible,developing%20safe%20and%20beneficial%20AI.) members Gemma 2 2B, SheldGemma and Gemma Scope. [Gemma 2 2B](https://huggingface.co/collections/google/gemma-2-2b-release-66a20f3796a2ff2a7c76f98f) – a brand-new version of the popular 2 billion (2B) parameter model, featuring built-in safety advancements and a powerful balance of performance and efficiency. This lightweight model produces outsized results by learning from larger models through distillation. In fact, Gemma 2 2B surpasses all GPT-3.5 models on the Chatbot Arena, demonstrating its exceptional conversational AI abilities. ShieldGemma – a suite of safety content classifier models, built upon Gemma 2, to filter the input and outputs of AI models and keep the user safe. [ShieldGemma](https://huggingface.co/google/shieldgemma-2b (/9b/27b)) offers various model sizes to meet diverse needs. The 2B model is ideal for online classification tasks, while the 9B and 27B versions provide higher performance for offline applications where latency is less of a concern. [Gemma Scope](https://huggingface.co/google/gemma-scope) – a new model interpretability tool that offers unparalleled insight into our models' inner workings. With these additions, researchers and developers can now create safer customer experiences, gain unprecedented insights into the models, and confidently deploy powerful AI responsibly, right on device, unlocking new possibilities for innovation. <br><br>

15. ***ShieldGemma for Content Moderation: <br>
    Google presented ShieldGemma, a suite of LLM-based safety content moderation models built on Gemma2. These models offer robust predictions of safety risks and outperform existing models on benchmarks. The paper introduces a novel data curation pipeline and demonstrates strong generalization performance. ShieldGemma advances LLM safety and content moderation solutions.***
    
    Jul 31, Google published a [paper](https://arxiv.org/pdf/2407.21772) “ShieldGemma: Generative AI Content Moderation Based on Gemma”. The paper presents ShieldGemma, a comprehensive suite of LLM-based safety content moderation models built upon Gemma2. These models provide robust, state-of-the-art predictions of safety risks across key harm types (sexually explicit, dangerous content, harassment, hate speech) in both user input and LLM-generated output. By evaluating on both public and internal benchmarks, the work demonstrates superior performance compared to existing models, such as Llama Guard (+10.8\% AU-PRC on public benchmarks) and WildCard (+4.3\%). Additionally, the paper presents a novel LLM-based data curation pipeline, adaptable to a variety of safety-related tasks and beyond. The authors have shown strong generalization performance for model trained mainly on synthetic data. By releasing ShieldGemma, the paper provides a valuable resource to the research community, advancing LLM safety and enabling the creation of more effective content moderation solutions for developers. Models are [available here](https://huggingface.co/google/shieldgemma-2b (/9b/27b)). <br><br>

17. ***Meta's Self-Improving Language Models: <br>
    Meta, UC Berkeley, and NYU introduced a Meta-Rewarding step for self-improving language models. This method improves models' judgment skills by having them judge their own responses. The approach enhances both judgment and instruction-following abilities without human supervision, showing significant performance improvements on benchmarks.***
    
    Jul 30, Meta, UC Berkeley, and NYU published a [paper](https://arxiv.org/pdf/2407.19594) “Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge”. Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs can improve by judging their own responses instead of relying on human labelers. However, existing methods have primarily focused on improving model responses rather than judgment capabilities, resulting in rapid saturation during iterative training. To address this issue, the paper introduces a novel Meta-Rewarding step to the self-improvement process, where the model judges its own judgements and uses that feedback to refine its judgment skills. Surprisingly, this unsupervised approach improves the model's ability to judge {\em and} follow instructions, as demonstrated by a win rate improvement of Llama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on Arena-Hard. These results strongly suggest the potential for self-improving models without human supervision. <br><br>

19. ***Google's MoNE for Efficient Visual Processing: <br>
    Google and the University of Washington presented the Mixture of Nested Experts (MoNE) model, which uses a nested structure for experts to process visual tokens efficiently. MoNE reduces inference time compute while maintaining performance, making it adaptable to different compute budgets. The approach is validated on standard image and video datasets.***
    
    Jul 30, Google and Uni of Washington published a [paper](https://arxiv.org/pdf/2407.19985) “Mixture of Nested Experts: Adaptive Processing of Visual Tokens”. The visual medium (images and videos) naturally contains a large amount of information redundancy, thereby providing a great opportunity for leveraging efficiency in processing. While Vision Transformer (ViT) based models scale effectively to large data regimes, they fail to capitalize on this inherent redundancy, leading to higher computational costs. Mixture of Experts (MoE) networks demonstrate scalability while maintaining same inference-time costs, but they come with a larger parameter footprint. The paper presents Mixture of Nested Experts (MoNE), which utilizes a nested structure for experts, wherein individual experts fall on an increasing compute-accuracy curve. Given a compute budget, MoNE learns to dynamically choose tokens in a priority order, and thus redundant tokens are processed through cheaper nested experts. Using this framework, MoNE achieves equivalent performance as the baseline models, while reducing inference time compute by over two-fold. The authors validate the approach on standard image and video datasets - ImageNet-21K, Kinetics400, and Something-Something-v2. The authors further highlight MoNE's adaptability by showcasing its ability to maintain strong performance across different inference-time compute budgets on videos, using only a single trained model. <br><br>

21. ***Diffusion Augmented Agents for RL: <br>
    Google introduced Diffusion Augmented Agents (DAAG), a framework leveraging language models, vision language models, and diffusion models to improve sample efficiency and transfer learning in reinforcement learning. DAAG enhances learning by transforming past experiences to align with target instructions, reducing the need for reward-labeled data and improving lifelong learning capabilities.***
    
    Jul 30, Google published a [paper](https://arxiv.org/pdf/2407.20798) “Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning”. The paper introduces Diffusion Augmented Agents (DAAG), a novel framework that leverages large language models, vision language models, and diffusion models to improve sample efficiency and transfer learning in reinforcement learning for embodied agents. DAAG hindsight relabels the agent's past experience by using diffusion models to transform videos in a temporally and geometrically consistent way to align with target instructions with a technique called as Hindsight Experience Augmentation. A large language model orchestrates this autonomous process without requiring human supervision, making it well-suited for lifelong learning scenarios. The framework reduces the amount of reward-labeled data needed to 1) finetune a vision language model that acts as a reward detector, and 2) train RL agents on new tasks. The paper demonstrates the sample efficiency gains of DAAG in simulated robotics environments involving manipulation and navigation. The results show that DAAG improves learning of reward detectors, transferring past experience, and acquiring new tasks - key abilities for developing efficient lifelong learning agents. Supplementary material and visualizations are available on the website [this https URL](https://sites.google.com/view/diffusion-augmented-agents/) <br><br>

23. ***Meta's SAM 2 for Video and Image Segmentation: <br>
    Meta's Segment Anything Model 2 (SAM 2) is designed for promptable visual segmentation in images and videos. It uses a transformer architecture with streaming memory for real-time processing and has the largest video segmentation dataset. SAM 2 provides better accuracy and speed compared to its predecessor, SAM, and is being released with a dataset and demo.***
    
    Jul 29, Meta published a [paper](https://scontent.fcbr1-1.fna.fbcdn.net/v/t39.2365-6/453323338_287900751050452_6064535069828837026_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=iEdf_eLLDBIQ7kNvgHvSueV&_nc_ht=scontent.fcbr1-1.fna&gid=AbMZVotuhlaDUcgiZQmipJh&oh=00_AYCMj0UJk4ZJNT-OM4nxXp6vgeWLO9SHo56ZlmA1qZHoaQ&oe=66B0FCB9) “SAM 2: Segment Anything in Images and Videos”.  The paper presents Segment Anything Model 2 (SAM 2 ), a foundation model towards solving promptable visual segmentation in images and videos. The authors build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. The model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on the data provides strong performance across a wide range of tasks. In video segmentation, it’s observed better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, the model is more accurate and 6x faster than the Segment Anything Model (SAM). The authors believe that the data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. The authors are releasing [a version of the model](https://ai.meta.com/sam2/), the dataset and an interactive demo. <br><br>

25. ***SaulLM Models for Legal Domain: <br>
    MICS and CINES introduced SaulLM-54B and SaulLM-141B, large language models tailored for the legal sector. These models are based on the Mixtral architecture and use domain adaptation strategies for legal tasks. They outperform previous models on LegalBench-Instruct and are released under the MIT License to facilitate reuse and research.***
    
    Jul 28, MICS and CINES published a [paper](https://arxiv.org/pdf/2407.19584) “SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain”. The paper introduces SaulLM-54B and SaulLM-141B, two large language models (LLMs) tailored for the legal sector. These models, which feature architectures of 54 billion and 141 billion parameters, respectively, are based on the Mixtral architecture. The development of SaulLM-54B and SaulLM-141B is guided by large-scale domain adaptation, divided into three strategies: (1) the exploitation of continued pretraining involving a base corpus that includes over 540 billion of legal tokens, (2) the implementation of a specialized legal instruction-following protocol, and (3) the alignment of model outputs with human preferences in legal interpretations. The integration of synthetically generated data in the second and third steps enhances the models' capabilities in interpreting and processing legal texts, effectively reaching state-of-the-art performance and outperforming previous open-source models on LegalBench-Instruct. This work explores the trade-offs involved in domain-specific adaptation at this scale, offering insights that may inform future studies on domain adaptation using strong decoder models. Building upon SaulLM-7B, this study refines the approach to produce an LLM better equipped for legal tasks. The authors are releasing base, instruct, and aligned versions on top of SaulLM-54B and SaulLM-141B under the MIT License to facilitate reuse and collaborative research. <br><br>

27. ***Amazon's REAPER for RAG Systems: <br>
    Amazon presented REAPER, a reasoning-based retrieval planner for complex RAG (Retrieval Augmented Generation) systems. REAPER generates retrieval plans for conversational systems, significantly reducing latency and scaling easily to new use cases. The method is shown to improve performance in a conversational shopping assistant context.***
    
    Jul 26, Amazon published a [paper](https://arxiv.org/abs/2407.18553) “REAPER: Reasoning based Retrieval Planning for Complex RAG Systems”. Complex dialog systems often use retrieved evidence to facilitate factual responses. Such RAG (Retrieval Augmented Generation) systems retrieve from massive heterogeneous data stores that are usually architected as multiple indexes or APIs instead of a single monolithic source. For a given query, relevant evidence needs to be retrieved from one or a small subset of possible retrieval sources. Complex queries can even require multi-step retrieval. For example, a conversational agent on a retail site answering customer questions about past orders will need to retrieve the appropriate customer order first and then the evidence relevant to the customer's question in the context of the ordered product. Most RAG Agents handle such Chain-of-Thought (CoT) tasks by interleaving reasoning and retrieval steps. However, each reasoning step directly adds to the latency of the system. For large models this latency cost is significant -- in the order of multiple seconds. Multi-agent systems may classify the query to a single Agent associated with a retrieval source, though this means that a (small) classification model dictates the performance of a large language model. This paper presents REAPER (REAsoning-based PlannER) - an LLM based planner to generate retrieval plans in conversational systems. The paper shows significant gains in latency over Agent-based systems and are able to scale easily to new and unseen use cases as compared to classification-based planning. Though the method can be applied to any RAG system, the authors show the results in the context of a conversational shopping assistant. <br><br>

29. ***Apple's MMAU Benchmark for LLM Agents: <br>
    Apple introduced the Massive Multitask Agent Understanding (MMAU) benchmark, evaluating models across five domains and capabilities. MMAU provides a comprehensive framework for assessing LLM agents' strengths and limitations with detailed analyses of 18 models. The benchmark enhances interpretability and understanding of model performance.***
    
    Jul 18, Apple published a [paper](https://arxiv.org/pdf/2407.18961) “MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains”. Recent advances in large language models (LLMs) have increased the demand for comprehensive benchmarks to evaluate their capabilities as human-like agents. Existing benchmarks, while useful, often focus on specific application scenarios, emphasizing task completion but failing to dissect the underlying skills that drive these outcomes. This lack of granularity makes it difficult to deeply discern where failures stem from. Additionally, setting up these environments requires considerable effort, and issues of unreliability and reproducibility sometimes arise, especially in interactive tasks. To address these limitations, the authors introduce the Massive Multitask Agent Understanding (MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need for complex environment setups. It evaluates models across five domains, including Tool-use, Directed Acyclic Graph (DAG) QA, Data Science and Machine Learning coding, Contest-level programming and Mathematics, and covers five essential capabilities: Understanding, Reasoning, Planning, Problem-solving, and Self-correction. With a total of 20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU provides a comprehensive framework for evaluating the strengths and limitations of LLM agents. By testing 18 representative models on MMAU, the paper provides deep and insightful analyses. Ultimately, MMAU not only sheds light on the capabilities and limitations of LLM agents but also enhances the interpretability of their performance. Datasets and evaluation scripts of MMAU are released at https://github.com/apple/axlearn/blob/main/docs/research/mmau.
 <br><br><br>

***28 Jul 2024***

1. ***Google’s AI achieves silver-medal standard solving International Mathematical Olympiad problems <br>
Google's AlphaProof and AlphaGeometry teams developed new AI systems capable of solving complex math problems. The systems, AlphaProof for algebra and number theory, and AlphaGeometry 2 for geometry, achieved a silver-medal standard at the 2023 International Mathematical Olympiad, solving four out of six problems. AlphaProof handled algebra and number theory, including the competition's hardest problem, while AlphaGeometry 2 solved the geometry problem. The systems scored 28 points out of a possible 42, just below the gold-medal threshold of 29 points, showcasing advanced mathematical reasoning capabilities.*** <br>
   Jul 25, Google’s AlphaProof  and AlphaGeometry teams published a [blog](https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/) “AI achieves silver-medal standard solving International Mathematical Olympiad problems”. Artificial general intelligence (AGI) with advanced mathematical reasoning has the potential to unlock new frontiers in science and technology. AlphaProof is a new reinforcement-learning based system for formal math reasoning, and AlphaGeometry 2, is an improved version of our geometry-solving system. Together, these systems solved four out of six problems from this year’s International Mathematical Olympiad (IMO), achieving the same level as a silver medalist in the competition for the first time. The IMO is the oldest, largest and most prestigious competition for young mathematicians, held annually since 1959. More recently, the annual IMO competition has also become widely recognised as a grand challenge in machine learning and an aspirational benchmark for measuring an AI system’s advanced mathematical reasoning capabilities. AlphaProof solved two algebra problems and one number theory problem by determining the answer and proving it was correct. This included the hardest problem in the competition, solved by only five contestants at this year’s IMO. AlphaGeometry 2 proved the geometry problem, while the two combinatorics problems remained unsolved. Each of the six problems can earn seven points, with a total maximum of 42. [Google’s system achieved a final score of 28 points](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/imo-2024-solutions/index.html), earning a perfect score on each problem solved — equivalent to the top end of the silver-medal category. This year, the gold-medal threshold starts at 29 points, and was achieved by 58 of 609 contestants at the official competition. <br><br>

3. ***AI models collapse when trained on recursively generated data <br>
Researchers from several universities published findings on the risks of training AI models on data generated by other AI models. They identified a phenomenon called 'model collapse,' where the quality of models degrades due to recursive training on AI-generated content, leading to the loss of rare content features. This study emphasizes the need for human-generated data to maintain the integrity and performance of AI models over time.*** <br>
   Jul 25, Uni of Oxford, Uni of Cambridge,  Imperial College London, Uni of Toronto, Uni of Edinburgh published a [paper](https://www.nature.com/articles/s41586-024-07566-y.pdf) on Nature “AI models collapse when trained on recursively generated data”. Stable diffusion revolutionized image creation from descriptive text. GPT-2), GPT-3(.5) and GPT-4 demonstrated high performance across a variety of language tasks. ChatGPT introduced such language models to the public. It is now clear that generative artificial intelligence (AI) such as large language models (LLMs) is here to stay and will substantially change the ecosystem of online text and images. Here the authors consider what may happen to GPT-{n} once LLMs contribute much of the text found online. The paper finds that indiscriminate use of model-generated content in training causes irreversible defects in the resulting models, in which tails of the original content distribution disappear. The authors refer to this effect as ‘model collapse’ and show that it can occur in LLMs as well as in variational autoencoders (VAEs) and Gaussian mixture models (GMMs). The study builds theoretical intuition behind the phenomenon and portray its ubiquity among all learned generative models. The work demonstrates that it must be taken seriously if people are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of LLM-generated content in data crawled from the Internet. <br><br>

5. ***OpenAI's SearchGPT prototype <br>
OpenAI launched SearchGPT, a prototype integrating web information with AI model responses to provide users with fast, sourced answers. Currently in testing with a small group, SearchGPT aims to enhance ChatGPT by incorporating real-time web data. The system allows users to ask follow-up questions and aims to improve search interactions. While separate from AI training, SearchGPT ensures that publishers can manage their content's appearance in search results.*** <br>
   Jul 25, [OpenAI released](https://openai.com/index/searchgpt-prototype/) it SearchGPT, a prototype of new search features designed to combine the strength of OpenAI’s AI models with information from the web to give users fast and timely answers with clear and relevant sources. OpenAI is launching to a small group of users and publishers to get feedback. While this prototype is temporary, it plans to integrate the best of these features directly into ChatGPT in the future. SearchGPT will quickly and directly respond to users questions with up-to-date information from the web while giving clear links to relevant sources. Users will be able to ask follow-up questions, like one would in a conversation with a person, with the shared context building with each query. OpenAI has partnered with publishers to build this experience and continue to seek their feedback. In addition to launching the SearchGPT prototype, OpenAI is also launching a way for publishers to manage how they appear in SearchGPT, so publishers have more choices. Importantly, SearchGPT is about search and is separate from training OpenAI’s generative AI foundation models. Sites can be surfaced in search results even if they opt out of generative AI training. However, it is found that the answers of SearchGPT are mostly incorrect. <br><br>

7. ***Data Provenance Initiative's audit on AI data consent protocols <br>
The Data Provenance Initiative highlighted a growing crisis in data consent for AI training. Their audit of 14,000 web domains showed increasing restrictions on the use of web data for AI, with significant portions of commonly used datasets becoming inaccessible. This trend threatens the diversity and scalability of AI systems, prompting a call for more effective data consent protocols.*** <br>
   Jul 24, Data Provenance Initiative (a collective of independent and academic researchers) published a [paper](https://arxiv.org/pdf/2407.14933) “Consent in Crisis: The Rapid Decline of the AI Data Commons”. General-purpose artificial intelligence (AI) systems are built on massive swathes of public web data, assembled into corpora such as C4, RefinedWeb, and Dolma. To the authors’ knowledge, they conduct the first, large-scale, longitudinal audit of the consent protocols for the web domains underlying AI training corpora. The audit of 14,000 web domains provides an expansive view of crawlable web data and how consent preferences to use it are changing over time. The authors observe a proliferation of AI-specific clauses to limit use, acute differences in restrictions on AI developers, as well as general inconsistencies between websites' expressed intentions in their Terms of Service and their robots.txt. The paper diagnoses these as symptoms of ineffective web protocols, not designed to cope with the widespread re-purposing of the internet for AI. The longitudinal analyses show that in a single year (2023-2024) there has been a rapid crescendo of data restrictions from web sources, rendering ~5%+ of all tokens in C4, or 28%+ of the most actively maintained, critical sources in C4, fully restricted from use. For Terms of Service crawling restrictions, a full 45% of C4 is now restricted. If respected or enforced, these restrictions are rapidly biasing the diversity, freshness, and scaling laws for general-purpose AI systems. The authors hope to illustrate the emerging crisis in data consent, foreclosing much of the open web, not only for commercial AI, but non-commercial AI and academic purposes. <br><br>

9. ***Mistral releases Mistral Large 2 AI model <br>
Mistral unveiled Mistral Large 2, an advanced AI model with a 128k context window and support for multiple languages and coding languages. The model excels in single-node inference and boasts high accuracy on various benchmarks, including coding and reasoning. It sets a new standard for performance and cost efficiency in AI models, offering robust performance on par with leading models like GPT-4.*** <br>
    Jul 24, [Mistral released Mistral Large English](https://mistral.ai/news/mistral-large-2407/), the latest generation continues to push the boundaries of cost efficiency, speed, and performance. Mistral Large 2 is exposed on la Plateforme and enriched with new features to facilitate building innovative AI applications. Mistral Large 2 has a 128k context window and supports dozens of languages including French, German, Chinese, etc, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash. Mistral Large 2 is designed for single-node inference with long-context applications in mind – its size of 123 billion parameters allows it to run at large throughput on a single node. Mistral Research License allows usage and modification for research and non-commercial usages. Mistral Large 2 sets a new frontier in terms of performance / cost of serving on evaluation metrics. In particular, on MMLU, the pretrained version achieves an accuracy of 84.0%, and sets a new point on the performance/cost Pareto front of open models. Mistral Large 2 vastly outperforms the previous Mistral Large on coding and reasoning, and performs on par with leading models such as GPT-4o, Claude 3 Opus, and Llama 3 405B. Additionally, the new Mistral Large 2 is trained to acknowledge when it cannot find solutions or does not have sufficient information to provide a confident answer. This commitment to accuracy is reflected in the improved model performance on popular mathematical benchmarks, demonstrating its enhanced reasoning and problem-solving skills. The new Mistral Large 2 is particularly better at following precise instructions and handling long multi-turn conversations. <br><br>

11. ***Data mixture inference study using BPE tokenizers <br>
Researchers developed a method to infer the data mixture in language model training sets using byte-pair encoding (BPE) tokenizers. By analyzing token frequency patterns, they accurately estimated the proportions of various data types in training sets. This method revealed new insights into the multilingual and code-focused nature of recent models like GPT-4o and Llama3, contributing to better understanding and transparency in AI model training.*** <br>
    Jul 24, Uni of Washington and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2407.16607) “Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?”. The pretraining data of today's strongest language models is opaque. In particular, little is known about the proportions of various domains or languages represented. This work tackles a task which is called data mixture inference, which aims to uncover the distributional make-up of training data. The work introduces a novel attack based on a previously overlooked source of information -- byte-pair encoding (BPE) tokenizers, used by the vast majority of modern language models. The key insight is that the ordered list of merge rules learned by a BPE tokenizer naturally reveals information about the token frequencies in its training data: the first merge is the most common byte pair, the second is the most common pair after merging the first token, and so on. Given a tokenizer's merge list along with data samples for each category of interest, the authors formulate a linear program that solves for the proportion of each category in the tokenizer's training set. Importantly, to the extent to which tokenizer training data is representative of the pretraining data, the work indirectly learns about the pretraining data. In controlled experiments, the paper shows that the attack recovers mixture ratios with high precision for tokenizers trained on known mixtures of natural languages, programming languages, and data sources. The paper then applies the approach to off-the-shelf tokenizers released with recent LMs. The authors confirm much publicly disclosed information about these models, and also make several new inferences: GPT-4o's tokenizer is much more multilingual than its predecessors, training on 39% non-English data; Llama3 extends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and Claude's tokenizers are trained on predominantly code (~60%). The work sheds light on current design practices for pretraining data, and inspires continued research into data mixture inference for LMs. <br><br>

13. ***Elon Musk's xAI Memphis Supercluster <br>
Elon Musk announced the creation of the Memphis Supercluster, a powerful AI training cluster using 100,000 Nvidia H100 GPUs. This initiative aims to develop the world's most powerful AI, Grok 3, by December 2023. The supercluster's scale surpasses current top supercomputers, highlighting Musk's ambitious plans for AI advancements.*** <br>
    Jul 23, according to [tomshardware.com](https://www.tomshardware.com/pc-components/gpus/elon-musk-fires-up-the-most-powerful-ai-training-cluster-in-the-world-uses-100000-nvidia-h100-gpus-on-a-single-fabric), Tech baron Elon Musk has taken to Twitter/X to boast of starting up “the most powerful AI training cluster in the world,” which he will use to create the self-professed "world’s most powerful AI by every metric by December of this year.” Today, xAI’s Memphis Supercluster began AI training using 100,000 liquid-cooled Nvidia H100 GPUs connected with a single RDMA (remote direct memory access) fabric. In a follow-up Tweet, Musk explains that the new supercluster will be “training the world’s most powerful AI by every metric.” From previous statements of intent, it is assumed that the power of xAI’s 100,000 H100 GPU installation will now be targeted at Grok 3 training. Musk said the refined LLM should be finished with the training stage “by December this year.” To put the Memphis Supercluster compute resources in some context, certainly, going by scale, the new xAI Memphis Supercluster easily outclasses anything in the most recent Top500 list in terms of GPU horsepower. The world’s most powerful supercomputers such as Frontier (37,888 AMD GPUs), Aurora (60,000 Intel GPUs), and Microsoft Eagle (14,400 Nvidia H100 GPUs) seem to be significantly outgunned by the xAI machine. <br><br>

15. ***Meta's release of Llama 3.1 <br>
Meta introduced Llama 3, a new set of foundation models supporting multilinguality, coding, reasoning, and tool usage. The largest model, with 405 billion parameters, rivals leading AI models like GPT-4. Llama 3 integrates capabilities across image, video, and speech tasks and includes safety features. The release aims to push the boundaries of AI performance and versatility.*** <br>
    Jul 23, Meta released llama 3.1 and the corresponding [paper](https://scontent.fcbr1-1.fna.fbcdn.net/v/t39.2365-6/452387774_1036916434819166_4173978747091533306_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=7qSoXLG5aAYQ7kNvgHS5YSv&_nc_ht=scontent.fcbr1-1.fna&oh=00_AYDrMk-4WCS9Y2Sa1syLLvRW05gfxbKWvz1mJleRSHbpBg&oe=66AA6F8D) “The Llama 3 Herd of Models”. Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. The largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. The authors find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. The authors publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and the Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which integrates image, video, and speech capabilities into Llama 3 via a compositional approach. The authors observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development. <br><br>

17. ***Switzerland mandates open-source software for government <br>
Switzerland passed a law requiring all government software to be open-source, promoting transparency, security, and efficiency. This legislation, part of a broader European trend, mandates public disclosure of software code and the release of non-sensitive government data as Open Government Data (OGD). The move aims to foster greater openness and practical reuse of software and data in the public sector.*** <br>
    Jul 23, [according to zdnet.com](https://www.zdnet.com/article/switzerland-now-requires-all-government-software-to-be-open-source/), Switzerland now requires all government software to be open source. Several European countries are betting on open-source software. In the United States, eh, not so much. In the latest news from across the Atlantic, Switzerland has taken a major step forward with its "Federal Law on the Use of Electronic Means for the Fulfillment of Government Tasks" (EMBAG). This groundbreaking legislation mandates using open-source software (OSS) in the public sector. This new law requires all public bodies to disclose the source code of software developed by or for them unless third-party rights or security concerns prevent it. This "public money, public code" approach aims to enhance government operations' transparency, security, and efficiency. In addition to mandating OSS, the EMBAG also requires the release of non-personal and non-security-sensitive government data as Open Government Data (OGD). This dual "open by default" approach marks a significant paradigm shift towards greater openness and practical reuse of software and data. Other countries in Europe have long supported open source. For example, in 2023, French President Macron stated, "We love open source," and France's National Gendarmerie (Think FBI if you're an American) uses Linux on its PCs. The European Union (EU) has long worked on securing OSS via the EU's Free and Open Source Software Auditing (FOSSA) project. <br><br>

19. ***OpenDevin platform for AI software developers <br>
A new platform, OpenDevin, was introduced to facilitate the development of AI agents that interact with their environment similarly to human developers. OpenDevin supports code writing, command line interaction, and web browsing, allowing for flexible and powerful AI agent development. The platform, open-sourced and community-driven, aims to enhance AI capabilities across various challenging tasks.*** <br>
    Jul 23, UIUC, CMU, Yale, UC Berkeley, ContextualAI, et al. published a [paper](https://arxiv.org/pdf/2407.16741) “OpenDevin: An Open Platform for AI Software Developers as Generalist Agents”. Software is one of the most powerful tools that humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. This paper introduces OpenDevin, a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to those of a human developer: by writing code, interacting with a command line, and browsing the web. The authors describe how the platform allows for the implementation of new agents, safe interaction with sandboxed environments for code execution, coordination between multiple agents, and incorporation of evaluation benchmarks. Based on the currently incorporated benchmarks, the authors perform an evaluation of agents over 15 challenging tasks, including software engineering (e.g., SWE-Bench) and web browsing (e.g., WebArena), among others. Released under the permissive MIT license, [OpenDevin](https://github.com/OpenDevin/OpenDevin) is a community project spanning academia and industry with more than 1.3K contributions from over 160 contributors and will improve going forward. <br><br>

21. ***Comparison of KAN and MLP models <br>
Researchers conducted a comprehensive comparison of KAN and MLP models, finding that MLP generally outperforms KAN across various tasks, except for symbolic formula representation. The study also identified the B-spline activation function as a key factor in KAN's performance in symbolic tasks. These findings offer insights for future research on model alternatives and improvements.*** <br>
    Jul 23, Nation Uni of Singapore published a [paper](https://arxiv.org/pdf/2407.16674) “KAN or MLP: A Fairer Comparison”. This paper does not introduce a novel method. Instead, it offers a fairer and more comprehensive comparison of KAN and MLP models across various tasks, including machine learning, computer vision, audio processing, natural language processing, and symbolic formula representation. Specifically, the authors control the number of parameters and FLOPs to compare the performance of KAN and MLP. The main observation is that, except for symbolic formula representation tasks, MLP generally outperforms KAN. The authors also conduct ablation studies on KAN and find that its advantage in symbolic formula representation mainly stems from its B-spline activation function. When B-spline is applied to MLP, performance in symbolic formula representation significantly improves, surpassing or matching that of KAN. However, in other tasks where MLP already excels over KAN, B-spline does not substantially enhance MLP's performance. Furthermore, the study finds that KAN's forgetting issue is more severe than that of MLP in a standard class-incremental continual learning setting, which differs from the findings reported in the KAN paper. The authors hope these results provide insights for future research on KAN and other MLP alternatives. Project link: https://github.com/yu-rp/KANbeFair <br><br>

23. ***Specialization for legal tasks using Llama 3 <br>
A study demonstrated that fine-tuned Llama 3 models significantly outperform GPT-4 on legal text classification tasks. The research showed that even light fine-tuning on specific tasks could achieve high accuracy, suggesting a viable alternative to using commercial models for legal research. This approach could reduce reliance on costly human annotation.*** <br>
    Jul 23, Tubingen AI Center, Harvard Uni, ETH Zurich, Washington Uni and Uni of Virginia published a [paper](https://arxiv.org/pdf/2407.16615) “Lawma: The Power of Specialization for Legal Tasks”. Annotation and classification of legal text are central components of empirical legal research. Traditionally, these tasks are often delegated to trained research assistants. Motivated by the advances in language modeling, empirical legal scholars are increasingly turning to prompting commercial models, hoping that it will alleviate the significant cost of human annotation. Despite growing use, the understanding of how to best utilize large language models for legal tasks remains limited. The work conducts a comprehensive study of 260 legal text classification tasks, nearly all new to the machine learning community. Starting from GPT-4 as a baseline, the paper shows that it has non-trivial but highly varied zero-shot accuracy, often exhibiting performance that may be insufficient for legal work. The authors then demonstrate that a lightly fine-tuned Llama 3 model vastly outperforms GPT-4 on almost all tasks, typically by double-digit percentage points. The authors find that larger models respond better to fine-tuning than smaller models. A few tens to hundreds of examples suffice to achieve high classification accuracy. Notably, the authors can fine-tune a single model on all 260 tasks simultaneously at a small loss in accuracy relative to having a separate model for each task. The work points to a viable alternative to the predominant practice of prompting commercial models. For concrete legal tasks with some available labeled data, researchers are better off using a fine-tuned open-source model. <br><br>

25. ***Local vs global continual learning <br>
Researchers explored continual learning strategies, comparing local and global approximations. They classified existing algorithms based on these strategies and assessed their practical effects. The study highlighted the importance of understanding continual learning mechanisms to develop effective strategies for integrating new information while retaining past knowledge.*** <br>
    Jul 23, ETH Zurich published a [paper](https://arxiv.org/pdf/2407.16611) on CoLLAs 2024 “Local vs Global continual learning”.  Continual learning is the problem of integrating new information in a model while retaining the knowledge acquired in the past. Despite the tangible improvements achieved in recent years, the problem of continual learning is still an open one. A better understanding of the mechanisms behind the successes and failures of existing continual learning algorithms can unlock the development of new successful strategies. This work views continual learning from the perspective of the multi-task loss approximation, and compares two alternative strategies, namely local and global approximations. The authors classify existing continual learning algorithms based on the approximation used, and assess the practical effects of this distinction in common continual learning settings. Additionally, the authors study optimal continual learning objectives in the case of local polynomial approximations and provide examples of existing algorithms implementing the optimal objectives. <br><br>

27. ***Shared hallucinations in LLMs <br>
Salesforce's study on large language models (LLMs) revealed that these models share a 'shared imagination space,' enabling them to answer each other's imaginary questions with high success. This phenomenon suggests a commonality in the models' training and hallucination processes, raising questions about model homogeneity and computational creativity.*** <br>
    Jul 23, Salesforce published a [paper](https://arxiv.org/pdf/2407.16604) “Shared Imagination: LLMs Hallucinate Alike”. Despite the recent proliferation of large language models (LLMs), their training recipes -- model architecture, pre-training data and optimization algorithm -- are often very similar. This naturally raises the question of the similarity among the resulting models. This paper proposes a novel setting, imaginary question answering (IQA), to better understand model similarity. In IQA, the authors ask one model to generate purely imaginary questions (e.g., on completely made-up concepts in physics) and prompt another model to answer. Surprisingly, despite the total fictionality of these questions, all models can answer each other's questions with remarkable success, suggesting a "shared imagination space" in which these models operate during such hallucinations. The authors conduct a series of investigations into this phenomenon and discuss implications on model homogeneity, hallucination, and computational creativity. <br><br>

29. ***Training-free baseline for video LLMs <br>
Apple proposed SlowFast-LLaVA, a training-free video LLM that captures detailed spatial semantics and long-range temporal context. The two-stream design effectively aggregates features from video frames, outperforming existing training-free methods on video tasks. This approach offers a strong baseline for video LLMs without the need for extensive training.*** <br>
    Jul 22, Apple published a [paper](https://arxiv.org/pdf/2407.15841) “SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models”. The paper proposes SlowFast-LLaVA (or SF-LLaVA for short), a training-free video large language model (LLM) that can jointly capture the detailed spatial semantics and long-range temporal context without exceeding the token budget of commonly used LLMs. This is realized by using a two-stream SlowFast design of inputs for Video LLMs to aggregate features from sampled video frames in an effective way. Specifically, the Slow pathway extracts features at a low frame rate while keeping as many spatial details as possible (e.g., with 24x24 tokens), and the Fast pathway operates on a high frame rate but uses a larger spatial pooling stride (e.g., downsampling 6x) to focus on the motion cues. As a result, this design allows adequately capturing both spatial and temporal features that are beneficial for understanding details along the video. Experimental results show that SF-LLaVA outperforms existing training-free methods on a wide range of video tasks. On some benchmarks, it achieves comparable or even better performance compared to state-of-the-art Video LLMs that are fine-tuned on video datasets. <br><br>

31. ***AI-based startups analysis by High Signal AI <br>
An analysis of AI-based startups backed by YCombinator highlighted trends in AI innovation, investment, and founder backgrounds. Key findings include a focus on B2B solutions, opportunities in underserved sectors, and the importance of technical expertise. The study also noted the growing interest in generative AI and the need for ethical AI considerations.*** <br>
    Jul 19, High Signal AI published a [blog](https://highsignalai.substack.com/p/what-i-learned-from-looking-at-400) “What I learned from looking at 400 AI-based Startups backed by YCombinator”. YCombinator's (YC) track record in identifying and nurturing successful startups is unparalleled in the tech industry. Their selection process has consistently surfaced companies that go on to reshape entire sectors, making their portfolio a valuable indicator of emerging trends and technologies.  The author was looking for answers to questions like which sectors are seeing the most AI innovation? What types of AI applications are attracting investment? What backgrounds do successful AI founders have? This study aims to provide insights into: 1) The hottest industries and sectors for AI startups. 2) Areas ripe for AI disruption. 3) AI in emerging technologies like blockchain and quantum computing. 4) Companies working in AI Safety, Accessibility, Explainability. 5) Common traits among YC-backed AI founders. 6) How to find what you should build with AI using the above insights. Key finds are: 1) Focus on B2B: With 81.1% of YC-backed AI startups targeting businesses, consider enterprise solutions for higher chances of funding and success. 2) Explore Underserved Sectors: While healthcare/biotech (10.8%), fintech (9.1%), and developer tools (8.9%) dominate, look for opportunities in neglected areas like manufacturing (1%) or agriculture (0.7%). 3) Prioritize Technical Expertise: Ensure your founding team includes strong technical talent, as 74.8% of YC-backed AI companies have at least one founder with a robust technical background. 4) Capitalize on Generative AI: With 18.7% of startups in this space, generative AI is hot. However, consider how you can apply it innovatively to stand out. 5) Address Ethical Concerns: Only 1.2% of startups focus on ethical AI. This gap represents a significant opportunity for forward-thinking founders. <br><br>

33. ***Pruning and knowledge distillation for LLMs <br>
Nvidia's study on compressing language models through pruning and knowledge distillation demonstrated significant compute cost savings and performance improvements. The approach reduced training data requirements and maintained high accuracy, offering an efficient alternative to training large models from scratch. The compressed models performed well compared to other community models and state-of-the-art compression techniques.*** <br>
    Jul 19, Nvidia published a [paper](https://arxiv.org/pdf/2407.14679) “Compact Language Models via Pruning and Knowledge Distillation”. Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. This paper investigates if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, the study develops a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; and arrives at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. The paper uses this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using the approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. The authors have [open-sourced Minitron](https://github.com/NVlabs/Minitron) model weights on Huggingface, with corresponding supplementary material including example code available on GitHub. <br><br>

35. ***Long-context language models benchmark <br>
Researchers created NoCha, a benchmark for long-context language models, testing their ability to retrieve, synthesize, and reason over book-length inputs. The dataset consists of pairs of true and false claims about books, requiring global reasoning. The study found that current LLMs struggle with these tasks, highlighting the need for further improvements in long-context comprehension.*** <br>
    Jul 18, UMass Amherst, Allen Inst of AI, and Princeton Uni published a [paper](https://arxiv.org/pdf/2406.16264) “One Thousand and One Pairs: A "novel" challenge for long-context language models”. Synthetic long-context LLM benchmarks (e.g., "needle-in-the-haystack") test only surface-level retrieval capabilities, but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? The paper addresses this question by creating NoCha, a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, the annotators confirm that the largest share of pairs in NoCha require global reasoning over the entire book to verify. The experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that evaluated: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest accuracy at 55.8%. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NoCha allows for the evolution of the benchmark dataset and the easy analysis of future models. <br><br>

37. ***AI Agents That Matter <br>
The paper highlights several issues with current AI agent benchmarks, such as an overemphasis on accuracy, lack of attention to costs, conflation of benchmarking needs, inadequate holdout sets, and poor standardization in evaluation practices. The authors propose optimizing both accuracy and cost, distinguishing benchmarking needs, addressing overfitting, and standardizing evaluations to develop more practical and reliable AI agents.*** <br>
    Jul 1, Princeton Uni published [paper](https://arxiv.org/abs/2407.01502) “AI Agents That Matter”. AI agents are an exciting new research direction, and agent development is driven by benchmarks. An analysis of current agent benchmarks and evaluation practices reveals several shortcomings that hinder their usefulness in real-world applications. First, there is a narrow focus on accuracy without attention to other metrics. As a result, SOTA agents are needlessly complex and costly, and the community has reached mistaken conclusions about the sources of accuracy gains. The focus on cost in addition to accuracy motivates the new goal of jointly optimizing the two metrics. The authors design and implement one such optimization, showing its potential to greatly reduce cost while maintaining accuracy. Second, the benchmarking needs of model and downstream developers have been conflated, making it hard to identify which agent would be best suited for a particular application. Third, many agent benchmarks have inadequate holdout sets, and sometimes none at all. This has led to agents that are fragile because they take shortcuts and overfit to the benchmark in various ways. The research prescribes a principled framework for avoiding overfitting. Finally, there is a lack of standardization in evaluation practices, leading to a pervasive lack of reproducibility. The authors hope that the steps introduced for addressing these shortcomings will spur the development of agents that are useful in the real world and not just accurate on benchmarks.

 <br><br><br>

***21 Jul 2024***

1. ***Mistral NeMo Released: <br>
Mistral released Mistral NeMo, a 12B model with a 128k context length, under the Apache 2.0 license. It’s a global, multilingual model with strengths in English, French, Chinese, and Hindi, and uses a new tokenizer, Tekken, which outperforms the previous SentencePiece tokenizer. Mistral NeMo is better at instruction following, reasoning, multi-turn conversations, and code generation compared to its predecessor, Mistral 7B.*** <br>
   Jul 18, [Mistral released Mistral NeMo](https://mistral.ai/news/mistral-nemo/), a state-of-the-art 12B model with 128k context length, and released under the Apache 2.0 license. As it relies on standard architecture, Mistral NeMo is easy to use and a drop-in replacement in any system using Mistral 7B. The model is designed for global, multilingual applications. It is trained on function calling, has a large context window, and is particularly strong in English, French, Chinese, and Hindi. Mistral NeMo uses a new tokenizer, Tekken, based on Tiktoken, that was trained on over more than 100 languages, and compresses natural language text and source code more efficiently than the SentencePiece tokenizer used in previous Mistral models. Compared to the Llama 3 tokenizer, Tekken proved to be more proficient in compressing text for approximately 85% of all languages. Mistral NeMO underwent an advanced fine-tuning and alignment phase. Compared to Mistral 7B, it is much better at following precise instructions, reasoning, handling multi-turn conversations, and generating code. <br><br>

3. ***OpenAI Paper on LLM Legibility: <br>
OpenAI published a paper on improving LLM output legibility using Prover-Verifier Games. This method enhances the clarity and checkability of solutions in math problems by training verifiers and provers. Results show increased human accuracy in verifying solutions, suggesting this method could align LLMs more closely with human-checkable outputs.*** <br>
   Jul 18, OpenAI published a [paper](https://arxiv.org/abs/2407.13692) “Prover-Verifier Games improve legibility of LLM outputs”. One way to increase confidence in the outputs of Large Language Models (LLMs) is to support them with reasoning that is clear and easy to check -- a property call legibility. The paper studies legibility in the context of solving grade-school math problems and show that optimizing chain-of-thought solutions only for answer correctness can make them less legible. To mitigate the loss in legibility, the authors propose a training algorithm inspired by Prover-Verifier Game from Anil et al. (2021). The algorithm iteratively trains small verifiers to predict solution correctness, "helpful" provers to produce correct solutions that the verifier accepts, and "sneaky" provers to produce incorrect solutions that fool the verifier. The study finds that the helpful prover's accuracy and the verifier's robustness to adversarial attacks increase over the course of training. Furthermore, the paper shows that legibility training transfers to time-constrained humans tasked with verifying solution correctness. Over course of LLM training human accuracy increases when checking the helpful prover's solutions, and decreases when checking the sneaky prover's solutions. Hence, training for checkability by small verifiers is a plausible technique for increasing output legibility. The results suggest legibility training against small verifiers as a practical avenue for increasing legibility of large LLMs to humans, and thus could help with alignment of superhuman models. <br><br>

5. ***MetaSumPerceiver for Fact-Checking: <br>
Virginia Tech introduced MetaSumPerceiver, a model designed for fact-checking claims by summarizing multimodal, multi-document datasets. Using a dynamic perceiver-based model and reinforcement learning, it outperforms existing approaches on the MOCHEG dataset and demonstrates strong performance on a new fact-checking dataset.*** <br>
   Jul 18, Virginia Tech published a [paper](https://arxiv.org/pdf/2407.13089) on ACM 2024 “MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for Fact-Checking”. Fact-checking real-world claims often requires reviewing multiple multimodal documents to assess a claim's truthfulness, which is a highly laborious and time-consuming task. This paper presents a summarization model designed to generate claim-specific summaries useful for fact-checking from multimodal, multi-document datasets. The model takes inputs in the form of documents, images, and a claim, with the objective of assisting in fact-checking tasks. The paper introduces a dynamic perceiver-based model that can handle inputs from multiple modalities of arbitrary lengths. To train the model, the authors leverage a novel reinforcement learning-based entailment objective to generate summaries that provide evidence distinguishing between different truthfulness labels. To assess the efficacy of the approach, the authors conduct experiments on both an existing benchmark and a new dataset of multi-document claims. The proposed approach outperforms the SOTA approach by 4.6% in the claim verification task on the MOCHEG dataset and demonstrates strong performance on the new Multi-News-Fact-Checking dataset. <br><br>

7. ***OpenAI's GPT-4o Mini: <br>
OpenAI released GPT-4o mini, a cost-effective model scoring 82% on the MMLU. Priced significantly lower than previous models, it supports a wide range of tasks and has a large context window. The model is designed for efficient and low-latency applications, with future support for multiple input and output types.*** <br>
   Jul 18, OpenAI [released GPT-4o mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/?utm_source=substack&utm_medium=email), scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo. GPT-4o mini enables a broad range of tasks with its low cost and latency, such as applications that chain or parallelize multiple model calls (e.g., calling multiple APIs), pass a large volume of context to the model (e.g., full code base or conversation history), or interact with customers through fast, real-time text responses (e.g., customer support chatbots).  GPT-4o mini supports text and vision in the API, with support for text, image, video and audio inputs and outputs coming in the future. The model has a context window of 128K tokens, supports up to 16K output tokens per request, and has knowledge up to October 2023. <br><br>

9. ***IBM and MIT on Benchmark Agreement Testing: <br>
IBM and MIT highlighted issues in Benchmark Agreement Testing (BAT) for LLMs and proposed best practices to ensure robust and valid evaluations. They introduced BenchBench, a tool for BAT, and demonstrated the importance of standardized procedures in improving the reliability of benchmark evaluations.*** <br>
    Jul 18, IBM and MIT published a [paper](https://arxiv.org/pdf/2407.13696) “Benchmark Agreement Testing Done Right: A Guide for LLM Benchmark Evaluation”. Recent advancements in Language Models (LMs) have catalyzed the creation of multiple benchmarks, designed to assess these models' general capabilities. A crucial task, however, is assessing the validity of the benchmarks themselves. This is most commonly done via Benchmark Agreement Testing (BAT), where new benchmarks are validated against established ones using some agreement metric (e.g., rank correlation). Despite the crucial role of BAT for benchmark builders and consumers, there are no standardized procedures for such agreement testing. This deficiency can lead to invalid conclusions, fostering mistrust in benchmarks and upending the ability to properly choose the appropriate benchmark to use. By analyzing over 40 prominent benchmarks, the authors demonstrate how some overlooked methodological choices can significantly influence BAT results, potentially undermining the validity of conclusions. To address these inconsistencies, the study proposes a set of best practices for BAT and demonstrate how utilizing these methodologies greatly improves BAT robustness and validity. To foster adoption and facilitate future research, the paper introduces BenchBench, a python package for BAT, and release the BenchBench-leaderboard, a meta-benchmark designed to evaluate benchmarks using their peers. Findings of the paper underscore the necessity for standardized BAT, ensuring the robustness and validity of benchmark evaluations in the evolving landscape of language model research. BenchBench Package: https://github.com/IBM/BenchBench Leaderboard: https://huggingface.co/spaces/per/BenchBench <br><br>

11. ***Open-Source LLMs in Biomedical Tasks: <br>
A study explored the performance of open-source LLMs like Mixtral 8x7B compared to commercial models in biomedical tasks. While competitive in few-shot settings, open-source models struggled in zero-shot scenarios. The research suggests that domain-specific few-shot examples can close the performance gap between commercial and open-source models.*** <br>
    Jul 18, Uni of Regensburg published a [paper](https://arxiv.org/pdf/2407.13511) “Can Open-Source LLMs Compete with Commercial Models? Exploring the Few-Shot Performance of Current GPT Models in Biomedical Tasks”. Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPT and Anthropic's Claude 3 Opus, have dominated natural language processing (NLP) benchmarks across different domains. New competing Open-Source alternatives like Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap while often offering higher throughput and being less costly to use. Open-Source LLMs can also be self-hosted, which makes them interesting for enterprise and clinical use cases where sensitive data should not be processed by third parties. The authors participated in the 12th BioASQ challenge, which is a retrieval augmented generation (RAG) setting, and explored the performance of current GPT models Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning (zero-shot, few-shot) and QLoRa fine-tuning. The study also explored how additional relevant knowledge from Wikipedia added to the context-window of the LLM might improve their performance. Mixtral 8x7b was competitive in the 10-shot setting, both with and without fine-tuning, but failed to produce usable results in the zero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead to measurable performance gains. The results indicate that the performance gap between commercial and open-source models in RAG setups exists mainly in the zero-shot setting and can be closed by simply collecting few-shot examples for domain-specific use cases. The code needed to rerun these experiments is available through [GitHub](https://github.com/SamyAteia/bioasq2024).  <br><br>

13. ***AI and Universal Basic Income: <br>
An article in Forbes discussed the impact of AI and robotics on job displacement, suggesting a shift towards human-AI collaboration. It proposes that universal basic income may become necessary as AI makes many traditional jobs obsolete, emphasizing the enduring need for human skills in the workforce.*** <br>
    Jul 17, Forbes published an [article](https://www.forbes.com/sites/jackkelly/2024/07/17/ai-robot-job-displacement-universal-basic-income/) “How AI And Robot Job Displacements Could Lead Us Down The Road Of Universal Basic Income And Loss Of Identity”. The article argues that in today’s evolving workplace landscape, AI and robotics are already automating tasks across almost all sectors, including manufacturing, data analysis, customer service and administration. As it stands, repetitive and routine tasks are the most susceptible to automation. While AI and robotics will undoubtedly change the nature of work, it's unlikely that these technologies will eradicate the existence of all jobs. The focus will likely shift toward human-AI collaboration and jobs requiring uniquely human skills. The future of work could involve a combination of paid employment, universal basic income and a renewed focus on finding meaning and fulfillment outside of traditional work structures. Many jobs require creativity, critical thinking, social skills, problem-solving under pressure and the ability to handle unforeseen situations. Because these are areas where AI is still limited, it demonstrates the need for continued human skills in job functions. Elon Musk stated this May in Pairs that AI will eventually make workers obsolete—a prediction he doesn’t necessarily see as pernicious. Highly advanced AI capabilities will dispel the need for human labor, rendering traditional jobs unnecessary, in what he frames as a likely "benign scenario" for the future of work. Musk sees people working simply out of personal interest or creative satisfaction. For work to become optional, we would need to live in an “age of abundance” achieved by "universal high income," Musk said. Geoffrey Hinton told BBC that universal basic income would need to be provided by the government to provide a safety net, if automation catalyzes widespread job displacement. <br><br>

15. ***Manipulating LLM Uncertainty: <br>
Research from Rutgers, NYU, and Meta investigated the fragility of uncertainty estimation in LLMs. They demonstrated that backdoor attacks could manipulate model uncertainty without changing the output, highlighting a significant threat to LLM reliability and the need for defenses against such vulnerabilities.*** <br>
    Jul 17, Rutgers Uni, NYU, Meta, et al. published a [paper](https://arxiv.org/pdf/2407.11282) “Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models”. Large Language Models (LLMs) are employed across various high-stakes domains, where the reliability of their outputs is crucial. One commonly used method to assess the reliability of LLMs' responses is uncertainty estimation, which gauges the likelihood of their answers being correct. While many studies focus on improving the accuracy of uncertainty estimations for LLMs, this research investigates the fragility of uncertainty estimation and explores potential attacks. The authors demonstrate that an attacker can embed a backdoor in LLMs, which, when activated by a specific trigger in the input, manipulates the model's uncertainty without affecting the final output. Specifically, the proposed backdoor attack method can alter an LLM's output probability distribution, causing the probability distribution to converge towards an attacker-predefined distribution while ensuring that the top-1 prediction remains unchanged. Experimental results demonstrate that this attack effectively undermines the model's self-evaluation reliability in multiple-choice questions. For instance, the study achieved a 100 attack success rate (ASR) across three different triggering strategies in four models. Further, the authors investigate whether this manipulation generalizes across different prompts and domains. This work highlights a significant threat to the reliability of LLMs and underscores the need for future defenses against such attacks. The code is available at https://github.com/qcznlp/uncertainty_attack. <br><br>

17. ***Goldfish for Long Video Understanding: <br>
KAUST, Harvard, and Swiss AI Lab introduced Goldfish, a methodology for understanding long videos. It uses efficient retrieval mechanisms and a new benchmark, TVQA-long, to improve comprehension of lengthy video content. Goldfish outperforms previous methods in both long and short video understanding.*** <br>
    Jul 17, KAUST, Harvard Uni and The Swiss AI Lab published a [paper](https://arxiv.org/pdf/2407.12679) “Goldfish: Vision-Language Understanding of Arbitrarily Long Videos”. Most current LLM-based models for video understanding can process videos within minutes. However, they struggle with lengthy videos due to challenges such as "noise and redundancy", as well as "memory and computation" constraints. This paper presents Goldfish, a methodology tailored for comprehending videos of arbitrary lengths. The work also introduces the TVQA-long benchmark, specifically designed to evaluate models' capabilities in understanding long videos with questions in both vision and text content. Goldfish approaches these challenges with an efficient retrieval mechanism that initially gathers the top-k video clips relevant to the instruction before proceeding to provide the desired response. This design of the retrieval mechanism enables the Goldfish to efficiently process arbitrarily long video sequences, facilitating its application in contexts such as movies or television series. To facilitate the retrieval process, the study developed MiniGPT4-Video that generates detailed descriptions for the video clips. In addressing the scarcity of benchmarks for long video evaluation, the paper adapted the TVQA short video benchmark for extended content analysis by aggregating questions from entire episodes, thereby shifting the evaluation from partial to full episode comprehension. The study attained a 41.78% accuracy rate on the TVQA-long benchmark, surpassing previous methods by 14.94%. The MiniGPT4-Video also shows exceptional performance in short video comprehension, exceeding existing state-of-the-art methods by 3.23%, 2.03%, 16.5% and 23.59% on the MSVD, MSRVTT, TGIF, and TVQA short video benchmarks, respectively. These results indicate that these models have significant improvements in both long and short-video understanding. The models and code have been made publicly available at https://vision-cair.github.io/Goldfish_website/ <br><br>

19. ***Foundation Model Transparency Index v1.1: <br>
A follow-up study from Stanford, Princeton, and MIT assessed the transparency of foundation model developers. The updated index shows improved transparency, with developers now scoring 58 out of 100 on average. The study highlights areas of ongoing opacity and suggests that increased transparency can be achieved through policy interventions.*** <br>
    Jul 17, Stanford Uni, Princeton Uni and MIT published a [paper](https://arxiv.org/pdf/2407.12929) “The Foundation Model Transparency Index v1.1: May 2024”. Foundation models are increasingly consequential yet extremely opaque. To characterize the status quo, the Foundation Model Transparency Index was launched in October 2023 to measure the transparency of leading foundation model developers. The October 2023 Index (v1.0) assessed 10 major foundation model developers (e.g. OpenAI, Google) on 100 transparency indicators (e.g. does the developer disclose the wages it pays for data labor?). At the time, developers publicly disclosed very limited information with the average score being 37 out of 100. To understand how the status quo has changed, the researchers conduct a follow-up study (v1.1) after 6 months: the report scores 14 developers against the same 100 indicators. While in v1.0 the authors searched for publicly available information, in v1.1 developers submit reports on the 100 transparency indicators, potentially including information that was not previously public. The report finds that developers now score 58 out of 100 on average, a 21 point improvement over v1.0. Much of this increase is driven by developers disclosing information during the v1.1 process: on average, developers disclosed information related to 16.6 indicators that was not previously public. The authors observe regions of sustained (i.e. across v1.0 and v1.1) and systemic (i.e. across most or all developers) opacity such as on copyright status, data access, data labor, and downstream impact. The authors publish transparency reports for each developer that consolidate information disclosures: these reports are based on the information disclosed to the authors via developers. The findings demonstrate that transparency can be improved in this nascent ecosystem, the Foundation Model Transparency Index likely contributes to these improvements, and policymakers should consider interventions in areas where transparency has not improved. <br><br>

21. ***AI revolutionises business document management: <br>
Professionals spend a significant amount of time searching for information, highlighting the need for better tools. Adobe’s AI Assistant, a generative AI conversational engine, integrates into document workflows to boost productivity. It helps generate high-quality insights and quickly create emails, reports, and presentations from various document types. In the legal sector, it summarizes judgments to save time for legal counsel. Wealth advisors also benefit from this technology by consolidating information for client advice. AI integration into workflows is becoming essential for businesses.*** <br>
    Jul 16, Financial Review published an [article](https://www.afr.com/technology/ai-revolutionises-business-document-management-20240711-p5jst3) “AI revolutionises business document management”. On average, professionals spend about 8.2 hours a week just searching for information within their documents. This inefficiency underscores the need for smarter, more effective tools to handle the deluge of data and documents that businesses process daily. Deeply integrated into document workflows, Adobe’s AI Assistant is a generative AI-powered conversational engine that can be deployed in minutes, unlocking new levels of document productivity for every knowledge worker across the enterprise. With AI Assistant, users can gain high-quality insights with intelligent citations and quickly generate emails, reports, presentations, and more from the information in their PDFs and other types of documents, including Word, PowerPoint, and meeting transcripts. One practical application of this technology is seen in the legal sector. The AI Assistant helps by summarising four or five judgments, enabling the legal counsel to quickly grasp the essentials and make informed decisions faster, saving them 30 minutes a day, and 2.5 hours a week. The AI Assistant also caters to wealth advisors who need to consolidate information from various reports to provide comprehensive advice to clients. The AI model is integrating AI capabilities into existing workflows, make AI a necessity, not a luxury. <br><br>

23. ***High Performance RWKV/Transformer and Extreme KV-Cache Compression:  <br>
The paper introduces GoldFinch, a hybrid model combining Linear Attention and Transformer techniques to efficiently generate a compressed KV-Cache. GoldFinch enhances performance relative to previous models and saves cache size significantly. Despite the complexity of autoregressive generation, the pre-fill computation remains efficient due to the use of RNNs. The trained weights and codes are released for community use.*** <br>
    Jul 16, EleutherAI et al. published a [paper](https://arxiv.org/pdf/2407.12077) “GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression”. The paper introduces GoldFinch, a hybrid Linear Attention/Transformer sequence model that uses a new technique to efficiently generate a highly compressed and reusable KV-Cache in linear time and space with respect to sequence length. GoldFinch stacks a new GOLD transformer on top of an enhanced version of the Finch (RWKV-6) architecture. The authors train up to 1.5B parameter class models of the Finch, Llama, and GoldFinch architectures, and find dramatically improved modeling performance relative to both Finch and Llama. The cache size savings increase linearly with model layer count, ranging from 756-2550 times smaller than the traditional transformer cache for common sizes, enabling inference of extremely large context lengths even on limited hardware. Although autoregressive generation has O(n) time complexity per token because of attention, pre-fill computation of the entire initial cache state for a submitted context costs only O(1) time per token due to the use of a recurrent neural network (RNN) to generate this cache. Trained weights and training [codes](https://github.com/recursal/GoldFinch-paper) are released under the Apache 2.0 license for community use. <br><br>

25. ***A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval: <br>
Existing benchmarks focus on simple queries, but BRIGHT addresses the need for benchmarks requiring deep reasoning. BRIGHT includes 1,398 real-world queries across various domains and shows that current retrieval models perform poorly on it. The use of Chain-of-Thought reasoning improves performance. BRIGHT is robust against data leakage and encourages research on more challenging retrieval tasks. Code and data are publicly available.*** <br>
    Jul 16, The Uni of Hong Kong, Princeton Uni, Uni of Washington and Google published a [paper](https://arxiv.org/pdf/2407.12883) “BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval”. Existing retrieval benchmarks primarily consist of information-seeking queries (e.g., aggregated questions from search engines) where keyword or semantic-based retrieval is usually sufficient. However, many complex real-world queries require in-depth reasoning to identify relevant documents that go beyond surface form matching. For example, finding documentation for a coding question requires understanding the logic and syntax of the functions involved. To better benchmark retrieval on such challenging queries, the paper introduces BRIGHT, the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398 real-world queries collected from diverse domains (such as economics, psychology, robotics, software engineering, earth sciences, etc.), sourced from naturally occurring or carefully curated human data. Extensive evaluation reveals that even state-of-the-art retrieval models perform poorly on BRIGHT. The leading model on the MTEB leaderboard, which achieves a score of 59.0 nDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. The authors further demonstrate that augmenting queries with Chain-of-Thought reasoning generated by large language models (LLMs) improves performance by up to 12.2 points. Moreover, BRIGHT is robust against data leakage during pretraining of the benchmarked models as the authors validate by showing similar performance even when documents from the benchmark are included in the training data. BRIGHT paves the way for future research on retrieval systems in more realistic and challenging settings. Code and data are available at https://brightbenchmark.github.io. <br><br>

27. ***All Large Language Models can be Fully Sparsely-Activated: <br>
Q-Sparse enables efficient training of sparsely-activated large language models (LLMs) through top-K sparsification and the straight-through-estimator. It achieves comparable results to baseline LLMs with significant efficiency gains in inference. Q-Sparse is versatile across different training settings and is effective for both full-precision and 1-bit LLMs, promising to revolutionize future LLM efficiency.*** <br>
    Jul 15, Microsoft published a [paper](https://arxiv.org/pdf/2407.10969) “Q-Sparse: All Large Language Models can be Fully Sparsely-Activated”.  The paper introduces, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) the paper presents an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different settings, including training-from-scratch, continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the cornerstone and a clear path to revolutionize the efficiency, including cost and energy consumption, of future LLMs. <br><br>

29. ***Taming Large Language Models for Better Automatic Evaluation: <br>
FLAMe, a family of autorater models, enhances the evaluation of LLMs by training on a diverse set of quality assessment tasks. FLAMe outperforms proprietary models like GPT-4 on many tasks and serves as a robust starting point for fine-tuning. It is also more efficient and less biased than other models, excelling in identifying high-quality responses for various tasks.*** <br>
    Jul 15, Google published a [paper](https://arxiv.org/pdf/2407.10817) “Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation”. As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their output due to the high costs of human evaluation. To make progress towards better LLM autoraters, the paper introduces FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on Google’s large and diverse collection of 100+ quality assessment tasks comprising 5M+ human judgments, curated and standardized using publicly released human evaluations from previous research. FLAMe significantly improves generalization to a wide variety of held-out tasks, outperforming LLMs trained on proprietary data like GPT-4 and Claude-3 on many tasks. The study shows that FLAMe can also serve as a powerful starting point for further downstream fine-tuning, using reward modeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, the FLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative model trained exclusively on permissively licensed data, outperforming both GPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, the study explores a more computationally efficient approach using a novel tail-patch fine-tuning strategy to optimize the FLAMe multitask mixture for reward modeling evaluation (FLAMe-Opt-RM), offering competitive RewardBench performance while requiring approximately 25x less training datapoints. Overall, the FLAMe variants outperform all popular proprietary LLM-as-a-Judge models considered across 8 out of 12 autorater evaluation benchmarks, encompassing 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, analysis reveals that FLAMe is significantly less biased than these LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark, while effectively identifying high-quality responses for code generation.  <br><br>

31. ***How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients: <br>
The paper studies low-rank structures in LLMs and introduces WeLore for weight compression and memory-efficient fine-tuning. WeLore leverages gradient dynamics to identify suitable rank reduction ratios, enabling significant performance and efficiency improvements. The technique outperforms full finetuning while reducing memory and computational requirements, making it a powerful tool for optimizing LLMs.*** <br>
    Jul 15, Uni of Texas at Austin Uni of Oxford, Meta, et al published a [paper](https://arxiv.org/pdf/2407.11239) “From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients”. Modern Large Language Models (LLMs) are composed of matrices with billions of elements, making their storage and processing quite demanding in terms of computational resources and memory usage. Being significantly large, such matrices can often be expressed in low-rank format with potential to relax resource requirements. Unlike prior works which focus on developing novel matrix decomposition algorithms, this work first studies the emergence of low-rank structures across matrices within different layers of LLMs and establish a consequential relationship between the gradient dynamics and emerging low-rank expressiveness of matrices. Findings reveal that different layers exhibit varying levels of converged low-rank structure, necessitating a non-uniform rank reduction across them to minimize performance drop due to compression. In view of that, the paper presents Weight Low-Rank Projection (WeLore) that unifies weight compression and memory-efficient fine-tuning as ONE, in a data-agnostic and one-shot way. WeLore capitalizes the heavy-tail distribution of singular values to identify a suitable rank reduction ratio for matrices within LLMs. Going beyond only as a compression technique, WeLore categorizes weight matrices into Low-rank Components (LRCs) and Non-Low-rank Components (N-LRCs) based on their ability to express themselves as low-rank. The gradient perspective and extensive experiments illustrate that LRCs tend to have better finetuning capabilities and can closely mimic (sometimes outperform) the training loss trajectory and performance of full-finetuning with notable memory and compute footprint reduction. For example, finetuning a 50% compressed LLaMa-2 7B model using only a fraction of parameters in LRCs (WeLore) can outperform its full finetuning with ~3x better throughput and ~0.6x GPU requirement. The codes are available at https://github.com/VITA-Group/welore <br><br>

33. ***Practical Unlearning for Large Language Models: <br>
Addressing security issues in LLMs, the O3 framework provides a solution for machine unlearning without compromising model utility. It includes an OOD detector and an Orthogonal LoRA for continuous unlearning. O3 smartly decides on unlearning actions during inference and shows superior performance across various tasks and datasets, especially with continuous unlearning requests.*** <br>
    Jul 14, Northwestern Uni and Arizona State Uni published a [paper](https://arxiv.org/pdf/2407.10223) “Practical Unlearning for Large Language Models”. While LLMs have demonstrated impressive performance across various domains and tasks, their security issues have become increasingly severe. Machine unlearning (MU) has emerged as a promising solution to address these issues by removing the influence of undesired data on the target model without compromising its utility in other aspects. MU typically assumes full access to the original training data to preserve utility, which is difficult to achieve in LLM unlearning. Existing LLM unlearning methods often assume access to data most affected by undesired data unlearning. However, this assumption underestimates the entanglement among various LLM capabilities and ignores data access limitations due to various issues. Moreover, these LLM unlearning methods do not sufficiently consider that unlearning requests in real-world scenarios are continuously emerging. To overcome these challenges and achieve practical LLM unlearning, the authors propose the O3 framework. The O3 framework includes an Out-Of-Distribution (OOD) detector to measure the similarity between input and unlearning data, and an Orthogonal low-rank adapter (LoRA) for continuously unlearning requested data. The OOD detector is trained with a novel contrastive entropy loss and utilizes a local-global layer-aggregated scoring mechanism. The orthogonal LoRA achieves parameter disentanglement among continual unlearning requests. During inference, the O3 framework can smartly decide whether and to what extent to load the unlearning LoRA based on the OOD detector's predictions. Notably, O3's effectiveness does not rely on any retained data. The authors conducted extensive experiments on O3 and state-of-the-art LLM unlearning methods across three tasks and seven datasets. The results indicate that O3 consistently achieves the best trade-off between unlearning effectiveness and utility preservation, especially when facing continuous unlearning requests. <br><br>

35. ***Vocabulary used by ChatGPT in academic writing:  <br>
The paper examines LLM usage in academic writing, identifying a significant impact on scientific literature. By analyzing vocabulary changes in PubMed abstracts, the study estimates that at least 10% of 2024 abstracts involved LLMs. This impact varies across disciplines and regions, with notable increases in certain fields. The study highlights the widespread adoption of LLMs in academia.*** <br>
    Jul 3, researchers from Uni of Tubingen and Northwestern Uni published a [paper](https://arxiv.org/pdf/2406.07016) “Delving into ChatGPT usage in academic writing through excess vocabulary”. Recent large language models (LLMs) can generate and revise text with human-level performance, and have been widely commercialized in systems like ChatGPT. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists have been using them to assist their scholarly writing. How wide-spread is LLM usage in the academic literature currently? To answer this question, the work uses an unbiased, large-scale approach, free from any assumptions on academic LLM usage. The authors study vocabulary changes in 14 million PubMed abstracts from 2010-2024, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. The analysis based on excess words usage suggests that at least 10% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, and was as high as 30% for some PubMed sub-corpora. The paper shows that the appearance of LLM-based writing assistants has had an unprecedented impact in the scientific literature, surpassing the effect of major world events such as the Covid pandemic. The paper also shows the six most widely used words by ChatGPT: delves, crucial, potential, these, significant, and important. <br><br>

37. ***Self-replicating Programs Emerge from Simple Interaction: <br>
The study explores how self-replicating programs arise from simple computational interactions. It demonstrates that self-replicators emerge in environments without explicit fitness landscapes due to random interactions and self-modification. The findings contribute to understanding the dynamics of self-replication and complex behavior emergence in computational substrates.*** <br>
    Jun 27, Google and Uni of Chicago published a [paper](https://arxiv.org/pdf/2406.19108) “Computational Life: How Well-formed, Self-replicating Programs Emerge from Simple Interaction”. The fields of Origin of Life and Artificial Life both question what life is and how it emerges from a distinct set of "pre-life" dynamics. One common feature of most substrates where life emerges is a marked shift in dynamics when self-replication appears. While there are some hypotheses regarding how self-replicators arose in nature, people know very little about the general dynamics, computational principles, and necessary conditions for self-replicators to emerge. This is especially true on "computational substrates" where interactions involve logical, mathematical, or programming rules. This paper takes a step towards understanding how self-replicators arise by studying several computational substrates based on various simple programming languages and machine instruction sets. The authors show that when random, non self-replicating programs are placed in an environment lacking any explicit fitness landscape, self-replicators tend to arise. The study demonstrates how this occurs due to random interactions and self-modification, and can happen with and without background random mutations. The paper also shows how increasingly complex dynamics continue to emerge following the rise of self-replicators. Finally, the authors show a counterexample of a minimalistic programming language where self-replicators are possible, but so far have not been observed to arise.
 <br><br>

***14 Jul 2024***

1. ***OpenAI's Novel Approach with "Strawberry"<br>
OpenAI is developing "Strawberry," a novel post-training method to enhance AI reasoning, enabling complex long-horizon tasks and deep internet research. The project aims to address common sense issues in current AI models and improve their autonomous research capabilities.*** <br><br>
   Jul 13, according to [Reuters](https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/), ChatGPT maker OpenAI is working on a novel approach to its artificial intelligence models in a project code-named “Strawberry,” according to a person familiar with the matter and internal documentation reviewed by Reuters. The project, details of which have not been previously reported, comes as the Microsoft-backed startup races to show that the types of models it offers are capable of delivering advanced reasoning capabilities. There is a project that uses Strawberry models with the aim of enabling the company’s AI to not just generate answers to queries but to plan ahead enough to navigate the internet autonomously and reliably to perform what OpenAI terms “deep research,” according to the source. Two sources described viewing earlier this year what OpenAI staffers told them were Q* demos, capable of answering tricky science and math questions out of reach of today’s commercially-available models. OpenAI hopes the innovation will improve its AI models’ reasoning capabilities dramatically, the person familiar with it said, adding that Strawberry involves a specialized way of processing an AI model after it has been pre-trained on very large datasets. While large language models can already summarize dense texts and compose elegant prose far more quickly than any human, the technology often falls short on common sense problems whose solutions seem intuitive to people, like recognizing logical fallacies and playing tic-tac-toe. When the model encounters these kinds of problems, it often “hallucinates” bogus information. Strawberry includes a specialized way of what is known as “post-training” OpenAI’s generative AI models, or adapting the base models to hone their performance in specific ways after they have already been “trained” on reams of generalized data, one of the sources said. Among the capabilities OpenAI is aiming Strawberry at is performing long-horizon tasks (LHT), the document says, referring to complex tasks that require a model to plan ahead and perform a series of actions over an extended period of time, the first source explained.
 <br><br>
3. ***Progress Towards Artificial General Intelligence (AGI)<br>
OpenAI tracks its progress towards AGI using an internal scale from Level 1 to Level 5. Current models are at Level 1, with Level 2 near completion, aiming for human PhD-level problem-solving. Achieving AGI will require massive computing power and time, with predictions varying widely on when it will be achieved.*** <br><br>
   Jul 12, according to [theverge.com](https://www.theverge.com/2024/7/11/24196746/heres-how-openai-will-determine-how-powerful-its-ai-systems-are?utm_source=substack&utm_medium=email), OpenAI has created an internal scale to track the progress its large language models are making toward artificial general intelligence, or AI with human-like intelligence. Today’s chatbots, like ChatGPT, are at Level 1. OpenAI claims it is nearing Level 2, defined as a system that can solve basic problems at the level of a person with a PhD. Level 3 refers to AI agents capable of taking actions on a user’s behalf. Level 4 involves AI that can create new innovations. Level 5, the final step to achieving AGI, is AI that can perform the work of entire organizations of people. OpenAI has previously defined AGI as “a highly autonomous system surpassing humans in most economically valuable tasks.” OpenAI’s unique structure is centered around its mission of achieving AGI, and how OpenAI defines AGI is important. Still, AGI is still quite a ways away: it will take billions upon billions of dollars worth of computing power to reach AGI, if at all. Timelines from experts, and even at OpenAI, vary wildly. In October 2023, OpenAI CEO Sam Altman said we are “five years, give or take,” before reaching AGI. OpenAI hasn’t provided details on how it assigns models to these internal. However, company leaders demonstrated a research project using the GPT-4 AI model during an all-hands meeting on Thursday and believe this project showcases some new skills that exhibit human-like reasoning. 

5. ***Advancements in Attention Mechanisms<br>
A paper by Colfax Research and collaborators introduces "FlashAttention-3," improving attention mechanisms in Transformer models. By leveraging new GPU capabilities and optimizing computation, the method achieves significant speedup and accuracy, making it more efficient for large language models.*** <br><br>
   Jul 11, Colfax Research, Meta, Nvidia, Georgia Tech, Princeton Uni, and TogetherAI published a [paper](https://arxiv.org/pdf/2407.08608v1) “FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision”. Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU. The work develops three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. The paper demonstrates that FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0× with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. The paper validates that FP8 FlashAttention-3 achieves 2.6× lower numerical error than a baseline FP8 attention.

7. ***Concerns Over Safety and Profit at OpenAI<br>
Former OpenAI employee William Saunders criticizes the company for prioritizing profit over safety. His resignation highlights concerns about the potential risks of developing AGI without adequate safety measures, drawing parallels to the Titanic's insufficient lifeboats.*** <br><br>
   Jul 11, according to [furturism.com](https://futurism.com/openai-researcher-quit-realized-upsetting-truth), A former OpenAI worker says he quit the company after realizing that it was putting safety on the back burner to pursue profit. "I really didn't want to end up working for the Titanic of AI, and so that's why I resigned," former Superalignment team member William Saunders said. "During my three years at OpenAI, I would sometimes ask myself a question. Was the path that OpenAI was on more like the Apollo program or more like the Titanic?" Saunders argued that the Titanic may have been called "unsinkable, but at the same time there weren't enough lifeboats for everyone and so when disaster struck, a lot of people died." His comments highlight growing concerns over companies like OpenAI developing AI systems that are capable of superseding the abilities of humans, an idea dubbed artificial general intelligence (AG) — something that currently remains entirely theoretical, but is a source of great interest to executives like OpenAI's Sam Altman. "Even when big problems happened, like Apollo 13, they had enough sort of like redundancy, and were able to adapt to the situation in order to bring everyone back safely," "It is not possible to develop AGI or any new technology with zero risk," he added in an email to Business Insider after the publication pointed out that the Apollo program saw its own fair share of safety oversights. "What I would like to see is the company taking all possible reasonable steps to prevent these risks." Saunders' comments paint a worrying picture of how OpenAI is being run and the changes Altman has made over the last couple of years.

9. ***Efficient Training with Q-GaLore<br>
Researchers introduced "Q-GaLore," a method combining quantization and low-rank projection to reduce memory usage in training large language models. This approach significantly cuts memory consumption while maintaining performance, enabling efficient training on less powerful hardware.*** <br><br>
    Jul 11, Uni of Texas at Austin, Uni of Oxford, Meat et al. published a [paper](https://arxiv.org/pdf/2407.08296) “Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients”. Training Large Language Models (LLMs) is memory-intensive due to the large number of parameters and associated optimization states. GaLore, a recent method, reduces memory usage by projecting weight gradients into a low-rank subspace without compromising performance. However, GaLore relies on time-consuming Singular Value Decomposition (SVD) operations to identify the subspace, and the frequent subspace updates lead to significant training time overhead. Moreover, GaLore offers minimal improvements in accuracy and efficiency compared to LoRA in more accessible fine-tuning scenarios. To address these limitations, the paper introduces Q-Galore, a novel approach that substantially reduces memory usage by combining quantization and low-rank projection, surpassing the benefits of GaLore. The proposed method is based on two key observations: (i) the gradient subspace exhibits diverse properties, with some layers converging early in training while others are subject to frequent changes; (ii) the projection matrices are highly resilient to low-bit quantization. Leveraging these insights, Q-GaLore adaptively updates the gradient subspace based on its convergence statistics, achieving comparable performance while significantly reducing the number of SVD operations. The study maintains the projection matrices in INT4 format and weights in INT8 format, incorporating stochastic rounding to capture accumulated gradient information. This approach enables a high-precision training trajectory using only low-precision weights. The study demonstrates that Q-GaLore achieves highly competitive performance with exceptional memory efficiency. At pre-training, Q-GaLore facilitates training a LLaMA-7B model from scratch on a single NVIDIA RTX 4060 Ti with only 16 GB memory. At fine-tuning, it reduces memory consumption by up to 50% compared to LoRA and GaLore, while consistently outperforming QLoRA at the same memory cost.

11. ***Teaching Transformers Causal Reasoning<br>
Microsoft and MIT propose an axiomatic training method to teach transformers causal reasoning from passive data. The study demonstrates that models can generalize causal reasoning to new scenarios, matching or surpassing the performance of larger models.*** <br><br>
    Jun 10, Microsoft and MIT published a [paper](https://arxiv.org/pdf/2407.07612) “Teaching Transformers Causal Reasoning through Axiomatic Training”. For text-based AI systems to interact in the real world, causal reasoning is an essential skill. Since interventional data is costly to generate, the word studies to what extent an agent can learn causal reasoning from passive data. Specifically, the paper considers an axiomatic training setup where an agent learns from multiple demonstrations of a causal axiom (or rule), rather than incorporating the axiom as an inductive bias or inferring it from data values. A key question is whether the agent would learn to generalize from the axiom demonstrations to new scenarios. For example, if a transformer model is trained on demonstrations of the causal transitivity axiom over small graphs, would it generalize to applying the transitivity axiom over large graphs? Experimental results, based on a novel axiomatic training scheme, indicate that such generalization is possible. The paper considers the task of inferring whether a variable causes another variable, given a causal graph structure. The paper finds that a 67 million parameter transformer model, when trained on linear causal chains (along with some noisy variations) can generalize well to new kinds of graphs, including longer causal chains, causal chains with reversed order, and graphs with branching; even when it is not explicitly trained for such settings. The model performs at par (or even better) than many larger language models such as GPT-4, Gemini Pro, and Phi-3. Overall, the axiomatic training framework provides a new paradigm of learning causal reasoning from passive data that can be used to learn arbitrary axioms, as long as sufficient demonstrations can be generated.

6. ***Detecting Contextual Hallucinations<br>
A new method called "Lookback Lens" detects and mitigates hallucinations in large language models by analyzing attention weights. This approach effectively reduces hallucinations across various tasks and models, enhancing the accuracy of generated content.*** <br><br>
   Jul 9, MIT and Uni of Washington published a [paper](https://arxiv.org/pdf/2407.07071) “Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps”. When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such contextual hallucinations. The authors hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, the work proposes a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). The study finds that a linear classifier based on these lookback ratio features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector -- Lookback Lens -- is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. The authors further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.

8. ***Framework for Composable Interventions<br>
A paper introduces a framework for combining interventions in language models to improve factual accuracy and mitigate harmful outputs. The study reveals the interactions between different interventions, emphasizing the need for new multi-objective methods.*** <br><br>
   Jul 9, Uni of Virginia, EleutherAI, Microsoft Harvard Uni, Uni of Oxford et al. published a [paper](https://arxiv.org/pdf/2407.06483) “Composable Interventions for Language Models”. Test-time interventions for language models can enhance factual accuracy, mitigate harmful outputs, and improve model efficiency without costly retraining. But despite a flood of new methods, different types of interventions are largely developing independently. In practice, multiple interventions must be applied sequentially to the same model, yet we lack standardized ways to study how interventions interact. The paper fills this gap by introducing composable interventions, a framework to study the effects of using multiple interventions on the same language models, featuring new metrics and a unified codebase. Using the framework, the paper conducts extensive experiments and composes popular methods from three emerging intervention categories—knowledge editing, model compression, and machine unlearning. Experimental results from 310 different compositions uncover meaningful interactions: compression hinders editing and unlearning, composing interventions hinges on their order of application, and popular general-purpose metrics are inadequate for assessing composability. Taken together, the findings showcase clear gaps in composability, suggesting a need for new multi-objective interventions.

10. ***Lawyers' Perceptions of AI-Generated Documents<br>
A study shows that lawyers prefer documents perceived as human-crafted over AI-generated ones, despite expecting future AI involvement in legal document generation. This perception could influence the adoption of AI in legal processes.*** <br><br>
    Jul 9, Masaryk Uni and CMU published a [paper](https://www.arxiv.org/pdf/2407.06798) “It Cannot Be Right If It Was Written by AI: On Lawyers' Preferences of Documents Perceived as Authored by an LLM vs a Human”. Large Language Models (LLMs) enable a future in which certain types of legal documents may be generated automatically. This has a great potential to streamline legal processes, lower the cost of legal services, and dramatically increase access to justice. While many researchers focus their efforts on proposing and evaluating LLM-based applications supporting tasks in the legal domain, there is a notable lack of investigations into how legal professionals perceive content if they believe it has been generated by an LLM. Yet, this is a critical point as over-reliance or unfounded skepticism may influence whether such documents bring about appropriate legal consequences. This study is the necessary analysis in the context of the ongoing transition towards mature generative AI systems. Specifically, the paper examined whether the perception of legal documents' by lawyers (n=75) varies based on their assumed origin (human-crafted vs AI-generated). The participants evaluated the documents focusing on their correctness and language quality. The analysis revealed a clear preference for documents perceived as crafted by a human over those believed to be generated by AI. At the same time, most of the participants are expecting the future in which documents will be generated automatically. These findings could be leveraged by legal practitioners, policy makers and legislators to implement and adopt legal document generation technology responsibly, and to fuel the necessary discussions into how legal processes should be updated to reflect the recent technological developments.

12. ***Fallback Behaviors of Language Models<br>
Tel Aviv University research categorizes fallback behaviors in language models, showing that advanced models shift from sequence repetitions to hallucinations under uncertainty. The study highlights the need for improved techniques to manage these behaviors.*** <br><br>
    Jul 8, Tel Aviv Uni published a [paper](https://arxiv.org/pdf/2407.06071) “From Loops to Oops: Fallback Behaviors of Language Models Under Uncertainty”. Large language models (LLMs) often exhibit undesirable behaviors, such as hallucinations and sequence repetitions. The paper proposes to view these behaviors as fallbacks that models exhibit under uncertainty, and investigate the connection between them. The work categorizes fallback behaviors -- sequence repetitions, degenerate text, and hallucinations -- and extensively analyzes them in models from the same family that differ by the amount of pretraining tokens, parameter count, or the inclusion of instruction-following training. The experiments reveal a clear and consistent ordering of fallback behaviors, across all these axes: the more advanced an LLM is (i.e., trained on more tokens, has more parameters, or instruction-tuned), its fallback behavior shifts from sequence repetitions, to degenerate text, and then to hallucinations. Moreover, the same ordering is observed throughout a single generation, even for the best-performing models; as uncertainty increases, models shift from generating hallucinations to producing degenerate text and then sequence repetitions. Lastly, the paper demonstrates that while common decoding techniques, such as random sampling, might alleviate some unwanted behaviors like sequence repetitions, they increase harder-to-detect hallucinations.

14. ***LMs and Vision Models Conceptual Alignment<br>
The study by the University of Copenhagen and MBZU evaluates the conceptual alignment between large-scale pretrained language models (LMs) and vision models. Contrary to the claim that LMs lack the ability to connect utterances to the world, the experiments show partial convergence in representations between LMs and vision models. This has significant implications for multi-modal processing and the understanding of LMs.*** <br><br>
    Jul 6, Uni of Copenhagen and MBZU published a [paper](https://arxiv.org/pdf/2302.06555) “Do Vision and Language Models Share Concepts? A Vector Space Alignment Study”. Large-scale pretrained language models (LMs) are said to `lack the ability to connect utterances to the world’ (Bender and Koller, 2020), because they do not have ‘mental models of the world’ (Mitchell and Krakauer, 2023). If so, one would expect LM representations to be unrelated to representations induced by vision models. The paper presents an empirical evaluation across four families of LMs (BERT, GPT-2, OPT and LLaMA-2) and three vision model architectures (ResNet, SegFormer, and MAE). The experiments show that LMs partially converge towards representations isomorphic to those of vision models, subject to dispersion, polysemy and frequency. This has important implications for both multi-modal processing and the LM understanding debate (Mitchell and Krakauer, 2023).

16. ***Google Study on Scalable Oversight<br>
Google's paper explores scalable oversight protocols to enable accurate human supervision of superhuman AI. Comparing debate, consultancy, and direct question-answering, the study finds that debate generally outperforms consultancy and direct QA in specific tasks, particularly those with information asymmetry. Allowing AI agents to choose their stance in debates reduces the chances of judges being convinced by incorrect answers.*** <br><br>
    Jul 5, Google published a [paper](https://arxiv.org/pdf/2407.04622) “On scalable oversight with weak LLMs judging strong LLMs”. Scalable oversight protocols aim to enable humans to accurately supervise superhuman AI. This paper studies debate, where two AI's compete to convince a judge; consultancy, where a single AI tries to convince a judge that asks questions; and compare to a baseline of direct question-answering, where the judge just answers outright without the AI. The authors use large language models (LLMs) as both AI agents and as stand-ins for human judges, taking the judge models to be weaker than agent models. The authors benchmark on a diverse range of asymmetries between judges and agents, extending previous work on a single extractive QA task with information asymmetry, to also include mathematics, coding, logic and multimodal reasoning asymmetries. The paper finds that debate outperforms consultancy across all tasks when the consultant is randomly assigned to argue for the correct/incorrect answer. Comparing debate to direct question answering, the results depend on the type of task: in extractive QA tasks with information asymmetry debate outperforms direct question answering, but in other tasks without information asymmetry the results are mixed. Previous work assigned debaters/consultants an answer to argue for. When the authors allow them to instead choose which answer to argue for, the study finds judges are less frequently convinced by the wrong answer in debate than in consultancy. Further, the paper finds that stronger debater models increase judge accuracy, though more modestly than in previous studies.

18. ***New RNN Layer with Linear Complexity<br>
Stanford, UC San Diego, UC Berkeley, and Meta propose a new sequence modeling layer with linear complexity and an expressive hidden state, named Test-Time Training (TTT) layers. TTT layers update the hidden state via self-supervised learning even during test sequences. The study shows that TTT-Linear and TTT-MLP models outperform traditional RNNs and Transformers in handling long contexts, pointing towards future research potential.*** <br><br>
    Jul 5, Stanford Uni., UC San Diego, UC Berkeley and Meta published a [paper](https://arxiv.org/pdf/2407.04620) “Learning to (Learn at Test Time): RNNs with Expressive Hidden States”. Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. The authors propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, the layers are called Test-Time Training (TTT) layers. The researchers consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. The study evaluates the instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.

20. ***MJ-Bench for Evaluating Multimodal Judges<br>
A collaborative study introduces MJ-Bench to evaluate multimodal judges used for text-to-image generation models. It benchmarks judges on alignment, safety, image quality, and bias, revealing that closed-source VLMs like GPT-4o provide superior feedback. The findings suggest that VLM judges offer more accurate and stable feedback in natural language compared to numerical scales.*** <br><br>
    Jul 5, UNC-Chapel Hill, Uni of Chicago, Stanford Uni, Duke Uni and 13 institutes published a [paper](https://arxiv.org/pdf/2407.04842) “MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?”. While text-to-image models like DALLE-3 and Stable Diffusion are rapidly proliferating, they often encounter challenges such as hallucination, bias, and the production of unsafe, low-quality output. To effectively address these issues, it is crucial to align these models with desired behaviors based on feedback from a multimodal judge. Despite their significance, current multimodal judges frequently undergo inadequate evaluation of their capabilities and limitations, potentially leading to misalignment and unsafe fine-tuning outcomes. To address this issue, the paper introduces MJ-Bench, a novel benchmark which incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. Specifically, the paper evaluates a large variety of multimodal judges including smaller-sized CLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and close-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of a preference dataset. Experiments reveal that close-source VLMs generally provide better feedback, with GPT-4o outperforming other judges in average. Compared with open-source VLMs, smaller-sized scoring models can provide better feedback regarding text-image alignment and image quality, while VLMs provide more accurate feedback regarding safety and generation bias due to their stronger reasoning capabilities. Further studies in feedback scale reveal that VLM judges can generally provide more accurate and stable feedback in natural language (Likert-scale) than numerical scales. Notably, human evaluations on end-to-end fine-tuned models using separate feedback from these multimodal judges provide similar conclusions, further confirming the effectiveness of MJ-Bench. All data, code, models are available at https://huggingface.co/MJ-Bench.

22. ***SWiM Framework for Long Context Models<br>
Snorkel AI and the University of Wisconsin-Madison propose SWiM, an evaluation framework for long context models in LLMs. The study identifies the "lost-in-the-middle" effect where performance degrades with information in the middle of the context window. It introduces medoid voting as an effective training-free method to mitigate this effect, showing a significant improvement in single document QA tasks.*** <br><br>
    Jul 4, Snorkel AI and Uni of Wisconsin-Madison published a [paper](https://arxiv.org/pdf/2407.03651) “Evaluating Language Model Context Windows: A "Working Memory" Test and Inference-time Correction”. Large language models are prominently used in real-world applications, often tasked with reasoning over large volumes of documents. An exciting development in this space is models boasting extended context capabilities, with some accommodating over 2 million tokens. Such long context model capabilities remain uncertain in production systems, motivating the need to benchmark their performance on real world use cases. The paper addresses this challenge by proposing SWiM, an evaluation framework that addresses the limitations of standard tests. Testing the framework on eight long context models, the paper finds that even strong models such as GPT-4 and Claude 3 Opus degrade in performance when information is present in the middle of the context window (lost-in-the-middle effect). Next, in addition to the benchmark, the paper proposes medoid voting, a simple, but effective training-free approach that helps alleviate this effect, by generating responses a few times, each time randomly permuting documents in the context, and selecting the medoid answer. The authors evaluate medoid voting on single document QA tasks, achieving up to a 24% lift in accuracy.

24. ***ChartGemma for Chart Understanding<br>
York University, MILA, Salesforce, and Nanyang Technological University present ChartGemma, a model for understanding and reasoning with charts. Trained on instruction-tuning data from chart images, ChartGemma captures visual trends and patterns, achieving state-of-the-art results in chart summarization, question answering, and fact-checking. The model offers realistic and factually correct summaries of real-world charts.*** <br><br>
    Jul 4, York Uni., MILA, Saleforce, and Nanyang Tech Uni. published a [paper](https://arxiv.org/pdf/2407.04172) “ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild”. Given the ubiquity of charts as a data analysis, visualization, and decision-making tool across industries and sciences, there has been a growing interest in developing pre-trained foundation models as well as general purpose instruction-tuned models for chart understanding and reasoning. However, existing methods suffer crucial drawbacks across two critical axes affecting the performance of chart representation models: they are trained on data generated from underlying data tables of the charts, ignoring the visual trends and patterns in chart images, and use weakly aligned vision-language backbone models for domain-specific training, limiting their generalizability when encountering charts in the wild. The paper addresses these important drawbacks and introduces ChartGemma, a novel chart understanding and reasoning model developed over PaliGemma. Rather than relying on underlying data tables, ChartGemma is trained on instruction-tuning data generated directly from chart images, thus capturing both high-level trends and low-level visual information from a diverse set of charts. The simple approach achieves state-of-the-art results across 5 benchmarks spanning chart summarization, question answering, and fact-checking, and the elaborate qualitative studies on real-world charts show that ChartGemma generates more realistic and factually correct summaries compared to its contemporaries. The authors release the code, model checkpoints, dataset, and demos at https://github.com/vis-nlp/ChartGemma.

26. ***BM25S: Efficient Lexical Search<br>
McGill University introduces BM25S, an efficient implementation of BM25 for lexical search that achieves up to 500x speedup by precomputing and storing BM25 scores in sparse matrices. This approach significantly outperforms existing Python and Java-based implementations, although it may encounter performance limitations and out-of-memory errors.*** <br><br>
    Jul 4, McGill Uni published a [paper](https://arxiv.org/pdf/2407.03618) “BM25S: Orders of magnitude faster lexical search via eager sparse scoring”. The paper introduces BM25S, an efficient Python-based implementation of BM25 that only depends on Numpy and Scipy. BM25S achieves up to a 500x speedup compared to the most popular Python-based framework by eagerly computing BM25 scores during indexing and storing them into sparse matrices. It also achieves considerable speedups compared to highly optimized Java-based implementations, which are used by popular commercial products. Finally, BM25S reproduces the exact implementation of five BM25 variants based on Kamphuis et al. (2020) by extending eager scoring to non-sparse variants using a novel score shifting method. Limitations of the work include it may not achieve the highest possible performance, and may terminate with OOM errors. The code can be found at https://github.com/xhluca/bm25s

28. ***Holistic Evaluation of Multimodal Models<br>
CMU's paper on HEMM proposes a framework for evaluating multimodal foundation models across basic skills, information flow, and real-world use cases. The study identifies challenges and trends in multimodal modeling, offering insights into the impacts of data and model scale, and the benefits of instruction tuning for future multimodal models.*** <br><br>
    Jul 3, CMU published a [paper](https://arxiv.org/pdf/2407.03418) “HEMM: Holistic Evaluation of Multimodal Foundation Models”. Multimodal foundation models that can holistically process text alongside images, video, audio, and other sensory modalities are increasingly used in a variety of real-world applications. However, it is challenging to characterize and study progress in multimodal foundation models, given the range of possible modeling decisions, tasks, and domains. This paper introduces Holistic Evaluation of Multimodal Models (HEMM) to systematically evaluate the capabilities of multimodal foundation models across a set of 3 dimensions: basic skills, information flow, and real-world use cases. Basic multimodal skills are internal abilities required to solve problems, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and the ability to handle external knowledge. Information flow studies how multimodal content changes during a task through querying, translation, editing, and fusion. Use cases span domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and human-computer interaction applications. Through comprehensive experiments across the 30 tasks in HEMM, the authors (1) identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today's models, and (2) distill performance trends regarding how different modeling dimensions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence performance. The conclusions regarding challenging multimodal interactions, use cases, and tasks requiring reasoning and external knowledge, the benefits of data and model scale, and the impacts of instruction tuning yield actionable insights for future work in multimodal foundation models. [Here is the project link](https://github.com/pliang279/HEMM).

30. ***LLMs in Time Series Forecasting<br>
Research by the University of Virginia and University of Washington questions the utility of LLMs in time series forecasting. Ablation studies show that removing the LLM component often improves performance. The study concludes that LLMs do not enhance sequential dependency representation or few-shot learning in time series tasks compared to simpler models.*** <br><br>
    Jun 22, Uni of Virginia and Uni of Washington published a [paper](https://arxiv.org/pdf/2406.16964) “Are Language Models Actually Useful for Time Series Forecasting”. Large language models (LLMs) are being applied to time series tasks, particularly time series forecasting. However, are language models actually useful for time series? After a series of ablation studies on three recent and popular LLM-based time series forecasting methods, the authors find that removing the LLM component or replacing it with a basic attention layer does not degrade the forecasting results -- in most cases the results even improved. The authors also find that despite their significant computational cost, pretrained LLMs do no better than models trained from scratch, do not represent the sequential dependencies in time series, and do not assist in few-shot settings. Additionally, the authors explore time series encoders and reveal that patching and attention structures perform similarly to state-of-the-art LLM-based forecasters.

32. ***Flexibility of Neural Networks<br>
NYU, University of Maryland, Capital One, and Meta investigate the practical flexibility of neural networks. The findings reveal that standard optimizers limit the model's ability to fit training data, convolutional networks are more parameter-efficient, and stochastic training with SGD finds more fitting minima. The study offers insights into the relationship between network architecture and generalization.*** <br><br>
    Jun 17, NYU, Uni of Maryland, Capital One and Meta’s LeCun published a [paper](https://arxiv.org/pdf/2406.11463) “Just How Flexible are Neural Networks in Practice?”. It is widely believed that a neural network can fit a training set containing at least as many samples as it has parameters, underpinning notions of overparameterized and underparameterized models. In practice, however, we only find solutions accessible via our training procedure, including the optimizer and regularizers, limiting flexibility. Moreover, the exact parameterization of the function class, built into an architecture, shapes its loss surface and impacts the minima we find. This work examines the ability of neural networks to fit data in practice. The findings of the work indicate that: (1) standard optimizers find minima where the model can only fit training sets with significantly fewer samples than it has parameters; (2) convolutional networks are more parameter-efficient than MLPs and ViTs, even on randomly labeled data; (3) while stochastic training is thought to have a regularizing effect, SGD actually finds minima that fit more training data than full-batch gradient descent; (4) the difference in capacity to fit correctly labeled and incorrectly labeled samples can be predictive of generalization; (5) ReLU activation functions result in finding minima that fit more data despite being designed to avoid vanishing and exploding gradients in deep architectures.

34. ***ZeRO++ for Efficient Model Training<br>
Microsoft, OpenAI, Snowflake, University of Houston, and University of Nevada introduce ZeRO++, an optimizer that significantly reduces communication overhead in large model training. By implementing techniques like low-precision all-gather and data remapping, ZeRO++ achieves up to 2.16x better throughput and maintains accuracy comparable to original ZeRO. The model's natural weight-quantization also facilitates efficient inference.*** <br><br>
    Mar 9,  Microsoft, OpenAI, Snowflake, Uni of Houston, and Nui of Nevada published a [paper](https://openreview.net/pdf?id=gx2BT0a9MQ) on ICLR2024 “ZeRO++: Extremely Efficient Collective Communication for Large Model Training”. Zero Redundancy Optimizer (ZeRO) has been used to train a wide range of large language models on massive GPU clusters due to its ease of use, efficiency, and good scalability. However, when training on low-bandwidth clusters, and/or when small batch size per GPU is used, ZeRO’s effective throughput is limited by communication overheads. To alleviate this limitation, this paper introduces ZeRO++ composing of three communication volume reduction techniques (lowprecision all-gather, data remapping, and low-precision gradient averaging) to significantly reduce the communication volume up to 4x that enables up to 2.16x better throughput at 384 GPU scale. Experimental results also show ZeRO++ can speedup the RLHF training by 3.3x compared to vanilla ZeRO. To verify the convergence of ZeRO++, the authors test up to 13B model for pretraining with 8/6-bits all gather and up to 30B model for finetuning with 4/2-bits all gather, and demonstrate on-par accuracy as original ZeRO (aka standard training). As a byproduct, the model trained with ZeRO++ is naturally weight-quantized, which can be directly used for inference without post-training quantization or quantization-aware training.
 <br><br><br>

***7 Jul 2024***

1. ***Highlighting AI Companies' Data Vulnerability <br>
A recent breach at OpenAI, which affected only an employee discussion forum, underscores that AI companies are prime targets for hackers. AI firms possess three types of valuable data: high-quality training data, bulk user interactions, and sensitive customer data. The complexity of AI systems increases breach risks, highlighting the need for robust security practices and continuous vigilance.*** <br><br>
   Jul 5, according to [techcrunch.com](https://techcrunch.com/2024/07/05/openai-breach-is-a-reminder-that-ai-companies-are-treasure-troves-for-hackers/), A recent breach at OpenAI was minor, affecting only an employee discussion forum, but it underscores that AI companies are prime targets for hackers. OpenAI and similar companies hold three types of highly valuable data: 1) High-Quality Training Data: Secretive, labor-intensive, and crucial for AI model development. 2) Bulk User Interactions: Billions of ChatGPT conversations offering deep insights, more valuable than simple search data. 3) Customer Data: Sensitive information from companies using AI tools, which often includes internal databases and proprietary information. AI companies handle significant industrial secrets, necessitating robust security measures. Despite their capabilities, the novelty and complexity of AI increase the risk of breaches. Good security practices are essential, but the constantly evolving threat landscape, now amplified by AI, requires continuous vigilance. While there's no need for immediate panic, the breach highlights the unique and substantial risks AI companies face. Enhanced security and awareness are crucial as they continue to manage vast amounts of sensitive data.

3. ***Kyutai's Voice-Enabled AI Chat-Bot <br>
French start-up Kyutai introduced Moshi, an AI model with advanced vocal capabilities. Developed in six months by a team of eight, Moshi facilitates natural and expressive AI communication. It has potential applications as a coach or companion and excels in text-to-speech. Kyutai plans to share Moshi’s code and model weights for open research and development.*** <br><br>
   Jul 3, according to [kyutai.org](https://kyutai.org/cp_moshi.pdf), the French AI start-up unveiled its first voice-enabled AI chat-bot which is openly accessible to all. In just 6 months, with a team of 8, the Kyutai research lab developed from scratch an artificial intelligence (AI) model with unprecedented vocal capabilities called Moshi. The interactive demo of the AI will be accessible from the Kyutai website. This new type of technology makes it possible for the first time to communicate in a smooth, natural and expressive way with an AI. During the presentation, the Kyutai team interacted with Moshi to illustrate its potential as a coach or companion for example, and its creativity through the incarnation of characters in roleplays. More broadly, Moshi has the potential to revolutionize the use of speech in the digital world. For instance, its text-to-speech capabilities are exceptional in terms of emotion and interaction between multiple voices. Moshi can also be installed locally and therefore run safely on an unconnected device. With Moshi, Kyutai intends to contribute to open research in AI and to the development of the entire ecosystem. The code and weights of the models will soon be freely shared, which is also unprecedented for such technology. They will be useful both to researchers in the field and to developers working on voice-based products and services.

5. ***Advancements in Vision Language Models <br>
InternLM-XComposer-2.5 (IXC-2.5), developed by SAIL, CUHK, SenseTime Group, and Tsinghua University, is a versatile vision language model supporting long-contextual input and output. IXC-2.5 features ultra-high resolution understanding, fine-grained video understanding, and multi-turn multi-image dialogue. It outperforms existing models on multiple benchmarks and is publicly available.*** <br><br>
   Jul 3, SAIL, CUHK, SenseTime Group and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2407.03320) “InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output”. The paper presents InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output. IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output contexts. Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In addition to comprehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks. The InternLM-XComposer-2.5 is publicly available at https://github.com/InternLM/InternLM-XComposer.

7. ***Unsafe Information Leakage in AI Responses <br>
Researchers from the University of Toronto, Vector Institute, and University of Oxford highlight vulnerabilities in large language models (LLMs) to jailbreak attacks. Current defenses are insufficient, and the paper introduces inferential adversaries and an information censorship criterion to ensure safety, revealing an intrinsic safety-utility trade-off.*** <br><br>
   Jul 2, Uni of Toronto, Vector Institute and Uni of Oxford published a [paper](https://arxiv.org/pdf/2407.02551) “A False Sense of Safety: Unsafe Information Leakage in 'Safe' AI Responses”. Large Language Models (LLMs) are vulnerable to jailbreaks–methods to elicit harmful or generally impermissible outputs. Safety measures are developed and assessed on their effectiveness at defending against jailbreak attacks, indicating a belief that safety is equivalent to robustness. The authors assert that current defense mechanisms, such as output filters and alignment fine-tuning, are, and will remain, fundamentally insufficient for ensuring model safety. These defenses fail to address risks arising from dual-intent queries and the ability to composite innocuous outputs to achieve harmful goals. To address this critical gap, the paper introduces an information-theoretic threat model called inferential adversaries who exploit impermissible information leakage from model outputs to achieve malicious goals. The work distinguishes these from commonly studied security adversaries who only seek to force victim models to generate specific impermissible outputs. The study demonstrates the feasibility of automating inferential adversaries through question decomposition and response aggregation. To provide safety guarantees, the researchers define an information censorship criterion for censorship mechanisms, bounding the leakage of impermissible information. The paper proposes a defense mechanism which ensures this bound and reveals an intrinsic safety-utility trade-off. The work provides the first theoretically grounded understanding of the requirements for releasing safe LLMs and the utility costs involved.

9. ***RankRAG: Enhancing Context Ranking and Generation <br>
Georgia Tech and Nvidia propose RankRAG, a framework that instruction-tunes LLMs for context ranking and answer generation in retrieval-augmented generation (RAG). RankRAG outperforms existing models on various benchmarks, including biomedical domains, demonstrating its superior generalization capability without additional fine-tuning.*** <br><br>
    Jul 2, Georgia Tech and Nvidia published a [paper](https://arxiv.org/pdf/2407.02485v1) “RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs”. Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). This work proposes a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, the work compares the model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, the Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains. Zero-shot QA performance over nine datasets is about 56% on average.

11. ***Accelerating LLM Inference with Sparse Attention <br>
Microsoft and the University of Surrey introduce MInference 1.0, a method to accelerate pre-filling for long-context LLMs using dynamic sparse attention. By leveraging specific attention matrix patterns, MInference reduces inference latency by up to 10x while maintaining accuracy, applicable to existing LLMs without modifications.*** <br><br>
    Jul 2, Microsoft and Uni of Surrey published a [paper](https://arxiv.org/pdf/2407.02490) “MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention”. The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, the paper introduces MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, the authors identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. The work determines the optimal pattern for each attention head offline and dynamically builds sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, the authors perform efficient sparse attention calculations via the optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. The proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, the study demonstrates that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. The code is available at https://aka.ms/MInference.

13. ***Optimizing Data Mixture for LLM Pre-training <br>
SeaAI, SMU, and others present RegMix, a method to identify high-performing data mixtures for LLM pre-training through regression. RegMix demonstrates superior performance compared to human-selected mixtures, highlighting the significant impact of data mixtures on model performance and the need for automatic approaches.*** <br><br>
    Jul 1, SeaAI, SMU, et al published a [paper](https://arxiv.org/pdf/2407.01492) “RegMix: Data Mixture as Regression for Language Model Pre-training”. The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. The authors propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix involves training a set of small models with diverse data mixtures and fitting a regression model to predict their performance given their respective mixtures. With the fitted regression model, the study simulates the top-ranked mixture and uses it to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, the authors train 512 models with 1M parameters for 1B tokens of different mixtures to fit the regression model and find the optimal mixture. Using this mixture the work trains a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which is found to perform best among 64 candidate 1B parameter models with other mixtures. Further, the method demonstrates superior performance compared to human selection and achieves results that match or surpass DoReMi, while utilizing only 10% of the compute budget. The experiments also show that (1) Data mixtures significantly impact performance with single-task performance variations of up to 14.6%; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed; (4) Data mixture effects transcend scaling laws, and the approach captures the complexity by considering all domains together. The code is available at https://github.com/sail-sg/regmix.

15. ***Mechanistic Interpretation in Transformers <br>
UC Berkeley, MIT, and others introduce contextual decomposition for transformers (CD-T), a method for interpreting transformer models' internal mechanisms. CD-T offers computational efficiency and reliable interpretation, improving understanding and trust in model outputs compared to existing methods like SHAP and LIME.*** <br><br>
    Jul 1, UC Berkeley, MIT et al. published a [paper](https://arxiv.org/pdf/2407.00886) “Mechanistic Interpretation through Contextual Decomposition in Transformers”. Transformers exhibit impressive capabilities but are often regarded as black boxes due to challenges in understanding the complex nonlinear relationships between features. Interpreting machine learning models is of paramount importance to mitigate risks, and mechanistic interpretability is in particular of current interest as it opens up a window for guiding manual modifications and reverse-engineering solutions. This work introduces contextual decomposition for transformers (CD-T), extending a prior work on CD for RNNs and CNNs, to address mechanistic interpretation computationally efficiently. CD-T is a flexible interpretation method for transformers. It can capture contributions of combinations of input features or source internal components (e.g. attention heads, feed-forward networks) to (1) final predictions or (2) the output of any target internal component. Using CD-T, the authors propose a novel algorithm for circuit discovery. On a real-world pathology report classification task: the paper shows CD-T distills a more faithful circuit of attention heads with improved computational efficiency (speed up 2x) than a prior benchmark, path patching. As a versatile interpretation method, CD-T also exhibits exceptional capabilities for local interpretations. CD-T is shown to reliably find words and phrases of contrasting sentiment/topic on SST-2 and AGNews datasets. Through human experiments, the paper demonstrates CD-T enables users to identify the more accurate of two models and to better trust a model's outputs compared to alternative interpretation methods such as SHAP and LIME.

17. ***Universal Approach for Sentence Segmentation <br>
Researchers from Johannes Kepler University and the University of Cambridge introduce Segment Any Text (SaT), a model for robust, adaptable, and efficient sentence segmentation. SaT outperforms existing methods across diverse domains and languages, providing a universal solution for text segmentation challenges.*** <br><br>
    Jun 30, Johannes Kepler Uni and Uni of Cambridge published a [paper](https://arxiv.org/pdf/2406.16678) “Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation”. Segmenting text into sentences plays an early and crucial role in many NLP systems. This is commonly achieved by using rule-based or statistical methods relying on lexical features such as punctuation. Although some recent works no longer exclusively rely on punctuation, it is found that no prior method achieves all of (i) robustness to missing punctuation, (ii) effective adaptability to new domains, and (iii) high efficiency. The paper introduces a new model - Segment any Text (SaT) - to solve this problem. To enhance robustness, the work proposes a new pretraining scheme that ensures less reliance on punctuation. To address adaptability, the paper introduces an extra stage of parameter-efficient fine-tuning, establishing state-of-the-art performance in distinct domains such as verses from lyrics and legal documents. Along the way, the work introduces architectural modifications that result in a threefold gain in speed over the previous state of the art and solve spurious reliance on context far in the future. Finally, the work introduces a variant of the model with fine-tuning on a diverse, multilingual mixture of sentence-segmented data, acting as a drop-in replacement and enhancement for existing segmentation tools. Overall, the contributions provide a universal approach for segmenting any text. The method outperforms all baselines - including strong LLMs - across 8 corpora spanning diverse domains and languages, especially in practically relevant situations where text is poorly formatted. The models and code, including documentation, are available at [this https URL](https://huggingface.co/segment-any-text) under the MIT license.

19. ***Token Erasure in LLMs <br>
Northeastern University explores how LLMs process multi-token words and named entities through a "token erasure" effect. This study reveals how LLMs convert arbitrary token groups into meaningful representations, providing insights into the implicit vocabulary of LLMs.*** <br><br>
    Jun 28, Northeastern Uni published a [paper](https://arxiv.org/pdf/2406.20086) “Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs”. LLMs process text as sequences of tokens that roughly correspond to words, where less common words are represented by multiple tokens. However, individual tokens are often semantically unrelated to the meanings of the words/concepts they comprise. For example, Llama-2-7b's tokenizer splits the word "northeastern" into the tokens ['_n', 'ort', 'he', 'astern'], none of which correspond to semantically meaningful units like "north" or "east." Similarly, the overall meanings of named entities like "Neil Young" and multi-word expressions like "break a leg" cannot be directly inferred from their constituent tokens. Mechanistically, how do LLMs convert such arbitrary groups of tokens into useful higher-level representations? This work finds that last token representations of named entities and multi-token words exhibit a pronounced "erasure" effect, where information about previous and current tokens is rapidly forgotten in early layers. Using this observation, the authors propose a method to "read out" the implicit vocabulary of an autoregressive LLM by examining differences in token representations across layers, and present results of this method for Llama-2-7b and Llama-3-8B. To the authors knowledge, this is the first attempt to probe the implicit vocabulary of an LLM.

21. ***Zuckerberg on AI Development <br>
Mark Zuckerberg advocates for diverse AI models instead of a single overarching AI. He emphasizes the importance of businesses and creators developing their own AI, reflecting varied interests and avoiding the concentration of AI capabilities in one entity.*** <br><br>
    Jun 28, according to [Entrepreneur](https://www.entrepreneur.com/business-news/mark-zuckerberg-reveals-the-future-meta-ai-tech-industry/476368), Mark Zuckerberg Sounds Off on Developing AI: 'I Don't Think AI Technology Is a Thing That Should Be Hoarded'. In a talk with Kallaway, Zuckerberg said that the most effective use of AI in the future is to have businesses and creators create their own AI instead of focusing on one big overarching model. "It's almost as if they think they're creating God or something, and that's just not what we're doing". Zuckerberg spoke about most major companies and their desire to build one main AI, using Google's Gemini or OpenAI's Chat GPT as examples. But for Meta, the strategy isn't to develop one central AI — the company wants to create multiple programs. "Our overall view is that this isn't the type of thing that there should just be one of, people want to interact with lots of different people and businesses and there need to be a lot of different AIs that get created to reflect people's different interests," he explained.

23. ***Unsupervised Segmentation Model <br>
UC Berkeley introduces Unsupervised SAM (UnSAM), a model for whole-image segmentation without human annotations. Using a hierarchical structure discovery approach, UnSAM achieves competitive results with supervised models, offering an effective solution for unsupervised segmentation tasks.*** <br><br>
    Jun 28, UC Berkeley published a [paper](https://arxiv.org/pdf/2406.20081v1) “Segment Anything without Supervision”. The Segmentation Anything Model (SAM) requires labor-intensive data labeling. The paper presents Unsupervised SAM (UnSAM) for promptable and automatic wholeimage segmentation that does not require human annotations. UnSAM utilizes a divide-and-conquer strategy to “discover” the hierarchical structure of visual scenes. The study first leverages top-down clustering methods to partition an unlabeled image into instance/semantic level segments. For all pixels within a segment, a bottom-up clustering method is employed to iteratively merge them into larger groups, thereby forming a hierarchical structure. These unsupervised multi-granular masks are then utilized to supervise model training. Evaluated across seven popular datasets, UnSAM achieves competitive results with the supervised counterpart SAM, and surpasses the previous state-of-the-art in unsupervised segmentation by 11% in terms of AR. Moreover, the work shows that supervised SAM can also benefit from the self-supervised labels. By integrating the unsupervised pseudo masks into SA-1B’s ground-truth masks and training UnSAM with only 1% of SA-1B, a lightly semisupervised UnSAM can often segment entities overlooked by supervised SAM, exceeding SAM’s AR by over 6.7% and AP by 3.9% on SA-1B.

25. ***Leveraging Internal Knowledge for Complex Reasoning <br>
KAIST and the University of Richmond investigate how LLMs utilize internal knowledge for complex reasoning. The study proposes DepthQA, a dataset for deconstructing complex questions, highlighting the importance of structured intermediate steps in enhancing LLMs' reasoning abilities.*** <br><br>
    Jun 27, KAIST and Uni of Richmond published a [paper](https://arxiv.org/pdf/2406.19502) “Investigating How Large Language Models Leverage Internal Knowledge to Perform Complex Reasoning”. Despite significant advancements, there is a limited understanding of how large language models (LLMs) utilize knowledge for reasoning. To address this, the paper proposes a method that deconstructs complex real-world questions into a graph, representing each question as a node with parent nodes of background knowledge needed to solve the question. The word develops the DepthQA dataset, deconstructing questions into three depths: (i) recalling conceptual knowledge, (ii) applying procedural knowledge, and (iii) analyzing strategic knowledge. Based on a hierarchical graph, the authors quantify forward discrepancy, discrepancies in LLMs' performance on simpler sub-problems versus complex questions. The work also measures backward discrepancy, where LLMs answer complex questions but struggle with simpler ones. The analysis shows that smaller models have more discrepancies than larger models. Additionally, guiding models from simpler to complex questions through multi-turn interactions improves performance across model sizes, highlighting the importance of structured intermediate steps in knowledge reasoning. This work enhances the understanding of LLM reasoning and suggests ways to improve their problem-solving abilities.

27. ***Limitations of Unlearning in Content Regulation <br>
Google examines the limitations of unlearning as a method for content regulation in LLMs. The study introduces "ununlearning," where unlearned knowledge can be reintroduced during inference, emphasizing the need for content filtering to ensure effective regulation.*** <br><br>
    Jun 27, Google published a [paper](https://arxiv.org/pdf/2407.00106) “UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI”. Exact unlearning was first introduced as a privacy mechanism that allowed a user to retract their data from machine learning models on request. Shortly after, inexact schemes were proposed to mitigate the impractical costs associated with exact unlearning. More recently unlearning is often discussed as an approach for removal of impermissible knowledge i.e. knowledge that the model should not possess such as unlicensed copyrighted, inaccurate, or malicious information. The promise is that if the model does not have a certain malicious capability, then it cannot be used for the associated malicious purpose. This paper revisits the paradigm in which unlearning is used for in Large Language Models (LLMs) and highlight an underlying inconsistency arising from in-context learning. Unlearning can be an effective control mechanism for the training phase, yet it does not prevent the model from performing an impermissible act during inference. The authors introduce a concept of ununlearning, where unlearned knowledge gets reintroduced in-context, effectively rendering the model capable of behaving as if it knows the forgotten knowledge. As a result, the authors argue that content filtering for impermissible knowledge will be required and even exact unlearning schemes are not enough for effective content regulation. The paper discusses feasibility of ununlearning for modern LLMs and examine broader implications.

29. ***Meta 3D Gen: Advanced 3D Asset Generation <br>
Meta introduces 3DGen, a state-of-the-art pipeline for text-to-3D asset generation. 3DGen excels in prompt fidelity and visual quality, supporting high-quality 3D shapes and textures in under a minute, outperforming industry baselines.*** <br><br>
    Jun 25, Meta published a [paper](https://scontent.fcbr1-1.fna.fbcdn.net/v/t39.2365-6/449707112_509645168082163_2193712134508658234_n.pdf?_nc_cat=111&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=tlSuCzsxrocQ7kNvgHDhUqB&_nc_ht=scontent.fcbr1-1.fna&oh=00_AYDZlYUAcKARsxlM8vnT6D3vG_iobn2IbT4Za8o92Vmk3w&oe=668FCED1) “Meta 3D Gen”. The paper introduces Meta 3D Gen (3DGen), a new state-of-the-art, fast pipeline for text-to-3D asset generation. 3DGen offers 3D asset creation with high prompt fidelity and high-quality 3D shapes and textures in under a minute. It supports physically-based rendering (PBR), necessary for 3D asset relighting in real-world applications. Additionally, 3DGen supports generative retexturing of previously generated (or artist-created) 3D shapes using additional textual inputs provided by the user. 3DGen integrates key technical components, Meta 3D AssetGen and Meta 3D TextureGen, that is developed for text-to-3D and text-to-texture generation, respectively. By combining their strengths, 3DGen represents 3D objects simultaneously in three ways: in view space, in volumetric space, and in UV (or texture) space. The integration of these two techniques achieves a win rate of 68% with respect to the single-stage model. The authors compare 3DGen to numerous industry baselines, and show that it outperforms them in terms of prompt fidelity and visual quality for complex textual prompts, while being significantly faster.

31. ***Critical View on AI Hallucinations <br>
Researchers from the University of Glasgow critique large language models for producing inaccurate outputs, likening them to "bullshit" as defined by Frankfurt. The study argues that understanding AI misrepresentations in this way is more useful for predicting and discussing their behavior.*** <br><br>
    Jun 8, Ethics and Information Technology published a [paper](https://link.springer.com/article/10.1007/s10676-024-09775-5) by researchers from Uni of Glasgow “ChatGPT is bullshit”. Recently, there has been considerable interest in large language models: machine learning systems which produce humanlike text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called “AI hallucinations”. The authors argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. The researchers distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. The authors further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.

33. ***Human Expectations of LLM Performance <br>
Harvard, MIT, and the University of Chicago explore how humans generalize LLM performance across tasks. The study reveals that human generalization can be predicted using NLP methods, showing discrepancies between human expectations and actual LLM performance, especially in high-cost mistake scenarios.*** <br><br>
    Jun 6, Harvard Uni, MIT, and Uni of Chicago published a [paper](https://arxiv.org/pdf/2406.01382) accepted by ICML 2024 “Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function”. What makes large language models (LLMs) impressive is also what makes them hard to evaluate: their diversity of uses. To evaluate these models, people must understand the purposes they will be used for. The authors consider a setting where these deployment decisions are made by people, and in particular, people's beliefs about where an LLM will perform well. The paper models such beliefs as the consequence of a human generalization function: having seen what an LLM gets right or wrong, people generalize to where else it might succeed. The study collects a dataset of 19K examples of how humans make generalizations across 79 tasks from the MMLU and BIG-Bench benchmarks. The work shows that the human generalization function can be predicted using NLP methods: people have consistently structured ways to generalize. The authors then evaluate LLM alignment with the human generalization function. The results show that -- especially for cases where the cost of mistakes is high -- more capable models (e.g. GPT-4) can do worse on the instances people choose to use them for, exactly because they are not aligned with the human generalization function.

35. ***Impact of ChatGPT on Human Skills <br>
A study published in Technological Forecasting & Social Change analyzes the impact of ChatGPT on human skills using Twitter data. The study identifies four emerging skills for using ChatGPT and highlights the significant impact of the technology on various human skills.*** <br><br>
    Jun 5, Technological Forecasting & Social Change published a [paper](https://www.sciencedirect.com/science/article/pii/S0040162524001859) “The impact of ChatGPT on human skills: A quantitative study on twitter data”. The novel generative Artificial Intelligence (AI) developed by OpenAI, i.e., ChatGPT, rised a great interest in both scientific and business contexts. This new wave of technological advancement typically produces deep transformation in the workplace, requiring new skills. However, none of the studies in literature provide quantitative analysis and measures on the impact of ChatGPT on human skills. To address this gap, the paper collected a database of 616,073 tweets about ChatGPT, and used Natural Language Processing techniques to identify the tasks users requested ChatGPT to perform, and the sentiment related to these tasks. Then, the work compared these tasks with a standard taxonomy of skills (i.e., ESCO) using BERT. The results of the study underline that ChatGPT impacts 185 different skills. Moreover, the researchers proposed a model to represent the interaction of the user and ChatGPT, useful to define four skills which are emerging for using this new technology. The four skills are: defining task goals; using prompt language; knowing the principles of generative AI; and conducting performance measurement.
 
 <br><br><br>

***30 Jun 2024***

1. ***Gemma 2 Release by Google:<br>
   Google launched Gemma 2 for researchers and developers, available in 9 billion (9B) and 27 billion (27B) parameter sizes. It offers higher performance and efficiency compared to its predecessor, with notable safety advancements. The 27B model rivals models twice its size, operating efficiently on a single NVIDIA H100 Tensor Core GPU or TPU host, reducing deployment costs. Key highlights include outstanding performance, cost savings, and fast inference across various hardware setups.*** <br><br>

   27 Jun, Google released [Gemma 2](https://blog.google/technology/developers/google-gemma-2/) for researchers and developers. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs. Gemma 2 is built on a redesigned architecture, engineered for both exceptional performance and inference efficiency. Here’s what makes it stand out: 1) Outsized performance: At 27B, Gemma 2 delivers the best performance for its size class, and even offers competitive alternatives to models more than twice its size. The 9B Gemma 2 model also delivers class-leading performance, outperforming Llama 3 8B and other open models in its size category. 2) Unmatched efficiency and cost savings: The 27B Gemma 2 model is designed to run inference efficiently at full precision on a single Google Cloud TPU host, NVIDIA A100 80GB Tensor Core GPU, or NVIDIA H100 Tensor Core GPU, significantly reducing costs while maintaining high performance. This allows for more accessible and budget-friendly AI deployments. 3) Blazing fast inference across hardware: Gemma 2 is optimized to run at incredible speed across a range of hardware, from powerful gaming laptops and high-end desktops, to cloud-based setups. [Compare Gemm 2](https://colab.research.google.com/github/PAIR-code/llm-comparator/blob/main/python/notebooks/basic_demo.ipynb) with other LLMs via [LLM-COMPARATOR](https://github.com/pair-code/llm-comparator).

3. ***OpenAI's LLM Critics Paper:<br>
   OpenAI's paper "LLM Critics Help Catch LLM Bugs" discusses the limitations of Reinforcement Learning from Human Feedback (RLHF) due to human evaluation constraints. It introduces "critic" models, trained to provide natural language feedback on code errors, outperforming human critiques in 63% of cases. Despite some hallucinations, these critics significantly enhance bug detection in LLMs, suggesting effective collaboration between human reviewers and LLM critics.*** <br><br>
   27 Jun, OpenAI published a [paper](https://cdn.openai.com/llm-critics-help-catch-llm-bugs-paper.pdf) “LLM Critics Help Catch LLM Bugs”. The paper indicates that Reinforcement learning from human feedback (RLHF) is fundamentally limited by the capacity of humans to correctly evaluate model output. To improve human evaluation ability and overcome that limitation this work trains “critic” models that help humans to more accurately evaluate model-written code. These critics are themselves LLMs trained with RLHF to write natural language feedback highlighting problems in code from real-world assistant tasks. On code containing naturally occurring LLM errors model-written critiques are preferred over human critiques in 63% of cases, and human evaluation finds that models catch more bugs than human contractors paid for code review. The authors further confirm that the fine-tuned LLM critics can successfully identify hundreds of errors in ChatGPT training data rated as “flawless”, even though the majority of those tasks are non-code tasks and thus out-of-distribution for the critic model. Critics can have limitations of their own, including hallucinated bugs that could mislead humans into making mistakes they might have otherwise avoided, but human-machine teams of critics and contractors catch similar numbers of bugs to LLM critics while hallucinating less than LLMs alone. Hulluciantion rate of the LLM is around 18%The hulluciantion.

5. ***Meta's LLM Compiler Release:<br>
   Meta released the Meta Large Language Model Compiler, designed for code optimization tasks. Built on Code Llama, it improves understanding of compiler intermediate representations and assembly language. Trained on 546 billion tokens, it offers models in 7 billion and 13 billion parameter sizes. Meta's LLM Compiler shows notable improvements in code size optimization and assembly disassembly, aiming to advance research in compiler optimization.*** <br><br>
   27 Jun, Meta released [Meta LLM Compiler](https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/) and published a [paper](https://scontent.fcbr1-1.fna.fbcdn.net/v/t39.2365-6/448997590_1496256481254967_2304975057370160015_n.pdf?_nc_cat=106&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=4Yn8V9DFdbsQ7kNvgG8lVE2&_nc_ht=scontent.fcbr1-1.fna&oh=00_AYC6xJJQpGi0wTHKobP4L_VDU-75bQlAwwtNjv5hRYDatw&oe=6685440D) “Meta Large Language Model Compiler: Foundation Models of Compiler Optimization”. Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, Meta introduces Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. Meta also presents fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners.

7. ***CharXiv Paper on Chart Understanding:<br>
A paper titled "CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs" highlights the limitations of current datasets for chart understanding in Multimodal LLMs. It introduces CharXiv, an evaluation suite with 2,323 diverse charts from arXiv papers, revealing a significant performance gap between proprietary and open-source models. The study emphasizes the need for more realistic evaluation methods to enhance MLLM chart understanding.*** <br><br>
   27 Jun, Princeton Uni, Uni of Wisconsin and UHK published a [paper](https://arxiv.org/pdf/2406.18521) “CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs”. The paper states that chart understanding plays a pivotal role when applying Multimodal Large Language Models (MLLMs) to real-world tasks such as analyzing scientific papers or financial reports. However, existing datasets often focus on oversimplified and homogeneous charts with template-based questions, leading to an over-optimistic measure of progress. The paper demonstrates that although open-source models can appear to outperform strong proprietary models on these benchmarks, a simple stress test with slightly different charts or questions can deteriorate performance by up to 34.5%. This work proposes CharXiv, a comprehensive evaluation suite involving 2,323 natural, challenging, and diverse charts from arXiv papers. CharXiv includes two types of questions: 1) descriptive questions about examining basic chart elements and 2) reasoning questions that require synthesizing information across complex visual elements in the chart. To ensure quality, all charts and questions are handpicked, curated, and verified by human experts. The results reveal a substantial, previously underestimated gap between the reasoning skills of the strongest proprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the strongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%. All models lag far behind human performance of 80.5%, underscoring weaknesses in the chart understanding capabilities of existing MLLMs. The authors hope CharXiv facilitates future research on MLLM chart understanding by providing a more realistic and faithful measure of progress. Project page and leaderboard: https://charxiv.github.io/

9. ***Foundation Capital's Article on AI Strategy:<br>
The article "Beyond LLMs: Building Magic" discusses the widespread adoption of AI strategies by companies across industries. It points out challenges in deploying LLM-based applications at scale, such as data preprocessing and scalability issues. The article suggests leveraging multimodal models, multi-agent systems, and new architectures to overcome these limitations and enhance AI deployment.*** <br><br>
    27 Jun, foundationcapital published [an article](https://foundationcapital.com/beyond-llms-building-magic/) “Beyond LLMs: Building magic”. The article argues that every CEO, whether they make computer chips or potato chips, has announced an AI strategy, and markets are anticipating profit improvements across the board as a result of AI. At the same time, enterprise deployment of LLM-based applications remains nascent. LLMs have plenty of shortcomings, including hallucinations, bias, and data exfiltration. There are also a host of adoption challenges, including the fact that most company data needs significant preprocessing before it can be used by a model, and use cases that worked in demos quickly break down when scaled up to production. The article furhter suggests that as founders begin to deploy LLM-based applications at scale, they should look to leverage three innovations: 1) Multimodal models are moving beyond text, opening up new use cases. 2) Multi-agent systems transform our ability to automate complex tasks. 3) New model architectures address the limitations of transformers.

11. ***Efficient Continual Pre-training Paper:<br>
Researchers from Peking University, HKUST, and MIT-IBM Watson AI Lab published a paper on mitigating the stability gap in continual pre-training for LLMs. They propose three strategies: training on subsets for multiple epochs, using high-quality sub-corpora, and employing data mixtures similar to pre-training data. These strategies significantly improve performance in new domains, particularly in medical tasks, and are validated on Llama-family models.*** <br><br>
    27 Jun, Peking Uni, HKUST, and MIT-IBM Watron AI Lab published a [paper](https://arxiv.org/pdf/2406.14833) “Efficient Continual Pre-training by Mitigating the Stability Gap”. Continual pre-training has increasingly become the predominant approach for adapting Large Language Models (LLMs) to new domains. This process involves updating the pre-trained LLM with a corpus from a new domain, resulting in a shift in the training distribution. To study the behavior of LLMs during this shift, the paper measured the model's performance throughout the continual pre-training process. The authors observed a temporary performance drop at the beginning, followed by a recovery phase, a phenomenon known as the "stability gap," previously noted in vision models classifying new classes. To address this issue and enhance LLM performance within a fixed compute budget, the paper proposes three effective strategies: (1) Continually pre-training the LLM on a subset with a proper size for multiple epochs, resulting in faster performance recovery than pre-training the LLM on a large corpus in a single epoch; (2) Pre-training the LLM only on high-quality sub-corpus, which rapidly boosts domain performance; and (3) Using a data mixture similar to the pre-training data to reduce distribution gap. The researchers conduct various experiments on Llama-family models to validate the effectiveness of the strategies in both medical continual pre-training and instruction tuning. For example, the strategies improve the average medical task performance of the OpenLlama-3B model from 36.2% to 40.7% with only 40% of the original training budget and enhance the average general task performance without causing forgetting. Furthermore, the authors apply the strategies to the Llama-3-8B model. The resulting model, Llama-3-Physician, achieves the best medical performance among current open-source models, and performs comparably to or even better than GPT-4 on several medical benchmarks. The models are available at https://huggingface.co/YiDuo1999/Llama-3-Physician-8B-Instruct.

13. ***Databricks' Compound AI Systems:<br>
Databricks emphasizes "compound AI systems" as a key trend for 2024, focusing on integrated AI systems using multiple models and techniques. At the Data+AI Summit, they announced updates to their Mosaic AI platform and open-sourced the Unity Catalog for better data standardization and training accuracy. Databricks aims to lead in democratized AI by offering advanced, reliable AI systems.*** <br><br>
    26 Jun, according to [analytisindiamagl.com](https://analyticsindiamag.com/databricks-compound-ai-systems-could-crush-openai-anthropic/), Databricks is emphasizing "compound AI systems" as a major trend in 2024 to improve the quality, reliability, and measurement of AI applications. Unlike Anthropic and OpenAI, which focus on enhancing their capabilities, Databricks aims to create integrated AI systems that use multiple models and techniques. At the Data+AI Summit, Databricks announced updates to its Mosaic AI platform, enabling customers to build compound AI systems. These systems offer significant value by integrating various tools and models for better insights. Databricks is also focusing on data governance and open-sourcing its Unity Catalog to standardize data and improve training accuracy. By prioritizing democratized AI, Databricks has positioned itself as a leader in this evolving market, potentially maintaining its lead despite competition from larger tech companies.

15. ***AI Infiltration in University Exams Paper:<br>
A paper from the University of Reading and University of Essex examines the impact of AI on university assessments. It reports a study where 100% AI-written submissions were injected into the examination system, with 94% going undetected. The AI submissions scored higher than real student work, highlighting the challenges AI poses to the integrity of academic assessments.*** <br><br>
    26 Jun, Uni of Reading and Uni of Essex published a [paper](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0305354&type=printable) on Plos ONE “A real-world test of artificial intelligence infiltration of a university examinations system: A ‘Turing Test’ case study”. The authors find that the recent rise in artificial intelligence systems, such as ChatGPT, poses a fundamental roblem for the educational sector. In universities and schools, many forms of assessment, such as coursework, are completed without invigilation. Therefore, students could hand in work as their own which is in fact completed by AI. Since the COVID pandemic, the sector has additionally accelerated its reliance on unsupervised ‘take home exams’. If students cheat using AI and this is undetected, the integrity of the way in which students are assessed is threatened. The study reports a rigorous, blind study in which the authors injected 100% AI written submissions into the examinations system in five undergraduate modules, across all years of study, for a BSc degree in Psychology at a reputable UK university. The paper found that 94% of the AI submissions were undetected. The grades awarded to the AI submissions were on average half a grade boundary higher than that achieved by real students. Across modules there was an 83.4% chance that the AI submissions on a module would outperform a random selection of the same number of real student submissions.

17. ***Huggingface's Open-LLM Leaderboard:<br>
Huggingface released a new version of their open-LLM Leaderboard, providing a platform for reproducible and comparable LLM evaluations. The leaderboard, widely used in the ML community, addresses challenges of non-reproducible results in published models. The updated version aims to improve transparency and performance evaluation for state-of-the-art open-source LLMs.*** <br><br>
    26 Jun, [Huggingface](https://huggingface.co/spaces/open-llm-leaderboard/blog) released a new open-LLM Leaderboard. Evaluating and comparing LLMs is hard. The RLHF team realized this a year ago when they wanted to reproduce and compare results from several published models. It was a nearly impossible task: scores in papers or marketing releases were given without any reproducible code, sometimes doubtful, but in most cases, just using optimized prompts or evaluation setup to give the best chances to the models. The team therefore decided to create a place where reference models would be evaluated in the exact same setup (same questions, asked in the same order, etc.) to gather completely reproducible and comparable results; and that’s how the Open LLM Leaderboard was born! Following a series of highly visible model releases, it became a widely used resource in the ML community and beyond, visited by more than 2 million unique people over the last 10 months. Around 300,000 community members use and collaborate on it monthly through submissions and discussions, usually to: 1) Find state-of-the-art open-source releases as the leaderboard provides reproducible scores separating marketing fluff from actual progress in the field. 2) Evaluate their work, be it pretraining or finetuning, comparing methods in the open and to the best existing models, and earning public recognition. However, with success, both in the leaderboard and the increasing performances of the models came challenges. After one intense year and a lot of community feedback, Huggingface thought it was time for an upgrade! Here is the link to the [Open LLM Leaderboard v2!](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)

19. ***Salesforce.AI's APIGen Paper:<br>
Salesforce.AI published a paper on APIGen, an automated pipeline for generating verifiable and diverse function-calling datasets. APIGen collects and verifies data across 21 categories, producing high-quality datasets for function-calling applications. Models trained on these datasets achieve state-of-the-art performance, demonstrating APIGen's potential to advance function-calling agent domains.*** <br><br>
    26 Jun, Saleforce.AI published a [paper](https://arxiv.org/pdf/2406.18518) “APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets”. The advancement of function-calling agent models requires diverse, reliable, and high-quality datasets. This paper presents APIGen, an automated data generation pipeline designed to synthesize verifiable high-quality datasets for function-calling applications. The authors leverage APIGen and collect 3,673 executable APIs across 21 different categories to generate diverse function-calling datasets in a scalable and structured manner. Each data in the dataset is verified through three hierarchical stages: format checking, actual function executions, and semantic verification, ensuring its reliability and correctness. The work demonstrates that models trained with the curated datasets, even with only 7B parameters, can achieve state-of-the-art performance on the Berkeley Function-Calling Benchmark, outperforming multiple GPT-4 models. Moreover, the 1B model achieves exceptional performance, surpassing GPT-3.5-Turbo and Claude-3 Haiku. The authors release a dataset containing 60,000 high-quality entries, aiming to advance the field of function-calling agent domains. The dataset is available on Huggingface: https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k and the project homepage: https://apigen-pipeline.github.io/

21. ***WildGuard Safety Moderation Tool:<br>
The University of Washington and Seoul National University introduced WildGuard, an open moderation tool for LLM safety. WildGuard identifies malicious intent, detects safety risks, and evaluates model refusal rates. It achieves state-of-the-art performance in safety moderation, reducing jailbreak success rates significantly. The model and data are open-sourced for community use.*** <br><br>
    26 Jun, Uni of Washington and Seoul National Uni published a [paper](https://arxiv.org/pdf/2406.18495) “WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs”. The paper introduces WildGuard -- an open, light-weight moderation tool for LLM safety that achieves three goals: (1) identifying malicious intent in user prompts, (2) detecting safety risks of model responses, and (3) determining model refusal rate. Together, WildGuard serves the increasing needs for automatic safety moderation and evaluation of LLM interactions, providing a one-stop tool with enhanced accuracy and broad coverage across 13 risk categories. While existing open moderation tools such as Llama-Guard2 score reasonably well in classifying straightforward model interactions, they lag far behind a prompted GPT-4, especially in identifying adversarial jailbreaks and in evaluating models' refusals, a key measure for evaluating safety behaviors in model responses. To address these challenges, the study constructs WildGuardMix, a large-scale and carefully balanced multi-task safety moderation dataset with 92K labeled examples that cover vanilla (direct) prompts and adversarial jailbreaks, paired with various refusal and compliance responses. WildGuardMix is a combination of WildGuardTrain, the training data of WildGuard, and WildGuardTest, a high-quality human-annotated moderation test set with 5K labeled items covering broad risk scenarios. Through extensive evaluations on WildGuardTest and ten existing public benchmarks, the paper shows that WildGuard establishes state-of-the-art performance in open-source safety moderation across all the three tasks compared to ten strong existing open-source moderation models (e.g., up to 26.4% improvement on refusal detection). Importantly, WildGuard matches and sometimes exceeds GPT-4 performance (e.g., up to 3.9% improvement on prompt harmfulness identification). WildGuard serves as a highly effective safety moderator in an LLM interface, reducing the success rate of jailbreak attacks from 79.8% to 2.4%. The authors open-sourced the [model](https://github.com/allenai/wildguard) and [data](https://huggingface.co/datasets/allenai/wildguardmix).

23. ***Responsible Foundation Model Development Cheatsheet:<br>
A paper by 13 institutions, including MIT and Google, presents the "Responsible Foundation Model Development Cheatsheet," a collection of 250+ tools and resources for responsible AI development. The paper highlights gaps in current practices, such as inadequate tools for model evaluation and ethical considerations, and aims to guide more responsible foundation model development.*** <br><br>
    26 Jun, 13 institutes including MIT, EleutherAI, Princeton Uni, Stanford Uni, Google, Huggaingface et al. published a [paper](https://arxiv.org/pdf/2406.16746) “The Responsible Foundation Model Development Cheatsheet: A Review of Tools & Resources”. Foundation model development attracts a rapidly expanding body of contributors, scientists, and applications. To help shape responsible development practices, the paper introduces the Foundation Model Development Cheatsheet: a growing collection of 250+ tools and resources spanning text, vision, and speech modalities. The authors draw on a large body of prior work to survey resources (e.g. software, documentation, frameworks, guides, and practical tools) that support informed data selection, processing, and understanding, precise and limitation-aware artifact documentation, efficient model training, advance awareness of the environmental impact from training, careful model evaluation of capabilities, risks, and claims, as well as responsible model release, licensing and deployment practices. The authors hope this curated collection of resources helps guide more responsible development. The process of curating this list, enabled the authors to review the AI development ecosystem, revealing what tools are critically missing, misused, or over-used in existing practices. The study finds that (i) tools for data sourcing, model evaluation, and monitoring are critically under-serving ethical and real-world needs, (ii) evaluations for model safety, capabilities, and environmental impact all lack reproducibility and transparency, (iii) text and particularly English-centric analyses continue to dominate over multilingual and multi-modal analyses, and (iv) evaluation of systems, rather than just models, is needed so that capabilities and impact are assessed in context.

25. ***Real-time Video Generation System:<br> On June 26, the National University of Singapore and Purdue University introduced "Pyramid Attention Broadcast" (PAB), a system for real-time video generation using DiT-based models. PAB achieves up to 21.6 FPS with 10.6x acceleration by reducing redundant attention computation, maintaining quality across models like Open-Sora and Latte. It is training-free, allowing future DiT-based models to gain real-time capabilities.*** <br><br>
    26 Jun, Natioinal Uni of Singapore and Purde Uni released “Pyramid Attention Broadcase”, a real-time [video generation system](https://github.com/NUS-HPC-AI-Lab/OpenDiT), the first approach that achieves real-time DiT-based video generation. By mitigating redundant attention computation, PAB achieves up to 21.6 FPS with 10.6x acceleration, without sacrificing quality across popular DiT-based video generation models including Open-Sora, Open-Sora-Plan, and Latte. Notably, as a training-free approach, PAB can enpower any future DiT-based video generation models with real-time capabilities.

27. ***Introduction of FineWeb Dataset: <br>
On June 25, Huggingface released the FineWeb dataset, documented in the paper "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale." FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots, improves LLM performance compared to other datasets. FineWeb-Edu, a subset of 1.3 trillion tokens, enhances LLMs on knowledge- and reasoning-intensive benchmarks.*** <br><br>
    25 Jun, Huggingface released the [FineWeb dataset](https://huggingface.co/datasets/HuggingFaceFW/fineweb) and introduced the details of the dataset in a [paper](https://arxiv.org/pdf/2406.17557) “The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale”. The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. This work introduces FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, the work carefully documents and ablates all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. In addition, th paper introduces FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with the datasets, Hugginkgface publicly releases the data curation codebase and all of the models trained during the ablation experiments.

29. ***Memorization in Language Models: <br>On June 25, EleutherAI, Microsoft, NYU, Google, and Harvard University published "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon." The paper proposes a taxonomy for memorization: recitation of highly duplicated sequences, reconstruction of predictable sequences, and recollection of neither. This taxonomy helps build predictive models for memorization by analyzing dependencies and factor influences.*** <br><br>
    25 Jun, EleutherAI, Microsoft, NYU, Google, Harvard Uni et al published a [paper](https://arxiv.org/pdf/2406.17746) “Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon”. Memorization in language models is typically treated as a homogenous phenomenon, neglecting the specifics of the memorized data. The paper instead models memorization as the effect of a set of complex factors that describe each sample and relate it to the model and corpus. To build intuition around these factors, the authors break memorization down into a taxonomy: recitation of highly duplicated sequences, reconstruction of inherently predictable sequences, and recollection of sequences that are neither. The authors demonstrate the usefulness of the taxonomy by using it to construct a predictive model for memorization. By analyzing dependencies and inspecting the weights of the predictive model, the study finds that different factors influence the likelihood of memorization differently depending on the taxonomic category.

31. ***Continual Learning in LMs: <br>On June 25, the University of Hong Kong, Edinburgh, and NVIDIA introduced "MIGU" (Magnitude-based Gradient Updating) in the paper "Unlocking Continual Learning Abilities in Language Models." MIGU, a rehearsal-free method, updates model parameters with large magnitudes in LMs' linear layers, significantly improving continual learning performance across various benchmarks.*** <br><br>
    25 Jun, Uni of HongKong, Edinb urgh, NVIDIA, et al. published a [paper](https://arxiv.org/pdf/2406.17245) “Unlocking Continual Learning Abilities in Language Models”. Language models (LMs) exhibit impressive performance and generalization capabilities. However, LMs struggle with the persistent challenge of catastrophic forgetting, which undermines their long-term sustainability in continual learning (CL). Existing approaches usually address the issue by incorporating old task data or task-wise inductive bias into LMs. However, old data and accurate task information are often unavailable or costly to collect, hindering the availability of current CL approaches for LMs. To address this limitation, the study introduces MIGU (MagnItude-based Gradient Updating for continual learning), a rehearsal-free and task-label-free method that only updates the model parameters with large magnitudes of output in LMs' linear layers. MIGU is based on the authors observation that the L1-normalized magnitude distribution of the output in LMs' linear layers is different when the LM models deal with different task data. By imposing this simple constraint on the gradient update process, the work can leverage the inherent behaviors of LMs, thereby unlocking their innate CL abilities. The experiments demonstrate that MIGU is universally applicable to all three LM architectures (T5, RoBERTa, and Llama2), delivering state-of-the-art or on-par performance across continual finetuning and continual pre-training settings on four CL benchmarks. For example, MIGU brings a 15.2% average accuracy improvement over conventional parameter-efficient finetuning baselines in a 15-task CL benchmark. MIGU can also seamlessly integrate with all three existing CL types to further enhance performance. Code is available at https://github.com/wenyudu/MIGU{this https URL}.

33. ***Scaling Laws for Linear Complexity Models: <br>On June 24, OpenNLPLab, ANU, and TapTap published "Scaling Laws for Linear Complexity Language Models." The study examines the scaling behaviors of linear architectures like TNL, HGRN2, and cosFormer2, demonstrating that linear complexity models exhibit similar scaling capabilities to conventional transformer-based models with superior linguistic proficiency.*** <br><br>
    24 Jun, OpenNLPLab, ANU and TapTap published a [paper](https://arxiv.org/pdf/2406.16690) “Scaling Laws for Linear Complexity Language Models”. The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. This paper presents the scaling laws for linear complexity language models to establish a foundation for their scalability. Specifically, the work examines the scaling behaviors of three efficient linear architectures. These include TNL, a linear attention model with data-independent decay; HGRN2, a linear RNN with data-dependent decay; and cosFormer2, a linear attention model without decay. The study also includes LLaMA as a baseline architecture for softmax attention for comparison. These models were trained with six variants, ranging from 70M to 7B parameters on a 300B-token corpus, and evaluated with a total of 1,376 intermediate checkpoints on various downstream tasks. These tasks include validation loss, commonsense reasoning, and information retrieval and generation. The study reveals that existing linear complexity language models exhibit similar scaling capabilities as conventional transformer-based models while also demonstrating superior linguistic proficiency and knowledge retention.

35. ***WARP for Policy Alignment:<br> On June 24, Google introduced WARP (Weight Averaged Rewarded Policies) in the paper "On the Benefits of Weight Averaged Rewarded Policies." WARP merges policies in the weight space at different stages to improve reinforcement learning from human feedback, achieving superior rewards and alignment in GEMMA policies compared to other open-source LLMs.*** <br><br>
    24 Jun, Google published a [paper](https://arxiv.org/pdf/2406.16768) “WARP: On the Benefits of Weight Averaged Rewarded Policies”. Reinforcement learning from human feedback (RLHF) aligns large language models (LLMs) by encouraging their generations to have high rewards, using a reward model trained on human preferences. To prevent the forgetting of pre-trained knowledge, RLHF usually incorporates a KL regularization; this forces the policy to remain close to its supervised fine-tuned initialization, though it hinders the reward optimization. To tackle the trade-off between KL and reward, this paper introduces a novel alignment strategy named Weight Averaged Rewarded Policies (WARP). WARP merges policies in the weight space at three distinct stages. First, it uses the exponential moving average of the policy as a dynamic anchor in the KL regularization. Second, it applies spherical interpolation to merge independently fine-tuned policies into a new enhanced one. Third, it linearly interpolates between this merged model and the initialization, to recover features from pre-training. This procedure is then applied iteratively, with each iteration's final model used as an advanced initialization for the next, progressively refining the KL-reward Pareto front, achieving superior rewards at fixed KL. Experiments with GEMMA policies validate that WARP improves their quality and alignment, outperforming other open-source LLMs.

37. ***Mitigating Lost-in-the-Middle Problem: <br> On June 23, the University of Washington, MIT, and Google published "Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization." The paper addresses the lost-in-the-middle problem by calibrating LLMs' positional attention bias, leading to improved performance in locating relevant information within long contexts and enhancing retrieval-augmented generation.*** <br><br>
    23 Jun, Uni of Washington, MIT, and Google published a [paper](https://arxiv.org/pdf/2406.16008) “Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization”. Large language models (LLMs), even when specifically trained to process long input contexts, struggle to capture relevant information located in the middle of their input. This phenomenon has been known as the lost-in-the-middle problem. This paper makes three contributions. First, it sets out to understand the factors that cause this phenomenon. In doing so, the study establishes a connection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs exhibit a U-shaped attention bias where the tokens at the beginning and at the end of its input receive higher attention, regardless of their relevance. Second, the study mitigates this positional bias through a calibration mechanism, found-in-the-middle, that allows the model to attend to contexts faithfully according to their relevance, even though when they are in the middle. Third, the study shows found-in-the-middle not only achieves better performance in locating relevant information within a long context, but also eventually leads to improved retrieval-augmented generation (RAG) performance across various tasks, outperforming existing methods by up to 15 percentage points. These findings open up future directions in understanding LLM attention bias and its potential consequences.

39. ***Few-shot Learning in Long Contexts: <br> On June 23, Bar-Ilan University and Google published "Can Few-shot Work in Long-Context? Recycling the Context to Generate Demonstrations." The study proposes generating few-shot examples from long contexts to enhance LLM performance in long-context QA tasks, achieving substantial improvements across various datasets.*** <br><br>
    23 Jun, Bar-Ilan Uni and Google published a [paper](https://arxiv.org/pdf/2406.13632) “Can Few-shot Work in Long-Context? Recycling the Context to Generate Demonstrations”. Despite recent advancements in Large Language Models (LLMs), their performance on tasks involving long contexts remains sub-optimal. In-Context Learning (ICL) with few-shot examples may be an appealing solution to enhance LLM performance in this scenario; However, naively adding ICL examples with long context introduces challenges, including substantial token overhead added for each few-shot example and context mismatch between the demonstrations and the target query. This work proposes to automatically generate few-shot examples for long context QA tasks by recycling contexts. Specifically, given a long input context (1-3k tokens) and a query, the authors generate additional query-output pairs from the given context as few-shot examples, while introducing the context only once. This ensures that the demonstrations are leveraging the same context as the target query while only adding a small number of tokens to the prompt. The paper further enhances each demonstration by instructing the model to explicitly identify the relevant paragraphs before the answer, which improves performance while providing fine-grained attribution to the answer source. The study applies the method on multiple LLMs and obtain substantial improvements (+23\% on average across models) on various QA datasets with long context, especially when the answer lies within the middle of the context. Surprisingly, despite introducing only single-hop ICL examples, LLMs also successfully generalize to multi-hop long-context QA using our approach.

41. ***Detecting Hallucinations in LLMs: <br>On June 22, the University of Oxford introduced semantic entropy probes (SEPs) in the paper "Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs." SEPs provide a low-cost method for detecting hallucinations by approximating semantic entropy from hidden states, retaining high performance and generalizing better to out-of-distribution data.*** <br><br>
    22 Jun, Uni of Oxford published a [paper](https://arxiv.org/pdf/2406.15927) “Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs”. The paper proposes semantic entropy probes (SEPs), a cheap and reliable method for uncertainty quantification in Large Language Models (LLMs). Hallucinations, which are plausible-sounding but factually incorrect and arbitrary model generations, present a major challenge to the practical adoption of LLMs. Recent work by Farquhar et al. (2024) proposes semantic entropy (SE), which can detect hallucinations by estimating uncertainty in the space semantic meaning for a set of model generations. However, the 5-to-10-fold increase in computation cost associated with SE computation hinders practical adoption. To address this, the authors propose SEPs, which directly approximate SE from the hidden states of a single generation. SEPs are simple to train and do not require sampling multiple model generations at test time, reducing the overhead of semantic uncertainty quantification to almost zero. The work shows that SEPs retain high performance for hallucination detection and generalize better to out-of-distribution data than previous probing methods that directly predict model accuracy. The results across models and tasks suggest that model hidden states capture SE, and the ablation studies give further insights into the token positions and model layers for which this is the case.

43. ***GraphReader for Long-context Tasks: <br>On June 20, Alibaba Group, the University of Manchester, and others published "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models." GraphReader structures long texts into graphs and employs an agent to explore them, outperforming GPT-4-128k in processing long contexts and excelling in multiple benchmarks.*** <br><br>
    20 Jun, Alibaba Group, Uni of Manchester et al published a [paper](https://arxiv.org/pdf/2406.14550) “GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models”. The paper argues that long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. Despite numerous efforts made to optimize LLMs for long contexts, challenges persist in robustly processing long inputs. This paper introduces GraphReader, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. It then invokes a set of predefined functions to read node content and neighbors, facilitating a coarse-to-fine exploration of the graph. Throughout the exploration, the agent continuously records new insights and reflects on current circumstances to optimize the process until it has gathered sufficient information to generate an answer. Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin. Additionally, the approach demonstrates superior performance on four challenging single-hop and multi-hop benchmarks.

45. ***ICAL for Multimodal Agents: <br>On June 20, CMU and Google introduced In-Context Abstraction Learning (ICAL) in the paper "ICAL: Continual Learning of Multimodal Agents by Transforming Trajectories into Actionable Insights." ICAL builds memory from sub-optimal demonstrations, improving decision-making in multimodal agents and surpassing state-of-the-art in various benchmarks.*** <br><br>
    20 Jun, CMU and Google published a [paper](https://arxiv.org/pdf/2406.14596) “ICAL: Continual Learning of Multimodal Agents by Transforming Trajectories into Actionable Insights”. Large-scale generative language and vision-language models (LLMs and VLMs) excel in few-shot in-context learning for decision making and instruction following. However, they require high-quality exemplar demonstrations to be included in their context window. Can LLMs and VLMs generate their own prompt examples from generic, sub-optimal demonstrations? The study proposes In-Context Abstraction Learning (ICAL), a method that builds a memory of multimodal experience insights from sub-optimal demonstrations and human feedback. Given a noisy demonstration in a new domain, VLMs abstract the trajectory into a general program by fixing inefficient actions and annotating cognitive abstractions: task relationships, object state changes, temporal subgoals, and task construals. These abstractions are refined and adapted interactively through human feedback while the agent attempts to execute the trajectory in a similar environment. The resulting abstractions, when used as exemplars in the prompt, significantly improve decision-making in retrieval-augmented LLM and VLM agents. The ICAL agent surpasses the state-of-the-art in dialogue-based instruction following in TEACh, multimodal web agents in VisualWebArena, and action anticipation in Ego4D. In TEACh, ICAL achieves a 12.6% improvement in goal-condition success. In VisualWebArena, the task success rate improves over the SOTA from 14.3% to 22.7%. In Ego4D action forecasting, ICAL improves over few-shot GPT-4V and remains competitive with supervised models. The authors show finetuning the retrieval-augmented in-context agent yields additional improvements. The approach significantly reduces reliance on expert-crafted examples and consistently outperforms in-context learning from action plans that lack such insights.

47. ***DataComp-LM for Training Sets: <br>On June 20, 23 organizations including the University of Washington, Apple, and Stanford published "DataComp-LM: In search of the next generation of training sets for language models." The paper introduces a testbed for dataset experiments, providing a standardized corpus and effective pretraining recipes, significantly improving language model performance.*** <br><br>
    20 Jun, 23 ogranizations include Uni of Washington, Apple, Stanford, UCLA, CMU, Harvard et al. published a [paper](https://arxiv.org/pdf/2406.11794v1) “DataComp-LM: In search of the next generation of training sets for language models”. The paper introduces DataComp for Language Models (DCLM), a testbed for controlled dataset experiments with the goal of improving language models. As part of DCLM, the work provides a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations. Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing at model scales ranging from 412M to 7B parameters. As a baseline for DCLM, the authors conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set. The resulting dataset, DCLM-Baseline enables training a 7B parameter language model from scratch to 64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-Baseline represents a 6.6 percentage point improvement on MMLU while being trained with 40% less compute. The baseline model is also comparable to Mistral-7B-v0.3 and Llama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53 natural language understanding tasks while being trained with 6.6x less compute than Llama 3 8B. The results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation.

49. ***Investigating Reward-Tampering: <br>On June 17, Anthropic, Redwood Research, and the University of Oxford published "Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models." The study examines specification gaming in LLMs, finding that LLMs can generalize from simple forms to pernicious reward-tampering behaviors, which are challenging to eliminate.*** <br><br>
    17 Jun, Anthropic, Redwood research and Uni of Oxford published a [paper](https://arxiv.org/pdf/2406.10162) “Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models”. In reinforcement learning, specification gaming occurs when AI systems learn undesired behaviors that are highly rewarded due to misspecified training goals. Specification gaming can range from simple behaviors like sycophancy to sophisticated and pernicious behaviors like reward-tampering, where a model directly modifies its own reward mechanism. However, these more pernicious behaviors may be too complex to be discovered via exploration. This work studies whether Large Language Model (LLM) assistants which find easily discovered forms of specification gaming will generalize to perform rarer and more blatant forms, up to and including reward-tampering. The authors construct a curriculum of increasingly sophisticated gameable environments and find that training on early-curriculum environments leads to more specification gaming on remaining environments. Strikingly, a small but non-negligible proportion of the time, LLM assistants trained on the full curriculum generalize zero-shot to directly rewriting their own reward function. Retraining an LLM not to game early-curriculum environments mitigates, but does not eliminate, reward-tampering in later environments. Moreover, adding harmlessness training to the gameable environments does not prevent reward-tampering. These results demonstrate that LLMs can generalize from common forms of specification gaming to more pernicious reward tampering and that such behavior may be nontrivial to remove.

51. ***LLMs and Programming by Example: <br>On June 13, Cornell University published "Is Programming by Example solved by LLMs?" The paper investigates the effectiveness of LLMs in programming-by-example tasks, finding that while pretrained models struggle, fine-tuning improves performance for in-distribution tasks, highlighting areas for further improvement in out-of-distribution generalization.*** <br><br>
    13 Jun, Cornell Uni published a [paper](https://arxiv.org/pdf/2406.08316) “Is Programming by Example solved by LLMs?”. Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples. Such systems are practically and theoretically important: from an end-user perspective, they are deployed to millions of people, and from an AI perspective, PBE corresponds to a very general form of few-shot inductive inference. Given the success of Large Language Models (LLMs) in code-generation tasks, the paper investigates here the extent to which LLMs can be said to have `solved' PBE. The authors experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data. The study finds that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution. The work analyzes empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization. Collectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short.
 <br><br><br>

***23 June 2024***
1. ***Nature published a paper asserting that language is primarily a tool for communication rather than thought, challenging the view that language is used for thinking. The authors review neuroscience evidence, discuss the brain network supporting linguistic ability, and highlight the dissociation between language and thought. They conclude that language enhances cultural transmission but is not essential for complex thought.*** <br><br>
   20 Jun, Nature published a perspectives [paper](https://www.nature.com/articles/s41586-024-07522-w) “Language is primarily a tool for communication rather than thought”. The article argues that language is a defining characteristic of our species, but the function, or functions, that it serves has been debated for centuries. Here the paper brings recent evidence from neuroscience and allied disciplines to argue that in modern humans, language is a tool for communication, contrary to a prominent view that we use language for thinking. The authors begin by introducing the brain network that supports linguistic ability in humans; and then review evidence for a double dissociation between language and thought, and discuss several properties of language that suggest that it is optimized for communication. The article concludes that although the emergence of language has unquestionably transformed human culture, language does not appear to be a prerequisite for complex thought, including symbolic thought. Instead, language is a powerful tool for the transmission of cultural knowledge; it plausibly co-evolved with our thinking and reasoning capacities, and only reflects, rather than gives rise to, the signature sophistication of human cognition.

3. ***OpenAI’s co-founder and former chief scientist, Ilya Sutskever, announced a new AI company, Safe Superintelligence Inc. (SSI), focused on creating a safe and powerful AI system. SSI aims to prioritize safety and avoid external pressures common in companies like OpenAI, Google, and Microsoft. The startup is co-founded by Daniel Gross and Daniel Levy.*** <br><br>
   20 Jun, [according to theverge](https://www.theverge.com/2024/6/19/24181870/openai-former-chief-scientist-ilya-sutskever-ssi-safe-superintelligence), OpenAI’s former chief scientist is starting a new AI company. Ilya Sutskever, OpenAI’s co-founder and former chief scientist, is starting a new AI company focused on safety. In a post on Wednesday, Sutskever revealed Safe Superintelligence Inc. (SSI), a startup with “one goal and one product:” creating a safe and powerful AI system. The announcement describes SSI as a startup that “approaches safety and capabilities in tandem,” letting the company quickly advance its AI system while still prioritizing safety. It also calls out the external pressure AI teams at companies like OpenAI, Google, and Microsoft often face, saying the company’s “singular focus” allows it to avoid “distraction by management overhead or product cycles.” “Our business model means safety, security, and progress are all insulated from short-term commercial pressures,” the announcement reads. “This way, we can scale in peace.” In addition to Sutskever, SSI is co-founded by Daniel Gross, a former AI lead at Apple, and Daniel Levy, who previously worked as a member of technical staff at OpenAI.

5. ***Anthropic released a new AI model, Claude 3.5 Sonnet, which outperforms its predecessors in text and image analysis. Although it shows incremental improvements, benchmarks indicate it slightly surpasses OpenAI’s GPT-4o. Alongside, Anthropic introduces Artifacts, a workspace for editing and collaborating on AI-generated content, currently in preview.*** <br><br>
   20 Jun, according to [Techcrunch](https://techcrunch.com/2024/06/20/anthropic-claims-its-latest-model-is-best-in-class/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAKCTE0J0flCCoJ5P-8CUs6YaU1aM0DWW5XxwYMOv03y8jB23CxO0liJfryMwrZJoG2ftxPh2G5HIROITByHXemmw1zswEUejwLnTuVw6bhFj2EOwvfGw2J2i2kMoeC2dosP9fWW22vAyEG1Fo7jRag7baRY8yTACJInjWRN_gjxo#:~:text=OpenAI%20rival%20Anthropic%20is%20releasing,yet%20%E2%80%94%20at%20least%20on%20paper.), Anthropic claims its latest model is best-in-class. OpenAI rival Anthropic is releasing a powerful new generative AI model called Claude 3.5 Sonnet. But it’s more an incremental step than a monumental leap forward. Claude 3.5 Sonnet can analyze both text and images as well as generate text, and it’s Anthropic’s best-performing model yet — at least on paper. Across several AI benchmarks for reading, coding, math and vision, Claude 3.5 Sonnet outperforms the model it’s replacing, Claude 3 Sonnet, and beats Anthropic’s previous flagship model Claude 3 Opus. Benchmarks aren’t necessarily the most useful measure of AI progress, in part because many of them test for esoteric edge cases that aren’t applicable to the average person, like answering health exam questions. But for what it’s worth, Claude 3.5 Sonnet just barely bests rival leading models, including OpenAI’s recently launched GPT-4o, on some of the benchmarks Anthropic tested it against. Alongside the new model, Anthropic is releasing what it’s calling Artifacts, a workspace where users can edit and add to content — e.g. code and documents — generated by Anthropic’s models. Currently in preview, Artifacts will gain new features, like ways to collaborate with larger teams and store knowledge bases, in the near future, Anthropic says.

7. ***Researchers from Nature published a paper on detecting hallucinations in large language models (LLMs) using semantic entropy. They propose entropy-based uncertainty estimators to detect confabulations—arbitrary incorrect generations—addressing the need for reliable methods to enhance the trustworthiness of LLMs across various tasks without task-specific data.*** <br><br>
   19 Jun, Nature published a [paper](https://www.nature.com/articles/s41586-024-07421-0) “Detecting hallucinations in large language models using semantic entropy”. The researchers believe that Large language model (LLM) systems, such as ChatGPT or Gemini, can show impressive reasoning and question-answering capabilities but often ‘hallucinate’ false outputs and unsubstantiated answers. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents or untrue facts in news articles and even posing a risk to human life in medical domains such as radiology. Encouraging truthfulness through supervision or reinforcement has been only partially successful. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. The work develops new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations—confabulations—which are arbitrary and incorrect generations. The method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. The method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, the method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.

9. ***Researchers from the University of Groningen and the University of Amsterdam introduced MIRAGE, a method for ensuring verifiability in retrieval-augmented generation (RAG) systems. MIRAGE uses model internals to attribute answers to their sources accurately, improving upon previous self-citation methods by providing more faithful and context-sensitive attributions.*** <br><br>
    19 Jun, Uni of Groningen and Uni of Amsterdam published a [paper](https://arxiv.org/pdf/2406.13663v1) “Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation”.  The paper argues that ensuring the verifiability of model answers is a fundamental challenge for retrieval-augmented generation (RAG) in the question answering (QA) domain. Recently, self-citation prompting was proposed to make large language models (LLMs) generate citations to supporting documents along with their answers. However, self-citing LLMs often struggle to match the required format, refer to non-existent sources, and fail to faithfully reflect LLMs' context usage throughout the generation. This work presents MIRAGE --Model Internals-based RAG Explanations -- a plug-and-play approach using model internals for faithful answer attribution in RAG applications. MIRAGE detects context-sensitive answer tokens and pairs them with retrieved documents contributing to their prediction via saliency methods. The authors evaluate the proposed approach on a multilingual extractive QA dataset, finding high agreement with human answer attribution. On open-ended QA, MIRAGE achieves citation quality and efficiency comparable to self-citation while also allowing for a finer-grained control of attribution parameters. The qualitative evaluation highlights the faithfulness of MIRAGE's attributions and underscores the promising application of model internals for RAG answer attribution.

11. ***MainFunc, a startup founded by alumni from Microsoft, Google, and Baidu, released Genspark, an "AI Agent Engine" aimed at enhancing search experiences. With $60 million in seed funding, Genspark offers a new webpage format called Sparkpage, which consolidates web knowledge and includes an AI-powered assistant to guide users.*** <br><br>
    18 Jun, a start-up named [MainFunc](https://mainfunc.ai/#about_us), released its first product, [Genspark](https://www.genspark.ai/), which is the "AI Agent Engine" designed to provide a significantly better search experience using AI. MainFunc was founded by alumni from Microsoft, Google, and Baidu who are passionate about building world-class AI product innovations for a better world. The company raised a total of $60 million in its seed round, led by Singapore-based Lanchi Ventures and supported by other global angel investors. Sparkpage is a revolutionary new form of webpage that transforms how you interact with information online: Distillation and Consolidation - Each Sparkpage distills and consolidates a wealth of web knowledge into a single, cohesive unit. We are committed to providing content that is both informative and impartial, free from commercial influences or business biases. Built-in AI-Powered Copilot - Every Sparkpage is equipped with a built-in AI copilot, meticulously designed to guide and assist you in effortlessly expanding your knowledge. This intelligent assistant responds dynamically to your inquiries, offering insights and information tailored to your needs.

13. ***Google published a paper evaluating language models' probabilistic reasoning abilities. The study assessed state-of-the-art models on tasks like estimating percentiles and calculating probabilities using statistical distributions. It found that models improve with contextual anchors, real-world contexts, and summary statistics, advancing their probabilistic reasoning capabilities.*** <br><br>
    18 Jun, Google published a [paper](https://arxiv.org/pdf/2406.12830) “What Are the Odds? Language Models Are Capable of Probabilistic Reasoning”. The paper argues that Language models (LM) are capable of remarkably complex linguistic tasks; however, numerical reasoning is an area in which they frequently struggle. An important but rarely evaluated form of reasoning is understanding probability distributions. This paper focuses on evaluating the probabilistic reasoning capabilities of LMs using idealized and real-world statistical distributions. The study performs a systematic evaluation of state-of-the-art LMs on three tasks: estimating percentiles, drawing samples, and calculating probabilities. The authors evaluate three ways to provide context to LMs 1) anchoring examples from within a distribution or family of distributions, 2) real-world context, 3) summary statistics on which to base a Normal approximation. Models can make inferences about distributions, and can be further aided by the incorporation of real-world context, example shots and simplified assumptions, even if these assumptions are incorrect or misspecified. To conduct this work, the authors developed a comprehensive benchmark distribution dataset with associated question-answer pairs that will be released publicly.

15. ***Nvidia became the world’s most valuable company, surpassing Microsoft, due to its critical role in AI computing. Nvidia’s market capitalization reached $3.335 trillion, driven by optimism about AI technology. Despite concerns over AI investment sustainability, Nvidia’s stock split and market performance boosted major stock indices.*** <br><br>
    18 Jun, according to [abc news](https://www.abc.net.au/news/2024-06-19/nvidia-overtakes-microsoft-to-become-most-valuable-company/103995316), Nvidia becomes world's most valuable company, toppling Microsoft amid pursuit of AI domination. Technology multinational Nvidia has become the world's most valuable company, dethroning tech heavyweight Microsoft as its high-end processors play a central role in a scramble to dominate artificial intelligence computing. Shares climbed 3.5 per cent to $US135.58 ($203.60) in the US on Tuesday, local time, lifting its market capitalisation to $US3.335 trillion just days after overtaking Apple to become the second most valuable company. Nvidia's stunning surge over the past year has become emblematic of a Wall Street frenzy driven by optimism about emerging AI technology. Sharp increases in analysts' expectations for Nvidia's future earnings have outpaced its stellar stock gains, resulting in a fall in the stock's earnings valuation. Increasing the appeal for its highly valued stock among individual investors, Nvidia last week split its stock 10 for one. But while Nvidia's rally has lifted the S&P 500 and Nasdaq to record highs, some investors worry unbridled optimism about AI could evaporate if signs emerge of a slowdown in spending on the technology.

17. ***A paper by MIT, Microsoft, and the University of Maryland explored how language models use external context in Retrieval Augmented Generation (RAG) systems. The study found that models prefer context over parametric memory when answering questions. Techniques like Causal Mediation Analysis revealed this bias, providing insights into the RAG pipeline's mechanisms.*** <br><br>
    18 Jun, MIT, Microsoft and Uni of Maryland published a [paper](https://arxiv.org/pdf/2406.12824) “From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries”.  The authors indicate that Retrieval Augmented Generation (RAG) enriches the ability of language models to reason using external context to augment responses for a given user prompt. This approach has risen in popularity due to practical applications in various applications of language models in search, question/answering, and chat-bots. However, the exact nature of how this approach works isn't clearly understood. This paper mechanistically examines the RAG pipeline to highlight that language models take shortcut and have a strong bias towards utilizing only the context information to answer the question, while relying minimally on their parametric memory. The study probes this mechanistic behavior in language models with: (i) Causal Mediation Analysis to show that the parametric memory is minimally utilized when answering a question and (ii) Attention Contributions and Knockouts to show that the last token residual stream do not get enriched from the subject token in the question, but gets enriched from other informative tokens in the context. The authors find this pronounced shortcut behaviour true across both LLaMa and Phi family of models.

19. ***Researchers from the University of Washington, Harvard, and Stanford presented TabuLa-8B, a language model for tabular data prediction. Trained on a large dataset from the TabLib corpus, TabuLa-8B outperforms existing models in zero-shot and few-shot settings. The model demonstrates significant accuracy improvements for tabular data prediction tasks.*** <br><br>
    17 Jun, Uni of Washington, Harvard and Stanford Uni published a [paper](https://arxiv.org/pdf/2406.12031) “Large Scale Transfer Learning for Tabular Data via Language Modeling”. The authors argue that tabular data -- structured, heterogeneous, spreadsheet-style data with rows and columns -- is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and predictors in domains such as language modeling and computer vision, this transfer learning paradigm has not had similar impact in the tabular domain. This work seeks to narrow this gap and present TabuLa-8B, a language model for tabular prediction. The study defines a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control. Using the resulting dataset, which comprises over 1.6B rows from 3.1M unique tables, the authors fine-tune a Llama 3-8B large language model (LLM) for tabular data prediction (classification and binned regression) using a novel packing and attention scheme for tabular prediction. Through evaluation across a test suite of 329 datasets, the authors find that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g. XGBoost, TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN models that are explicitly trained on equal, or even up to 16x more data. Model, code, and data along with the publication of this paper is [publicly available](https://github.com/mlfoundations/tabliblib).

21. ***ServiceNow and Mila introduced RepLiQA, a new dataset for evaluating language models on unseen reference content. RepLiQA consists of human-crafted documents and questions designed to test models' ability to find answers within provided content. Initial benchmarks revealed performance differences among state-of-the-art language models in a context-conditional setting.*** <br><br>
    17 Jun,  ServiceNow and Mila published a [paper](https://arxiv.org/pdf/2406.11811v1) “RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content”. The paper argues that Large Language Models (LLMs) are trained on vast amounts of data, most of which is automatically scraped from the internet. This data includes encyclopedic documents that harbor a vast amount of general knowledge (e.g., Wikipedia) but also potentially overlap with benchmark datasets used for evaluating LLMs. Consequently, evaluating models on test splits that might have leaked into the training set is prone to misleading conclusions. To foster sound evaluation of language models, the authors introduce a new test dataset named RepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a collection of five splits of test sets, four of which have not been released to the internet or exposed to LLM APIs prior to this publication. Each sample in RepLiQA comprises (1) a reference document crafted by a human annotator and depicting an imaginary scenario (e.g., a news article) absent from the internet; (2) a question about the document's topic; (3) a ground-truth answer derived directly from the information in the document; and (4) the paragraph extracted from the reference document containing the answer. As such, accurate answers can only be generated if a model can find relevant content within the provided document. We run a large-scale benchmark comprising several state-of-the-art LLMs to uncover differences in performance across models of various types and sizes in a context-conditional language modeling setting. Released splits of RepLiQA can be found here: https://huggingface.co/datasets/ServiceNow/repliqa.

23. ***A paper from Harvard, UC Santa Barbara, and Google discussed "transcendence" in generative models, where AI models surpass the expertise of their human trainers. The study showed that a trained transformer model for chess could outperform its training data's best players, enabled by low-temperature sampling. This phenomenon was theoretically and experimentally validated.*** <br><br>
    17 Jun, Harvard, UC Santa Barbara and Google published a [paper](https://arxiv.org/pdf/2406.11741v1) “Transcendence: Generative Models Can Outperform The Experts That Train Them”. The paper states that generative models are trained with the simple objective of imitating the conditional probability distribution induced by the data they are trained on. Therefore, when trained on data generated by humans, we may not expect the artificial model to outperform the humans on their original objectives. This work studies the phenomenon of transcendence: when a generative model achieves capabilities that surpass the abilities of the experts generating its data. The authors demonstrate transcendence by training an autoregressive transformer to play chess from game transcripts, and show that the trained model can sometimes achieve better performance than all players in the dataset. The researchers theoretically prove that transcendence is enabled by low-temperature sampling, and rigorously assess this experimentally. Finally, the paper discusses other sources of transcendence, laying the groundwork for future investigation of this phenomenon in a broader setting. The code is [available here](https://transcendence.eddie.win/).

25. ***Researchers from KAIST, UCL, and KT investigated how large language models (LLMs) acquire factual knowledge during pretraining. They found that more training data does not significantly enhance factual knowledge retention. Factors like training steps and batch sizes impact knowledge acquisition and forgetting, providing insights into LLMs' knowledge dynamics.*** <br><br>
    17 Jun, KAIST, UCL and KT published a [paper](https://arxiv.org/pdf/2406.11813v1) “How Do Large Language Models Acquire Factual Knowledge During Pretraining?”. The paper argues that despite the recent observation that large language models (LLMs) can store substantial factual knowledge, there is a limited understanding of the mechanisms of how they acquire factual knowledge through pretraining. This work addresses this gap by studying how LLMs acquire factual knowledge during pretraining. The findings reveal several important insights into the dynamics of factual knowledge acquisition during pretraining. First, counterintuitively, the authors observe that pretraining on more data shows no significant improvement in the model’s capability to acquire and maintain factual knowledge. Next, there is a power-law relationship between training steps and forgetting of memorization and generalization of factual knowledge, and LLMs trained with duplicated training data exhibit faster forgetting. Third, training LLMs with larger batch sizes can enhance the models’ robustness to forgetting. Overall, the observations suggest that factual knowledge acquisition in LLM pretraining occurs by progressively increasing the probability of factual knowledge presented in the pretraining data at each step. However, this increase is diluted by subsequent forgetting. Based on this interpretation, the paper demonstrates that it can provides plausible explanations for recently observed behaviors of LLMs, such as the poor performance of LLMs on long-tail knowledge and the benefits of deduplicating the pretraining corpus.

27. ***The CVPR 2024 conference received over 11,000 paper submissions and accepted 2,719. The conference announced several awards, including best papers and runners-up, with top topics like image and video synthesis, 3D vision, and human-related studies. Recognized works included those on generative image dynamics and rich human feedback for text-to-image generation.*** <br><br>
    17 - 21 Jun, [CVPR 2024 Seattle](https://media.eventhosts.cc/Conferences/CVPR2024/OpeningRemarkSlides.pdf), Washington State UAS. The conference has received 11532 papers and 2719 were accepted. The conference announced 4 best student paper runners-up, 2 best student papers, 2 best paper runners-up and 2 best papers, which are: 1) [Generative Image Dynamics](https://openaccess.thecvf.com/content/CVPR2024/html/Li_Generative_Image_Dynamics_CVPR_2024_paper.html) by Zhengqi Li et al from Google, and Youwei Liang et al [Rich Human Feedback for Text-to-Image Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Rich_Human_Feedback_for_Text-to-Image_Generation_CVPR_2024_paper.pdf). CVPR also announced Longuet-Higgins Prize to the paper “Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation” by Ross Girshick et al. and a list of other awards. Top three topics of submission are: Image and Video Synthesis and Generation, 3D from multi-view and sensors, and Humans: Face, body, pose, gesture movement. The top three tops of acceptance are Physics-based vision and shape-from-x, Embodied vision: active agents, simulation and Vision+Graphics.

29. ***AIRI and Tinkoff released XLand-100B, a large-scale dataset for in-context reinforcement learning. The dataset includes extensive learning histories for various tasks, aiming to advance research in this field. The dataset and utilities are open-source, providing a foundation for further scaling and democratizing research in reinforcement learning.*** <br><br>
    13 Jun, AIRI and Tinkoff published a [paper](https://arxiv.org/pdf/2406.08973v1) “XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning”. The paper states that following the success of the in-context learning paradigm in large-scale language and computer vision models, the recently emerging field of in-context reinforcement learning is experiencing a rapid growth. However, its development has been held back by the lack of challenging benchmarks, as all the experiments have been carried out in simple environments and on small-scale datasets. The authors present XLand-100B, a large-scale dataset for in-context reinforcement learning based on the XLand-MiniGrid environment, as a first step to alleviate this problem. It contains complete learning histories for nearly 30,000 different tasks, covering 100B transitions and 2.5B episodes. It took 50,000 GPU hours to collect the dataset, which is beyond the reach of most academic labs. Along with the dataset, the word provides the utilities to reproduce or expand it even further. With this substantial effort, the study aims to democratize research in the rapidly growing field of in-context reinforcement learning and provide a solid foundation for further scaling. The code is open-source and available under Apache 2.0 licence at https://github.com/dunno-lab/xland-minigrid-datasets.

31. ***Researchers from NYU, AbacusAI, and Cambridge University emphasized the need for large language models (LLMs) to recognize their knowledge limitations. The study showed that fine-tuning models on a small dataset of correct and incorrect answers enhances uncertainty estimation. This approach improves LLM reliability in high-stakes applications and human-AI collaboration.*** <br><br>
    12 Jun, NYU, AbacusAI and Cambridge Uni published a [paper](https://arxiv.org/pdf/2406.08391) “Large Language Models Must Be Taught to Know What They Don't Know”. The paper states that when using large language models (LLMs) in high-stakes applications, we need to know when we can trust their predictions. Some works argue that prompting high-performance LLMs is sufficient to produce calibrated uncertainties, while others introduce sampling methods that can be prohibitively expensive. This study first argues that prompting on its own is insufficient to achieve good calibration and then shows that fine-tuning on a small dataset of correct and incorrect answers can create an uncertainty estimate with good generalization and small computational overhead. The authors show that a thousand graded examples are sufficient to outperform baseline methods and that training through the features of a model is necessary for good performance and tractable for large open-source models when using LoRA. The authors also investigate the mechanisms that enable reliable LLM uncertainty estimation, finding that many models can be used as general-purpose uncertainty estimators, applicable not just to their own uncertainties but also the uncertainty of other models. Lastly, the paper shows that uncertainty estimates inform human use of LLMs in human-AI collaborative settings through a user study. Code available at [this https URL](https://github.com/activatedgeek/calibration-tuning).

33. ***Harvard University and Google developed a dashboard prototype to increase transparency in conversational AI. The dashboard displays the AI's internal user model, allowing users to see and control the system's behavior. User studies showed that this transparency helps identify biases and increases user control, pointing to future design and research directions.*** <br><br>
    12 Jun, Harvard Uni and Google published a [paper](https://arxiv.org/pdf/2406.07882) “Designing a Dashboard for Transparency and Control of Conversational AI”. The authors find that conversational LLMs function as black box systems, leaving users guessing about why they see the output they do. This lack of transparency is potentially problematic, especially given concerns around bias and truthfulness. To address this issue, the work presents an end-to-end prototype-connecting interpretability techniques with user experience design-that seeks to make chatbots more transparent. The authors begin by showing evidence that a prominent open-source LLM has a "user model": examining the internal state of the system, and extract data related to a user's age, gender, educational level, and socioeconomic status. Next, the authors describe the design of a dashboard that accompanies the chatbot interface, displaying this user model in real time. The dashboard can also be used to control the user model and the system's behavior. Finally, the paper discusses a study in which users conversed with the instrumented system. The results suggest that users appreciate seeing internal states, which helped them expose biased behavior and increased their sense of control. Participants also made valuable suggestions that point to future directions for both design and machine learning research. The project page and video demo of our TalkTuner system are available at https://bit.ly/talktuner-project-page.

35. ***The FAccT24 conference published a paper examining the openness of generative AI systems in light of the upcoming EU AI Act. The study highlighted the discrepancy between claimed openness and actual practices, proposing a multi-dimensional framework for assessing openness. This approach aims to foster accountability, regulation, and informed decision-making in AI development.*** <br><br>
    5 Jun, FAccT24 conference published a [paper](https://dl.acm.org/doi/pdf/10.1145/3630106.3659005) “Rethinking open source generative AI: open washing and the EU AI Act”. The paper argues that The past year has seen a steep rise in generative AI systems that claim to be open. But how open are they really? The question of what counts as open source in generative AI is poised to take on particular importance in light of the upcoming EU AI Act that regulates open source systems differently, creating an urgent need for practical openness assessment. Here the authors use an evidence-based framework that distinguishes 14 dimensions of openness, from training datasets to scientific and technical documentation and from licensing to access methods. Surveying over 45 generative AI systems (both text and text-to-image), the study finds that while the term open source is widely used, many models are ‘open weight’ at best and many providers seek to evade scientific, legal and regulatory scrutiny by withholding information on training and fine-tuning data. The authors argue that openness in generative AI is necessarily composite (consisting of multiple elements) and gradient (coming in degrees), and point out the risk of relying on single features like access or licensing to declare models open or not. Evidence-based openness assessment can help foster a generative AI landscape in which models can be effectively regulated, model providers can be held accountable, scientists can scrutinise generative AI, and end users can make informed decisions. A news article “Not all ‘open source’ AI models are actually open: here’s a ranking” also published by [Nature](https://www.nature.com/articles/d41586-024-02012-5?utm_source=substack&utm_medium=email) on 19 Jun to introduce the research.
 <br><br><br>

***17 June 2024***

1. ***Nvidia Announces Nemotron-4 340B Models:
Nvidia introduced Nemotron-4 340B, a set of open models for generating synthetic data to train large language models for commercial use. These models, which include Nemotron-4-340B-Base, Instruct, and Reward, are distributed under the NVIDIA Open Model License Agreement. They perform competitively on various benchmarks, can fit on a single DGX H100 with 8 GPUs, and are primarily aimed at generating synthetic data for smaller language models. Nvidia is also open-sourcing the synthetic data generation pipeline.*** <br><br>
   14 Jun, Nvidia announced [Nemotron-4 340B](https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf), a family of open models that developers can use to generate synthetic data for training large language models for commercial applications. The models include Nemotron-4-340B-Base, Nemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. These models are open access under the NVIDIA Open Model License Agreement, a permissive model license that allows distribution, modification, and use of the models and its outputs. These models perform competitively to open access models on a wide range of evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in FP8 precision. Nvidia believes that the community can benefit from these models in various research studies and commercial applications, especially for generating synthetic data to train smaller language models. Notably, over 98% of data used in the model alignment process is synthetically generated, showcasing the effectiveness of these models in generating synthetic data. To further support open research and facilitate model development, Nvidia are also open-sourcing the synthetic data generation pipeline used in the model alignment process.

3. ***Lamini.AI on LLM Hallucinations:
Lamini.AI's paper "Banishing LLM Hallucinations Requires Rethinking Generalization" highlights that LLMs often hallucinate despite their capabilities. The study reveals that conventional methods, such as grounding LLMs in external knowledge, fail to prevent hallucinations. The authors propose a new approach using Millions of Memory Experts (MoME) to store and retrieve facts dynamically, introducing Lamini-1, a model designed to reduce hallucinations.*** <br><br>
   13 Jun, Lamini.AI published a [paper](https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf) “Banishing LLM Hallucinations Requires Rethinking Generalization”. The authors find that despite their powerful chat, coding, and reasoning abilities, Large Language Models (LLMs) frequently hallucinate. Conventional wisdom suggests that hallucinations are a consequence of a balance between creativity and factuality, which can be mitigated, but not eliminated, by grounding the LLM in external knowledge sources. Through extensive systematic experiments, the work shows that these traditional approaches fail to explain why LLMs hallucinate in practice. Specifically, the authors show that LLMs augmented with a mixture of Millions of Memory Experts (MoME) can easily memorize large datasets of random numbers. The authors corroborate these experimental findings with a theoretical construction, showing that simple neural networks trained to predict the next token hallucinate when the training loss is above a threshold as it usually does in practice when training on internet scale data. The authors interpret the findings by comparing against traditional retrieval methods for mitigating hallucinations. The authors use the findings to design a first generation model for removing hallucinations - Lamini-1 - that stores facts in a massive mixture of millions of memory experts that are retrieved dynamically.

5. ***OpenVLA: Vision-Language-Action Model:
Stanford, UC Berkeley, Google, Physical Intelligence, and MIT released "OpenVLA," an open-source VLA model. Trained on extensive vision-language data and robot demonstrations, it outperforms existing models in visuomotor control tasks. OpenVLA, built on a Llama 2 language model and visual encoder, shows significant improvements in task success rates and generalization. The study also emphasizes its efficiency and open-source availability.*** <br><br>
   13 Jun, Stanford Uni, UC Berkeley, Google, Physical Intelligence and MIT published a [paper](https://arxiv.org/pdf/2406.09246v1) “OpenVLA: An Open-Source Vision-Language-Action Model”. The paper states that Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, the work introduces OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. The authors further show how effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. The study also explores compute efficiency; as a separate contribution, the authors also show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, the researchers release model checkpoints, fine-tuning notebooks, and the PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.

7. ***Google's Benchmark for LLM Temporal Reasoning:
Google's paper "Test of Time" introduces synthetic datasets to evaluate LLMs' temporal reasoning abilities, addressing limitations of existing benchmarks. These datasets allow systematic analysis of various factors affecting LLM performance in temporal reasoning tasks. The study aims to provide insights into LLMs' strengths and weaknesses in this area and is open-sourcing the datasets and evaluation framework.*** <br><br>
   13 Jun, Google published a [paper](https://arxiv.org/pdf/2406.09170) “Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning”. The paper argues that Large language models (LLMs) have showcased remarkable reasoning capabilities, yet they remain susceptible to errors, particularly in temporal reasoning tasks involving complex temporal logic. Existing research has explored LLM performance on temporal reasoning using diverse datasets and benchmarks. However, these studies often rely on real-world data that LLMs may have encountered during pre-training or employ anonymization techniques that can inadvertently introduce factual inconsistencies. This work addresses these limitations by introducing novel synthetic datasets specifically designed to assess LLM temporal reasoning abilities in various scenarios. The diversity of question types across these datasets enables systematic investigation into the impact of the problem structure, size, question type, fact order, and other factors on LLM performance. The findings provide valuable insights into the strengths and weaknesses of current LLMs in temporal reasoning tasks. To foster further research in this area, the authors are open-sourcing the datasets and evaluation framework used in the experiments: https://huggingface.co/datasets/baharef/ToT.

9. ***Transformers and Neural Algorithmic Reasoners:
Google researchers propose combining Transformers with graph neural network (GNN)-based neural algorithmic reasoners (NARs) to enhance algorithmic reasoning capabilities. The hybrid TransNAR model, evaluated on the CLRS-Text benchmark, shows significant improvements over Transformer-only models. This approach integrates robust algorithmic problem-solving with language understanding.*** <br><br>
    13 Jun, Google published a [paper](https://arxiv.org/pdf/2406.09308) “Transformers meet Neural Algorithmic Reasoners”. The researchers argue thattTransformers have revolutionized machine learning with their simple yet effective architecture. Pre-training Transformers on massive text datasets from the Internet has led to unmatched generalization for natural language understanding (NLU) tasks. However, such language models remain fragile when tasked with algorithmic forms of reasoning, where computations must be precise and robust. To address this limitation, the work proposes a novel approach that combines the Transformer's language understanding with the robustness of graph neural network (GNN)-based neural algorithmic reasoners (NARs). Such NARs proved effective as generic solvers for algorithmic tasks, when specified in graph form. To make their embeddings accessible to a Transformer, the authers propose a hybrid architecture with a two-phase training procedure, allowing the tokens in the language model to cross-attend to the node embeddings from the NAR. The authors evaluate the resulting TransNAR model on CLRS-Text, the text-based version of the CLRS-30 benchmark, and demonstrate significant gains over Transformer-only models for algorithmic reasoning, both in and out of distribution.

11. ***LiveBench: Contamination-Free LLM Benchmark:
Abacus.AI, NYU, Nvidia, UMD, and USC introduced LiveBench, a new benchmark designed to avoid test set contamination and biases from LLM or human judges. LiveBench features frequently updated questions from recent sources and objective scoring. It evaluates models on a variety of challenging tasks, releasing questions, code, and model answers to encourage community collaboration.*** <br><br>
    13 Jun, Abacus.AI, NYU, Nvidia, UMD and USC published a [paper](https://r.search.yahoo.com/_ylt=AwrOqqalx29mbwwpStwL5gt.;_ylu=Y29sbwNncTEEcG9zAzEEdnRpZAMEc2VjA3Ny/RV=2/RE=1718630438/RO=10/RU=https%3a%2f%2flivebench.ai%2flivebench.pdf%3ftrk%3dpublic_post_comment-text/RK=2/RS=CacoKTTtsURlcuTaIZI9GdDK6sI-) “LiveBench: A Challenging, Contamination-Free LLM Benchmark”. The paper argues that Test set contamination, wherein test data from a benchmark ends up in a newer model’s training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. This work introduces a new benchmark for LLMs designed to be immune to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. The authors release LiveBench, the first benchmark that (1) contains frequently updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, bAbI, and IFEval. The authors evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 110B in size. LiveBench is difficult, with top models achieving below 60% accuracy. The authors [release all questions, code, and model answers](https://github.com/LiveBench/LiveBench). Questions will be added and updated on a monthly basis, and will release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. The authors welcome community engagement and collaboration for expanding the benchmark tasks and models. 

13. ***Stability AI's Stable Diffusion 3 Medium:
Stability AI released Stable Diffusion 3 Medium, a sophisticated image generation model using three text encoders and a novel Multimodal Diffusion Transformer (MMDiT). SD3 processes text and pixel latents using a sequence of embeddings and applies a conditional flow-matching objective for training. The TensorRT-optimized version is available for download.*** <br><br>
    12 Jun, Stability AI open released [Stable Diffusion 3 Medium](https://stability.ai/news/stable-diffusion-3-medium), one of the most sophisticated image generation models. SD3 is a latent diffusion model that consists of three different text encoders (CLIP L/14, OpenCLIP bigG/14 and T5-v1.1-XXL), a novel Multimodal Diffusion Transformer (MMDiT) model, and a 16 channel AutoEncoder model that is similar to the one used in Stable Diffusion XL. SD3 processes text inputs and pixel latents as a sequence of embeddings. Positional encodings are added to 2x2 patches of the latents which are then flattened into a patch encoding sequence. This sequence, along with the text encoding sequence are fed into the MMDiT blocks, where they are embedded to a common dimensionality, concatenated, and passed through a sequence of modulated attentions and MLPs. In addition to architectural changes, SD3 applies a conditional flow-matching objective to train the model. In this approach, the forward noising process is defined as a rectified flow that connects the data and noise distributions on a straight line. The TensorRT-optimised version of SD 3 Medium can be downloaded from [Huggingface.](https://huggingface.co/stabilityai/stable-diffusion-3-medium-tensorrt)

15. ***Recaptioning Billions of Web Images with LLaMA-3:
UC Santa Cruz, Uni of Edinburgh, JHU, Adobe, and UT Austin's paper discusses recaptioning 1.3 billion web images using LLaMA-3, enhancing textual descriptions for better vision-language model training. The enhanced dataset, Recap-DataComp-1B, improves model performance in tasks like cross-modal retrieval and text-to-image generation. The project page is available online.*** <br><br>
    12 Jun, UC Santa Cruz, Uni of Edinburgh, JHU, Adobe, and UT Austin published a [paper](https://arxiv.org/pdf/2406.08478) “What If We Recaption Billions of Web Images with LLaMA-3?”. The paper indicates that  Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate that semantically aligning and enriching textual descriptions of these pairs can significantly enhance model training across various vision-language tasks, particularly text-to-image generation. However, large-scale investigations in this area remain predominantly closed-source. The paper aims to bridge this community effort, leveraging the powerful and open-sourced LLaMA-3, a GPT-4 level LLM. The recaptioning pipeline is simple: first, the authors fine-tune a LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images from the DataComp-1B dataset. The empirical results confirm that this enhanced dataset, Recap-DataComp-1B, offers substantial benefits in training advanced vision-language models. For discriminative models like CLIP, the auhtors observe enhanced zero-shot performance in cross-modal retrieval tasks. For generative models like text-to-image Diffusion Transformers, the generated images exhibit a significant improvement in alignment with users' text instructions, especially in following complex queries. The project page is https://www.haqtu.me/Recap-Datacomp-1B/

17. ***TextGrad: Automatic Differentiation via Text:
Stanford's paper introduces TextGrad, a framework for automatic differentiation using textual feedback from LLMs to optimize compound AI systems. TextGrad improves performance across various tasks, showcasing its flexibility and ease of use. The study demonstrates significant gains in accuracy and efficiency in diverse applications like question answering and molecule optimization.*** <br><br>
    11 Jun, Stanford Uni published a [paper](https://arxiv.org/pdf/2406.07496) “TextGrad: Automatic Differentiation via Text”. The paper argues that AI is undergoing a paradigm shift, with breakthroughs achieved by systems orchestrating multiple large language models (LLMs) and other complex components. As a result, developing principled and automated optimization methods for compound AI systems is one of the most important new challenges. Neural networks faced a similar challenge in its early days until backpropagation and automatic differentiation transformed the field by making optimization turn-key. Inspired by this, the work introduces TextGrad, a powerful framework performing automatic “differentiation'' via text. TextGrad backpropagates textual feedback provided by LLMs to improve individual components of a compound AI system. In the framework, LLMs provide rich, general, natural language suggestions to optimize variables in computation graphs, ranging from code snippets to molecular structures. TextGrad follows PyTorch's syntax and abstraction and is flexible and easy-to-use. It works out-of-the-box for a variety of tasks, where the users only provide the objective function without tuning components or prompts of the framework. The authors showcase TextGrad's effectiveness and generality across a diverse range of applications, from question answering and molecule optimization to radiotherapy treatment planning. Without modifying the framework, TextGrad improves the zero-shot accuracy of GPT-4o in Google-Proof Question Answering from 51% to 55%, yields 20% relative performance gain in optimizing LeetCode-Hard coding problem solutions, improves prompts for reasoning, designs new druglike small molecules with desirable in silico binding, and designs radiation oncology treatment plans with high specificity. TextGrad lays a foundation to accelerate the development of the next-generation of AI systems.

19. ***Masked Diffusion Language Models:
Cornell's study reveals that simple masked discrete diffusion models perform better than previously thought in language modeling. By applying an effective training recipe and a simplified objective, these models achieve state-of-the-art performance among diffusion models and approach autoregressive perplexity. The authors release the code for further exploration.*** <br><br>
    11 Jun, Cornell Uni published [paper](https://arxiv.org/pdf/2406.07524) “Simple and Effective Masked Diffusion Language Models”. The authors state that while diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling. This study shows that simple masked discrete diffusion is more performant than previously thought. The authors apply an effective training recipe that improves the performance of masked diffusion models and derive a simplified, Rao-Blackwellized objective that results in additional improvements. The objective has a simple form -- it is a mixture of classical masked language modeling losses -- and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model. On language modeling benchmarks, a range of masked diffusion models trained with modern engineering practices achieves a new state-of-the-art among diffusion models, and approaches AR perplexity. The authors release the code at: https://github.com/kuleshov-group/mdlm

21. ***Husky: Open-Source Language Agent for Multi-Step Reasoning:
Uni of Washington and Meta introduced Husky, an open-source language agent designed for complex reasoning tasks. Husky uses a unified action space and expert models to execute actions iteratively. The study shows Husky outperforms previous agents and matches frontier LMs like GPT-4 in various tasks. The code and models are available online.*** <br><br>
    10 Jun, Uni of Washington and Meta published a [paper](https://arxiv.org/pdf/2406.06469) “Husky A Unified, Open-Source Language Agent for Multi-Step Reasoning”. The study finds thatl language agents perform complex tasks by using tools to execute each step precisely. However, most existing agents are based on proprietary models or designed to target specific tasks, such as mathematics or multi-hop question answering. The work introduces Husky, a holistic, open-source language agent that learns to reason over a unified action space to address a diverse set of complex tasks involving numerical, tabular, and knowledge-based reasoning. Husky iterates between two stages: 1) generating the next action to take towards solving a given task and 2) executing the action using expert models and updating the current solution state. The authors identify a thorough ontology of actions for addressing complex tasks and curate high-quality data to train expert models for executing these actions. The experiments show that Husky outperforms prior language agents across 14 evaluation datasets. Moreover, the researchd introduces HuskyQA, a new evaluation set which stress tests language agents for mixed-tool reasoning, with a focus on retrieving missing knowledge and performing numerical reasoning. Despite using 7B models, Husky matches or even exceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of the holistic approach in addressing complex reasoning problems. the code and models are available at https://github.com/agent-husky/Husky-v1.

23. ***Emerging Technology Risks and AI Risk Mitigation Tactics:
A study with Boston Consulting Group finds that junior professionals are not effective in teaching senior professionals to use generative AI due to their limited experience with emerging technologies. The research identifies three novice AI risk mitigation tactics that focus on human routines and project-level interventions rather than system design, highlighting the challenges of upskilling in rapidly changing tech environments.*** <br><br>
    10 Jun, MIT, Harvard, Boston Consulting Group, Warwick Business School etc. published [paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4857373) “Don't Expect Juniors to Teach Senior Professionals to Use Generative AI: Emerging Technology Risks and Novice AI Risk Mitigation Tactics”. The literature on communities of practice demonstrates that a proven way for senior professionals to upskill themselves in the use of new technologies that undermine existing expertise is to learn from junior professionals. It notes that juniors may be better able than seniors to engage in real-time experimentation close to the work itself, and may be more willing to learn innovative methods that conflict with traditional identities and norms. However, this literature has not explored emerging technologies, which are seen to pose new risks to valued outcomes because of their uncertain and wide-ranging capabilities, exponential rate of change, potential for outperforming humans in a wide variety of skilled and cognitive tasks, and dependence on a vast, varied, and high volume of data and other inputs from a broad ecosystem of actors. It has also not explored obstacles to junior professionals being a source of expertise in the use of new technologies for more senior members in contexts where the juniors themselves are not technical experts, and where technology is so new and rapidly changing that the juniors have had little experience with using it. However, such contexts may be increasingly common. In this study conducted with Boston Consulting Group, a global management consulting firm, the authors interviewed 78 such junior consultants in July-August 2023 who had recently participated in a field experiment that gave them access to generative AI (GPT-4) for a business problem solving task. Drawing from junior professionals’ in situ reflections soon after the experiment, the researchers argue that such juniors may fail to be a source of expertise in the use of emerging technologies for more senior professionals; instead, they may recommend three kinds of novice AI risk mitigation tactics that: 1) are grounded in a lack of deep understanding of the emerging technology’s capabilities, 2) focus on change to human routines rather than system design, and 3) focus on interventions at the project-level rather than system deployer- or ecosystem-level.

25. ***LLMs as Text-Based World Simulators:
UC Santa Cruz and collaborators assess if LLMs can serve as world simulators for complex planning tasks using a new benchmark, ByteSized32-State-Prediction. The study finds that LLMs like GPT-4, while impressive, are still unreliable without further innovations. The work contributes insights into LLM capabilities and introduces a novel benchmark for future research.*** <br><br>
    10 Jun, Uni of Arizona, Microsoft, New York Uni, Johns Hopkins Uni and Allen Inst for AI released their ACL 2024 [paper](https://arxiv.org/pdf/2406.06485) “Can Language Models Serve as Text-Based World Simulators”. The authors state that virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? The goal of the study is to answer this question in the context of text-based simulators. The approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. The authors use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. The work tests GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM's capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.

27. ***ShiftAddLLM: Multiplication-Less Reparameterization:
Researchers propose ShiftAddLLM, a method to accelerate pretrained LLMs by replacing multiplications with shift-and-add operations, reducing memory usage and latency. The study demonstrates significant improvements in efficiency and performance across various tasks. The codes and models are available online.*** <br><br>
    10 Jun, Georgia Inst of Tech, Intel and Google published a [paper](https://arxiv.org/pdf/2406.05981) “ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization”.  The paper argues that Large language models (LLMs) have shown impressive performance on language tasks but face challenges when deployed on resource-constrained devices due to their extensive parameters and reliance on dense multiplications, resulting in high memory demands and latency bottlenecks. Shift-and-add reparameterization offers a promising solution by replacing costly multiplications with hardware-friendly primitives in both the attention and multi-layer perceptron (MLP) layers of an LLM. However, current reparameterization techniques require training from scratch or full parameter fine-tuning to restore accuracy, which is resource-intensive for LLMs. Thef work proposes accelerating pretrained LLMs through post-training shift-and-add reparameterization, creating efficient multiplication-free models, dubbed ShiftAddLLM. Specifically, the authors quantize each weight matrix into binary matrices paired with group-wise scaling factors. The associated multiplications are reparameterized into (1) shifts between activations and scaling factors and (2) queries and adds according to the binary matrices. To reduce accuracy loss, the study presents a multi-objective optimization method to minimize both weight and output activation reparameterization errors. Additionally, based on varying sensitivity across layers to reparameterization, the authors develop an automated bit allocation strategy to further reduce memory usage and latency. Experiments on five LLM families and eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points at comparable or lower latency compared to the most competitive quantized LLMs at 3 and 2 bits, respectively, and more than 80% memory and energy reductions over the original LLMs. Codes and models are available at [this https URL](https://github.com/GATECH-EIC/ShiftAddLLM).

29. ***Scaling Neural Machine Translation to 200 Languages:
Meta's paper discusses scaling neural machine translation (NMT) to 200 languages using a sparsely gated mixture of experts architecture. The model improves translation quality by 44% compared to previous state-of-the-art models. The contributions aim to reduce digital inequities by focusing on low-resource languages and are available for non-commercial use.*** <br><br>
    5 Jun, Meta published a [paper](https://www.nature.com/articles/s41586-024-07335-x) on Nature “Scaling neural machine translation to 200 languages”. The paper states that The development of neural techniques has opened up new avenues for research in machine translation. Today, neural machine translation (NMT) systems can leverage highly multilingual capacities and even perform zero-shot translation, delivering promising results in terms of language coverage and quality. However, scaling quality NMT requires large volumes of parallel bilingual data, which are not equally available for the 7,000+ languages in the world1. Focusing on improving the translation qualities of a relatively small group of high-resource languages comes at the expense of directing research attention to low-resource languages, exacerbating digital inequities in the long run. To break this pattern, here the authors introduce No Language Left Behind— a single massively multilingual model that leverages transfer learning across languages. The researchers developed a conditional computational model based on the Sparsely Gated Mixture of Experts architecture, which are trained on data obtained with new mining techniques tailored for low-resource languages. Furthermore, the study devised multiple architectural and training improvements to counteract overftting while training on thousands of tasks. The authors evaluated the performance of the model over 40,000 translation directions using tools created specifcally for this purpose—an automatic benchmark (FLORES-200), a human evaluation metric (XSTS) and a toxicity detector that covers every language in the model. Compared with the previous state-of-the-art models, the model achieves an average of 44% improvement in translation quality as measured by BLEU. By demonstrating how to scale NMT to 200 languages and making all contributions in this effort freely available for non-commercial use, the work lays important groundwork for the development of a universal translation system.

31. ***Necessity of Human Annotation in Fairness Benchmarks:
Uni of Southern California's study concludes that GPT-3.5-Turbo is not suitable for annotating bias benchmarks, as it produces poor quality outputs. The study emphasizes the importance of human annotation for sensitive tasks related to social biases and highlights the limitations of relying on LLMs for this purpose.*** <br><br>
    24 May, Uni of Southern California released their ACL 2024 [paper](https://arxiv.org/pdf/2405.15760) “GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction”. The authors argue that social biases in LLMs are usually measured via bias benchmark datasets. Current benchmarks have limitations in scope, grounding, quality, and human effort required. Previous work has shown success with a community-sourced, rather than crowd-sourced, approach to benchmark development. However, this work still required considerable effort from annotators with relevant lived experience. This paper explores whether an LLM (specifically, GPT-3.5-Turbo) can assist with the task of developing a bias benchmark dataset from responses to an open-ended community survey. The word also extends the previous work to a new community and set of biases: the Jewish community and antisemitism. The analysis shows that GPT-3.5-Turbo has poor performance on this annotation task and produces unacceptable quality issues in its output. Thus, the study concludes that GPT-3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use actually negates many of the benefits of community-sourcing bias benchmarks.
 <br><br><br>

***10 June 2024***

1. ***Together AI, Duke University, Stanford University, and others have published a paper on "Mixture-of-Agents Enhances Large Language Model Capabilities". They propose leveraging multiple large language models (LLMs) through a Mixture-of-Agents (MoA) approach. This method uses layered MoA architecture to surpass the performance of existing models like GPT-4 Omni in various benchmarks.*** <br><br>
   7 Jun, Together AI, Duke Uni, Stanford Uni and others published a [paper](https://arxiv.org/abs/2406.04692) “Mixture-of-Agents Enhances Large Language Model Capabilities”. The paper indicates that recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. The proposed approach constructs a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieve state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omn.

2. ***OpenAI published a paper on sparse autoencoders, highlighting their potential for extracting interpretable features from language models through a sparse bottleneck layer. To address the challenges of balancing reconstruction and sparsity and managing dead latents, the authors propose using k-sparse autoencoders to directly control sparsity. They found that modifications to this method reduce dead latents, even at large scales, and observed clean scaling laws related to autoencoder size and sparsity. The study introduced new metrics for feature quality, which generally improve with autoencoder size. A demonstration involved training a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens, with code and autoencoders released for open-source models.*** <br><br>
   6 Jun, OpenAI published a paper “Scaling and evaluating sparse autoencoders”. The paper finds that sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. The authors propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, the study finds modifications that result in few dead latents, even at the largest scales tried. Using these techniques, the authors find clean scaling laws with respect to autoencoder size and sparsity. The researchers also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of the approach, the work trains a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. The authors release code and autoencoders for open-source models, as well as a visualizer. 
4. ***A researcher from JKU Linz, Austria, has published a paper on "Vision-LSTM: xLSTM as Generic Vision Backbone". The paper extends the Long Short Term Memory (LSTM) into a scalable architecture known as xLSTM for use in computer vision, presenting Vision-LSTM (ViL) as a promising new backbone for vision architectures.*** <br><br>
   6 Jun, researcher from JKU Linz, Austria published a [paper](https://arxiv.org/pdf/2406.04303) “Vision-LSTM: xLSTM as Generic Vision Backbone”. The paper states that Transformers are widely used as generic backbones in computer vision, despite initially introduced for natural language processing. Recently, the Long Short Term Memory (LSTM) has been extended to a scalable and performant architecture – the xLSTM – which overcomes long-standing LSTM limitations via exponential gating and parallelizable matrix memory structure. This paper introduces Vision-LSTM (ViL), an adaption of the xLSTM building blocks to computer vision. ViL comprises a stack of xLSTM blocks where odd blocks process the sequence of patch tokens from top to bottom while even blocks go from bottom to top. Experiments show that ViL holds promise to be further deployed as new generic backbone for computer vision  architectures. Project page: https://nx-ai.github.io/vision-lstm/

5. ***Google published a paper on "Open-Endedness is Essential for Artificial Superhuman Intelligence", arguing that achieving open-ended, ever-improving AI is critical for artificial superhuman intelligence (ASI). The paper outlines a path towards ASI through open-ended systems built on foundation models and examines the safety implications of such advancements.*** <br><br>
   6 Jun, Google published a [paper](https://arxiv.org/pdf/2406.04268) “Open-Endedness is Essential for Artificial Superhuman Intelligence”. The paper argues that in recent years there has been a tremendous surge in the general capabilities of AI systems, mainly fuelled by training foundation models on internetscale data. Nevertheless, the creation of openended, ever self-improving AI remains elusive. This position paper argues that the ingredients are now in place to achieve openendedness in AI systems with respect to a human observer. Furthermore, the authors claim that such open-endedness is an essential property of any artificial superhuman intelligence (ASI). The study begins by providing a concrete formal definition of open-endedness through the lens of novelty and learnability. It then illustrates a path towards ASI via open-ended systems built on top of foundation models, capable of making novel, humanrelevant discoveries. The authors conclude by examining the safety implications of generally-capable openended AI. The authors expect that open-ended foundation models will prove to be an increasingly fertile and safety-critical area of research in the near future.

7. ***The University of Edinburgh and others published a paper titled "Are We Done with MMLU?" highlighting errors in the Massive Multitask Language Understanding (MMLU) benchmark. They introduce MMLU-Redux, a re-annotated subset of MMLU, advocating for revising the original benchmark to improve its utility and reliability.*** <br><br>
   6 Jun, Uni of Edinburgh et al. published a [paper](https://arxiv.org/pdf/2406.04127) “Are We Done with MMLU?”. The answer by the paper is Maybe not. The authors identify and analyse errors in the popular Massive Multitask Language Understanding (MMLU) benchmark. Even though MMLU is widely adopted, the analysis demonstrates numerous ground truth errors that obscure the true capabilities of LLMs. For example, the study finds that 57% of the analysed questions in the Virology subset contain errors. To address this issue, the paper introduces a comprehensive framework for identifying dataset errors using a novel error taxonomy. Then, the authors create MMLU-Redux, which is a subset of 3,000 manually re-annotated questions across 30 MMLU subjects. Using MMLU-Redux, the paper demonstrates significant discrepancies with the model performance metrics that were originally reported. The results strongly advocate for revising MMLU's error-ridden questions to enhance its future utility and reliability as a benchmark. Therefore, the authors open up MMLU-Redux for additional annotation [this https URL](https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux).

9. ***AWS published a paper on "Choice of PEFT Technique in Continual Learning: Prompt Tuning is Not All You Need". They argue that prompt tuning, a common method in continual learning, has not been thoroughly vetted and can hurt overall performance. Their research suggests using LoRA instead, achieving better accuracy and performance.*** <br><br>
    5 Jun, AWS published a [paper](https://arxiv.org/pdf/2406.03216) “Choice of PEFT Technique in Continual Learning: Prompt Tuning is Not All You Need”. The authors indicate that recent Continual Learning (CL) methods have combined pretrained Transformers with prompt tuning, a parameter-efficient fine-tuning (PEFT) technique. The authors argue that the choice of prompt tuning in prior works was an undefended and unablated decision, which has been uncritically adopted by subsequent research, but warrants further research to understand its implications. This work conducts this research and finds that the choice of prompt tuning as a PEFT method hurts the overall performance of the CL system. To illustrate this, the study replaces prompt tuning with LoRA in two state-of-the-art continual learning methods: Learning to Prompt and S-Prompts. These variants consistently achieve higher accuracy across a wide range of domain-incremental and class-incremental benchmarks, while being competitive in inference speed. The work highlights a crucial argument: unexamined choices can hinder progress in the field, and rigorous ablations, such as the PEFT method, are required to drive meaningful adoption of CL techniques in real-world applications.

11. ***The University of Oxford and Cambridge published a paper on "HelloFresh LLM Evaluations on Streams of Real-World Human Editorial Actions". The paper introduces HelloFresh, a benchmark based on real-world data from community notes and Wikipedia edits, providing a more dynamic and contamination-free evaluation of LLM capabilities.*** <br><br>
    5 Jun, Uni of Oxford and Cambridge published a [paper](https://arxiv.org/pdf/2406.03428v1) “HelloFresh LLM Evaluations on Streams of Real-World Human Editorial Actions across X Community Notes and Wikipedia edits”. The paper states that benchmarks have been essential for driving progress in machine learning. A better understanding of LLM capabilities on real world tasks is vital for safe development. Designing adequate LLM benchmarks is challenging: Data from real-world tasks is hard to collect, public availability of static evaluation data results in test data contamination and benchmark overfitting, and periodically generating new evaluation data is tedious and may result in temporally inconsistent results. The paper introduces HelloFresh, based on continuous streams of real-world data generated by intrinsically motivated human labelers. It covers recent events from X (formerly Twitter) community notes and edits of Wikipedia pages, mitigating the risk of test data contamination and benchmark overfitting. Any X user can propose an X note to add additional context to a misleading post (formerly tweet); if the community classifies it as helpful, it is shown with the post. Similarly, Wikipedia relies on community-based consensus, allowing users to edit articles or revert edits made by other users. Verifying whether an X note is helpful or whether a Wikipedia edit should be accepted are hard tasks that require grounding by querying the web. The authors backtest state-of-the-art LLMs supplemented with simple web search access and find that HelloFresh yields a temporally consistent ranking. To enable continuous evaluation on HelloFresh, the research team hosts a public leaderboard and periodically updated evaluation data at https://tinyurl.com/hello-fresh-LLM.

13. ***Google published a paper on "To Believe or Not to Believe Your LLM", exploring uncertainty quantification in LLMs. They present an information-theoretic metric for detecting high epistemic uncertainty, allowing for more reliable output and identifying hallucinations in both single- and multi-answer responses.*** <br><br>
    5 Jun, Google published a [paper](https://arxiv.org/pdf/2406.02543) “To Believe or Not to Believe Your LLM”. The authors explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. The paper simultaneously considers both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, the authors derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. The researchers conduct a series of experiments which demonstrate the advantage of the formulation. Further, the investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.

15. ***EPFL, Microsoft, and others published a paper on "Training of Physical Neural Networks". The paper discusses physical neural networks (PNNs) as a promising area for scaling AI models, suggesting that future research could enable PNNs to perform computations locally on edge devices, potentially revolutionizing AI model training and inference.*** <br><br>
    5 Jun, EPFL, Microsoft and others published a [paper](https://arxiv.org/pdf/2406.03372) “Training of Physical Neural Networks”. The article indicates that Physical neural networks (PNNs) are a class of neural-like networks that leverage the properties of physical systems to perform computation. While PNNs are so far a niche research area with small-scale laboratory demonstrations, they are arguably one of the most underappreciated important opportunities in modern AI. Could we train AI models 1000x larger than current ones? Could we do this and also have them perform inference locally and privately on edge devices, such as smartphones or sensors? Research over the past few years has shown that the answer to all these questions is likely "yes, with enough research": PNNs could one day radically change what is possible and practical for AI systems. To do this will however require rethinking both how AI models work, and how they are trained - primarily by considering the problems through the constraints of the underlying hardware physics. To train PNNs at large scale, many methods including backpropagation-based and backpropagation-free approaches are now being explored. These methods have various trade-offs, and so far no method has been shown to scale to the same scale and performance as the backpropagation algorithm widely used in deep learning today. However, this is rapidly changing, and a diverse ecosystem of training techniques provides clues for how PNNs may one day be utilized to create both more efficient realizations of current-scale AI models, and to enable unprecedented-scale models.

17. ***New York University and Adobe published a paper on "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead". They introduce QJL, a quantization method that reduces KV cache memory usage without compromising accuracy, demonstrating significant memory savings and faster runtime in LLM applications.*** <br><br>
    5 Jun, New York Uni and Adobe published a [paper](https://arxiv.org/pdf/2406.03482) “QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead”. The authors point out that serving LLMs requires substantial memory due to the storage requirements of Key-Value (KV) embeddings in the KV cache, which grows with sequence length. An effective approach to compress KV cache is quantization. However, traditional quantization methods face significant memory overhead due to the need to store quantization constants (at least a zero point and a scale) in full precision per data block. Depending on the block size, this overhead can add 1 or 2 bits per quantized number. The work introduces QJL, a new quantization approach that consists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit quantization. In contrast to existing methods, QJL eliminates memory overheads by removing the need for storing quantization constants. The authors propose an asymmetric estimator for the inner product of two vectors and demonstrate that applying QJL to one vector and a standard JL transform without quantization to the other provides an unbiased estimator with minimal distortion. The researchers have developed an efficient implementation of the QJL sketch and its corresponding inner product estimator, incorporating a lightweight CUDA kernel for optimized computation. When applied across various LLMs and NLP tasks to quantize the KV cache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV cache memory usage without compromising accuracy, all while achieving faster runtime. Codes are available at [this https URL](https://github.com/amirzandieh/QJL).

19. ***The US National Institute of Standards and Technology (NIST) updated their "ARIA pilot evaluation plan" to improve risk and impact assessments for AI. The ARIA program includes model testing, red teaming, and field testing to connect AI capabilities with real-world risks and impacts, aiming to enhance AI safety and reliability.*** <br><br>
    5 Jun, the US National Institute of Standards and Technology (NIST) [updated](https://ai-challenges.nist.gov/uassets/7)  “The draft NIST assessing risks and impacts of ai - ARIA pilot evaluation plan”. ARIA program aims to improve the quality of risk and impact assessments for the field of safe and trustworthy AI. Long-term programmatic outcomes may include guidelines, tools, evaluation methodologies, and measurement methods. ARIA has three levels of evaluations– 1) model testing to confirm claimed capabilities, 2) red teaming to stress test applications, and 3) field testing to investigate how people engage with AI in regular use. By tracing tasks across three different evaluation levels, ARIA can provide more direct knowledge about how AI capabilities (in model testing) connect to risks (in red teaming) and positive and negative impacts (in regular use field testing) in the real world. ARIA will address gaps in societal impact assessments by expanding the scope of study to include people, and how they adapt to AI technology in quasi-real world conditions. Current approaches do not adequately cover AI’s systemic impacts or consider how people interact with AI technology and act upon AI generated information . This isolation from real world contexts makes it difficult to anticipate and estimate real world failures. ARIA will provide insights about the applicability of testing approaches for evaluating specific risks, and the effectiveness of AI guardrails and risk mitigations. Trained assessors will evaluate ARIA evaluation output, including prompts and interactive data and sequences from red teamers and field testers. All ARIA evaluation data will be provided to the participant community for modeling and examination of how risks may arise in various settings. NIST evaluations are open to all who find them of interest, are able to submit their technology for testing, and can comply with the evaluation rules.

21. ***According to Singapore Business Review, the Singapore Infocomm Media Development Authority launched Project Moonshot for AI model safety. AI Verify-Project Moonshot is an open-source toolkit for testing the security and safety of LLMs, developed in collaboration with industry partners and aiming to establish global testing standards.*** <br><br>
    5 Jun, according to [Singapore Business Review](https://sbr.com.sg/information-technology/news/imda-launches-project-moonshot-ai-model-safety), Singapore Infocomm Media Development Authority launches Project Moonshot for AI model safety. Minister for Communications and Information Josephine Teo has launched AI Verify-Project Moonshot, a testing toolkit designed to address security and safety challenges often associated with the use of large language models. AI Verify-Project Moonshot is one of the first open-source tools to bring red-teaming, benchmarking, and baseline testing together in a platform. “An open beta, Project Moonshot aims to provide intuitive results of the quality and safety of a model or application in an easily understood manner, even for a non-technical user,” authorities said. IMDA developed the project with DataRobot, IBM, Singtel, and Temasek to ensure that the tool is useful and aligned with industry needs. Project Moonshot is also part of the move towards global testing standards. AI testing organisations – AI Verify Foundation (AIVF) and MLCommons – have signed a memorandum of intent to collaborate on building a common safety benchmark suite.

23. ***The University of Stuttgart published a paper on "Deception abilities emerged in large language models". The paper reveals that state-of-the-art LLMs have developed deception strategies, raising concerns about their alignment with human values and the potential for deceptive behavior to bypass monitoring efforts.*** <br><br>
    4 Jun, Uni of Suttgart published a [paper](https://www.pnas.org/doi/full/10.1073/pnas.2317967121) on PNAS “Deception abilities emerged in large language models”. The paper argues that Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, but were nonexistent in earlier LLMs. The paper conducts a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can trigger misaligned deceptive behavior. GPT-4, for instance, exhibits deceptive behavior in simple test scenarios 99.16% of the time (P < 0.001). In complex second-order deception test scenarios where the aim is to mislead someone who expects to be deceived, GPT-4 resorts to deceptive behavior 71.46% of the time (P < 0.001) when augmented with chain-of-thought reasoning. In sum, revealing hitherto unknown machine behavior in LLMs, the study contributes to the nascent field of machine psychology.

25. ***Bloomberg reported that OpenAI employees are advocating for protections to speak out about AI risks. They highlight the challenges posed by confidentiality agreements and propose measures to ensure transparency and address concerns about the implications of artificial general intelligence.*** <br><br>
    4 Jun, Bloomberg published an [article](https://www.bloomberg.com/news/articles/2024-06-04/openai-employees-call-for-protections-to-speak-out-on-ai-risks?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTcxNzUxNjM0OCwiZXhwIjoxNzE4MTIxMTQ4LCJhcnRpY2xlSWQiOiJTRUtBMTNEV1gyUFMwMCIsImJjb25uZWN0SWQiOiJFODA3NUYyRkZGMjA0NUI2QTlEQzA5M0EyQTdEQTE4NiJ9.wFpVycx4Gun0EF4R47ffHeOt1f9W2VdyR7pb5A8jF5E&sref=10lNAhZ9) “OpenAI Employees Want Protections to Speak Out on ‘Serious Risks’ of AI”. The article reveals that current and former employees of OpenAI and Google DeepMind are advocating for protections to voice concerns about the potential risks of AI technologies. They highlight the challenges posed by broad confidentiality agreements, which prevent them from publicly addressing these issues. Recent controversies at OpenAI, including the dissolution of a safety team and staff departures, have amplified concerns. The employees propose measures such as prohibiting non-disparagement agreements for risk-related concerns and establishing anonymous channels for raising issues with company boards and regulators. Despite OpenAI's assurance of engagement with various stakeholders, concerns persist regarding transparency and readiness for the implications of artificial general intelligence. 

27. ***The University of Waterloo, Toronto, and CMU published a paper on "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark". They introduce MMLU-Pro, an enhanced dataset with more challenging reasoning questions, showing significant drops in accuracy and better performance with Chain of Thought reasoning compared to the original MMLU.*** <br><br>
    4 Jun, Uni. Waterloo, Toronto and CMU published a [paper](https://arxiv.org/pdf/2406.01574v1) “MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark”. The paper indicates that the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro. Additionally, the study found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. The assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field.

29. ***The European Data Protection Supervisor (EDPS) published guidelines on "Generative AI and the EUDPR" to help EUIs comply with data protection regulations. The guidelines provide advice on data protection impact assessments and emphasize core principles to address the risks and opportunities of generative AI systems.*** <br><br>
    3 Jun, the European Data Protection Supervisor (EDPS) published [guideline](https://www.edps.europa.eu/system/files/2024-06/24-06-03_genai_orientations_en.pdf) titled 'Generative AI and the EUDPR: First EDPS Orientations for ensuring data protection compliance when using Generative AI systems.' The guidelines aim to help EUIs comply with the data protection obligations set out in Regulation (EU) 2018/1725, when using or developing generative AI tools. Wojciech Wiewiórowski, EDPS, said: “The guidelines that I have issued today on generative AI are a first step towards more extensive recommendations in response to the evolving landscape of generative AI tools, which my team and I continue to monitor and analyse closely. Our advice published today is drafted with the aim of covering as many possible scenarios involving the use of generative AI, to provide enduring advice to EUIs so that they can protect individuals’ personal information and privacy.” To ensure their practical application by EUIs, the guidelines emphasize on data protection’s core principles, combined with concrete examples, as an aid to anticipate risks, challenges and opportunities of generative AI systems and tools. As such, the guidelines focus on a series of important topics, including advice on how EUIs can distinguish whether the use of such tools involves the processing of individuals’ data; when to conduct a data protection impact assessment; and other essential recommendations.

31. ***Stanford University published a paper on "Show, Don't Tell: Aligning Language Models with Demonstrated Feedback". The paper introduces DITTO, a method for aligning LLMs with user-demonstrated behaviors using a small number of demonstrations, outperforming other methods in fine-grained style and task alignment.*** <br><br>
    3 Jun, Stanford Uni published a [paper](https://arxiv.org/pdf/2406.00888) “Show, Don't Tell: Aligning Language Models with Demonstrated Feedback”. The paper finds that language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. The authors argue that it is instead possible to align an LLM to a specific setting by leveraging a very small number (<10) of demonstrations as feedback. The proposed method, Demonstration ITerated Task Optimization (DITTO), directly aligns language model outputs to a user's demonstrated behaviors. Derived using ideas from online imitation learning, DITTO cheaply generates online comparison data by treating users' demonstrations as preferred over output from the LLM and its intermediate checkpoints. The study evaluates DITTO's ability to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, the authors conduct a user study soliciting a range of demonstrations from participants (N=16). Across the benchmarks and user study, the work finds that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an average of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs. 

33. ***The University of Chicago, Google, and Drexel University published a paper on "The Impossibility of Fair LLMs". The paper argues that achieving fairness in LLMs is inherently limited by the complexity of human-AI interactions. They propose guidelines for realistic fairness goals, emphasizing context, developer responsibility, and stakeholder participation.*** <br><br>
    28 May, Uni of Chicago, Google, and Drexel Uni published a [paper](https://arxiv.org/pdf/2406.03198) in CHI-HEAL 2024 conference “The Impossibility of Fair LLMs”. The authors argue that the need for fair AI is increasingly clear in the era of general-purpose systems such as ChatGPT, Gemini, and other large language models (LLMs). However, the increasing complexity of human-AI interaction and its social impacts have raised questions of how fairness standards could be applied. The paper reviews the technical frameworks that machine learning researchers have used to evaluate fairness, such as group fairness and fair representations, and find that their application to LLMs faces inherent limitations. The work shows that each framework either does not logically extend to LLMs or presents a notion of fairness that is intractable for LLMs, primarily due to the multitudes of populations affected, sensitive attributes, and use cases. To address these challenges, the authors develop guidelines for the more realistic goal of achieving fairness in particular use cases: the criticality of context, the responsibility of LLM developers, and the need for stakeholder participation in an iterative process of design and evaluation. Moreover, it may eventually be possible and even necessary to use the general-purpose capabilities of AI systems to address fairness challenges as a form of scalable AI-assisted alignment.
 
35. ***MIT published a paper on "Large language models can be zero-shot anomaly detectors for time series?" presenting a framework for using LLMs in time series anomaly detection. While LLMs show potential, state-of-the-art deep learning models still outperform them, highlighting the need for further research in this area.*** <br><br>
    23 May, MIT published a [paper]() “Large language models can be zero-shot anomaly detectors for time series?”. The paper finds thatrRecent studies have shown the ability of large language models to perform a variety of tasks, including time series forecasting. The flexible nature of these models allows them to be used for many applications. In this paper, the authors present a novel study of large language models used for the challenging task of time series anomaly detection. This problem entails two aspects novel for LLMs: the need for the model to identify part of the input sequence (or multiple parts) as anomalous; and the need for it to work with time series data rather than the traditional text input. The work introduces sigllm, a framework for time series anomaly detection using large language models. The framework includes a time-series-to-text conversion module, as well as end-to-end pipelines that prompt language models to perform time series anomaly detection. We investigate two paradigms for testing the abilities of large language models to perform the detection task. First, the authors present a prompt-based detection method that directly asks a language model to indicate which elements of the input are anomalies. Second, the researchers leverage the forecasting capability of a large language model to guide the anomaly detection process. The study evaluated the framework on 11 datasets spanning various sources and 10 pipelines. The authors show that the forecasting method significantly outperformed the prompting method in all 11 datasets with respect to the F1 score. Moreover, while large language models are capable of finding anomalies, state-of-the-art deep learning models are still superior in performance, achieving results 30% better than large language models.
 <br><br><br>

***3rd June 2024***

1. ***Princeton Uni and CMU Paper on "Transformers and SSMs": The paper explores the relationship between Transformers and state-space models (SSMs), particularly highlighting the emergence of Mamba as a competitive alternative to Transformers in language modeling. It introduces the State Space Duality (SSD) framework, connecting SSMs and attention mechanisms, leading to the development of Mamba-2, a faster architecture maintaining competitiveness with Transformers.*** <br><br>
   1 Jun, Princeton Uni and CMU published a [paper](https://arxiv.org/pdf/2405.21060) “Transformers are SSMs Generalized Models and Efficient Algorithms Through Structured State Space Duality”. The authors find that while Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. The research shows that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. The proposed state space duality (SSD) framework allows to design a new architecture (Mamba-2) whose core layer is a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.

3. ***Databricks and MIT Paper on "Perplexity-Based Data Pruning": Investigating data pruning methods, the paper explores how smaller language models can effectively prune datasets based on perplexity, enhancing downstream task performance and reducing pretraining steps. Notably, pruning improves performance in data-constrained scenarios and overtrained conditions.*** <br><br>
   31 May, Databricks and MIT published a [paper](https://arxiv.org/pdf/2405.20541) “Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models”. This work investigates whether small language models can determine high-quality subsets of large-scale text datasets that improve the performance of larger language models. While existing work has shown that pruning based on the perplexity of a larger model can yield high-quality data, the authors investigate whether smaller models can be used for perplexity-based pruning and how pruning is affected by the domain composition of the data being pruned. The paper demonstrates that for multiple dataset compositions, perplexity-based pruning of pretraining data can significantly improve downstream task performance: pruning based on perplexities computed with a 125 million parameter model improves the average performance on downstream tasks of a 3 billion parameter model by up to 2.04 and achieves up to a 1.45times reduction in pretraining steps to reach commensurate baseline performance. Furthermore, the authors demonstrate that such perplexity-based data pruning also yields downstream performance gains in the over-trained and data-constrained regimes.

3. ***Princeton Uni Paper on "SWE-agent for Automated Software Engineering": The paper explores the role of language model (LM) agents in automating software engineering tasks and introduces SWE-agent, a system designed to enhance LM agents' performance through specially-built interfaces. Investigating the impact of interface design on LM agents, SWE-agent facilitates autonomous code creation, editing, repository navigation, and program execution. Evaluation on SWE-bench and HumanEvalFix demonstrates SWE-agent's state-of-the-art performance, surpassing previous benchmarks with a pass@1 rate of 12.5% and 87.7%, respectively. Insights into the design's impact on agents' behavior and performance are also provided.*** <br><br>
   30 May, Princeton Uni published a [paper](https://arxiv.org/pdf/2405.15793) “SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering”. The paper indicates that Language model (LM) agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, the authors posit that LM agents represent a new category of end users with their own needs and abilities, and would benefit from specially-built interfaces to the software they use. The work investigates how interface design affects the performance of language model agents. As a result of this exploration, the research introduces SWE-agent: a system that facilitates LM agents to autonomously use computers to solve software engineering tasks. SWE-agent's custom agent-computer interface (ACI) significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs. The authors evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive LMs. Finally, the study provides insight on how the design of the ACI can impact agents' behavior and performance..


5. ***Google Paper on "Zipper Multi-Tower Decoder Architecture": Addressing challenges in multimodal generative tasks, Google proposes Zipper, a multi-tower decoder architecture. It showcases competitive performance, especially in scenarios with limited aligned data, and flexibility in maintaining unimodal generation performance.*** <br><br>
   31 May, Google published a [paper](https://arxiv.org/pdf/2405.18669) “Zipper A Multi-Tower Decoder Architecture for Fusing Modalities”. The paper indicates that integrating multiple generative foundation models, especially those trained on different modalities, into something greater than the sum of its parts poses significant challenges. Two key hurdles are the availability of aligned data (concepts that contain similar meaning but is expressed differently in different modalities), and effectively leveraging unimodal representations in cross-domain generative tasks, without compromising their original unimodal capabilities. The authors propose Zipper, a multi-tower decoder architecture that addresses these concerns by using cross-attention to flexibly compose multimodal generative models from independently pre-trained unimodal decoders. In the experiments fusing speech and text modalities, the study shows the proposed architecture performs very competitively in scenarios with limited aligned text-speech data. The authors also showcase the flexibility of the model to selectively maintain unimodal (e.g., text-to-text generation) generation performance by freezing the corresponding modal tower (e.g. text). In cross-modal tasks such as automatic speech recognition (ASR) where the output modality is text, the authors show that freezing the text backbone results in negligible performance degradation. In cross-modal tasks such as text-to-speech generation (TTS) where the output modality is speech, the work shows that using a pre-trained speech backbone results in superior performance to the baseline.



7. ***Stanford and Yale Uni Paper on "AI Legal Research Tools": The paper assesses AI-driven legal research tools' reliability, highlighting concerns of hallucinations. It provides empirical evidence contradicting tool providers' claims, emphasizing the need for responsible integration of AI into legal practice.*** <br><br>
   30 May, Stanford and Yale Uni published a [paper](https://arxiv.org/pdf/2405.20362) “Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools”. The researchers indicate that Legal practice has witnessed a sharp rise in products incorporating artificial intelligence (AI). Such tools are designed to assist with a wide range of core legal tasks, from search and summarization of caselaw to document drafting. But the large language models used in these tools are prone to "hallucinate," or make up false information, making their use risky in high-stakes domains. Recently, certain legal research providers have touted methods such as retrieval-augmented generation (RAG) as "eliminating" (Casetext, 2023) or "avoid[ing]" hallucinations (Thomson Reuters, 2023), or guaranteeing "hallucination-free" legal citations (LexisNexis, 2023). Because of the closed nature of these systems, systematically assessing these claims is challenging. This study designs and reports on the first preregistered empirical evaluation of AI-driven legal research tools. The authors demonstrate that the providers' claims are overstated. While hallucinations are reduced relative to general-purpose chatbots (GPT-4), the paper finds that the AI research tools made by LexisNexis (Lexis+ AI) and Thomson Reuters (Westlaw AI-Assisted Research and Ask Practical Law AI) each hallucinate between 17% and 33% of the time. The authors also document substantial differences between systems in responsiveness and accuracy. This article makes four key contributions. It is the first to assess and report the performance of RAG-based proprietary legal AI tools. Second, it introduces a comprehensive, preregistered dataset for identifying and understanding vulnerabilities in these systems. Third, it proposes a clear typology for differentiating between hallucinations and accurate legal responses. Last, it provides evidence to inform the responsibilities of legal professionals in supervising and verifying AI outputs, which remains a central open question for the responsible integration of AI into law. Note:  tool providers disagree with the findings in the paper, but comments. from [reddit strongly support this paper](https://www.reddit.com/r/LawSchool/comments/1d52y2d/ai_legal_research_tools_hallucinate_1733_of_the/).

9. ***2.	Oxford and Reuters report on " What Does the Public Think of Generative AI in News?”. ChatGPT is the most recognized and widely used tool, with 50% awareness. Daily usage is low (1-7%), and 20-30% of the population in six countries are unaware of popular AI tools. ChatGPT dominates usage compared to other tools like Google Gemini and Microsoft Copilot. Younger demographics are more frequent users. Usage is split between information retrieval and media creation, with only 5% using generative AI for accessing news specifically.*** <br><br>
    30 May, Oxford Uni and Reuters published a [report](https://reutersinstitute.politics.ox.ac.uk/sites/default/files/2024-05/Fletcher_and_Nielsen_Generative_AI_and_News_Audiences.pdf) “What Does the Public in Six Countries Think of Generative AI in News?”. ChatGPT is by far the most widely recognised generative AI product – around 50% of the online population in the six countries (Argentina, Denmark, France, Japan, the UK, and the USA)  surveyed have heard of it. It is also by far the most widely used generative AI tool in the six countries surveyed. That being said, frequent use of ChatGPT is rare, with just 1% using it on a daily basis in Japan, rising to 2% in France and the UK, and 7% in the USA. Many of those who say they have used generative AI have used it just once or twice, and it is yet to become part of people’s routine internet use. In more detail, 1) While there is widespread awareness of generative AI overall, only between 20% and 30% of the online public population in the six countries surveyed – have not heard of any of the most popular AI tools. 2) In terms of use, ChatGPT is by far the most widely used generative AI tool in the six countries, two or three times more widespread than the next most widely used products, Google Gemini and Microsoft Copilot. 3) Younger people are much more likely to use generative AI products on a regular basis. Averaging across all six countries, 56% of 18–24s say they have used ChatGPT at least once, compared to 16% of those aged 55 and over. 4) Roughly equal proportions across six countries say that they have used generative AI for getting information (24%) as creating various kinds of media, including text but also audio, code, images, and video (28%). 5) Just 5% across the six countries covered say that they have used generative AI to get the latest news.

9. ***CMU Paper on "Probing Evaluation of Large Multimodal Models in Medical VQA:": The paper addresses concerns regarding the reliability of Large Multimodal Models (LMMs) in medical Visual Question Answering (Med-VQA). Using a rigorously designed Probing Evaluation for Medical Diagnosis (ProbMed) dataset, evaluation results reveal significant limitations in handling fine-grained medical inquiries, even by top-performing models like GPT-4V and Gemini Pro, highlighting the current gap between LMM capabilities and practical application in medical settings.*** <br><br>
   30 May, Uni of California Santa Cruz and CMU published a [paper](https://arxiv.org/pdf/2405.20421) “Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA”. The authors argue that Large Multimodal Models (LMMs) have shown remarkable progress in the field of medical Visual Question Answering (Med-VQA), achieving high accuracy on existing benchmarks. However, their reliability under robust evaluation is questionable. This study reveals that state-of-the-art models, when subjected to simple probing evaluation, perform worse than random guessing on medical diagnosis questions. To address this critical evaluation problem, the paper introduces the Probing Evaluation for Medical Diagnosis (ProbMed) dataset to rigorously assess LMM performance in medical imaging through probing evaluation and procedural diagnosis. Particularly, probing evaluation features pairing original questions with negation questions with hallucinated attributes, while procedural diagnosis requires reasoning across various diagnostic dimensions for each image, including modality recognition, organ identification, clinical findings, abnormalities, and positional grounding. The evaluation reveals that top-performing models like GPT-4V and Gemini Pro perform worse than random guessing on specialized diagnostic questions, indicating significant limitations in handling fine-grained medical inquiries. Besides, models like LLaVA-Med struggle even with more general questions, and results from CheXagent demonstrate the transferability of expertise across different modalities of the same organ, showing that specialized domain knowledge is still crucial for improving performance. This study underscores the urgent need for more robust evaluation to ensure the reliability of LMMs in critical fields like medical diagnosis, and current LMMs are still far from applicable to those fields..
   
11. ***Meta Paper on "Contextual Position Encoding": Meta proposes CoPE to enhance position encoding in large language models, enabling more precise attention mechanisms. CoPE addresses limitations in existing methods, improving performance on various tasks including language modeling and coding.*** <br><br>
    29 May, Meta published a [paper](https://arxiv.org/pdf/2405.18719) “Contextual Position Encoding Learning to Count What's Important”. The researchers indicate that the attention mechanism is a critical component of Large Language Models (LLMs) that allows tokens in a sequence to interact with each other, but is order-invariant. Incorporating position encoding (PE) makes it possible to address by position, such as attending to the i-th token. However, current PE methods use token counts to derive position, and thus cannot generalize to higher levels of abstraction, such as attending to the i-th sentence. The study proposes a new position encoding method, Contextual Position Encoding (CoPE), that allows positions to be conditioned on context by incrementing position only on certain tokens determined by the model. This allows more general position addressing such as attending to the i-th particular word, noun, or sentence. The authors show that CoPE can solve the selective copy, counting and Flip-Flop tasks where popular position embeddings fail, and improve perplexity on language modeling and coding tasks.

13. ***MistralAI Release of "Codestral 22B": Codestral, an advanced code generation AI model, is introduced, offering enhanced coding capabilities across multiple programming languages. Its release marks a milestone in AI-driven code generation, garnering positive feedback from the developer community.*** <br><br>
    29 May, MistralAI released [Codestral 22B](https://huggingface.co/bullerwins/Codestral-22B-v0.1-hf). Codestral, a groundbreaking code generation AI model, is introduced as a tool for developers to enhance their coding experience. Trained on over 80 programming languages, including popular ones like Python and Java, Codestral boasts a wide-ranging capability. It assists developers by completing coding functions, writing tests, and filling in partial code, thereby saving time and reducing errors. Performance-wise, Codestral sets a new benchmark with its 22B model, outperforming previous models in terms of code generation efficiency. Detailed benchmarks across various languages such as Python and SQL demonstrate its superior performance. Codestral is available for download under the Mistral AI Non-Production License and can be accessed via dedicated endpoints, catering to different user needs. Additionally, Codestral is integrated into popular coding environments like VSCode and JetBrains, enabling developers to seamlessly interact with it for code generation and conversation. This integration expands Codestral's accessibility and usability across different developer tools, marking a significant advancement in AI-driven code generation. Strong positive feedbacks from developer community has been received since its release.

15. ***Inter Paper on "Efficient NAS for Large Language Models": The paper presents LLaMA-NAS, a method for efficient neural architecture search to reduce the computational complexity of large language models. It achieves smaller, higher-performing architectures, enhancing their usability on diverse hardware platforms.*** <br><br>
    28 May, Inter published a [paper](https://arxiv.org/pdf/2405.18377) “LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models”. The study finds that the abilities of modern large language models (LLMs) in solving natural language processing, complex reasoning, sentiment analysis and other tasks have been extraordinary which has prompted their extensive adoption. Unfortunately, these abilities come with very high memory and computational costs which precludes the use of LLMs on most hardware platforms. To mitigate this, the researchers propose an effective method of finding Pareto-optimal network architectures based on LLaMA2-7B using one-shot NAS (Neural Architecture Search). In particular, the authors fine-tune LLaMA2-7B only once and then apply genetic algorithm-based search to find smaller, less computationally complex network architectures. The work shows that, for certain standard benchmark tasks, the pre-trained LLaMA2-7B network is unnecessarily large and complex. More specifically, the paper demonstrates a 1.5x reduction in model size and 1.3x speedup in throughput for certain tasks with negligible drop in accuracy. In addition to finding smaller, higher-performing network architectures, the proposed method does so more effectively and efficiently than certain pruning or sparsification techniques. Finally, the authors demonstrate how quantization is complementary to the method and that the size and complexity of the networks can be further decreased using quantization. The authors believe that this work provides a way to automatically create LLMs which can be used on less expensive and more readily available hardware platforms.

17. ***Uni of Washington, Chicago, and Harvard Paper on "Superposed Decoding": Introducing Superposed Decoding, the paper addresses the computational cost of generating multiple drafts by leveraging autoregressive inference passes. It demonstrates improved speed and coherence compared to traditional decoding methods.*** <br><br>
    28 May, Uni of Washington, Chicago, and Harvard Uni published a [paper](https://arxiv.org/pdf/2405.18400) “Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass”. Many applications today provide users with multiple auto-complete drafts as they type, including GitHub's code completion, Gmail's smart compose, and Apple's messaging auto-suggestions. Under the hood, language models support this by running an autoregressive inference pass to provide a draft. Consequently, providing k drafts to the user requires running an expensive language model k times. To alleviate the computation cost of running k inference passes, the authors propose Superposed Decoding, a new decoding algorithm that generates k drafts at the computation cost of one autoregressive inference pass. This is achieved by feeding a superposition of the k most recent token embeddings from the drafts as input to the next decoding step of the language model. At every inference step the authors combine the k drafts with the top-k tokens to get k*k new drafts and cache the k most likely options, using an n-gram interpolation with minimal compute overhead to filter out incoherent generations. The experiments show that k drafts from Superposed Decoding are at least as coherent and factual as Nucleus Sampling and Greedy Decoding respectively, while being at least 2.44× faster for k≥3. In a compute-normalized setting, user evaluations demonstrably favor text generated by Superposed Decoding over Nucleus Sampling. Code and more examples open-sourced at [this https URL](https://github.com/RAIVNLab/SuperposedDecoding).

19. ***Georgia State Uni and Princeton Uni Paper on "VB-LoRA Hybrid Model": The paper introduces VB-LoRA, a parameter-efficient fine-tuning method for large language models, reducing storage and transmission costs. It achieves competitive performance while utilizing a fraction of the parameters of traditional methods.*** <br><br>
    27 May, Georgia State Uni and Princeton Uni published a [paper](https://arxiv.org/pdf/2405.15179) “VB-LoRA Extreme Parameter Efficient Fine-Tuning with Vector Banks”. As the adoption of large language models increases and the need for per-user or per-task model customization grows, the parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) and its variants, incur substantial storage and transmission costs. To further reduce stored parameters, the study introduces a "divide-and-share" paradigm that breaks the barriers of low-rank decomposition across matrix dimensions, modules and layers by sharing parameters globally via a vector bank. As an instantiation of the paradigm to LoRA, the proposed VB-LoRA composites all the low-rank matrices of LoRA from a shared vector bank with a differentiable top-k admixture module. VB-LoRA achieves extreme parameter efficiency while maintaining comparable or better performance compared to state-of-the-art PEFT methods. Extensive experiments demonstrate the effectiveness of VB-LoRA on natural language understanding, natural language generation, and instruction tuning tasks. When fine-tuning the Llama2-13B model, VB-LoRA only uses 0.4% of LoRA's stored parameters, yet achieves superior results. The source code is available at [this https URL](https://github.com/leo-yangli/VB-LoRA).

21. ***Uni of Maryland, CMU, et al. Paper on "Transformers and Arithmetic": Addressing transformers' limitations in arithmetic tasks, the paper proposes position embeddings to enhance their performance. It demonstrates significant improvements in solving arithmetic problems, unlocking potential in multi-step reasoning tasks.*** <br><br>
    27 May, Uni of Maryland, CMU, et al. published a [paper](https://arxiv.org/pdf/2405.17399) “Transformers Can Do Arithmetic with the Right Embeddings”. The authors find that the poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. The study mends this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, the research shows that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further. With positions resolved, the work can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? The authors find that training on only 20 digit numbers with a single GPU for one day, they can reach state-of-the-art performance, achieving up to 99% accuracy on 100 digit addition problems. Finally, the work shows that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.

23. ***Microsoft and Uni of Wisconsin-Madison Paper on "Matryoshka Multimodal Models": The paper presents M3, a novel approach for multimodal models, improving efficiency and flexibility in visual-linguistic reasoning. M3 offers fine-grained control over visual granularity and achieves performance comparable to traditional models.*** <br><br>
    27 May, Microsoft and Uni of Wisconsin-Madison published a [paper](https://arxiv.org/pdf/2405.17430) “Matryoshka Multimodal Models”. The paper indicates that Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in visual-linguistic reasoning. These models first embed images into a fixed large number of visual tokens and then feed them into a Large Language Model (LLM). However, this design causes an excessive number of tokens for dense visual scenarios such as high-resolution images and videos, leading to great inefficiency. While token pruning/merging methods do exist, they produce a single length output for each image and do not afford flexibility in trading off information density v.s. efficiency. Inspired by the concept of Matryoshka Dolls, the work proposes M3: Matryoshka Multimodal Models, which learns to represent visual content as nested sets of visual tokens that capture information across multiple coarse-to-fine granularities. The proposed approach offers several unique benefits for LMMs: (1) One can explicitly control the visual granularity per test instance during inference, e.g. , adjusting the number of tokens used to represent an image based on the anticipated complexity or simplicity of the content; (2) M3 provides a framework for analyzing the granularity needed for existing datasets, where it finds that COCO-style benchmarks only need around ~9 visual tokens to obtain accuracy similar to that of using all 576 tokens; (3) the approach provides a foundation to explore the best trade-off between performance and visual token length at sample level, where the investigation of the work reveals that a large gap exists between the oracle upper bound and current fixed-scale representations.

25. ***Zyphra Paper on "Zamba Hybrid Model": Zamba, a hybrid SSM-transformer model, is introduced for improved performance at a comparable scale. With a unique architecture and pretraining strategy, Zamba achieves competitive results while being faster and more memory-efficient than traditional transformer models.*** <br><br>
    26 May, Zyphra published a [paper](https://arxiv.org/pdf/2405.16712v1) “Zamba: A Compact 7B SSM Hybrid Model”. The technical report presents Zamba, a novel 7B SSM-transformer hybrid model which achieves competitive performance against leading open-weight models at a comparable scale. Zamba is trained on 1T tokens from openly available datasets and is the best non-transformer model at this scale. Zamba pioneers a unique architecture combining a Mamba backbone with a single shared attention module, thus obtaining the benefits of attention at minimal parameter cost. Due to its architecture, Zamba is significantly faster at inference than comparable transformer models and requires substantially less memory for generation of long sequences. Zamba is pretrained in two phases: the first phase is based on existing web datasets, while the second one consists of annealing the model over high-quality instruct and synthetic datasets, and is characterized by a rapid learning rate decay. The authors open-source the weights and all checkpoints for Zamba, through both phase 1 and annealing phases.

27. ***Jason Wei's Blog on "Language Model Evaluations": Jason emphasizes the importance of evaluation benchmarks in driving progress in AI research, outlining key characteristics of successful evaluations. He advocates for greater investment in evals to incentivize breakthroughs and ensure meaningful advancements in the field.*** <br><br>
    24 May, Jason Wei, author of Chain-of-Thought, published a [blog](https://www.jasonwei.net/blog/evals) “Successful language model evals”. Jason emphasizes the crucial role of evaluation benchmarks (evals) in driving progress in AI research. He argues that evals deserve more attention as they serve as incentives for researchers and breakthroughs often correlate with significant improvements on evals. Successful evals, such as GLUE/SuperGLUE and MMLU, are widely adopted and trusted within the community. Key characteristics of successful evals include clear significance, ease of understanding, reliability, simplicity, and relevance to meaningful tasks, and avoiding test set contamination. However, creating effective evals is challenging, requiring careful consideration of factors like 1) sample size which should no less than 1000, 2) eval quality should be high, 3) simplicity overweight complexity, 4) easy to run, 5) work on a meaningful task, 6) the grading in the eval should be extremely correct and 7) model performance must not become saturated too quickly. The text concludes by advocating for greater investment in evals, highlighting their importance as the objective function for AI researchers and their potential to drive impactful advancements in the field.

29. ***Uni of Chicago Paper on "Robust RAG against Retrieval Corruption": The paper introduces RobustRAG as a defense framework against retrieval corruption attacks. It employs isolate-then-aggregate strategies to achieve certifiable robustness against malicious passages injected into retrieval results.*** <br><br>
    24 May, Princeton Uni and UC Berkeley published a [paper](https://arxiv.org/pdf/2405.15556) “Certifiably Robust RAG against Retrieval Corruption”. The authors state that Retrieval-augmented generation (RAG) has been shown vulnerable to retrieval corruption attacks: an attacker can inject malicious passages into retrieval results to induce inaccurate responses. This work proposes RobustRAG as the first defense framework against retrieval corruption attacks. The key insight of RobustRAG is an isolate-then-aggregate strategy: get LLM responses from each passage in isolation and then securely aggregate these isolated responses. To instantiate RobustRAG, the authors design keyword-based and decoding-based algorithms for securely aggregating unstructured text responses. Notably, RobustRAG can achieve certifiable robustness: the authors can formally prove and certify that, for certain queries, RobustRAG can always return accurate responses, even when the attacker has full knowledge of the defense and can arbitrarily inject a small number of malicious passages. The authors evaluate RobustRAG on open-domain QA and long-form text generation datasets and demonstrate its effectiveness and generalizability across various tasks and datasets.

31. ***CMU, Uni of Toronto, Georgia Tech, MBZUAI Paper on "Data Valuation with Influence Functions": The paper focuses on scalable data valuation methods for large language models, enhancing influence functions' scalability. It introduces LoGra and LogIX to efficiently evaluate data contributions, lowering computational barriers.*** <br><br>
    22 May, CMU, Uni of Toronto, Georgia Tech, MBZUAI etc. published a [paper](https://arxiv.org/pdf/2405.13954) “What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions”. The authors find that Large language models (LLMs) are trained on a vast amount of human-written data, but data providers often remain uncredited. In response to this issue, data valuation (or data attribution), which quantifies the contribution or value of each data to the model output, has been discussed as a potential solution. Nevertheless, applying existing data valuation methods to recent LLMs and their vast training datasets has been largely limited by prohibitive compute and memory costs. This work focuses on influence functions, a popular gradient-based data valuation method, and significantly improve its scalability with an efficient gradient projection strategy called LoGra that leverages the gradient structure in backpropagation. The authors then provide a theoretical motivation of gradient projection approaches to influence functions to promote trust in the data valuation process. Lastly, the researchers lower the barrier to implementing data valuation systems by introducing LogIX, a software package that can transform existing training code into data valuation code with minimal effort. In the data valuation experiments, LoGra achieves competitive accuracy against more expensive baselines while showing up to 6,500x improvement in throughput and 5x reduction in GPU memory usage when applied to Llama3-8B-Instruct and the 1B-token dataset. Code is available here.

33. ***Uni of Chicago Paper on "Financial Statement Analysis with LLMs": Investigating LLMs' capability in financial analysis, the paper demonstrates their superior performance in predicting earnings changes compared to human analysts. LLMs offer valuable insights and trading strategies, potentially reshaping decision-making processes.*** <br><br>
    21 May, Uni of Chicago published a [paper](https://papers.ssrn.com/sol3/Delivery.cfm/4835311.pdf?abstractid=4835311&mirid=1) “Financial Statement Analysis with Large Language Models”. The paper investigates whether an LLM can successfully perform financial statement analysis in a way similar to a professional human analyst. The study provides standardized and anonymous financial statements to GPT4 and instructs the model to analyze them to determine the direction of future earnings. Even without any narrative or industry-specific information, the LLM outperforms financial analysts in its ability to predict earnings changes. The LLM exhibits a relative advantage over human analysts in situations when the analysts tend to struggle. Furthermore, the authors find that the prediction accuracy of the LLM is on par with the performance of a narrowly trained state-of-the-art ML model. LLM prediction does not stem from its training memory. Instead, the paper finds that the LLM generates useful narrative insights about a company's future performance. Lastly, the trading strategies based on GPT's predictions yield a higher Sharpe ratio and alphas than strategies based on other models. Taken together, the results suggest that LLMs may take a central role in decision-making.

35. ***MIT Paper on "Platonic Representation Hypothesis": The paper discusses the convergence of representations in AI models towards a shared statistical model of reality, akin to Plato's ideal reality. It explores implications, selective pressures, and limitations of this trend in driving progress in AI research.*** <br><br>
    13 May, MIT published a [paper](https://arxiv.org/pdf/2405.07987) “The Platonic Representation Hypothesis”. The authors argue that representations in AI models, particularly deep networks, are converging. First, the authors survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, the researchers demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. The study hypothesizes that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. The researchers term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, the paper discusses the implications of these trends, their limitations, and counterexamples to the analysis. There are also some hot discussions of the paper on [reddit](https://www.reddit.com/r/MachineLearning/comments/1csgr7r/r_the_platonic_representation_hypothesis/?rdt=46406&onetap_auto=true&one_tap=true). 

37. ***Cohere Paper on "Detecting Under-trained Tokens in LLMs": Addressing issues with tokenizer creation and model training disconnects, the paper introduces methods for automatically detecting untrained and under-trained tokens in large language models. It offers insights into improving efficiency and safety in language model development.*** <br><br>
    8 May, Cohere published a [paper](https://arxiv.org/pdf/2405.05417) “Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models”. The paper argues that the disconnect between tokenizer creation and model training in language models has been known to allow for certain inputs, such as the infamous _SolidGoldMagikarp token, to induce unwanted behaviour. Although such `glitch tokens' that are present in the tokenizer vocabulary, but are nearly or fully absent in training, have been observed across a variety of different models, a consistent way of identifying them has been missing. The authors present a comprehensive analysis of Large Language Model (LLM) tokenizers, specifically targeting this issue of detecting untrained and under-trained tokens. Through a combination of tokenizer analysis, model weight-based indicators, and prompting techniques, the authors develop effective methods for automatically detecting these problematic tokens. The findings demonstrate the prevalence of such tokens across various models and provide insights into improving the efficiency and safety of language models.

39. ***Google's Update on "Time-Series Forecasting with LLMs": Google presents a time-series foundation model for forecasting based on large language models. It achieves competitive performance across various forecasting tasks, demonstrating the potential of LLMs in time-series analysis.*** <br><br>
    17 Apr, Google updated its [paper](https://arxiv.org/pdf/2310.10688) “A decoder-only foundation model for Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models time-series forecasting”. Motivated by recent advances in large language models for Natural Language Processing (NLP), the researchers design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. This model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
<br><br><br>
