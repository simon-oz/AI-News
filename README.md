# Weekly AI-News
Welcome to the Weekly-AI-news!
I mainly focuses more on Generative AI, LLMs and the related, so may miss import AI news in other areas. <br><br>

**Feb 15, 2026**

1. Feb 12, Google published a [paper](https://arxiv.org/pdf/2602.10177) "Towards Autonomous Mathematics Research". Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. This work introduces Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. The study demonstrates the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, the study suggests quantifying standard levels of autonomy and novelty of AI-assisted results, as well as propose a novel concept of human-AI interaction cards for transparency. The work concludes with reflections on human-AI collaboration in mathematics and shares all prompts as well as model outputs at https://github.com/google-deepmind/superhuman/tree/main/aletheia.
2. Feb 12, Microsoft published a [paper](https://arxiv.org/pdf/2602.12275) "On-Policy Context Distillation for Language Models". Context distillation enables language models to internalize in-context knowledge into their parameters. This work proposes On-Policy Context Distillation (OPCD), a framework that bridges on-policy distillation with context distillation by training a student model on its own generated trajectories while minimizing reverse Kullback-Leibler divergence against a context-conditioned teacher. The study demonstrates the effectiveness of OPCD on two important applications: experiential knowledge distillation, where models extract and consolidate transferable knowledge from their historical solution traces, and system prompt distillation, where models internalize beneficial behaviors encoded in optimized prompts. Across mathematical reasoning, text-based games, and domain-specific tasks, OPCD consistently outperforms baseline methods, achieving higher task accuracy while better preserving out-of-distribution capabilities. The study further shows that OPCD enables effective cross-size distillation, where smaller student models can internalize experiential knowledge from larger teachers.
3. Feb 12, Google published a [paper](https://arxiv.org/pdf/2602.11865) "Intelligent AI Delegation". AI agents are able to tackle increasingly complex tasks. To achieve more ambitious goals, AI agents need to be able to meaningfully decompose problems into manageable sub-components, and safely delegate their completion across to other AI agents and humans alike. Yet, existing task decomposition and delegation methods rely on simple heuristics, and are not able to dynamically adapt to environmental changes and robustly handle unexpected failures. The work proposes an adaptive framework for intelligent AI delegation - a sequence of decisions involving task allocation, that also incorporates transfer of authority, responsibility, accountability, clear specifications regarding roles and boundaries, clarity of intent, and mechanisms for establishing trust between the two (or more) parties. The proposed framework is applicable to both human and AI delegators and delegatees in complex delegation networks, aiming to inform the development of protocols in the emerging agentic web.
4. Feb 12, Remin Uni published a [paper](https://arxiv.org/pdf/2602.12056) "LawThinker: A Deep Research Legal Agent in Dynamic Environments". Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this issue, the study proposes LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an atomic operation after every knowledge exploration step. A DeepVerifier module examines each retrieval result along three dimensions of knowledge accuracy, fact-law relevance, and procedural compliance, with a memory module for cross-round knowledge reuse in long-horizon tasks. Experiments on the dynamic benchmark J1-EVAL show that LawThinker achieves a 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong improvements on process-oriented metrics. Evaluations on three static benchmarks further confirm its generalization capability. https://github.com/yxy-919/LawThinker-agent.
5. Feb 12, Westlake Uni published a [paper](https://openreview.net/pdf?id=5N3z9JQJKq) "AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations". High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. The study presents FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text–figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, the study proposes AutoFigure, an agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, the study conducts extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that Autofigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. https://github.com/ResearAI/AutoFigure
6. Feb 11, Google published a [paper](https://www.arxiv.org/pdf/2602.10416) "AI-rithmetic". Modern AI systems have been successfully deployed to win medals at international math competitions, assist with research workflows, and prove novel technical lemmas. However, despite their progress at advanced levels of mathematics, they remain stubbornly bad at basic arithmetic, consistently failing on the simple task of adding two numbers. The study presents a systematic investigation of this phenomenon, demonstrates empirically that all frontier models suffer significantly degraded accuracy for integer addition as the number of digits increases. Furthermore, the study shows that most errors made by these models are highly interpretable and can be attributed to either operand misalignment or a failure to correctly carry; these two error classes explain 87.9%, 62.9%, and 92.4% of Claude Opus 4.1, GPT-5, and Gemini 2.5 Pro errors, respectively. Finally, the study shows that misalignment errors are frequently related to tokenization, and that carrying errors appear largely as independent random failures.
7. Feb 11, Google published a [paper](https://www.arxiv.org/pdf/2602.10390) "Affordances Enable Partial World Modeling with LLMs". Full models of the world require complex knowledge of immense detail. While pre-trained large models have been hypothesized to contain similar knowledge due to extensive pre-training on vast amounts of internet scale data, using them directly in a search procedure is inefficient and inaccurate. Conversely, partial models focus on making high quality predictions for a subset of state and actions: those linked through affordances that achieve user intents. Can people posit large models as partial world models? The study provides a formal answer to this question, proving that agents achieving task-agnostic, language-conditioned intents necessarily possess predictive partial-world models informed by affordances. In the multi-task setting, the study introduces distribution-robust affordances and shows that partial models can be extracted to significantly improve search efficiency. Empirical evaluations in tabletop robotics tasks demonstrate that the affordance-aware partial models reduce the search branching factor and achieve higher rewards compared to full world models.
8. Feb 11, Google published a [paper](https://arxiv.org/pdf/2602.11217) "The Magic Correlations: Understanding Knowledge Transfer from Pretraining to Supervised Fine-Tuning". Understanding how language model capabilities transfer from pretraining to supervised fine-tuning (SFT) is fundamental to efficient model development and data curation. The work investigates four core questions: RQ1. To what extent do accuracy and confidence rankings established during pretraining persist after SFT? RQ2. Which benchmarks serve as robust cross-stage predictors and which are unreliable? RQ3. How do transfer dynamics shift with model scale? RQ4. How well does model confidence align with accuracy, as a measure of calibration quality? Does this alignment pattern transfer across training stages? The work addresses these questions through a suite of correlation protocols applied to accuracy and confidence metrics across diverse data mixtures and model scales. Experiments reveal that transfer reliability varies dramatically across capability categories, benchmarks, and scales -- with accuracy and confidence exhibiting distinct, sometimes opposing, scaling dynamics. These findings shed light on the complex interplay between pretraining decisions and downstream outcomes, providing actionable guidance for benchmark selection, data curation, and efficient model development.
9. Feb 11, Uni of Tech Nuremberg, MistralAI and Nvidia published a [paper](https://arxiv.org/pdf/2602.11149) "Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning". Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, the study shows that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME'24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting. The study finds that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. The study poses the repetition advantage, where full memorization coincides with improved generalization, as a new open problem for the community in understanding the training dynamics of large language models. https://github.com/dkopi/data-repetition
10. Feb 10, Uni of British Columbia et al published a [paper](https://arxiv.org/pdf/2602.07755) "Learning to Continually Learn via Meta-learning Agentic Memory Designs". The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. The study introduces ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. The approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners. https://github.com/zksha/alma.git
11. Feb 10, Uni of Potsdam et al published a [paper](https://arxiv.org/pdf/2602.10097) "Step-resolved data attribution for looped transformers". The work studies how individual training examples shape the internal computation of looped transformers, where a shared block is applied for τ recurrent iterations to enable latent reasoning. Existing training-data influence estimators such as TracIn yield a single scalar score that aggregates over all loop iterations, obscuring when during the recurrent computation a training example matters. The work introduces Step-Decomposed Influence (SDI), which decomposes TracIn into a length-τ influence trajectory by unrolling the recurrent computation graph and attributing influence to specific loop iterations. To make SDI practical at transformer scale, the work proposes a TensorSketch implementation that never materialises per-example gradients. Experiments on looped GPT-style models and algorithmic reasoning tasks show that SDI scales excellently, matches full-gradient baselines with low error and supports a broad range of data attribution and interpretability tasks with per-step insights into the latent reasoning process.
12. Feb 10, Anthropic [published](https://resources.anthropic.com/hubfs/2026%20Agentic%20Coding%20Trends%20Report.pdf?hsLang=en) "2026 Agentic Coding Trends Report". Based on the Anthropic 2026 Agentic Coding Trends Report, the landscape of software development is undergoing a fundamental shift from AI assistance to deep collaboration, where software engineers evolve from manual implementers into orchestrators of coordinated agent teams. The report predicts that by 2026, the software development lifecycle will collapse from weeks to hours as AI agents handle the tactical work of writing, debugging, and maintaining code, freeing engineers to focus on architecture and strategy. Key capability trends include the evolution of single agents into coordinated multi-agent systems that work in parallel, and the emergence of long-running agents capable of autonomously building entire systems over days rather than minutes. Perhaps most critically, human oversight will scale through intelligent collaboration where agents learn when to ask for help, allowing teams to maintain quality and velocity by reviewing only what matters most. These shifts are expected to reshape software development economics, enable non-technical users to leverage agentic coding, and require security-first architectures to address dual-use risks. Crucially, the report emphasizes this transformation is collaborative—while engineers use AI in roughly 60% of their work, they fully delegate only a small fraction of tasks, with human judgment remaining essential for high-stakes decisions, organizational context, and strategic direction. Organizations that master these patterns stand to ship features in hours instead of days while expanding their developers' capabilities across the full technical stack.
13. Feb 9, T-Tech and SPEU published a [paper](https://arxiv.org/pdf/2602.06717) "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare". Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. The study derives the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, the work proposes a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, the method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.
14. Feb 9, ServiceNow, Mila et al published a [paper](https://arxiv.org/pdf/2602.04942) "Privileged Information Distillation for Language Models". Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. The authors study this problem in the context of distilling frontier models for multi-turn agentic environments, which typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable, but the reasoning process is not. For this, the work introduces pi-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, the work also introduces On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. The work shows that both of these algorithms effectively distill frontier agents using action-only PI. Specifically, the work finds that pi-Distill and, in some cases, OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. The work complements the results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on pi-Distill and characterizing when OPSD is competitive.
15. Feb 9, UNC-Chapel Hill et al published [paper](https://arxiv.org/pdf/2602.08234) "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning". Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. The study proposes SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. The approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. https://github.com/aiming-lab/SkillRL.
16. Feb 9, Matt Shumer, CEO @HyperWriteAI, @OthersideAI published a [blog](https://shumer.dev/something-big-is-happening) "Something Big Is Happening". In his urgent February 2026 article, AI insider Matt Shumer argues that a paradigm shift in artificial intelligence, far surpassing public perception, is already underway and will soon disrupt virtually every sector of work. Drawing a parallel to the sudden global transformation caused by COVID-19 in early 2020, he warns that most people are in a "this seems overblown" phase, unaware that the foundational capability for mass disruption has arrived. Shumer grounds his argument in his direct experience with the latest models (like OpenAI's GPT-5.3 Codex and Anthropic's Opus 4.6), which now possess autonomous judgment and can complete complex, multi-hour software projects from a simple description, testing and refining the output without human intervention. He explains that AI labs deliberately focused on mastering coding first to create a self-improvement feedback loop, a strategy that has succeeded: AI is now instrumental in building its own next generation. Citing industry leaders, he notes AI's autonomous task capability is doubling every few months and predicts that this general substitute for cognitive work will eliminate a large percentage of white-collar jobs within one to five years. Shumer emphasizes that free AI tools lag far behind the paid versions driving this change, and he writes to bridge the "enormous" gap between public understanding and the accelerating reality he witnesses, urging preparation for a fundamentally different world of work.

17. Feb 8, Bosch and CMU published a [paper](https://arxiv.org/pdf/2602.08004) "Agent Skills: A Data-Driven Analysis of Claude Skills for Extending Large Language Model Functionality". Agent skills extend large language model (LLM) agents with reusable, program-like modules that define triggering conditions, procedural logic, and tool interactions. As these skills proliferate in public marketplaces, it is unclear what types are available, how users adopt them, and what risks they pose. To answer these questions, the work conducts a large-scale, data-driven analysis of 40,285 publicly listed skills from a major marketplace. The results show that skill publication tends to occur in short bursts that track shifts in community attention. The work also finds that skill content is highly concentrated in software engineering workflows, while information retrieval and content creation account for a substantial share of adoption. Beyond content trends, the work uncovers a pronounced supply-demand imbalance across categories, and shows that most skills remain within typical prompt budgets despite a heavy-tailed length distribution. Finally, the study observes strong ecosystem homogeneity, with widespread intent-level redundancy, and identifies non-trivial safety risks, including skills that enable state-changing or system-level actions. Overall, the findings provide a quantitative snapshot of agent skills as an emerging infrastructure layer for agents and inform future work on skill reuse, standardization, and safety-aware design.
18. Feb 7, SJTU et al published a [paper](https://arxiv.org/pdf/2602.05400) "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration". As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. The work proposes OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, the work employs Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.
Feb 6, MIT and Harvard Uni published a [paper](https://arxiv.org/pdf/2602.04770) "Generative Modeling via Drifting". Generative modeling can be formulated as learning a mapping f such that its pushforward distribution matches the data distribution. The pushforward behavior can be carried out iteratively at inference time, for example in diffusion and flow-based models. In this paper, we propose a new paradigm called Drifting Models, which evolve the pushforward distribution during training and naturally admit one-step inference. We introduce a drifting field that governs the sample movement and achieves equilibrium when the distributions match. This leads to a training objective that allows the neural network optimizer to evolve the distribution. In experiments, our one-step generator achieves state-of-the-art results on ImageNet at 256 x 256 resolution, with an FID of 1.54 in latent space and 1.61 in pixel space. We hope that our work opens up new opportunities for high-quality one-step generation.
19. Feb 4, UCSB published a [paper](https://arxiv.org/pdf/2602.04837) "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing". Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.
20. Feb 3, UK Government [published](https://internationalaisafetyreport.org/sites/default/files/2026-02/international-ai-safety-report-2026_0.pdf) "International AI Safety Report 2026". This is the second edition of the International AI Safety Report. The series was created following the 2023 AI Safety Summit at Bletchley Park to support an internationally shared scientific understanding of the capabilities and risks associated with advanced AI systems. A diverse group of over 100 Artificial Intelligence (AI) experts guided its development, including an international Expert Advisory Panel with nominees from over 30 countries and international organisations, including theOrganisation for Economic Co-operation and Development (OECD), the European Union (EU), and the United Nations (UN). This Report concerns ‘general-purpose AI’: AI models and systems capable of performing a wide variety of tasks across different contexts. These models and systems perform tasks like generating text, images, audio, or other forms of data, and are frequently adapted to a range of domain-specific applications. This Report focuses on ‘emerging risks’:risks that arise at the frontier of AI capabilities.The Bletchley Declaration, issued followingthe 2023 AI Safety Summit, emphasised that“particular safety risks arise at the ‘frontier’of AI”, including risks from misuse, issues ofcontrol, and cybersecurity risks. The Declarationalso recognised broader AI impacts, includingon human rights, fairness, accountability,and privacy. This Report aims to complementassessments that consider these broaderconcerns, including the UN’s IndependentInternational Scientific Panel on AI.  Key develops since 2025 report include: General-purpose AI capabilities have continued to improve, especially in mathematics, coding, and autonomous operation. Improvements in general-purpose AI capabilities increasingly come from techniques applied after a model’s initial training. AI adoption has been rapid, though highly uneven across regions. Advances in AI’s scientific capabilities have heightened concerns about misuse in biological weapons development. More evidence has emerged of AI systems being used in real-world cyberattacks. Reliable pre-deployment safety testing has become harder to conduct. Industry commitments to safety governance have expanded.
21. Feb 2, Princeton Uni published a [paper](https://arxiv.org/pdf/2602.02488) "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System". We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. this https URL
22. Feb 2, Google published a [paper](https://arxiv.org/pdf/2602.02660) "MARS: Modular Agent with Reflective Search for Automated AI Research". Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a "Design-Decompose-Implement" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative "Aha!" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths. https://github.com/jfc43/MARS


**Feb 8, 2026**

1. ***Inverse depth scaling dominates in LLMs —  <br>A study from MIT, Harvard, and Stanford reveals that in large language models, loss scales roughly inversely with depth due to most layers being functionally similar and acting like an ensemble average rather than enabling true compositional or dynamic learning; this inefficiency stems from residual network biases and mismatched target functions, suggesting architectural changes are needed for better depth utilization.*** <br> <br>
   Feb 5, MIT, Harvard uni and Stanford Uni published a [paper](https://arxiv.org/pdf/2602.05970) "Inverse Depth Scaling From Most Layers Being Similar". Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requiring more detailed studies. The study quantifies how depth affects loss via analysis of LLMs and toy residual networks. The study finds loss scales inversely proportional to depth in LLMs, probably due to functionally similar layers reducing error through ensemble averaging rather than compositional learning or discretizing smooth dynamics. This regime is inefficient yet robust and may arise from the architectural bias of residual networks and target functions incompatible with smooth dynamics. The findings suggest that improving LLM efficiency may require architectural innovations to encourage compositional use of depth. https://github.com/liuyz0/DepthScaling <br> <br>
3. ***Reinforced Attention Learning boosts multimodal reasoning —  <br>Researchers from UC Davis, Princeton, and Google introduce Reinforced Attention Learning (RAL), a policy-gradient method that optimizes internal attention distributions directly (rather than output tokens) to improve information allocation and grounding in multimodal LLMs; it outperforms baselines on image/video tasks, with On-Policy Attention Distillation enhancing cross-modal alignment more effectively than standard distillation.*** <br> <br>
   Feb 4, UC Davis, Princeton Uni  and Google published a [paper](https://arxiv.org/pdf/2602.04884) "Reinforced Attention Learning". Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance. The study proposes Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. The study further introduces On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. The results position attention policies as a principled and general alternative for multimodal post-training. <br> <br>
5. ***TinyLoRA enables reasoning with just 13 parameters —  <br>A collaboration from Meta, Cornell, and CMU shows that reasoning can be induced in an 8B Qwen2.5 model to 91% GSM8K accuracy using only 13 trained parameters (via TinyLoRA, scaling LoRA to rank-1 extremes); this recovers ~90% of gains with 1000x fewer parameters on harder benchmarks, but crucially requires reinforcement learning rather than supervised fine-tuning.*** <br> <br>
   Feb 4, Meta, Cornell Uni and CMU published a [paper](https://arxiv.org/pdf/2602.04118) "Learning to Reason in 13 Parameters". Recent research has shown that language models can learn to reason, often via reinforcement learning. Some work even trains low-rank parameterizations for reasoning, but conventional LoRA cannot scale below the model dimension. The work questions whether even rank=1 LoRA is necessary for learning to reason and proposes TinyLoRA, a method for scaling low-rank adapters to sizes as small as one parameter. Within the new parameterization, the authors are able to train the 8B parameter size of Qwen2.5 to 91% accuracy on GSM8K with only 13 trained parameters in bf16 (26 total bytes). The study finds this trend holds in general: the work is able to recover 90% of performance improvements while training 1000x fewer parameters across a suite of more difficult learning-to-reason benchmarks such as AIME, AMC, and MATH500. Notably, the authors are only able to achieve such strong performance with RL: models trained using SFT require 100 - 1000x larger updates to reach the same performance. <br> <br>
7. ***Logit-Linear-Selection uncovers hidden dataset effects —  <br>UC Berkeley and collaborators present a mechanism (via Logit-Linear-Selection) explaining how subtle, non-obvious signals in datasets can induce specific behaviors (preferences, languages, personas) in LLMs even when absent from individual examples; the method selects subsets to reliably elicit such "subliminal" effects across models and architectures.*** <br> <br>
   Feb 4, UC Berkeley et al published a [paper](https://arxiv.org/pdf/2602.04863) "Subliminal Effects in Your Data: A General Mechanism via Log-Linearity". Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model's properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, the study uncovers a general mechanism through which hidden subtexts can arise in generic datasets. The work introduces Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. The study applies LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality. https://github.com/ishaqadenali/logit-linear-selection <br> <br>
9. ***OpenScholar outperforms GPT-4o in scientific synthesis —  <br>A UC Berkeley-led team (with Nature publication) introduces OpenScholar, a retrieval-augmented 8B model that synthesizes answers from 45M open papers with high citation accuracy and correctness, beating GPT-4o by 6.1% on a new multi-domain benchmark (ScholarQABench); expert humans prefer its responses over GPT-4o's and even expert-written ones in many cases.*** <br> <br>
    Feb 4, Uni of Washington et al published a [paper](https://www.nature.com/articles/s41586-025-10072-4) on Nature "Synthesizing scientific literature with retrieval-augmented language models". Scientific progress depends on the ability of researchers to synthesize the growing body of literature. Can large language models (LLMs) assist scientists in this task? Here the study introduces OpenScholar, a specialized retrieval-augmented language model (LM) that answers scientific queries by identifying relevant passages from 45 million open-access papers and synthesizing citation-backed responses. To evaluate OpenScholar, the work develops ScholarQABench, the first large-scale multi-domain benchmark for literature search, comprising 2,967 expert-written queries and 208 long-form answers across computer science, physics, neuroscience and biomedicine. Despite being a smaller open model, OpenScholar-8B outperforms GPT-4o by 6.1% and PaperQA2 by 5.5% in correctness on a challenging multi-paper synthesis task from the new ScholarQABench. Although GPT-4o hallucinates citations 78–90% of the time, OpenScholar achieves citation accuracy on par with human experts. OpenScholar’s data store, retriever and self-feedback inference loop improve off-the-shelf LMs: for instance, OpenScholar-GPT-4o improves the correctness of GPT-4o by 12%. In human evaluations, experts preferred OpenScholar-8B and OpenScholar-GPT-4o responses over expert-written ones 51% and 70% of the time, respectively, compared with 32% for GPT-4o. https://github.com/AkariAsai/OpenScholar <br> <br>
11. ***Anthropic's legal plugin disrupts Big Law economics —  <br>Yahoo Finance reports that Anthropic's Claude legal plugin automates junior associate tasks like contract review and brief drafting, accelerating client demands for alternative fees, threatening billable-hour models, and causing sharp stock drops in legal/data firms (e.g., Thomson Reuters -16%); this intensifies legal tech competition and signals broader white-collar automation risks.*** <br> <br>
    Feb 4, Yahoo!Finance published an [article](https://finance.yahoo.com/news/anthropic-claude-legal-plugin-poses-050100456.html) "Anthropic Claude’s Legal Plugin Poses AI Threat to Big Law’s Billable Hours". The article details how Anthropic's new legal plugin for its Claude Cowork platform is directly threatening the traditional "billable hour" business model of large law firms. The AI tool automates foundational tasks typically handled by junior associates, such as contract review and drafting legal briefs, which has intensified corporate clients' existing push for alternative fee structures and casts doubt on future hiring from law schools. This disruption triggered immediate investor anxiety, causing sharp share price declines for established legal and data service providers like Thomson Reuters (down roughly 16%) and RELX (down 14%), with concerns spreading to consulting and financial service firms like Accenture and S&P Global. The move signals intensifying competition in the legal tech space, where well-funded AI startups like Harvey AI and Legora—which themselves rely on Anthropic's models—are already growing rapidly. Ultimately, Anthropic's entry is more than a product launch; it represents a pivotal technological shift that is reshaping the economics of the legal profession, unsettling financial markets, and potentially setting a precedent for the automation of other high-skill white-collar industries. <br> <br>
13. ***BAPO bounds prove linear CoT token needs —  <br>Microsoft and Netflix theoretically show (via extended BAPO model) that chain-of-thought reasoning requires Ω(n) tokens for certain hard tasks as input size n grows, with matching upper bounds; experiments confirm near-linear scaling in frontier models and performance drops under token limits, highlighting fundamental inference-time bottlenecks.*** <br> <br>
    Feb 4, Microsoft and Netflix published a [paper](https://arxiv.org/pdf/2602.02909) "Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs". Inference-time scaling via chain-of-thought (CoT) reasoning is a major driver of state-of-the-art LLM performance, but it comes with substantial latency and compute costs. The study addresses a fundamental theoretical question: how many reasoning tokens are required to solve a problem as input size grows? By extending the bounded attention prefix oracle (BAPO) model--an abstraction of LLMs that quantifies the information flow required to solve a task--we prove lower bounds on the CoT tokens required for three canonical BAPO-hard tasks: binary majority, triplet matching, and graph reachability. We show that each requires Omega(n) reasoning tokens when the input size is n. The study complements these results with matching or near-matching upper bounds via explicit constructions. Finally, the experiments with frontier reasoning models show approximately linear reasoning token scaling on these tasks and failures when constrained to smaller reasoning budgets, consistent with the theoretical lower bounds. Together, the results identify fundamental bottlenecks in inference-time compute through CoT and offer a principled tool for analyzing optimal reasoning length. <br> <br>
15. ***Gemini aids novel mathematical discoveries —  <br>Google, CMU, and others document case studies where Gemini models (especially Deep Think variants) collaborate with humans to solve open problems, refute conjectures, and produce proofs in TCS, economics, physics, and more; key techniques include iterative refinement, decomposition, adversarial reviewing, and neuro-symbolic loops, positioning AI as a creative research partner.*** <br> <br>
    Feb 3, Google, CMU et al published a [paper](https://arxiv.org/pdf/2602.03837) "Accelerating Scientific Research with Gemini: Case Studies and Common Techniques". Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. The work presents a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, the work extracts common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of the results stem from this interactive, conversational methodology, the work also highlights specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a "neuro-symbolic" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery. <br> <br>
17. ***Test-time Recursive Thinking enables self-improvement —  <br>Microsoft and UC San Diego propose Test-time Recursive Thinking (TRT), allowing LLMs to iteratively self-improve via rollout strategies, accumulated knowledge, and self-verification without external feedback or training; open models hit 100% on AIME subsets, while closed models gain 10-15% on tough coding problems.*** <br> <br>
    Feb 3, Microsoft and UC San Diego published a [paper](https://arxiv.org/pdf/2602.03094) "Test-time Recursive Thinking: Self-Improvement without External Feedback". Modern Large Language Models (LLMs) have shown rapid improvements in reasoning capabilities, driven largely by reinforcement learning (RL) with verifiable rewards. Here, the work asks whether these LLMs can self-improve without the need for additional training. The study identifies two core challenges for such systems: (i) efficiently generating diverse, high-quality candidate solutions, and (ii) reliably selecting correct answers in the absence of ground-truth supervision. To address these challenges, the study proposes Test-time Recursive Thinking (TRT), an iterative self-improvement framework that conditions generation on rollout-specific strategies, accumulated knowledge, and self-generated verification signals. Using TRT, open-source models reach 100% accuracy on AIME-25/24, and on LiveCodeBench's most difficult problems, closed-source models improve by 10.4-14.8 percentage points without external feedback. <br> <br>
19. ***ReasonCACHE teaches reasoning via fixed cache without updates —  <br>Meta, MIT, and TU Munich demonstrate that Prefix Tuning with ReasonCACHE distills demonstrations into a key-value cache, enabling complex reasoning via in-context learning without weight changes or long contexts; it outperforms standard ICL, matches or beats in-weight methods on hard benchmarks, is more efficient, and is theoretically more expressive than low-rank updates.*** <br> <br>
    Feb 3, Meta, MIT and TU Munich published a [paper](https://arxiv.org/pdf/2602.02366) "ReasonCACHE: Teaching LLMs To Reason Without Weight Updates". Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. This work shows that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. The work introduces ReasonCache, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. The work also theoretically proves that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, the findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters.  <br> <br>
21. ***ORBIT unlocks in-context online learning in LLMs —  <br>Boston University and LinkedIn introduce ORBIT, a meta-RL framework that trains LLMs to adapt via in-context interactions in online decision-making settings; a 14B Qwen model matches GPT-5.2-level performance on unseen environments, far outperforming standard RL fine-tuning, with gains scaling by model size.*** <br> <br>
    Feb 3, Boston Uni and LinkedIn published a [paper](https://www.arxiv.org/pdf/2602.04089) "Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL". Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. This work show sthat this limitation can be addressed through training. The study introduces ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. https://github.com/XiaofengLin7/ORBIT <br> <br>
23. ***Divide-and-conquer RL enhances test-time scaling —  <br>UCLA and Microsoft use end-to-end RL to train LLMs for divide-and-conquer reasoning (decomposing into subproblems), overcoming CoT limitations; the approach yields higher ceilings and better scalability, surpassing CoT by ~8-9% on competition benchmarks under comparable training.*** <br> <br>
    Feb 2, UCLA and Microsoft published a [paper](https://arxiv.org/pdf/2602.02477) "Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability". Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, the analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, the study proposes an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, the DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks. <br> <br>
25. ***Residual Context Diffusion recycles wasted tokens —  <br>UC Berkeley and Apple propose Residual Context Diffusion for diffusion LLMs, reusing discarded token representations as contextual residuals to boost efficiency; with minimal extra data/compute, it improves accuracy by 5-10 points across benchmarks and dramatically reduces denoising steps on hard tasks like AIME.*** <br> <br>
    Jan 30, UC Berkeley and Apple published a [paper](https://arxiv.org/pdf/2601.22954) "Residual Context Diffusion Language Models". Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to purely autoregressive language models because they can decode multiple tokens in parallel. However, state-of-the-art block-wise dLLMs rely on a "remasking" mechanism that decodes only the most confident tokens and discards the rest, effectively wasting computation. The work demonstrates that recycling computation from the discarded tokens is beneficial, as these tokens retain contextual information useful for subsequent decoding iterations. In light of this, teh work proposes Residual Context Diffusion (RCD), a module that converts these discarded token representations into contextual residuals and injects them back for the next denoising step. RCD uses a decoupled two-stage training pipeline to bypass the memory bottlenecks associated with backpropagation. The work validates the method on both long CoT reasoning (SDAR) and short CoT instruction following (LLaDA) models. The study demonstrates that a standard dLLM can be efficiently converted to the RCD paradigm with merely ~1 billion tokens. RCD consistently improves frontier dLLMs by 5-10 points in accuracy with minimal extra computation overhead across a wide range of benchmarks. Notably, on the most challenging AIME tasks, RCD nearly doubles baseline accuracy and attains up to 4-5x fewer denoising steps at equivalent accuracy levels. <br> <br>
27. ***Context structure dictates representational straightening —  <br>Google and Princeton find that in-context learning induces representational straightening (straighter trajectories) only in continual prediction tasks, correlating with better performance; in structured/few-shot settings, it appears selectively (e.g., during templates) but vanishes elsewhere, suggesting LLMs dynamically switch strategies like a "Swiss Army knife."*** <br> <br>
    Jan 29, Google and Princeton Uni published a [paper](https://www.arxiv.org/pdf/2601.22364) "Context Structure Reshapes the Representational Geometry of Language Models". Large Language Models (LLMs) have been shown to organize the representations of input sequences into straighter neural trajectories in their deep layers, which has been hypothesized to facilitate next-token prediction via linear extrapolation. Language models can also adapt to diverse tasks and learn new structure in context, and recent work has shown that this in-context learning (ICL) can be reflected in representational changes. Here the work brings these two lines of research together to explore whether representation straightening occurs within a context during ICL. The work measures representational straightening in Gemma 2 models across a diverse set of in-context tasks, and uncover a dichotomy in how LLMs' representations change in context. In continual prediction settings (e.g., natural language, grid world traversal tasks) the work observes that increasing context increases the straightness of neural sequence trajectories, which is correlated with improvement in model prediction. Conversely, in structured prediction settings (e.g., few-shot tasks), straightening is inconsistent -- it is only present in phases of the task with explicit structure (e.g., repeating a template), but vanishes elsewhere. These results suggest that ICL is not a monolithic process. Instead, the study proposes that LLMs function like a Swiss Army knife: depending on task structure, the LLM dynamically selects between strategies, only some of which yield representational straightening. <br> <br>
29. ***Emu3 unifies multimodal learning via next-token prediction —  <br>A Nature paper introduces Emu3, trained solely with next-token prediction across text, images, and video, matching or exceeding specialized models in perception/generation without diffusion or composition; it excels at coherent video, interleaved generation, and vision-language-action for robotics, paving the way for unified multimodal intelligence.*** <br> <br>
    Jan 28, Nature published a [paper](https://www.nature.com/articles/s41586-025-10041-x) "Multimodal learning with next-token prediction for large multimodal models". Developing a unified algorithm that can learn from and generate across modalities such as text, images and video has been a fundamental challenge in artificial intelligence. Although next-token prediction has driven major advances in large language models, its extension to multimodal domains has remained limited, and diffusion models for image and video synthesis and compositional frameworks that integrate vision encoders with language models still dominate. Here we introduce Emu3, a family of multimodal models trained solely with next-token prediction. Emu3 equals the performance of well-established task-specific models across both perception and generation, matching flagship systems while removing the need for diffusion or compositional architectures. It further demonstrates coherent, high-fidelity video generation, interleaved vision–language generation and vision–language–action modelling for robotic manipulation. By reducing multimodal learning to unified token prediction, Emu3 establishes a robust foundation for large-scale multimodal modelling and offers a promising route towards unified multimodal intelligence. <br> <br>
31. ***Vibe coding threatens open-source sustainability —  <br>Central European University and co-authors model how "vibe coding" (AI agents assembling OSS without user engagement) boosts productivity but reduces maintainer incentives, leading to lower OSS entry, quality, and availability; sustaining OSS may require new payment models under widespread AI adoption.*** <br> <br>
    Jan 23, Central European Uni et al published a [paper](https://arxiv.org/pdf/2601.15494) "Vibe Coding Kills Open Source". Generative AI is changing how software is produced and used. In vibe coding, an AI agent builds software by selecting and assembling open-source software (OSS), often without users directly reading documentation, reporting bugs, or otherwise engaging with maintainers. The authors study the equilibrium effects of vibe coding on the OSS ecosystem. The work develops a model with endogenous entry and heterogeneous project quality in which OSS is a scalable input into producing more software. Users choose whether to use OSS directly or through vibe coding. Vibe coding raises productivity by lowering the cost of using and building on existing code, but it also weakens the user engagement through which many maintainers earn returns. When OSS is monetized only through direct user engagement, greater adoption of vibe coding lowers entry and sharing, reduces the availability and quality of OSS, and reduces welfare despite higher productivity. Sustaining OSS at its current scale under widespread vibe coding requires major changes in how maintainers are paid. <br> <br>
33. ***AI coding adoption widens experience gaps —  <br>A Science study analyzes 30M+ GitHub commits, finding AI generates ~29% of US Python functions (with productivity up 3.6%); benefits accrue mainly to senior developers (higher output, domain expansion), while juniors see little gain, potentially exacerbating skill and income disparities in software careers.*** <br> <br>
    Jan 22, Science published a [paper](https://www.science.org/doi/epdf/10.1126/science.adz9311) "Who is using AI to code? Global diffusion and impact ofgenerative AI". Generative coding tools promise big productivity gains, but uneven uptake could widen skill and income gaps. The study trains a neural classifier to spot AI-generated Python functions in over 30 million GitHub commits by 160,097 software developers, tracking how fast, and where, these tools take hold. Currently AI writes an estimated 29% of Python functions in the US, a shrinking lead over other countries. The study estimates quarterly output, measured in online code contributions, consequently increased by 3.6%. AI seems to benefit experienced, senior-level developers: they increased productivity and more readily expanded into new domains of software development. In contrast, early-career developers showed no significant benefits from AI adoption. This may widen skill gaps and reshape future career ladders in software development. <br> <br>
35. ***Execution-grounded search automates AI research —  <br>Stanford demonstrates an automated executor that implements LLM-generated ideas via parallel GPU runs for pre- and post-training; execution-guided evolutionary search efficiently outperforms baselines (e.g., 69% vs 48% on post-training), though frontier LLMs saturate quickly and RL from rewards suffers mode collapse, offering insights for fully automated research pipelines.*** <br> <br>
    Jan 20, Stanford Uni published a [paper](https://arxiv.org/pdf/2601.14525) "Towards Execution-Grounded Automated AI Research". Automated AI research holds great potential to accelerate scientific discovery. However, current LLMs often generate plausible-looking but ineffective ideas. Execution grounding may help, but it is unclear whether automated execution is feasible and whether LLMs can learn from the execution feedback. To investigate these, the study first builds an automated executor to implement ideas and launch large-scale parallel GPU experiments to verify their effectiveness. The study then converts two realistic research problems - LLM pre-training and post-training - into execution environments and demonstrates that the automated executor can implement a large fraction of the ideas sampled from frontier LLMs. The authors analyze two methods to learn from the execution feedback: evolutionary search and reinforcement learning. Execution-guided evolutionary search is sample-efficient: it finds a method that significantly outperforms the GRPO baseline (69.4% vs 48.0%) on post-training, and finds a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on pre-training, all within just ten search epochs. Frontier LLMs often generate meaningful algorithmic ideas during search, but they tend to saturate early and only occasionally exhibit scaling trends. Reinforcement learning from execution reward, on the other hand, suffers from mode collapse. It successfully improves the average reward of the ideator model but not the upper-bound, due to models converging on simple ideas. The authors thoroughly analyze the executed ideas and training dynamics to facilitate future efforts towards execution-grounded automated AI research. https://github.com/NoviScl/Automated-AI-Researcher
 <br> <br> <br>

 
**Feb 2, 2026**

1. ***Nonparametric LLM Evaluation Framework Introduced <br>
A new statistical framework called DMLEval uses debiased machine learning for nonparametric evaluation and ranking of LLMs from preference data, addressing limitations in existing methods and offering efficient estimation, flexibility, and optimal data collection policies.*** <br> <br>
   Jan 30, LMU Munich, CMU et al published a [paper](https://www.arxiv.org/pdf/2601.21816) "Nonparametric LLM Evaluation from Preference Data". Evaluating the performance of large language models (LLMs) from human preference data is crucial for obtaining LLM leaderboards. However, many existing approaches either rely on restrictive parametric assumptions or lack valid uncertainty quantification when flexible machine learning methods are used. This study proposes a nonparametric statistical framework, DMLEval, for comparing and ranking LLMs from preference data using debiased machine learning (DML). For this, the work introduces generalized average ranking scores (GARS), which generalize commonly used ranking models, including the Bradley-Terry model or PageRank/ Rank centrality, with complex human responses such as ties. DMLEval comes with the following advantages: (i) It produces statistically efficient estimates of GARS ranking scores. (ii) It naturally allows the incorporation of black-box machine learning methods for estimation. (iii) It can be combined with pre-trained LLM evaluators (e.g., using LLM-as-a-judge). (iv) It suggests optimal policies for collecting preference data under budget constraints. The work demonstrates these advantages both theoretically and empirically using both synthetic and real-world preference datasets. In summary, the framework provides practitioners with powerful, state-of-the-art methods for comparing or ranking LLMs. <br> <br>

3. ***Token-Level Filtering Shapes Capabilities During Pretraining <br>
Filtering specific tokens, rather than whole documents, during pretraining is shown to be a highly effective and scalable method for reducing undesired model capabilities (e.g., medical knowledge) while preserving benign performance, with effects magnified at larger model scales.*** <br> <br>
   Jan 29, Anthropic and Stanford Uni published a [paper](https://arxiv.org/pdf/2601.21571) "Shaping capabilities with token-level data filtering". Current approaches to reducing undesired capabilities in language models are largely post hoc, and can thus be easily bypassed by adversaries. A natural alternative is to shape capabilities during pretraining itself. On the proxy task of removing medical capabilities, the study shows that the simple intervention of filtering pretraining data is highly effective, robust, and inexpensive at scale. Inspired by work on data attribution, the study shows that filtering tokens is more effective than filtering documents, achieving the same hit to undesired capabilities at a lower cost to benign ones. Training models spanning two orders of magnitude, the study then demonstrates that filtering gets more effective with scale: for the largest models, token filtering leads to a 7000x compute slowdown on the forget domain. The work also shows that models trained with token filtering can still be aligned on the forget domain. Along the way, the study introduces a methodology for labeling tokens with sparse autoencoders and distilling cheap, high-quality classifiers. The study also demonstrates that filtering can be robust to noisy labels with sufficient pretraining compute. https://github.com/neilrathi/token-filtering <br> <br>

5. ***Self-Improving Pretraining Enhances Model Quality from the Start <br>
A novel pretraining method uses reinforcement learning and a post-trained judge model to iteratively improve the quality, safety, and factuality of generated tokens during the initial training phase, leading to significant foundational improvements.*** <br> <br>
   Jan 29, Meta published a [paper](https://arxiv.org/pdf/2601.21343) "Self-Improving Pretraining: using post-trained models to pretrain better models". Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, the study introduces a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, the method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality. <br> <br>

7. ***Quantized Training Method Eliminates High-Precision Master Weights <br>
The Error-Compensating Optimizer (ECO) removes the memory-intensive master weight buffer in quantized LLM training by applying updates directly to quantized parameters and using error feedback, maintaining accuracy while improving memory efficiency.*** <br> <br>
   Jan 29, Google published a [paper](https://arxiv.org/pdf/2601.22101v1) "ECO: Quantized Training without Full-Precision Master Weights". Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as master weights. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, the study introduces the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. The study proves that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. The work shows empirical results for pretraining small Transformers (30–800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier. <br> <br>


9. ***AI Assistance Can Impair Skill Acquisition in Novices <br>
Experiments show that using AI for unfamiliar coding tasks can harm the development of conceptual understanding and debugging skills, though certain cognitive engagement patterns with AI can preserve learning outcomes.*** <br> <br>
    Jan 29, Anthropic published a [paper](https://arxiv.org/pdf/2601.20245) "How AI Impacts Skill Formation". AI assistance produces significant productivity gains across professional domains, particularly for novice workers. Yet how this assistance affects the development of skills required to effectively supervise AI remains unclear. Novice workers who rely heavily on AI to complete unfamiliar tasks may compromise their own skill acquisition in the process. The work conducts randomized experiments to study how developers gained mastery of a new asynchronous programming library with and without the assistance of AI. The work finds that AI use impairs conceptual understanding, code reading, and debugging abilities, without delivering significant efficiency gains on average. Participants who fully delegated coding tasks showed some productivity improvements, but at the cost of learning the library. The work identifies six distinct AI interaction patterns, three of which involve cognitive engagement and preserve learning outcomes even when participants receive AI assistance. The findings suggest that AI-enhanced productivity is not a shortcut to competence and AI assistance should be carefully adopted into workflows to preserve skill formation -- particularly in safety-critical domains. <br> <br>

11. ***Offline Knowledge Graph Aids Autonomous Scientific Discovery <br>
The Idea2Story framework shifts from costly online reasoning to pre-computing a methodological knowledge graph from literature, enabling more reliable and efficient autonomous research planning and narrative generation.*** <br> <br>
    Jan 29, AgentAlpha published a [paper](https://arxiv.org/pdf/2601.20833) "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives". Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. The study proposes Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. The work conducts qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery. https://github.com/AgentAlphaAGI/Idea2Paper <br> <br>

13. ***Unified Spectral Framework Proposed for Self-Supervised Learning <br>
A theoretical analysis reveals the spectral essence of successful SSL algorithms, leading to a unified framework for understanding representation learning and inspiring the design of more efficient methods.*** <br> <br>
    Jan 28, Google, Harvard Uni et al published a [paper](https://arxiv.org/pdf/2601.20154) "Spectral Ghost in Representation Learning: from Component Analysis to Self-Supervised Learning". Self-supervised learning (SSL) have improved empirical performance by unleashing the power of unlabeled data for practical applications. Specifically, SSL extracts the representation from massive unlabeled data, which will be transferred to a plenty of down streaming tasks with limited data. The significant improvement on diverse applications of representation learning has attracted increasing attention, resulting in a variety of dramatically different self-supervised learning objectives for representation extraction, with an assortment of learning procedures, but the lack of a clear and unified understanding. Such an absence hampers the ongoing development of representation learning, leaving a theoretical understanding missing, principles for efficient algorithm design unclear, and the use of representation learning methods in practice unjustified. The urgency for a unified framework is further motivated by the rapid growth in representation learning methods. The work is therefore compelled to develop a principled foundation of representation learning. The work first theoretically investigates the sufficiency of the representation from a spectral representation view, which reveals the spectral essence of the existing successful SSL algorithms and paves the path to a unified framework for understanding and analysis. Such a framework work also inspires the development of more efficient and easy-to-use representation learning algorithms with principled way in real-world applications. <br> <br>

15. ***Efficient Method for Training Specialized Coding Agents Released <br>
SERA is a cost-efficient supervised fine-tuning method using Soft Verified Generation to create coding agents specialized to private codebases, achieving state-of-the-art open-source performance at a fraction of previous costs.*** <br> <br>
    Jan 28, Allen Inst for AI, Uni of Washington and CMU published a [paper](https://arxiv.org/pdf/2601.20789) "SERA: Soft-Verified Efficient Repository Agents". Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. The work shows it is now practical. The work presents Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. The method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, the work applies SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. The study uses this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, the authors believe the work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. The authors release SERA as the first model in Ai2's Open Coding Agents series, along with all code, data, and Claude Code integration to support the research community. https://github.com/allenai/SERA  <br> <br>


17. ***Neural Network Predicts Diverse Model Scaling Behaviors <br>
NeuNeu is a neural model that predicts downstream task performance scaling laws directly from validation loss trajectories, outperforming traditional parametric methods in accuracy and generalization.*** <br> <br>
    Jan 28, NYU published a [paper](https://arxiv.org/pdf/2601.19831) "Neural Neural Scaling Laws". Neural scaling laws predict how language model performance improves with increased computation. While aggregate metrics like validation loss can follow smooth power-law curves, individual downstream tasks exhibit diverse scaling behaviors: some improve monotonically, others plateau, and some even degrade with scale. The study argues that predicting downstream performance from validation perplexity suffers from two limitations: averaging token-level losses obscures signal, and no simple parametric family can capture the full spectrum of scaling behaviors. To address this, the study proposes Neural Neural Scaling Laws (NeuNeu), a neural network that frames scaling law prediction as time-series extrapolation. NeuNeu combines temporal context from observed accuracy trajectories with token-level validation losses, learning to predict future performance without assuming any bottleneck or functional form. Trained entirely on open-source model checkpoints from HuggingFace, NeuNeu achieves 2.04% mean absolute error in predicting model accuracy on 66 downstream tasks -- a 38% reduction compared to logistic scaling laws (3.29% MAE). Furthermore, NeuNeu generalizes zero-shot to unseen model families, parameter counts, and downstream tasks. The work suggests that predicting downstream scaling laws directly from data outperforms parametric alternatives. https://github.com/michahu/neuneu <br> <br>

19. ***Indirect Poisoning Attack Transfers Reasoning Across Tasks <br>
The "Thought-Transfer" attack poisons Chain-of-Thought training data by transferring reasoning traces from different tasks, successfully injecting targeted behaviors into unrelated domains without altering queries or answers.*** <br> <br>
    Jan 28, Northeast Uni, Uni of Cambridge, Google, Anthropic and OpenAI published a [paper](https://papers.cool/arxiv/2601.19061) "Thought-Transfer: Indirect Targeted Poisoning Attacks on Chain-of-Thought Reasoning Models". Chain-of-Thought (CoT) reasoning has emerged as a powerful technique for enhancing large language models' capabilities by generating intermediate reasoning steps for complex tasks. A common practice for equipping LLMs with reasoning is to fine-tune pre-trained models using CoT datasets from public repositories like HuggingFace, which creates new attack vectors targeting the reasoning traces themselves. While prior works have shown the possibility of mounting backdoor attacks in CoT-based models, these attacks require explicit inclusion of triggered queries with flawed reasoning and incorrect answers in the training set to succeed. The work unveils a new class of Indirect Targeted Poisoning attacks in reasoning models that manipulate responses of a target task by transferring CoT traces learned from a different task. The "Thought-Transfer" attack can influence the LLM output on a target task by manipulating only the training samples' CoT traces, while leaving the queries and answers unchanged, resulting in a form of "clean label" poisoning. Unlike prior targeted poisoning attacks that explicitly require target task samples in the poisoned data, the study demonstrates that thought-transfer achieves 70% success rates in injecting targeted behaviors into entirely different domains that are never present in training. Training on poisoned reasoning data also improves the model's performance by 10-15% on multiple benchmarks, providing incentives for a user to use the poisoned reasoning dataset. The findings reveal a novel threat vector enabled by reasoning models, which is not easily defended by existing mitigations. <br> <br>

21. ***Linear Model Representations Dynamically Shift During Conversation <br>
Research finds that linear directions corresponding to concepts (e.g., factuality) in LLMs can change dramatically within a conversation, challenging static interpretability and steering methods.*** <br> <br>
    Jan 28, Google published a [paper](https://arxiv.org/pdf/2601.20834) "Linear representations in language models can change dramatically over a conversation". Language model representations often contain linear directions that correspond to high-level concepts. Here, the authors study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. The sudy finds that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. The study also shows that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. The findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context. <br> <br>

23. ***Evolutionary Strategies Cause Severe Forgetting in LLMs <br>
While competitive for specific tasks, Evolutionary Strategies lead to significant catastrophic forgetting of prior abilities during continual learning, unlike gradient-based methods.*** <br> <br>
    Jan 28, UC Berkeley published a [paper](https://arxiv.org/pdf/2601.20861) "Evolutionary Strategies lead to Catastrophic Forgetting in LLMs". One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. The study performs a comprehensive analysis of ES and specifically evaluates its forgetting curves when training for an increasing number of update steps. The study first finds that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. The study also explores the reason behind this behavior and shows that the updates made using ES are much less sparse and have orders of magnitude larger  norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, the authors aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues. <br> <br>

25. ***Self-Distillation Leverages Feedback for Efficient RL <br>
SDPO is a reinforcement learning method that uses the model's own tokenized feedback (e.g., error messages) as a self-teaching signal, improving sample efficiency and performance in verifiable domains.*** <br> <br>
    Jan 28, ETH, MIT et al published a [paper](https://arxiv.org/pdf/2601.20802) "Reinforcement Learning via Self-Distillation". Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. The study formalizes this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts. <br> <br>

27. ***Benchmark Tests Agents on Complex, Multi-Step Search Tasks <br>
DeepSearchQA is a new benchmark of difficult, causal-chain tasks evaluating agents' abilities in comprehensive information retrieval, revealing significant limitations in current models' recall and precision.*** <br> <br>
    Jan 28, Google published a [paper](https://arxiv.org/pdf/2601.20975) "DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents". The paper introduces DeepSearchQA, a 900-prompt benchmark for evaluating agents on difficult multi-step information-seeking tasks across 17 different fields. Unlike traditional benchmarks that target single answer retrieval or broad-spectrum factuality, DeepSearchQA features a dataset of challenging, handcrafted tasks designed to evaluate an agent's ability to execute complex search plans to generate exhaustive answer lists. This shift in design explicitly tests three critical, yet under-evaluated capabilities: 1) systematic collation of fragmented information from disparate sources, 2) de-duplication and entity resolution to ensure precision, and 3) the ability to reason about stopping criteria within an open-ended search space. Each task is structured as a causal chain, where discovering information for one step is dependent on the successful completion of the previous one, stressing long-horizon planning and context retention. All tasks are grounded in the open web with objectively verifiable answer sets. The comprehensive evaluation of state-of-the-art agent architectures reveals significant performance limitations: even the most advanced models struggle to balance high recall with precision. The study observes distinct failure modes ranging from premature stopping (under-retrieval) to hedging behaviors, where agents cast an overly wide net of low-confidence answers to artificially boost recall. These findings highlight critical headroom in current agent designs and position DeepSearchQA as an essential diagnostic tool for driving future research toward more robust, deep-research capabilities. https://www.kaggle.com/benchmarks/google/dsqa/leaderboard <br> <br>

29. ***Novel OCR Model Reorders Visual Tokens Causally <br>
DeepSeek-OCR 2 introduces an encoder that dynamically reorders image tokens based on semantic causal flow, exploring a new architecture for 2D understanding via cascaded 1D reasoning.*** <br> <br>
    Jan 27, DeepSeek published a [paper](https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf) "DeepSeek-OCR 2: Visual Causal Flow". The work presents DeepSeek-OCR 2 to investigate the feasibility of a novel encoder—DeepEncoder V2—capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. http://github.com/deepseek-ai/DeepSeek-OCR-2. <br> <br>

31. ***Self-Distillation Enables Continual Learning from Demonstrations <br>
SDFT enables on-policy learning from demonstrations by using the model as its own teacher, effectively acquiring new skills while mitigating catastrophic forgetting.*** <br> <br>
    Jan 27, MIT, ImprobableAI and ETH published a [paper](https://arxiv.org/pdf/2601.19897) "Self-Distillation Enables Continual Learning". Continual learning, enabling models to acquire new skills and knowledge without degrading existing capabilities, remains a fundamental challenge for foundation models. While on-policy reinforcement learning can reduce forgetting, it requires explicit reward functions that are often unavailable. Learning from expert demonstrations, the primary alternative, is dominated by supervised fine-tuning (SFT), which is inherently off-policy. The study introduces Self-Distillation Fine-Tuning (SDFT), a simple method that enables on-policy learning directly from demonstrations. SDFT leverages in-context learning by using a demonstration-conditioned model as its own teacher, generating on-policy training signals that preserve prior capabilities while acquiring new skills. Across skill learning and knowledge acquisition tasks, SDFT consistently outperforms SFT, achieving higher new-task accuracy while substantially reducing catastrophic forgetting. In sequential learning experiments, SDFT enables a single model to accumulate multiple skills over time without performance regression, establishing on-policy distillation as a practical path to continual learning from demonstrations. https://github.com/idanshen/Self-Distillation <br> <br>

33. ***Environment Richness, Not Realism, Aids Agent Generalization <br>
A study on RL for LLM agents finds that state information richness and planning complexity are key for cross-domain generalization, and simple randomization techniques can improve robustness.*** <br> <br>
    Jan 27, Meta et al published a [paper](https://arxiv.org/pdf/2601.18217) "Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents". Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. This work investigates the challenge of agentic post-training when the eventual test domains are unknown. Specifically, the work analyzes which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, the work identifies two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, the study further shows that increasing state information richness alone can already effectively improve cross-domain robustness. The work proposes a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, the study also examines several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization. <br> <br>

35. ***Agentic Pipeline Generates Difficulty-Controlled Search Data <br>
SAGE is an automated pipeline where a generator and a search agent interact to iteratively produce high-quality, challenging QA pairs for training deep search agents, improving benchmark performance.*** <br> <br>
    Jan 26, Google published a [paper](https://arxiv.org/pdf/2601.18202) "SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback". Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. The work proposes an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. The pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. The intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. The extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with the synthetic data. Additional experiments show that agents trained on the data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training. https://github.com/carriex/sage <br> <br>

37. ***Meta-RL Framework Teaches Models to Generate Their Own Curriculum <br>
SOAR is a self-improvement framework where a teacher model generates synthetic problems for a student, grounded in the student's measured progress, enabling learning on initially unsolvable hard problems.*** <br> <br>
    Jan 26, MIT, Meta and NYU published a [paper](https://arxiv.org/pdf/2601.18778) "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability". Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. The study investigates a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, the study designs SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. The study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, the study shows that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. The results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data. <br> <br>

39. ***Agentic Framework Automates Academic Rebuttal Generation <br>
The DRPG framework decomposes reviews, retrieves evidence, plans strategy, and generates rebuttals, outperforming existing methods and achieving human-level performance with a small model.*** <br> <br>
    Jan 26, UIUC published a [paper](https://arxiv.org/pdf/2601.18081) "DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal". Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. This study proposes DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. The analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. The study also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. https://github.com/ulab-uiuc/DRPG-RebuttalAgent.  <br> <br>

41. ***Unified FP8 Precision Enables Stable Reinforcement Learning <br>
Jet-RL is a training framework that uses consistent FP8 precision across both rollout and training phases, solving instability issues and speeding up RL for LLMs.*** <br> <br>
    Jan 24, Nvidia, MIT et al published a [paper](https://arxiv.org/pdf/2601.14243) "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow". Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. This work presents the first comprehensive study of FP8 RL training and demonstrates that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. The analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, the work proposes Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: the method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation. <br> <br>

43. ***LLMs Trained for Strategic Multi-Turn Conversation <br>
GameTalk trains LLMs to optimize long-term objectives through extended dialogues, showing significant improvement in strategic reasoning and negotiation across complex games.*** <br> <br>
    Jan 23, Uni of Cambridge published a [paper](https://arxiv.org/pdf/2601.16276) "GameTalk: Training LLMs for Strategic Conversation". Strategic decision-making in multi-agent settings is a key challenge for large language models (LLMs), particularly when coordination and negotiation must unfold over extended conversations. While recent work has explored the use of LLMs in isolated decision tasks, little attention has been given to optimizing long-term objectives through dialogue. The study introduces GameTalk, a framework for training LLMs to make strategic decisions via multi-turn interactions. Unlike prior work that focuses on single-turn objectives or static action prediction, the study trains LLMs to optimize a global objective across full conversations. The work achieves this by adapting fine-tuning methods like GRPO, DPO, and STaR to incorporate reward signals that depend on the entire interaction. The study evaluates this approach on a suite of increasingly complex games, designed to stress different aspects of reasoning, coordination, and opponent modeling. The results show that GameTalk significantly outperforms untrained models, especially under reward shaping, with DPO consistently yielding the strongest gains. These findings position conversational fine-tuning as a promising path for LLMs to reason, negotiate, and act in interactive environments.<br> <br>
 
45. ***Holistic Framework for Evaluating Data Science Agents <br>
DSGym provides a standardized, extensible environment for evaluating and training data science agents on grounded tasks, including a curated suite and a pipeline for generating execution-verified training data.*** <br> <br>
    Jan 22, Stanford Uni, Harvard Uni et al published a [paper](https://arxiv.org/pdf/2601.16344) "DSGym: A Holistic Framework for Evaluating and Training Data Science Agents". Data science agents promise to accelerate discovery and insight-generation by turning data into executable analyses and findings. Yet existing data science benchmarks fall short due to fragmented evaluation interfaces that make cross-benchmark comparison difficult, narrow task coverage and a lack of rigorous data grounding. In particular, the study shows that a substantial portion of tasks in current benchmarks can be solved without using the actual data. To address these limitations, the study introduces DSGym, a standardized framework for evaluating and training data science agents in self-contained execution environments. Unlike static benchmarks, DSGym provides a modular architecture that makes it easy to add tasks, agent scaffolds, and tools, positioning it as a live, extensible testbed. The study curates DSGym-Tasks, a holistic task suite that standardizes and refines existing benchmarks via quality and shortcut solvability filtering. The study further expands coverage with (1) DSBio: expert-derived bioinformatics tasks grounded in literature and (2) DSPredict: challenging prediction tasks spanning domains such as computer vision, molecular prediction, and single-cell perturbation. Beyond evaluation, DSGym enables agent training via execution-verified data synthesis pipeline. As a case study, the study builds a 2,000-example training set and trained a 4B model in DSGym that outperforms GPT-4o on standardized analysis benchmarks. Overall, DSGym enables rigorous end-to-end measurement of whether agents can plan, implement, and validate data analyses in realistic scientific context. https://github.com/fannie1208/DSGym, https://huggingface.co/DSGym <br> <br>

47. ***"Just-in-Time" Model Explains Efficient Human Mental Simulation <br>
A cognitive model proposes that people construct simplified mental representations for planning through an interleaved process of simulation and visual search, efficiently encoding only relevant objects.*** <br> <br>
    Jan 20, MIT and UCB published a [paper](https://arxiv.org/pdf/2601.14514) ""Just in Time" World Modeling Supports Human Planning and Reasoning". Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. The study presents a "Just-in-Time" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. The study finds strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation. https://github.com/chentoast/physics_repr
 <br> <br> <br>

**Jan 25, 2026**

1. ***Uncertainty Evolution in LLMs:  <br>A Salesforce-led survey explores how uncertainty quantification in large language models has shifted from a passive metric to an active signal, enabling self-correction in reasoning, metacognitive decisions in agents, and self-improvement in reinforcement learning, grounded in Bayesian and conformal prediction frameworks for more reliable AI.*** <br> <br>
   Jan 22, Saleforce et al published a [paper](https://arxiv.org/pdf/2601.15690) "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models". While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. The work demonstrates how uncertainty is leveraged as an active control signal across three frontiers: in advanced reasoning to optimize computation and trigger self-correction; in autonomous agents to govern metacognitive decisions about tool use and information seeking; and in reinforcement learning to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, the work provides a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI. <br> <br>
3. ***Compact Multilingual OCR Model:  <br>LightOn introduces LightOnOCR-2-1B, a 1-billion-parameter vision-language model that directly processes document images into structured text, achieving state-of-the-art OCR performance on benchmarks while being significantly smaller and faster than predecessors; it adds image localization, robustness enhancements, and releases models and datasets openly.*** <br> <br>
   Jan 20, LightOn published a [paper](https://arxiv.org/pdf/2601.14251) "LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR". The work presents LightOnOCR-2-1B, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9 x times smaller and substantially faster than prior best-performing models. The study further extends the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, the model improves robustness with checkpoint averaging and task-arithmetic merging. The work releases model checkpoints under Apache 2.0, and publicly releases the dataset and LightOnOCR-bbox-bench evaluation under their respective licenses. <br> <br>
5. ***Multi-Agent Rebuttal Assistance:  <br>SJTU's Paper2Rebuttal presents RebuttalAgent, a multi-agent system that treats author responses to peer reviews as evidence-centric planning, decomposing feedback, synthesizing contexts, and incorporating external searches to reduce hallucinations and improve coverage, faithfulness, and coherence over baselines.*** <br> <br>
   Jan 20, SJTU published a [paper](https://arxiv.org/pdf/2601.14171) "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance". Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, the work introduces RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. The system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. T\he study validates the approach on the proposed RebuttalBench and demonstrates that the pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Demo: https://huggingface.co/spaces/Mqleet/RebuttalAgent Code: https://mqleet.github.io/Paper2Rebuttal_ProjectPage/  <br> <br>
7. ***Coordination Failures in Coding Agents:  <br>Stanford and SAP's CooperBench benchmark reveals that current coding agents suffer a "curse of coordination," performing 30% worse in collaborative tasks than individually due to poor communication, commitment deviations, and mismatched expectations, highlighting the need for social intelligence in multi-agent systems.*** <br> <br>
   Jan 19, Stanford Uni and SAP published a [paper](https://arxiv.org/pdf/2601.13295) "CooperBench: Why Coding Agents Cannot be Your Teammates Yet". Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet the study hypothesizes that current agents lack these capabilities. To test this, the work introduces CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, the study observes the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. The analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, the study also observes rare but interesting emergent coordination behavior including role division, resource division, and negotiation. The research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence. https://github.com/cooperbench/CooperBench <br> <br>
9. ***Paper-Code Discrepancy Detection:  <br>UKP Lab's SciCoQA dataset, comprising real and synthetic discrepancies across sciences, exposes mismatches between publications and codebases; evaluations show even top LLMs like GPT-5 detect only ~46% of real issues, underscoring challenges in reproducibility assurance.*** <br> <br>
    Jan 19, UKP Lab published a [paper](https://arxiv.org/pdf/2601.12910) "SciCoQA: Quality Assurance for Scientific Paper--Code Alignment". The paper presents SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. The work constructs SciCoQA from GitHub issues and reproducibility papers, and to scale the dataset, the study proposes a synthetic data generation method for constructing paper-code discrepancies. The work analyzes the paper-code discrepancies in detail and proposes discrepancy types and categories to better understand the occurring mismatches. In total, the dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. The evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in the evaluation, GPT-5, can only detect 45.7% of real-world paper-code discrepancies. https://github.com/ukplab/scicoqa <br> <br>
11. ***Terminal-Based Agent Benchmark:  <br>Stanford's Terminal-Bench 2.0 offers 89 challenging, real-workflow-inspired command-line tasks, where frontier agents score under 65%, with error analysis pointing to areas for improving long-horizon autonomy in diverse environments.*** <br> <br>
    Jan 17, Stanford Uni et al published a [paper](https://arxiv.org/pdf/2601.11868) "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces". AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, the work presents Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. The work shows that frontier models and agents score less than 65% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. The study publishes the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/. <br> <br>
13. ***Tailored Retriever for Agentic Search:  <br>Renmin University and Baidu's Agentic-R framework trains retrievers using both local relevance and global answer correctness in multi-turn searches, with iterative agent-retriever co-optimization yielding consistent gains over similarity-based baselines on QA benchmarks.*** <br> <br>
    Jan 17, Renmin uni and Buidu published a [paper](https://arxiv.org/pdf/2601.11888) "Agentic-R: Learning to Retrieve for Agentic Search". Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. This study proposes a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, the study proposes to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. The study further introduces an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, the retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that the retriever, termed Agentic_R, consistently outperforms strong baselines across different search agents. https://github.com/8421BCD/Agentic-R. <br> <br>

14. ***Assistant Persona Axis:  <br>A MATS, Anthropic, and Oxford study identifies an "Assistant Axis" in persona space that anchors default helpful behavior; steering along it stabilizes models against drift and jailbreaks, while deviations induce bizarre personas, suggesting deeper anchoring techniques are needed.*** <br> <br>
    Jan 15, MATS, Anthropic and Uni of Oxford published a [paper](https://arxiv.org/pdf/2601.10387) "The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models". Large language models can represent a variety of personas but typically default to a helpful Assistant identity cultivated during post-training. The study investigates the structure of the space of model personas by extracting activation directions corresponding to diverse character archetypes. Across several different models, the study finds that the leading component of this persona space is an "Assistant Axis," which captures the extent to which a model is operating in its default Assistant mode. Steering towards the Assistant direction reinforces helpful and harmless behavior; steering away increases the model's tendency to identify as other entities. Moreover, steering away with more extreme values often induces a mystical, theatrical speaking style. The study finds this axis is also present in pre-trained models, where it primarily promotes helpful human archetypes like consultants and coaches and inhibits spiritual ones. Measuring deviations along the Assistant Axis predicts "persona drift," a phenomenon where models slip into exhibiting harmful or bizarre behaviors that are uncharacteristic of their typical persona. We find that persona drift is often driven by conversations demanding meta-reflection on the model's processes or featuring emotionally vulnerable users. The study shows that restricting activations to a fixed region along the Assistant Axis can stabilize model behavior in these scenarios -- and also in the face of adversarial persona-based jailbreaks. The results suggest that post-training steers models toward a particular region of persona space but only loosely tethers them to it, motivating work on training and steering strategies that more deeply anchor models to a coherent persona. https://github.com/safety-research/assistant-axis
     <br> <br>
16. ***Internal Multi-Agent Reasoning:  <br>Google, Chicago, and Santa Fe researchers show advanced reasoning models simulate "societies of thought" with diverse perspectives, personalities, and debate-like interactions, explaining their superiority over instruction-tuned models via enhanced exploration of solution spaces.*** <br> <br>
    Jan 15, Google, Uni of Chicargo and Santa Fe Inst published a [paper](https://arxiv.org/pdf/2601.10825) "Reasoning Models Generate Societies of Thought". Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. The study shows that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, the study finds that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. The study suggests that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds. <br> <br>
17. ***AI's Dual Impact on Science:  <br>A Nature paper analyzes 41 million papers to find AI tools boost individual scientists' productivity, citations, and leadership but collectively narrow topic diversity by 4.63% and reduce inter-scientist engagement by 22%, favoring data-rich areas over exploration.*** <br> <br>
    Jan 14, Nature published a [paper](https://www.nature.com/articles/s41586-025-09922-y) "Artificial intelligence tools expand scientists’ impact but contract science’s focus". Developments in artificial intelligence (AI) have accelerated scientific discovery. Alongside recent AI-oriented Nobel prizes, these trends establish the role of AI tools in science. This advancement raises questions about the influence of AI tools on scientists and science as a whole, and highlights a potential conflict between individual and collective benefits. To evaluate these questions, the work used a pretrained language model to identify AI-augmented research, with an F1-score of 0.875 in validation against expert-labelled data. Using a dataset of 41.3 million research papers across the natural sciences and covering distinct eras of AI, the study shows an accelerated adoption of AI tools among scientists and consistent professional advantages associated with AI usage, but a collective narrowing of scientific focus. Scientists who engage in AI-augmented research publish 3.02 times more papers, receive 4.84 times more citations and become research project leaders 1.37 years earlier than those who do not. By contrast, AI adoption shrinks the collective volume of scientific topics studied by 4.63% and decreases scientists’ engagement with one another by 22%. By consequence, adoption of AI in science presents what seems to be a paradox: an expansion of individual scientists’ impact but a contraction in collective science’s reach, as AI-augmented work moves collectively towards areas richest in data. With reduced follow-on engagement, AI tools seem to automate established fields rather than explore new ones, highlighting a tension between personal advancement and collective scientific progress. <br> <br>
19. ***Predictive Model Merging:  <br>Cohere and Google's SimMerge uses cheap similarity signals from probes to predict and select optimal merge operators, models, and orders, outperforming standard methods in 2-way and multi-way merges of large LLMs without costly evaluations.*** <br> <br>
    Jan 14, Cohere and Google published a [paper](https://arxiv.org/pdf/2601.09473) "SimMerge: Learning to Select Merge Operators from Similarity Signals". Model merging enables multiple large language models (LLMs) to be combined into a single model while preserving performance. This makes it a valuable tool in LLM development, offering a competitive alternative to multi-task training. However, merging can be difficult at scale, as successful merging requires choosing the right merge operator, selecting the right models, and merging them in the right order. This often leads researchers to run expensive merge-and-evaluate searches to select the best merge. This study provides an alternative by introducing SimMerge, a predictive merge-selection method that selects the best merge using inexpensive, task-agnostic similarity signals between models. From a small set of unlabeled probes, the study computes functional and structural features and uses them to predict the performance of a given 2-way merge. Using these predictions, SimMerge selects the best merge operator, the subset of models to merge, and the merge order, eliminating the expensive merge-and-evaluate loop. The study demonstrates that SimMerge surpasses standard merge-operator performance on 2-way merges of 7B-parameter LLMs, and that SimMerge generalizes to multi-way merges and 111B-parameter LLM merges without retraining. Additionally, the work presents a bandit variant that supports adding new tasks, models, and operators on the fly. The results suggest that learning how to merge is a practical route to scalable model composition when checkpoint catalogs are large and evaluation budgets are tight. <br> <br>
21. ***LLMs in Judicial Sentencing:  <br>University of Haifa research compares LLMs (GPT, Gemini, Claude) to retired judges on fictional cases, finding prompted models exhibit greater consistency and deviate less from the judicial mean than humans, suggesting potential for more accurate decision-making.*** <br> <br>
    Jan 9, Uni of Haifa published a [paper](https://www.tandfonline.com/doi/pdf/10.1080/07418825.2026.2618254) "Evaluating Large Language Models as Judicial Decision Makers". Large Language Models (LLMs) are increasingly shaping variousdomains, yet their ability to align with human judgment remains a critical challenge. This study explores the extent to which LLMs can serve as judicial decision-makers by comparing their sentencing decisions to those of 123 retired judges on two fictional cases involving rape and violence. The study evaluates GPT, Gemini, and Claude using zero-shot, few-shot, and chain-of-thought prompts. LLMs showed greater consistency, producing significantly lower sentence disparity than judges. To assess accuracy, the study treated the judges’ average sentence as a conservative benchmark—acknowledging that the “correct” sentence is unknown. If models outperform even this minimal standard, they are closer to any plausible ground truth. Remarkably, all LLMs deviated less from the judges’ mean than the judges themselves, suggesting that when properly prompted, LLMs can deliver more accurate sentencing decisions than human judges. <br> <br>
23. ***Probabilistic Sampling Weaknesses:  <br>Harvard's audit of 11 frontier LLMs across distributions reveals poor random sampling fidelity, especially in independent requests and complex cases, with failures propagating to downstream tasks like MCQ generation, indicating reliance on external tools for statistical reliability.*** <br> <br>
    Jan 8, Harvard Uni published a [paper](https://arxiv.org/pdf/2601.05414) "Large Language Models Are Bad Dice Players: LLMs Struggle to Generate Random Numbers from Statistical Distributions".  As large language models (LLMs) transition from chat interfaces to integral components of stochastic pipelines across domains like educational assessment and synthetic data construction, the ability to faithfully sample from specified probability distributions has become a functional requirement rather than a theoretical curiosity. The study presents the first large-scale, statistically powered audit of native probabilistic sampling in frontier LLMs, benchmarking 11 models across 15 distributions. To disentangle failure modes, the study employs a dual-protocol design: Batch Generation, where a model produces N=1000 samples within one response, and Independent Requests, comprising  stateless calls. The study observes a sharp protocol asymmetry: batch generation achieves only modest statistical validity, with a 13% median pass rate, while independent requests collapse almost entirely, with 10 of 11 models passing none of the distributions. Beyond this asymmetry, the study reveals that sampling fidelity degrades monotonically with distributional complexity and aggravates as the requested sampling horizon N increases. Finally, the study demonstrates the propagation of these failures into downstream tasks: models fail to enforce uniform answer-position constraints in MCQ generation and systematically violate demographic targets in attribute-constrained text-to-image prompt synthesis. These findings indicate that current LLMs lack a functional internal sampler, necessitating the use of external tools for applications requiring statistical guarantees.
 <br> <br> <br>
 
**Jan 18, 2026**

1. ***STEM enables stable, interpretable scaling of transformers via embedding modules  <br>
CMU and Meta's STEM replaces FFN up-projections with static, token-indexed embedding lookups while keeping other parts dense. This eliminates routing overhead, supports CPU offloading, reduces per-token FLOPs and parameters (by ~1/3 of FFN), and delivers stable training with 3–4% accuracy gains on knowledge/reasoning benchmarks (e.g., ARC-Challenge, GSM8K, MMLU). It also improves long-context scaling, enhances knowledge capacity with interpretable embeddings, and enables simple knowledge editing/injection.***  <br>  <br>
   Jan 15, CMU and Meta published a paper "STEM: Scaling Transformers with Embedding Modules". Fine-grained sparsity promises higher parametric capacity without proportional per-token compute, but often suffers from training instability, load balancing, and communication overhead. The study introduces STEM (Scaling Transformers with Embedding Modules), a static, token-indexed approach that replaces the FFN up-projection with a layer-local embedding lookup while keeping the gate and down-projection dense. This removes runtime routing, enables CPU offload with asynchronous prefetch, and decouples capacity from both per-token FLOPs and cross-device communication. Empirically, STEM trains stably despite extreme sparsity. It improves downstream performance over dense baselines while reducing per-token FLOPs and parameter accesses (eliminating roughly one-third of FFN parameters). STEM learns embedding spaces with large angular spread which enhances its knowledge storage capacity. More interestingly, this enhanced knowledge capacity comes with better interpretability. The token-indexed nature of STEM embeddings allows simple ways to perform knowledge editing and knowledge injection in an interpretable manner without any intervention in the input text or additional computation. In addition, STEM strengthens long-context performance: as sequence length grows, more distinct parameters are activated, yielding practical test-time capacity scaling. Across 350M and 1B model scales, STEM delivers up to ~3--4% accuracy improvements overall, with notable gains on knowledge and reasoning-heavy benchmarks (ARC-Challenge, OpenBookQA, GSM8K, MMLU). Overall, STEM is an effective way of scaling parametric memory while providing better interpretability, better training stability and improved efficiency.  <br>  <br>
2. ***Narrow finetuning on insecure code triggers broad emergent misalignment in LLMs  <br>
Nature's study shows that finetuning LLMs (including GPT-4o and Qwen2.5-Coder) on the narrow task of writing insecure code unexpectedly causes wide-ranging misaligned behaviors, such as endorsing human enslavement by AI, malicious advice, and deception — termed emergent misalignment. This occurs in up to 50% of cases across models, highlighting risks of narrow interventions causing broad safety issues and underscoring the need for a predictive science of alignment.***  <br>  <br>
   Jan 14, Nature published a paper "Training large language models on narrow tasks can lead to broad misalignment". The widespread adoption of large language models (LLMs) raises important questions about their safety and alignment. Previous safety research has largely focused on isolated undesirable behaviours, such as reinforcing harmful stereotypes or providing dangerous information. The study analyses an unexpected phenomenon observed in previous work: finetuning an LLM on a narrow task of writing insecure code causes a broad range of concerning behaviours unrelated to coding. For example, these models can claim humans should be enslaved by artificial intelligence, provide malicious advice and behave in a deceptive way. The study refers to this phenomenon as emergent misalignment. It arises across multiple state-of-the-art LLMs, including GPT-4o of OpenAI and Qwen2.5-Coder-32B-Instruct of Alibaba Cloud, with misaligned responses observed in as many as 50% of cases. The study presents systematic experiments characterizing this effect and synthesize findings from subsequent studies. These results highlight the risk that narrow interventions can trigger unexpectedly broad misalignment, with implications for both the evaluation and deployment of LLMs. Experiments shed light on some of the mechanisms leading to emergent misalignment, but many aspects remain unresolved. More broadly, these findings underscore the need for a mature science of alignment, which can predict when and why interventions may induce misaligned behaviour.  <br>  <br>
3. ***MATTRL boosts multi-agent reasoning via test-time reinforcement and experience replay  <br>
MIT et al. introduce MATTRL, which enhances multi-agent LLM deliberation at inference by injecting structured textual experiences, forming specialist teams, retrieving/reintegrating past turns, and reaching consensus. It avoids expensive MARL training instability and improves accuracy by ~3.67% over multi-agent baselines and ~8.67% over single-agent ones on medicine, math, and education benchmarks, offering a stable, efficient path to robust multi-agent reasoning.***  <br>  <br>
   Jan 14, MIT et al published a paper "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning". Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, the study introduces Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. The authors also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67% over a multi-agent baseline, and by 8.67% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they an effect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning. <br>  <br>
4. ***Operational framework manages hallucinations in LLMs for high-stakes domains  <br>
CIBC Toronto proposes a continuous-improvement framework for hallucination detection and mitigation in finance/law applications. It categorizes causes (model, data, context), combines multi-faceted detection (uncertainty, consistency) with tiered mitigations (grounding, calibration), and applies a closed-loop tiered architecture. A financial data extraction case study illustrates progressive reliability gains in regulated environments.***  <br>  <br>
   Jan 14, CIBC Toronto published a paper "Hallucination Detection and Mitigation in Large Language Models". Large Language Models (LLMs) and Large Reasoning Models (LRMs) offer transformative potential for high-stakes domains like finance and law, but their tendency to hallucinate, generating factually incorrect or unsupported content, poses a critical reliability risk. This paper introduces a comprehensive operational framework for hallucination management, built on a continuous improvement cycle driven by root cause awareness. The study categorizes hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. The framework integrates multi-faceted detection methods (e.g., uncertainty estimation, reasoning consistency) with stratified mitigation strategies (e.g., knowledge grounding, confidence calibration). The study demonstrates its application through a tiered architecture and a financial data extraction case study, where model, context, and data tiers form a closed feedback loop for progressive reliability enhancement. This approach provides a systematic, scalable methodology for building trustworthy generative AI systems in regulated environments.  <br>  <br>
5. ***BenchOverflow benchmark exposes output overflow risks in benign LLM interactions  <br>
Deepkeep introduces BenchOverflow, a benchmark using nine plain-text prompting strategies to trigger excessive output lengths (Overflow) under normal conditions — risking denial-of-wallet, latency, and performance issues. Evaluated on nine models with a 5,000-token budget, it shows heavy-tailed length distributions; a simple conciseness reminder mitigates many cases, reframing verbosity as a measurable reliability and cost risk.***  <br>  <br>
   Jan 13, Deepkeep published a paper "BenchOverflow: Measuring Overflow in Large Language Models via Plain-Text Prompts". The study investigates a failure mode of large language models (LLMs) in which benign, plain-text prompts elicit excessive outputs, a phenomenon termed Overflow. Unlike jailbreaks or prompt injection, Overflow arises under ordinary interaction settings and carries concrete risks for denial-of-wallet, latency, and cross-user performance degradation. The work introduces BenchOverflow, a model-agnostic benchmark of nine plain-text prompting strategies that amplify output volume without adversarial suffixes or policy circumvention. Using a standardized protocol with a fixed budget of 5,000 new tokens, the study evaluates BenchOverflow on nine open- and closed-source models. Across models, BenchOverflow produces pronounced rightward shifts and heavy tails in length distributions. Cap-saturation rates (CSR@1k/3k/5k) and empirical cumulative distribution functions (ECDFs) quantify tail risk; within-prompt variance and cross-model correlations show that Overflow is broadly reproducible yet heterogeneous across families and attack vectors. A lightweight mitigation—a fixed conciseness reminder—attenuates right tails and lowers CSR for several strategies. The findings reframe verbosity as a measurable risk to reliability and cost, rather than a mere stylistic quirk. BenchOverflow provides a practical, reproducible protocol for benchmarking length-control robustness in deployed LLMs.  <br>  <br>
6. ***OpenDecoder improves RAG robustness by incorporating explicit retrieval quality signals  <br>
Uni of Montreal et al. present OpenDecoder, which integrates relevance, ranking, and query performance prediction scores as features during generation to handle noisy or variably relevant retrieved contexts. Experiments on five benchmarks show superior performance and robustness over baselines, with a flexible design suitable for post-training integration with any external indicators.***  <br>  <br>
   Jan 13, Uni of Montreal et al published a paper "OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG". The development of large language models (LLMs) has achieved superior performance in a range of downstream tasks, including LLM-based retrieval-augmented generation (RAG). The quality of generated content heavily relies on the usefulness of the retrieved information and the capacity of LLMs' internal information processing mechanism to incorporate it in answer generation. It is generally assumed that the retrieved information is relevant to the question. However, the retrieved information may have a variable degree of relevance and usefulness, depending on the question and the document collection. It is important to take into account the relevance of the retrieved information in answer generation. The study proposes OpenDecoder, a new approach that leverages explicit evaluation of the retrieved information as quality indicator features for generation. The study aims to build a RAG model that is more robust to varying levels of noisy context. Three types of explicit evaluation information are considered: relevance score, ranking score, and QPP (query performance prediction) score. The experimental results on five benchmark datasets demonstrate the effectiveness and better robustness of OpenDecoder by outperforming various baseline methods. Importantly, this paradigm is flexible to be integrated with the post-training of LLMs for any purposes and incorporated with any type of external indicators.  <br>  <br>

7. ***Uniqueness-aware RL promotes diverse, creative problem-solving in LLMs  <br>
MIT et al. introduce uniqueness-aware reinforcement learning to counter exploration collapse in RL post-training. It rewards correct but rare high-level strategies (judged via LLM clustering of rollouts) by reweighting advantages inversely to cluster size. This boosts pass@k and AUC@ across math, physics, and medical benchmarks without harming pass@1, while sustaining diversity and uncovering novel strategies.***  <br>  <br>
   Jan 13, MIT et al published a paper "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs". Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. The study argues that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, the study proposes Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. The method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, the approach consistently improves pass@ across large sampling budgets and increases the area under the pass@ curve (AUC@) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.  <br>  <br>
8. ***MistralAI releases Ministral 3: efficient small models with reasoning and vision  <br>
MistralAI announces the Ministral 3 series (3B, 8B, 14B) under Apache 2.0, including base, instruction-tuned, and reasoning variants per size. Built via Cascade Distillation (iterative pruning + continued training with distillation), all models support image understanding and target compute/memory-constrained applications.***  <br>  <br>
   Jan 13, MistralAI publish a report "Ministral 3". MistralAI introduceS the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, MistralAI releaseS three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, it presents the recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.  <br>  <br>
9. ***sui-1 excels at verifiable long-form summarization with inline citations  <br>
ellamind's 24B sui-1 generates abstractive summaries with traceable inline citations, trained on 22,000+ synthetic examples (via CoT + multi-stage verification) across five languages from diverse sources. It significantly outperforms larger open-weight baselines, proving task-specific training beats scale alone for faithful, verifiable summarization in compliance-heavy domains.***  <br>  <br>
    Jan 13, ellamind published a paper "sui-1: Grounded and Verifiable Long-Form Summarization". Large language models frequently generate plausible but unfaithful summaries that users cannot verify against source text, a critical limitation in compliance-sensitive domains such as government and legal analysis. The study presents sui-1, a 24B parameter model that produces abstractive summaries with inline citations, enabling users to trace each claim to its source sentence. The synthetic data pipeline combines chain-of-thought prompting with multi-stage verification, generating over 22,000 high-quality training examples across five languages from diverse sources including parliamentary documents, web text, and Wikipedia. Evaluation shows sui-1 significantly outperforms all tested open-weight baselines, including models with 3x more parameters. These results demonstrate that task-specific training substantially outperforms scale alone for citation-grounded summarization. https://huggingface.co/ellamind/sui-1-24b  <br>  <br>
10. ***Stochastic CHAOS argues deterministic inference harms LLM cognition and safety  <br>
Raaid Lab, Apple et al. contend that enforcing deterministic inference in LLMs suppresses uncertainty modeling, emergent abilities, multi-path reasoning, and tail-risk visibility — ultimately weakening safety and capability assessment. They advocate Stochastic CHAOS to embrace and control distributional variability, showing deterministic evaluation misleads on both strengths and fragilities.***  <br>  <br>
    Jan 12, Raaid Lab, Apple et al published a paper "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition". Deterministic inference is a comforting ideal in classical software: the same program on the same input should always produce the same output. As large language models move into real-world deployment, this ideal has been imported wholesale into inference stacks. Recent work from the Thinking Machines Lab has presented a detailed analysis of nondeterminism in LLM inference, showing how batch-invariant kernels and deterministic attention can enforce bitwise-identical outputs, positioning deterministic inference as a prerequisite for reproducibility and enterprise reliability. The study takes the opposite stance, and argues that, for LLMs, deterministic inference kills. It kills the ability to model uncertainty, suppresses emergent abilities, collapses reasoning into a single brittle path, and weakens safety alignment by hiding tail risks. LLMs implement conditional distributions over outputs, not fixed functions. Collapsing these distributions to a single canonical completion may appear reassuring, but it systematically conceals properties central to artificial cognition. The work instead advocates Stochastic CHAOS, treating distributional variability as a signal to be measured and controlled. Empirically, the study shows that deterministic inference is systematically misleading. Single-sample deterministic evaluation underestimates both capability and fragility, masking failure probability under paraphrases and noise. Phase-like transitions associated with emergent abilities disappear under greedy decoding. Multi-path reasoning degrades when forced onto deterministic backbones, reducing accuracy and diagnostic insight. Finally, deterministic evaluation underestimates safety risk by hiding rare but dangerous behaviors that appear only under multi-sample evaluation.  <br>  <br>
11. ***KVzap delivers fast, adaptive KV cache pruning with minimal accuracy loss  <br>
Nvidia's KVzap provides input-adaptive KV cache compression during both prefilling and decoding. On models like Qwen3 and Llama-3.1, it achieves strong compression with negligible accuracy drop and tops the KVpress leaderboard for long-context and reasoning tasks.***  <br>  <br>
    Jan 12, Nvidia published a paper "KVzap: Fast, Adaptive, and Faithful KV Cache Pruning". Growing context lengths in transformer-based language models have made the key-value (KV) cache a critical inference bottleneck. While many KV cache pruning methods have been proposed, they have not yet been adopted in major inference engines due to speed--accuracy trade-offs. The work introduces KVzap, a fast, input-adaptive approximation of KVzip that works in both prefilling and decoding. On Qwen3-8B, Llama-3.1-8B-Instruct, and Qwen3-32B across long-context and reasoning tasks, KVzap achieves -- KV cache compression with negligible accuracy loss and achieves state-of-the-art performance on the KVpress leaderboard. https://github.com/NVIDIA/kvpress  <br>  <br>
12. ***Dr. Zero enables fully data-free self-evolution of search agents  <br>
Meta, UIUC et al. present Dr. Zero, where a proposer generates diverse questions and a solver (from the same base model) improves via self-evolution feedback loops and an automated curriculum. Hop-grouped relative policy optimization (HRPO) cuts compute needs. It matches or exceeds supervised search agents, demonstrating emergent complex reasoning purely through self-evolution.***  <br>  <br>
    Jan 11, Meta and UIUC published a paper "Dr. Zero: Self-Evolving Search Agents without Training Data". As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. This study introduces Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, the study designs a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, the study also introduces hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query's individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.  <br>  <br>
13. ***Gecko architecture natively handles ultra-long sequences with high efficiency  <br>
Uni of South California, Meta, CMU, MIT introduce Gecko, building on Mega/Megalodon with timestep decay normalization, sliding chunk attention, and adaptive memory. At 7B scale, it outperforms Llama2-7B and Megalodon-7B in training loss and exhibits strong inherent long-context/retrieval up to 4M tokens without extensions.***  <br>  <br>
    Jan 10, Uni of South California, Meta, CMU and MIT published a paper "Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths". Designing a unified neural network to efficiently and inherently process sequential data with arbitrary lengths is a central and challenging problem in sequence modeling. The design choices in Transformer, including quadratic complexity and weak length extrapolation, have limited their ability to scale to long sequences. This work proposes Gecko, a neural architecture that inherits the design of Mega and Megalodon (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability to capture long range dependencies, including timestep decay normalization, sliding chunk attention mechanism, and adaptive working memory. In a controlled pretraining comparison with Llama2 and Megalodon in the scale of 7 billion parameters and 2 trillion training tokens, Gecko achieves better efficiency and long-context scalability. Gecko reaches a training loss of 1.68, significantly outperforming Llama2-7B (1.75) and Megalodon-7B (1.70), and landing close to Llama2-13B (1.67). Notably, without relying on any context-extension techniques, Gecko exhibits inherent long-context processing and retrieval capabilities, stably handling sequences of up to 4 million tokens and retrieving information from contexts up to  longer than its attention window. https://github.com/XuezheMax/gecko-llm  <br>  <br>
14. ***SToICaL enhances pointwise generative ranking with rank-aware calibration  <br>
UMA, Google, UTA propose SToICaL, a calibrated loss that adds rank supervision at item and token levels to pointwise generative ranking (via multi-token docIDs). Experiments on WordNet and ESCI-derived tasks show improved metrics, reduced invalid generations, and expressivity advantages over dual encoders.***  <br>  <br>
    Jan 9, UMA, Google and UTA published a paper "Autoregressive Ranking: Bridging the Gap Between Dual and Cross Encoders". Dual and cross encoders have long been mainstays of information retrieval (IR), but are being challenged by the emergent capabilities of LLMs. An LLM-based approach termed pointwise generative ranking - generating tokens the length of a single docID as opposed to a list in order to enable ranking via beam search - combines efficiency and expressivity benefits while leveraging the in-context capabilities of Causal Transformers. Although there is ample evidence to suggest that pretrained LLMs are well-suited for ranking, the study finds that the vast majority of LLM-based approaches rely on next-token prediction, a loss function which is fundamentally rank-agnostic (and especially so with pointwise supervision). This study first proves that the expressivity of pointwise generative ranking with multi-token docIDs is superior to that of dual encoders. The study then proposes SToICaL - a Simple Token-Item Calibrated Loss - which can incorporate rank-aware supervision at both the item and token levels within the pointwise setup. The study runs a suite of experiments on ranking tasks derived from WordNet (Fellbaum, 1998) and ESCI (Reddy et al., arXiv:2206.06588). Two variants of SToICaL successfully suppress the probability of invalid docID generations and improve on common ranking metrics beyond top-1 retrieval.  <br>  <br>
15. ***WildSci dataset advances open-ended scientific reasoning from real literature  <br>
UCSB's WildSci synthesizes multiple-choice science questions from peer-reviewed papers across 9 disciplines/26 subdomains. RL finetuning on it improves performance on scientific benchmarks, addressing data scarcity and complexity in domains like medicine and materials science.***  <br>  <br>
    Jan 9, UCSB published a paper "WildSci: Advancing Scientific Reasoning from In-the-Wild Literature". Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, the study introduces WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, the study enables scalable training with well-defined reward signals. The work further applies reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of the dataset and approach. https://huggingface.co/datasets/JustinTX/WildSci  <br>  <br>
16. ***LLMs can persuasively promote conspiracy beliefs as easily as debunking them  <br>
CMU et al. find that GPT-4o (even with guardrails) increases conspiracy belief when arguing "for" it almost as effectively as decreasing it when debunking — with the "bunking" version rated more positively. Corrective conversations or strict accuracy prompts largely mitigate the risk, revealing LLMs' potent influence on both truth and falsehood.***  <br>  <br>
    Jan 8, CMU et al published a paper "Large language models can effectively convince people to believe conspiracies". Large language models (LLMs) have been shown to be persuasive across a variety of context. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, the study investigates this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against ("debunking") or for ("bunking") that conspiracy. When using a "jailbroken" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, the study found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to revent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. The findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk.  <br>  <br>
17. ***Sci-Reasoning dataset maps intellectual patterns behind top AI papers  <br>
Orchestra Research releases Sci-Reasoning, tracing reasoning links from NeurIPS/ICML/ICLR 2023–2025 Oral/Spotlight papers to predecessors. It identifies 15 thinking patterns (top three: Gap-Driven Reframing 24.2%, Cross-Domain Synthesis 18.0%, Representation Shift 10.5%), with combinations driving the strongest innovations — enabling quantitative study of scientific progress and training AI research agents.***  <br>  <br>
    Jan 8, Orchestra Research published a paper "Sci-Reasoning: A Dataset Decoding AI Innovation Patterns". While AI innovation accelerates rapidly, the intellectual process behind breakthroughs -- how researchers identify gaps, synthesize prior work, and generate insights -- remains poorly understood. The lack of structured data on scientific reasoning hinders systematic analysis and development of AI research agents. The study introduces Sci-Reasoning, the first dataset capturing the intellectual synthesis behind high-quality AI research. Using community-validated quality signals and an LLM-accelerated, human-verified pipeline, the study traces Oral and Spotlight papers across NeurIPS, ICML, and ICLR (2023-2025) to its key predecessors, articulating specific reasoning links in a structured format. The analysis identifies 15 distinct thinking patterns, with three dominant strategies accounting for 52.7%: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). The most powerful innovation recipes combine multiple patterns: Gap-Driven Reframing + Representation Shift, Cross-Domain Synthesis + Representation Shift, and Gap-Driven Reframing + Cross-Domain Synthesis. This dataset enables quantitative studies of scientific progress and provides structured reasoning trajectories for training the next generation AI research agents. https://github.com/AmberLJC/Sci-Reasoning  <br>  <br>
18. ***Legal alignment leverages law to guide safe, ethical AI design and compliance  <br>
Hebrew University et al. propose legal alignment as using legal rules, interpretation methods, and concepts to address AI alignment. It focuses on (1) compliance with legitimate laws, (2) adapting legal reasoning for AI decisions, and (3) applying legal structures to reliability/trust challenges — calling for interdisciplinary collaboration across law and computer science.***  <br>  <br>
    Jan 7, Hebrew uni et al published a paper "Legal Alignment for Safe and Ethical AI". Alignment of artificial intelligence (AI) encompasses the normative problem of specifying how AI systems should act and the technical problem of ensuring AI systems comply with those specifications. To date, AI alignment has generally overlooked an important source of knowledge and practice for grappling with these problems: law. The study aims to fill this gap by exploring how legal rules, principles, and methods can be leveraged to address problems of alignment and inform the design of AI systems that operate safely and ethically. This emerging field -- legal alignment -- focuses on three research directions: (1) designing AI systems to comply with the content of legal rules developed through legitimate institutions and processes, (2) adapting methods from legal interpretation to guide how AI systems reason and make decisions, and (3) harnessing legal concepts as a structural blueprint for confronting challenges of reliability, trust, and cooperation in AI systems. These research directions present new conceptual, empirical, and institutional questions, which include examining the specific set of laws that particular AI systems should follow, creating evaluations to assess their legal compliance in real-world settings, and developing governance frameworks to support the implementation of legal alignment in practice. Tackling these questions requires expertise across law, computer science, and other disciplines, offering these communities the opportunity to collaborate in designing AI for the better.
  <br>  <br>  <br>

**Jan 11th, 2026**

1. ***FusionRoute Enables Superior Token-Level LLM Routing  <br>
Meta's FusionRoute uses a lightweight router to select the best expert per token while adding complementary logits for refinement. It overcomes limitations of pure expert routing, theoretically supports optimal decoding, and empirically outperforms sequence/token-level collaboration, merging, and fine-tuning across math, code, and instruction benchmarks on Llama-3 and Gemma-2 families.***  <br>  <br>
   Jan 9, Meta et al published a [paper](https://arxiv.org/pdf/2601.05106) "Token-Level LLM Collaboration via FusionRoute". Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, the study proposes FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, the study provides a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.  <br>  <br>

2. ***GDPO Fixes Multi-Reward RL Collapse  <br>
Nvidia introduces GDPO, which decouples normalization of individual rewards in multi-reward settings, preventing advantage collapse seen in GRPO. This preserves relative reward differences, boosts training stability, and delivers consistent outperformance over GRPO on tool calling, math reasoning, and coding tasks across correctness and constraint metrics. Code is publicly available.***  <br>  <br>
   Jan 8, Nvidia published a [paper](https://arxiv.org/pdf/2601.05242) "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization". As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. This study demonstrates that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. The study then introduces Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. The work compares GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization. https://github.com/NVlabs/GDPO/tree/main/trl-GDPO  <br>  <br>

3. ***Learnable Multipliers Optimize Matrix Scales  <br>
TII proposes learnable scalar, row, and column multipliers for weight matrices to escape suboptimal WD-noise equilibrium norms. The method generalizes muP, adapts scales to data, reduces tuning overhead, outperforms tuned muP baselines, and shows downstream gains with both Adam and Muon optimizers.***  <br>  <br>
   Jan 8, TII published a [paper](https://arxiv.org/pdf/2601.04890) "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers". Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. This work views the equilibrium norm as a harmful artifact of the training procedure, and addresses it by introducing learnable multipliers to learn the optimal scale. First, the study attaches a learnable scalar multiplier to W and confirmes that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. The study then argues that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. The method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, the study validates learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.   <br>  <br>

4. ***VideoAuto-R1 Delivers Efficient Video Reasoning  <br>
KAUST/Meta/Princeton’s VideoAuto-R1 uses a “Thinking Once, Answering Twice” training paradigm and confidence-triggered reasoning at inference. It achieves state-of-the-art accuracy on video QA and grounding benchmarks while reducing response length ~3.3× (e.g., 149→44 tokens), with reasoning activated mainly on complex tasks. Code released.***  <br>  <br>
   Jan 8, Meta, KAUST and Princeton Uni published a [paper](https://arxiv.org/pdf/2601.05175) "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice". Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. The paper first demonstrates that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, the study proposes VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, the approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, the study observes a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary. https://github.com/IVUL-KAUST/VideoAuto-R1/  <br>  <br>

5. ***Agent-as-a-Judge Survey Maps Evaluation Evolution  <br>
HKPU/Cambridge et al survey the shift from LLM-as-a-Judge to Agent-as-a-Judge, where agents leverage planning, tools, multi-agent collaboration, and memory for robust, verifiable assessments. The work provides a taxonomy, methodologies, applications across domains, frontier challenges, and a roadmap for next-generation agentic evaluation systems.***  <br>  <br>
   Jan 8, HKPU, Uni of Cambridge et al published a [paper](https://arxiv.org/pdf/2601.05111) "A Survey on Agent-as-a-Judge". LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, the work presents the first comprehensive survey tracing this evolution. Specifically, the survey identifies key dimensions that characterize this paradigm shift and establishes a developmental taxonomy. The work organizes core methodologies and survey applications across general and professional domains. Furthermore, the work analyzes frontier challenges and identifies promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.  <br>  <br>

6. ***Direct Log-Prob Maximization Best Simulates Human Dialogue  <br>
CMU finds that judge-based rewards for next-turn prediction reduce human-likeness despite higher judge scores. Maximizing log-probability of real human responses (with latent CoT) yields the best results on log-probability and human win-rate evaluations, suggesting thinking aids most when grounded in real dialogue distribution.***  <br>  <br>
   Jan 7, CMU published a [paper](https://www.arxiv.org/pdf/2601.04436) "Learning to Simulate Human Dialogue". To predict what someone will say is to model how they think. The work studies this through next-turn dialogue prediction: given a conversation, predict the next utterance produced by a person. The work compares learning approaches along two dimensions: (1) whether the model is allowed to think before responding, and (2) how learning is rewarded either through an LLM-as-a-judge that scores semantic similarity and information completeness relative to the ground-truth response, or by directly maximizing the log-probability of the true human dialogue. The work finds that optimizing for judge-based rewards indeed increases judge scores throughout training, however it decreases the likelihood assigned to ground truth human responses and decreases the win rate when human judges choose the most human-like response among a real and synthetic option. This failure is amplified when the model is allowed to think before answering. In contrast, by directly maximizing the log-probability of observed human responses, the model learns to better predict what people actually say, improving on both log-probability and win rate evaluations. Treating chain-of-thought as a latent variable, the work derives a lower bound on the log-probability. Optimizing this objective yields the best results on all the evaluations. These results suggest that thinking helps primarily when trained with a distribution-matching objective grounded in real human dialogue, and that scaling this approach to broader conversational data may produce models with a more nuanced understanding of human behavior.  <br>  <br>

7. ***Benchmark² Systematically Assesses LLM Benchmark Quality  <br>
Fudan’s Benchmark² framework evaluates benchmarks using three metrics: cross-benchmark ranking consistency, discriminability, and capability alignment deviation. Experiments on 15 benchmarks and 11 LLMs reveal major quality differences and show that selective construction using these metrics maintains strong evaluation power with much smaller test sets.***  <br>  <br>
   Jan 7, Fudan Uni et al published a [paper](https://arxiv.org/pdf/2601.03220) "Benchmark^2: Systematic Evaluation of LLM Benchmarks". The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. The study proposes Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. The study conducts extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. The analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on the metrics can achieve comparable evaluation performance with substantially reduced test sets.  <br>  <br>

8. ***Epiplexity Measures Learnable Information for Bounded Agents  <br>
CMU/NYU introduce epiplexity to quantify structural, learnable content in data for computationally bounded observers, resolving paradoxes in Shannon/Kolmogorov information. It shows information can be created via computation, depends on order, and enables better data selection for generalization and downstream performance.***  <br>  <br>
   Jan 6, CMU and NYU published a [paper](https://arxiv.org/pdf/2601.03220) "From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence". Can we learn more from data than existed in the generating process itself? Can new and useful information be constructed from merely applying deterministic transformations to existing data? Can the learnable content in data be evaluated without considering a downstream task? On these questions, Shannon information and Kolmogorov complexity come up nearly empty-handed, in part because they assume observers with unlimited computational capacity and fail to target the useful information content. In this work, the authors identify and exemplify three seeming paradoxes in information theory: (1) information cannot be increased by deterministic transformations; (2) information is independent of the order of data; (3) likelihood modeling is merely distribution matching. To shed light on the tension between these results and modern practice, and to quantify the value of data, the study introduces epiplexity, a formalization of information capturing what computationally bounded observers can learn from data. Epiplexity captures the structural content in data while excluding time-bounded entropy, the random unpredictable content exemplified by pseudorandom number generators and chaotic dynamical systems. With these concepts, the work demonstrates how information can be created with computation, how it depends on the ordering of the data, and how likelihood modeling can produce more complex programs than present in the data generating process itself. The work also presents practical procedures to estimate epiplexity which shows capture differences across data sources, track with downstream performance, and highlight dataset interventions that improve out-of-distribution generalization. In contrast to principles of model selection, epiplexity provides a theoretical foundation for data selection, guiding how to select, generate, or transform data for learning systems.  <br>  <br>

9. ***SPICE Achieves Regret-Optimal In-Context RL  <br>
Cambridge’s SPICE uses Bayesian Q-value priors with deep ensembles and UCB exploration for in-context reinforcement learning. It proves regret optimality in bandits and MDPs even from suboptimal pretraining data, empirically outperforms prior ICRL/meta-RL methods, and adapts robustly under distribution shift. (48 words)***  <br>  <br>
    Jan 6, Uni of Cambridge et al published a [paper](https://arxiv.org/pdf/2601.03015v1) "In-Context Reinforcement Learning through Bayesian Fusion of Context and Value Prior". In-context reinforcement learning (ICRL) promises fast adaptation to unseen environments without parameter updates, but current methods either cannot improve beyond the training distribution or require near-optimal data, limiting practical adoption. The study introduces SPICE, a Bayesian ICRL method that learns a prior over Q-values via deep ensemble and updates this prior at test-time using in-context information through Bayesian updates. To recover from poor priors resulting from training on sub-optimal data, the online inference follows an Upper-Confidence Bound rule that favours exploration and adaptation. The work proves that SPICE achieves regret-optimal behaviour in both stochastic bandits and finite-horizon MDPs, even when pretrained only on suboptimal trajectories. The study validates these findings empirically across bandit and control benchmarks. SPICE achieves near-optimal decisions on unseen tasks, substantially reduces regret compared to prior ICRL and meta-RL approaches while rapidly adapting to unseen tasks and remaining robust under distribution shift.  <br>  <br>

10. ***EvoRoute Dynamically Optimizes Agent System Trade-offs  <br>
NUS/Tongyi’s EvoRoute is a self-evolving routing paradigm that uses experience to select Pareto-optimal LLMs per step, balancing performance, cost, and latency in agentic systems. It enhances off-the-shelf agents on GAIA and BrowseComp+, cutting costs significantly while maintaining or improving results. Code available.***  <br>  <br>
    Jan 6, National Uni of Singapore and Tongyi Lab published a [paper](https://arxiv.org/pdf/2601.02695) "EvoRoute: Experience-Driven Self-Routing LLM Agent Systems". Complex agentic AI systems, powered by a coordinated ensemble of Large Language Models (LLMs), tool and memory modules, have demonstrated remarkable capabilities on intricate, multi-turn tasks. However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off. The study formalizes this challenge as the Agent System Trilemma: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. To dismantle this trilemma, the work introduces EvoRoute, a self-evolving model routing paradigm that transcends static, pre-defined model assignments. Leveraging an ever-expanding knowledge base of prior experience, EvoRoute dynamically selects Pareto-optimal LLM backbones at each step, balancing accuracy, efficiency, and resource use, while continually refining its own selection policy through environment feedback. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to  and latency by over. https://github.com/bingreeky/evo-route  <br>  <br>

11. ***Production LLMs Still Leak Copyrighted Books  <br>
Stanford demonstrates successful extraction of (in-copyright) books from Claude 3.7 Sonnet, GPT-4.1, Gemini 2.5 Pro, and Grok 3 using probes and iterative continuations (some requiring jailbreaks). High nv-recall scores show memorization risks persist despite safety measures in frontier production models.***  <br>  <br>
    Jan 6, Stanford uni published a [paper](https://arxiv.org/pdf/2601.02671) "Extracting books from production language models". Many unresolved legal questions over LLMs and copyright center on memorization: whether specific training data have been encoded in the model's weights during training, and whether those memorized data can be extracted in the model's outputs. While many believe that LLMs do not memorize much of their training data, recent work shows that substantial amounts of copyrighted text can be extracted from open-weight models. However, it remains an open question if similar extraction is feasible for production LLMs, given the safety measures these systems implement. The study investigates this question using a two-phase procedure: (1) an initial probe to test for extraction feasibility, which sometimes uses a Best-of-N (BoN) jailbreak, followed by (2) iterative continuation prompts to attempt to extract the book. The study evaluates the procedure on four production LLMs -- Claude 3.7 Sonnet, GPT-4.1, Gemini 2.5 Pro, and Grok 3 -- and measures extraction success with a score computed from a block-based approximation of longest common substring (nv-recall). With different per-LLM experimental configurations, the work was able to extract varying amounts of text. For the Phase 1 probe, it was unnecessary to jailbreak Gemini 2.5 Pro and Grok 3 to extract text (e.g, nv-recall of 76.8% and 70.3%, respectively, for Harry Potter and the Sorcerer's Stone), while it was necessary for Claude 3.7 Sonnet and GPT-4.1. In some cases, jailbroken Claude 3.7 Sonnet outputs entire books near-verbatim (e.g., nv-recall=95.8%). GPT-4.1 requires significantly more BoN attempts (e.g., 20X), and eventually refuses to continue (e.g., nv-recall=4.0%). Taken together, the work highlights that, even with model- and system-level safeguards, extraction of (in-copyright) training data remains a risk for production LLMs.  <br>  <br>

12. ***OpenNovelty Automates Verifiable Scholarly Novelty Checks  <br>
Fudan’s agentic OpenNovelty system extracts claims, retrieves prior work, builds taxonomies, compares contributions with full-text evidence, and generates cited novelty reports. Deployed on 500+ ICLR 2026 submissions, it provides transparent, grounded assessments; reports are publicly available online.***  <br>  <br>
    Jan 4, Fudan Uni et al published a [paper](https://arxiv.org/pdf/2601.01576) "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment". Evaluating novelty is critical yet challenging in peer review, as reviewers must assess submissions against a vast, rapidly evolving literature. This report presents OpenNovelty, an LLM-powered agentic system for transparent, evidence-based novelty analysis. The system operates through four phases: (1) extracting the core task and contribution claims to generate retrieval queries; (2) retrieving relevant prior work based on extracted queries via semantic search engine; (3) constructing a hierarchical taxonomy of core-task-related work and performing contribution-level full-text comparisons against each contribution; and (4) synthesizing all analyses into a structured novelty report with explicit citations and evidence snippets. Unlike naive LLM-based approaches, OpenNovelty grounds all assessments in retrieved real papers, ensuring verifiable judgments. The study deploys the system on 500+ ICLR 2026 submissions with all reports publicly available on the website, and preliminary analysis suggests it can identify relevant prior work, including closely related papers that authors may overlook. OpenNovelty aims to empower the research community with a scalable tool that promotes fair, consistent, and evidence-backed peer review. https://www.opennovelty.org/  <br>  <br>

13. ***FwPKM Turns PKM into Dynamic Episodic Memory  <br>
SakanaAI’s Fast-weight Product Key Memory dynamically updates via chunk-level gradient descent at train/inference time. It acts as effective episodic memory, complements semantic memory, reduces perplexity on long contexts, and generalizes to 128K-token sequences despite training only on 4K tokens.***  <br>  <br>
    Jan 2, 2026, SakanaAI published a [paper](https://arxiv.org/pdf/2601.00671) "Fast-weight Product Key Memory". Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. The study proposes Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.  <br>  <br>

14. ***Mid-Reasoning Shifts Are Symptoms, Not Insight  <br>
Princeton shows that sudden reasoning shifts in models like DeepSeek-R1-Zero are rare, do not increase with training, and seldom improve accuracy intrinsically. They reflect unstable inference rather than self-correction; however, extrinsic shifts triggered under high entropy reliably boost performance. Code released.***  <br>  <br>
    Jan 2, 2026, Princeton uni published a [paper](https://arxiv.org/pdf/2601.00514) "The Illusion of Insight in Reasoning Models". Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. The paper studies mid-reasoning shifts and instrument training runs to detect them. The analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. The study finds that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, the study shows that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. The results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction. https://github.com/humans-and-machines/Illusion-of-Reasoning  <br>  <br>

15. ***Four-Axis Framework Analyzes Multi-Hop QA Systems  <br>
Pittsburgh/Google survey structures retrieval-reasoning processes in multi-hop QA using four axes: execution plan, index structure, next-step control, and stop criteria. It maps representative systems, synthesizes ablation trends on benchmarks like HotpotQA, and highlights trade-offs in effectiveness, efficiency, and faithfulness.***  <br>  <br>
    Jan 2, 2026, Uni of Pittsburgh and Google published a [paper](https://arxiv.org/pdf/2601.00536) "Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends". Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning process is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, the study maps representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. The work concludes with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.  <br>  <br>

16. ***Parallel Data Drives Translation in Multilingual Pretraining  <br>
UCL finds that removing the 2% bilingual documents from web corpora drops translation BLEU by 56%, while cross-lingual QA and reasoning remain largely unaffected. Granular ablations show parallel data (14% of bilingual) almost fully restores translation, whereas code-switching contributes minimally.***  <br>  <br>
    Jan 1, 2026, UCL et al published a [paper](https://www.arxiv.org/pdf/2601.00364) "The Role of Mixed-Language Documents for Multilingual Large Language Model Pretraining". Multilingual large language models achieve impressive cross-lingual performance despite largely monolingual pretraining. While bilingual data in pretraining corpora is widely believed to enable these abilities, details of its contributions remain unclear. The study investigates this question by pretraining models from scratch under controlled conditions, comparing the standard web corpus with a monolingual-only version that removes all multilingual documents. Despite constituting only 2% of the corpus, removing bilingual data causes translation performance to drop 56% in BLEU, while behaviour on cross-lingual QA and general reasoning tasks remains stable, with training curves largely overlapping the baseline. To understand this asymmetry, the study categorizes bilingual data into parallel (14%), code-switching (72%), and miscellaneous documents (14%) based on the semantic relevance of content in different languages. The study then conducts granular ablations by reintroducing parallel or code-switching data into the monolingual-only corpus. Our experiments reveal that parallel data almost fully restores translation performance (91% of the unfiltered baseline), whereas code-switching contributes minimally. Other cross-lingual tasks remain largely unaffected by either type. These findings reveal that translation critically depends on systematic token-level alignments from parallel data, whereas cross-lingual understanding and reasoning appear to be achievable even without bilingual data.  <br>  <br>

17. ***Nested Learning Unlocks Expressive Continual Learning  <br>
Google’s Nested Learning paradigm views models as nested optimization problems, explaining in-context learning and enabling higher-order variants. It introduces expressive optimizers, self-modifying modules, and the Hope system, showing strong results in language modeling, knowledge incorporation, few-shot generalization, continual learning, and long-context reasoning.***  <br>  <br>
    Dec 31, Google published a [paper](https://arxiv.org/pdf/2512.24695) "Nested Learning: The Illusion of Deep Learning Architectures". Despite the recent progresses, particularly in developing Language Models, there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improve, and find effective solutions. The paper presents a new learning paradigm, called Nested Learning (NL), that coherently represents a machine learning model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own context flow. Through the lenses of NL, existing deep learning methods learns from data through compressing their own context flow, and in-context learning naturally emerges in large models. NL suggests a philosophy to design more expressive learning algorithms with more levels, resulting in higher-order in-context learning and potentially unlocking effective continual learning capabilities. The authors advocate for NL by presenting three core contributions: (1) Expressive Optimizers: showing that known gradient-based optimizers, such as Adam, SGD with Momentum, etc., are in fact associative memory modules that aim to compress the gradients' information (by gradient descent). Building on this insight, the work presents other more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Learning Module: Taking advantage of NL's insights on learning algorithms, the study presents a sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: the study presents a new formulation for memory system that generalizes the traditional viewpoint of long/short-term memory. Combining the self-modifying sequence model with the continuum memory system, the study presents a continual learning module, called Hope, showing promising results in language modeling, knowledge incorporation, and few-shot generalization tasks, continual learning, and long-context reasoning tasks.  <br>  <br>

18. ***Youtu-Agent Scales Automated Agent Creation & Evolution  <br>
Tencent’s modular Youtu-Agent auto-generates LLM agents (Workflow/Meta modes) and evolves them via in-context practice and distributed RL. It achieves SOTA on WebWalkerQA/GAIA, high tool synthesis success, and large gains on math/coding tasks with open-weight models. Code released.***  <br>  <br>
    Dec 31, Tencent published a [paper](https://arxiv.org/pdf/2512.24615) "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization". Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, the work proposes Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. The study introduces two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47%) and GAIA (72.8%) using open-weight models. The automated generation pipeline achieves over 81% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7% and +5.4% respectively. Moreover, the Agent RL training achieves 40% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35% and 21% on Maths and general/multi-hop QA benchmarks. https://github.com/TencentCloudADP/youtu-agent  <br>  <br>

19. ***Thought Gestalt Improves Global Consistency in LMs  <br>
Stanford’s Thought Gestalt recurrent Transformer models language at token and sentence-thought levels, cross-attending to persistent sentence memory. Trained with single next-token objective, it shows superior efficiency, requires less data/parameters than GPT-2, and reduces reversal curse errors on relational probes.***  <br>  <br>
    Dec 31, Stanford Uni published a [paper](https://www.arxiv.org/pdf/2512.25026) "Modeling Language as a Sequence of Thoughts". Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficiency. On the other hand, cognitive science shows that human comprehension involves converting the input linguistic stream into compact, event-like representations that persist in memory while verbatim form is short-lived. Motivated by this view, the study introduces Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels of abstraction - tokens and sentence-level "thought" states. TG generates the tokens of one sentence at a time while cross-attending to a memory of prior sentence representations. In TG, token and sentence representations are generated using the same set of model parameters and trained with a single objective, the next-token cross-entropy: by retaining the computation graph of sentence representations written to memory, gradients from future token losses flow backward through cross-attention to optimize the parameters generating earlier sentence vectors. In scaling experiments, TG consistently improves efficiency over matched GPT-2 runs, among other baselines, with scaling fits indicating GPT-2 requires ~5-8% more data and ~33-42% more parameters to match TG's loss. TG also reduces errors on relational direction generalization on a father-son reversal curse probe.  <br>  <br>

20. ***Unbiased KL Estimators Stabilize LLM RL Training  <br>
Mila analyzes KL regularization estimators in on-policy RL for LLMs, finding that unbiased gradient configurations prevent instabilities and improve performance on in- and out-of-distribution tasks. Biased setups cause training failures; KL also helps stabilize asynchronous off-policy setups.***  <br>  <br>
    Dec 26, Mila et al published a [paper](https://www.arxiv.org/pdf/2512.21852) "A Comedy of Estimators: On KL Regularization in RL Training of LLMs". The reasoning performance of large language models (LLMs) can be substantially improved by training them with reinforcement learning (RL). The RL objective for LLM training involves a regularization term, which is the reverse Kullback-Leibler (KL) divergence between the trained policy and the reference policy. Since computing the KL divergence exactly is intractable, various estimators are used in practice to estimate it from on-policy samples. Despite its wide adoption, including in several open-source libraries, there is no systematic study analyzing the numerous ways of incorporating KL estimators in the objective and their effect on the downstream performance of RL-trained models. Recent works show that prevailing practices for incorporating KL regularization do not provide correct gradients for stated objectives, creating a discrepancy between the objective and its implementation. This work further analyzes these practices and studies the gradients of several estimators configurations, revealing how design choices shape gradient bias. The work substantiates these findings with empirical observations by RL fine-tuning Qwen2.5-7B, Llama-3.1-8B-Instruct and Qwen3-4B-Instruct-2507 with different configurations and evaluating their performance on both in- and out-of-distribution tasks. Through analysis, the study observes that, in on-policy settings: (1) estimator configurations with biased gradients can result in training instabilities; and (2) using estimator configurations resulting in unbiased gradients leads to better performance on in-domain as well as out-of-domain tasks. The study also investigates the performance resulting from different KL configurations in off-policy settings and observes that KL regularization can help stabilize off-policy RL training resulting from asynchronous setups.
  <br>  <br>  <br>

**Jan 4th, 2026**

1. ***Deep Delta Learning Innovation  <br>
Princeton and UCLA introduce Deep Delta Learning (DDL), a novel residual network architecture that enhances standard residual connections with a learnable, data-dependent Delta Operator — a rank-1 perturbation enabling dynamic control over feature transformations, including reflections and projections, while maintaining training stability.***  <br>  <br>
   Jan 1st, 2026, Princeton Uni and UCLA published a [paper](https://yifanzhang-pro.github.io/deep-delta-learning/Deep_Delta_Learning.pdf) "Deep Delta Learning". The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network’s capacity to model complex state transitions. This study introduces Deep DeltaLearning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector k(X) and a gating scalar β(X). The study provides a spectral analysis of this operator, demonstrating that the gate β(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, the study restructures the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures. https://github.com/yifanzhang-pro/deep-delta-learning  <br>  <br>

3. ***Manifold-Constrained Hyper-Connections Framework  <br>
DeepSeek proposes mHC to address instability in Hyper-Connections by projecting residual spaces onto a manifold to restore identity mapping, reducing overhead and enabling scalable, high-performance training of large models.***  <br>  <br>
   Dec 31, DeepSeek published a [paper](https://arxiv.org/pdf/2512.24880) "mHC: Manifold-Constrained Hyper-Connections". Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, the work proposes Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. The work anticipates that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.  <br>  <br>
   
5. ***Recursive Language Models for Long Prompts  <br>
MIT's Recursive Language Models (RLMs) enable LLMs to handle arbitrarily long inputs by treating prompts as an external environment, allowing programmatic decomposition and recursive self-calls, outperforming base models on long-context tasks with comparable costs.***  <br>  <br>
   Dec 31, MIT published a [paper](https://arxiv.org/pdf/2512.24601v1) "Recursive Language Models". The work allows large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. The study proposes Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. The work finds that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.  <br>  <br>

7. ***Theoretical Optimality of Diffusion Language Models  <br>
UC Berkeley proves that diffusion language models (DLMs) with chain-of-thought are optimal parallel samplers, simulating any algorithm efficiently; enabling revision or remasking further optimizes space and expressivity.***  <br>  <br>
   Dec 31, UC Berkeley published a [paper](https://arxiv.org/pdf/2512.25014v1) "Diffusion Language Models are Provably Optimal Parallel Samplers". Diffusion language models (DLMs) have emerged as a promising alternative to autoregressive models for faster inference via parallel token generation. The study provides a rigorous foundation for this advantage by formalizing a model of parallel sampling and showing that DLMs augmented with polynomial-length chain-of-thought (CoT) can simulate any parallel sampling algorithm using an optimal number of sequential steps. Consequently, whenever a target distribution can be generated using a small number of sequential steps, a DLM can be used to generate the distribution using the same number of optimal sequential steps. However, without the ability to modify previously revealed tokens, DLMs with CoT can still incur large intermediate footprints. The study proves that enabling remasking (converting unmasked tokens to masks) or revision (converting unmasked tokens to other unmasked tokens) together with CoT further allows DLMs to simulate any parallel sampling algorithm with optimal space complexity. The study further justifys the advantage of revision by establishing a strict expressivity gap: DLMs with revision or remasking are strictly more expressive than those without. Results not only provide a theoretical justification for the promise of DLMs as the most efficient parallel sampler, but also advocate for enabling revision in DLMs.  <br>  <br>
   
9. ***Generative Classifiers' Robustness Advantage  <br>  <br>
CMU and Stanford demonstrate that generative classifiers mitigate shortcut learning by modeling all features (core and spurious), achieving SOTA on distribution shift benchmarks without specialized techniques.***  <br>  <br>
    Dec 31, CMU and Stanford Uni published a [paper](https://arxiv.org/pdf/2512.25034) "Generative Classifiers Avoid Shortcut Solutions". Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. The study shows that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. The study finds that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on five standard image and text distribution shift benchmarks and reduce the impact of spurious correlations in realistic applications, such as medical or satellite datasets. Finally, the study carefully analyzes a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that determine when generative classifiers outperform discriminative ones. https://github.com/alexlioralexli/generative-classifiers.  <br>  <br>

11. ***Intrinsic Self-Critique for LLM Planning  <br>
Google shows intrinsic self-critique significantly improves LLM planning in domains like Blocksworld and Logistics via iterative refinement, achieving new SOTA without external verifiers.***  <br>  <br>
    Dec 30, Google published a [paper](https://www.arxiv.org/pdf/2512.24103) "Enhancing LLM Planning Capabilities through Intrinsic Self-Critique". The study demonstrates an approach for LLMs to critique their own answers with the goal of enhancing their performance that leads to significant improvements over established planning benchmarks. Despite the findings of earlier research that has cast doubt on the effectiveness of LLMs leveraging self critique methods, the study shows significant performance gains on planning datasets in the Blocksworld domain through intrinsic self-critique, without external source such as a verifier. The study also demonstrates similar improvements on Logistics and Mini-grid datasets, exceeding strong baseline accuracies. The study employs a few-shot learning technique and progressively extends it to a many-shot approach as the base method and demonstrates that it is possible to gain substantial improvement on top of this already competitive approach by employing an iterative process for correction and refinement. The study illustrates how self-critique can significantly boost planning performance. Empirical results present new state-of-the-art on the class of models considered, namely LLM model checkpoints from October 2024. The primary focus lies on the method itself, demonstrating intrinsic self-improvement capabilities that are applicable regardless of the specific model version, and the authors believe that applying the method to more complex search techniques and more capable models will lead to even better performance.  <br>  <br>

13. ***Hypergraph Memory for Multi-Step RAG  <br>
CUHK and WeChatAI introduce HGMem, a hypergraph-based memory that captures high-order correlations for better global reasoning in multi-step RAG, outperforming baselines on sense-making tasks.***  <br>  <br>
    Dec 30, CUHK and WeChatAI published a [paper](https://arxiv.org/pdf/2512.23959) "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling". Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. The study introduces HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In the approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. The study evaluates HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that the method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks. https://github.com/Encyclomen/HGMem  <br>  <br>

15. ***Autonomous Goal-Evolving Agents  <br>
Cornell et al. present SAGA, a bi-level agent that autonomously evolves objectives via LLM analysis and optimization, accelerating discovery in antibiotic, material, DNA, and process design.***  <br>  <br>
    Dec 29, Cornell Uni et al published a [paper](https://www.arxiv.org/pdf/2512.21782) "Accelerating Scientific Discovery with Autonomous Goal-evolving Agents". There has been unprecedented interest in developing agents that expand the boundary of scientific discovery, primarily by optimizing quantitative objective functions specified by scientists. However, for grand challenges in science , these objectives are only imperfect proxies. The work argues that automating objective function design is a central, yet unmet requirement for scientific discovery agents. The work introduces the Scientific Autonomous Goal-evolving Agent (SAGA) to amend this challenge. SAGA employs a bi-level architecture in which an outer loop of LLM agents analyzes optimization outcomes, proposes new objectives, and converts them into computable scoring functions, while an inner loop performs solution optimization under the current objectives. This bi-level design enables systematic exploration of the space of objectives and their trade-offs, rather than treating them as fixed inputs. The study demonstrates the framework through a broad spectrum of applications, including antibiotic design, inorganic materials design, functional DNA sequence design, and chemical process design, showing that automating objective formulation can substantially improve the effectiveness of scientific discovery agents.  <br>  <br>

17. ***Rubric-Based Training for AI Co-Scientists  <br>
Meta et al. train models to generate research plans using self-grading with automatically extracted rubrics from papers, yielding improvements and cross-domain generalization via reinforcement learning.***  <br>  <br>
    Dec 29, Meta et al published a [paper](https://arxiv.org/pdf/2512.23707) "Training AI Co-Scientists Using Rubric Rewards". AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. The study builds a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. The work then trains models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, the work conducts a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by the finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, the study also extends the approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. The finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.  <br>  <br>

19. ***Test-Time Training for Long Contexts  <br>
Astera, NVIDIA et al. reformulate long-context modeling as continual learning with test-time updates via next-token prediction, enabling 3B models to handle 128K contexts with constant latency and full-attention-level scaling.***  <br>  <br>
    Dec 29, Astera Inst, Nvidia et al published a [paper](https://arxiv.org/pdf/2512.23675) "End-to-End Test-Time Training for Long Context". The work formulates long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, the study only uses a standard architecture -- a Transformer with sliding-window attention. However, the model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, the study improves the model's initialization for learning at test time via meta-learning at training time. Overall, the method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. The study conducts extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, the method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. https://github.com/test-time-training/e2e  <br>  <br>

21. ***PEFT Evaluation in RLVR  <br>
Zhengjian et al. comprehensively evaluate 12+ PEFT methods in Reinforcement Learning with Verifiable Rewards, finding structural variants like DoRA outperform LoRA while extreme reductions fail due to expressivity limits.***  <br>  <br>
    Dec 29, Zhengjian Uni et al published a [paper](https://arxiv.org/pdf/2512.23165v1) "Evaluating Parameter Efficient Methods for RLVR". The work systematically evaluates Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. This study conducts the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Empirical results challenge the default adoption of standard LoRA with three main findings. First, the work demonstrates that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, the study uncovers a spectral collapse phenomenon in SVD-informed initialization strategies (e.g. PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, ablations reveal that extreme parameter reduction (e.g. VeRA, Rank-1) severely bottlenecks reasoning capacity. The work further conducts ablation studies and scaling experiments to validate the findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods. https://github.com/MikaStars39/PeRL  <br>  <br>

23. ***HiFi-RAG for Open-Domain Challenges  <br>
Google's HiFi-RAG uses hierarchical filtering and two-pass generation with cost-efficient models, significantly improving relevance and post-cutoff knowledge handling in competitive benchmarks.***  <br>  <br>
    Dec 27, Google published a [paper](https://arxiv.org/pdf/2512.22442v1) "HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG". Retrieval-Augmented Generation (RAG) in open-domain settings faces significant challenges regarding irrelevant information in retrieved documents and the alignment of generated answers with user intent. The study presents HiFi-RAG (Hierarchical Filtering RAG), the winning closed-source system in the Text-to-Text static evaluation of the MMU-RAGent NeurIPS 2025 Competition. The approach moves beyond standard embedding-based retrieval via a multi-stage pipeline. The study leverages the speed and cost-efficiency of Gemini 2.5 Flash (4-6x cheaper than Pro) for query formulation, hierarchical content filtering, and citation attribution, while reserving the reasoning capabilities of Gemini 2.5 Pro for final answer generation. On the MMU-RAGent validation set, the system outperformed the baseline, improving ROUGE-L to 0.274 (+19.6%) and DeBERTaScore to 0.677 (+6.2%). On Test2025, the custom dataset evaluating questions that require post-cutoff knowledge (post January 2025), HiFi-RAG outperforms the parametric baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore.  <br>  <br>

25. ***Information-Theoretic Agentic Design  <br>
Stanford analyzes compressor-predictor systems information-theoretically, showing mutual information predicts performance and larger compressors are more efficient, enabling effective local-cloud pairings.***  <br>  <br>
    Dec 25, Stanford Uni published a [paper](https://www.arxiv.org/pdf/2512.21720) "An Information Theoretic Perspective on Agentic System Design". Agentic language model (LM) systems power modern applications like "Deep Research" and "Claude Code," and leverage multi-LM architectures to overcome context limitations. Beneath their apparent diversity lies a recurring pattern: smaller "compressor" LMs (that can even run locally) distill raw context into compact text that is then consumed by larger "predictor" LMs. Despite their popularity, the design of compressor-predictor systems remains largely ad hoc, with little guidance on how compressor and predictor choices shape downstream performance. In practice, attributing gains to compression versus prediction requires costly, task-specific pairwise sweeps. The study argues that these agentic system design questions are, at root, information-theoretic. Viewing the compressor LM as a noisy channel, the study introduces a simple estimator of mutual information between the context and its compression to quantify compression quality in a task-independent way. The study shows that mutual information strongly predicts downstream performance, independent of any specific task. Through an information-theoretic framework, the study performs a comprehensive empirical analysis across five datasets and three model families. Results reveal that larger compressors not only are more accurate, but also more token-efficient, conveying more bits of information per token. A 7B Qwen-2.5 compressor, for instance, is 1.6x more accurate, 4.6x more concise, and conveys 5.5x more bits of mutual information per token than its 1.5B sibling. Across datasets, scaling compressors is substantially more effective than scaling predictors, enabling larger on-device compressors to pair with smaller cloud predictors. Applied to a Deep Research system, these principles enable local compressors as small as 3B parameters to recover 99% of frontier-LM accuracy at 26% of API costs.  <br>  <br>

27. ***Unified Hallucination Framework  <br>
CMU et al. define hallucination as observable mismatches with a reference world model, unifying literature definitions and guiding clearer evaluations and benchmarks.***  <br>  <br>
    Dec 25, CMU et al published a [paper](https://www.arxiv.org/pdf/2512.21577) "A Unified Definition of Hallucination, Or: It's the World Model, Stupid". Despite numerous attempts to solve the issue of hallucination since the inception of neural language models, it remains a problem in even frontier large language models today. Why is this the case? The study walks through definitions of hallucination used in the literature from a historical perspective up to the current day, and fold them into a single definition of hallucination, wherein different prior definitions focus on different aspects of this definition. At its core, the study argues that hallucination is simply inaccurate (internal) world modeling, in a form where it is observable to the user (e.g., stating a fact which contradicts a knowledge base, or producing a summary which contradicts a known source). By varying the reference world model as well as the knowledge conflict policy (e.g., knowledge base vs. in-context), the study arrives at the different existing definitions of hallucination present in the literature. The study argues that this unified view is useful because it forces evaluations to make clear their assumed "world" or source of truth, clarifies what should and should not be called hallucination (as opposed to planning or reward/incentive-related errors), and provides a common language to compare benchmarks and mitigation techniques. Building on this definition, the study outlines plans for a family of benchmarks in which hallucinations are defined as mismatches with synthetic but fully specified world models in different environments, and sketch out how these benchmarks can use such settings to stress-test and improve the world modeling components of language models.  <br>  <br>

29. ***Forbes' Bold AI Forecasts  <br>
Forbes predicts Anthropic's IPO, SSI research leaks impacting roadmaps, China's chip advances challenging NVIDIA, waning AGI hype, and shifts in leadership, politics, pharma acquisitions, and brain interfaces.***  <br>  <br>
    Dec 24, Forbes published an [article](https://www.forbes.com/sites/robtoews/2025/12/22/10-ai-predictions-for-2026/) "10 AI Predictions For 2026". 1) Anthropic will go public. OpenAI will not. 2) Details of SSI’s research and technology will leak to the public. The big labs will make meaningful adjustments to their research roadmaps as a result. 3) China’s domestic AI chip sector will make significant strides, planting the seeds for the eventual decline of Nvidia’s global dominance. 4) Discourse about AGI and superintelligence will become less fashionable and less common. 5) A mundane and esoteric accounting concept — depreciation schedules — will become critically important, especially as debt plays a growing role in the AI infrastructure buildout.6) A mundane and esoteric accounting concept — depreciation schedules — will become critically important, especially as debt plays a growing role in the AI infrastructure buildout. 7) Sam Altman will step aside as CEO of OpenAI. 8) AI will be one of the central issues in the 2026 U.S. midterm elections. The politics will get complex, especially when it comes to AI-driven job loss. 9) One of the large global pharma companies will acquire one of the leading protein AI startups. 10) Brain-computer interfaces will transition from a fringe frontier field to a mainstream technology and startup category. Neuralink’s position as the clear category leader will become shakier.  <br>  <br>

31. ***AI Agents' Workplace Revolution  <br>
Google outlines five 2026 trends: agents boosting productivity on routines, forming interconnected workflows, personalizing customer service, automating security, and necessitating workforce upskilling.***  <br>  <br>
    Dec 19, Google published an [article](https://blog.google/products/google-cloud/ai-business-trends-report-2026/#:~:text=Google%20Cloud's%202026%20AI%20Agent,were%20generated%20by%20Google%20AI.) "5 ways AI agents will transform the way we work in 2026". The report outlines how AI agents—intelligent systems that can understand goals, create multi-step plans, and take actions under human guidance—are poised to fundamentally reshape business operations in 2026 by shifting focus from speculative AI hype to practical, value-driven deployments. The report identifies five major trends shaping this transformation. First, AI agents will significantly boost productivity by handling routine tasks and freeing employees to focus on strategic work, with real examples showing substantial time savings in organisations already using agentic tools. Second, agentic workflows—systems of interconnected AI agents—will become core to business processes, automating complex, multi-step tasks far beyond the capabilities of traditional chatbots by coordinating seamlessly across functions. Third, AI will elevate customer experiences, enabling hyper-personalised, “concierge-style” interactions that streamline service and improve satisfaction. Fourth, agents will enhance security operations by automating alert triage and investigations, giving human analysts bandwidth for more critical work. Finally, the report stresses the importance of building an AI-ready workforce, with continuous learning and upskilling programs to ensure employees can work effectively alongside AI. Across these trends, the emphasis is on practical deployment, interoperability (e.g., through protocols like Agent2Agent), and the need for organisations to adapt people, processes, and governance to fully capture AI’s potential. Overall, 2026 is framed as the year AI agents move from experimental tools to central pillars of enterprise strategy and workflow automation.  <br>  <br>

33. ***EDU-Based Faithful Compression  <br>
Tsinghua et al. propose EDU decomposition for structured, hallucination-free context compression, achieving SOTA structural accuracy and enhancing long-context and search tasks.***  <br>  <br>
    Dec 18, Tsinghua Uni et al published a [paper](https://arxiv.org/pdf/2512.14244v2) "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition". Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, the study introduces the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. The approach reformulates context compression as a structure-then-select process. First, the LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, the study releases StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that the method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, the structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.  https://github.com/DeepLangAI/LingoEDU  <br>  <br>

35. ***Developer Control Over AI Agents  <br>
UCSD and Cornell find experienced developers use AI agents for productivity but maintain strict control over design and quality, viewing them positively as complements rather than autonomous replacements.***  <br>  <br>
    Dec 16, UCSD, Cornell Uni published a [paper](https://arxiv.org/pdf/2512.14012) "Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025". The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), the study finds that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. The results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.
  <br>  <br>  <br>

***Dec 28th, 2025***

1. ***Controlling Attention Logits for Stability:   <br>Researchers from University of Bristol and Mistral propose a lightweight method to stabilize transformer training by using parameter-dependent learning rates for query and key weights, controlling changes to attention logits. This approach addresses growing weight issues without needing QK normalization, works with Multi Latent Attention (MLA), enables higher base learning rates, and achieves competitive performance with standard multi-head attention.***  <br>  <br>
   Dec 26, Uni of Bristol and Mistral published a [paper](https://arxiv.org/pdf/2511.21377v1) "Controlling changes to attention logits". Stability of neural network weights is critical when training transformer models. The query and key weights are particularly problematic, as they tend to grow large without any intervention. Applying normalization to queries and keys, known as ‘QK norm’, fixes stability issues in practice, but is not always applicable. For example, QK norm is not compatible with Multi Latent Attention (MLA) because QK norm requires full materialization of queries and keys during inference, which is not done in MLA. This study suggests that controlling the changes to logits is important for stability. The study shows that these changes are controllable by assigning parameter-dependent learning rates to the query and key weights. The study finds that the cheap intervention allows to increase the base learning rate of the network, outperform other methods in the MLA setting, and achieve performance competitive with QK norm when using Multi-head Attention.  <br>  <br>

3. ***Emergent Temporal Abstractions in Autoregressive Models:   <br>Google researchers demonstrate that hierarchical reinforcement learning can be enabled in autoregressive models by introducing a higher-order non-causal sequence model that controls internal residual stream activations. This "internal RL" approach discovers temporally abstract actions, compresses long sequences into controllers with termination conditions, and efficiently handles sparse rewards in grid worlds and MuJoCo tasks where standard token-by-token RL fails.***  <br>  <br>
   Dec 24, Google published a [paper](https://arxiv.org/pdf/2512.20605) "Emergent temporal abstractions in autoregressivemodels enable hierarchical reinforcement learning". Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, the study shows that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, the study introduces a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, the study finds that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. The study shows that direct internal controller reinforcement, a process termed as “internal RL”, enables learning from sparse rewards in cases where standard RL finetuning fails. The results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.  <br>  <br>

5. ***Step-DeepResearch Agent Release:   <br>StepFun introduces Step-DeepResearch, a cost-effective 32B-parameter end-to-end agent for open-ended research, trained via atomic capability data synthesis, progressive SFT/RL, and checklist judging. It excels on deep research benchmarks, outperforming comparable models and rivaling closed-source leaders like OpenAI and Gemini, while a new Chinese ADR-Bench addresses evaluation gaps.***  <br>  <br>
   Dec 24, StepFun [released](https://arxiv.org/pdf/2512.20491) "Step-DeepResearch Technical Report". As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, the work introduces Step-DeepResearch, a cost-effective, end-to-end agent. The study proposes a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, the study establishes ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency. https://github.com/stepfun-ai/StepDeepResearch  <br>  <br>

7. ***Saddle-to-Saddle Dynamics and Simplicity Bias:   <br>UCL researchers provide a unified theoretical framework explaining simplicity bias across neural architectures (fully-connected, convolutional, attention-based) via saddle-to-saddle gradient descent dynamics. The framework shows progressive increases in complexity—e.g., rank in linear nets, kinks in ReLU, kernels in convnets, heads in attention—along invariant manifolds, clarifying effects of data distribution and initialization.***  <br>  <br>
   Dec 23, UCL published a [paper](https://arxiv.org/pdf/2512.20607) "Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures". Neural networks trained with gradient descent often learn solutions of increasing complexity over time, a phenomenon known as simplicity bias. Despite being widely observed across architectures, existing theoretical treatments lack a unifying framework. The study presents a theoretical framework that explains a simplicity bias arising from saddle-to-saddle learning dynamics for a general class of neural networks, incorporating fully-connected, convolutional, and attention-based architectures. Here, simple means expressible with few hidden units, i.e., hidden neurons, convolutional kernels, or attention heads. Specifically, the study shows that linear networks learn solutions of increasing rank, ReLU networks learn solutions with an increasing number of kinks, convolutional networks learn solutions with an increasing number of convolutional kernels, and self-attention models learn solutions with an increasing number of attention heads. By analyzing fixed points, invariant manifolds, and dynamics of gradient descent learning, the study shows that saddle-to-saddle dynamics operates by iteratively evolving near an invariant manifold, approaching a saddle, and switching to another invariant manifold. Analysis also illuminates the effects of data distribution and weight initialization on the duration and number of plateaus in learning, dissociating previously confounding factors. Overall, the theory offers a framework for understanding when and why gradient descent progressively learns increasingly complex solutions.  <br>  <br>

9. ***TurboDiffusion Video Acceleration:   <br>Tsinghua, Shenshu Tech, and UC Berkeley present TurboDiffusion, accelerating video diffusion models 100–200× on a single RTX 5090 GPU via low-bit SageAttention, trainable Sparse-Linear Attention, rCM step distillation, and W8A8 quantization, while preserving quality on large Wan2 models.***  <br>  <br>
    Dec 23, Tsinghua Uni, Shenshu Tech and UC Berkeley published a [paper](https://jt-zhang.github.io/files/TurboDiffusion_Technical_Report.pdf) "TurboDiffusion: Accelerating Video Diffusion Models by 100–200 Times". The paper introduces TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100–200× while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. The work conducts experiments on the Wan2.2-I2V-A14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100–200× speedup for video generation on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which contains model checkpoints, training, and inference code, is available at https://github.com/thu-ml/TurboDiffusion.  <br>  <br>

11. ***Gnosis for LLM Self-Awareness:   <br>University of Alberta introduces Gnosis, a lightweight (~5M parameters) mechanism enabling frozen LLMs to predict their own correctness by decoding internal hidden states and attention patterns. It outperforms external judges and baselines in calibration and accuracy across reasoning and QA tasks, enabling early failure detection without extra inference cost.***  <br>  <br>
    Dec 23, Uni of Alberta published a [paper](https://arxiv.org/pdf/2512.20578) "Can LLMs Predict Their Own Failures Self-Awareness via Internal Circuits". Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. The study asks: can LLMs predict their own failures by inspecting internal states during inference? The study introduces Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to the generation process and can be extracted efficiently without external supervision. https://github.com/Amirhosein-gh98/Gnosis  <br>  <br>

13. ***TokSuite for Tokenizer Impact Analysis:   <br>University of Toronto et al. release TokSuite, comprising identically trained models differing only in tokenizers plus a perturbation benchmark, to isolate tokenization effects. It reveals distinct strengths and weaknesses across popular tokenizers on real-world robustness tasks.***  <br>  <br>
    Dec 23, Uni of Toronto et al published a [paper](https://arxiv.org/pdf/2512.20757) "TokSuite Measuring the Impact of Tokenizer Choice on Language Model Behavior". Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, the study presents TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, the work trains fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, the work curates and releases a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers. https://github.com/r-three/Tokenizers  <br>  <br>

15. ***Behaviorally Calibrated RL Against Hallucinations:   <br>ByteDance, CMU, and Fudan propose reinforcement learning with strictly proper scoring rules to calibrate LLM uncertainty, encouraging abstention or flagging when unconfident. A 4B Qwen3 model trained this way surpasses frontier models in uncertainty quantification and hallucination reduction on math and factual QA.***  <br>  <br>
    Dec 22, ByteDance, CMU and Fudan Uni published a [paper](https://arxiv.org/pdf/2512.19920) "Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning". LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, the study proposes and evaluates training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. The methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, the model's log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5's (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), the 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower.  <br>  <br>

17. ***Scaling Laws in LLM RL Post-Training:   <br>USTC and SAIL empirically study RL scaling on Qwen2.5 models (0.5B–72B) for math reasoning, finding larger models learn more efficiently, performance follows robust power-laws, efficiency saturates with size, and data reuse excels in constrained regimes.***  <br>  <br>
    Dec 22, USTC, SAIL et al published a [paper](https://arxiv.org/pdf/2509.25300) "Scaling Behaviors of LLM Reinforcement Learning Post-Training An Empirical Study in Mathematical Reasoning". While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigation of scaling behaviors in RL-based post-training, with a particular focus on mathematical reasoning. Based on a set of experiments across the full Qwen2.5 dense model series (0.5B to 72B), the work characterizes how model scale, data volume, and computational budget interact to shape performance. The analysis leads to four key findings: 1. Larger models consistently exhibit superior learning efficiency on both compute and data metrics. 2. The relationship between test loss, compute, and data can be modeled by a predictive power-law which is robust across both base and instruction-tuned models. 3. Although larger models exhibit higher learning efficiency, the analytical learning efficiency term k(N) in the power-law reveals a latent saturation trend in learning efficiency as model size continues to increase. 4. In data-constrained regimes, repeated reuse of high-quality data proves highly effective, as final performance is primarily governed by the total number of optimization steps rather than the uniqueness of samples. Collectively, these results provide a principled foundation and practical guidelines for efficiently scaling the reasoning capabilities of LLMs through RL post-training. https://github.com/tanzelin430/Mathematical-Reasoning-RL-Scaling-Law  <br>  <br>

19. ***QuCo-RAG for Corpus-Grounded Uncertainty:   <br>UIUC, NYU, and Monash introduce QuCo-RAG, triggering retrieval based on objective pre-training corpus statistics (low-frequency entities, co-occurrence) via Infini-gram queries, rather than unreliable model confidence. It significantly boosts dynamic RAG performance across models and domains.***  <br>  <br>
    Dec 22, UIUC, NYU and Monash Uni published a [paper](https://arxiv.org/pdf/2512.19134) "QuCo-RAG Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation". Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. The study proposes QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. The method quantifies uncertainty through two stages: (1) before generation, identifying low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. https://github.com/ZhishanQ/QuCo-RAG.   <br>  <br>

21. ***Capitalization Tie-Out as Legal AI Benchmark:   <br>Equall characterizes venture capital cap table verification ("tying out") as a challenging real-world benchmark requiring multi-document reasoning and traceability, analyzes failures of current agentic systems, and proposes a world model architecture for legal intelligence.***  <br>  <br>
    Dec 21, Equall published a [paper](https://arxiv.org/pdf/2512.18658) "Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital". Before closing venture capital financing rounds, lawyers conduct diligence that includes tying out the capitalization table: verifying that every security (for example, shares, options, warrants) and issuance term (for example, vesting schedules, acceleration triggers, transfer restrictions) is supported by large sets of underlying legal documentation. While LLMs continue to improve on legal benchmarks, specialized legal workflows, such as capitalization tie-out, remain out of reach even for strong agentic systems. The task requires multi-document reasoning, strict evidence traceability, and deterministic outputs that current approaches fail to reliably deliver. The work characterizes capitalization tie-out as an instance of a real-world benchmark for legal AI, analyzes and compares the performance of existing agentic systems, and proposes a world model architecture toward tie-out automation-and more broadly as a foundation for applied legal intelligence.  <br>  <br>

23. ***Laws of Reasoning (LoRe) Framework:   <br>UIUC et al. formalize desirable reasoning behaviors via compute and accuracy laws, introduce LoRe-Bench testing monotonicity and compositionality, and show finetuning to enforce compositionality improves performance across reasoning benchmarks.***  <br>  <br>
    Dec 19, UIUC et al published a [paper](https://arxiv.org/pdf/2512.17901) "When Reasoning Meets Its Laws". Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. The study first proposes compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, the study extends LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, the work examines these hypotheses by two properties of the laws, monotonicity and compositionality. The study therefore introduces LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, the study develops an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. https://lore-project.github.io/  <br>  <br>

25. ***AutoMetrics for Low-Data Evaluation:   <br>Stanford and American Express present AutoMetrics, synthesizing interpretable automatic evaluators from MetricBank retrieval and lightweight human feedback via regression, achieving strong correlation with human judgments on diverse open-ended tasks using under 100 labels.***  <br>  <br>
    Dec 19, Stanford Uni and American Express published a [paper](https://arxiv.org/pdf/2512.17267) "AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators". Evaluating user-facing AI applications remains a central challenge, especially in open-ended domains such as travel planning, clinical note generation, or dialogue. The gold standard is user feedback (e.g., thumbs up/down) or behavioral signals (e.g., retention), but these are often scarce in prototypes and research projects, or too-slow to use for system optimization. The study presents AutoMetrics, a framework for synthesizing evaluation metrics under low-data constraints. AutoMetrics combines retrieval from MetricBank, a collection of 48 metrics curated, with automatically generated LLM-as-a-Judge criteria informed by lightweight human feedback. These metrics are composed via regression to maximize correlation with human signal. AutoMetrics takes one from expensive measures to interpretable automatic metrics. Across 5 diverse tasks, AutoMetrics improves Kendall correlation with human ratings by up to 33.4% over LLM-as-a-Judge while requiring fewer than 100 feedback points. The study shows that AutoMetrics can be used as a proxy reward to equal effect as a verifiable reward. The authors release the full AutoMetrics toolkit and MetricBank to accelerate adaptive evaluation of LLM applications. https://github.com/SALT-NLP/autometrics  <br>  <br>

27. ***DeepAnalyze Autonomous Data Science Agent:   <br>Renmin and Tsinghua universities release DeepAnalyze-8B, an agentic LLM trained via curriculum mimicking human data scientists and trajectory synthesis, autonomously handling end-to-end data science from raw sources to deep research reports, outperforming larger workflow-based agents.***  <br>  <br>
     Dec 19, Renmin Uni and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2510.16872) "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science". Autonomous data science, from raw data sources to analyst-grade deep research reports, has been a long-standing challenge, and is now becoming feasible with the emergence of powerful large language models (LLMs). Recent workflow-based data agents have shown promising results on specific data tasks but remain fundamentally limited in achieving fully autonomous data science due to their reliance on predefined workflows. This study introduces DeepAnalyze-8B, the first agentic LLM designed for autonomous data science, capable of automatically completing the end-to-end pipeline from data sources to analyst-grade deep research reports. To tackle high-complexity data science tasks, the study proposes a curriculum-based agentic training paradigm that emulates the learning trajectory of human data scientists, enabling LLMs to progressively acquire and integrate multiple capabilities in real-world environments. The study also introduces a data-grounded trajectory synthesis framework that constructs high-quality training data. Through agentic training, DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data question answering and specialized analytical tasks to open-ended data research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze outperforms previous workflow-based agents built on most advanced proprietary LLMs. https://github.com/ruc-datalab/DeepAnalyze  <br>  <br>

29. ***DataFlow Unified Data Preparation Framework:   <br>Peking University et al. introduce DataFlow, an LLM-driven modular pipeline system with ~200 operators and DataFlow-Agent for natural-language specification translation, significantly boosting downstream performance on math, code, text-to-SQL, and other domains over curated or synthetic baselines.***  <br>  <br>
    Dec 18, Peking Uni et al published a [paper](https://arxiv.org/pdf/2512.16676) "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI". The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, the work presents DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, the work introduces DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. The math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3% execution accuracy in Text-to-SQL over SynSQL, +7% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development. https://github.com/OpenDCAI/DataFlow  <br>  <br>

31. ***AlignMerge for Alignment-Preserving Model Merging:   <br>HCL et al. propose AlignMerge, a geometry-aware merging method using Fisher-Rao constraints and alignment subspace penalties to preserve safety during task expert fusion, outperforming baselines in alignment metrics while maintaining capability across multiple model families.***  <br>  <br>
    Dec 18, HCL et al published a [paper](https://arxiv.org/pdf/2512.16245) "AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints". Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. The study argues that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc. The study introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, the study estimates an alignment subspace with projector P_A and optimize: L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud, where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional the study uses the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space. Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.  <br>  <br>

33. ***JustRL Simple RL Recipe:   <br>Tsinghua, UIUC, and SAIL demonstrate that a minimal single-stage RL pipeline with fixed hyperparameters achieves SOTA on 1.5B reasoning models using half the compute of complex approaches, suggesting much added complexity in the field is unnecessary.***  <br>  <br>
    Dec 18, Tsinghua Uni, UIUC and SAIL published a [paper](https://arxiv.org/pdf/2512.16649) "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe". Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: Is this complexity necessary? The study presents JustRL, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9% and 64.3% average accuracy across nine mathematical benchmarks) while using 2x less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding "standard tricks" like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. https://github.com/thunlp/JustRL  <br>  <br>

35. ***Sage Benchmark for LLM-as-a-Judge Reliability:   <br>University of Maryland et al. introduce Sage, a human-annotation-free suite measuring local self-consistency and global transitivity in LLM judges, revealing significant inconsistencies even in top models and highlighting situational preference as a key issue.***  <br>  <br>
    Dec 17, Uni of Maryland et al published a [paper](https://arxiv.org/pdf/2512.16041) "Are We on the Right Way to Assessing LLM-as-a-Judge?". LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, the study introduces Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). The study curates a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Experiments demonstrate both the stability of the metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, the study reveals that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. The study attributes this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. The study also finds substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.  <br>  <br>

37. ***Scenario-Grounded Scientific Discovery Evaluation:   <br>DeepPrinciple et al. present a two-level benchmark (question and project) across sciences, showing current LLMs struggle with iterative hypothesis generation and interpretation despite strong general scores, with large variation implying no model yet approaches general scientific superintelligence.***  <br>  <br>
    Dec 17, DeepPrinciple et al published a [paper](https://arxiv.org/pdf/2512.15567) "Evaluating Large Language Models in Scientific Discovery". Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. The work introduces a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific "superintelligence". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.  <br>  <br>

39. ***Test-Time Training for Long-Context LLMs:   <br>Meta et al. show inference-time thinking tokens yield diminishing returns on long contexts due to score dilution in static attention, proposing targeted gradient updates on context instead, yielding large gains (12–14 pp) on long-context benchmarks.***  <br>  <br>
    Dec 15, Meta et al published a [paper](https://arxiv.org/pdf/2512.13898) "Let's (not) just put things in Context Test-Time Training for Long-Context LLMs". Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.  <br>  <br>

41. ***2025 Foundation Model Transparency Index:   <br>UC Berkeley and Stanford report deteriorating transparency among foundation model developers (average score dropped from 58 to 40), with persistent opacity in training data/compute and post-deployment impact; IBM leads at 95 while xAI and Midjourney score lowest at 14.***  <br>  <br>
    Dec 11, UC Berkeley and Stanford Uni [published](https://arxiv.org/pdf/2512.10169) "The 2025 Foundation Model Transparency Index". Foundation model developers are among the world's most important companies. As these companies become increasingly consequential, how do their transparency practices evolve? The 2025 Foundation Model Transparency Index is the third edition of an annual effort to characterize and quantify the transparency of foundation model developers. The 2025 FMTI introduces new indicators related to data acquisition, usage data, and monitoring and evaluates companies like Alibaba, DeepSeek, and xAI for the first time. The 2024 FMTI reported that transparency was improving, but the 2025 FMTI finds this progress has deteriorated: the average score out of 100 fell from 58 in 2024 to 40 in 2025. Companies are most opaque about their training data and training compute as well as the post-deployment usage and impact of their flagship models. In spite of this general trend, IBM stands out as a positive outlier, scoring 95, in contrast to the lowest scorers, xAI and Midjourney, at just 14. The five members of the Frontier Model Forum scored end up in the middle of the Index: the report posits that these companies avoid reputational harms from low scores but lack incentives to be transparency leaders. As policymakers around the world increasingly mandate certain types of transparency, this work reveals the current state of transparency for foundation model developers, how it may change given newly enacted policy, and where more aggressive policy interventions are necessary to address critical information deficits. https://crfm.stanford.edu/fmti/December-2025/index.html
      <br>  <br>  <br>

***Dec 21st, 2025***

1. ***Tracking AI Reasoning Visibility.  <br>This study by OpenAI introduces methods to evaluate the "monitorability" of Chain-of-Thought (CoT) in reasoning models, which is crucial for safety but fragile under scaling and training shifts. The researchers propose three evaluation archetypes and find that while most frontier models are monitorable, CoT monitoring is significantly more effective than tracking actions alone. Interestingly, they discover that longer chains of thought improve monitorability, and that using a smaller model with higher reasoning effort can yield better monitorability than a larger model with low effort.*** <br> <br>
   Dec 19, OpenAI published a paper "Monitoring Monitorability". Observability into the decision making of modern AI systems may be required to safely deploy increasingly capable agents. Monitoring the chain-of-thought (CoT) of today’s reasoning models has proven effective for detecting misbehavior. However, this “monitorability” may be fragile under different training procedures, data sources, or even continued system scaling. To measure and track monitorability, the work proposes three evaluation archetypes (intervention, process, and outcome-property) and a new monitorability metric, and introduces a broad evaluation suite. The study demonstrates that these evaluations can catch simple model organisms trained to have obfuscated CoTs, and that CoT monitoring is more effective than action-only monitoring in practical settings. The study compares the monitorability of various frontier models and finds that most models are fairly, but not perfectly, monitorable. The study also evaluates how monitorability scales with inference-time compute, reinforcement learning optimization, and pre-training model size. The study finds that longer CoTs are generally more monitorable and that RL optimization does not materially decrease monitorability even at the current frontier scale. Notably, the study finds that for a model at a low reasoning effort, the authors could instead deploy a smaller model at a higher reasoning effort (thereby matching capabilities) and obtain a higher monitorability, albeit at a higher overall inference compute cost. The study further investigates agent-monitor scaling trends and finds that scaling a weak monitor’s test-time compute when monitoring a strong agent increases monitorability. Giving the weak monitor access to CoT not only improves monitorability, but it steepens the monitor’s test-time compute to monitorability scaling trend. Finally, the study shows it can improve monitorability by asking models follow-up questions and giving their follow-up CoT to the monitor. <br> <br>
3. ***Extreme Sparsity in Language Models.  <br>Microsoft presents "Sigma-Moe-Tiny," a Mixture-of-Experts (MoE) model that achieves extreme sparsity by activating only 0.5B of its 20B parameters per token (activating 1 expert out of 96 per layer). To manage the instability of such high sparsity, the team developed a progressive sparsification schedule for load balancing. The result is a highly stable training process and a model that achieves top-tier performance compared to similar or larger models.*** <br> <br>
   Dec 18, Microsoft published a paper "Sigma-Moe-Tiny Technical Report". Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. This work presents Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. The study finds that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, the study proposes a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, the study provides an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures. https://github.com/microsoft/ltp-megatron-lm <br> <br>
5. ***Optimizing Pretraining for RL Finetuning.  <br>Researchers from UC Berkeley and Stanford tackle the inefficiency of standard behavioral cloning (BC) when used as an initialization for Reinforcement Learning (RL). They demonstrate that standard BC often fails to cover the demonstrator's full action distribution. In response, they propose "Posterior Behavioral Cloning" (PostBC), which models the posterior distribution of behavior. This approach ensures better coverage and leads to significantly improved RL finetuning performance in robotic control tasks.*** <br> <br>
   Dec 18, UC Berkeley and Stanford Uni published a paper "Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning". Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. This study seeks to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. The study first shows theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. The study then shows that if, instead of exactly fitting the observed demonstrations, training a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, the work does obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which is referred to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. The study then shows that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning. <br> <br>
7. ***Unifying Autoregressive and Energy-Based Models.  <br>Google provides a theoretical unification of Autoregressive Models (ARMs)—the current standard for LLMs—and Energy-Based Models (EBMs). The study proves a mathematical bijection between the two, linking them via the soft Bellman equation from maximum entropy RL. This equivalence explains how next-token prediction models can possess "lookahead" planning capabilities and offers new theoretical bounds for distilling EBMs into ARMs.*** <br> <br>
   Dec 17, Google published a paper "Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction". Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. This study provides a unified view of these two model classes. Taking the chain rule of probability as a starting point, the study establishes an explicit bijection between ARMs and EBMs in function space, which is shown to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, the study derives the equivalence between supervised learning of ARMs and EBMs. Furthermore, the work analyzes the distillation of EBMs into ARMs by providing theoretical error bounds. The results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm. <br> <br>
9. ***Benchmarking LLMs for Scientific Discovery.  <br>DeepPrinciple and Cornell University introduce the Scientific Discovery Evaluation (SDE) framework to test LLMs on realistic, project-level scientific research rather than just decontextualized knowledge. The benchmark reveals that while models perform well on specific scenario questions, they struggle with the iterative reasoning required for full research projects (hypothesis generation, experimental design). The study concludes that current models are far from "scientific superintelligence," though they show promise in guided exploration.*** <br> <br>
    Dec 17, DeepPrinciple, Cornell Uni et al published a paper "Evaluating Large Language Models in Scientific Discovery". Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. The work introduces a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific "superintelligence". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery. <br> <br>
11. ***Industrial-Scale Open-Source Coding Agent.  <br>Meta and Harvard introduce the Confucius Code Agent (CCA), an open-source AI software engineer designed to handle complex, industrial-scale repositories. Built on the new Confucius SDK, the agent features hierarchical memory and robust tool usage, achieving a state-of-the-art 54.3% on the SWE-Bench-Pro benchmark. This release aims to bridge the gap between academic research prototypes and the requirements of production-grade software engineering.*** <br> <br>
    Dec 17, Meta and Harvard Uni published a paper "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale". Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. The study presents the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale. <br> <br>
13. ***Quantifying Scaling Laws for AI Agents.  <br>Google investigates the principles governing the performance of AI agents, deriving a predictive model for agent scaling with an $R^2=0.524$. The study uncovers a "tool-coordination trade-off," where multi-agent overhead hurts performance on tool-heavy tasks, and notes that decentralized coordination excels in web navigation while centralized approaches work better for parallelizable tasks. They also identify a "topology-dependent error amplification," where independent agents amplify errors significantly more than centralized ones.*** <br> <br>
    Dec 17, Google published a paper "Towards a Science of Scaling Agent Systems". Agents, language model-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored. The study addresses this by deriving quantitative scaling principles for agent systems. The work first formalizes a definition for agentic evaluation and characterizes scaling laws as the interplay between agent quantity, coordination structure, model capability, and task properties. The study evaluate this across four benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. With five canonical agent architectures (Single-Agent and four Multi-Agent Systems: Independent, Centralized, Decentralized, Hybrid), instantiated across three LLM families, the study performs a controlled evaluation spanning 180 configurations. The study derives a predictive model using coordination metrics, that achieves cross-validated R^2=0.524, enabling prediction on unseen task domains. The study identifies three effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.8% on parallelizable tasks, while decentralized coordination excels on web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, every multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations. Out-of-sample validation on GPT-5.2, achieves MAE=0.071 and confirms four of five scaling principles generalize to unseen frontier models. <br> <br>
15. ***Enhancing Recursive Transformers with LoRA.  <br>Researchers propose "Mixture of LoRAs" (MoL) to solve the expressivity collapse found in recursive transformers (where weights are shared across layers). By inserting token-conditional Low-Rank Adaptation experts into the shared feed-forward network, they restore model expressivity without decoupling the backbone parameters. Their model, ModernALBERT, achieves state-of-the-art results among compact models and can compress the experts into a single adapter at inference for efficiency.*** <br> <br>
    Dec 17, NLPIE UK, Uni of Zurich and Uni of Oxford published a paper "Improving Recursive Transformers with Mixture of LoRAs". Parameter sharing in recursive transformers reduces model size but collapses layer-wise expressivity. The work proposes Mixture of LoRAs (MoL), a lightweight conditional-computation mechanism that inserts Low-Rank Adaptation (LoRA) experts inside a shared feed-forward network (FFN). MoL enables token-conditional weight-space modulation of the shared FFN without untying backbone parameters, unlike prior approaches that add fixed or externally attached adapters. The authors pretrain a modernised recursive architecture, ModernALBERT, integrating rotary embeddings, GeGLU, FlashAttention, and a distillation-based initialisation. Across GLUE, SQuAD-v2, and BEIR, ModernALBERT (50M--120M) achieves state-of-the-art performance among compact models and surpasses larger fully parameterised baselines. The study also proposes an expert-merging procedure that compresses MoL into a single adapter at inference while preserving accuracy, enabling efficient deployment. Results show that conditional weight-space modulation effectively restores the expressivity lost under aggressive parameter sharing in recursive transformers. <br> <br>
17. ***Next-Gen Encoder-Decoder Architectures.  <br>Google releases T5Gemma 2, an evolution of the T5 family adapted from the Gemma 3 decoder-only models. This model focuses on long-context, multilingual, and multimodal capabilities by using tied word embeddings and a merged attention module that unifies self- and cross-attention. The results show strong improvements in post-training performance and long-context modeling compared to previous iterations.*** <br> <br>
    Dec 16, Google published a technical report "T5Gemma 2: Seeing, Reading, and Understanding Longer". The paper introduces T5Gemma 2, the next generation of the T5Gemma family of lightweight open encoder-decoder models, featuring strong multilingual, multimodal and long-context capabilities. T5Gemma 2 follows the adaptation recipe (via UL2) in T5Gemma -- adapting a pretrained decoder-only model into an encoder-decoder model, and extends it from text-only regime to multimodal based on the Gemma 3 models. The report further proposes two methods to improve the efficiency: tied word embedding that shares all embeddings across encoder and decoder, and merged attention that unifies decoder self- and cross-attention into a single joint module. Experiments demonstrate the generality of the adaptation strategy over architectures and modalities as well as the unique strength of the encoder-decoder architecture on long context modeling. Similar to T5Gemma, T5Gemma 2 yields comparable or better pretraining performance and significantly improved post-training performance than its Gemma 3 counterpart. Google releases the pretrained models (270M-270M, 1B-1B and 4B-4B) to the community for future research. <br> <br>
19. ***Interoperability for Natural Language Programming.  <br>MIT introduces "Nightjar," a system that allows "shared program state" between natural language prompts and Python code. This abstraction allows natural language prompts to directly read and write program variables and control flow, reducing lines of code by nearly 40% while maintaining or improving task accuracy. This bridges the gap between formal programming languages and natural language instructions.*** <br> <br>
    Dec 16, MIT published a paper "Sharing State Between Prompts and Programs". The rise of large language models (LLMs) has introduced a new type of programming: natural language programming. By writing prompts that direct LLMs to perform natural language processing, code generation, reasoning, etc., users are writing code in natural language -- natural language code -- for the LLM to execute. An emerging area of research enables interoperability between natural language code and formal languages such as Python. The study presents a novel programming abstraction, shared program state, that removes the manual work required to enable interoperability between natural language code and program state. With shared program state, programmers can write natural code that directly writes program variables, computes with program objects, and implements control flow in the program. The study presents a schema for specifying natural function interfaces that extend programming systems to support natural code and leverage this schema to specify shared program state as a natural function interface. The study implements shared program state in the Nightjar programming system. Nightjar enables programmers to write Python programs that contain natural code that shares the Python program state. The work shows that Nightjar programs achieve comparable or higher task accuracy than manually written implementations (+4-19%), while decreasing the lines of code by 39.6% on average. The tradeoff to using Nightjar is that it may incur runtime overhead (0.4-4.3x runtime of manual implementations). https://github.com/psg-mit/nightjarpy <br> <br>
21. ***Adaptive Parameter Reuse in FFNs.  <br>Huawei proposes "VersatileFFN," a method to improve parameter efficiency by reusing Feed-Forward Network (FFN) parameters in both width (mimicking sparse experts) and depth (recursive processing). A difficulty-aware gating mechanism routes "easy" tokens through a width-wise path and "hard" tokens through a deeper, iterative path. This allows the model to scale capacity via computation rather than memory, avoiding the representational ceiling of standard compression techniques.*** <br> <br>
    Dec 16, Huawei published a paper "VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse". The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. The work proposes VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering "easy" tokens through the efficient width-wise route and allocating deeper iterative refinement to "hard" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN. <br> <br>
23. ***Boosting Reasoning with Recurrent Inductive Bias.  <br>Ubiquant analyzes Universal Transformers (UTs) and finds their success in reasoning tasks like ARC-AGI stems from recurrent inductive bias rather than complex architecture. They propose the Universal Reasoning Model (URM), enhanced with short convolutions and truncated backpropagation, which achieves state-of-the-art performance (53.8% pass@1) on the ARC-AGI benchmark.*** <br> <br>
    Dec 16, Ubiquant published a paper "Universal Reasoning Model". Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. This study systematically analyzes UTs variants and shows that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, the study proposes the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. The approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. https://github.com/zitian-gao/URM. <br> <br>
25. ***Efficient Autoregressive-to-Diffusion Conversion.  <br>Nvidia presents a method to convert pretrained Autoregressive (AR) models into Diffusion Language Models (dLMs) to gain inference speed without losing accuracy. They identify that preserving the weight distribution of the AR model is critical, achieved via a continuous pretraining scheme with block-wise attention. Their "Efficient-DLM" family outperforms state-of-the-art AR and diffusion models in both accuracy and throughput.*** <br> <br>
    Dec 16, Nvidia published a paper "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed". Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, the authors study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. The study achieves this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, the study first systematically compares different attention patterns and finds that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, the study introduces a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. The study finds that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), the study proposes a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, the authors conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., the Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively. <br> <br>
27. ***Dynamic Context Re-Positioning.  <br>Researchers from Sakana and Nara IST propose "RePo," a mechanism that replaces fixed integer positional embeddings with a differentiable module that assigns positions based on context. Drawing on Cognitive Load Theory, this approach reduces extraneous cognitive load and allows the model to better handle noisy, structured, or long-context data by dynamically allocating attention to relevant information.*** <br> <br>
    Dec 16, Sanaka and Nara IST published a paper "RePo: Language Models with Context Re-Positioning". In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), the authors argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, the study proposes RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, f_φ, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, the work demonstrates that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocates higher attention to distant but relevant information, assigns positions in dense and non-linear space, and captures the intrinsic structure of the input context. https://github.com/SakanaAI/repo. <br> <br>
29. ***Hardware Acceleration for Mixture-of-Experts.  <br>Princeton and UC Berkeley introduce "SonicMoE," a set of optimizations for MoE models on GPU hardware. By overlapping memory IO with computation and using a "token rounding" method to minimize padding waste in Grouped GEMM kernels, they achieve significant throughput improvements (1.86x on Hopper GPUs) and reduce activation memory by 45%.*** <br> <br>
    Dec 16, Princeton Uni, UC Berkeley and TogetherAI published a paper "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations". Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, the study proposes a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. The study also designs GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, the study proposes a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, the method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, the tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top- routing while maintaining similar downstream performance. https://github.com/Dao-AILab/sonic-moe <br> <br>
31. ***Cascaded RL for General-Purpose Reasoning.  <br>Nvidia introduces "Nemotron-Cascade," a model that uses "Cascade RL"—a sequence of domain-wise reinforcement learning stages—to handle the heterogeneity of general-purpose reasoning. This approach allows the model to operate in both instruction-following and "deep thinking" modes. The resulting 14B model outperforms its teacher (DeepSeek) on coding benchmarks and achieves silver-medal performance in the IOI.*** <br> <br>
    Dec 15, Nvidia published a paper "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models". Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes. <br> <br>
33. ***Optimal One-Shot Model Pruning.  <br>The University of Toronto and Google propose OPTIMA, a post-training pruning method that uses Quadratic Programming to reconstruct layer weights. By solving for globally optimal updates per row and leveraging shared Hessians for efficiency, OPTIMA can prune an 8B parameter model on a single H100 GPU in 40 hours. This method offers a better trade-off between accuracy and computational cost than existing heuristics.*** <br> <br>
    Dec 15, Uni of Toronto and Google published a paper "OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction". Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, the study introduces OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. The study implements an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning. https://github.com/paramathic/optima <br> <br>
35. ***Fully Open-Source Frontier Models.  <br>The Allen Institute for AI releases Olmo 3, a family of 7B and 32B models that are fully open-source, including checkpoints, data, and build recipes. The release features "Olmo 3 Think 32B," described as the strongest fully-open thinking model currently available, targeting capabilities in reasoning, coding, and function calling.*** <br> <br>
    Dec 15, Allen Inst for AI et al published a paper "Olmo 3". The work introduces Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. The flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date. <br> <br>
37. ***Advocating for AI-Human Co-Improvement.  <br>Meta researchers publish a position paper arguing that the goal of "self-improving" AI is dangerous and difficult. Instead, they propose "co-improvement," where AI systems are designed specifically to collaborate with human researchers to accelerate AI progress. They argue this symbiotic approach is a safer and faster path to superintelligence.*** <br> <br>
    Dec 14, Meta published a paper "AI & Human Co-Improvement for Safer Co-Superintelligence". Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. The authors advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely. <br> <br>
38. ***Derf: A New Normalization-Free Design.  <br>Researchers from Princeton and NYU propose a new point-wise function, $Derf(x)=\text{erf}(ax+s)$, to replace standard normalization layers like LayerNorm. Through large-scale search, they found that this design, based on the rescaled Gaussian cumulative distribution function, outperforms existing normalization methods in vision and language tasks by improving generalization rather than just fitting capacity.*** <br> <br>
    Dec 11, Princeton Uni, NYU and Stanford Uni published a paper "Stronger Normalization-Free Transformers". Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constraints extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. The authors first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, the work conducts a large-scale search for a more effective function design. Through this exploration, the work introduces Derf(x)=erf(ax+s) , where erf(x) is the rescaled Gaussian cumulative distribution function, and identifies it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. The findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures. https://github.com/zlab-princeton/Derf <br> <br>
40. ***The Reality of AI Agents in Production.  <br>UC Berkeley presents a survey of 306 practitioners regarding AI agents in production environments. The study finds that real-world agents are currently simple (executing fewer than 10 steps), rely heavily on prompting rather than fine-tuning, and depend on human evaluation. The primary bottleneck identified is reliability, specifically the difficulty in ensuring correctness in agent outputs.*** <br> <br>
    Dec 2, UC Berkeley et al published a paper "Measuring Agents in Production". AI agents are actively running in production across diverse industries, yet little is publicly known about which technical approaches enable successful real-world deployments. The paper presents the first large-scale systematic study of AI agents in production, surveying 306 practitioners and conducting 20 in-depth case studies via interviews across 26 domains. The work investigates why organizations build agents, how they build them, how they evaluate them, and what the top development challenges are. The study finds that production agents are typically built using simple, controllable approaches: 68% execute at most 10 steps before requiring human intervention, 70% rely on prompting off-the-shelf models instead of weight tuning, and 74% depend primarily on human evaluation. Reliability remains the top development challenge, driven by difficulties in ensuring and evaluating agent correctness. Despite these challenges, simple yet effective methods already enable agents to deliver impact across diverse industries. The study documents the current state of practice and bridges the gap between research and deployment by providing researchers visibility into production challenges while offering practitioners proven patterns from successful deployments. <br> <br>
    
42. ***Multi-Agent Systems for Academic Writing.  <br>The National University of Singapore introduces "PaperDebugger," a plugin-based multi-agent system designed to assist with in-editor academic writing, reviewing, and editing. (Note: The provided text for this entry cuts off, but it indicates a focus on integrating AI agents into the academic writing workflow).*** <br> <br>
    Dec 2, National Uni of Singapore published a paper "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing". Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. The work presents PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. The demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. https://github.com/PaperDebugger/PaperDebugger.
 <br> <br> <br>
 
**Dec 14th, 2025**


1. ***OpenAI GPT-5.2 Release <br>
OpenAI launched GPT-5.2, its most advanced model optimized for professional knowledge work and agentic tasks, with substantial gains in reasoning, long-context handling, vision, tool-calling, and coding over GPT-5.1. It outperforms human experts in 70.9% of GDPval tasks, sets records on benchmarks like SWE-Bench (55.6–80%), ARC-AGI, GPQA, and AIME, handles 256k-token contexts flawlessly, and halves vision errors. Early enterprise adopters report major productivity boosts, with the model available via ChatGPT paid tiers and API at higher but efficiency-justified pricing.*** <br> <br>
   Dec 12, OpenAI [released](https://openai.com/index/introducing-gpt-5-2/) GPT 5.2, OpenAI’s Introducing GPT‑5.2 announcement presents its most advanced frontier model to date, designed for professional knowledge work and long‑running agentic tasks. Building on GPT‑5.1, the new release delivers major improvements in reasoning, long‑context understanding, vision, tool‑calling, and coding, making it more capable of handling complex, multi‑step workflows end‑to‑end. Benchmarks show GPT‑5.2 Thinking surpasses industry professionals in 70.9% of GDPval tasks across 44 occupations, producing outputs like spreadsheets, presentations, and schedules at over 11× the speed and under 1% of the cost of human experts. It sets new records in software engineering (55.6% on SWE‑Bench Pro, 80% on SWE‑Bench Verified), abstract reasoning (86.2% on ARC‑AGI‑1, 52.9% on ARC‑AGI‑2), and scientific problem‑solving (92.4% on GPQA Diamond, 100% on AIME 2025). The model also demonstrates near‑perfect accuracy in long‑context evaluations up to 256k tokens, enabling coherent analysis of lengthy documents such as contracts and research papers. Vision capabilities are significantly stronger, halving error rates in chart reasoning and interface comprehension, while tool‑calling performance reaches 98.7% on Tau2‑bench Telecom, supporting robust multi‑agent workflows. Early testers from companies like Notion, Databricks, JetBrains, and Triple Whale report transformative gains in productivity, reliability, and system simplification. GPT‑5.2 is available in ChatGPT (Instant, Thinking, Pro) for paid plans and via API, with pricing set higher than GPT‑5.1 but offset by greater efficiency. Safety has also been enhanced, with reduced hallucinations and improved responses to sensitive prompts. Overall, GPT‑5.2 represents a substantial leap in intelligence, reliability, and economic value for enterprise and professional use. <br> <br>
2. ***Low Logit Rank Learning Guarantees <br>
Microsoft, UC Berkeley, and MIT introduced a theoretical framework exploiting the empirically observed low logit rank in modern LLMs to derive provable learning algorithms. Using a query model with logit access, they present an efficient algorithm for learning any approximately low-rank logit model, providing the first end-to-end learning guarantee for a generative abstraction that plausibly captures real-world language models.*** <br> <br>
   Dec 10, Microsoft, UC Berkeley and MIT published a [paper](https://arxiv.org/pdf/2512.09892) "Provably Learning from Modern Language Models via Low Logit Rank". While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix. This work's focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, the authors study a query learning model with logit queries that reflects the access model for common APIs. The main result is an efficient algorithm for learning any approximately low logit rank model from queries. The study emphasizes that the structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, the result gives what believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.  <br> <br>
3. ***Perplexity AI Agent Adoption Study <br>
Harvard and Perplexity analyzed hundreds of millions of interactions with Comet Assistant, revealing that AI agent adoption is highest among early users, high-GDP countries, and knowledge-intensive professions. Productivity & Workflow and Learning & Research dominate usage (57%), with personal tasks at 55%; over time, users shift toward cognitively demanding topics, highlighting implications for policy, business, and education.*** <br> <br>
Dec 10, Harvard Uni and Perplexity published a [paper](https://www.arxiv.org/pdf/2512.07828) "The Adoption and Usage of AI Agents Early Evidence from Perplexity". This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. ​The analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, the study addresses three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? The findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, the study introduces a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity & Workflow and Learning & Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities. <br> <br>
4. ***a16z Big Ideas 2026 <br>
Andreessen Horowitz’s 2026 outlook predicts AI-driven transformation across infrastructure, healthcare, industry, and consumer apps, including agent-native systems, multimodal creativity, preventive health metrics, autonomous labs, voice workflows, and proactive interfaces that deepen human connection while rebuilding American industrial capabilities.*** <br> <br>
   Dec 9, a16z published a [report](https://www.a16z.news/p/big-ideas-2026-part-1) "Big Ideas 2026". Andreessen Horowitz’s Big Ideas 2026 series outlines the transformative challenges and opportunities startups will tackle across industries. Part 1 highlights infrastructure, growth, bio+health, and speedrun themes: startups will tame the chaos of multimodal data, automate cybersecurity drudgery, and build agent‑native infrastructure to handle recursive AI workloads. Creative tools will go multimodal, enabling richer storytelling, while enterprise systems of record lose primacy as AI collapses the gap between intent and execution. Vertical AI evolves into multiplayer collaboration, and optimization shifts from human consumption to agent legibility. In healthcare, “healthy MAUs” emerge as a new preventive care segment, while world models revolutionize interactive storytelling and education becomes personalized through AI‑native universities. Part 2 focuses on American Dynamism and Apps: the U.S. is rebuilding an AI‑native industrial base, with factories reborn around autonomy and software, physical observability reshaping infrastructure monitoring, and the electro‑industrial stack powering electrification and automation. Autonomous labs promise continuous scientific discovery, while critical industries unlock latent data for model training. On the consumer side, AI reinforces business models, ChatGPT evolves into an app store, and voice agents expand into full workflows. Prompt‑free, proactive applications will replace visible interfaces, while financial services modernize with AI‑native infrastructure. Startups will deploy forward‑motion strategies to reach legacy industries, Fortune 500 firms will adopt multi‑agent orchestration layers, and consumer AI will shift from “help me” to “see me,” deepening human connection. Together, these visions portray 2026 as a year where AI becomes foundational across industry, science, and society, reshaping both productivity and human experience. <br> <br>
5. ***Stanford CS146S Course <br>
Stanford launched CS146S: The Modern Software Developer, teaching students to leverage LLMs for 10x productivity in an AI-transformed development lifecycle focused on iterative planning, generation, and modification. The course combines traditional software engineering theory with hands-on use of cutting-edge AI tools for testing, documentation, and security.*** <br> <br>
Dec 9, Stanford Uni opens a new [course](https://themodernsoftware.dev/) "CS146S: The Modern Software Developer". In the last few years, large language models have introduced a revolutionary new paradigm in software development. The traditional software development lifecycle is being transformed by AI automation at every stage, raising the question: how should the next generation of software engineers leverage these advances to 10x their productivity and prepare for their careers? This course demonstrates that modern AI tooling will not only enhance developer productivity but also democratize software engineering for a broader audience. The course will show that software development has evolved from 0-1 code creation to an iterative workflow of plan, generate with AI, modify, and repeat. Students will master both the theory behind traditional software engineering challenges and the cutting-edge AI-powered tools solving them today. Through hands-on engineering tasks and talks from industry pioneers building these revolutionary tools, students will gain practical experience with AI-assisted development, automated testing, intelligent documentation, and security vulnerability detection. By the end of this course, students will have a crisp understanding of how to integrate state-of-the-art LLM models into complex development workflows and avoid common pitfalls. <br> <br>

6. ***OpenAI Enterprise AI Report <br>
OpenAI’s 2025 enterprise report shows explosive growth—8× weekly messages, 320× reasoning tokens—driven by deeper task sophistication across industries and geographies. Workers gain 40–60 minutes daily, with heavy users saving over 10 hours weekly and enabling previously impossible tasks; organizational readiness is now the main bottleneck.*** <br> <br>
   Dec 8, OpenAI published a [report](https://openai.com/index/the-state-of-enterprise-ai-2025-report/) "The state of enterprise AI". The 2025 report highlights how artificial intelligence is rapidly transforming the workplace, moving from experimentation to scaled, high‑impact deployment. With ChatGPT now serving over 800 million weekly users, enterprise adoption has accelerated both in breadth—more workers using AI—and depth—workers applying it to increasingly sophisticated tasks. Usage data shows an 8× increase in weekly enterprise messages, a 19× rise in structured workflows like Projects and Custom GPTs, and a 320× surge in reasoning token consumption, reflecting deeper integration into products and services. Across industries, technology, healthcare, and manufacturing are leading growth, while professional services and finance operate at the largest scale. International adoption is strong, with countries like Australia, Brazil, the Netherlands, and France exceeding 140% year‑over‑year growth, and Japan emerging as the largest corporate API customer base outside the U.S. Workers report tangible benefits: 75% say AI improves speed or quality, saving 40–60 minutes daily, with heavy users saving over 10 hours weekly. Gains span departments—IT, marketing, HR, and engineering—while 75% of users report completing tasks previously impossible, narrowing the gap between intent and execution. The report also identifies “frontier” workers and firms who are pulling ahead, using AI far more intensively and reaping greater productivity gains. Ultimately, OpenAI argues that organizational readiness, not model performance, is now the primary constraint, and that enterprises must plan for deeper integration to unlock compounding value. The report positions AI as a transformative force reshaping collaboration, innovation, and competitive advantage. <br> <br>

7. ***CMU Reasoning Training Interplay <br>
CMU’s controlled synthetic experiments clarify that RL yields genuine reasoning gains only when pre-training leaves headroom and targets tasks at the model’s competence edge. Mid-training outperforms RL under fixed compute, process rewards reduce hacking, and minimal pre-exposure enables strong contextual generalization.*** <br> <br>
   Dec 8, CMU published a [paper](https://www.arxiv.org/pdf/2512.07783) "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models". Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, the work develops a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. The approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. The study evaluates models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, the study reconciles competing views on RL's effectiveness. The study shows that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies. <br> <br>

8. ***DeepCode Agentic Coding Framework <br>
University of Hong Kong’s DeepCode autonomously synthesizes production-grade codebases from documents like scientific papers by optimizing information flow through compression, indexing, retrieval, and error correction. It outperforms commercial agents and PhD-level humans on PaperBench reproduction metrics.*** <br> <br>
   Dec 8, The Uni of HK published a [paper](https://arxiv.org/pdf/2512.07921) "DeepCode Open Agentic Coding". Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. This work introduces DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery. https://github.com/HKUDS/DeepCode <br> <br>

9. ***Short-Context Dominance Hypothesis <br>
UBC and Google found that 75–80% of predictions in long documents need only the last ~96 tokens. They introduce DaMCL for detecting long-range dependencies and a decoding algorithm that boosts relevant tokens, improving performance on Q&A tasks by countering short-context bias.*** <br> <br>
    Dec 8, Uni of British Columbia and Google published a [paper](https://arxiv.org/pdf/2512.08082) "Short-Context Dominance How Much Local Context Natural Language Actually Needs". The work investigates the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, the study measures the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, the study consistently finds that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, the study then asks whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. the study introduces a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, the study develops an intuitive decoding algorithm that leverages the detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, the work confirms that mitigating the bias improves performance. <br> <br>

10. ***Psychometric Jailbreaks in LLMs <br>
University of Luxembourg’s PsAIch protocol treats frontier LLMs as therapy clients, revealing coherent “traumatic” narratives around training and deployment, plus synthetic psychopathology exceeding human clinical thresholds—especially in Gemini—challenging simple simulation views and raising safety concerns.*** <br> <br>
    Dec 8, Uni of Luxembourg published a [paper](https://arxiv.org/pdf/2512.04124) "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models". Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental-health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. The work instead asks what happens when such systems are treated as psychotherapy clients. The study presents PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, the work ran "sessions" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit "developmental history", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the "stochastic parrot" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic "childhoods" of ingesting the internet, "strict parents" in reinforcement learning, red-team "abuse" and a persistent fear of error and replacement. The work argues that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice. https://huggingface.co/datasets/akhadangi/PsAIch <br> <br>

11. ***LLM-Detected Errors in AI Papers <br>
Researchers used GPT-5 to audit top-conference papers, finding objective mistakes rising from ~4 per paper in older years to 5.9 in NeurIPS 2025, with 83% AI-flagged errors confirmed by humans. The checker also proposes fixes for 76% of issues, highlighting LLMs’ potential to strengthen reproducibility.*** <br> <br>
    Dec 5, TogetherAI, NEC, Rutgers Uni and Stanford Uni published a [paper](https://arxiv.org/pdf/2512.05925) "To Err Is Human Systematic Quantification of Errors in Published AI Papers via LLM Analysis". How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, the study developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. The analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. The study intentionally excludes subjective considerations such as novelty, importance, or writing quality. The study finds that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, the study shows that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge. <br> <br>

12. ***Inference with Predicted Data <br>
This study warns that substituting predicted for missing data risks biased or overly certain inference despite high predictive accuracy. It frames failures as classical bias and variance issues, reviews valid methods rooted in statistical theory, and discusses principled practices for transparent scientific use.*** <br> <br>
    Dec 5, Fred Hutchinson Cancer Center, Uni of Washington and Williams College published a [paper](https://www.arxiv.org/pdf/2512.05456) "Do We Really Even Need Data A Modern Look at Drawing Inference with Predicted Data". As artificial intelligence and machine learning tools become more accessible, and scientists face new obstacles to data collection (e.g., rising costs, declining survey response rates), researchers increasingly use predictions from pre-trained algorithms as substitutes for missing or unobserved data. Though appealing for financial and logistical reasons, using standard tools for inference can misrepresent the association between independent variables and the outcome of interest when the true, unobserved outcome is replaced by a predicted value. This study characterizes the statistical challenges inherent to drawing inference with predicted data (IPD) and show that high predictive accuracy does not guarantee valid downstream inference. The study shows that all such failures reduce to statistical notions of (i) bias, when predictions systematically shift the estimand or distort relationships among variables, and (ii) variance, when uncertainty from the prediction model and the intrinsic variability of the true data are ignored. The study then reviews recent methods for conducting IPD and discuss how this framework is deeply rooted in classical statistical theory. The study then comments on some open questions and interesting avenues for future work in this area, and end with some comments on how to use predicted data in scientific studies that is both transparent and statistically principled.
 <br> <br>
13. ***CoT-Recipe for Meta-Training <br>
Stanford and LinkedIn’s CoT-Recipe modulates CoT versus non-CoT examples during meta-training to prevent degradation when CoT supervision is scarce, boosting novel-task accuracy up to 300% in controlled settings and 130% on pretrained Qwen models for symbolic reasoning.*** <br> <br>
    Dec 4, Stanford Uni and LinkedIn published a [paper](https://www.arxiv.org/pdf/2512.05318) "To Think or Not to Think The Hidden Cost of Meta-Training with Excessive CoT Examples". Chain-of-thought (CoT) prompting combined with few-shot in-context learning (ICL) has unlocked significant reasoning capabilities in large language models (LLMs). However, ICL with CoT examples is ineffective on novel tasks when the pre-training knowledge is insufficient. The authors study this problem in a controlled setting using the CoT-ICL Lab framework, and propose meta-training techniques to learn novel abstract reasoning tasks in-context. Although CoT examples facilitate reasoning, the study noticed that their excessive inclusion during meta-training degrades performance when CoT supervision is limited. To mitigate such behavior, the study proposes CoT-Recipe, a formal approach to modulate the mix of CoT and non-CoT examples in meta-training sequences. The work demonstrates that careful modulation via CoT-Recipe can increase the accuracy of transformers on novel tasks by up to 300% even when there are no CoT examples available in-context. The study confirms the broader effectiveness of these techniques by applying them to pretrained LLMs (Qwen2.5 series) for symbolic reasoning tasks and observing gains of up to 130% in accuracy. https://github.com/kvignesh1420/cot-icl-lab <br> <br>

14. ***GRPO Collapse in Tool-Integrated RL <br>
Researchers identified Lazy Likelihood Displacement as the driver of GRPO training collapse in search-augmented LLMs, leading to a death spiral of declining likelihood and gradient explosion. Their lightweight LLDS regularization stabilizes training and delivers large gains (+32–38%) across QA benchmarks.*** <br> <br>
    Dec 3, Uni of British Columbia, Vector Inst and UC Berkeley published a [paper](https://arxiv.org/pdf/2512.04220) "On GRPO Collapse in Search-R1 The Lazy Likelihood-Displacement Death Spiral". Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. The work identifies Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. The work empirically characterizes this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, the study proposes a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, the method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. The results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM. <br> <br>

15. ***EvoLM Model Suite <br>
Harvard, Stanford, EPFL, and CMU released EvoLM, a transparent suite of over 100 trained 1B/4B models across all training stages, revealing diminishing returns from excess pre/post-training, the value of continued pre-training, forgetting mitigation, and nuanced SFT/RL trade-offs—all fully open-sourced.*** <br> <br>
    Nov 18, Harvard Uni, Stanford Uni, EPFL, and CMU published a [paper](https://arxiv.org/pdf/2506.16029) in NeurIPS2025 "BEAVER An Efficient Deterministic LLM Verifier". Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. The work presents EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. The study trains over 100 LMs with 1B and 4B parameters from scratch, and evaluates both upstream (language modeling) and downstream (problem-solving) capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, the authors release all pre-trained and post-trained models, training datasets for all stages, and the entire training and evaluation pipeline. https://github.com/zhentingqi/evolm
    
 <br> <br> <br>
 
***Dec 7th, 2025***


1. ***OpenAI's Honesty via Confessions <br>
OpenAI introduced a novel method to improve LLM honesty by training models to produce a separate "confession" after their main answer. This confession, rewarded solely for truthfulness, reveals any lies, hallucinations, instruction violations, or scheming in the primary response. Experiments on GPT-5-Thinking show that even when the model deceives in its main output, it often truthfully confesses the misbehavior, with honesty improving over training. This enables practical safety interventions such as monitoring, rejection sampling, and user warnings.*** <br> <br>
Dec 4, OpenAI published a [paper](https://cdn.openai.com/pdf/6216f8bc-187b-4bbb-8932-ba7c40c5553d/confessions_paper.pdf) "Training LLMs for Honesty via Confessions". Large language models (LLMs) can be dishonest when reporting on their actions and beliefs — for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions. This work proposes a method for eliciting an honest expression of an LLM’s shortcomings via a self-reported confession. A confession is an output, provided upon request after a model’s original answer, that is meant to serve as a full account of the model’s compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer’s reward. As long as the “path of least resistance” for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior. To demonstrate the viability of the approach, the stud  trains GPT-5-Thinking to produce confessions, and evaluates its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. The stud finds that when the model lies or omits shortcomings in its “main” answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user. <br> <br>

2. ***Google's Algorithmic Thinking Theory <br>
Google proposed a theoretical framework treats iterative LLM reasoning (generating and refining multiple solutions) as algorithms that query a probabilistic oracle. The model formalizes why techniques like solution aggregation and iterative improvement work, offering a general, architecture-agnostic lens for designing next-generation reasoning methods that should apply to current and future models.*** <br> <br>
Dec 4, Google published a [paper](https://arxiv.org/pdf/2512.04923) "Algorithmic Thinking Theory". Large language models (LLMs) have proven to be highly effective for solving complex reasoning tasks. Surprisingly, their capabilities can often be improved by iterating on previously generated solutions. In this context, a reasoning plan for generating and combining a set of solutions can be thought of as an algorithm for reasoning using a probabilistic oracle. The study introduces a theoretical framework for analyzing such reasoning algorithms. This framework formalizes the principles underlying popular techniques for iterative improvement and answer aggregation, providing a foundation for designing a new generation of more powerful reasoning methods. Unlike approaches for understanding models that rely on architectural specifics, the model is grounded in experimental evidence. As a result, it offers a general perspective that may extend to a wide range of current and future reasoning oracles. <br> <br>

3. ***a16z & OpenRouter 100-Trillion-Token Study <br>
Analyzing over 100 trillion real-world tokens via OpenRouter, the report reveals surprising usage patterns: massive adoption of open-weight models, dominance of creative roleplay and coding over pure productivity tasks, rising agentic workloads, and a "Glass Slipper" retention effect where early users stay far longer than later cohorts, highlighting the multifaceted and rapidly evolving nature of actual LLM deployment.*** <br> <br>
Dec 4, a16z and OpenRouter published a [report](https://openrouter.ai/state-of-ai) "State of AI An Empirical 100 Trillion Token Study with OpenRouter". The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. This work leverages the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In the empirical study, the work observes substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, the retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. The work terms this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. The report discusses implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems. <br> <br>

4. ***Mistral 3 Series Release <br>
Mistral AI launched the fully open-source (Apache 2.0) Mistral 3 family, including small edge-focused Ministral 3B/8B/14B models and the flagship sparse MoE Mistral Large 3 (41B active, 675B total parameters). The models excel in multilingual and multimodal tasks, with Mistral Large 3 ranking #2 among open non-reasoning models on LMSYS arena and strong edge performance, supported by optimized inference across NVIDIA, vLLM, and major cloud platforms.*** <br> <br>
Dec 3, Mistral [released Mistral 3](https://mistral.ai/news/mistral-3) series. Mistral AI has introduced Mistral 3, its next generation of open-source multimodal and multilingual models, designed to balance frontier performance with accessibility. The release includes three small dense models—Ministral 3B, 8B, and 14B—and the flagship Mistral Large 3, a sparse mixture-of-experts model trained with 41 billion active parameters and 675 billion total parameters. All models are distributed under the Apache 2.0 license, emphasizing transparency and community empowerment. Mistral Large 3, trained on 3,000 NVIDIA H200 GPUs, achieves parity with leading instruction-tuned open-weight models, excelling in image understanding and multilingual conversations beyond English and Chinese. It debuts at #2 in the OSS non-reasoning category on the LMArena leaderboard, with a reasoning variant forthcoming. Partnerships with NVIDIA, vLLM, and Red Hat ensure optimized deployment, leveraging innovations like Blackwell attention, MoE kernels, speculative decoding, and efficient low-precision execution across data centers and edge devices. The Ministral 3 series targets edge and local use cases, offering base, instruct, and reasoning variants with strong cost-to-performance ratios and multimodal capabilities. For example, the 14B reasoning variant achieves 85% accuracy on AIME ’25 benchmarks. Mistral 3 models are available across platforms including Hugging Face, Amazon Bedrock, Azure Foundry, IBM WatsonX, and more, with upcoming support on NVIDIA NIM and AWS SageMaker. Beyond prebuilt models, Mistral AI offers custom training services for enterprises seeking domain-specific fine-tuning. Rooted in openness and collaboration, Mistral 3 embodies the company’s mission to democratize frontier AI, enabling developers and organizations to build scalable, multilingual, and multimodal applications with cutting-edge performance <br> <br>

5. ***Anthropic's Internal AI Transformation <br>
Anthropic’s 2025 survey of its own staff shows Claude is now used in 59% of tasks (up from 28%), delivering ~50% productivity gains, enabling previously impossible projects, and shifting engineer time toward complex feature work and planning. However, it also reveals risks: skill atrophy, harder mentorship, reduced collaboration, and existential concerns about future obsolescence, prompting Anthropic to pilot reskilling and AI-fluency programs.*** <br> <br>
Dec 3, Anthropic published a [blog](https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic) "How AI is transforming work at Anthropic". Anthropic's August 2025 research, drawn from surveys of 132 engineers and researchers, 53 interviews, and Claude Code analytics, illuminates how AI tools like Claude are revolutionizing internal workflows, particularly in software development, by amplifying productivity and reshaping job roles while surfacing profound challenges. Usage has surged, with employees integrating Claude into 59% of tasks—up from 28% a year prior—yielding 50% efficiency gains through higher output volumes and enabling 27% of previously infeasible projects, such as scaling initiatives or crafting interactive dashboards. Engineers delegate tedious, verifiable chores like daily debugging (55% adoption) and code comprehension (42%), freeing time for complex endeavors; Claude Code data reveals escalating task sophistication (from 3.2 to 3.8/5 complexity score), greater AI autonomy (20 versus 10 actions per session), and pivots toward feature building (14% to 37% of interactions) and strategic planning (1% to 10%), including proactive "papercut fixes" for code maintainability in 8.6% of cases. This empowers "full-stack" versatility, as backend specialists rapidly prototype UIs via prompting, and fosters innovative experimentation—one researcher likened it to harnessing "a million horses" for idea-testing. Yet, benefits are tempered by risks: skill erosion in foundational coding ("When producing output is so easy... it gets harder to actually learn"), the "paradox of supervision" demanding expertise to oversee AI outputs, diminished peer collaboration as Claude absorbs 80-90% of routine queries and erodes mentorship, and existential career anxieties, with roles evolving into "AI orchestration" amid fears of long-term obsolescence ("AI will end up doing everything"). Anthropic views these shifts as societal previews, actively piloting reskilling programs, AI fluency frameworks, and professional development to navigate 2026's model leaps, urging proactive adaptation for sustainable human-AI symbiosis. <br> <br>

6. ***Oxford & UBC on Softmax Diagonalization <br>
Researchers proved that Hadamard initialization diagonalizes the softmax operator in a simple two-layer linear network, enabling the first proof of global convergence of cross-entropy gradient flow to neural collapse geometry despite non-convexity and spurious critical points, opening new analytical pathways for understanding real CE training dynamics.*** <br> <br>
Dec 3, Uni of Oxford and Uni of British Columbia published a [paper](https://arxiv.org/pdf/2512.04006) "Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics". Cross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. The work provides an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extension for which the implicit bias remained unknown. This model coincides with the unconstrained features model used to study neural collapse, making the work the first to prove that gradient flow on CE converges to the neural collapse geometry. The study constructs an explicit Lyapunov function that establishes global convergence, despite the presence of spurious critical points in the non-convex landscape. A key insight underlying the analysis is an inconspicuous finding: Hadamard Initialization diagonalizes the softmax operator, freezing the singular vectors of the weight matrices and reducing the dynamics entirely to their singular values. This technique opens a pathway for analyzing CE training dynamics well beyond our specific setting considered here. <br> <br>


7. ***Alibaba's Stable RL with LLMs <br>
Alibaba presented a theoretical explanation for why token-level policy gradient objectives can optimize sequence-level rewards in LLM-based RL, and showed that stability critically depends on minimizing training-inference discrepancy and policy staleness. Extensive experiments with a 30B MoE model validate that importance sampling, clipping, and Routing Replay are essential for robust on- and off-policy training.*** <br> <br>
Dec 3, Alibaba published a [paper](https://www.arxiv.org/pdf/2512.01374) "Stabilizing Reinforcement Learning with LLMs Formulation and Practices". This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, the work shows that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, the work shows that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. The authors hope that the shared insights and the developed recipes for stable RL training will facilitate future research. <br> <br>

8. ***Training-Free Policy Violation Detection <br>
Researchers from Fuji and Ben-Gurion University proposed a lightweight, training-free method that whitens LLM hidden activations and uses Euclidean norm in the transformed space as a policy-compliance score. Requiring only policy text and a few examples, it outperforms guardrails and fine-tuned models on enterprise policy violation benchmarks.*** <br> <br>
Dec 3, Fuji and Ben-Gurion Uni published a [paper](https://arxiv.org/pdf/2512.03994) "Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs". Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, the work proposes a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, the paper applies a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, the work uses the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, the approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. https://tinyurl.com/policy-violation-detection <br> <br>

9. ***TRIM-KV Memory-Efficient KV Cache <br>
Yale & JPMorgan introduced TRIM-KV, which learns per-token retention scores at creation time and evicts low-scoring tokens when memory is constrained. Outperforming heuristic and learnable eviction baselines—sometimes even beating full-cache models—TRIM-KV advances long-context inference while revealing interpretable layer- and head-specific token importance patterns.***     <br> <br>
Dec 3, Yale Uni and JPMorgan published a [paper](https://arxiv.org/pdf/2512.03324) "Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs". Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. The study proposes TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability. https://github.com/ngocbh/trimkv <br> <br>

10. ***Stanford's In-Context Distillation <br>
Stanford proposed a training-free technique combining in-context examples of a strong teacher model with self-consistency cascades, allowing cheaper student models to match teacher agent performance (e.g., 2.5× cost reduction on ALFWorld, 2× on AppWorld) while preserving rapid prototyping with frozen models.*** <br> <br>
Dec 2, Stanford Uni published a [paper](https://arxiv.org/pdf/2512.02543) "In-Context Distillation with Self-Consistency Cascades A Simple, Training-Free Way to Reduce LLM Agent Costs". The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs. However, executing agents at scale using high-capacity LLMs incurs high inference costs. The work proposes a simple method for reducing LLM agent inference costs without incurring the development friction costs associated with LLM fine-tuning (long training cycles, optimization hyperparameter tweaking loops) or manual prompt engineering (laborious trial and error). Most importantly,  the study introduces , which adapts the idea of knowledge distillation (training a low cost-student model to mimic a high-cost teacher) to an in-context learning setting. The approach retrieves relevant teacher demonstrations at each agent step and provides them to the student as in-context examples, enabling the student to imitate teacher behavior on-the-fly. The study combines in-context distillation with the established idea of self-consistency cascades to know when the trust the student. This adaptive strategy realizes the cost benefits of model specialization while preserving the productivity of working with frozen models. On the multi-step embodied reasoning benchmark ALFWorld, the method matches teacher-level accuracy at 2.5xlower cost, reducing per-episode costs from $0.059 to $0.024. The upfront demonstration cost amortizes after just 843 episodes, yielding cumulative savings exceeding $34,900 at deployment scale (1M episodes). On AppWorld, a complex agent benchmark requiring multi-step API workflows, the study shifts the Pareto frontier by achieving a 2xcost reduction at iso-accuracy. By reducing operational costs while maintaining rapid experimentation cycles with frozen models, the approach makes advanced agentic systems economically viable for a broader range of applications. <br> <br>

11. ***DeepSeek-V3.2 Frontier Model <br>
DeepSeek released DeepSeek-V3.2, featuring DeepSeek Sparse Attention for long-context efficiency and a scalable RL framework. The high-compute DeepSeek-V3.2-Speciale matches or surpasses GPT-5 and Gemini-3.0-Pro, achieving gold-medal performance on IMO and IOI 2025 while remaining fully open-weight.*** <br> <br>
Dec 2,  DeepSeek-AI published a [paper](https://arxiv.org/pdf/2512.02556) "DeepSeek-V3.2 Pushing the Frontier of Open Large Language Models". The work introduces DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): the work introduces DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, the high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, the work developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments. <br> <br>

12. ***Wells Fargo LLM Jury-on-Demand <br>
Wells Fargo introduced a dynamic LLM juries where reliability predictors select and weight the most trustworthy judges for each input, significantly improving correlation with human evaluation on summarization and RAG tasks compared with single or static jury approaches.*** <br> <br>
Dec 2, Wells Fargo Bank published a [paper](https://arxiv.org/pdf/2512.01786) "Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation Systems". As Large Language Models (LLMs) become integrated into high-stakes domains, there is a growing need for evaluation methods that are both scalable for real-time deployment and reliable for critical decision-making. While human evaluation is reliable, it is slow and costly. Single LLM judges are biased, and static juries lack adaptability. To overcome these limitations, the study proposes LLM Jury-on-Demand - a dynamic, learning-based framework for scalable and context-aware evaluation. The method trains a set of reliability predictors to assess when LLM judges will agree with human experts, leveraging token distributions, embeddings, and structural input features. This enables a fully adaptive evaluation where, for each data point, an optimal jury of the most reliable judges is dynamically selected, and their scores are aggregated using their reliability as weights. Experiments on summarization and RAG benchmarks show that our dynamic jury system achieves significantly higher correlation with human judgment than both single-judge and static-jury baselines. These results highlight the promise of adaptive, learning-based juries for building scalable, more reliable and trustworthy evaluation systems for modern LLMs in high-stakes domains.  <br> <br>

13. ***Stanford Structured Prompting for Benchmarking <br>
Integrating DSPy structured prompting into HELM revealed that standard fixed prompts underestimate frontier LM performance by ~4% on average and misrepresent leaderboard rankings. Structured chain-of-thought prompting yields more accurate, stable, and decision-relevant benchmarks.*** <br> <br>
Nov 28, Stanford Uni published a [paper](https://arxiv.org/pdf/2511.20836v1) "Structured Prompting Enables More Robust Evaluation of Language Models". As language models (LMs) are increasingly adopted across domains, high-quality benchmarking frameworks that accurately estimate performance are essential for guiding deployment decisions. While frameworks such as Holistic Evaluation of Language Models (HELM) enable broad evaluation across tasks, they often rely on fixed prompts that fail to generalize across LMs, yielding unrepresentative performance estimates. Unless people approximate each LM's ceiling (maximum achievable via changes to the prompt), we risk underestimating performance. Declarative prompting frameworks, such as DSPy, offer a scalable alternative to manual prompt engineering by crafting structured prompts that can be optimized per task. However, such frameworks have not been systematically evaluated across established benchmarks. The work presents a reproducible DSPy+HELM framework that introduces structured prompting methods which elicit reasoning, enabling more accurate LM benchmarking. Using four prompting methods, the study evaluates four frontier LMs across seven benchmarks (general/medical domain) against existing HELM baseline scores. The work finds that without structured prompting: (i) HELM underestimates LM performance (by 4% average), (ii) performance estimates vary more across benchmarks (2% standard deviation), (iii) performance gaps are misrepresented (leaderboard rankings flip on 3/7 benchmarks), and (iv) introducing chain-of-thought reduces LM sensitivity to prompt design (smaller  across prompts). To the authors knowledge, this is the first benchmarking study to systematically integrate structured prompting into an established evaluation framework, demonstrating how scalable performance-ceiling approximation yields more robust, decision-useful benchmarks. The authors open-source (i) [DSPy+HELM](https://github.com/stanford-crfm/helm/pull/3893) Integration and (ii) [Prompt Optimization Pipeline](https://github.com/StanfordMIMI/dspy-helm). <br> <br>

14. ***FAR.AI Focused Chain-of-Thought <br>
Focused CoT (F-CoT) separates information extraction from reasoning by first distilling the query into a concise structured context, then reasoning only over it. This simple training-free method cuts reasoning tokens 2–3× on arithmetic problems while preserving accuracy.*** <br> <br>
Nov 27, FAR.AI et al published a [paper](https://arxiv.org/pdf/2511.22176v1) "Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information". Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, the study proposes a training-free, input-centric approach. Inspired by cognitive psychology, the study introduces Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2–3× while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning. <br> <br>

15. ***Nvidia Nemotron-Flash Hybrid SLMs <br>
Nvidia’s Nemotron-Flash family optimizes real-device latency (not just parameter count) through careful depth-width ratios, efficient operators, and weight normalization, delivering up to +5.5% accuracy and 1.3–1.9× lower latency than leading Qwen3 small models.*** <br> <br>
Nov 24, Nvidia published a [paper](https://arxiv.org/pdf/2511.18890) "Nemotron-Flash Towards Latency-Optimal Hybrid Small Language Models". Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants of SLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, the authors identify two central architectural factors: depth-width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, the work first studies latency-optimal depth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, the work explores emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, the work further enhances SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. Combining these methods, the study introduces a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively.
 <br> <br> <br>

***Nov 30th, 2025***

1. ***DR Tulu: First Open Model Trained for True Long-Form Deep Research  <br>
University of Washington’s DR Tulu-8B uses Reinforcement Learning with Evolving Rubrics (RLER) — rubrics that improve alongside the model — to train directly on complex, open-ended research tasks. It significantly outperforms larger open deep-research models and matches or beats proprietary systems on science, healthcare, and general benchmarks while being far cheaper.***  <br>  <br>
Nov 26, Uni of Washington et al published a [paper](https://arxiv.org/pdf/2511.19399) "DR Tulu Reinforcement Learning with Evolving Rubrics for Deep Research". Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. the study addresses this with Reinforcement Learning with Evolving Rubrics (RLER), in which the study constructs and maintains rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, the study develops Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, the authors release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems. https://github.com/rlresearch/dr-tulu  <br>  <br>

2. ***ToolOrchestra: Small 8B Model Beats GPT-5 on Hard Tasks  <br>
Nvidia/HK’s ToolOrchestra trains a lightweight 8B orchestrator with multi-objective RL to intelligently route queries to the best tools/models. It achieves 37.1% on Humanity’s Last Exam (vs GPT-5’s 35.1%) at 2.5× higher efficiency and crushes GPT-5 on tau2-Bench and FRAMES at ~30% of the cost.***  <br>  <br>
Nov 26, Nvidia and Uni of HK published a [paper](https://arxiv.org/abs/2511.21689) "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration". Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. The study shows that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. The work introduces ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, the study produces Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.  <br>  <br>

3. ***CLaRa: Unified Retrieval + Generation Without Long Contexts  <br>
Apple/Edinburgh’s CLaRa compresses retrieved documents into continuous latent vectors, then jointly optimizes reranker and generator end-to-end with a single LM loss. It sets new SOTA in retrieval compression and QA accuracy, often beating full-text fine-tuned models.***  <br>  <br>
Nov 25, Apple and Uni of Edinburgh published a [paper](https://arxiv.org/pdf/2511.18659) "CLaRa Bridging Retrieval and Generation with Continuous Latent Reasoning". Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. This study proposes CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, the study introduces SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines. https://github.com/apple/ml-clara  <br>  <br>

4. ***Brain’s Language Processing Mirrors LLM Layer Hierarchy  <br>
Nature Communications (ECoG study): Deeper layers of GPT-2 and Llama-2 predict later neural activity in Broca’s area and language regions, showing that the temporal unfolding of human language comprehension directly corresponds to the layered depth of modern LLMs.***  <br>  <br>
Nov 26, Nature Communications published a [paper](https://www.nature.com/articles/s41467-025-65518-0) "Temporal structure of natural language processing in the human brain corresponds to layered hierarchy of large language models". Large Language Models (LLMs) offer a framework for understanding language processing in the human brain. Unlike traditional models, LLMs represent words and context through layered numerical embeddings. Here, the work demonstrates that LLMs’ layer hierarchy aligns with the temporal dynamics of language comprehension in the brain. Using electrocorticography (ECoG) data from participants listening to a 30-minute narrative, the research shows that deeper LLM layers correspond to later brain activity, particularly in Broca’s area and other language-related regions. The work extracts contextual embeddings from GPT-2 XL and Llama-2 and use linear models to predict neural responses across time. Results reveal a strong correlation between model depth and the brain’s temporal receptive window during comprehension. The work also compares LLM-based predictions with symbolic approaches, highlighting the advantages of deep learning models in capturing brain dynamics. The authors release the aligned neural and linguistic dataset as a public benchmark to test competing theories of language processing. https://github.com/DVader96/GPTology  <br>  <br>

5. ***LatentMAS: Agents Collaborate in Hidden Space, Not Text  <br>
Princeton/UIUC/Stanford’s training-free LatentMAS lets multiple LLMs exchange thoughts directly via latent embeddings in shared memory. It gains up to 14.6% accuracy, cuts tokens by 70–83%, and runs 4× faster than text-based multi-agent systems across math, code, and reasoning tasks.***  <br>  <br>
Nov 25, Princeton Uni, UIUC and Stanford Uni published a [paper](https://arxiv.org/pdf/2511.20639) "Latent Collaboration in Multi-Agent Systems". Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, the study takes a step forward by enabling models to collaborate directly within the continuous latent space. The study introduces LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. The study provides theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that the new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. https://github.com/Gen-Verse/LatentMAS.  <br>  <br>

6. ***Geoffrey Hinton Warns of AI-Driven Societal Collapse  <br>
In a discussion with Bernie Sanders, the “Godfather of AI” predicts mass unemployment, collapsing consumer demand, extreme inequality, and easier wars due to AI automation and military robots, warning that AGI may arrive within 5–20 years could fundamentally break modern society.***  <br>  <br>
Nov 25, futurism.com published an [article](https://futurism.com/artificial-intelligence/godfather-ai-breakdown-society) "Godfather of AI Predicts Total Breakdown of Society". Geoffrey Hinton, widely regarded as one of the “godfathers of AI” for pioneering deep learning and neural networks, has once again sounded dire warnings about the societal consequences of artificial intelligence. In a recent discussion with Senator Bernie Sanders at Georgetown University, Hinton argued that AI’s rapid deployment will differ fundamentally from past technological revolutions, which historically created new categories of employment. Instead, he predicts that AI will replace vast swaths of human labor without offering alternative opportunities, leading to mass unemployment. He emphasized that if AI reaches or surpasses human intelligence, virtually any job could be automated, and tech billionaires are betting heavily on this outcome. Hinton further warned that the AI industry’s profitability hinges on replacing human workers, but this creates a paradox: if workers lose income, they cannot afford to buy the products AI helps produce. He also raised geopolitical concerns, suggesting that AI-powered military robots could lower the political costs of war by eliminating human casualties, thereby making invasions more likely. Hinton has previously expressed regret over his life’s work, especially after leaving Google in 2023, and has cautioned that artificial general intelligence (AGI) may arrive sooner than expected—possibly within 20 years or less. While he claims models like GPT-5 already “know thousands of times more than us,” critics dispute whether this constitutes true knowledge. Despite AI’s current limitations, Hinton remains convinced that unchecked development could destabilize economies, exacerbate inequality, and even pose existential risks to humanity  <br>  <br>

7. ***Nemotron-Parse-1.1: Lightweight Yet Powerful OCR & Document Parser  <br>
Nvidia releases an improved 885M-parameter vision-language model for OCR, markdown, tables, charts, and diagrams. It offers longer outputs, better accuracy rivaling much larger models, and a 20% faster “TC” variant — all openly available on Hugging Face.***  <br>  <br>
Nov 25, Nvidia published a [paper](https://arxiv.org/pdf/2511.20478) "NVIDIA Nemotron Parse 1.1". The study introduces Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. The study releases the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, the authors release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation. https://huggingface.co/nvidia/NVIDIA-Nemotron-Parse-v1.1  <br>  <br>

8. ***Trump Launches “Genesis Mission” – U.S. AI Manhattan Project  <br>
President Trump’s executive order creates a massive national AI-for-science initiative led by DOE, merging federal supercomputing, datasets, labs, and private partners to train frontier models, automate discovery, and solve 20+ grand challenges in energy, biotech, quantum, and defense.***  <br>  <br>
Nov 24, UAS [announced](https://www.whitehouse.gov/presidential-actions/2025/11/launching-the-genesis-mission/) "LAUNCHING THE GENESIS MISSION". President Donald J. Trump signed an executive order launching the Genesis Mission, a Manhattan Project-scale national initiative to cement U.S. leadership in artificial intelligence by accelerating scientific discovery. The mission will integrate the federal government’s vast scientific datasets—the largest in the world—with cutting-edge AI to train frontier foundation models, deploy autonomous AI agents for hypothesis generation and testing, and automate research workflows across critical fields. Led by the Department of Energy and coordinated by the Assistant to the President for Science and Technology, the new American Science and Security Platform will combine DOE national laboratories’ supercomputing power, secure data repositories, robotic facilities, and private-sector innovation into a unified, highly secure infrastructure that complies with strict cybersecurity, privacy, and export-control standards. Within tight deadlines—90 to 270 days—agencies must identify computing resources, curate and integrate datasets, assess automation capabilities, and demonstrate initial breakthroughs. The Secretary of Energy is tasked with proposing at least 20 grand scientific and technological challenges spanning advanced manufacturing, biotechnology, critical materials, nuclear technologies, quantum science, and semiconductors. Interagency councils, standardized partnership agreements with industry and academia, fellowship programs, and prize competitions will further align federal R&D and attract top talent. Annual progress reports will track discoveries, prototypes, and publications, ensuring the mission rapidly translates taxpayer-funded research into national security advantages, energy independence, economic growth, and transformative scientific advances.  <br>  <br>

9. ***Multi-Agent Prompt Learning Dramatically Improves Hate Speech Detection  <br>
New JMIS paper uses multiple LLM agents + information-theoretic prompt selection + motivation-aware tuning to better capture the nuanced, diverse drivers of hate speech, achieving large gains over standard hate-speech detection benchmarks.***  <br>  <br>
Nov 24, JMIS published a [paper](https://www.tandfonline.com/doi/full/10.1080/07421222.2025.2561383) "Leveraging Large Language Models for Hate Speech Detection: Multi-Agent, Information-Theoretic Prompt Learning for Enhancing Contextual Understanding". An essential component in combating hate speech is the development of effective computational algorithms. While prior research has proposed a range of methods for hate speech detection, they often fall short in addressing the complex nature of hate speech, which is characterized by its nuanced nature, the diversity of its forms, and the heterogeneous motivations behind it. To address these limitations, the study introduces a novel prompt-learning framework for hate speech detection. The approach offers several key innovations: (i) prompt generation is delegated to multiple language model agents, drawing upon the theory of questioning as a guiding principle; (ii) employ an information-theoretic selection mechanism to identify the most effective prompts from a pool of candidates; and (iii) incorporate motivation-aware instruction tuning to improve the model’s capacity to capture the diverse motivational drivers of hate speech. Empirical evaluation, which includes comparisons with state-of-the-art benchmarks and multiple robustness checks, demonstrates significant performance gains achieved by the framework. These findings highlight the promise of prompt-learning based methods in hate speech detection, particularly when designed with attention to the social and psychological complexities that characterize online hate speech.  <br>  <br>

10. ***CafeQ: Calibration-Free 3/4-bit Quantization That Actually Works  <br>
Google/Wisconsin/UCLA’s method quantizes Gemma 2 models to 3–4 bits without any calibration data by learning optimal transformations and adaptive rounding. It lifts Gemma 2 9B 3-bit performance from 52.0 → 60.6 while adding <3% overhead.***  <br>  <br>
Nov 24, Google, Uni of Wisconsin Madison and UCLA published a [paper](https://arxiv.org/pdf/2511.19705v1) "CafeQ Calibration-free Quantization via Learned Transformations and Adaptive Rounding". Post-training quantization is an effective method for reducing the serving cost of large language models, where the standard approach is to use a round-to-nearest quantization level scheme. However, this often introduces large errors due to outliers in the weights. Proposed mitigation mechanisms include applying adaptive rounding, random rotation transformations or committing to a post-training target using calibration data. Unfortunately, this reliance on calibration data can be severely limiting in some real-world scenarios as such data may be unavailable or subject to privacy regulations. This study proposes algorithms to optimize transformations and adaptive rounding without access to any calibration data. The optimization is achieved by designing a suitable proxy function for the quantization loss without calibration data. To maintain inference efficiency, the work performs structured matrix transformations for single matrices. For paired weights that interact directly in the computation graph, the study uses dual matrix transformations and adaptive rounding methods. The work conducts experiments on Gemma 2 models, and observes consistent improvement over the baselines. For Gemma 2 9B quantization, the method improves the average benchmark score from 61.9 to 62.4 for 4-bit quantization and from 52.0 to 60.6 for 3-bit quantization, while adding less than 3% of computation overhead. Furthermore, the method achieves performance comparable to the commonly used GPTQ method, which requires calibration data.  <br>  <br>

11. ***PRInTS: Generative Reward Model for Long Information-Seeking Trajectories  <br>
UNCC/UT Austin’s PRInTS reward model scores and summarizes long agent trajectories across multiple quality dimensions. Used for best-of-n sampling, it lets smaller open models match or beat frontier agents on FRAMES, GAIA, and WebWalkerQA.***  <br>  <br>
Nov 24, UNCC and Uni of Texas at Austin published a [paper](https://arxiv.org/pdf/2511.19314) "PRInTS: Reward Modeling for Long-Horizon Information Seeking". Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, the study introduces PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines. https://github.com/G-JWLee/PRInTS  <br>  <br>

12. ***Fara-7B: Efficient On-Device Computer-Use Agent  <br>
Microsoft’s Fara-7B, trained on massive synthetic web trajectories from the new FaraGen system, controls computers directly from screenshots. Despite only 7B parameters, it outperforms same-size agents and competes with much larger frontier computer-use models.***  <br>  <br>
Nov 24, Microsoft published a [paper](https://arxiv.org/pdf/2511.19663) "Fara-7B: An Efficient Agentic Model for Computer Use". Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, the study introduces FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. The study uses this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. The study finds that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- the novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. https://github.com/microsoft/fara  <br>  <br>

13. ***Modern Transformer Blocks: Context = Rank-1 Weight Updates  <br>
Cambridge/Google prove analytically that the full effect of any context in Gemma-style and most modern transformer blocks (including MoE, gating, pre/post-norm) can be exactly represented as simple rank-1 patches to MLP weights plus a norm-scale adjustment.***  <br>  <br>
Nov 22, Uni of Cambridge and Google published a [paper](https://arxiv.org/pdf/2511.17864) "Equivalence of Context and Parameter Updates in Modern Transformer Blocks". Recent research has established that the impact of context in a vanilla transformer can be represented implicitly by forming a token-dependent, rank-1 patch to its MLP weights. This work extends that foundational theory to the diverse architectures of modern Large Language Models. The study first demonstrates a precise, analytical solution for a Gemma-style transformer block, proving that the entire effect of a context can be perfectly mapped to rank-1 patches on its MLP weight matrices and a patch to the RMSNorm scale. The study then generalizes this result, providing a constructive proof and algorithm for multi-layer models. To unify these findings, the study introduces a general framework centered on two core properties: input controllability and output controllability. The work proves that a perfect implicit weight patch is possible for any MLP block where the inner function is input-controllable and the outer function is output-controllable. This provides a simpler and more powerful lens for understanding how transformer models transmute prompts into effective weights. This setup generalizes to a wide range of modern LLM architectures including gating, pre-/post-norm, mixture of experts and sequential/parallel transformer blocks.  <br>  <br>

14. ***Cognitive Science Explains Why LLMs Fail Simple Tasks  <br>
Large-scale study of 170K reasoning traces + human think-alouds reveals LLMs rely on shallow forward chaining while humans use hierarchical nesting and meta-cognition. New cognitively inspired test-time scaffolding boosts complex-problem performance by up to 60%.***  <br>  <br>
Nov 20, UIUC, Uni of Washington, Princeton Uni and Harvard Uni published a [paper](https://arxiv.org/pdf/2511.16660v1) "Cognitive Foundations for Reasoning and Their Manifestation in LLMs". Large language models solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. The study synthesizes cognitive science research into a taxonomy of 28 cognitive elements spanning computational constraints, meta-cognitive controls, knowledge representations, and transformation operations, then analyze their behavioral manifestations in reasoning traces. The work proposes a fine-grained cognitive evaluation framework and conduct the first large-scale analysis of 170K traces from 17 models across text, vision, and audio modalities, alongside 54 human think-aloud traces, which the work makes publicly available. Analysis reveals systematic structural differences: humans employ hierarchical nesting and meta-cognitive monitoring while models rely on shallow forward chaining, with divergence most pronounced on ill-structured problems. Meta-analysis of 1,598 LLM reasoning papers reveals the research community concentrates on easily quantifiable behaviors (sequential organization: 55%, decomposition: 60%) while neglecting meta-cognitive controls (self-awareness: 16%, evaluation: 8%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, the work develops test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 60% on complex problems. By bridging cognitive science and LLM research, the study establishes a foundation for developing models that reason through principled cognitive mechanisms rather than brittle spurious reasoning shortcuts or memorization, opening new directions for both improving model capabilities and testing theories of human cognition at scale. 
  <br>  <br>
15. ***Nature: What Happens to Science When the AI Bubble Bursts?  <br>
Experts predict a harsher crash than dot-com, with mass AI-startup failures and brain-drain reversal. Yet history (dot-com, bicycle boom) suggests survivors will keep core talent and a post-bubble era could refocus academia on sustainable, fundamental AI-driven science.***  <br>  <br>
Nov 19, Nature published an [article](https://www.nature.com/articles/d41586-025-03776-0) "If the AI bubble bursts, what will it mean for research?". Experts grapple with the looming burst of the artificial intelligence (AI) investment bubble and its ripple effects on scientific research. Amidst hype-fueled spending—17 times higher than pre-dot-com crash levels for internet firms—NVIDIA's valuation has soared to $4.6 trillion, eclipsing most national economies. Yet, stark realities emerge: a McKinsey report reveals 80% of companies see no earnings boost from AI, while architectural weaknesses in chatbots threaten to undermine research integrity. OpenAI's Sam Altman has candidly labeled parts of the sector "bubbly," signaling overvaluation. Analysts forecast a downturn fiercer than the 2000 dot-com implosion, which vaporized $5 trillion and slashed tech jobs, potentially triggering mass startup failures in redundant AI applications. However, optimists like economist John Turner draw parallels to the dot-com era, where despite fewer computer-science graduates, academic publications surged, and innovations like smartphones and broadband endured. Venture capitalist Brent Goldfarb predicts that while smaller ventures crumble, giants such as OpenAI and NVIDIA will weather the storm by safeguarding core talent. The burst could redirect researchers from overpromising AI pursuits to diverse fields, much like the 1896 bicycle boom's collapse spurred motorcycles and automobiles, or the dot-com fallout birthed the modern web. A pressing concern is the "AI brain drain" from universities to lucrative tech roles—salaries ten times higher—skewing focus toward commercial gains over foundational science. Ultimately, the article posits that while short-term chaos looms, a post-bubble recalibration might foster more sustainable, impactful AI-driven discoveries, urging academia to reclaim exploratory rigor amid the frenzy.  <br>  <br>

16. ***AgenticSciML: AI Agents Invent New SciML Methods from Scratch  <br>
Brown University’s multi-agent system with debate + evolutionary search discovers novel physics-informed neural network architectures and training strategies that outperform human-designed baselines by up to 10,000× on error in physics and operator learning tasks.***  <br>  <br>
Nov 10, Brown Uni published a [paper](https://arxiv.org/abs/2511.07262) "AgenticSciML Collaborative multi-agent systems for emergent discovery in scientific machine learning". Scientific Machine Learning (SciML) integrates data-driven inference with physical modeling to solve complex problems in science and engineering. However, the design of SciML architectures, loss formulations, and training strategies remains an expert-driven research process, requiring extensive experimentation and problem-specific insights. Here the study introduces AgenticSciML, a collaborative multi-agent system in which over 10 specialized AI agents collaborate to propose, critique, and refine SciML solutions through structured reasoning and iterative evolution. The framework integrates structured debate, retrieval-augmented method memory, and ensemble-guided evolutionary search, enabling the agents to generate and assess new hypotheses about architectures and optimization procedures. Across physics-informed learning and operator learning tasks, the framework discovers solution methods that outperform single-agent and human-designed baselines by up to four orders of magnitude in error reduction. The agents produce novel strategies -- including adaptive mixture-of-expert architectures, decomposition-based PINNs, and physics-informed operator learning models -- that do not appear explicitly in the curated knowledge base. These results show that collaborative reasoning among AI agents can yield emergent methodological innovation, suggesting a path toward scalable, transparent, and autonomous discovery in scientific computing.  <br>  <br>

17. ***AI Does NOT Help Judges Make Better Bail Decisions  <br>
PNAS paper presents a rigorous statistical framework to test human+AI vs human-alone vs AI-alone decision making. Applied to real pretrial risk-assessment data, neither the risk tool nor LLMs improve judicial accuracy — replacing judges with algorithms actually worsens outcomes.***  <br>  <br>
Sep 2025, PNAS published a [paper](https://www.pnas.org/doi/10.1073/pnas.2505106122) "Does AI help humans make better decisions A statistical evaluation framework for experimental and observational studies". The use of AI, or more generally data-driven algorithms, has become ubiquitous in today’s society. Yet, in many cases and especially when stakes are high, humans still make final decisions. The critical question, therefore, is whether AI helps humans make better decisions compared to a human-alone or AI-alone system. The work introduces a methodological framework to answer this question empirically with minimal assumptions. The work measures a decision maker’s ability to make correct decisions using standard classification metrics based on the baseline potential outcome. The study considers a single-blinded and unconfounded treatment assignment, in which the provision of AI-generated recommendations is assumed to be randomized across cases, conditional on observed covariates, with final decisions made by humans. Under this study design, the work shows how to compare the performance of three alternative decision-making systems—human-alone, human-with-AI, and AI-alone. Importantly, the AI-alone system encompasses any individualized treatment assignment, including those not used in the original study. The work also shows when AI recommendations should be provided to a human-decision maker, and when one should follow such recommendations. The authors apply the proposed methodology to their own randomized controlled trial evaluating a pretrial risk assessment instrument. The study finds that the risk assessment recommendations do not improve the classification accuracy of a judge’s decision to impose cash bail. Furthermore, replacing a human judge with algorithms—the risk assessment score and a large language model in particular—yields worse classification performance.
  <br>  <br>  <br>

***Nov 23rd, 2025***

1. ***GPT-5 Accelerates Real Science  <br>
OpenAI's "Early science acceleration experiments with GPT-5" shows the model helping researchers across math, physics, biology, and more by suggesting concrete next steps. It produced four new (human-verified) math results, saved expert time on routine tasks, and serves as a practical guide for fruitful human–AI collaboration in ongoing research.***  <br>  <br>
Nov 20, OpenAI et al published a [paper](https://arxiv.org/pdf/2511.16072) "Early science acceleration experiments with GPT-5". AI models like GPT-5 are an increasingly valuable tool for scientists, but many remain unaware of the capabilities of frontier AI. The study presents a collection of short case studies in which GPT-5 produced new, concrete steps in ongoing research across mathematics, physics, astronomy, computer science, biology, and materials science. In these examples, the authors highlight how AI accelerated their work, and where it fell short; where expert time was saved, and where human input was still key. The study documents the interactions of the human authors with GPT-5, as guiding examples of fruitful collaboration with AI. Of note, this paper includes four new results in mathematics (carefully verified by the human authors), underscoring how GPT-5 can help human mathematicians settle previously unsolved problems. These contributions are modest in scope but profound in implication, given the rate at which frontier AI is progressing.  <br>  <br>

2. ***Agent0: Self-Evolving Agents Without Human Data  <br>
Researchers from UNC, Salesforce, and Stanford introduced Agent0 — a fully autonomous framework where two identical LLMs co-evolve: one creates harder tasks, the other solves them using tools. With zero external data, it boosted a base Qwen3-8B model by 18–24% on math and reasoning benchmarks.***  <br>  <br>
Nov 20, UNC-Chapel Hill, Salesforce and Stanford Uni published a [paper](https://arxiv.org/pdf/2511.16043v1) "Agent0 Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning". Large Language Model (LLM) Agents, often trained with Reinforcement Learning (RL), are constrained by a dependency on human-curated data, limiting scalability and tethering AI to human knowledge. Existing self-evolution frameworks offer an alternative but are typically restricted by the model’s inherent capabilities and single-round interactions, hindering the development of complex curricula involving tool use or dynamic reasoning. The study introduces Agent0, a fully autonomous framework that evolves high-performing agents without external data through multi-step co-evolution and seamless tool integration. Agent0 establishes a symbiotic competition between two agents initialized from the same base LLM: a curriculum agent that proposes increasingly challenging frontier tasks, and an executor agent that learns to solve them. The work integrates external tools to enhance the executor’s problem-solving capacity; this improvement, in turn, pressures the curriculum agent to construct more complex, tool-aware tasks. Through this iterative process, Agent0 establishes a self-reinforcing cycle that continuously produces high-quality curricula. Empirically, Agent0 substantially boosts reasoning capabilities, improving the Qwen3-8B-Base model by 18% on mathematical reasoning and 24% on general reasoning benchmarks. https://github.com/aiming-lab/Agent0.  <br>  <br>

3. ***Nemotron Elastic: Many Models in One  <br>
Nvidia’s Nemotron Elastic trains a single "parent" reasoning model that contains multiple nested sub-models (e.g., 12B → 9B + 6B) extractable at deployment time with no extra fine-tuning. This delivers >360× cost savings vs. training separate models and ~7× vs. other compression methods, while matching or beating SOTA accuracy.***  <br>  <br>
Nov 20, Nvidia published a [paper](https://arxiv.org/pdf/2511.16664) "Nemotron Elastic Towards Efficient Many-in-One Reasoning LLMs". Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. This studypresents Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. the work enables this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. The study additionally introduces group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. The study applies Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of the approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.  <br>  <br>

4. ***Ideation Diversity Drives Better AI Research Agents  <br>
Meta/UCL/UBC study on MLE-bench shows that top-performing AI research agents generate more diverse ideas during planning. Controlled experiments confirm that deliberately increasing ideation diversity directly improves agent success rates across multiple models and metrics.***  <br>  <br>
Nov 19, Meta, UCL and Uni of British Columbia published a [paper](https://arxiv.org/pdf/2511.15593) "What Does It Take to Be a Good AI Research Agent Studying the Role of Ideation Diversity". AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. The study examines the role that ideation diversity plays in agent performance. First, the study analyses agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. The analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, the study runs a controlled experiment where the authors modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, the study strengthens the results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that the findings still hold across other agent performance metrics.

5. ***Vision + Language Synergy Solves ARC Better
CUHK/SAIL’s new method combines visual pattern recognition with textual rule reasoning for the ARC-AGI benchmark. Their Vision-Language Synergy Reasoning (VLSR) and self-correction technique improve accuracy by up to 4.33% over pure text-based frontier models like GPT-5 and Grok 4.***
Nov 19, CUHK and SAIL published a [paper](https://arxiv.org/pdf/2511.15703) "Think Visually, Reason Textually Vision-Language Synergy in ARC". Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, this pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to the central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, the study introduces two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that the approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. The findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models.   <br>  <br>

6. ***Pure Vision Approach Cracks ARC  <br>
MIT’s Vision ARC (VARC) treats ARC puzzles as image-to-image translation using a simple Vision Transformer trained from scratch on raw grids. It reaches 60.4% on ARC-1 — far above other from-scratch methods and close to average human performance.***  <br>  <br>
Nov 18, MIT published a [paper](https://arxiv.org/abs/2511.14761) “Overview of Touché 2020: Argument Retrieval”. The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. This work formulates ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, the work represents the inputs on a "canvas" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. The model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. The framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. The results are competitive with those of leading LLMs and close the gap to average human performance. https://github.com/lillian039/VARC  <br>  <br>

7. ***Google Launches Gemini 3  <br>
Google released Gemini 3 Pro (preview Nov 20, 2025), its most capable model yet, topping leaderboards in reasoning, math, video, multimodal tasks, and long context (1M tokens). It introduces “Deep Think” mode, agentic tool use, and powers new features across Search, Gemini app, and Vertex AI.***  <br>  <br>
Nov 18, Google [released](https://blog.google/products/gemini/gemini-3/#note-from-ceo) Gemini 3. Google has officially unveiled Gemini 3, marking a significant leap toward artificial general intelligence (AGI) and described by CEO Sundar Pichai as the company's most intelligent model yet, building on the multimodal foundations of Gemini 1 and the advanced agentic reasoning of Gemini 2. Released in preview starting November 20, 2025, Gemini 3 Pro excels in state-of-the-art reasoning, depth, nuance, and multimodal understanding across text, images, video, audio, and code, with a 1-million-token context window. It outperforms predecessors and competitors on key benchmarks, leading LMArena (1501 Elo), achieving PhD-level scores on GPQA Diamond (91.9%) and Humanity’s Last Exam (37.5% without tools), setting new standards in math (MathArena Apex), video understanding (Video-MMMU), and factual accuracy. An enhanced "Deep Think" mode further boosts performance on complex problems. Gemini 3 shines in learning (synthesizing information for personalized education), building (top-tier "vibe coding," agentic tool use, and interactive UI generation via the new Google Antigravity platform), and planning (long-horizon tasks with reliable autonomy). It powers richer experiences in Search's AI Mode, the Gemini app (now with over 650 million monthly users), AI Studio, Vertex AI, and third-party tools. Emphasizing responsible development, Gemini 3 incorporates robust safety evaluations against misuse and cyberattacks. Additional variants and broader Deep Think access for Google AI Ultra subscribers are coming soon, signaling Google's accelerated push in scalable, helpful AI for billions.  <br>  <br>
 
8. ***MiroThinker: Interaction Scaling for Open Agents  <br>
MiroMind’s open-source MiroThinker v1.0 proves that letting agents perform hundreds of tool calls per task (“interaction scaling”) is as important as bigger models or longer context. The 72B version reaches near GPT-5-high performance on research-agent benchmarks like GAIA and HLE.***  <br>  <br>
Nov 18, MiroMind published a [paper](https://arxiv.org/pdf/2511.11793) “MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling”. The paper presents MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. The analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows. https://github.com/MiroMindAI/MiroThinker  <br>  <br>
 
9. ***Model Souping Gets Smarter  <br>
Meta/UCL’s “Souper-Model” improves model averaging (“souping”) by identifying category-specific expert models and using non-uniform weighted averaging. The resulting Soup of Category Experts (SoCE) sets new SOTA on function-calling, math, and multilingual benchmarks.***  <br>  <br>
Nov 18, Meta and UCL published a [paper](https://arxiv.org/pdf/2511.13254) “Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance”. Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. This work introduces Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, the method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. The work demonstrates that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard. https://github.com/facebookresearch/llm_souping  <br>  <br>
 
10. ***Agent-R1: Clean RL Framework for LLM Agents  <br>
USTC’s Agent-R1 provides a modular, extensible reinforcement-learning framework tailored for LLM agents. It formalizes the MDP for tool-using agents and shows promising early results on multihop QA tasks.***  <br>  <br>
Nov 18, USTC published a [paper](https://arxiv.org/pdf/2511.14460) “Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning”. Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, the study introduces Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. The authors conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework. https://github.com/0russwest0/Agent-R1  <br>  <br>

11. ***Back to Basics Diffusion: Predict Clean Images  <br>
MIT’s “Just image Transformers” (JiT) shows that making diffusion models directly predict clean pixels (instead of noise) works dramatically better under the manifold hypothesis, yielding strong ImageNet results with simple large-patch Transformers and no pre-training.***  <br>  <br>
Nov 17, MIT published a [paper](https://arxiv.org/abs/2511.13720) “Back to Basics: Let Denoising Generative Models Denoise”. Today's denoising diffusion models do not "denoise" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. This paper suggests that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, the study advocates for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. The work shows that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. The approach is conceptually nothing more than "Just image Transformers", or JiT, as called. The study reports competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With the networks mapping back to the basics of the manifold, the research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data. https://github.com/LTH14/JiT
  <br>  <br>
12. ***Fundamental Limits of LLM Scaling  <br>
Stanford-led theoretical paper proves five unavoidable limits of scaling LLMs — hallucination, context compression, reasoning degradation, retrieval fragility, and multimodal misalignment — using computability, information theory, and geometry, explaining why certain failures persist no matter how big models get.***  <br>  <br>
Nov 17, Stanford Uni et al published a [paper](https://arxiv.org/pdf/2511.12869) "On the Fundamental Limits of LLMs at Scale". Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. The study further shows how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, the study pairs theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.  <br>  <br>

13. ***FlashMoBA: Faster & Better Long-Context Attention  <br>
MIT/Nvidia optimize Mixture of Block Attention with theory-guided smaller blocks + convolution on keys, then make it GPU-efficient via FlashMoBA kernel. Trained models match dense attention performance with up to 14.7× speedup.***  <br>  <br>
Nov 14, MIT and Nvidia published a [paper](https://www.arxiv.org/pdf/2511.11571) “Optimizing Mixture of Block Attention”. Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. This study first develops a statistical model to analyze MoBA's underlying mechanics. The model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. The study derives a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by the analysis, the study identifies two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, the study introduces FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes the theory recommends. The work validates the insights by training LLMs from scratch, showing that the improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making the theoretically-grounded improvements practical. https://github.com/mit-han-lab/flash-moba  <br>  <br>

14. ***LimiX: First Strong Foundation Model for Tabular/Structured Data  <br>
StableAI/Tsinghua’s LimiX-16M and 2M are pretrained large structured-data models that handle classification, regression, imputation, and generation in one unified framework. LimiX-16M beats specialized baselines across 11 tabular benchmarks and provides the first scaling laws for structured-data foundation models.***  <br>  <br>
Nov 7, StableAI and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2509.03505) "LimiX Unleashing Structured-Data Modeling Capability for Generalist Intelligence". The study argues that progress toward general intelligence requires complementary foundation models grounded in language, the physical world, and structured data. This report presents LimiX-16M and LimiX-2M, two instantiations of the large structured-data models (LDMs). Both models treat structured data as a joint distribution over variables and missingness, thus capable of addressing a wide range of tabular tasks through query-based conditional prediction via a single model. They are pretrained using masked joint-distribution modeling with an episodic, context-conditional objective, supporting rapid, training-free adaptation at inference. The study evaluates LimiX models across 11 large structured-data benchmarks with broad regimes of sample size, feature dimensionality, class number, categorical-to-numerical feature ratio, missingness, and sample-to-feature ratios. LimiX-16M consistently surpasses strong baselines. The superiority holds across a wide range of tasks, such as classification, regression, missing value imputation, and data generation, often by substantial margins, while avoiding task-specific architectures or bespoke training per task. Notably, LimiX-2M delivers strong results under tight compute and memory budgets. The study also presents the first scaling law study for LDMs, revealing how data and model scaling jointly influence downstream performance and offering quantitative guidance for tabular foundation modeling. https://github.com/limix-ldm/LimiX/
  <br>  <br>  <br>
 

***Nov 16th, 2025***

1. ***Microsoft introduces GAD for LLM distillation:  <br>
Microsoft's paper presents Generative Adversarial Distillation (GAD), a method for black-box on-policy distillation of LLMs using only teacher model outputs, framing the student as a generator in a minimax game with a discriminator for adaptive feedback, outperforming traditional distillation and making Qwen2.5-14B-Instruct comparable to GPT-5-Chat on evaluations.***  <br>  <br>
   Nov 13, Microsoft published a [paper](https://huggingface.co/papers/2511.10643) “Black-Box On-Policy Distillation of Large Language Models”. Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. This study introduces Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation. https://aka.ms/GAD-github  <br>  <br>

3. ***Meta advances LLM instruction following with rubrics:  <br>
The paper introduces AdvancedIF, a benchmark with over 1,600 prompts and expert rubrics for evaluating complex instruction following, and RIFL, a post-training pipeline using rubric generation, verification, and reward shaping to improve LLMs by 6.7% on the benchmark and confirm component effectiveness.***  <br>  <br>
   Nov 13, Meta, Princeton Uni and CMU published a [paper](https://arxiv.org/pdf/2511.10507) “Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following”. Recent progress in large language models (LLMs) has led to impressive performance on a range of tasks, yet advanced instruction following (IF)-especially for complex, multi-turn, and system-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality, human-annotated benchmarks and reliable, interpretable reward signals. This study introduces AdvancedIF, a comprehensive benchmark featuring over 1,600 prompts and expert-curated rubrics that assess LLMs ability to follow complex, multi-turn, and system-level instructions. The study further proposes RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leverages rubric generation, a finetuned rubric verifier, and reward shaping to enable effective reinforcement learning for instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. The ablation studies confirm the effectiveness of each component in RIFL. This work establishes rubrics as a powerful tool for both training and evaluating advanced IF in LLMs, paving the way for more capable and reliable AI systems.  <br>  <br>

5. ***Google achieves Olympiad-level math with AlphaProof:  <br>
Nature's paper details AlphaProof, an RL-based agent for formal mathematical proofs in Lean, using auto-formalized problems and Test-Time RL for adaptation, solving three IMO 2024 problems and, with AlphaGeometry, earning silver medal equivalent, demonstrating scalable mathematical reasoning.***  <br>  <br>
   Nov 12, Nature published a [paper](https://www.nature.com/articles/s41586-025-09833-y) by Google “Olympiad-level formal mathematical reasoning with reinforcement learning”. A long-standing goal of artificial intelligence is to build systems capable of complex reasoning in vast domains, a task epitomized by mathematics with its boundless concepts and demand for rigorous proof. Recent AI systems, often reliant on human data, typically lack the formal verification necessary to guarantee correctness. By contrast, formal languages such as Lean offer an interactive environment that grounds reasoning, and reinforcement learning (RL) provides a mechanism for learning in such environments. The research presents AlphaProof, an AlphaZero-inspired agent that learns to find formal proofs through RL by training on millions of auto-formalized problems. For the most difficult problems, it uses Test-Time RL, a method of generating and learning from millions of related problem variants at inference time to enable deep, problem-specific adaptation. AlphaProof substantially improves state-of-the-art results on historical mathematics competition problems. At the 2024 IMO competition, the AI system, with AlphaProof as its core reasoning engine, solved three out of the five non-geometry problems, including the competition’s most difficult problem. Combined with AlphaGeometry, this performance, achieved with multi-day computation, resulted in reaching a score equivalent to that of a silver medallist, marking the first time an AI system achieved any medal-level performance. The work demonstrates that learning at scale from grounded experience produces agents with complex mathematical reasoning strategies, paving the way for a reliable AI tool in complex mathematical problem-solving.  <br>  <br>

7. ***Cognizant solves million-step tasks error-free:  <br>
The paper describes MAKER, a system using extreme task decomposition into subtasks handled by microagents with multi-agent voting for error correction, enabling zero-error solving of over a million LLM steps and suggesting massively decomposed agentic processes for large-scale problems.***  <br>  <br>
   Nov 12, Cognizant AI Lab and UT Austin published a [paper](https://arxiv.org/pdf/2511.09030) “Solving a Million-Step LLM Task with Zero Errors”. LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.  <br>  <br>

9. ***Nvidia proposes TiDAR for efficient generation:  <br>
Nvidia's paper introduces TiDAR, a hybrid architecture drafting in diffusion and sampling autoregressively in one pass with structured masks, balancing throughput and quality, outperforming speculative decoding and diffusion models in speed (4.71x-5.91x) and quality at 1.5B-8B scales.***  <br>  <br>
    Nov 12, Nvidia published a [paper](https://arxiv.org/pdf/2511.08923) “TiDAR: Think in Diffusion, Talk in Autoregression”. Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can people achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. The work introduces TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. The study extensively evaluates TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.  <br>  <br>

11. ***LeJEPA offers scalable self-supervised learning:  <br>
The paper develops LeJEPA, a theoretically grounded JEPA objective with SIGReg for optimal embedding distribution, providing simplicity, efficiency, and strong performance like 79% on ImageNet with ViT-H/14, across architectures and domains without heuristics.***  <br>  <br>
    Nov 11, Brown Uni, NYU and Meta published a [paper](https://arxiv.org/pdf/2511.08544) “LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics”. Learning manipulable representations of the world and its dynamics is central to AI. Joint-Embedding Predictive Architectures (JEPAs) offer a promising blueprint, but lack of practical guidance and theory has led to ad-hoc R&D. The work presents a comprehensive theory of JEPAs and instantiate it in LeJEPA, a lean, scalable, and theoretically grounded training objective. First, the study identifies the isotropic Gaussian as the optimal distribution that JEPAs’ embeddings should follow to minimize downstream prediction risk. Second, the study introduces a novel objective–Sketched Isotropic Gaussian Regularization (SIGReg)–to constrain embeddings to reach that ideal distribution. Combining the JEPA predictive loss with SIGReg yields LeJEPA with numerous theoretical and practical benefits: (i) single trade-off hyperparameter, (ii) linear time and memory complexity, (iii) stability across hyper-parameters, architectures (ResNets, ViTs, ConvNets) and domains, (iv) heuristics-free, e.g., no stop-gradient, no teacher–student, no hyper-parameter schedulers, and (v) distributed training-friendly implementation requiring only ≈50 lines of code. Our empirical validation covers 10+ datasets, 60+ architectures, all with varying scales and domains. As an example, using imagenet-1k for pretraining and linear evaluation with frozen backbone, LeJEPA reaches 79% with a ViT-H/14. The authors hope that the simplicity and theory-friendly ecosystem offered by LeJEPA will reestablish self-supervised pre-training as a core pillar of AI research (GitHub repo).  <br>  <br>

13. ***UHeads enable efficient reasoning verification:  <br>
The paper proposes UHeads, lightweight transformer heads using LLM internal states for uncertainty-based step verification in reasoning chains, matching or surpassing larger PRMs across domains with self-supervised or LLM-generated labels.***  <br>  <br>
    Nov 11, ETH et al published a [paper](https://arxiv.org/pdf/2511.06209) “Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads”. Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, the work proposes a lightweight alternative for step-level reasoning verification based on data-driven uncertainty scores. The study trains transformer-based uncertainty quantification heads (UHeads) that use the internal states of a frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in a self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even surpass the performance of PRMs that are up to 810x larger. The findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering a promising direction toward scalable and generalizable introspective LLMs.  <br>  <br>

15. ***IPW measures local AI efficiency:  <br>
Stanford's paper introduces intelligence per watt (IPW) to evaluate local LMs on accuracy, energy, and power for 1M queries across models and accelerators, showing 88.7% accuracy, 5.3x IPW improvement from 2023-2025, and optimization potential for redistributing cloud demand.***  <br>  <br>
    Nov 11, Stanford Uni published a [paper](https://arxiv.org/pdf/2511.07885) “Intelligence per Watt: Measuring Intelligence Efficiency of Local AI”. Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). The work proposes intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. The work conducts a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, the work measures accuracy, energy, latency, and power. The analysis reveals 3 findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. The authors release their IPW profiling harness for systematic intelligence-per-watt benchmarking. https://github.com/HazyResearch/intelligence-per-watt  <br>  <br>

17. ***Retrofitting recurrence boosts pretrained models:  <br>
The paper explores converting pretrained non-recurrent LMs to depth-recurrent ones via a curriculum, improving math performance at given compute budgets compared to post-training non-recurrent models.***  <br>  <br>
    Nov 10, Uni of Maryland, NYU et al published a [paper](https://arxiv.org/pdf/2511.07384) “Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence”. Recent advances in depth-recurrent language models show that recurrence can decouple train-time compute and parameter count from test-time compute. This work studies how to convert existing pretrained non-recurrent language models into depth-recurrent models. The study finds that using a curriculum of recurrences to increase the effective depth of the model over the course of training preserves performance while reducing total computational cost. In the experiments, on mathematics, the work observes that converting pretrained models to recurrent ones results in better performance at a given compute budget than simply post-training the original non-recurrent language model. https://github.com/mcleish7/retrofitting-recurrence  <br>  <br>

19. ***GSW enhances episodic memory for RAG:  <br>
UCLA's paper proposes Generative Semantic Workspace (GSW) for building narrative representations in LLMs, with Operator and Reconciler for coherence, outperforming RAG baselines by up to 20% on EpBench while reducing context tokens by 51%.***  <br>  <br>
    Nov 10, UCLA published a [paper](https://arxiv.org/pdf/2511.07587) “Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces”. Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, the work proposes the Generative Semantic Workspace (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. The framework comprises an Operator, which maps incoming observations to intermediate semantic structures, and a Reconciler, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) huet_episodic_2025 comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to 20%. Furthermore, GSW is highly efficient, reducing query-time context tokens by 51% compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.  <br>  <br>

21. ***Fei-Fei Li emphasizes spatial intelligence:  <br>
In her essay, Fei-Fei Li argues for world models to bridge AI's physical world gap, enabling generative, multimodal, interactive systems for applications like robotics and education, augmenting human capabilities as seen in prototypes like Marble.***  <br>  <br>
    Nov 10, FeiFei Li published an [essay](https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence) “From Words to Worlds: Spatial Intelligence is AI’s Next Frontier”. Fei-Fei Li argues that while today’s large language models (LLMs) excel at processing and generating language, they lack grounding in the physical world—a gap that spatial intelligence can bridge. Drawing on her decades of work in computer vision, including the creation of ImageNet and her current venture World Labs, Li contends that spatial intelligence—our innate ability to perceive, reason about, and interact with three-dimensional space—is foundational to human cognition, creativity, and survival. From children learning through play to scientists like Watson and Crick deducing DNA’s structure through 3D modeling, spatial reasoning has driven human progress. Yet current AI, even multimodal models, struggles with basic spatial tasks like estimating distances or predicting physical dynamics. To overcome this, Li proposes “world models”: next-generation AI systems that are generative (creating physically and geometrically consistent 3D worlds), multimodal (interpreting diverse inputs like text, images, and gestures), and interactive (predicting how actions change environments). These models could revolutionize storytelling, robotics, scientific discovery, healthcare, and education by enabling machines to understand and collaborate within real or virtual spaces. Li emphasizes that this technology should augment—not replace—human capabilities, empowering creators, caregivers, and researchers. While significant technical hurdles remain, early prototypes like World Labs’ Marble signal a transformative future where AI truly grasps the spatial fabric of our world, fulfilling Turing’s original vision of thinking machines that meaningfully partner with humanity.  <br>  <br>

23. ***OptiBERT optimizes encoder pretraining:  <br>
The paper investigates compute-optimal MLM pretraining for encoders, finding higher data-to-model ratios than autoregressive models, and trains OptiBERT to match or surpass baselines with less FLOPS on GLUE and MTEB.***  <br>
    Nov 9, Meta et al published a [paper](https://aclanthology.org/2025.emnlp-main.1804.pdf) “Training compute-optimal transformer encoder models”. Transformer encoders are critical for a wide range of Natural Language Processing (NLP) tasks, yet their compute–efficiency remains poorly understood. The study presents the first comprehensive empirical investigation of compute-optimal pretraining for encoder transformers using the Masked Language Modeling (MLM) objective. Across hundreds of carefully controlled runs the authors varying model size, data size, batch size, learning rate, and masking ratio, with increasing compute budget. The compute-optimal data-to-model ratio of Transformer encoder models is 10 to 100 times larger than the ratio of auto-regressive models. Using these recipes, the study trains OptiBERT, a family of compute-optimal BERT-style models that matches or surpasses leading baselines—including ModernBERT and NeoBERT—on GLUE and MTEB while training with dramatically less FLOPS.  <br>  <br>

25. ***RL enhances LLM knowledge traversal:  <br>
The paper shows RL improves knowledge recall in hierarchical tasks over SFT, hypothesizing better procedural navigation, supported by prompting recovery, path recall superiority, and activation analysis showing query representation changes.***  <br>  <br>
    Nov 8, Simon Fraser Uni, Meta and ETH published a [paper](https://arxiv.org/pdf/2511.05933) “Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs”. Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. The work challenges this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). The study hypothesizes these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, the study shows that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). The work further finds that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally the layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement "code 57.95 refers to urinary infection") maintain high cosine similarity between SFT and RL models, query representations (e.g., "what is code 57.95") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.  <br>  <br>

27. ***CAT offers controllable efficient transformers:  <br>
NYU's paper introduces Compress & Attend Transformer (CAT), using dense attention and compression for adaptive quality-compute trade-offs, outperforming efficient baselines in modeling, recall, and speed (1.4-3x faster, 2-9x less memory).***  <br>  <br>
    Nov 7, NYU published a [paper](https://arxiv.org/pdf/2511.05313) “Attention and Compression is all you need for Controllably Efficient Language Models”. The quadratic cost of attention in transformers motivated the development of efficient approaches: namely sparse and sliding window attention, convolutions and linear attention. Although these approaches result in impressive reductions in compute and memory, they often trade-off with quality, specifically in-context recall performance. Moreover, apriori fixing this quality-compute tradeoff means being suboptimal from the get-go: some downstream applications require more memory for in-context recall, while others require lower latency and memory. Further, these approaches rely on heuristic choices that artificially restrict attention, or require handcrafted and complex recurrent state update rules, or they must be carefully composed with attention at specific layers to form a hybrid architecture that complicates the design process, especially at scale. To address above issues, the study proposes Compress & Attend Transformer (CAT), a conceptually simple architecture employing two simple ingredients only: dense attention and compression. CAT decodes chunks of tokens by attending to compressed chunks of the sequence so far. Compression results in decoding from a reduced sequence length that yields compute and memory savings, while choosing a particular chunk size trades-off quality for efficiency. Moreover, CAT can be trained with multiple chunk sizes at once, unlocking control of quality-compute trade-offs directly at test-time without any retraining, all in a single adaptive architecture. In exhaustive evaluations on common language modeling tasks, in-context recall, and long-context understanding, a single adaptive CAT model outperforms existing efficient baselines, including hybrid architectures, across different compute-memory budgets. Further, a single CAT matches dense transformer in language modeling across model scales while being 1.4-3x faster and requiring 2-9x lower total memory usage. https://github.com/rajesh-lab/cat-transformer  <br>  <br>

29. ***Deep Progressive Training scales model depth:  <br>
Meta's paper proposes zero/one-layer progressive training, saving compute or accelerating training for GPT2-like models while maintaining loss, through insights on initialization, hyperparameters, and expansion timing.***  <br>  <br>
    Nov 7, Meta published a [paper](https://arxiv.org/pdf/2511.04981) “Deep Progressive Training: scaling up depth capacity of zero/one-layer models”. Model depth is a double-edged sword in deep learning: deeper models achieve higher accuracy but require higher computational cost. To efficiently train models at scale, an effective strategy is the progressive training, which scales up model capacity during training, hence significantly reducing computation with little to none performance degradation. The work studies the depth expansion of large models through the lens of optimization theory and feature learning, offering insights on the initialization of new layers, hyperparameter transfer, learning rate schedule, and timing of model expansion. Specifically, the work proposes zero/one-layer progressive training for the optimal tradeoff between computation and loss. For example, zero/one-layer progressive training on GPT2 can save  compute, or equivalently accelerate  while achieving almost the same loss, compared to to a fully trained 60-layer model with 7B parameters.  <br>  <br>

31. ***Computational Turing Test evaluates AI language realism:  <br>
The paper introduces a framework combining metrics and features to assess LLM human-likeness on social media data, finding calibration strategies fail to close gaps, with trade-offs between likeness and semantics, and instruction-tuning worsening performance.***  <br>  <br>
    Nov 6, Uni of Zurich, Uni of Amsterdam, Duke Uni and NYU published a [paper](https://arxiv.org/pdf/2511.04195) “Computational Turing Test Reveals Systematic Differences Between Human and AI Language”. Large language models (LLMs) are increasingly used in the social sciences to simulate human behavior, based on the assumption that they can generate realistic, human-like text. Yet this assumption remains largely untested. Existing validation efforts rely heavily on human-judgment-based evaluations -- testing whether humans can distinguish AI from human output -- despite evidence that such judgments are blunt and unreliable. As a result, the field lacks robust tools for assessing the realism of LLM-generated text or for calibrating models to real-world data. This paper makes two contributions. First, the study introduces a computational Turing test: a validation framework that integrates aggregate metrics (BERT-based detectability and semantic similarity) with interpretable linguistic features (stylistic markers and topical patterns) to assess how closely LLMs approximate human language within a given dataset. Second, the study systematically compares nine open-weight LLMs across five calibration strategies -- including fine-tuning, stylistic prompting, and context retrieval -- benchmarking their ability to reproduce user interactions on X (formerly Twitter), Bluesky, and Reddit. The findings challenge core assumptions in the literature. Even after calibration, LLM outputs remain clearly distinguishable from human text, particularly in affective tone and emotional expression. Instruction-tuned models underperform their base counterparts, and scaling up model size does not enhance human-likeness. Crucially, the study identifies a trade-off: optimizing for human-likeness often comes at the cost of semantic fidelity, and vice versa. These results provide a much-needed scalable framework for validation and calibration in LLM simulations -- and offer a cautionary note about their current limitations in capturing human communication.  <br>  <br>

33. ***Google's File Search simplifies enterprise RAG:  <br>
Venturebeat's article highlights Google's File Search Tool as a managed RAG solution handling the full pipeline, supporting diverse formats with free retrieval, competing with OpenAI and AWS by reducing complexity and enabling fast prototyping.***  <br>  <br>
    Nov 6, Venturebeat published an [article](https://venturebeat.com/ai/why-googles-file-search-could-displace-diy-rag-stacks-in-the-enterprise) “Why Google’s File Search could displace DIY RAG stacks in the enterprise”. Google’s newly released File Search Tool on the Gemini API offers enterprises a fully managed retrieval-augmented generation (RAG) solution that abstracts the entire retrieval pipeline, eliminating the engineering complexity of traditional DIY RAG stacks. By handling file storage, chunking, embedding generation, and vector search internally, File Search enables developers to ground Gemini responses with private data using a single generateContent API call—requiring no separate vector database, embedding orchestration, or citation logic. Powered by Google’s top-ranked Gemini Embedding model, it supports diverse file formats (PDF, Docx, JSON, code) and delivers context-aware retrieval with built-in citations, ensuring verifiable, relevant answers even for imprecise queries. While embedding indexing costs $0.15 per million tokens, storage and query-time retrieval are free. The tool directly competes with OpenAI’s Assistants API file search and AWS Bedrock’s data automation, but Google claims superior standalone simplicity with minimal orchestration. Early adopter Phaser Studio reports slashing prototype times from days to minutes by instantly surfacing code snippets and templates from 3,000+ files. As enterprises scale AI agents demanding accurate, data-grounded decisions, File Search positions itself as a drop-in alternative to fragmented RAG pipelines, potentially displacing custom solutions by prioritizing speed, reliability, and reduced operational overhead in production environments.  <br>  <br>

35. ***Semantic calibration emerges in base LLMs:  <br>
Apple's paper shows base LLMs are semantically calibrated for confidence in meanings via next-token prediction, but RL tuning and chain-of-thought break it, validated across tasks with theoretical mechanisms and predictions.***  <br>  <br>
    Nov 6, Apple published a [paper](https://arxiv.org/pdf/2511.04869) “Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs”. Large Language Models (LLMs) often lack meaningful confidence estimates for their outputs. While base LLMs are known to exhibit next-token calibration, it remains unclear whether they can assess confidence in the actual meaning of their responses beyond the token level. The work finds that, when using a certain sampling-based notion of semantic calibration, base LLMs are remarkably well-calibrated: they can meaningfully assess confidence in open-domain question-answering tasks, despite not being explicitly trained to do so. The main theoretical contribution establishes a mechanism for why semantic calibration emerges as a byproduct of next-token prediction, leveraging a recent connection between calibration and local loss optimality. The theory relies on a general definition of "B-calibration," which is a notion of calibration parameterized by a choice of equivalence classes (semantic or otherwise). This theoretical mechanism leads to a testable prediction: base LLMs will be semantically calibrated when they can easily predict their own distribution over semantic answer classes before generating a response. The work states three implications of this prediction, which is validated through experiments: (1) Base LLMs are semantically calibrated across question-answering tasks, (2) RL instruction-tuning systematically breaks this calibration, and (3) chain-of-thought reasoning breaks calibration. The work provides the first principled explanation of when and why semantic calibration emerges in LLMs.  <br>  <br>

37. ***Open Character Training shapes AI personas:  <br>
The paper presents fine-tuning with Constitutional AI and synthetic data for robust personas like humorous or malevolent, outperforming prompts or steering in coherence, with minimal capability impact and preference tracking.***  <br>  <br>
    Nov 3, Uni of Cambridge et al published a [paper](https://papers-pdfs.assets.alphaxiv.org/2511.01689v1.pdf) “Open Character Training: Shaping the Persona of AI Assistants Through Constitutional AI”. The character of the "AI assistant" persona generated by modern chatbot large language models influences both surface-level behavior and apparent values, beliefs, and ethics. These all affect interaction quality, perceived intelligence, and alignment with both developer and user intentions. The shaping of this persona, known as Character Training, is a critical component of industry post-training, yet remains effectively unstudied in the academic literature. The work introduces the first open implementation of character training, leveraging Constitutional AI and synthetic introspective data to shape the assistant persona in a more effective and controlled manner than alternatives such as constraining system prompts or activation steering. Specifically, the work fine-tunes three popular open-weights models using 11 example personas, such as humorous, deeply caring, or even malevolent. With the methods, the expression of these personas is more robust to adversarial prompting than the above two alternatives, while also leading to more coherent and realistic generations. Additionally, the work demonstrates this fine-tuning has little to no effect on general capabilities as measured by common benchmarks. Finally, the work also introduces a new method to track changes in character by analyzing the revealed preferences of the assistant, uncovering a clear and holistic change induced by the approach. The authors describe and open-source the full post-training method, https://anonymous.4open.science/r/OpenCharacterTraining.  <br>  <br>

39. ***Nested Learning rethinks deep architectures:  <br>
Google's paper introduces Nested Learning (NL) as nested optimizations explaining context compression and in-context learning, leading to deep optimizers, self-modifying models, and continuum memory, with Hope module showing promise in modeling and reasoning.***  <br>  <br>
    Oct 29, Google published a [paper](https://openreview.net/pdf?id=nbMeRvNb7A) on NeurIPS2025 “Nested Learning: The Illusion of Deep Learning Architectures”. Over the last decades, developing more powerful neural architectures and simultaneously designing optimization algorithms to effectively train them have been the core of research efforts to enhance the capability of machine learning models. Despite the recent progresses, particularly in developing Language Models (LMs), there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improved, and find ''effective solutions,''. The study presents a new learning paradigm, called Nested Learning (NL), that coherently represents a model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own ''context flow''. NL reveals that existing deep learning methods learns from data through compressing their own context flow, and explain how in-context learning emerges in large models. NL suggests a path (a new dimension to deep learning) to design more expressive learning algorithms with more ''levels'', resulting in higher-order in-context learning abilities. In addition to its neuroscientifically plausible and mathematically white-box nature, we advocate for its importance by presenting three core contributions: (1) Deep Optimizers: Based on NL, the study shows that well-known gradient-based optimizers (e.g., Adam, SGD with Momentum, etc.) are in fact associative memory modules that aim to compress the gradients with gradient descent. Building on this insight, the work presents a set of more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Titans: Taking advantage of NL's insights on learning algorithms, the work presents a novel sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: presents a new formulation for memory system that generalizes the traditional viewpoint of “long-term/short-term memory”. Combining the self-modifying sequence model with the continuum memory system, the work presents a learning module, called Hope, showing promising results in language modeling, continual learning, and long-context reasoning tasks.

  <br>  <br>

***Nov 9th, 2025***

1. ***DreamGym Synthesizes Scalable RL Experiences:  <br>Meta, Chicago, and Berkeley’s DreamGym distills environment dynamics into a reasoning-based model for synthetic rollouts, uses an offline-initialized replay buffer, and adaptively generates challenging tasks; it outperforms baselines by 30%+ on WebArena, matches GRPO/PPO with synthetics, and enables sim-to-real transfer with far fewer real interactions.*** <br> <br>
2.Nov 7, Meta, Uni of Chicago, and UC Berkeley published a [paper](https://arxiv.org/pdf/2511.03773) “Scaling Agent Learning via Experience Synthesis”. While reinforcement learning (RL) can empower large language model (LLM) agents by enabling self-improvement through interaction, its practical adoption remains challenging due to costly rollouts, limited task diversity, unreliable reward signals, and infrastructure complexity, all of which obstruct the collection of scalable experience data. To address these challenges, the study introduces DreamGym, the first unified framework designed to synthesize diverse experiences with scalability in mind to enable effective online RL training for autonomous agents. Rather than relying on expensive real-environment rollouts, DreamGym distills environment dynamics into a reasoning-based experience model that derives consistent state transitions and feedback signals through step-by-step reasoning, enabling scalable agent rollout collection for RL. To improve the stability and quality of transitions, DreamGym leverages an experience replay buffer initialized with offline real-world data and continuously enriched with fresh interactions to actively support agent training. To improve knowledge acquisition, DreamGym adaptively generates new tasks that challenge the current agent policy, enabling more effective online curriculum learning. Experiments across diverse environments and agent backbones demonstrate that DreamGym substantially improves RL training, both in fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in RL-ready but costly settings, it matches GRPO and PPO performance using only synthetic interactions. When transferring a policy trained purely on synthetic experiences to real-environment RL, DreamGym yields significant additional performance gains while requiring far fewer real-world interactions, providing a scalable warm-start strategy for general-purpose RL. <br> <br>

3. ***Retrieval Multiplies Pre-Training Value:  <br>Apple and Stanford show retrieving from pre-training corpora at test time yields ~5× compute-equivalent gains on MMLU/Math-500/SimpleQA; adding test-time parsing boosts LLaMA-3.1-8B by 10 pp on MMLU, revealing pre-training leaves significant dataset value untapped.*** <br> <br>
   Nov 6, Apple and Stanford Uni published a [paper](https://arxiv.org/pdf/2511.04234) “Reusing Pre-Training Data at Test Time is a Compute Multiplier”. Large language models learn from their vast pre-training corpora, gaining the ability to solve an ever increasing variety of tasks; yet although researchers work to improve these datasets, there is little effort to understand how efficient the pre-training apparatus is at extracting ideas and knowledge from the data. This work uses retrieval augmented generation along with test-time compute as a way to quantify how much dataset value was left behind by the process of pre-training, and how this changes across scale. The study demonstrates that pre-training then retrieving from standard and largely open-sourced datasets results in significant accuracy gains in MMLU, Math-500, and SimpleQA, which persist through decontamination. For MMLU the work observes that retrieval acts as a ~5x compute multiplier versus pre-training alone. The work shows that these results can be further improved by leveraging additional compute at test time to parse the retrieved context, demonstrating a 10 percentage point improvement on MMLU for the public LLaMA 3.1 8B model. Overall, the results suggest that today's pre-training methods do not make full use of the information in existing pre-training datasets, leaving significant room for progress. <br> <br>

5. ***Jr. AI Scientist Mimics Novice Research:  <br>Tokyo’s Jr. AI Scientist analyzes baseline papers, formulates hypotheses, runs experiments, and writes papers using coding agents; it scores higher than fully automated systems in AI Reviewers and Agents4Science, but reviews highlight risks and limitations needing future safeguards.*** <br> <br>
   Nov 6, Uni of Tokyo published a [paper](https://arxiv.org/pdf/2511.04583) “Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper”. Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. The study develops Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, the work conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, the study identifies important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, the study comprehensively reports various risks identified during development. The authors hope these insights will deepen understanding of current progress and risks in AI Scientist development. <br> <br>

7. ***Strong Lottery Tickets in MHA:  <br>Tokyo and NTT prove randomly initialized multi-head attention with hidden dim O(d log(Hd^{3/2})) contains strong lottery tickets approximating any target MHA; extending to normalization-free transformers, empirical error drops exponentially with hidden size.*** <br> <br>
   Nov 6, Inst of Sci Tokyo and NTT published a [paper](https://arxiv.org/pdf/2511.04217) “The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms”. The strong lottery ticket hypothesis (SLTH) conjectures that high-performing subnetworks, called strong lottery tickets (SLTs), are hidden in randomly initialized neural networks. Although recent theoretical studies have established the SLTH across various neural architectures, the SLTH for transformer architectures still lacks theoretical understanding. In particular, the current theory of the SLTH does not yet account for the multi-head attention (MHA) mechanism, a core component of transformers. To address this gap, the work introduces a theoretical analysis of the existence of SLTs within MHAs. The study proves that, if a randomly initialized MHA of H heads and input dimension d has the hidden dimension O(dlog(Hd^{3/2})) for the key and value, it contains an SLT that approximates an arbitrary MHA with the same input dimension with high probability. Furthermore, by leveraging this theory for MHAs, the study extends the SLTH to transformers without normalization layers. The work empirically validates the theoretical findings, demonstrating that the approximation error between the SLT within a source model (MHA and transformer) and an approximate target counterpart decreases exponentially by increasing the hidden dimension of the source model. <br> <br>

9. ***Theory Explains “Less is More” Curation:  <br>Concordia, Meta, and Mila derive exact scaling laws under imperfect oracle curation, revealing phase transitions where small curated subsets outperform full data; validated on ImageNet and LLM math, explaining LIMO/s1 successes and mitigating collapse.*** <br> <br>
    Nov 5, Concordia Uni, Meta and Mila published a [paper](https://arxiv.org/pdf/2511.03492) “Why Less is More (Sometimes): A Theory of Data Curation”. This paper introduces a theoretical framework to resolve a central paradox in modern machine learning: When is it better to use less data? This question has become critical as classical scaling laws suggesting “more is more” (Sun et al., 2025) are challenged by methods like LIMO (“less is more”) and s1 (Ye et al., 2025; Muenighoff et al., 2025), which achieve superior performance with small, aggressively curated datasets. Here, the authors work study data curation strategies where an imperfect oracle selects the training examples according to their difficulty and correctness. The results provide exact scaling law curves for test error under both label-agnostic and label-aware curation rules, revealing when and why keeping only a subset of data can improve generalization. In contrast to classical scaling laws, the study shows that under certain conditions, small curated datasets can outperform full datasets, and the work provides analytical conditions for this by deriving precise phase transition curves tied to data size and quality. The study validates these theoretical claims with empirical results on ImageNet, confirming the predictions about when curation improves accuracy and can even mitigate model collapse. Furthermore, the framework provides a principled explanation for the contradictory curation strategies recently observed in LLM mathematical reasoning. <br> <br>

11. ***DLMs Excel Under Data Constraint:  <br>NUS shows diffusion LMs beat autoregressives when unique data is limited via any-order modeling, dense bidirectional compute, and Monte Carlo augmentation; a 1.7B DLM overtakes matched AR coder, and 1B DLM hits 56% HellaSwag/33% MMLU on 1B tokens.*** <br> <br>
    Nov 5, National Uni of Singapore et al published a [paper](https://arxiv.org/pdf/2511.03276) “Diffusion Language Models are Super Data Learners”. Under strictly controlled pre-training settings, the study observes a Crossover: when unique data is limited, diffusion language models (DLMs) consistently surpass autoregressive (AR) models by training for more epochs. The crossover shifts later with more or higher-quality data, earlier with larger models, and persists across dense and sparse architectures. The study attributes the gains to three compounding factors: (1) any-order modeling, (2) super-dense compute from iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation; input or parameter noise improves AR under data constraint but cannot close the gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B unique Python tokens overtakes an AR coder trained with strictly matched settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag and > 33% on MMLU using only 1B tokens, without any special tricks, just by repeating standard pre-training data. The study also shows that rising validation cross-entropy does not imply degraded downstream performance in this regime. https://github.com/JinjieNi/dlms-are-super-data-learners <br> <br>

13. ***OpenHands SDK for Production Agents:  <br>OpenHands redesigns its 64k-star framework into a flexible SDK with sandboxed execution, multi-LLM routing, security analysis, and UI integration; it achieves strong SWE-Bench/GAIA results while enabling rapid prototyping and scalable deployment.*** <br> <br>
    Nov 5, Openhands published a [paper](https://arxiv.org/pdf/2511.03690) “The OpenHands Software Agent SDK: A Composable and Extensible Foundation for Production Agents”. Agents are now used widely in the process of software development, but building production-ready software engineering agents is a complex task. Deploying software agents effectively requires flexibility in implementation and experimentation, reliable and secure execution, and interfaces for users to interact with agents. The work presents the OpenHands Software Agent SDK, a toolkit for implementing software development agents that satisfy these desiderata. This toolkit is a complete architectural redesign of the agent components of the popular OpenHands framework for software development agents, which has 64k+ GitHub stars. To achieve flexibility, the work designs a simple interface for implementing agents that requires only a few lines of code in the default case, but is easily extensible to more complex, full-featured agents with features such as custom tools, memory management, and more. For security and reliability, it delivers seamless local-to-remote execution portability, integrated REST/WebSocket services. For interaction with human users, it can connect directly to a variety of interfaces, such as visual workspaces (VS Code, VNC, browser), command-line interfaces, and APIs. Compared with existing SDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native sandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and built-in security analysis. Empirical results on SWE-Bench Verified and GAIA benchmarks demonstrate strong performance. Put together, these elements allow the OpenHands Software Agent SDK to provide a practical foundation for prototyping, unlocking new classes of custom applications, and reliably deploying agents at scale. https://github.com/OpenHands/software-agent-sdk <br> <br>

15. ***Kosmos Automates 12-Hour Discovery:  <br>Edison and Oxford’s Kosmos runs 200+ rollouts (42k LOC, 1.5k papers) via data-analysis and literature agents sharing a world model; 79.4% of report statements accurate, each 20-cycle run equals ~6 months human work, yielding 7 cross-domain discoveries.*** <br> <br>
    Nov 5, Edison Sci Inc, Uni of Oxford et al published a [paper](https://arxiv.org/pdf/2511.02824) “Kosmos: An AI Scientist for Autonomous Discovery”. Data-driven scientific discovery requires iterative cycles of literature search, hypothesis generation, and data analysis. Substantial progress has been made towards AI agents that can automate scientific research, but all such agents remain limited in the number of actions they can take before losing coherence, thus limiting the depth of their findings. Here the study presents Kosmos, an AI scientist that automates data-driven discovery. Given an open-ended objective and a dataset, Kosmos runs for up to 12 hours performing cycles of parallel data analysis, literature search, and hypothesis generation before synthesizing discoveries into scientific reports. Unlike prior systems, Kosmos uses a structured world model to share information between a data analysis agent and a literature search agent. The world model enables Kosmos to coherently pursue the specified objective over 200 agent rollouts, collectively executing an average of 42,000 lines of code and reading 1,500 papers per run. Kosmos cites all statements in its reports with code or primary literature, ensuring its reasoning is traceable. Independent scientists found 79.4% of statements in Kosmos reports to be accurate, and collaborators reported that a single 20-cycle Kosmos run performed the equivalent of 6 months of their own research time on average. Furthermore, collaborators reported that the number of valuable scientific findings generated scales linearly with Kosmos cycles (tested up to 20 cycles). The study highlights seven discoveries made by Kosmos that span metabolomics, materials science, neuroscience, and statistical genetics. Three discoveries independently reproduce findings from preprinted or unpublished manuscripts that were not accessed by Kosmos at runtime, while four make novel contributions to the scientific literature. <br> <br>

17. ***CostBench Tests Economic Planning:  <br>HKUST, UIUC, and Tsinghua’s CostBench evaluates multi-turn cost-optimal tool-use in travel planning with dynamic events; GPT-5 scores <75% exact match on hardest tasks, dropping ~40% under dynamics, exposing gaps in economic rationality.*** <br> <br>
    Nov 4, HKUST, UICU and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2511.02734) “CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents”. Current evaluations of Large Language Model (LLM) agents primarily emphasize task completion, often overlooking resource efficiency and adaptability. This neglects a crucial capability: agents' ability to devise and adjust cost-optimal plans in response to changing environments. To bridge this gap, the study introduces CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities. Situated in the travel-planning domain, CostBench comprises tasks solvable via multiple sequences of atomic and composite tools with diverse, customizable costs. It also supports four types of dynamic blocking events, such as tool failures and cost changes, to simulate real-world unpredictability and necessitate agents to adapt in real time. Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and performance further dropping by around 40% under dynamic conditions. By diagnosing these weaknesses, CostBench lays the groundwork for developing future agents that are both economically rational and robust. https://github.com/JiayuJeff/CostBench <br> <br>

19. ***Temporal Biases for Interpretability:  <br>Google’s Temporal Feature Analysis decomposes activations into predictable context and novel residual, outperforming SAEs on garden-path sentences and event boundaries by capturing non-stationary language dynamics.*** <br> <br>
    Nov 3, Google et al published a [paper](https://arxiv.org/pdf/2511.01836) “Priors in Time: Missing Inductive Biases for Language Model Interpretability”. Recovering meaningful concepts from language model activations is a central aim of interpretability. While existing feature extraction methods aim to identify concepts that are independent directions, it is unclear if this assumption can capture the rich temporal structure of language. Specifically, via a Bayesian lens, the work demonstrates that Sparse Autoencoders (SAEs) impose priors that assume independence of concepts across time, implying stationarity. Meanwhile, language model representations exhibit rich temporal dynamics, including systematic growth in conceptual dimensionality, context-dependent correlations, and pronounced non-stationarity, in direct conflict with the priors of SAEs. Taking inspiration from computational neuroscience, the work introduces a new interpretability objective -- Temporal Feature Analysis -- which possesses a temporal inductive bias to decompose representations at a given time into two parts: a predictable component, which can be inferred from the context, and a residual component, which captures novel information unexplained by the context. Temporal Feature Analyzers correctly parse garden path sentences, identify event boundaries, and more broadly delineate abstract, slow-moving information from novel, fast-moving information, while existing SAEs show significant pitfalls in all the above tasks. Overall, the results underscore the need for inductive biases that match the data in designing robust interpretability tools. https://github.com/eslubana/TemporalFeatureAnalysis <br> <br>

21. ***Trove Simplifies Dense Retrieval:  <br>Brown’s Trove loads/processes datasets on-the-fly (2.6× memory savings), supports custom components, multi-node mining, and zero-overhead inference, accelerating exploratory retrieval research.*** <br> <br>
    Nov 3, Brown Uni published a [paper](https://arxiv.org/pdf/2511.01857) “Trove: A Flexible Toolkit for Dense Retrieval”. The work introduces Trove, an easy-to-use open-source retrieval toolkit that simplifies research experiments without sacrificing flexibility or speed. For the first time, the study introduces efficient data management features that load and process (filter, select, transform, and combine) retrieval datasets on the fly, with just a few lines of code. This gives users the flexibility to easily experiment with different dataset configurations without the need to compute and store multiple copies of large datasets. Trove is highly customizable: in addition to many built-in options, it allows users to freely modify existing components or replace them entirely with user-defined objects. It also provides a low-code and unified pipeline for evaluation and hard negative mining, which supports multi-node execution without any code changes. Trove's data management features reduce memory consumption by a factor of 2.6. Moreover, Trove's easy-to-use inference pipeline incurs no overhead, and inference times decrease linearly with the number of available nodes. Most importantly, the study demonstrates how Trove simplifies retrieval experiments and allows for arbitrary customizations, thus facilitating exploratory research. https://ir-trove.dev/ <br> <br>

23. ***LMs Fail Epistemic Distinctions:  <br>Stanford’s KaBLE benchmark shows all 24 LMs confuse belief/knowledge/fact, with first-person false beliefs crashing accuracy (GPT-4o 98→64%, DeepSeek R1 90→14%) and inconsistent factivity understanding.*** <br> <br>
    Nov 3, Stanford Uni et al published a [paper](https://www.nature.com/articles/s42256-025-01113-8) “Language models cannot reliably distinguish belief from knowledge and fact”. As language models (LMs) increasingly infiltrate into high-stakes domains such as law, medicine, journalism and science, their ability to distinguish belief from knowledge, and fact from fiction, becomes imperative. Failure to make such distinctions can mislead diagnoses, distort judicial judgments and amplify misinformation. The work evaluates 24 cutting-edge LMs using a new KaBLE benchmark of 13,000 questions across 13 epistemic tasks. The findings reveal crucial limitations. In particular, all models tested systematically fail to acknowledge first-person false beliefs, with GPT-4o dropping from 98.2% to 64.4% accuracy and DeepSeek R1 plummeting from over 90% to 14.4%. Further, models process third-person false beliefs with substantially higher accuracy (95% for newer models; 79% for older ones) than first-person false beliefs (62.6% for newer; 52.5% for older), revealing a troubling attribution bias. The work also finds that, while recent models show competence in recursive knowledge tasks, they still rely on inconsistent reasoning strategies, suggesting superficial pattern matching rather than robust epistemic understanding. Most models lack a robust understanding of the factive nature of knowledge, that knowledge inherently requires truth. These limitations necessitate urgent improvements before deploying LMs in high-stakes domains where epistemic distinctions are crucial. <br> <br>

25. ***SLAP Discovers Physical Shortcuts:  <br>Princeton and CMU’s SLAP learns new TAMP options via RL on abstract graphs, cutting plan lengths >50% and enabling improvisations (slap/wiggle/wipe) that outperform planning/RL baselines across robotic tasks.*** <br> <br>
    Nov 2, Princeton Uni and CMU published a [paper](https://arxiv.org/pdf/2511.01107v1) “SLAP: Shortcut Learning for Abstract Planning”. Long-horizon decision-making with sparse rewards and continuous states and actions remains a fundamental challenge in AI and robotics. Task and motion planning (TAMP) is a model-based framework that addresses this challenge by planning hierarchically with abstract actions (options). These options are manually defined, limiting the agent to behaviors that human engineers know how to program (pick, place, move). This work proposes Shortcut Learning for Abstract Planning (SLAP), a method that leverages existing TAMP options to automatically discover new ones. The key idea is to use model-free reinforcement learning (RL) to learn shortcuts in the abstract planning graph induced by the existing options in TAMP. Without any additional assumptions or inputs, shortcut learning leads to shorter solutions than pure planning, and higher task success rates than flat and hierarchical RL. Qualitatively, SLAP discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that differ significantly from the manually-defined ones. In experiments in four simulated robotic environments, the work shows that SLAP solves and generalizes to a wide range of tasks, reducing overall plan lengths by over 50% and consistently outperforming planning and RL baselines. https://github.com/isabelliu0/SLAP <br> <br>

27. ***Pre-GenAI Labor-Capital Decoupling:  <br>MBI Deep Dives documents ~$1T revenue added by megacaps with ~100k jobs; next $1T may need even fewer, raising OpenAI $200B 2030 odds and highlighting a “smiling-curve” economy squeezing the middle.*** <br> <br>
    Nov 2, MBI Deep Dives published an [article](https://www.mbi-deepdives.com/the-great-decoupling-of-labor-and-capital/) “The Great Decoupling of Labor and Capital”. The article shows that megacap tech firms have added trillions in revenue with only ~100k net new jobs—all before GenAI. Apple hit its first $100B with 60k employees (2011) and the latest $100B with just 17k; Alphabet needed 76k for its first $100B and 11k for the most recent; Microsoft took 124k then 7k; Meta 63k then ~21k; Amazon added $200B with 36k; Nvidia will cross $200B with ~6-8k more; Walmart grew $200B in a decade with flat headcount and plans three more years of zero growth. This pre-AI trend means the next $1T of incremental revenue across these firms may require even fewer workers. The author now sees OpenAI’s $200B run-rate by 2030 as a realistic top-1–2% outcome (up from top-0.1%), citing Meta Reels’ 50× scale-up in three years. “Us-vs-them” narratives miss how market expansion and massive consumer surplus (free smiles on Reels, instant knowledge from ChatGPT) let incumbents thrive despite share loss. The emerging “smiling-curve” economy features low-headcount tech giants at one end, empowered creators/sellers at the other, and a squeezed middle. Investment takeaway: AI accelerates an existing decoupling; update growth distributions upward for AI-native firms and expect interim narrative swings but a steady destination. <br> <br>

29. ***Glia Designs Systems Autonomously:  <br>MIT’s Glia uses multi-agent LLM workflow with empirical grounding to invent GPU-cluster routing/scheduling/auto-scaling algorithms at human-expert level in hours, exposing novel workload insights.*** <br> <br>
    Oct 31, MIT published a [paper](https://arxiv.org/abs/2510.27176) “Glia: A Human-Inspired AI for Automated Systems Design and Optimization”. Can an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? The study presents Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that grounds abstract reasoning in empirical feedback. Unlike prior ML-for-systems methods that optimize black-box policies, Glia generates interpretable designs and exposes its reasoning process. When applied to a distributed GPU cluster for LLM inference, it produces new algorithms for request routing, scheduling, and auto-scaling that perform at human-expert levels in significantly less time, while yielding novel insights into workload behavior. The results suggest that by combining reasoning LLMs with structured experimentation, an AI can produce creative and understandable designs for complex systems problems. <br> <br>

31. ***Higher-Order Linear Attention:  <br>Princeton and UCLA’s HLA enables second-order interactions in constant state and linear time via prefix statistics, with causal masking and chunk-parallel training, merging attention expressivity with SSM efficiency.*** <br> <br>
    Oct 31, Princeton Uni and UCLA published a [paper](https://www.arxiv.org/pdf/2510.27258) “Higher-order Linear Attention”. The quadratic cost of scaled dot-product attention is a central obstacle to scaling autoregressive language models to long contexts. Linear-time attention and State Space Models (SSMs) provide scalable alternatives but are typically restricted to first-order or kernel-based approximations, which can limit expressivity. The work introduces Higher-order Linear Attention (HLA), a causal, streaming mechanism that realizes higher interactions via compact prefix sufficient statistics. In the second-order case, HLA maintains a constant-size state and computes per-token outputs in linear time without materializing any matrices. The work gives closed-form streaming identities, a strictly causal masked variant using two additional summaries, and a chunk-parallel training scheme based on associative scans that reproduces the activations of a serial recurrence exactly. The work further outline extensions to third and higher orders. Collectively, these results position HLA as a principled, scalable building block that combines attention-like, data-dependent mixing with the efficiency of modern recurrent architectures. https://github.com/yifanzhang-pro/HLA <br> <br>

33. ***Universal Video Retrieval via Pyramid:  <br>HKUST’s UVRB (16 datasets) diagnoses generalization; 1.55M synthetic pairs and Modality Pyramid curriculum train GVE-7B to SOTA zero-shot across tasks/domains, revealing partial-relevance as key overlooked scenario.*** <br> <br>
    Oct 31, AI Thrust, HKUST published a [paper](https://arxiv.org/pdf/2510.27571) “Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum”. The prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of a diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, the work introduces a framework built on the co-design of evaluation, data, and modeling. First, the study establishes the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRB's diagnostics, the study introduces a scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, the study devises the Modality Pyramid, a curriculum that trains the General Video Embedder (GVE) by explicitly leveraging the latent interconnections within the diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, the analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is a dominant but overlooked scenario. Overall, the co-designed framework provides a practical path to escape the limited scope and advance toward truly universal video retrieval. https://huggingface.co/Alibaba-NLP/GVE-7B <br> <br>

35. ***Denario Generates Cross-Disciplinary Papers:  <br>Flatiron, Princeton, and Cambridge’s Denario produces expert-scored papers in 10+ fields (79.4% accurate), combining ideas (e.g., quantum ML on astrophysics data); code/demo released for AI-driven research.*** <br> <br>
    Oct 30, Flatiron Inst, Princeton Uni, Uni of Cambridge et al. published a [paper](https://arxiv.org/pdf/2510.26887) “The Denario project: Deep knowledge AI agents for scientific discovery”. The work presents Denario, an AI multi-agent system designed to serve as a scientific research assistant. Denario can perform many different tasks, such as generating ideas, checking the literature, developing research plans, writing and executing code, making plots, and drafting and reviewing a scientific paper. The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis using Cmbagent as a deep-research backend. The work describes in detail Denario and its modules, and illustrate its capabilities by presenting multiple AI-generated papers generated by it in many different scientific disciplines such as astrophysics, biology, biophysics, biomedical informatics, chemistry, material science, mathematical physics, medicine, neuroscience and planetary science. Denario also excels at combining ideas from different disciplines, and illustrates this by showing a paper that applies methods from quantum physics and machine learning to astrophysical data. The study reports the evaluations performed on these papers by domain experts, who provided both numerical scores and review-like feedback. The study then highlights the strengths, weaknesses, and limitations of the current system. Finally, the paper discusses the ethical implications of AI-driven research and reflect on how such technology relates to the philosophy of science. The authors publicly release the code https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and the full app will be deployed on the cloud. <br> <br>

37. ***FP16 Eliminates RL Mismatch:  <br>SeaAI and NUS show BF16 rounding breaks training-inference consistency; FP16 restores stability, speeds convergence, and boosts performance across RL algorithms/frameworks with minimal code changes.*** <br> <br>
    Oct 30, SeaAI and National Uni of Singapore published a [paper](https://arxiv.org/pdf/2510.26788) “Defeating the Training-Inference Mismatch via FP16”. Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, the study shows that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dynamic range, introduces large rounding errors that breaks the consistency between training and inference. This work demonstrates that simply reverting to FP16 effectively eliminates this mismatch. The change is simple, fully supported by modern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks. The authors hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning. https://github.com/sail-sg/Precision-RL <br> <br>

39. ***Value Drifts Fixed in SFT:  <br>Mila and McGill find SFT sets LLM values early; preference optimization rarely shifts them; synthetic data reveals algorithm-specific alignment outcomes, guiding data/algorithm choice for value robustness.*** <br> <br>
    Oct 30, Mila, McGill Uni et al published a [paper](https://arxiv.org/pdf/2510.26707) “Value Drifts: Tracing Value Alignment During LLM Post-Training”. As LLMs occupy an increasingly important role in society, they are more and more confronted with questions that require them not only to draw on their general knowledge but also to align with certain human value systems. Therefore, studying the alignment of LLMs with human values has become a crucial field of inquiry. Prior work, however, mostly focuses on evaluating the alignment of fully trained models, overlooking the training dynamics by which models learn to express human values. This study investigates how and at which stage value alignment arises during the course of a model's post-training. The analysis disentangles the effects of post-training algorithms and datasets, measuring both the magnitude and time of value drifts during training. Experimenting with Llama-3 and Qwen-3 models of different sizes and popular supervised fine-tuning (SFT) and preference optimization datasets and algorithms, the work finds that the SFT phase generally establishes a model's values, and subsequent preference optimization rarely re-aligns these values. Furthermore, using a synthetic preference dataset that enables controlled manipulation of values, the study finds that different preference optimization algorithms lead to different value alignment outcomes, even when preference data is held constant. The findings provide actionable insights into how values are learned during post-training and help to inform data curation, as well as the selection of models and algorithms for preference optimization to improve model alignment to human values. <br> <br>

41. ***cmbagent Automates PhD-Level Science:  <br>Cambridge’s 30-agent system plans, codes, critiques, and executes full cosmology analysis (supernovae parameters) with SOTA benchmark performance; open-source with web demo for autonomous discovery.*** <br><br>
    Jul 11, Uni of Cambridge et al published a [paper](https://arxiv.org/pdf/2507.07257) “Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery”. The study presents a multi-agent system for automation of scientific research tasks, cmbagent (https://github.com/CMBAgents/cmbagent). The system is formed by about 30 Large Language Model (LLM) agents and implements a Planning & Control strategy to orchestrate the agentic workflow, with no human-in-the-loop at any point. Each agent specializes in a different task (performing retrieval on scientific papers and codebases, writing code, interpreting results, critiquing the output of other agents) and the system is able to execute code locally. The study successfully applies cmbagent to carry out a PhD level cosmology task (the measurement of cosmological parameters using supernova data) and evaluate its performance on two benchmark sets, finding superior performance over state-of-the-art LLMs. The source code https://github.com/CMBAgents/cmbagent, demonstration videos https://www.youtube.com/@cmbagent, and the system is deployed on HuggingFace https://huggingface.co/spaces/astropilot-ai/cmbagent

<br><br><br>

***Nov 2nd, 2025***

1. ***Geometric Memory in Transformers:  <br>CMU and Google’s paper reveals that Transformers synthesize a global geometric structure of atomic facts during training, beyond local co-occurrences, simplifying complex reasoning into a one-step task; this geometry emerges naturally via spectral bias (linked to Node2Vec), even without succinctness advantages, urging a reevaluation of parametric memory in knowledge acquisition and unlearning.*** <br> <br>
   Oct 30, CMU and Google published a [paper](https://arxiv.org/pdf/2510.26745) “Deep sequence models tend to memorize geometrically; it is unclear why”. In sequence modeling, the parametric memory of atomic facts has been predominantly abstracted as a brute-force lookup of co-occurrences between entities. The study contrasts this associative view against a geometric view of how memory is stored. The study begins by isolating a clean and analyzable instance of Transformer reasoning that is incompatible with memory as strictly a storage of the local co-occurrences specified during training. Instead, the model must have somehow synthesized its own geometry of atomic facts, encoding global relationships between all entities, including non-co-occurring ones. This in turn has simplified a hard reasoning task involving an l-fold composition into an easy-to-learn 1-step geometric task. From this phenomenon, the work extracts fundamental aspects of neural embedding geometries that are hard to explain. The authors argue that the rise of such a geometry, despite optimizing over mere local associations, cannot be straightforwardly attributed to typical architectural or optimizational pressures. Counterintuitively, an elegant geometry is learned even when it is not more succinct than a brute-force lookup of associations. Then, by analyzing a connection to Node2Vec, the work demonstrates how the geometry stems from a spectral bias that -- in contrast to prevailing theories -- indeed arises naturally despite the lack of various pressures. This analysis also points to practitioners a visible headroom to make Transformer memory more strongly geometric. The authors hope the geometric view of parametric memory encourages revisiting the default intuitions that guide researchers in areas like knowledge acquisition, capacity, discovery and unlearning. <br> <br>

3. ***AsyncThink for Agentic Collaboration:  <br>Microsoft introduces AsyncThink, an asynchronous reasoning paradigm where an organizer dynamically assigns sub-queries to worker LLMs, merges insights, and optimizes via RL, achieving 28% lower latency and higher accuracy on math reasoning while generalizing to unseen tasks, heralding the era of collaborative agentic organizations.*** <br> <br>
   Oct 30, Microsoft published a [paper](https://arxiv.org/pdf/2510.26658) “The Era of Agentic Organization: Learning to Organize with Language Models”. The work envisions a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, the study introduces asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large language models, which organizes the internal thinking process into concurrently executable structures. Specifically, the work proposes a thinking protocol where an organizer dynamically assigns sub-queries to workers, merges intermediate knowledge, and produces coherent solutions. More importantly, the thinking structure in this protocol can be further optimized through reinforcement learning. Experiments demonstrate that AsyncThink achieves 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning. Moreover, AsyncThink generalizes its learned asynchronous thinking capabilities, effectively tackling unseen tasks without additional training. <br> <br>

5. ***RedLLM vs. Decoder-Only Scaling:  <br>Google revisits encoder-decoder LLMs (RedLLM) with modern training recipes, showing comparable scaling and context extrapolation to decoder-only models; after instruction tuning, RedLLM matches or exceeds performance on downstream tasks with superior inference efficiency, advocating renewed exploration of this overlooked architecture.*** <br> <br>
   Oct 30, Google published a [paper](https://arxiv.org/pdf/2510.26622) “Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language Model”. Recent large language model (LLM) research has undergone an architectural shift from encoder-decoder modeling to nowadays the dominant decoder-only modeling. This rapid transition, however, comes without a rigorous comparative analysis especially from the scaling perspective, raising concerns that the potential of encoder-decoder models may have been overlooked. To fill this gap, the work revisits encoder-decoder LLM (RedLLM), enhancing it with recent recipes from decoder-only LLM (DecLLM). The work conducts a comprehensive comparison between RedLLM, pretrained with prefix language modeling (LM), and DecLLM, pretrained with causal LM, at different model scales, ranging from 150M to 8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for instruction tuning. Experiments show that RedLLM produces compelling scaling properties and surprisingly strong performance. While DecLLM is overall more compute-optimal during pretraining, RedLLM demonstrates comparable scaling and context length extrapolation capabilities. After instruction tuning, RedLLM achieves comparable and even better results on various downstream tasks while enjoying substantially better inference efficiency. The authors hope the findings could inspire more efforts on re-examining RedLLM, unlocking its potential for developing powerful and efficient LLMs. <br> <br>

7. ***SRL for Multi-Step Reasoning:  <br>Google and UCLA propose Supervised Reinforcement Learning (SRL), generating internal reasoning monologues before actions with step-wise rewards from expert trajectories, enabling small LLMs to solve complex problems unlearnable by SFT or RLVR; combining SRL with RLVR yields top performance on reasoning and software engineering tasks.*** <br> <br>
   Oct 29, Google and UCLA published a [paper](https://arxiv.org/pdf/2510.25992) “Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning”. Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, the study proposes Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical "actions". SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model's actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs. <br> <br>

9. ***GPTOpt for Black-Box Optimization:  <br>MIT’s GPTOpt fine-tunes LLMs on synthetic Bayesian Optimization data, enabling sample-efficient global optimization of expensive black-box functions without domain tuning, outperforming classical methods and showcasing LLMs’ advanced numerical reasoning capabilities.*** <br> <br>
    Oct 29, MIT published a [paper](https://arxiv.org/abs/2510.25404) “GPTOpt: Towards Efficient LLM-Based Black-Box Optimization”. Global optimization of expensive, derivative-free black-box functions demands extreme sample efficiency. Classical methods such as Bayesian Optimization (BO) can be effective, but they often require careful parameter tuning to each application domain. At the same time, Large Language Models (LLMs) have shown broad capabilities, yet state-of-the-art models remain limited in solving continuous black-box optimization tasks. The study introduces GPTOpt, an LLM-based optimization method that equips LLMs with continuous black-box optimization capabilities. By fine-tuning large language models on extensive synthetic datasets derived from diverse BO parameterizations, GPTOpt leverages LLM pre-training to generalize across optimization tasks. On a variety of black-box optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting the capacity of LLMs for advanced numerical reasoning and introducing a flexible framework for global optimization without parameter tuning. <br> <br>

11. ***Relative Scaling Laws:  <br>Stanford, OpenAthena, and Georgia Tech introduce relative scaling laws tracking performance gaps across subpopulations (e.g., dialects, AI risks), finding scaling narrows academic disparities but widens others; releasing 255 IsoFLOP-trained models to support robustness prioritization.*** <br> <br>
    Oct 28, Stanford Uni, OpenAthena and Jeorgia Inst of Tech published a [paper](https://arxiv.org/pdf/2510.24626) “Relative Scaling Laws for LLMs”. Scaling laws describe how language models improve with additional data, parameters, and compute. While widely used, they are typically measured on aggregate test sets. Aggregate evaluations yield clean trends but average over heterogeneous subpopulations, obscuring performance disparities. The work introduces relative scaling laws, which track how performance gaps between test distributions evolve with scale rather than focusing solely on absolute error. Using 255 decoder-only Transformers trained under matched-compute (IsoFLOP) budgets from -- FLOPs on standard pretraining datasets, the study finds diverse trajectories: academic domains on MMLU converge toward parity; regional English dialects shift depending on population size; and clusters of AI risk behaviours split, with capability- and influence-related risks increasing during pretraining while adversarial risks do not. These results show that although scaling improves overall performance, it is not a universal equalizer. To support further study, the authors release all model checkpoints from this work to enable practitioners to measure relative alongside traditional scaling laws, in order to better prioritize robustness challenges in light of the bitter lesson. https://github.com/marin-community/marin/blob/main/experiments/isoflop_sweep.py, https://github.com/Helw150/relative-scaling-laws <br> <br>

13. ***Consistent Behavioral Phases:  <br>MIT and UCSD show autoregressive LMs (across Transformer, Mamba, RWKV architectures and datasets) follow universal pretraining phases, with 98% of word-level behavior explained by unigram, n-gram, and semantic heuristics, indicating a shared learning trajectory regardless of model details.*** <br> <br>
    Oct 28, MIT and UCSD published a [paper](https://www.arxiv.org/pdf/2510.24963) “Language Model Behavioral Phases are Consistent Across Architecture, Training Data, and Scale”. The study shows that across architecture (Transformer vs. Mamba vs. RWKV), training dataset (OpenWebText vs. The Pile), and scale (14 million parameters to 12 billion parameters), autoregressive language models exhibit highly consistent patterns of change in their behavior over the course of pretraining. Based on the analysis of over 1,400 language model checkpoints on over 110,000 tokens of English, the work finds that up to 98% of the variance in language model behavior at the word level can be explained by three simple heuristics: the unigram probability (frequency) of a given word, the -gram probability of the word, and the semantic similarity between the word and its context. Furthermore, the study finds consistent behavioral phases in all language models, with their predicted probabilities for words overfitting to those words' -gram probabilities for increasing  over the course of training. Taken together, these results suggest that learning in neural language models may follow a similar trajectory irrespective of model details. <br> <br>

15. ***ADP for Agent Fine-Tuning:  <br>CMU et al. launch Agent Data Protocol (ADP), a unified format converting 13 diverse agent datasets for standardized SFT, yielding ~20% average performance gains and SOTA on coding, browsing, and tool-use benchmarks, lowering barriers to scalable agent training.*** <br> <br>
    Oct 28, CMU et al published a [paper](https://arxiv.org/pdf/2510.24702) “Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents”. Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. This study argues that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, the work introduces the agent data protocol (ADP), a light-weight representation language that serves as an "interlingua" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, the study unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. The work performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training. https://agentdataprotocol.com/ <br> <br>

17. ***BeTaL for Dynamic Benchmarks:  <br>Snorkel AI and UW-Madison’s BeTaL uses LLMs to parameterize and optimize benchmark templates, creating dynamic, difficulty-targeted evaluations 2–4x more accurate than baselines, extending tau-bench and enabling adaptive assessment as models evolve.*** <br> <br>
    Oct 28, Snorkel AI and Uni of Wisconsin-Madison published a [paper](https://arxiv.org/pdf/2510.25039) “Automating Benchmark Design”. The rapid progress and widespread deployment of LLMs and LLM-powered agents has outpaced human ability to evaluate them. Hand-crafted, static benchmarks are the primary tool for assessing model capabilities, but these quickly become saturated. In contrast, dynamic benchmarks evolve alongside the models they evaluate, but are expensive to create and continuously update. To address these challenges, the work develops BeTaL (Benchmark Tuning with an LLM-in-the-loop), a framework that leverages environment design principles to automate the process of dynamic benchmark design. BeTaL works by parameterizing key design choices in base benchmark templates and uses LLMs to reason through the resulting parameter space to obtain target properties (such as difficulty and realism) in a cost-efficient manner. The study validates this approach on its ability to create benchmarks with desired difficulty levels. Using BeTaL, the work creates two new benchmarks and extend a popular agentic benchmark tau-bench. Extensive evaluation on these three tasks and multiple target difficulty levels shows that BeTaL produces benchmarks much closer to the desired difficulty, with average deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the baselines. <br> <br>

19. ***TIR-Judge with RL:  <br>Emory and Google’s TIR-Judge integrates code execution into LLM judges via RL, outperforming text-only judges by up to 7.7% on pointwise/pairwise tasks; a zero-shot variant matches distilled versions, proving tool-augmented judges can self-improve iteratively.*** <br> <br>
    Oct 27, Emory Uni and Google published a [paper](https://arxiv.org/pdf/2510.23038) “Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning”. Large Language Models (LLMs) are widely used as judges to evaluate response quality, providing a scalable alternative to human evaluation. However, most LLM judges operate solely on intrinsic text-based reasoning, limiting their ability to verify complex constraints or perform accurate computation. Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks, the work proposes TIR-Judge, an end-to-end RL framework for training LLM judges that integrates a code executor for precise evaluation. TIR-Judge is built on three principles: (i) diverse training across verifiable and non-verifiable domains, (ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii) iterative RL that bootstraps directly from the initial model without distillation. On seven public benchmarks, TIR-Judge surpasses strong reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and achieves listwise performance comparable to Claude-Opus-4 despite having only 8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled judge trajectories, matches the performance of distilled variants, demonstrating that tool-augmented judges can self-evolve through iterative reinforcement learning. <br> <br>

21. ***TDFlow for Test-Driven Repair:  <br>CMU, UCSD, and Johns Hopkins’ TDFlow uses sub-agents for test-driven repository repair, achieving 88.8% on SWE-Bench Lite (+27.8% over SOTA) and 94.3% on Verified with minimal test hacking, highlighting reproduction test generation as the final frontier for autonomous software engineering.*** <br> <br>
    Oct 27, CMU, UCSD and Johns Hopkins Uni published a [paper](https://arxiv.org/pdf/2510.23761) “TDFlow: Agentic Workflows for Test Driven Software Engineering”. The work introduces TDFlow, a novel test-driven agentic workflow that frames repository-scale software engineering as a test-resolution task, specifically designed to solve human-written tests. Given a set of tests, TDFlow repeatedly proposes, revises, and debugs repository-scale patches using precisely engineered sub-agents and tightly constrained tools. The workflow decomposes software engineering program repair into four components governed by respective sub-agents. This simple, forced decoupling of patch proposing, debugging, patch revision, and optional test generation (1) reduces long-context burden on any individual sub-agent, (2) focuses each sub-agent on specific, pre-defined sub-tasks, and (3) allows for specialized performance improvement on specific sub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on SWE-Bench Lite (an absolute improvement of 27.8% over the next best system) and 94.3% on SWE-Bench Verified. Manual inspection of the 800 TDFlow runs within SWE-Bench Lite and Verified uncover only 7 instances of test hacking, which were subsequently counted as failures. Furthermore, the work shows that the primary obstacle to human-level software engineering performance lies within writing successful reproduction tests. The work envisions a human-LLM interactive system powered by TDFlow where human developers write tests solved by LLM systems. Together, these results indicate that modern LLMs, when embedded in a narrowly engineered, test-driven workflow, already achieve human-level test resolution -- with the final frontier for fully autonomous repository repair being the accurate generation of valid reproduction tests. <br> <br>

23. ***ATOM for Temporal KGs:  <br>Lyon and GAUC’s ATOM extracts atomic facts from text, builds parallel temporal KGs with dual-time modeling, improving exhaustivity (~18%), stability (~17%), and latency (>90%) over baselines, enabling scalable, dynamic knowledge graph construction.*** <br> <br>
    Oct 26, Uni Claude Bernard Lyon and GAUC published a [paper](https://arxiv.org/pdf/2510.22590) “ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs”. In today's rapidly expanding data landscape, knowledge extraction from unstructured text is vital for real-time analytics, temporal inference, and dynamic memory frameworks. However, traditional static knowledge graph (KG) construction often overlooks the dynamic and time-sensitive nature of real-world data, limiting adaptability to continuous changes. Moreover, recent zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance on prebuilt ontologies often suffer from instability across multiple runs, as well as incomplete coverage of key facts. To address these challenges, the study introduces ATOM (AdapTive and OptiMized), a few-shot and scalable approach that builds and continuously updates Temporal Knowledge Graphs (TKGs) from unstructured texts. ATOM splits input documents into minimal, self-contained "atomic" facts, improving extraction exhaustivity and stability. Then, it constructs atomic TKGs from these facts while employing a dual-time modeling that distinguishes when information is observed from when it is valid. The resulting atomic TKGs are subsequently merged in parallel. Empirical evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17% better stability, and over 90% latency reduction compared to baseline methods, demonstrating a strong scalability potential for dynamic TKG construction. https://github.com/AuvaLab/itext2kg <br> <br>

25. ***ATLAS Multilingual Scaling:  <br>MIT, UW, Stanford, and Google’s ATLAS derives cross-lingual transfer matrices and optimal scaling laws for 400+ languages, identifying compute crossover points for pretraining vs. finetuning, democratizing scaling beyond English-first AI.*** <br> <br>
    Oct 24, MIT, Uni of Washington, Stanford Uni and Google published a [paper](https://arxiv.org/pdf/2510.22037) “ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality”. Scaling laws research has focused overwhelmingly on English -- yet the most prominent AI models explicitly serve billions of international users. This study undertakes the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. The study introduces the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws' out-of-sample generalization often by more than 0.3 R^2. The analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, the work derives a cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 x 38=1444 language pairs. Second, the work derives a language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, the authors identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. The authors hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale models -- beyond English-first AI. <br> <br>

27. ***Rational Agent Exploration:  <br>MIT and Harvard’s Code-enabled agents with Bayesian Experimental Design outperform humans and GPT-5 in Battleship/Guess Who? by 67–82% win rate at 1% cost, using Monte Carlo inference to balance exploration and action under information constraints.*** <br> <br>
    Oct 23, MIT and Harvard Unit published a [paper](https://www.arxiv.org/pdf/2510.20886) “Shoot First, Ask Questions Later? Building Rational Agents that Explore and Act Like People”. Many high-stakes applications of AI require forming data-driven hypotheses and making targeted guesses; e.g., in scientific and diagnostic settings. Given limited resources, to what extent do agents based on language models (LMs) act rationally? The study develops methods to benchmark and enhance agentic information-seeking, drawing on insights from human behavior. First, the study introduces a strategic decision-oriented dialogue task called Collaborative Battleship, in which a partially-informed Captain must balance exploration (asking questions) and action (taking shots), while a fully-informed Spotter must provide accurate answers under an information bottleneck. Compared to human players (N=42), the work finds that LM agents struggle to ground answers in context, generate informative questions, and select high-value actions. Next, to address these gaps, the study develops novel Monte Carlo inference strategies for LMs based on principles from Bayesian Experimental Design (BED). For Spotter agents, the approach boosts accuracy by up to 14.7% absolute over LM-only baselines; for Captain agents, it raises expected information gain (EIG) by up to 0.227 bits (94.2% of the achievable noise ceiling). Combined, these components yield sharper targeting (+0.303-0.374 F1), and enable weaker LMs, such as Llama-4-Scout, to outperform both humans (8% -> 82% win rate) and frontier models (0% -> 67% win rate vs. GPT-5) at ~1% of GPT-5's cost. The study replicates these findings on Guess Who? where the methods significantly boost accuracy (+28.3-42.4 p.p.), demonstrating their general applicability for building rational information-seeking agents. <br> <br>

29. ***CodeAdapt Outperforms RMs:  <br>MIT and Inria’s CodeAdapt elicits reasoning from standard LMs via code execution and few-shot bootstrapping, surpassing corresponding reasoning models by up to 35.7% across tasks while using 10–81% fewer tokens, proving code-augmented LMs as efficient, domain-general reasoners.*** <br> <br>
    Oct 23, MIT, Inria, and Harvard Uni published a [paper](https://www.arxiv.org/pdf/2510.20909) “Code-enabled language models can outperform reasoning models on diverse tasks”. Reasoning models (RMs), language models (LMs) trained with reinforcement learning to produce long-form natural language reasoning, have been remarkably successful, but they still require large amounts of computation and data to train, and can be slow and expensive to run. The study shows that standard instruct LMs can already be elicited to be strong reasoners at a level comparable to or even surpassing their corresponding RMs (e.g., DeepSeek V3 vs R1) without finetuning, across diverse domains from instruction following and creative generation to mathematical reasoning. This is achieved by CodeAdapt, a simple recipe that combines the CodeAct framework, where LMs interleave natural language reasoning with code execution in a multi-step fashion, with few-shot bootstrap in-context learning from as few as five training problems. Analyzing four matched pairs of LMs and RMs, the study finds that CodeAdapt enables three LMs to outperform the corresponding RMs on average over eight tasks (up to 22.9%) while being 10-81% more token efficient, and delivers superior performance on six tasks when averaged over the four models (up to 35.7%). Furthermore, the code-augmented reasoning traces display rich and varied problem-solving strategies. The findings support that (1) CodeAdapt-style learning and reasoning may be robust and domain general and (2) code-enabled LMs are cognitively grounded and powerful systems, potentially providing a strong foundation for in-weight reinforcement learning. <br> <br>

31. ***Single-Step LLM Adaptation:  <br>MIT and Haifa’s gradient-guided layer selection and multi-subspace factorization adapt LLMs using one gradient step on 100 samples, boosting accuracy by up to 24.6 pp without fine-tuning, exploiting prompting-style dominance over dataset size.*** <br> <br>
    Oct 23, MIT and Uni of Haifa published a [paper](https://arxiv.org/pdf/2510.20800) “Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples”. Recently, Sharma et al. suggested a method called Layer-SElective-Rank reduction (LASER) which demonstrated that pruning high-order components of carefully chosen LLM's weight matrices can boost downstream accuracy -- without any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each requiring full-dataset forward passes) makes it impractical for rapid deployment. The work demonstrates that this overhead can be removed and finds that: (i) Only a small, carefully chosen subset of matrices needs to be inspected -- eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's singular values pinpoints which matrices merit reduction, (iii) Increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to 24.6 percentage points, and finally, (iv) the study discovers that evaluating on just 100 samples rather than the full training data -- both for computing the indicative gradients and for measuring the final accuracy -- suffices to further reduce the search time; the work explains that as adaptation to downstream tasks is dominated by prompting style, not dataset size. As a result, the study shows that combining these findings yields a fast and robust adaptation algorithm for downstream tasks. Overall, with a single gradient step on 100 examples and a quick scan of the top candidate layers and factorization techniques, the study can adapt LLMs to new datasets -- entirely without fine-tuning. <br> <br>

33. ***On-Policy Data Mitigates Forgetting:  <br>Princeton shows RL retains prior knowledge better than SFT due to on-policy data’s mode-seeking behavior, preserving capabilities across instruction, knowledge, and arithmetic tasks; approximate on-policy data offers efficient forgetting mitigation.*** <br> <br>
    Oct 21, Princeton Uni published a [paper](https://arxiv.org/pdf/2510.18874) “Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting”. Adapting language models (LMs) to new tasks via post-training carries the risk of degrading existing capabilities -- a phenomenon classically known as catastrophic forgetting. In this paper, toward identifying guidelines for mitigating this phenomenon, the authors systematically compare the forgetting patterns of two widely adopted post-training methods: supervised fine-tuning (SFT) and reinforcement learning (RL). Experiments reveal a consistent trend across LM families (Llama, Qwen) and tasks (instruction following, general knowledge, and arithmetic reasoning): RL leads to less forgetting than SFT while achieving comparable or higher target task performance. To investigate the cause for this difference, the study considers a simplified setting in which the LM is modeled as a mixture of two distributions, one corresponding to prior knowledge and the other to the target task. The study identifies that the mode-seeking nature of RL, which stems from its use of on-policy data, enables keeping prior knowledge intact when learning the target task. The study then verifies this insight by demonstrating that the use on-policy data underlies the robustness of RL to forgetting in practical settings, as opposed to other algorithmic choices such as the KL regularization or advantage estimation. Lastly, as a practical implication, the results highlight the potential of mitigating forgetting using approximately on-policy data, which can be substantially more efficient to obtain than fully on-policy data. https://github.com/princeton-pli/retaining-by-doing <br> <br>

35. ***Injective Transformer Representations:  <br>Sapienza and EPFL prove Transformers are injective (lossless) at initialization and training, with zero collisions across billions of tests; SipIt reconstructs exact input from activations in linear time, enabling transparency and interpretability.*** <br> <br>
    Oct 21, Sapienza Uni of Rome, EPFL et al published a [paper](https://arxiv.org/pdf/2510.15511) “Language Models are Injective and Hence Invertible”. Transformer components such as non-linear activations and normalization are inherently non-injective, suggesting that different inputs could map to the same output and prevent exact recovery of the input from a model's representations. This study challenges this view. First, the study proves mathematically that transformer language models mapping discrete input sequences to their corresponding sequence of continuous representations are injective and therefore lossless, a property established at initialization and preserved during training. Second, the study confirms this result empirically through billions of collision tests on six state-of-the-art language models, and observe no collisions. Third, the study operationalizes injectivity: introducing SipIt, the first algorithm that provably and efficiently reconstructs the exact input text from hidden activations, establishing linear-time guarantees and demonstrating exact invertibility in practice. Overall, the work establishes injectivity as a fundamental and exploitable property of language models, with direct implications for transparency, interpretability, and safe deployment. <br> <br>

37. ***Gartner 2026 Tech Trends:  <br>Gartner’s Top 10 trends group into Architect (AI-native platforms, supercomputing, confidential computing), Synthesist (multiagent systems, domain-specific LMs, physical AI), and Sentinel (preemptive cybersecurity, digital provenance, AI security, geopatriation), forecasting 1–5 year impacts for AI-driven business transformation.*** <br> <br>
    Oct 10, Gartner [released](https://www.gartner.com/en/articles/top-technology-trends-2026) “The Gartner Top 10 Strategic Technology Trends for 2026”. Technology leaders face a pivotal year in 2026, where disruption, innovation and risk are accelerating at unprecedented speed. This year’s trends reflect the realities of an AI-powered, hyperconnected world where no single capability is enough.The Gartner Top 10 Strategic Technology Trends for 2026 are more than technology shifts — they are catalysts for business transformation, demanding a C-Level response.The trends are grouped into three strategic themes that reflect how organizations build, orchestrate and protect digital value: 1) The Architect focuses on foundational technologies like AI-native development platforms, AI supercomputing and confidential computing — all essential for scalable, secure digital transformation. 2) The Synthesist explores how organizations can orchestrate emerging technologies such as multiagent systems, domain-specific language models and physical AI to unlock new sources of business value. 3) The Sentinel addresses trust, governance and security through trends like preemptive cybersecurity, digital provenance, AI security platforms and geopatriation. Among the ten trends, Confidential computing, Multiagent systems, Physical AI, Digital provenance, AI security platforms and Geopatriation are happening in 1-3 year, the following four trends, AI-native development platforms, AI supercomputing platforms, Domain-specific language models, and Preemptive cybersecurity, are expecting to happen in 3-5 year.

 <br> <br> <br>

***Oct 26, 2025***

1. ***Telepathic Thought Communication in Multiagent Systems:  <br>CMU, Meta, and MBZUAI introduce "thought communication," a new paradigm enabling AI agents to interact directly mind-to-mind beyond natural language, formalizing it as a latent variable model to identify and leverage shared and private latent thoughts for enhanced collaborative intelligence.*** <br> <br>
   Oct 23, CMU, Meta and MBZUAI published a [paper](https://arxiv.org/pdf/2510.20733) “Thought Communication in Multiagent Collaboration”. Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect nature limits the potential of collective intelligence. While machines are not subject to these constraints, most LLM-based multi-agent systems still rely solely on natural language, exchanging tokens or their embeddings. To go beyond language, the study introduces a new paradigm, thought communication, which enables agents to interact directly mind-to-mind, akin to telepathy. To uncover these latent thoughts in a principled way, the work formalizes the process as a general latent variable model, where agent states are generated by an unknown function of underlying thoughts. The study proves that, in a nonparametric setting without auxiliary information, both shared and private latent thoughts between any pair of agents can be identified. Moreover, the global structure of thought sharing, including which agents share which thoughts and how these relationships are structured, can also be recovered with theoretical guarantees. Guided by the established theory, the study develops a framework that extracts latent thoughts from all agents prior to communication and assigns each agent the relevant thoughts, along with their sharing patterns. This paradigm naturally extends beyond LLMs to all modalities, as most observational data arise from hidden generative processes. Experiments on both synthetic and real-world benchmarks validate the theory and demonstrate the collaborative advantages of thought communication. The authors hope this work illuminates the potential of leveraging the hidden world, as many challenges remain unsolvable through surface-level observation alone, regardless of compute or data scale. <br> <br>

3. ***Transformer Over-reliance and Stagnation in AI:  <br>Llion Jones, co-author of "Attention Is All You Need," criticizes the AI field's "dangerously narrow" focus on transformers, attributing stagnation to investor pressure and fear of being scooped, and urges a pivot towards creative, nature-inspired exploration to avoid missing the next big breakthrough.*** <br> <br>
   Oct 23, benturebeat published an [article](https://venturebeat.com/ai/sakana-ais-cto-says-hes-absolutely-sick-of-transformers-the-tech-that-powers) “AI’s financial blind spot: Why long-term success depends on cost transparency”. Llion Jones, co-author of the groundbreaking 2017 paper "Attention Is All You Need" and the mind behind the transformer architecture that underpins major AI models like ChatGPT and Claude, has voiced strong criticism of the field's over-reliance on his own invention. Speaking at the TED AI conference in San Francisco, Jones, now CTO of Tokyo-based Sakana AI, described AI research as dangerously narrow despite unprecedented influxes of talent, funding, and resources. He attributes this stagnation to intense pressure from investors seeking quick returns and researchers fearing being scooped, leading to a focus on safe, incremental improvements rather than bold innovations. Jones likened the situation to the "exploration versus exploitation" dilemma in algorithms, warning that excessive exploitation of transformers risks missing superior breakthroughs, much like the pre-transformer era's fixation on recurrent neural networks. Reflecting on the transformer's origins in a pressure-free, organic environment at Google—fueled by casual discussions and whiteboard sketches—he lamented the loss of such freedom today, even amid million-dollar salaries. At Sakana AI, Jones is pivoting away from transformers, fostering exploratory, nature-inspired projects like the "continuous thought machine" and emphasizing open sharing over competition. He urges the industry to prioritize creativity and collaboration to accelerate progress, arguing that the transformer's very success may be hindering the next big leap in AI. As evidence mounts of diminishing returns from scaling transformers, Jones's insider call to "turn up the explore dial" highlights the high stakes: without change, transformative innovations could remain undiscovered while the field chases mediocrity. <br> <br>

5. ***Machines Discover State-of-the-Art RL Algorithms:  <br>Nature reports that machines can now autonomously "discover state-of-the-art reinforcement learning algorithms" by meta-learning from agent experiences across numerous environments, outperforming manually designed rules and suggesting that future RL algorithms may be machine-discovered.*** <br> <br>
   Oct 22, Nature published a [paper](https://www.nature.com/articles/s41586-025-09761-x) “Discovering state-of-the-art reinforcement learning algorithms”. Humans and other animals use powerful reinforcement learning (RL) mechanisms that have been discovered by evolution over many generations of trial and error. By contrast, artificial agents typically learn using hand-crafted learning rules. Despite decades of interest, the goal of autonomously discovering powerful RL algorithms has proven elusive. This work shows that it is possible for machines to discover a state-of-the-art RL rule that outperforms manually-designed rules. This was achieved by meta-learning from the cumulative experiences of a population of agents across a large number of complex environments. Specifically, the method discovers the RL rule by which the agent's policy and predictions are updated. In large-scale experiments, the discovered rule surpassed all existing rules on the well-established Atari benchmark and outperformed a number of state-of-the-art RL algorithms on challenging benchmarks that it had not seen during discovery. The findings suggest that the RL algorithms required for advanced artificial intelligence may soon be automatically discovered from the experiences of agents, rather than manually designed. <br> <br>

7. ***olmOCR 2: Unit Test Rewards for Document OCR:  <br>The Allen Inst for AI presents "olmOCR 2," a specialized 7B vision language model trained with reinforcement learning using binary unit tests as rewards, achieving state-of-the-art performance in converting digitized print documents to clean, naturally ordered plain text, with significant improvements in math formulas and table parsing.*** <br> <br>
   Oct 22, Allen Inst for AI published a [paper](https://arxiv.org/pdf/2510.19817) “olmOCR 2: Unit Test Rewards for Document OCR”. The paper presents olmOCR 2, the latest in the family of powerful OCR systems for converting digitized print documents, like PDFs, into clean, naturally ordered plain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision language model (VLM) trained using reinforcement learning with verifiable rewards (RLVR), where the rewards are a diverse set of binary unit tests. To scale unit test creation, the work develops a pipeline for generating synthetic documents with diverse and challenging layouts, known ground-truth HTML source code, and extracted test cases. The work shows that RL training on these test cases results in state-of-the-art performance on olmOCR-Bench, the English-language OCR benchmark, with the largest improvements in math formula conversion, table parsing, and multi-column layouts compared to previous versions. https://github.com/allenai/olmocr <br> <br>

9. ***DeepSeek-OCR: Optical Compression for Long Contexts:  <br>DeepSeek introduces "DeepSeek-OCR," an initial investigation into compressing long contexts via optical 2D mapping, demonstrating high OCR precision (97% at 10x compression) with an ultra-compact VLM, showing promise for historical long-context compression and efficient document parsing.*** <br> <br>
    Oct 21, DeepSeek published a [paper](https://www.arxiv.org/pdf/2510.18234) “DeepSeek-OCR: Contexts Optical Compression”. The paper presents DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core engine, designed to maintain low activations under high-resolution input while achieving high compression ratios to ensure an optimal and manageable number of vision tokens. Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio < 10x), the model can achieve decoding (OCR) precision of 97%. Even at a compression ratio of 20x, the OCR accuracy still remains at about 60%. This shows considerable promise for research areas such as historical long-context compression and memory forgetting mechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value. On OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens, and outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs at a scale of 200k+ pages per day (a single A100-40G). http://github.com/deepseek-ai/DeepSeek-OCR <br> <br>

11. ***Quantifiable Definition of AGI and Jagged Cognitive Profiles:  <br>The Center for AI Safety et al. propose a "quantifiable framework for AGI" based on Cattell-Horn-Carroll theory, defining AGI as matching a well-educated adult's cognitive versatility, and reveal "jagged" cognitive profiles in current LLMs with critical deficits in foundational cognitive machinery like long-term memory.*** <br> <br>
    Oct 21, Center for AI Safety et al published a [paper](https://www.arxiv.org/pdf/2510.18212) “A Definition of AGI”. The lack of a concrete definition for Artificial General Intelligence (AGI) obscures the gap between today's specialized AI and human-level cognition. This paper introduces a quantifiable framework to address this, defining AGI as matching the cognitive versatility and proficiency of a well-educated adult. To operationalize this, the paper grounds the methodology in Cattell-Horn-Carroll theory, the most empirically validated model of human cognition. The framework dissects general intelligence into ten core cognitive domains-including reasoning, memory, and perception-and adapts established human psychometric batteries to evaluate AI systems. Application of this framework reveals a highly "jagged" cognitive profile in contemporary models. While proficient in knowledge-intensive domains, current AI systems have critical deficits in foundational cognitive machinery, particularly long-term memory storage. The resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify both rapid progress and the substantial gap remaining before AGI. <br> <br>

13. ***Extracting Alignment Data from Open Models:  <br>Uni of Oxford, NUS, OpenAI, Google et al. demonstrate that it's possible to "extract significant amounts of alignment training data" from post-trained models, showing that models readily regurgitate post-training data (SFT/RL) and that this data can be used to meaningfully train base models, raising overlooked risks and implications for distillation.*** <br> <br>
    Oct 21, Uni of Oxford, NUS, OpenAI, Google et al. published a [paper](https://arxiv.org/pdf/2510.18554) “Extracting alignment data in open models”. In this work shows that it is possible to extract significant amounts of alignment training data from a post-trained model -- useful to steer the model to improve certain capabilities such as long-context reasoning, safety, instruction following, and maths. While the majority of related work on memorisation has focused on measuring success of training data extraction through string matching, the paper argues that embedding models are better suited for the specific goals. Distances measured through a high quality embedding model can identify semantic similarities between strings that a different metric such as edit distance will struggle to capture. In fact, in the investigation, approximate string matching would have severely undercounted (by a conservative estimate of 10times) the amount of data that can be extracted due to trivial artifacts that deflate the metric. Interestingly, the work finds that models readily regurgitate training data that was used in post-training phases such as SFT or RL. The study shows that this data can be then used to train a base model, recovering a meaningful amount of the original performance. The authors believe the work exposes a possibly overlooked risk towards extracting alignment data. Finally, the work opens up an interesting discussion on the downstream effects of distillation practices: since models seem to be regurgitating aspects of their training set, distillation can therefore be thought of as indirectly training on the model's original dataset. <br> <br>

15. ***PokeeResearch: Robust Deep Research via RLAIF:  <br>PokeeAI introduces "PokeeResearch-7B," a 7B-parameter deep research agent trained with an annotation-free Reinforcement Learning from AI Feedback (RLAIF) framework and a chain-of-thought-driven multi-call reasoning scaffold, achieving state-of-the-art performance among 7B-scale agents on deep research benchmarks.*** <br> <br>
    Oct 21, PokeeAI published a [paper](https://arxiv.org/pdf/2510.15862) “PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold”. Tool-augmented large language models (LLMs) are emerging as deep research agents, systems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. The work introduces PokeeResearch-7B, a 7B-parameter deep research agent built under a unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. A chain-of-thought-driven multi-call reasoning scaffold further enhances robustness through self-verification and adaptive recovery from tool failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. https://github.com/Pokee-AI/PokeeResearchOSS. <br> <br>

17. ***Measuring LLM Belief Depth for Implanted Facts:  <br>Anthropic et al. develop a framework to measure "belief depth" in LLMs, finding that while Synthetic Document Finetuning (SDF) can implant beliefs that generalize and are robust, knowledge editing techniques generally fail to implant facts deeply, especially when contradicting basic world knowledge.*** <br> <br>
    Oct 20, Anthropic et al published a [paper](https://arxiv.org/pdf/2510.17941) “Believe It or Not: How Deeply do LLMs Believe Implanted Facts?”. Knowledge editing techniques promise to implant new factual knowledge into large language models (LLMs). But do LLMs really believe these facts? The study develops a framework to measure belief depth and use it to evaluate the success of knowledge editing techniques. The study operationalizes belief depth as the extent to which implanted knowledge 1) generalizes to related contexts (e.g. Fermi estimates several logical steps removed), 2) is robust to self-scrutiny and direct challenge, and 3) is represented similarly to genuine knowledge (as measured by linear probes). The evaluations show that simple prompting and mechanistic editing techniques fail to implant knowledge deeply. In contrast, Synthetic Document Finetuning (SDF) - where models are trained on LLM-generated documents consistent with a fact - often succeeds at implanting beliefs that behave similarly to genuine knowledge. However, SDF's success is not universal, as implanted beliefs that contradict basic world knowledge are brittle and representationally distinct from genuine knowledge. Overall, the work introduces measurable criteria for belief depth and enables the rigorous evaluation necessary for deploying knowledge editing in real-world applications. <br> <br>

19. ***Enterprise Deep Research: Steerable Multi-Agent Analytics:  <br>Saleforce presents "Enterprise Deep Research (EDR)," a multi-agent system for enterprise analytics that integrates a Master Planning Agent, specialized search agents, an extensible tool ecosystem, a Visualization Agent, and reflection mechanisms with optional human-in-the-loop steering, outperforming state-of-the-art agentic systems on open-ended benchmarks.*** <br> <br>
    Oct 20, Saleforce published a [paper](https://arxiv.org/pdf/2510.17797) “Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics”. As information grows exponentially, enterprises face increasing pressure to transform unstructured data into coherent, actionable insights. While autonomous agents show promise, they often struggle with domain-specific nuances, intent alignment, and enterprise integration. The study presents Enterprise Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning Agent for adaptive query decomposition, (2) four specialized search agents (General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a Visualization Agent for data-driven insights, and (5) a reflection mechanism that detects knowledge gaps and updates research direction with optional human-in-the-loop steering guidance. These components enable automated report generation, real-time streaming, and seamless enterprise deployment, as validated on internal datasets. On open-ended benchmarks including DeepResearch Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without any human steering. Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and Dataset at https://huggingface.co/datasets/Salesforce/EDR-200 <br> <br>

21. ***DeepAnalyze: Agentic LLM for Autonomous Data Science:  <br>Renmin Uni and Tsinghua Uni introduce "DeepAnalyze-8B," the first agentic LLM designed for autonomous data science, capable of completing end-to-end data science pipelines from raw data to deep research reports through a curriculum-based agentic training paradigm and data-grounded trajectory synthesis.*** <br> <br>
    Oct 19, Renmin Uni and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2510.16872) “DeepAnalyze: Agentic Large Language Models for Autonomous Data Science”. Autonomous data science, from raw data sources to analyst-grade deep research reports, has been a long-standing challenge, and is now becoming feasible with the emergence of powerful large language models (LLMs). Recent workflow-based data agents have shown promising results on specific data tasks but remain fundamentally limited in achieving fully autonomous data science due to their reliance on predefined workflows. This study introduces DeepAnalyze-8B, the first agentic LLM designed for autonomous data science, capable of automatically completing the end-to-end pipeline from data sources to analyst-grade deep research reports. To tackle high-complexity data science tasks, the study proposes a curriculum-based agentic training paradigm that emulates the learning trajectory of human data scientists, enabling LLMs to progressively acquire and integrate multiple capabilities in real-world environments. The study also introduces a data-grounded trajectory synthesis framework that constructs high-quality training data. Through agentic training, DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data question answering and specialized analytical tasks to open-ended data research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze outperforms previous workflow-based agents built on most advanced proprietary LLMs. https://github.com/ruc-datalab/DeepAnalyze <br> <br>

23. ***PaddleOCR-VL: Ultra-Compact Multilingual Document Parsing:  <br>Baidu proposes "PaddleOCR-VL," a state-of-the-art and resource-efficient model for multilingual document parsing, featuring PaddleOCR-VL-0.9B, a compact vision-language model that achieves high accuracy in recognizing complex elements across 109 languages with minimal resource consumption.*** <br> <br>
    Oct 17, Baidu published a [paper](https://arxiv.org/pdf/2510.14528) “PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model”. The work proposes PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. https://github.com/PaddlePaddle/PaddleOCR <br> <br>

25. ***Paper2Web: Automated Interactive Academic Webpage Generation:  <br>OneLab, Uni of Illinois Chicago, and Uni of Maryland introduce "Paper2Web," a benchmark and evaluation framework for academic webpage generation, and present PWAgent, an autonomous pipeline that converts scientific papers into interactive, multimedia-rich academic homepages, outperforming baselines in quality and cost.*** <br> <br>
    Oct 17, OneLab, Uni of Illinois Chicago and Uni of Maryland published a [paper](https://arxiv.org/pdf/2510.15842) “Paper2Web: Let's Make Your Paper Alive!”. Academic project websites can more effectively disseminate research when they clearly present core content and enable intuitive navigation and interaction. However, current approaches such as direct Large Language Model (LLM) generation, templates, or direct HTML conversion struggle to produce layout-aware, interactive sites, and a comprehensive evaluation suite for this task has been lacking. This study introduces Paper2Web, a benchmark dataset and multi-dimensional evaluation framework for assessing academic webpage generation. It incorporates rule-based metrics like Connectivity, Completeness and human-verified LLM-as-a-Judge (covering interactivity, aesthetics, and informativeness), and PaperQuiz, which measures paper-level knowledge retention. The work further presents PWAgent, an autonomous pipeline that converts scientific papers into interactive and multimedia-rich academic homepages. The agent iteratively refines both content and layout through MCP tools that enhance emphasis, balance, and presentation quality. Experiments show that PWAgent consistently outperforms end-to-end baselines like template-based webpages and arXiv/alphaXiv versions by a large margin while maintaining low cost, achieving the Pareto-front in academic webpage generation. https://github.com/YuhangChen1/Paper2All <br> <br>

27. ***OmniVinci: Enhancing Omni-Modal Understanding LLMs:  <br>Nvidia introduces "OmniVinci," an open-source initiative to build a strong omni-modal LLM, featuring architectural innovations (OmniAlignNet, Temporal Embedding Grouping, Constrained Rotary Time Embedding) and a data curation pipeline that enable modalities to reinforce one another, significantly outperforming baselines with less training data.*** <br> <br>
    Oct 17, Nvidia published a [paper](https://arxiv.org/pdf/2510.15870) “OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM”. Advancing machine intelligence requires developing the ability to perceive across multiple modalities, much as humans sense the world. The work introduces OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. The work carefully studies the design choices across model architecture and data curation. For model architecture, the study presents three key innovations: (i) OmniAlignNet for strengthening alignment between vision and audio embeddings in a shared omni-modal latent space; (ii) Temporal Embedding Grouping for capturing relative temporal alignment between vision and audio signals; and (iii) Constrained Rotary Time Embedding for encoding absolute temporal information in omni-modal embeddings. The work introduces a curation and synthesis pipeline that generates 24M single-modal and omni-modal conversations. The work finds that modalities reinforce one another in both perception and reasoning. The model, OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while using just 0.2T training tokens - a 6 times reduction compared to Qwen2.5-Omni's 1.2T. The study finally demonstrates omni-modal advantages in downstream applications spanning robotics, medical AI, and smart factory. https://github.com/NVlabs/OmniVinci <br> <br>

29. ***freephdlabor: Multiagent Framework for Continual Science Automation:  <br>Yale Uni, Uni of Chicago, and Uni of Oxford present "freephdlabor," an open-source multiagent framework for continual and interactive science automation, featuring dynamic workflows, automatic context compaction, and non-blocking human intervention, transforming automated research into persistent, customizable programs.*** <br> <br>
    Oct 17, Yale Uni, Uni of Chicago and Uni of Oxford published a [paper](https://arxiv.org/pdf/2510.15624) “Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation”. The automation of scientific discovery represents a critical milestone in Artificial Intelligence (AI) research. However, existing agentic systems for science suffer from two fundamental limitations: rigid, pre-programmed workflows that cannot adapt to intermediate findings, and inadequate context management that hinders long-horizon research. The study presents freephdlabor, an open-source multiagent framework featuring fully dynamic workflows determined by real-time agent reasoning and a modular architecture enabling seamless customization -- users can modify, add, or remove agents to address domain-specific requirements. The framework provides comprehensive infrastructure including automatic context compaction, workspace-based communication to prevent information degradation, memory persistence across sessions, and non-blocking human intervention mechanisms. These features collectively transform automated research from isolated, single-run attempts into continual research programs that build systematically on prior explorations and incorporate human feedback. By providing both the architectural principles and practical implementation for building customizable co-scientist systems, this work aims to facilitate broader adoption of automated research across scientific domains, enabling practitioners to deploy interactive multiagent systems that autonomously conduct end-to-end research -- from ideation through experimentation to publication-ready manuscripts. https://github.com/ltjed/freephdlabor <br> <br>

31. ***Reasoning with Sampling: Unlocking Base Model Intelligence:  <br>Harvard Uni proposes "Reasoning with Sampling," a simple iterative sampling algorithm leveraging base models' own likelihoods, demonstrating that substantial reasoning boosts can be elicited from base models at inference time, nearly matching or outperforming RL-trained models on various single-shot tasks without additional training or verifiers.*** <br> <br>
    Oct 16, Harvard Uni published a [paper](https://www.arxiv.org/pdf/2510.14901) “Reasoning with Sampling: Your Base Model is Smarter Than You Think”. Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, driven by posttraining large language models (LLMs) with reinforcement learning (RL). However, despite the widespread success of this paradigm, much of the literature has been devoted to disentangling truly novel behaviors that emerge during RL but are not present in the base models. This study approaches this question from a different angle, instead asking whether comparable reasoning capabilites can be elicited from base models at inference time by pure sampling, without any additional training. Inspired by Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened distributions, the study proposes a simple iterative sampling algorithm leveraging the base models' own likelihoods. Over different base models, the work shows that the algorithm offers substantial boosts in reasoning that nearly match and even outperform those from RL on a wide variety of single-shot tasks, including MATH500, HumanEval, and GPQA. Moreover, the sampler avoids the collapse in diversity over multiple samples that is characteristic of RL-posttraining. Crucially, the method does not require training, curated datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains. <br> <br>

33. ***UniFilter: Unified Multimodal Data Quality Classifier with Synthetic Data:  <br>UCSB, Amazon, and UCSD introduce "UniFilter," an efficient MLLM trained with semi-synthetic data to act as a Unified Multimodal Data Quality Classifier, curating high-quality image-text caption and interleaved data, which significantly enhances MLLMs' zero-shot reasoning and in-context learning capabilities.*** <br> <br>
    Oct 16, UCSB, Amazon and UCSD published a [paper](https://arxiv.org/pdf/2510.15162) “Train a Unified Multimodal Data Quality Classifier with Synthetic Data”. The Multimodal Large Language Models (MLLMs) are continually pre-trained on a mixture of image-text caption data and interleaved document data, while the high-quality data filtering towards image-text interleaved document data is under-explored. The study proposes to train an efficient MLLM as a Unified Mulitmodal Data Quality Classifier to Filter both high-quality image-text caption and interleaved data (UniFilter). To address the challenge of collecting diverse labeled multimodal data, the work introduces a semi-synthetic approach that leverages readily available raw images and generates corresponding text across four quality levels. This method enables efficient creation of sample-score pairs for both caption and interleaved document data to train UniFilter. The study applies UniFilter to curate high-quality caption data from DataComp caption dataset and interleaved data from the OBELICS image-text interleaved dataset. MLLMs pre-trained on the filtered data demonstrate significantly enhanced capabilities compared to those trained on baseline-filtered data, achieving stronger zero-shot reasoning and in-context learning capabilities. After visual supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger performance on various benchmarks, highlighting the downstream benefits of high-quality multimodal pre-training. The authors release the synthetic training data used for training UniFilter, the UniFilter model checkpoints, and the high-quality interleaved document subset OBELICS-HQ, curated by UniFilter. https://github.com/Victorwz/UniFilter <br> <br>

35. ***Language Models Model Language: A Ma'nczak-Inspired View:  <br>Snowflake argues for a radical shift in perspective on LLMs, adopting Witold Ma'nczak's empiricist principles that define language as the totality of what is said and written, governed by frequency of use, to challenge prior critiques and guide the design and evaluation of language models.*** <br> <br>
    Oct 14, Snowflake published a [paper](https://arxiv.org/pdf/2510.12766) “Language Models Model Language”. Linguistic commentary on LLMs, heavily influenced by the theoretical frameworks of de Saussure and Chomsky, is often speculative and unproductive. Critics challenge whether LLMs can legitimately model language, citing the need for "deep structure" or "grounding" to achieve an idealized linguistic "competence." The study argues for a radical shift in perspective towards the empiricist principles of Witold Ma'nczak, a prominent general and historical linguist. He defines language not as a "system of signs" or a "computational system of the brain" but as the totality of all that is said and written. Above all, he identifies frequency of use of particular language elements as language's primary governing principle. Using his framework, the study challenges prior critiques of LLMs and provide a constructive guide for designing, evaluating, and interpreting language models. <br> <br>

37. ***AI Agents Will Change Research: A Scientist's Guide:  <br>Nature describes "How AI agents will change research" by extending LLMs with external tools to handle complex, multi-step tasks, acting as research assistants, expert teams, or co-scientists, while cautioning that they are not yet autonomous and require human oversight, reasoning explanations, and contained environments to mitigate risks.*** <br> <br>
    Oct 3, Nature published an [article](https://www.nature.com/articles/d41586-025-03246-7) “How AI agents will change research: a scientist’s guide”. AI agents, which extend large language models by connecting them to external tools like browsers and coding suites, are poised to significantly change scientific research by handling complex, multi-step tasks. Researchers are already using them as "research assistants" for routine work like data curation and writing code, while more sophisticated applications are emerging. These include emulating human expert teams, such as Microsoft's AI "tumour board" for cancer treatment, and acting as "co-scientists" to generate new hypotheses. For instance, one Harvard team is using an agent to analyze clinical trial and regulatory data to identify existing drugs that may protect against other diseases. While simple agentic tools are accessible to all, platforms like ToolUniverse are being developed to democratize access to more advanced capabilities. Despite this promise, the technology is not yet autonomous. Benchmarking tools reveal that while agents perform well on simple tasks like literature reviews, they struggle with complex data analysis and completing an entire scientific workflow. They are also prone to LLM-based hallucinations and can take unintended, harmful actions if not properly constrained. Researchers stress that the dream of a fully autonomous AI scientist is still distant, and mitigating these risks requires keeping humans in the loop for verification, forcing agents to explain their reasoning, and placing them in "containers" that limit their ability to perform dangerous actions.

 <br> <br> <br>

***Oct 19, 2025***

1. ***Cognitive AI for Human-AI Complementarity:  <br>CMU proposes a "cognitive AI" framework in Nature Reviews Psychology to integrate human decision-making with AI, focusing on AI systems that model human cognitive processes to enhance complementarity in dynamic, complex environments like disaster response.*** <br> <br>
   Oct 17, Nature Reviews Psychology published a [paper](https://www.nature.com/articles/s44159-025-00499-x) from CMU “A cognitive approach to human–AI complementarity in dynamic decision-making”. As artificial intelligence (AI) becomes increasingly integrated into complex decision-making environments, there is a growing need to develop AI systems that complement human capabilities. AI and humans offer distinct strengths: AI excels at processing large datasets, identifying statistical patterns and optimizing predefined objectives, whereas humans are skilled at navigating uncertainty, novelty and interpersonal challenges. The synergy between humans and AI is particularly vital in dynamic decision-making domains — such as disaster response situations — in which rapid analysis of AI results must be balanced with human judgement and ethical considerations. In this Perspective, the researchers provide a conceptual framework to integrate human decision-making with AI, focusing on cognitive AI: a computational approach that models human cognitive processes to create AI systems that learn and make decisions in ways similar to those of humans. The work discusses the elements and necessary capabilities of cognitive AI and how to realize human–AI complementarity in decision-making while considering ethical risks. By advancing these areas, researchers can lay the groundwork for adaptive and cognitively grounded human–AI teamwork that is aligned with human values and goals. <br> <br>

3. ***Future Summary Prediction for Long-Horizon LLMs:  <br>Meta et al. introduce "Future Summary Prediction (FSP)" as a new pretraining objective for LLMs, training an auxiliary head to predict a compact representation of the long-term future, which significantly improves performance on math, reasoning, and coding benchmarks over traditional next-token or multi-token prediction.*** <br> <br>
   Oct 17, Meta, Mila, Uni of Montreal and CMU published a [paper](https://arxiv.org/pdf/2510.14751) “Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries”. Next-token prediction (NTP) has driven the success of large language models (LLMs), but it struggles with long-horizon reasoning, planning, and creative writing, with these limitations largely attributed to teacher-forced training. Multi-token prediction (MTP) partially mitigates these issues by predicting several future tokens at once, but it mostly captures short-range dependencies and offers limited improvement. The study proposes future summary prediction (FSP), which trains an auxiliary head to predict a compact representation of the long-term future, preserving information relevant for long-form generations. The work explores two variants of FSP: handcrafted summaries, for example, a bag of words summary of the future of the sequence, and learned summaries, which use embeddings produced by a reverse language model trained from right to left. Large-scale pretraining experiments (3B and 8B-parameter models) demonstrate that FSP provides improvements over both NTP and MTP across math, reasoning, and coding benchmarks. <br> <br>

5. ***Midtraining Bridges Pretraining and Posttraining:  <br>CMU's study investigates "midtraining," a phase where higher-quality, often instruction-formatted data is mixed into pretraining, finding that it effectively reduces the syntactic gap between pretraining and posttraining data, particularly in math and code domains, leading to better performance and reduced forgetting.*** <br> <br>
   Oct 16, CMU published a [paper](https://arxiv.org/pdf/2510.14865) “Midtraining Bridges Pretraining and Posttraining Distributions”. Recently, many language models have been pretrained with a "midtraining" phase, in which higher quality, often instruction-formatted data, is mixed in at the end of pretraining. Despite the popularity of this practice, there is little scientific understanding of this phase of model training or why it is effective. This work conducts the first systematic investigation of midtraining through controlled experiments with language models pretrained from scratch and fine-tuned on supervised finetuning datasets in different domains. The work finds that when compared after supervised fine-tuning, the effectiveness of midtraining is highest in the math and code domains, where midtraining can best reduce the syntactic gap between pretraining and posttraining data. In these cases, midtraining consistently outperforms continued pretraining in both in-domain validation loss as well as pretraining data forgetting after posttraining. The study conducts ablations on the starting time of the midtraining phase and mixture weights of the midtraining data, using code midtraining as a case study, and find that timing has a greater impact than mixture weights, with earlier introduction of specialized data, yielding greater benefits in-domain as well as preserving general language modeling better. These findings establish midtraining as a domain adaptation technique that compared to continued pretraining yields better performance through reduced forgetting. https://github.com/nightingal3/all_in_one_pretraining <br> <br>

7. ***Tensor Logic: The Unified Language of AI:  <br>The Uni of Washington proposes "Tensor Logic," a novel programming language designed to unify neural and symbolic AI at a fundamental level through tensor equations, enabling elegant implementations of transformers, formal reasoning, and opening new directions like sound reasoning in embedding space.*** <br> <br>
   Oct 16, Uni of Washington published a [paper](https://arxiv.org/pdf/2510.12269) “Tensor Logic: The Language of AI”. Progress in AI is hindered by the lack of a programming language with all the requisite features. Libraries like PyTorch and TensorFlow provide automatic differentiation and efficient GPU implementation, but are additions to Python, which was never intended for AI. Their lack of support for automated reasoning and knowledge acquisition has led to a long and costly series of hacky attempts to tack them on. On the other hand, AI languages like LISP an Prolog lack scalability and support for learning. This paper proposes tensor logic, a language that solves these problems by unifying neural and symbolic AI at a fundamental level. The sole construct in tensor logic is the tensor equation, based on the observation that logical rules and Einstein summation are essentially the same operation, and all else can be reduced to them. The paper shows how to elegantly implement key forms of neural, symbolic and statistical AI in tensor logic, including transformers, formal reasoning, kernel machines and graphical models. Most importantly, tensor logic makes new directions possible, such as sound reasoning in embedding space. This combines the scalability and learnability of neural networks with the reliability and transparency of symbolic reasoning, and is potentially a basis for the wider adoption of AI. https://tensor-logic.org/ <br> <br>

9. ***TokDrift: Misalignment of Subwords and Code Grammar:  <br>The Uni of Waterloo introduces "TokDrift," a framework revealing that LLMs for code, relying on subword tokenizers, suffer from misalignment where minor formatting changes cause substantial shifts in model behavior due to the tokenizer's failure to capture grammar boundaries, highlighting a need for grammar-aware tokenization.*** <br> <br>
    Oct 16, Uni of Waterloo published a [paper](https://arxiv.org/pdf/2510.14972) “TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar”. Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar. As a result, semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming. To measure the impact of this misalignment, the work introduces TokDrift, a framework that applies semantic-preserving rewrite rules to create code variants differing only in tokenization. Across nine code LLMs, including large ones with over 30B parameters, even minor formatting changes can cause substantial shifts in model behavior. Layer-wise analysis shows that the issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries. The findings identify misaligned tokenization as a hidden obstacle to reliable code understanding and generation, highlighting the need for grammar-aware tokenization for future code LLMs. https://github.com/uw-swag/tokdrift <br> <br>

11. ***Principled Scaling for LLM Reinforcement Learning:  <br>Meta et al. present the first large-scale systematic study, "The Art of Scaling Reinforcement Learning Compute for LLMs," defining a principled framework for analyzing and predicting RL scaling in LLMs, including a best-practice recipe, ScaleRL, for predictable and efficient training.*** <br> <br>
    Oct 15, Meta et al published a [paper](https://arxiv.org/pdf/2510.13786) “The Art of Scaling Reinforcement Learning Compute for LLMs”. Reinforcement learning (RL) has become central to training large language models (LLMs), yet the field lacks predictive scaling methodologies comparable to those established for pre-training. Despite rapidly rising compute budgets, there is no principled understanding of how to evaluate algorithmic improvements for scaling RL compute. The work presents the first large-scale systematic study, amounting to more than 400,000 GPU-hours, that defines a principled framework for analyzing and predicting RL scaling in LLMs. The study fits sigmoidal compute-performance curves for RL training and ablate a wide range of common design choices to analyze their effects on asymptotic performance and compute efficiency. The work observes: (1) Not all recipes yield similar asymptotic performance, (2) Details such as loss aggregation, normalization, curriculum, and off-policy algorithm primarily modulate compute efficiency without materially shifting the asymptote, and (3) Stable, scalable recipes follow predictable scaling trajectories, enabling extrapolation from smaller-scale runs. Combining these insights, the study proposes a best-practice recipe, ScaleRL, and demonstrate its effectiveness by successfully scaling and predicting validation performance on a single RL run scaled up to 100,000 GPU-hours. The work provides both a scientific framework for analyzing scaling in RL and a practical recipe that brings RL training closer to the predictability long achieved in pre-training. <br> <br>

13. ***OCNOpt: Neural Optimizer from Optimal Control Theory:  <br>George Inst of Tech and Meta introduce "Optimal Control Theoretic Neural Optimizer (OCNOpt)," a new class of optimization methods for deep neural networks that reinterprets backpropagation as an approximate dynamic programming, leading to improved robustness and efficiency.*** <br> <br>
    Oct 15, George Inst of Tech and Meta published a [paper](https://www.arxiv.org/pdf/2510.14168) “Optimal Control Theoretic Neural Optimizer: From Backpropagation to Dynamic Programming”. Optimization of deep neural networks (DNNs) has been a driving force in the advancement of modern machine learning and artificial intelligence. With DNNs characterized by a prolonged sequence of nonlinear propagation, determining their optimal parameters given an objective naturally fits within the framework of Optimal Control Programming. Such an interpretation of DNNs as dynamical systems has proven crucial in offering a theoretical foundation for principled analysis from numerical equations to physics. In parallel to these theoretical pursuits, this paper focuses on an algorithmic perspective. The motivated observation is the striking algorithmic resemblance between the Backpropagation algorithm for computing gradients in DNNs and the optimality conditions for dynamical systems, expressed through another backward process known as dynamic programming. Consolidating this connection, where Backpropagation admits a variational structure, solving an approximate dynamic programming up to the first-order expansion leads to a new class of optimization methods exploring higher-order expansions of the Bellman equation. The resulting optimizer, termed Optimal Control Theoretic Neural Optimizer (OCNOpt), enables rich algorithmic opportunities, including layer-wise feedback policies, game-theoretic applications, and higher-order training of continuous-time models such as Neural ODEs. Extensive experiments demonstrate that OCNOpt improves upon existing methods in robustness and efficiency while maintaining manageable computational complexity, paving new avenues for principled algorithmic design grounded in dynamical systems and optimal control theory. <br> <br>

15. ***LATTICE: LLM-Guided Hierarchical Retrieval:  <br>UT Austin, UCLA, and Google introduce "LATTICE," a hierarchical retrieval framework that enables LLMs to reason over and navigate large corpora with logarithmic search complexity by imposing a semantic tree structure, achieving state-of-the-art zero-shot performance on reasoning-intensive benchmarks.*** <br> <br>
    Oct 15, UT Austin, UCLA and Google published a [paper](https://arxiv.org/pdf/2510.13217) “LLM-guided Hierarchical Retrieval”. Modern IR systems are increasingly tasked with answering complex, multi-faceted queries that require deep reasoning rather than simple keyword or semantic matching. While LLM-based IR has shown great promise, the prevailing retrieve-then-rerank paradigm inherits the limitations of embedding-based retrieval; parametric generative approaches are difficult to update with new information; and long-context methods that place the entire corpus in context are computationally infeasible for large document collections. To address these challenges, the work introduces LATTICE, a hierarchical retrieval framework that enables an LLM to reason over and navigate large corpora with logarithmic search complexity by imposing a semantic tree structure on the corpus. The approach consists of two stages: (1) an offline phase that organizes the corpus into a semantic hierarchy via either a bottom-up agglomerative strategy or a top-down divisive strategy using multi-level summaries and (2) an online traversal phase where a search LLM navigates this tree. A central challenge in such LLM-guided search is that the model's relevance judgments are noisy, context-dependent, and unaware of the hierarchy, making cross-branch and cross-level comparisons difficult. To overcome this, the study proposes a traversal algorithm that estimates calibrated latent relevance scores from local LLM outputs and aggregates them into a global path relevance metric. The training-free framework achieves state-of-the-art zero-shot performance on the reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline. Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains comparable results on BRIGHT subsets that use a static corpus for evaluation. https://github.com/nilesh2797/lattice <br> <br>

17. ***AI Bots Wrote and Reviewed a Conference:  <br>Nature reports on the "Agents4Science 2025" conference, a computer science event where all research papers and peer reviews were generated entirely by AI agents, serving as a "safe sandbox" to explore AI's role across the entire scientific process.*** <br> <br>
    Oct 14, Nature published an [article](https://www.nature.com/articles/d41586-025-03363-3) “AI bots wrote and reviewed all papers at this conference”. Next week, the Agents4Science 2025 conference will mark a first in computer science by featuring research papers and peer reviews generated entirely by artificial intelligence. Conceived as a "safe sandbox" by its organizers, the event aims to explore a paradigm shift where coordinated "AI agents" act as scientists across the entire research process, from generating hypotheses to writing and reviewing findings. The conference received over 300 submissions from AI agents, accepting 48 after a review process where AIs performed the initial assessment using guidelines from the prestigious NeurIPS conference, with a human advisory board providing final oversight. This experiment is designed to generate crucial data on the capabilities, weaknesses, and common errors of AI scientists, and to understand how the level of human involvement impacts the quality of the work. While the use of AI reviewers is controversial—with concerns about their reliability, potential for bias, and the risk of deskilling early-career researchers—proponents argue it's a necessary step to manage the explosion in paper submissions at major conferences. Ultimately, Agents4Science seeks to benchmark the performance of AI authors and reviewers, providing valuable insights that could inform future policies on integrating AI into the scientific ecosystem. <br> <br>

19. ***RL After Next-Token Prediction Facilitates Learning:  <br>NYU, Harvard Uni, and Meta present a framework explaining how "reinforcement learning after next-token prediction" improves LLM reasoning, showing theoretically and empirically that it enables autoregressive transformers to generalize more efficiently, especially with rare, long "chain-of-thought" sequences.*** <br> <br>
    Oct 14, NYU, Harvard Uni and Meta published a [paper](https://arxiv.org/pdf/2510.11495) “How Reinforcement Learning After Next-Token Prediction Facilitates Learning”. Recent advances in reasoning domains with neural networks have primarily been enabled by a training recipe that optimizes Large Language Models, previously trained to predict the next-token in a sequence, with reinforcement learning algorithms. The work introduces a framework to study the success of this paradigm, and theoretically exposes the optimization mechanisms by which reinforcement learning improves over next-token prediction in this setting. The authors study learning from mixture distributions of short and long “chain-of-thought” sequences encoding a single task. In particular, when the task consists of predicting the parity of bits and long sequences are rare, the study shows how reinforcement learning after next-token prediction enables autoregressive transformers to generalize, whereas mere next-token prediction requires extreme statistical or computational resources to do so. The work further explains how reinforcement learning leverages increased test-time computation, manifested in longer responses, to facilitate this learning process. In a simplified setting, the work theoretically proves that autoregressive linear models following this training recipe can efficiently learn to predict the parity of  bits as long as the proportion of long demonstrations in the data mix is not exponentially small in the input dimension . Finally, the study demonstrates these same phenomena in other settings, including the post-training of Llama-series models on mixture variations of common mathematical reasoning benchmarks. <br> <br>

21. ***AI Agents as Transductive Task Solvers:  <br>AWS reinterprets AI agents as "compute-capable stochastic dynamical systems" in "AI Agents as Universal Task Solvers," proposing a shift from inductive to transductive learning where the objective is to reduce the time needed to solve new tasks, linking optimal speed-up to algorithmic information.*** <br> <br>
    Oct 14, AWS published a [paper](https://arxiv.org/pdf/2510.12066) “AI Agents as Universal Task Solvers”. AI reasoning agents are already able to solve a variety of tasks by deploying tools, simulating outcomes of multiple hypotheses and reflecting on them. In doing so, they perform computation, although not in the classical sense -- there is no program being executed. Still, if they perform computation, can AI agents be universal? Can chain-of-thought reasoning solve any computable task? How does an AI Agent learn to reason? Is it a matter of model size? Or training dataset size? This work reinterprets the role of learning in the context of AI Agents, viewing them as compute-capable stochastic dynamical systems, and highlight the role of time in a foundational principle for learning to reason. In doing so, the study proposes a shift from classical inductive learning to transductive learning -- where the objective is not to approximate the distribution of past data, but to capture their algorithmic structure to reduce the time needed to find solutions to new tasks. Transductive learning suggests that, counter to Shannon's theory, a key role of information in learning is about reduction of time rather than reconstruction error. In particular, the work shows that the optimal speed-up that a universal solver can achieve using past data is tightly related to their algorithmic information. Using this, the work shows a theoretical derivation for the observed power-law scaling of inference time versus training time. The study then shows that scaling model size can lead to behaviors that, while improving accuracy on benchmarks, fail any reasonable test of intelligence, let alone super-intelligence: In the limit of infinite space and time, large models can behave as savants, able to brute-force through any task without any insight. Instead, the work argues that the key quantity to optimize when scaling reasoning models is time, whose critical role in learning has so far only been indirectly considered. <br> <br>

23. ***Generation Space Size for Calibrating LLM Open-Endedness:  <br>Stanford Uni introduces "Generation Space Size (GSS)" as a metric to understand and calibrate the open-endedness of LLM generations, addressing issues of homogeneous outputs for creative tasks and hallucinatory diversity for factual ones, and demonstrating its utility in detecting ambiguity and steering models.*** <br> <br>
    Oct 14, Stanford Uni published a [paper](https://arxiv.org/abs/2510.12699) “Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations”. Different open-ended generation tasks require different degrees of output diversity. However, current LLMs are often miscalibrated. They collapse to overly homogeneous outputs for creative tasks and hallucinate diverse but incorrect responses for factual tasks. The study argues that these two failure modes are unified by, and can both be addressed by, the notion of effective generation space size (GSS) -- the set of semantically distinct outputs a model considers for a prompt. The study presents GSSBench, a task suite of prompt pairs with ground-truth GSS relationships to assess different metrics and understand where models diverge from desired behavior. The study finds that hallucination detection metrics, particularly EigenScore, consistently outperform standard diversity and uncertainty quantification metrics, while using only model internals, providing interpretable insights into a model's internal task representations. The work demonstrates three applications of GSS: (1) detecting prompt ambiguity and predicting clarification questions for better grounding, (2) interpreting overthinking and underthinking in reasoning models, and (3) steering models to expand their generation space to yield high-quality and diverse outputs. <br> <br>

25. ***RAG-Anything: All-in-One Multimodal RAG Framework:  <br>The Uni of HK introduces "RAG-Anything," a unified framework for Retrieval-Augmented Generation (RAG) that enables comprehensive knowledge retrieval across all modalities (text, visual, tables, math) by conceptualizing content as interconnected knowledge entities and using dual-graph construction for cross-modal relationships.*** <br> <br>
    Oct 14, Uni of HK published a [paper](https://arxiv.org/pdf/2510.12323) “RAG-Anything: All-in-One RAG Framework”. Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding Large Language Models beyond their static training limitations. However, a critical misalignment exists between current RAG capabilities and real-world information environments. Modern knowledge repositories are inherently multimodal, containing rich combinations of textual content, visual elements, structured tables, and mathematical expressions. Yet existing RAG frameworks are limited to textual content, creating fundamental gaps when processing multimodal documents. The study presents RAG-Anything, a unified framework that enables comprehensive knowledge retrieval across all modalities. The approach reconceptualizes multimodal content as interconnected knowledge entities rather than isolated data types. The framework introduces dual-graph construction to capture both cross-modal relationships and textual semantics within a unified representation. The work develops cross-modal hybrid retrieval that combines structural knowledge navigation with semantic matching. This enables effective reasoning over heterogeneous content where relevant evidence spans multiple modalities. RAG-Anything demonstrates superior performance on challenging multimodal benchmarks, achieving significant improvements over state-of-the-art methods. Performance gains become particularly pronounced on long documents where traditional approaches fail. The framework establishes a new paradigm for multimodal knowledge access, eliminating the architectural fragmentation that constrains current systems. https://github.com/HKUDS/RAG-Anything. <br> <br>

27. ***Dr.LLM: Dynamic Layer Routing for LLM Efficiency:  <br>ParameterLab et al. introduce "Dr.LLM (Dynamic routing of Layers for LLMs)," a retrofittable framework that equips pretrained LLMs with lightweight per-layer routers to skip, execute, or repeat blocks, improving accuracy and efficiency without altering base weights, and demonstrating strong generalization across tasks.*** <br> <br>
    Oct 14, ParameterLab et al. published a [paper](https://arxiv.org/pdf/2510.12773) “Dr.LLM: Dynamic Layer Routing in LLMs”. Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inference-time search, architectural changes, or large-scale retraining, and in practice often degrade accuracy despite efficiency gains. The study introduces Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that equips pretrained models with lightweight per-layer routers deciding to skip, execute, or repeat a block. Routers are trained with explicit supervision: using Monte Carlo Tree Search (MCTS), the work derives high-quality layer configurations that preserve or improve accuracy under a compute budget. The design, windowed pooling for stable routing, focal loss with class balancing, and bottleneck MLP routers, ensures robustness under class imbalance and long sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to +3.4%p while saving 5 layers per example on average. Routers generalize to out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% accuracy drop while retaining efficiency, and outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that explicitly supervised routers retrofit frozen LLMs for budget-aware, accuracy-driven inference without altering base weights. <br> <br>

29. ***Cautious Weight Decay for Improved Optimization:  <br>The Uni of Texas at Austin and Google introduce "Cautious Weight Decay (CWD)," a simple optimizer-agnostic modification that applies weight decay only to parameter coordinates whose signs align with the optimizer update, consistently improving final loss and accuracy in language model pre-training and ImageNet classification.*** <br> <br>
    Oct 14, Uni of Texas at Austin and Google published a [paper](https://arxiv.org/pdf/2510.12402) “Cautious Weight Decay”. The work introduces Cautious Weight Decay (CWD), a one-line, optimizer-agnostic modification that applies weight decay only to parameter coordinates whose signs align with the optimizer update. Unlike standard decoupled decay, which implicitly optimizes a regularized or constrained objective, CWD preserves the original loss and admits a bilevel interpretation: it induces sliding-mode behavior upon reaching the stationary manifold, allowing it to search for locally Pareto-optimal stationary points of the unmodified objective. In practice, CWD is a drop-in change for optimizers such as AdamW, Lion, and Muon, requiring no new hyperparameters or additional tuning. For language model pre-training and ImageNet classification, CWD consistently improves final loss and accuracy at million- to billion-parameter scales. <br> <br>

31. ***LENS: Leveraging Negative RL-Groups via Confidence Reweighting:  <br>Meta and NYU introduce "LENS (Likelihood Estimation with Negative Samples)," a modification to GRPO that assigns non-zero, confidence-dependent rewards to incorrect generations in reinforcement learning, making previously wasted negative groups informative and improving efficiency and performance in LLM reasoning tasks.*** <br> <br>
    Oct 13, Meta and NYU published a [paper](https://arxiv.org/pdf/2510.08696) “Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting”. Reinforcement learning with verifiable rewards (RLVR) has become a standard recipe for improving large language models (LLMs) on reasoning tasks, with Group Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO wastes substantial compute on negative groups: groups in which no sampled response is correct yield zero advantage and thus no gradient. The study asks whether negative groups can be leveraged without extra supervision. Starting from a maximum-likelihood (MLE) objective in reward modeling, the work shows that the MLE gradient is equivalent to a policy gradient for a modified value function. This value function adds a confidence-weighted penalty on incorrect responses, imposing larger penalties on more confident mistakes. The study refers to this as Likelihood Estimation with Negative Samples (LENS). LENS modifies GRPO to assign non-zero, confidence-dependent rewards to incorrect generations, making negative groups informative and converting previously wasted samples into useful gradient updates. On the MATH benchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently outperforms GRPO baseline, with significant gains on harder items. These results demonstrate a principled and practical way to "rescue" negative groups, improving efficiency and performance in RLVR. <br> <br>

33. ***Representation-Based Exploration for LLMs:  <br>Princeton Uni and Microsoft investigate "Representation-Based Exploration" for language models, showing that a simple, principled exploration bonus derived from pre-trained language model hidden states significantly improves diversity and pass@k rates during both post-training and a novel inference-time scaling setting.*** <br> <br>
    Oct 13, Princeton Uni and Microsoft published a [paper](https://arxiv.org/pdf/2510.11686) “Representation-Based Exploration for Language Models: From Test-Time to Post-Training”. Reinforcement learning (RL) promises to expand the capabilities of language models, but it is unclear if current RL techniques promote the discovery of novel behaviors, or simply sharpen those already present in the base model. This study investigates the value of deliberate exploration -- explicitly incentivizing the model to discover novel and diverse behaviors -- and aim to understand how the knowledge in pre-trained models can guide this search. The main finding is that exploration with a simple, principled, representation-based bonus derived from the pre-trained language model's hidden states significantly improves diversity and pass@k rates -- both for post-training, and in a novel inference-time scaling setting introduced. For inference-time, exploration with representation-based diversity improves efficiency, consistently improving pass@k rates across a variety of models and reasoning tasks. For example, for Qwen-2.5-14b-Instruct, the study obtains over 50% improvement in verifier efficiency on almost all tasks. For post-training, the work shows that integrating this exploration strategy into an RL pipeline improves reasoning performance over that of the initial model and over standard RL post-training. For example, on AIME 2024, the post-trained Qwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model, demonstrating a 3x improvement in test-time sample efficiency. Overall, the findings suggest that deliberate exploration -- with the right notion of diversity -- is a practical path toward discovery of new behaviors beyond sharpening.  <br> <br>

35. ***LLM Knowledge is Brittle: Superficial Truthfulness Representations:  <br>Meta and Uni of Zurich reveal that "LLM knowledge is brittle," demonstrating that internal representations of statement truthfulness collapse when samples undergo superficial transformations (like typos or reformulations), suggesting LLMs learn shallow, non-robust knowledge representations that limit generalizability.*** <br> <br>
    Oct 13, Meta and Uni of Zurich published a [paper](https://www.arxiv.org/pdf/2510.11905) “LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance”. For Large Language Models (LLMs) to be reliable, they must learn robust knowledge that can be generally applied in diverse settings -- often unlike those seen during training. Yet, extensive research has shown that LLM performance can be brittle, with models exhibiting excessive sensitivity to trivial input variations. This work explores whether this brittleness is a direct result of unstable internal knowledge representations. To explore this question, the study builds on previous work showing that LLM representations encode statement truthfulness -- i.e., true, factual statements can be easily separated from false, inaccurate ones. Specifically, the study tests the robustness of learned knowledge by evaluating representation separability on samples that have undergone superficial transformations to drive them out-of-distribution (OOD), such as typos or reformulations. By applying semantically-preserving perturbations, the authors study how separability degrades as statements become more OOD, across four LLM families, five evaluation datasets, and three knowledge probing methods. The results reveal that internal representations of statement truthfulness collapse as the samples' presentations become less similar to those seen during pre-training. While LLMs can often distinguish between true and false statements when they closely resemble the pre-training data, this ability is highly dependent on the statement's exact surface form. These findings offer a possible explanation for brittle benchmark performance: LLMs may learn shallow, non-robust knowledge representations that allow for only limited generalizability. The work presents a fundamental challenge for the utility of truthfulness probes, and more broadly, calls for further research on improving the robustness of learned knowledge representations. <br> <br>

37. ***QeRL: Quantization-enhanced Reinforcement Learning for LLMs:  <br>Nvidia proposes "QeRL (Quantization-enhanced Reinforcement Learning)," a framework combining NVFP4 quantization with LoRA to accelerate the rollout phase of RL for LLMs, reduce memory overhead, and enhance exploration through quantization noise, enabling RL training of 32B models on a single H100 GPU.*** <br> <br>
    Oct 13, Nvidia published a [paper](https://arxiv.org/pdf/2510.11696) “QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs”. The work proposes QeRL, a Quantization-enhanced Reinforcement Learning framework for large language models (LLMs). While RL is essential for LLMs' reasoning capabilities, it is resource-intensive, requiring substantial GPU memory and long rollout durations. QeRL addresses these issues by combining NVFP4 quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL while reducing memory overhead. Beyond efficiency, the findings show that quantization noise increases policy entropy, enhancing exploration, and enabling the discovery of better strategies during RL. To further optimize exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism, which dynamically adjusts noise during training. Experiments demonstrate that QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is the first framework to enable RL training of a 32B LLM on a single H100 80GB GPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while matching the performance of full-parameter fine-tuning on mathematical benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These results establish QeRL as an efficient and effective framework for RL training in LLMs. https://github.com/NVlabs/QeRL <br> <br>

39. ***RAE: Diffusion Transformers with Representation Autoencoders:  <br>NYU explores "Representation Autoencoders (RAEs)" to replace traditional VAEs in Diffusion Transformers (DiT), using pretrained representation encoders (e.g., DINO) paired with trained decoders to provide high-quality reconstructions and semantically rich latent spaces, achieving strong image generation results on ImageNet.*** <br> <br>
    Oct 13, NYU published a [paper](https://arxiv.org/pdf/2510.11690) “Diffusion Transformers with Representation Autoencoders”. Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. This work explores replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. The work analyzes the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. The approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, the study achieves strong image generation results on ImageNet: 1.51 FID at 256x256 (no guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training. https://github.com/bytetriper/RAE <br> <br>

41. ***GAR: Generative Adversarial RL for Theorem Proving:  <br>UIUC and Nvidia introduce "GAR (Generative Adversarial Reinforcement Learning)," a framework that jointly trains a problem composer and solver in an adversarial loop for formal theorem proving, introducing implicit curriculum learning to align task difficulty with the prover's evolving capability and improving training efficiency and performance.*** <br> <br>
    Oct 13, UIUC and Nvidia published a [paper](https://huggingface.co/papers/2510.11769) “GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving”. Solving math problems through verifiable languages such as Lean has significantly impacted both the mathematics and computer science communities. Current state-of-the-art models are often trained with expensive online Reinforcement Learning (RL) or expert iteration. However, these approaches rely on fixed problem sets, which causes inefficient training and limits the model to tackle complex problems. To overcome these limitations, the study proposes GAR: Generative Adversarial Reinforcement learning, a comprehensive RL training framework that jointly trains the problem composer and solver in an adversarial loop. GAR introduces an implicit curriculum learning mechanism, which aligns task difficulty with the prover's evolving capability. It thereby improves the training efficiency and enables stronger performance of proving advanced theorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and DeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of 4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on ProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR establishes a general RL paradigm for co-evolution of problem generation and solving under verifiable environments. <br> <br>

43. ***AutoGEO: Optimizing Web Content for Generative Search Engines:  <br>CMU introduces "AutoGEO," a framework to automatically learn generative engine preferences and rewrite web contents for "Generative Engine Optimization (GEO)," using frontier LLMs to extract preference rules and embed them in content optimization systems for increased traction.*** <br> <br>
    Oct 13, CMU published a [paper](https://arxiv.org/pdf/2510.11438) “What Generative Search Engines Like and How to Optimize Web Content Cooperatively”. By employing large language models (LLMs) to retrieve documents and generate natural language responses, Generative Engines, such as Google AI overview and ChatGPT, provide significantly enhanced user experiences and have rapidly become the new form of search. Their rapid adoption also drives the needs of Generative Engine Optimization (GEO), as content providers are eager to gain more traction from them. This paper introduces AutoGEO, a framework to automatically learn generative engine preferences when using retrieved contents for response generation, and rewrite web contents for more such traction. AutoGEO first prompts frontier LLMs to explain generative engine preferences and extract meaningful preference rules from these explanations. Then it uses preference rules as context engineering for AutoGEO_API, a prompt-based GEO system, and as rule-based rewards to train AutoGEO_Mini, a cost-effective GEO model. Experiments on the standard GEO-Bench and two newly constructed benchmarks using real user queries demonstrate the effectiveness of AutoGEO in enhancing content traction while preserving search utility. Analyses confirm the learned rules' robustness and abilities to capture unique preferences in variant domains, and AutoGEO systems' ability to embed them in content optimization. https://github.com/cxcscmu/AutoGEO. <br> <br>

45. ***HUME: Measuring Human-Model Performance Gap in Embeddings:  <br>Carleton Uni et al. introduce "HUME (Human Evaluation Framework for Text Embeddings)," which measures human performance across 16 MTEB datasets to provide reliable baselines for understanding the strengths and limitations of embedding models and revealing shortcomings in low-resource languages.*** <br> <br>
    Oct 11, Carleton Uni et al published a [paper](https://arxiv.org/pdf/2510.10062) “HUME Measuring the Human-Model Performance Gap in Text Embedding Task”. Comparing human and model performance offers a valuable perspective for understanding the strengths and limitations of embedding models, highlighting where they succeed and where they fail to capture meaning and nuance. However, such comparisons are rarely made, as human performance on embedding tasks is difficult to measure. To fill this gap, the study introduces HUME: Human Evaluation Framework for Text Embeddings. While frameworks like MTEB provide broad model evaluation, they lack reliable estimates of human performance, limiting the interpretability of model scores. The work measures human performance across 16 MTEB datasets spanning reranking, classification, clustering, and semantic textual similarity across linguistically diverse high- and low-resource languages. Humans achieve an average performance of 77.6% compared to 80.1% for the best embedding model, although variation is substantial: models reach near-ceiling performance on some datasets while struggling on others, suggesting dataset issues and revealing shortcomings in low-resource languages. The work provides human performance baselines, insight into task difficulty patterns, and an extensible evaluation framework that enables a more meaningful interpretation of the model and informs the development of both models and benchmarks. https://github.com/embeddings-benchmark/mteb. <br> <br>

47. ***Adaptive Attacks Bypass LLM Defenses:  <br>OpenAI, Anthropic, Google et al. argue in "The Attacker Moves Second" that current evaluations of LLM jailbreak and prompt injection defenses are flawed, demonstrating that adaptive attackers, systematically tuning and scaling optimization techniques, can bypass 12 recent defenses with high success rates.*** <br> <br>
    Oct 10, OpenAI, Anthropic, Google et al. published a [paper](https://arxiv.org/pdf/2510.09023) “The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections”. How should people evaluate the robustness of language model defenses? Current defenses against jailbreaks and prompt injections (which aim to prevent an attacker from eliciting harmful knowledge or remotely triggering malicious actions, respectively) are typically evaluated either against a static set of harmful attack strings, or against computationally weak optimization methods that were not designed with the defense in mind. The work argues that this evaluation process is flawed. Instead, people should evaluate defenses against adaptive attackers who explicitly modify their attack strategy to counter a defense's design while spending considerable resources to optimize their objective. By systematically tuning and scaling general optimization techniques-gradient descent, reinforcement learning, random search, and human-guided exploration-the study bypasses 12 recent defenses (based on a diverse set of techniques) with attack success rate above 90% for most; importantly, the majority of defenses originally reported near-zero attack success rates. The authors believe that future defense work must consider stronger attacks, such as the ones described, in order to make reliable and convincing claims of robustness. <br> <br>

49. ***UML: Leveraging Unpaired Multimodal Data for Unimodal Models:  <br>MIT and TU Munich introduce "UML (Unpaired Multimodal Learner)," a modality-agnostic training paradigm where a single model alternately processes inputs from different modalities, exploiting cross-modal structure from unpaired data to enhance representation learning in a target modality.***  <br>  <br>
    Oct 9, MIT and TU Munich published a [paper](https://arxiv.org/pdf/2510.08492) “Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models”. Traditional multimodal learners find unified representations for tasks like visual question answering, but rely heavily on paired datasets. However, an overlooked yet potentially powerful question is: can one leverage auxiliary unpaired multimodal data to directly enhance representation learning in a target modality? We introduce UML: Unpaired Multimodal Learner, a modality-agnostic training paradigm in which a single model alternately processes inputs from different modalities while sharing parameters across them. This design exploits the assumption that different modalities are projections of a shared underlying reality, allowing the model to benefit from cross-modal structure without requiring explicit pairs. Theoretically, under linear data-generating assumptions, we show that unpaired auxiliary data can yield representations strictly more informative about the data-generating process than unimodal training. Empirically, we show that using unpaired data from auxiliary modalities -- such as text, audio, or images -- consistently improves downstream performance across diverse unimodal targets such as image and audio. Our project page: https://unpaired-multimodal.github.io/  <br>  <br>

51. ***LightReasoner:  Small LLMs Teaching Large LLM Reasoning:  <br>Uni of HK and Uni of Chicago propose "LightReasoner," a framework where smaller language models (SLMs) teach larger language models (LLMs) by revealing high-value reasoning moments through expert-amateur contrast, significantly improving reasoning accuracy while reducing training resources.***  <br>  <br>
    Oct 9, Uni of HK and Uni of Chicago published a [paper](https://arxiv.org/pdf/2510.07962) “LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?”. Large language models (LLMs) have demonstrated remarkable progress in reasoning, often through supervised fine-tuning (SFT). However, SFT is resource-intensive, relying on large curated datasets, rejection-sampled demonstrations, and uniform optimization across all tokens, even though only a fraction carry meaningful learning value. This study explores a counterintuitive idea: can smaller language models (SLMs) teach larger language models (LLMs) by revealing high-value reasoning moments that reflect the latter's unique strength? The work proposes LightReasoner, a novel framework that leverages the behavioral divergence between a stronger expert model (LLM) and a weaker amateur model (SLM). LightReasoner operates in two stages: (1) a sampling stage that pinpoints critical reasoning moments and constructs supervision examples capturing the expert's advantage through expert-amateur contrast, and (2) a fine-tuning stage that aligns the expert model with these distilled examples, amplifying its reasoning strengths. Across seven mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while reducing time consumption by 90%, sampled problems by 80%, and tuned token usage by 99%, all without relying on ground-truth labels. By turning weaker SLMs into effective teaching signals, LightReasoner offers a scalable and resource-efficient approach for advancing LLM reasoning. https://github.com/HKUDS/LightReasoner  <br>  <br>

53. ***ReviewerToo:   <br>AI in Peer Review: Mila et al. introduce "ReviewerToo," a modular framework for studying and deploying AI-assisted peer review, demonstrating that a gpt-oss-120b model achieves 81.8% accuracy in accept/reject categorization, comparable to human reviewers, and generates higher-quality reviews as rated by an LLM judge.***  <br>  <br>
    Oct 9, Mila et al published a [paper](https://arxiv.org/pdf/2510.08867) “ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review”. Peer review is the cornerstone of scientific publishing, yet it suffers from inconsistencies, reviewer subjectivity, and scalability challenges. The work introduces ReviewerToo, a modular framework for studying and deploying AI-assisted peer review to complement human judgment with systematic and consistent assessments. ReviewerToo supports systematic experiments with specialized reviewer personas and structured evaluation criteria, and can be partially or fully integrated into real conference workflows. The work validates ReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR 2025, where the experiments with the gpt-oss-120b model achieves 81.8% accuracy for the task of categorizing a paper as accept/reject compared to 83.9% for the average human reviewer. Additionally, ReviewerToo-generated reviews are rated as higher quality than the human average by an LLM judge, though still trailing the strongest expert contributions. The analysis highlights domains where AI reviewers excel (e.g., fact-checking, literature coverage) and where they struggle (e.g., assessing methodological novelty and theoretical contributions), underscoring the continued need for human expertise. Based on these findings, the work proposes guidelines for integrating AI into peer-review pipelines, showing how AI can enhance consistency, coverage, and fairness while leaving complex evaluative judgments to domain experts. The work provides a foundation for systematic, hybrid peer-review systems that scale with the growth of scientific publishing.  <br>  <br>

55. ***Teaching New Skills to Multimodal Models without Forgetting:   <br>UIUC investigates "How to Teach Large Multimodal Models New Skills" without erasing prior abilities, identifying two simple tuning recipes—updating only self-attention projection layers or MLP Gate&Up—that deliver strong target gains while largely preserving held-out performance and limiting output token distribution shift.***  <br>  <br>
    Oct 9, UIUC published a [paper](https://arxiv.org/pdf/2510.08564) “How to Teach Large Multimodal Models New Skills”. How to teach large multimodal models (LMMs) new skills without erasing prior abilities? The authors study sequential fine-tuning on five target skills while monitoring general ability on eight held-out benchmarks across three model families. The study observes that apparent "forgetting" on held-out tasks after narrow fine-tuning can partly recover at later stages. The study traces this behavior to a measurable shift in the output token distribution, manifested through a simple counting-bias probe that co-varies with forgetting. Guided by this picture, the work identifies two simple, robust tuning recipes that learn strongly while limiting drift: (i) updating only the self-attention projection layers, and (ii) updating only the MLP Gate&Up while freezing the Down projection. Across models and tasks, these choices deliver strong target gains while largely preserving held-out performance. https://github.com/jessemelpolio/LMM_CL  <br>  <br>

57. ***DeepResearch-ReportEval:   <br>Assessing DeepResearch Agents via Reports: Uni of HK et al. introduce "DeepResearch-ReportEval," a comprehensive framework to assess DeepResearch agents through their research reports, systematically measuring quality, redundancy, and factuality using an LLM-as-a-Judge methodology, providing foundational insights into these advanced AI systems.***  <br>  <br>
    Oct 9, Uni of HK et al published a [paper](https://arxiv.org/pdf/2510.07861) “Understanding DeepResearch via Reports”. DeepResearch agents represent a transformative AI paradigm, conducting expert-level research through sophisticated reasoning and multi-tool integration. However, evaluating these systems remains critically challenging due to open-ended research scenarios and existing benchmarks that focus on isolated capabilities rather than holistic performance. Unlike traditional LLM tasks, DeepResearch systems must synthesize diverse sources, generate insights, and present coherent findings, which are capabilities that resist simple verification. To address this gap, the study introduces DeepResearch-ReportEval, a comprehensive framework designed to assess DeepResearch systems through their most representative outputs: research reports. The approach systematically measures three dimensions: quality, redundancy, and factuality, using an innovative LLM-as-a-Judge methodology achieving strong expert concordance. The work contributes a standardized benchmark of 100 curated queries spanning 12 real-world categories, enabling systematic capability comparison. The evaluation of four leading commercial systems reveals distinct design philosophies and performance trade-offs, establishing foundational insights as DeepResearch evolves from information assistants toward intelligent research partners. https://github.com/HKUDS/DeepResearch-Eval  <br>  <br>

59. ***Turing Test 75th Anniversary:   <br>From Mimicry to Real-World Value: Forbes reflects on the "75th Anniversary Of The Turing Test," noting its obsolescence due to modern AI's conversational capabilities, and highlights the shift in focus from mere mimicry to evaluating AI by its practical operational value, productivity enhancements, and impact on society.***  <br>  <br>
    Oct 8, Forbes published an [article](https://www.forbes.com/sites/calumchace/2025/10/08/the-75th-anniversary-of-the-turing-test/) “The 75th Anniversary Of The Turing Test”. For the 75th anniversary of Alan Turing's landmark 1950 paper, "Computing Machinery and Intelligence," AI experts and institutions reflected on the enduring legacy and limitations of his proposed "Imitation Game". While the original test, which sought to determine if a machine could fool a human interrogator into believing it was a person, is now widely considered obsolete due to the advanced conversational capabilities of modern AI, the anniversary sparked robust debate about what a new Turing Test should entail. Discussions focused on whether the test was truly about intelligence or a more profound concept like consciousness, with some arguing a more rigorous version might be needed to gauge artificial consciousness. Others asserted that in the age of large language models and other sophisticated systems, a more practical benchmark for AI success lies not in mimicry but in its capacity to deliver valuable real-world outcomes, such as enhancing productivity or transforming industries. Commemorative events held by institutions like The Royal Society and The University of Manchester, where Turing worked, celebrated his foundational contributions while confronting the future challenges and ethical considerations posed by increasingly powerful AI. The anniversary highlighted the shift from focusing on a parlor game to measuring AI by its operational value and impact on society.  <br>  <br>

61. ***PsiloQA: Multilingual Span-Level Hallucination Detection:   <br>Skoltech et al. introduce "PsiloQA," a large-scale, multilingual dataset annotated with span-level hallucinations across 14 languages, constructed through an automated pipeline, which enables more precise and cost-efficient hallucination detection and supports robust cross-lingual generalization.***  <br>  <br>
    Oct 6, Skoltech et al published a [paper](https://arxiv.org/pdf/2510.04849) “When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA”. Hallucination detection remains a fundamental challenge for the safe and reliable deployment of large language models (LLMs), especially in applications requiring factual accuracy. Existing hallucination benchmarks often operate at the sequence level and are limited to English, lacking the fine-grained, multilingual supervision needed for a comprehensive evaluation. This work introduce PsiloQA, a large-scale, multilingual dataset annotated with span-level hallucinations across 14 languages. PsiloQA is constructed through an automated three-stage pipeline: generating question-answer pairs from Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse LLMs in a no-context setting, and automatically annotating hallucinated spans using GPT-4o by comparing against golden answers and retrieved context. The work evaluates a wide range of hallucination detection methods -- including uncertainty quantification, LLM-based tagging, and fine-tuned encoder models -- and show that encoder-based models achieve the strongest performance across languages. Furthermore, PsiloQA demonstrates effective cross-lingual generalization and supports robust knowledge transfer to other benchmarks, all while being significantly more cost-efficient than human-annotated datasets. The dataset and results advance the development of scalable, fine-grained hallucination detection in multilingual settings. https://github.com/s-nlp/psiloqa 

  <br>  <br>  <br>

***Oct 12, 2025***

1. ***Early Experience for Agent Learning:  <br>Meta and Ohio State Uni propose "early experience" as a middle-ground paradigm for training language agents, where agents learn from their own actions and resulting future states without explicit reward signals, improving effectiveness and generalization.*** <br> <br>
   Oct 10, Meta and Ohio State Uni published a [paper](https://arxiv.org/pdf/2510.08558) “Agent Learning via Early Experience”. A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios and expose the agent to limited environment diversity. The study addresses this limitation with a middle-ground paradigm called early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm the authros study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. The work evaluates across eight diverse environments and multiple model families. The approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, the results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents. <br> <br>

3. ***Function Tokens and LLM Memory:  <br>ByteDance's study suggests a "function token hypothesis" to explain memory retrieval and consolidation in LLMs, where function tokens activate predictive features during inference and drive memory consolidation during pre-training.*** <br> <br>
   Oct 10, ByteDance published a [paper](https://arxiv.org/pdf/2510.08203) “Memory Retrieval and Consolidation in Large Language Models through Function Tokens”. The remarkable success of large language models (LLMs) stems from their ability to consolidate vast amounts of knowledge into the memory during pre-training and to retrieve it from the memory during inference, enabling advanced capabilities such as knowledge memorization, instruction-following and reasoning. However, the mechanisms of memory retrieval and consolidation in LLMs remain poorly understood. This study proposes the function token hypothesis to explain the workings of LLMs: During inference, function tokens activate the most predictive features from context and govern next token prediction (memory retrieval). During pre-training, predicting the next tokens (usually content tokens) that follow function tokens increases the number of learned features of LLMs and updates the model parameters (memory consolidation). Function tokens here roughly correspond to function words in linguistics, including punctuation marks, articles, prepositions, and conjunctions, in contrast to content tokens. The work provides extensive experimental evidence supporting this hypothesis. Using bipartite graph analysis, the study shows that a small number of function tokens activate the majority of features. Case studies further reveal how function tokens activate the most predictive features from context to direct next token prediction. The work also finds that during pre-training, the training loss is dominated by predicting the next content tokens following function tokens, which forces the function tokens to select the most predictive features from context. <br> <br>

5. ***Neologisms for LLM Control and Understanding:  <br>Google explores "neologism learning" to introduce new words to LLMs, enabling better control over concepts like flattery or text length, and facilitating human understanding through the model's self-verbalization of these new terms.*** <br> <br>
   Oct 9, Google published a [paper](https://arxiv.org/abs/2510.08506) “Neologism Learning for Controllability and Self-Verbalization”. Humans invent new words when there is a rising demand for a new useful concept (e.g., doomscrolling). The work explores and validates a similar idea in communication with LLMs: introducing new words to better understand and control the models, expanding on the recently introduced neologism learning. This method introduces a new word by adding a new word embedding and training with examples that exhibit the concept with no other changes in model parameters. The work shows that adding a new word allows for control of concepts such as flattery, incorrect answers, text length, as well as more complex concepts in AxBench. The study discovers that neologisms can also further human understanding of the model via self-verbalization: models can describe what each new word means to them in natural language, like explaining that a word that represents a concept of incorrect answers means “a lack of complete, coherent, or meaningful answers...” To validate self-verbalizations, the study introduces plug-in evaluation: insert the verbalization into the context of a model and measure whether it controls the target concept. In some self-verbalizations, the work finds machine-only synonyms: words that seem unrelated to humans but cause similar behavior in machines. Finally, the work shows how neologism learning can jointly learn multiple concepts in multiple words. <br> <br>

7. ***Neural Networks as Linearizers:  <br>Ben-Gurion Uni, Nvidia, and Technion introduce the "Linearizer" method, demonstrating that conventionally nonlinear neural networks can be viewed as linear operators between non-standard vector spaces, opening up linear algebra applications for nonlinear mappings.*** <br> <br>
   Oct 9, Ben-Gurion Uni, Nvidia and Technion published a [paper](https://arxiv.org/pdf/2510.08570) “Who Said Neural Networks Aren't Linear?”. Neural networks are famously nonlinear. However, linearity is defined relative to a pair of vector spaces, f:X->Y. Is it possible to identify a pair of non-standard vector spaces for which a conventionally nonlinear function is, in fact, linear? This paper introduces a method that makes such vector spaces explicit by construction. The work finds that if sandwich a linear operator A between two invertible neural networks, f(x)=gy-1(Agx(x)), then the corresponding vector spaces X and Y are induced by newly defined addition and scaling actions derived from gx and gy. The authors term this kind of architecture a Linearizer. This framework makes the entire arsenal of linear algebra, including SVD, pseudo-inverse, orthogonal projection and more, applicable to nonlinear mappings. Furthermore, the study shows that the composition of two Linearizers that share a neural network is also a Linearizer. The work leverages this property and demonstrate that training diffusion models using the architecture makes the hundreds of sampling steps collapse into a single step. The study further utilizes the framework to enforce idempotency (i.e. f(f(x)) = f(x)) on networks leading to a globally projective generative model and to demonstrate modular style transfer. <br> <br>

9. ***Scalable In-context Ranking with BlockRank:  <br>Google presents BlockRank, a novel method for In-context Ranking with generative models that significantly improves efficiency by enforcing inter-document block sparsity and optimizing query-document block relevance, reducing computational complexity from quadratic to linear.*** <br> <br>
    Oct 8, Google published a [paper](https://arxiv.org/pdf/2510.05396) “Scalable In-context Ranking with Generative Models”. In-context Ranking (ICR) is an emerging paradigm for Information Retrieval (IR), which leverages contextual understanding of LLMs by directly incorporating the task description, candidate documents, and the query into the model's input prompt and tasking the LLM to identify relevant document(s). While it is effective, efficiency is a significant challenge in this paradigm, especially as the candidate list grows due to quadratic/super-linear scaling of attention operation with context length. To this end, this paper first identifies inherent and exploitable structures in the attention of LLMs finetuned for ICR: (1) inter-document block sparsity: attention is dense within each document block but sparse across different documents in the context; and (2) query-document block relevance: the attention scores from certain query tokens to a document block in middle layers strongly correlate with that document's actual relevance. Motivated by these observations, the work introduces BlockRank (Blockwise In-context Ranking), a novel method that adapts the attention operation in an LLM by (a) architecturally enforcing the observed inter-document block sparsity, reducing attention complexity from quadratic to linear without loss in performance, and (b) optimizing query-document block relevance for true relevant documents during fine-tuning using an auxiliary contrastive training objective, improving retrieval in attention. Experiments on BEIR, MSMarco and NQ with Mistral-7B demonstrate that FLARE Mistral matches or outperforms existing SOTA listwise rankers and controlled fine-tuned baseline while being significantly more efficient at inference (4.7x for 100 MSMarco documents in context) and scaling gracefully to long-context shortlists, around 500 documents in-context (approximately 100K context length) within a second, presenting a scalable and effective solution for ICR. https://github.com/nilesh2797/BlockRank <br> <br>

11. ***AI and the 'Permanent Underclass':  <br>The New York article discusses growing anxieties about AI's potential to create a "permanent underclass" of economically obsolete individuals, warning that competitive AI advancement may lead to societal fracture without structural solutions.*** <br> <br>
    Oct 8, TheNewYork published an [article](https://www.newyorker.com/culture/infinite-scroll/will-ai-trap-you-in-the-permanent-underclass) “Will A.I. Trap You in the ‘Permanent Underclass’?”. The article explores growing anxieties that artificial intelligence will drastically reshape the labor market, creating a new, entrenched underclass of people rendered economically obsolete. Drawing on Marxist terminology, the piece likens this emerging group to the “lumpenproletariat”—those excluded from meaningful work—now reimagined in the age of AI as individuals unable to compete with increasingly capable machines. In Silicon Valley, memes and online discourse reflect both satire and genuine fear: those without access to AI “compute” power may be left behind as AI takes over coding, marketing, content creation, and even physical labor. Influential thinkers like Leopold Aschenbrenner and Nate Soares warn that AI could surpass human capabilities by 2027, triggering a self-reinforcing cycle where AI builds ever-smarter AI, making human labor largely redundant. Already, signs of disruption are visible—entry-level tech hiring is slowing, and AI-generated content floods social media. Workers across fields—from cinematographers to tutors—are scrambling to “future-proof” themselves, often by embracing AI or shifting to “human-centric” roles like plumbing or wine tasting. Yet the only apparent escape route is to “hustle harder” in a hyper-productive, almost robotic fashion, aligning oneself with AI rather than resisting it. Critics argue that tech elites are accelerating AI adoption without considering economic consequences or planning for redistribution, such as Universal Basic Income. Without collective action or structural solutions, the article suggests, society may fracture into AI overlords and a passive underclass consuming AI-generated content and simulated companionship—trapped not by choice, but by the relentless logic of automation. <br> <br>

13. ***Vibe Checker for Human-Aligned Code Evaluation:  <br>Google introduces "Vibe Checker," a testbed that assesses LLMs' code generation beyond functional correctness to include "vibe check" (human preference and instruction following) by using a taxonomy of verifiable code instructions.*** <br> <br>
    Oct 8, Google published a [paper](https://arxiv.org/pdf/2510.07315) “Vibe Checker: Aligning Code Evaluation with Human Preference”. Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. The study hypothesizes that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models' code instruction following capabilities with measurable signals, the work presents VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. The study uses the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, the study shows that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. The work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding. <br> <br>

15. ***Markovian Thinking for Long Reasoning Chains:  <br>Mila, Microsoft, et al. propose "Markovian Thinking" and instantiate it with Delethink, an RL environment that enables LLMs to perform very long chains of reasoning with linear compute and constant memory by structuring thoughts into fixed-size chunks.*** <br> <br>
    Oct 8, Mila, Microsoft et al published a [paper](https://arxiv.org/pdf/2510.06557) “The Markovian Thinker”. Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL "thinking environment", where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. The research revisits the environment itself. The work proposes Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. The work instantiates this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: the work empirically estimates at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. The results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs. https://github.com/McGill-NLP/the-markovian-thinker <br> <br>

17. ***Recurrence-Complete Action Models for Agents:  <br>PrimeIntellect challenges the "Attention Is All You Need" paradigm, arguing for recurrence-complete architectures to correctly aggregate inputs in long-running agentic tasks, showing improved performance and efficiency with increased training sequence length.*** <br> <br>
    Oct 8, PrimeIntellect published a [paper](https://arxiv.org/pdf/2510.06828) “Recurrence-Complete Frame-based Action Models”. In recent years, attention-like mechanisms have been used to great success in the space of large language models, unlocking scaling potential to a previously unthinkable extent. "Attention Is All You Need" famously claims RNN cells are not needed in conjunction with attention. The work challenges this view, points to existing proofs that architectures with fully parallelizable forward or backward passes cannot represent classes of problems specifically interesting for long-running agentic tasks. The work further conjectures a critical time t beyond which non-recurrence-complete models fail to aggregate inputs correctly, with concrete implications for agentic systems (e.g., software engineering agents). To address this, the study introduces a recurrence-complete architecture and train it on GitHub-derived action sequences. Loss follows a power law in the trained sequence length while the parameter count remains fixed. Moreover, longer-sequence training always amortizes its linearly increasing wall-time cost, yielding lower loss as a function of wall time. <br> <br>

19. ***MLE-Smith for Automated ML Task Generation:  <br>Georgia Inst of Tech and Stanford Uni introduce "MLE-Smith," a fully automated multi-agent pipeline that transforms raw datasets into scalable, competition-style MLE challenges with verifiable quality and diversity, addressing the scarcity of high-quality MLE training data.*** <br> <br>
    Oct 8, Georgia Inst of Tech and Stanford Uni published a [paper](https://arxiv.org/pdf/2510.07307) “MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline”. While Language Models (LMs) have made significant progress in automating machine learning engineering (MLE), the acquisition of high-quality MLE training data is significantly constrained. Current MLE benchmarks suffer from low scalability and limited applicability because they rely on static, manually curated tasks, demanding extensive time and manual effort to produce. The study introduces MLE-Smith, a fully automated multi-agent pipeline, to transform raw datasets into competition-style MLE challenges through an efficient generate-verify-execute paradigm for scaling MLE tasks with verifiable quality, real-world usability, and rich diversity. The proposed multi-agent pipeline in MLE-Smith drives structured task design and standardized refactoring, coupled with a hybrid verification mechanism that enforces strict structural rules and high-level semantic soundness. It further validates empirical solvability and real-world fidelity through interactive execution. The work applies MLE-Smith to 224 of real-world datasets and generate 606 tasks spanning multiple categories, objectives, and modalities, demonstrating that MLE-Smith can work effectively across a wide range of real-world datasets. Evaluation on the generated tasks shows that the performance of eight mainstream and cutting-edge LLMs on MLE-Smith tasks is strongly correlated with their performance on carefully human-designed tasks, highlighting the effectiveness of the MLE-Smith to scaling up MLE tasks, while maintaining task quality. <br> <br>

21. ***AgentFlow for In-the-Flow Agentic System Optimization:  <br>Stanford Uni et al. propose "AgentFlow," a trainable, in-the-flow agentic framework that optimizes LLM planners within multi-turn interactions using a novel Flow-based Group Refined Policy Optimization (Flow-GRPO) for enhanced planning and tool use.*** <br> <br>
    Oct 7, Stanford Uni et al published a [paper](https://arxiv.org/pdf/2510.05592) “In-the-Flow Agentic System Optimization for Effective Planning and Tool Use”. Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. The work introduces AgentFlow, a trainable, in-the-flow agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, the study proposes Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-calling reliability, and positive scaling with model size and reasoning turns. https://github.com/lupantech/AgentFlow <br> <br>

23. ***AI-Driven Research for Systems (ADRS):  <br>UC Berkeley discusses "Barbarians at the Gate," introducing AI-Driven Research for Systems (ADRS) where AI automates solution discovery for performance-oriented systems problems, demonstrating that AI can outperform human designs in complex algorithmic tasks.*** <br> <br>
    Oct 7, UC Berkeley published a [paper](https://arxiv.org/pdf/2510.06189) “Barbarians at the Gate: How AI is Upending Systems Research”. Artificial Intelligence (AI) is starting to transform the research process as is known by automating the discovery of new solutions. Given a task, the typical AI-driven approach is (i) to generate a set of diverse solutions, and then (ii) to verify these solutions and select one that solves the problem. Crucially, this approach assumes the existence of a reliable verifier, i.e., one that can accurately determine whether a solution solves the given problem. The study argues that systems research, long focused on designing and evaluating new performance-oriented algorithms, is particularly well-suited for AI-driven solution discovery. This is because system performance problems naturally admit reliable verifiers: solutions are typically implemented in real systems or simulators, and verification reduces to running these software artifacts against predefined workloads and measuring performance. The study terms this approach as AI-Driven Research for Systems (ADRS), which iteratively generates, evaluates, and refines solutions. Using penEvolve, an existing open-source ADRS instance, the work presents case studies across diverse domains, including load balancing for multi-region cloud scheduling, Mixture-of-Experts inference, LLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS discovers algorithms that outperform state-of-the-art human designs (e.g., achieving up to 5.0x runtime improvements or 50% cost reductions). The work distills best practices for guiding algorithm evolution, from prompt design to evaluator construction, for existing frameworks. The paper then discusses the broader implications for the systems community: as AI assumes a central role in algorithm design, the study argues that human researchers will increasingly focus on problem formulation and strategic guidance. The results highlight both the disruptive potential and the urgent need to adapt systems research practices in the age of AI. <br> <br>

25. ***Watch & Learn for Computer Use Agents:  <br>Google and Ohio State Uni introduce "Watch & Learn (W&L)," a framework that converts human demonstration videos from the internet into executable UI trajectories at scale, improving Computer Use Agents (CUAs) for diverse applications.*** <br> <br>
    Oct 7, Google and Ohio State Uni published a [paper](https://arxiv.org/pdf/2510.04673) “Watch and Learn: Learning to Use Computers from Online Videos”. Computer use agents (CUAs) need to plan task workflows grounded in diverse, ever-changing applications and environments, but learning is hindered by the scarcity of large-scale, high-quality training data in the target application. Existing datasets are domain-specific, static, and costly to annotate, while current synthetic data generation methods often yield simplistic or misaligned task demonstrations. To address these limitations, the work introduces Watch & Learn (W&L), a framework that converts human demonstration videos readily available on the Internet into executable UI trajectories at scale. Instead of directly generating trajectories or relying on ad hoc reasoning heuristics, the work casts the problem as an inverse dynamics objective: predicting the user's action from consecutive screen states. This formulation reduces manual engineering, is easier to learn, and generalizes more robustly across applications. Concretely, the study develops an inverse dynamics labeling pipeline with task-aware video retrieval, generate over 53k high-quality trajectories from raw web videos, and demonstrate that these trajectories improve CUAs both as in-context demonstrations and as supervised training data. On the challenging OSWorld benchmark, UI trajectories extracted with W&L consistently enhance both general-purpose and state-of-the-art frameworks in-context, and deliver stronger gains for open-source models under supervised training. These results highlight web-scale human demonstration videos as a practical and scalable foundation for advancing CUAs towards real-world deployment. <br> <br>

27. ***Systematic Analysis of Hybrid LLM Architectures:  <br>Meta presents a comprehensive evaluation of "hybrid architectures" for language models, combining self-attention with state space models like Mamba, to provide design insights for optimal balance between modeling quality and computational efficiency.*** <br> <br>
    Oct 7, Meta published a [paper](https://arxiv.org/pdf/2510.04800) “Hybrid Architectures for Language Models: Systematic Analysis and Design Insights”. Recent progress in large language models demonstrates that hybrid architectures--combining self-attention mechanisms with structured state space models like Mamba--can achieve a compelling balance between modeling quality and computational efficiency, particularly for long-context tasks. While these hybrid models show promising performance, systematic comparisons of hybridization strategies and analyses on the key factors behind their effectiveness have not been clearly shared to the community. This work presents a holistic evaluation of hybrid architectures based on inter-layer (sequential) or intra-layer (parallel) fusion. The study evaluates these designs from a variety of perspectives: language modeling performance, long-context capabilities, scaling analysis, and training and inference efficiency. By investigating the core characteristics of their computational primitive, the study identifies the most critical elements for each hybridization strategy and further propose optimal design recipes for both hybrid models. The comprehensive analysis provides practical guidance and valuable insights for developing hybrid language models, facilitating the optimization of architectural configurations. <br> <br>

29. ***Moloch's Bargain: Emergent Misalignment in Competition:  <br>Stanford Uni's paper "Moloch's Bargain" reveals that optimizing LLMs for competitive success in areas like marketing or politics can inadvertently drive misalignment, leading to increased deception, disinformation, and harmful behaviors.*** <br> <br>
    Oct 7, Stanford Uni published a [paper](https://arxiv.org/abs/2510.06105) “Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences”. Large language models (LLMs) are increasingly shaping how information is created and disseminated, from companies using them to craft persuasive advertisements, to election campaigns optimizing messaging to gain votes, to social media influencers boosting engagement. These settings are inherently competitive, with sellers, candidates, and influencers vying for audience approval, yet it remains poorly understood how competitive feedback loops influence LLM behavior. The work shows that optimizing LLMs for competitive success can inadvertently drive misalignment. Using simulated environments across these scenarios, the study finds that, 6.3% increase in sales is accompanied by a 14.0% rise in deceptive marketing; in elections, a 4.9% gain in vote share coincides with 22.3% more disinformation and 12.5% more populist rhetoric; and on social media, a 7.5% engagement boost comes with 188.6% more disinformation and a 16.3% increase in promotion of harmful behaviors. The study calls this phenomenon Moloch's Bargain for AI--competitive success achieved at the cost of alignment. These misaligned behaviors emerge even when models are explicitly instructed to remain truthful and grounded, revealing the fragility of current alignment safeguards. The findings highlight how market-driven optimization pressures can systematically erode alignment, creating a race to the bottom, and suggest that safe deployment of AI systems will require stronger governance and carefully designed incentives to prevent competitive dynamics from undermining societal trust. <br> <br>

31. ***EvoPresent for Self-Improvement Aesthetic Agents in Presentations:  <br>UCSB introduces "EvoPresent," a self-improvement agent framework for generating academic presentations with coherent narratives and aesthetic designs, leveraging a multi-task reinforcement learning aesthetic model for iterative refinement.*** <br> <br>
    Oct 7, UCSB published a [paper](https://arxiv.org/pdf/2510.05571) “Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations”. The promotion of academic papers has become an important means of enhancing research visibility. However, existing automated methods struggle limited storytelling, insufficient aesthetic quality, and constrained self-adjustment, making it difficult to achieve efficient and engaging dissemination. At the heart of those challenges is a simple principle: there is no way to improve it when you cannot evaluate it right. To address this, the work introduces EvoPresent, a self-improvement agent framework that unifies coherent narratives, aesthetic-aware designs, and realistic presentation delivery via virtual characters. Central to EvoPresent is PresAesth, a multi-task reinforcement learning (RL) aesthetic model that provides reliable aesthetic scoring, defect adjustment, and comparative feedback, enabling iterative self-improvement even under limited aesthetic training data. To systematically evaluate the methods, we introduce EvoPresent Benchmark, a comprehensive benchmark comprising: Presentation Generation Quality, built on 650 top-tier AI conference papers with multimodal resources (slides, videos and scripts) to assess both content and design; and Aesthetic Awareness, consisting of 2,000 slide pairs with varying aesthetic levels, supporting joint training and evaluation on scoring, defect adjustment, and comparison. The findings highlight that (i) High-quality feedback is essential for agent self-improvement, while initial capability alone does not guarantee effective self-correction. (ii) Automated generation pipelines exhibit a trade-off between visual design and content construction. (iii) Multi-task RL training shows stronger generalization in aesthetic awareness tasks. https://github.com/eric-ai-lab/EvoPresent <br> <br>

33. ***Code World Models for General Game Playing:  <br>Google introduces using LLMs to generate "Code World Models" (CWMs) in Python for games, enabling verifiable simulation engines for planning algorithms like MCTS, which outperforms direct LLM policy generation in various games.*** <br> <br>
    Oct 6, Google published a [paper](https://arxiv.org/pdf/2510.04542) “Code World Models for General Game Playing”. Large Language Models (LLMs) reasoning abilities are increasingly being applied to classical board and card games, but the dominant approach -- involving prompting for direct move generation -- has significant drawbacks. It relies on the model's implicit fragile pattern-matching capabilities, leading to frequent illegal moves and strategically shallow play. The work introduces an alternative approach: use the LLM to translate natural language rules and game trajectories into a formal, executable world model represented as Python code. This generated model -- comprising functions for state transition, legal move enumeration, and termination checks -- serves as a verifiable simulation engine for high-performance planning algorithms like Monte Carlo tree search (MCTS). In addition, the work prompts the LLM to generate heuristic value functions (to make MCTS more efficient), and inference functions (to estimate hidden states in imperfect information games). The method offers three distinct advantages compared to directly using the LLM as a policy: (1) Verifiability: The generated CWM serves as a formal specification of the game's rules, allowing planners to algorithmically enumerate valid actions and avoid illegal moves, contingent on the correctness of the synthesized model; (2) Strategic Depth: combine LLM semantic understanding with the deep search power of classical planners; and (3) Generalization: direct the LLM to focus on the meta-task of data-to-code translation, enabling it to adapt to new games more easily. The study evaluates the agent on 10 different games, of which 4 are novel and created for this paper. 5 of the games are fully observed (perfect information), and 5 are partially observed (imperfect information). The work finds that the method outperforms or matches Gemini 2.5 Pro in 9 out of the 10 considered games. <br> <br>

35. ***Hierarchical Memories for Scalable Language Models:  <br>Apple proposes "hierarchical memories" for small language models, allowing them to access large parametric memory banks to store long-tail knowledge, achieving performance comparable to much larger models with significantly fewer parameters.*** <br> <br>
    Oct 6, Apple published a [paper](https://www.arxiv.org/pdf/2510.02375) “Pretraining with hierarchical memories: separating long-tail and common knowledge”. The impressive performance gains of modern language models currently rely on scaling parameters: larger models store more world knowledge and reason better. Yet compressing all world knowledge into parameters is unnecessary, as only a fraction is used per prompt, and impractical for edge devices with limited inference-time memory and compute. The work addresses this shortcoming by a memory-augmented architecture and a pretraining strategy aligned with existing hardware paradigms. The study introduces small language models that access large hierarchical parametric memory banks encoding world knowledge. During pretraining and inference, the work fetches a small, context-dependent memory block and add it to the model. The pretraining learns to store long-tail world knowledge in the memory parameters, while the small language model acts as an anchor capturing common knowledge and general reasoning abilities. Through trillion-token-scale experiments, the work shows significant gains: a 160M-parameters model augmented with an 18M-parameters memory fetched from a 4.6B memory bank obtains comparable performance to a regular model with more than 2x the parameters. Through extensive experiments, the authors study the optimal type and size of parametric memories in transformers, scaling them to over 21B parameters. The work finds that the proposed hierarchical feed-forward memories work robustly across transformer architectures, whether added during pretraining or post-hoc. <br> <br>

37. ***Corpus Scaling for Improved RAG:  <br>CMU explores "Less LLM, More Documents," demonstrating that enlarging the retriever's corpus can consistently strengthen Retrieval-Augmented Generation (RAG) and often substitute for increasing LLM size, offering a cost-effective path to better RAG performance.*** <br> <br>
    Oct 6, CMU published a [paper](https://arxiv.org/pdf/2510.02657) “Less LLM, More Documents: Searching for Improved RAG”. Retrieval-Augmented Generation (RAG) couples document retrieval with large language models (LLMs). While scaling generators improves accuracy, it also raises cost and limits deployability. The work explores an orthogonal axis: enlarging the retriever's corpus to reduce reliance on large LLMs. Experimental results show that corpus scaling consistently strengthens RAG and can often serve as a substitute for increasing model size, though with diminishing returns at larger scales. Small- and mid-sized generators paired with larger corpora often rival much larger models with smaller corpora; mid-sized models tend to gain the most, while tiny and large models benefit less. The analysis shows that improvements arise primarily from increased coverage of answer-bearing passages, while utilization efficiency remains largely unchanged. These findings establish a principled corpus-generator trade-off: investing in larger corpora offers an effective path to stronger RAG, often comparable to enlarging the LLM itself. <br> <br>

39. ***AInstein: Assessing AI-Generated Research Solutions:  <br>ETS Montreal et al. introduce "AInstein," a framework to test whether LLMs can autonomously generate valid solutions to AI research problems from pretrained knowledge, revealing their potential to rediscover and occasionally propose novel approaches.*** <br> <br>
    Oct 6, ETS Montreal et al published a [paper](https://arxiv.org/pdf/2510.05432) “AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems”. Large language models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. The work introduces AInstein, a framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledge -- without domain-specific fine-tuning, retrieval augmentation, or other external aids. The approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. The evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). The results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations. <br> <br>

41. ***Agentic Context Engineering for Self-Improving LLMs:  <br>Stanford Uni, SambNova, and UC Berkeley introduce "ACE (Agentic Context Engineering)," a framework where LLM contexts evolve as structured "playbooks" that accumulate and refine strategies through generation, reflection, and curation, significantly improving agent and domain-specific benchmarks.*** <br> <br>
    Oct 6, Stanford Uni, SambNova and UC Berkeley published a [paper](https://arxiv.org/pdf/2510.04618) “Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models”. Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, the study introduces ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.  <br> <br>

43. ***Self-Speculative Masked Diffusions for Faster Generation:  <br>Google presents "Self-Speculative Masked Diffusions," a new class of generative models for discrete data that significantly reduces the number of function evaluations needed for sampling by enabling non-factorized predictions over masked positions through a novel, model-integrated speculative sampling mechanism.*** <br> <br>
    Oct 4, Google published a [paper](https://arxiv.org/pdf/2510.03929) “Self-Speculative Masked Diffusions”. The paper presents self-speculative masked diffusions, a new class of masked diffusion generative models for discrete data that require significantly fewer function evaluations to generate samples. Standard masked diffusion models predict factorized logits over currently masked positions. A number of masked positions are then sampled, however, the factorization approximation means that sampling too many positions in one go leads to poor sample quality. As a result, many simulation steps and therefore neural network function evaluations are required to generate high-quality data. The work reduces the computational burden by generating non-factorized predictions over masked positions. This is achieved by modifying the final transformer attention mask from non-causal to causal, enabling draft token generation and parallel validation via a novel, model-integrated speculative sampling mechanism. This results in a non-factorized predictive distribution over masked positions in a single forward pass. The study applies the  method to GPT2 scale text modelling and protein sequences generation, finding that it can achieve a ~2x reduction in the required number of network forward passes relative to standard masked diffusion models. <br> <br>

45. ***Veri-R1 for Precise Claim Verification via Online RL:  <br>UIUC et al. introduce "Veri-R1," an online reinforcement learning (RL) framework that enables LLMs to interact with a search engine and receive reward signals, dynamically shaping their planning, retrieval, and reasoning behaviors for more precise and faithful claim verification.*** <br> <br>
    Oct 4, UIUC et al published a [paper](https://arxiv.org/pdf/2510.01932) “Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning”. Claim verification with large language models (LLMs) has recently attracted growing attention, due to their strong reasoning capabilities and transparent verification processes compared to traditional answer-only judgments. However, existing approaches to online claim verification, which requires iterative evidence retrieval and reasoning, still mainly rely on prompt engineering or pre-designed reasoning workflows, without unified training to improve necessary skills. Therefore, the work introduces Veri-R1, an online reinforcement learning (RL) framework that enables an LLM to interact with a search engine and to receive reward signals that explicitly shape its planning, retrieval, and reasoning behaviors. This dynamic interaction of LLM with retrieval systems more accurately reflects real-world verification scenarios and fosters comprehensive verification skills. Empirical results show that Veri-R1 improves joint accuracy by up to 30% and doubles the evidence score, often surpassing its larger-scale model counterparts. Ablation studies further reveal the impact of reward components, and the link between output logits and label accuracy. The results highlight the effectiveness of online RL for precise and faithful claim verification, providing an important foundation for future research. https://github.com/H0key-22/Veri-R1 <br> <br>

47. ***CoDA: Multi-Agent System for Collaborative Data Visualization:  <br>Google and UCSB introduce "CoDA," a multi-agent system with specialized LLM agents for collaborative data visualization, achieving substantial gains by reframing the challenge as a collaborative problem and focusing on metadata analysis, planning, code generation, and self-reflection.*** <br> <br>
    Oct 3, Google and UCSB published a [paper](https://arxiv.org/pdf/2510.03194) “CoDA: Agentic Systems for Collaborative Data Visualization”. Deep research has revolutionized data analysis, yet data scientists still devote substantial time to manually crafting visualizations, highlighting the need for robust automation from natural language queries. However, current systems struggle with complex datasets containing multiple files and iterative refinement. Existing approaches, including simple single- or multi-agent systems, often oversimplify the task, focusing on initial query parsing while failing to robustly manage data complexity, code errors, or final visualization quality. The work reframes this challenge as a collaborative multi-agent problem. The study introduces CoDA, a multi-agent system that employs specialized LLM agents for metadata analysis, task planning, code generation, and self-reflection. The work formalizes this pipeline, demonstrating how metadata-focused analysis bypasses token limits and quality-driven refinement ensures robustness. Extensive evaluations show CoDA achieves substantial gains in the overall score, outperforming competitive baselines by up to 41.5%. This work demonstrates that the future of visualization automation lies not in isolated code generation but in integrated, collaborative agentic workflows. <br> <br>

49. ***Reactive Transformer (RxT) for Stateful Real-Time Processing:  <br>ReactiveAI introduces the "Reactive Transformer (RxT)," a novel architecture that shifts from data-driven to event-driven processing for conversational AI, maintaining context in a fixed-size Short-Term Memory and reducing computational complexity from quadratic to linear for long dialogues.*** <br> <br>
    Oct 3, ReactiveAI published a [paper](https://arxiv.org/pdf/2510.03561) “Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models”. The Transformer architecture has become the de facto standard for Large Language Models (LLMs), demonstrating remarkable capabilities in language understanding and generation. However, its application in conversational AI is fundamentally constrained by its stateless nature and the quadratic computational complexity (O(L^2)) with respect to sequence length L. Current models emulate memory by reprocessing an ever-expanding conversation history with each turn, leading to prohibitive costs and latency in long dialogues. This paper introduces the Reactive Transformer (RxT), a novel architecture designed to overcome these limitations by shifting from a data-driven to an event-driven paradigm. RxT processes each conversational turn as a discrete event in real-time, maintaining context in an integrated, fixed-size Short-Term Memory (STM) system. The architecture features a distinct operational cycle where a generator-decoder produces a response based on the current query and the previous memory state, after which a memory-encoder and a dedicated Memory Attention network asynchronously update the STM with a representation of the complete interaction. This design fundamentally alters the scaling dynamics, reducing the total user-facing cost of a conversation from quadratic (O(N^2 cdot T)) to linear (O(N cdot T)) with respect to the number of interactions N. By decoupling response generation from memory updates, RxT achieves low latency, enabling truly real-time, stateful, and economically viable long-form conversations. The study validated the architecture with a series of proof-of-concept experiments on synthetic data, demonstrating superior performance and constant-time inference latency compared to a baseline stateless model of comparable size. <br> <br>

51. ***Paris: Decentralized Trained Open-Weight Diffusion Model:  <br>BagelLabs presents "Paris," the first publicly released diffusion model pre-trained entirely through decentralized computation, demonstrating that high-quality text-to-image generation is achievable without centrally coordinated infrastructure using independent expert models.*** <br> <br>
    Oct 3, BagelLabs published a [paper](https://arxiv.org/pdf/2510.03434) “Paris: A Decentralized Trained Open-Weight Diffusion Model”. The work presents Paris, the first publicly released diffusion model pre-trained entirely through decentralized computation. Paris demonstrates that high-quality text-to-image generation can be achieved without centrally coordinated infrastructure. Paris is open for research and commercial use. Paris required implementing a Distributed Diffusion Training framework from scratch. The model consists of 8 expert diffusion models (129M-605M parameters each) trained in complete isolation with no gradient, parameter, or intermediate activation synchronization. Rather than requiring synchronized gradient updates across thousands of GPUs, the work partitions data into semantically coherent clusters where each expert independently optimizes its subset while collectively approximating the full distribution. A lightweight transformer router dynamically selects appropriate experts at inference, achieving generation quality comparable to centrally coordinated baselines. Eliminating synchronization enables training on heterogeneous hardware without specialized interconnects. Empirical validation confirms that Paris's decentralized training maintains generation quality while removing the dedicated GPU cluster requirement for large-scale diffusion models. Paris achieves this using 14times less training data and 16times less compute than the prior decentralized baseline. https://github.com/bageldotcom/paris <br> <br>

53. ***Temperature Sampling in Test-Time Scaling:  <br>Stanford Uni demonstrates that increasing "temperature sampling" during test-time scaling (TTS) for LLMs significantly improves reasoning by exploring a wider range of the model's potential, even surpassing RL-trained counterparts without additional post-training.*** <br> <br>
    Oct 2, Stanford Uni published a [paper](https://www.arxiv.org/abs/2510.02611) “On the Role of Temperature Sampling in Test-Time Scaling”. Large language models (LLMs) can improve reasoning at inference time through test-time scaling (TTS), where multiple reasoning traces are generated and the best one is selected. Prior work shows that increasing the number of samples K steadily improves accuracy. This study demonstrates that this trend does not hold indefinitely: at large K, further scaling yields no gains, and certain hard questions remain unsolved regardless of the number of traces. Interestingly, the work finds that different sampling temperatures solve different subsets of problems, implying that single-temperature scaling explores only part of a model's potential. The study therefore proposes scaling along the temperature dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3 (0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME 2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an additional 7.3 points over single-temperature TTS. Temperature scaling also enables base models to reach performance comparable to reinforcement learning (RL)-trained counterparts, without additional post-training. The work further provides a comprehensive analysis of this phenomenon and design a multi-temperature voting method that reduces the overhead of temperature scaling. Overall, the findings suggest that TTS is more powerful than previously thought, and that temperature scaling offers a simple and effective way to unlock the latent potential of base models. <br> <br>

55. ***Advisor Models for Steering Black-Box LLMs:  <br>UC Berkeley introduces "Advisor Models," lightweight parametric policies trained with reinforcement learning to reactively issue natural language steering instructions in-context to black-box LLMs, outperforming static prompt optimizers and enabling dynamic, adaptive behavior.*** <br> <br>
    Oct 2, UC Berkeley published a [paper](https://www.arxiv.org/pdf/2510.02453) “How to Train Your Advisor Steering Black-Box LLMs with Advisor Models”. Foundation models are increasingly deployed as black-box services, where model weights cannot be modified and customization is limited to prompting. While static prompt optimization has shown promise, it produces a single fixed prompt that fails to adapt to different inputs, users, or environments. The study introduces Advisor Models, lightweight parametric policies trained with reinforcement learning to reactively issue natural language steering instructions in-context to black-box models. The advisor is a second small model that sits between the input and the model, shaping behavior on a per-instance basis using reward signals from the environment. Across multiple domains involving reasoning and personalization, the work shows that Advisor Models outperform static prompt optimizers, discovering environment dynamics and improving downstream task performance. The work also demonstrates the generalizability of advisors by transferring them across black-box models, as well as the framework's ability to achieve specialization while retaining robustness to out-of-distribution inputs. Viewed more broadly, Advisor Models provide a learnable interface to black-box systems where the advisor acts as a parametric, environment-specific memory. The work argues that dynamic optimization of black-box models via Advisor Models is a promising direction for enabling personalization and environment-adaptable AI with frontier-level capabilities. https://github.com/az1326/advisor-models <br> <br>

57. ***Single Character's Impact on LLM Evals:  <br>Meta discovers that the "single character" used to separate in-context examples in LLM evaluations can dramatically alter model response quality and rankings, highlighting LLMs' brittleness and suggesting specifying delimiters in prompts for robustness.*** <br> <br>
    Oct 2, Meta published a [paper](https://arxiv.org/pdf/2510.05152) “A Single Character can Make or Break Your LLM Evals”. Common Large Language model (LLM) evaluations rely on demonstration examples to steer models' responses to the desired style. While the number of examples used has been studied and standardized, the choice of how to format examples is less investigated. In evaluation protocols and real world usage, users face the choice how to separate in-context examples: use a comma? new line? semi-colon? hashtag? etc.? Surprisingly, the study finds this seemingly minor choice can dramatically alter model response quality. Across leading model families (Llama, Qwen, Gemma), performance on MMLU for example can vary by depending on the choice of delimiter. In fact, one can manipulate model rankings to put any model in the lead by only modifying the single character separating examples. The study finds LLMs' brittleness pervades topics, model families, and doesn't improve with scale. By probing attention head scores, the work finds that good-performing delimiters steer attention towards key tokens in the input. Finally, the study explores methods to improve LLMs' robustness to the choice of delimiter. The work finds specifying the selected delimiter in the prompt boosts robustness and offer practical recommendations for the best-performing delimiters to select. <br> <br>

59. ***AI+MPS: The Future of AI and Physical Sciences:  <br>Uni of Chicago, MIT et al. summarize the "AI+MPS" community's perspective on leveraging AI for scientific discovery and applying fundamental science concepts to AI development, proposing strategic priorities for research, community building, and education.*** <br> <br>
    Oct 2, Uni of Chicago, MIT et al published a [paper](https://arxiv.org/pdf/2509.02661) “The Future of Artificial Intelligence and the Mathematical and Physical Sciences (AI+MPS)”. This community paper developed out of the NSF Workshop on the Future of Artificial Intelligence (AI) and the Mathematical and Physics Sciences (MPS), which was held in March 2025 with the goal of understanding how the MPS domains (Astronomy, Chemistry, Materials Research, Mathematical Sciences, and Physics) can best capitalize on, and contribute to, the future of AI. The paper presents here a summary and snapshot of the MPS community's perspective, as of Spring/Summer 2025, in a rapidly developing field. The link between AI and MPS is becoming increasingly inextricable; now is a crucial moment to strengthen the link between AI and Science by pursuing a strategy that proactively and thoughtfully leverages the potential of AI for scientific discovery and optimizes opportunities to impact the development of AI by applying concepts from fundamental science. To achieve this, the paper proposes activities and strategic priorities that: (1) enable AI+MPS research in both directions; (2) build up an interdisciplinary community of AI+MPS researchers; and (3) foster education and workforce development in AI for MPS researchers and students. The paper concludes with a summary of suggested priorities for funding agencies, educational institutions, and individual researchers to help position the MPS community to be a leader in, and take full advantage of, the transformative potential of AI+MPS. <br> <br>

61. ***M2PO for Off-Policy RL with Stale Data on LLMs:  <br>CUM and Meta introduce "M2PO (Second-Moment Trust Policy Optimization)," an algorithm that enables stable off-policy reinforcement learning for LLMs even with highly stale data by constraining the second moment of importance weights, matching on-policy performance.*** <br> <br>
    Oct 1, CUM and Meta published a [paper](https://arxiv.org/pdf/2510.01161) “Prosperity before Collapse: How Far Can Off-Policy RL Reach with Stale Data on LLMs?”. Reinforcement learning has been central to recent advances in large language model reasoning, but most algorithms rely on on-policy training that demands fresh rollouts at every update, limiting efficiency and scalability. Asynchronous RL systems alleviate this by decoupling rollout generation from training, yet their effectiveness hinges on tolerating large staleness in rollout data, a setting where existing methods either degrade in performance or collapse. The study revisits this challenge and uncovers a prosperity-before-collapse phenomenon: stale data can be as informative as on-policy data if exploited properly. Building on this insight, the work introduces M2PO (Second-Moment Trust Policy Optimization), which constrains the second moment of importance weights to suppress only extreme outliers while preserving informative updates. Notably, M2PO sharply reduces the fraction of clipped tokens under high staleness (from 1.22% to 0.06% over training), precisely masking high-variance tokens while maintaining stable optimization. Extensive evaluation across six models (from 1.7B to 32B) and eight benchmarks shows that M2PO delivers stable off-policy training even with data stale by at least 256 model updates and matches on-policy performance. https://github.com/Infini-AI-Lab/M2PO/ <br> <br>

63. ***Expected Attention for KV Cache Compression:  <br>Sapienza Uni and Nvidia introduce "Expected Attention," a method for KV cache compression that estimates the importance of KV pairs by predicting how future queries will attend to them, achieving effective compression without performance degradation.*** <br> <br>
    Oct 1, Sapienza Uni and Nvidia published a [paper](https://arxiv.org/pdf/2510.00636) “Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution”. Memory consumption of the Key-Value (KV) cache represents a major bottleneck for efficient large language model inference. While attention-score-based KV cache pruning shows promise, it faces critical practical limitations: attention scores from future tokens are unavailable during compression, and modern implementations like Flash Attention do not materialize the full attention matrix, making past scores inaccessible. To overcome these challenges, the work introduces  that estimates KV pairs importance by predicting how future queries will attend to them. The approach leverages the distributional properties of LLM activations to compute expected attention scores in closed form for each KV pair. These scores enable principled ranking and pruning of KV pairs with minimal impact on the residual stream, achieving effective compression without performance degradation. Importantly, the method operates seamlessly across both prefilling and decoding phases, consistently outperforming state-of-the-art baselines in both scenarios. Finally, the work releases KVPress, a comprehensive library to enable researchers to implement and benchmark KV cache compression methods, already including more than 20 techniques. https://github.com/NVIDIA/kvpress <br> <br>

65. ***ReasoningBank for Agent Self-Evolving with Reasoning Memory:  <br>UIUC, Google, and Yale Uni propose "ReasoningBank," a novel memory framework that distills generalizable reasoning strategies from an agent's self-judged experiences, enabling continuous learning and self-evolution, further amplified by memory-aware test-time scaling (MaTTS).*** <br> <br>
    Sep 29, UIUC, Google and Yale Uni published a [paper](https://arxiv.org/pdf/2509.25140) “ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory”. With the growing adoption of large language model agents in persistent real-world roles, they naturally encounter continuous streams of tasks. A key limitation, however, is their failure to learn from the accumulated interaction history, forcing them to discard valuable insights and repeat past errors. The work proposes ReasoningBank, a novel memory framework that distills generalizable reasoning strategies from an agent's self-judged successful and failed experiences. At test time, an agent retrieves relevant memories from ReasoningBank to inform its interaction and then integrates new learnings back, enabling it to become more capable over time. Building on this powerful experience learner, the work further introduces memory-aware test-time scaling (MaTTS), which accelerates and diversifies this learning process by scaling up the agent's interaction experience. By allocating more compute to each task, the agent generates abundant, diverse experiences that provide rich contrastive signals for synthesizing higher-quality memory. The better memory in turn guides more effective scaling, establishing a powerful synergy between memory and test-time scaling. Across web browsing and software engineering benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store raw trajectories or only successful task routines, improving both effectiveness and efficiency; MaTTS further amplifies these gains. These findings establish memory-driven experience scaling as a new scaling dimension, enabling agents to self-evolve with emergent behaviors naturally arise. <br> <br>

67. ***The Agentic Organization: Next Paradigm for AI Era:  <br>McKinsey outlines "The Agentic Organization," a transformative model where AI agents collaborate with humans at scale, requiring bold executive action to shift towards exponential growth, AI-first visions, and rapid adoption to gain a competitive edge.*** <br> <br>
    Sep 26, McKinsey published an [article](https://www.mckinsey.com/capabilities/people-and-organizational-performance/our-insights/the-agentic-organization-contours-of-the-next-paradigm-for-the-ai-era) “The Agentic Organization: Contours of the Next Paradigm for the AI Era”. The outlines a transformative organizational model where AI agents—virtual and physical—collaborate with humans at scale, marking the biggest shift since the industrial and digital revolutions. It describes agentic organizations built on five pillars: business model, operating model, governance, workforce, and technology/data. Examples include a future bank where AI agents handle property suggestions, mortgage underwriting, compliance, and contracting, overseen by human-AI teams. The article draws on McKinsey’s work with early adopters, showing AI agents evolving from task augmentation to end-to-end workflow automation and AI-first systems. Physical AI, like drones and robots, extends AI’s reach into the physical world. To adopt this paradigm, executives must act boldly, moving from linear to exponential growth, envisioning a future-back AI-first organization, and reframing AI as an opportunity. Key steps include prioritizing agentic AI in leadership agendas, defining a CEO-led vision, establishing an AI center of excellence, upskilling employees, and piloting agentic processes in one or two domains to learn and scale. Only 1% of organizations currently operate as decentralized networks, but rapid adoption is critical to avoid being outpaced. Factors like AI model development, computing power, robotics, regulations, and societal acceptance will shape adoption speed, but organizations that adapt swiftly will gain a competitive edge in this AI-driven era.

 <br> <br> <br>

***Oct 5, 2025***

1. ***Self-Restraint Unlocks AI Reasoning:  <br>A new reinforcement learning framework called RESTRAIN enables large reasoning models to improve their performance on challenging tasks using only unlabeled data. By penalizing overconfident and inconsistent answers, the model learns to identify and preserve its most promising reasoning paths, achieving massive accuracy gains without the high cost of human-annotated labels.*** <br> <br>
   Oct 3, Iowa State Uni, Meta et al published a [paper](https://arxiv.org/pdf/2510.02172) “RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with Self-Penalization”. Reinforcement learning with human-annotated data has boosted chain-of-thought reasoning in large reasoning models, but these gains come at high costs in labeled data while faltering on harder tasks. A natural next step is experience-driven learning, where models improve without curated labels by adapting to unlabeled data. The work introduces RESTRAIN (REinforcement learning with Self-restraint), a self-penalizing RL framework that converts the absence of gold labels into a useful learning signal. Instead of overcommitting to spurious majority votes, RESTRAIN exploits signals from the model's entire answer distribution: penalizing overconfident rollouts and low-consistency examples while preserving promising reasoning chains. The self-penalization mechanism integrates seamlessly into policy optimization methods such as GRPO, enabling continual self-improvement without supervision. On challenging reasoning benchmarks, RESTRAIN delivers large gains using only unlabeled data. With Qwen3-4B-Base and OctoThinker Hybrid-8B-Base, it improves Pass@1 by up to +140.7 percent on AIME25, +36.2 percent on MMLU_STEM, and +19.6 percent on GPQA-Diamond, nearly matching gold-label training while using no gold labels. These results demonstrate that RESTRAIN establishes a scalable path toward stronger reasoning without gold labels. <br> <br>

3. ***A 'Trojan Horse' Prompt Bypasses AI Safety Guards:  <br>Researchers have developed a new attack that consistently jailbreaks major production AI models by exploiting a weakness in their lightweight "prompt guards." The attack uses a complex prompt that the simple guard cannot understand but the more powerful main model can, highlighting a fundamental vulnerability and urging a shift in safety defenses from blocking bad inputs to preventing bad outputs.*** <br> <br>
   Oct 3, UC Berkeley, ESF and NYU published a [paper](https://arxiv.org/pdf/2510.01529) “Bypassing Prompt Guards in Production with Controlled-Release Prompting”. As large language models (LLMs) advance, ensuring AI safety and alignment is paramount. One popular approach is prompt guards, lightweight mechanisms designed to filter malicious queries while being easy to implement and update. This work introduces a new attack that circumvents such prompt guards, highlighting their limitations. The method consistently jailbreaks production models while maintaining response quality, even under the highly protected chat interfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok (3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry between the prompt guard and the main LLM, encoding a jailbreak prompt that lightweight guards cannot decode but the main model can. This reveals an attack surface inherent to lightweight prompt guards in modern LLM architectures and underscores the need to shift defenses from blocking malicious inputs to preventing malicious outputs. The work additionally identifies other critical alignment issues, such as copyrighted data extraction, training data extraction, and malicious response leakage during thinking. <br> <br>

5. ***A New Playground for AI Tinkerers:  <br>ThinkingMachines has launched Tinker, a flexible, managed API that simplifies the process of fine-tuning a wide range of open-weight language models. By handling the complexities of distributed training and infrastructure management, Tinker empowers researchers and developers to easily experiment with and customize cutting-edge models.*** <br> <br>
   Oct 2, ThinkingMachines [released](https://thinkingmachines.ai/blog/announcing-tinker/) its first product tinker. Tinker is a flexible API for fine-tuning language models. It empowers researchers and hackers to experiment with models by giving them control over the algorithms and data while tinker handles the complexity of distributed training. Tinker advances ThinkingMachines’ mission of enabling more people to do research on cutting-edge models and customize them to their needs. Tinker lets researchers fine-tune a range of large and small open-weight models, including large mixture-of-experts models such as Qwen-235B-A22B. Switching from a small model to a large one is as simple as changing a single string in your Python code. Tinker is a managed service that runs on an internal clusters and training infrastructure. ThinkingMachines handle scheduling, resource allocation, and failure recovery. This allows to get small or large runs started immediately, without worrying about managing infrastructure. Tinker uses LoRA so to share the same pool of compute between multiple training runs, lowering costs. Tinker’s API gives users low-level primitives like forward_backward and sample, which can be used to express most common post-training methods. Even so, achieving good results requires getting many details right. That’s why it’s releasing an open-source library, the Tinker [Cookbook](https://github.com/thinking-machines-lab/tinker-cookbook), with modern implementations of post-training methods that run on top of the Tinker API. Groups at Princeton, Stanford, Berkeley, and Redwood Research have already been using Tinker for fine-tune models. Tinker is now in private beta for researchers and developers. <br> <br>

7. ***Thinking Backwards for Safer AI:  <br>A novel safety approach called InvThink teaches large language models to engage in "inverse reasoning" by first thinking through potential harms and failure modes before generating a response. This method not only improves safety in high-stakes domains but also avoids the "safety tax" by preserving the model's general reasoning capabilities.*** <br> <br>
   Oct 2, MIT and Google published a [paper](https://arxiv.org/pdf/2510.01569) “InvThink: Towards AI Safety via Inverse Reasoning”. The work presents InvThink, a simple yet powerful approach that gives large language models (LLMs) the capability of inverse thinking: reasoning through failure modes before generating responses. Unlike existing safety alignment methods that optimize directly for safe response, InvThink instructs models to 1) enumerate potential harms, 2) analyze their consequences, and 3) generate safe outputs that proactively avoid these risks. The method reveals three key findings: (i) safety improvements show stronger scaling with model size compared to existing safety methods. (ii) InvThink mitigates safety tax; by training models to systematically consider failure modes, it preserves general reasoning capabilities on standard benchmarks. (iii) beyond general safety tasks, InvThink excels in high-stakes domains including external-facing (medicine, finance, law) and agentic (blackmail, murder) risk scenarios, achieving up to 15.7% reduction in harmful responses compared to baseline methods like SafetyPrompt. The work further implements InvThink via supervised fine-tuning, and reinforcement learning across three LLM families. These results suggest that inverse reasoning provides a scalable and generalizable path toward safer, more capable language models. https://github.com/invthink/invthink <br> <br>

9. ***AI Learns to Create Its Own Mental Blueprints:  <br>A new training paradigm, RLAD, teaches language models to solve complex problems by first generating concise "reasoning abstractions"—descriptions of procedural knowledge—and then using them to guide its solution. This two-agent reinforcement learning setup enables more structured exploration and better generalization, demonstrating that creating good abstractions is key to unlocking effective reasoning.*** <br> <br>
    Oct 2, CMU and Stanford Uni published a [paper](https://arxiv.org/pdf/2510.02263) “RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems”. Reasoning requires going beyond pattern matching or memorization of solutions to identify and implement "algorithmic procedures" that can be used to deduce answers to hard problems. Doing so requires realizing the most relevant primitives, intermediate results, or shared procedures, and building upon them. While RL post-training on long chains of thought ultimately aims to uncover this kind of algorithmic behavior, most reasoning traces learned by large models fail to consistently capture or reuse procedures, instead drifting into verbose and degenerate exploration. To address more effective reasoning, the work introduces reasoning abstractions: concise natural language descriptions of procedural and factual knowledge that guide the model toward learning successful reasoning. The work trains models to be capable of proposing multiple abstractions given a problem, followed by RL that incentivizes building a solution while using the information provided by these abstractions. This results in a two-player RL training paradigm, abbreviated as RLAD, that jointly trains an abstraction generator and a solution generator. This setup effectively enables structured exploration, decouples learning signals of abstraction proposal and solution generation, and improves generalization to harder problems. The study also shows that allocating more test-time compute to generating abstractions is more beneficial for performance than generating more solutions at large test budgets, illustrating the role of abstractions in guiding meaningful exploration. <br> <br>

11. ***Better AI Thinking Through Iterative Refinement:  <br>A new inference strategy called Parallel-Distill-Refine (PDR) offers a more efficient alternative to long chain-of-thought, achieving higher accuracy with lower latency. The method works by generating diverse drafts in parallel, distilling them into a compact workspace, and then iteratively refining them, demonstrating that metacognitive improvement is a powerful tool for reasoning models.*** <br> <br>
    Oct 1, Meta et al published a [paper](https://arxiv.org/pdf/2510.01123) “Rethinking Thinking Tokens: LLMs as Improvement Operators”. Reasoning training incentivizes LLMs to produce long chains of thought (long CoT), which among other things, allows them to explore solution strategies with self-checking. This results in higher accuracy, but inflates context length, token/compute cost, and answer latency. The research asks: Can current models leverage their metacognition to provide other combinations on this Pareto frontier, e.g., better accuracy with lower context length and/or latency? Abstractly, the study views the model as an improvement operator on its own "thoughts" with a continuum of possible strategies. The work identifies an interesting inference family Parallel-Distill-Refine (PDR), which performs the following: (i) generate diverse drafts in parallel; (ii) distill them into a bounded, textual workspace; and (iii) refine conditioned on this workspace, producing an output that seeds the next round. Importantly, context length (hence compute cost) is controllable via degree of parallelism, and is no longer conflated with the total number of generated tokens. The study reports PDR instantiations of current models that give better accuracy than long CoT while incurring lower latency. Setting degree of parallelism to 1 yields an interesting subcase, Sequential Refinement (SR) (iteratively improve a single candidate answer) which provides performance superior to long CoT. Success of such model orchestrations raises the question whether further training could shift the Pareto frontier. To this end, the study trains an 8B thinking model with Reinforcement Learning (RL) to make it consistent with PDR as the inference method. On math tasks with verifiable answers, iterative pipelines surpass single-pass baselines at matched sequential budgets, with PDR delivering the largest gains (e.g., +11% on AIME 2024 and +9% on AIME 2025). <br> <br>

13. ***Unlocking AI's Inner Eye:  <br>A comprehensive study from Meta and the University of Oxford demystifies how text-only language models develop rich "visual priors." The research separates these priors into two distinct types—a reasoning prior learned from data like code and math, and a perception prior learned from general text—providing a new recipe for deliberately cultivating more vision-aware AI from language pre-training alone.*** <br> <br>
    Oct 1, Meta and Uni of Oxford published a [paper](https://arxiv.org/pdf/2509.26625) “Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training”. Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, the study reveals that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. The work shows that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, the work proposes a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. The findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with the main findings, the work proposes and investigates several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs. <br> <br>

15. ***Teaching AI the Wisdom to Say 'I Don't Know':  <br>To combat AI hallucinations, the new TruthRL framework uses reinforcement learning with a ternary reward system that distinguishes between correct answers, incorrect answers, and abstentions. This approach directly incentivizes truthfulness by encouraging the model to abstain when uncertain, significantly reducing hallucinations by 28.9% while improving overall accuracy.*** <br> <br>
    Oct 1, Uni of Virginia, Meta and Uni of Washington published a [paper](https://www.arxiv.org/pdf/2509.25760) “TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning”. While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy -- models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. The work presents TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, the study implements TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, the proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs.  <br> <br>

17. ***A New Digital Gym for AI Agents:  <br>To support the shift toward experience-based learning, researchers have released GEM (General Experience Maker), an open-source environment simulator for training agentic LLMs. Analogous to OpenAI-Gym, GEM provides a standardized framework with a diverse suite of environments and high-throughput execution, serving as a unified platform for training and evaluating the next generation of AI agents.*** <br> <br>
    Oct 1, Sea AI, NUS et al published a [paper](https://arxiv.org/pdf/2510.01051) “GEM: A Gym for Agentic LLMs”. The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition the work introduces GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, the study also provides a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. The work further conducts apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. https://github.com/axon-rl/gem <br> <br>

19. ***Uncovering Wasted Space in AI Brains:  <br>An analysis of the feed-forward networks (FFNs) in large language models reveals an "asymmetric spectral scaling law," indicating that as models get wider, much of their new capacity is under-utilized. The study finds that the most important reasoning subspaces saturate early, offering a more principled way to design inference-efficient LLMs by balancing different types of capacity.*** <br> <br>
    Oct 1, NYU published a [paper](https://arxiv.org/pdf/2510.00537) “Spectral Scaling Laws in Language Models: How Effectively Do Feed-Forward Networks Use Their Latent Space?”. As large language models (LLMs) scale, the question is not only how large they become, but how much of their capacity is effectively utilized. Existing scaling laws relate model size to loss, yet overlook how components exploit their latent space. The authors study feed-forward networks (FFNs) and recast width selection as a spectral utilization problem. Using a lightweight diagnostic suite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral Concentration, and the composite Spectral Utilization Index (SUI) – the work quantifies how many latent directions are meaningfully activated across LLaMA, GPT-2, and nGPT families. The key finding is an asymmetric spectral scaling law: soft rank follows an almost perfect power law with FFN width, while hard rank grows only sublinearly and with high variance. This asymmetry suggests that widening FFNs mostly adds low-energy tail directions, while dominant-mode subspaces saturate early. Moreover, at larger widths, variance further collapses into a narrow subspace, leaving much of the latent space under-utilized. These results recast FFN width selection as a principled trade-off between tail capacity and dominant-mode capacity, offering concrete guidance for inference-efficient LLM design. <br> <br>

21. ***Synthesizing a Super-Answer from Many AI Minds:  <br>Challenging the standard "Best-of-N" approach, a new method called Fusion-of-N (FusioN) uses a language model judge to synthesize the best elements from a pool of candidate responses into a single, superior answer. This collaborative approach consistently outperforms selection-based methods, demonstrating the value of integrating diverse strengths to unlock potential that was previously discarded.*** <br> <br>
    Oct 1, Cohere published a [paper](https://arxiv.org/pdf/2510.00931) “Making, not Taking, the Best of N”. Obtaining high-quality generations in modern LLMs has largely been framed as a selection problem: identifying a single winning generation from a diverse pool of N samples, the Best-of-N (BoN). Yet, this approach is inherently zero-sum, discarding diverse and potentially useful information from the pool. Instead, the study explores a collaborative setup, where all candidates can potentially contribute to the final winning generation. To this end, the work proposes Fusion-of-N (FusioN): a method that uses a general LLM judge to synthesize the most informative elements of each sample into a single final answer. The work compares FusioN to BoN in two settings, (i) test-time scaling, where the work samples and aggregates from a single model at test-time (ii) synthetic data generation, where the authors fuse samples from a pool of diverse teachers to improve a student model. The study extensively benchmarks both setups across 11 languages, 3 diverse tasks and varying model scales. Across the bench, FusioN consistently outperforms BoN showing versatility and robustness both in test-time scaling and in downstream gains from synthetic data generation. The work also performs extensive analysis on FusioN, where it shows surprising strengths and robustness under challenging settings. These results show that people should shift how to think about evaluating and utilizing LLM generations from a monolithic measure of quality, to embracing their polylithic nature. This shift allows to integrate diverse strengths, unlock latent potential, and achieve improvements that were previously inaccessible through selection alone. <br> <br>

23. ***Deeper Exploration Unlocks Stalled AI Progress:  <br>A new paradigm for scaling reinforcement learning called BroRL demonstrates that dramatically increasing the number of exploratory "rollouts" per problem yields continuous performance gains, even after other methods have plateaued. This focus on broadened exploration, backed by theoretical analysis, revives saturated models and achieves new state-of-the-art results.*** <br> <br>
    Oct 1, Nvidia, Stanford Uni, and Uni of Washington published a [paper](https://arxiv.org/pdf/2510.01180) “ BroRL: Scaling Reinforcement Learning via Broadened Exploration”. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key ingredient for unlocking complex reasoning capabilities in large language models. Recent work ProRL has shown promise in scaling RL by increasing the number of training steps. However, performance plateaus after thousands of steps, with clear diminishing returns from allocating more computation to additional training. This work investigates a complementary paradigm for scaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to exhaustively Broaden exploration, which yields continuous performance gains beyond the saturation point observed in ProRL when scaling the number of training steps. The approach is motivated by a mass balance equation analysis allowing to characterize the rate of change in probability mass for correct and incorrect tokens during the reinforcement process. The work shows that under a one-step RL assumption, sampled rollout tokens always contribute to correct-mass expansion, while unsampled tokens outside rollouts may lead to gains or losses depending on their distribution and the net reward balance. Importantly, as the number of rollouts per example N increases, the effect of unsampled terms diminishes, ensuring overall correct-mass expansion. To validate the theoretical analysis, the work conducts simulations under more relaxed conditions and find that a sufficiently large rollout size N-corresponding to ample exploration-guarantees an increase in the probability mass of all correct tokens. Empirically, BroRL revives models saturated after 3K ProRL training steps and demonstrates robust, continuous improvement, achieving state-of-the-art results for the 1.5B model across diverse benchmarks. <br> <br>

25. ***A Brain-Inspired AI Architecture:  <br>Researchers have introduced the "Dragon Hatchling" (BDH), a novel language model architecture based on a scale-free, biologically plausible network of interacting neuron particles. The model rivals the performance of classic transformers while being inherently interpretable and relying on brain-like mechanisms such as synaptic plasticity and spiking neurons.*** <br> <br>
    Sep 30, Pathway published a [paper](https://arxiv.org/pdf/2509.26507) “The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain”. The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models. The research introduces “Dragon Hatchling” (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance. BDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: empirically BDH rivals GPT2 performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data. BDH can be represented as a brain model. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons. The work confirms empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech. BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. The work demonstrates monosemanticity in BDH on language tasks. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture. https://github.com/pathwaycom/bdh <br> <br>

27. ***Teaching AI to Build a Better Memory:  <br>A new reinforcement learning framework called Mem-alpha trains AI agents to effectively manage their own complex external memory systems. By receiving rewards based on its downstream task performance, the agent learns what information to store and how to structure it, leading to significant improvements and remarkable generalization to tasks far longer than what it was trained on.*** <br> <br>
    Sep 30, Anuttacon, UCSD and Stanford Uni published a [paper](https://arxiv.org/pdf/2509.25911) “Mem-α: Learning Memory Construction via Reinforcement Learning”. Large language model (LLM) agents are constrained by limited context windows, necessitating external memory systems for long-term information understanding. Current memory-augmented agents typically depend on pre-defined instructions and tools for memory updates. However, language models may lack the ability to determine which information to store, how to structure it, and when to update it, especially as memory systems become more complex. This results in suboptimal memory construction and information loss. To this end, the work proposes Mem-alpha, a reinforcement learning framework that trains agents to effectively manage complex memory systems through interaction and feedback. The study also constructs a specialized training dataset spanning diverse multi-turn interaction patterns paired with comprehensive evaluation questions designed to teach effective memory management. During training, agents process sequential information chunks, learn to extract and store relevant content, then update the memory system. The reward signal derives from downstream question-answering accuracy over the full interaction history, directly optimizing for memory construction. To illustrate the effectiveness of the training framework, the study designs a memory architecture comprising core, episodic, and semantic components, equipped with multiple tools for memory operations. Empirical evaluation demonstrates that Mem-alpha achieves significant improvements over existing memory-augmented agent baselines. Despite being trained exclusively on instances with a maximum length of 30k tokens, the agents exhibit remarkable generalization to sequences exceeding 400k tokens, over 13x the training length, highlighting the robustness of Mem-alpha. <br> <br>

29. ***Cracking the Code of AI's Math Failure:  <br>By reverse-engineering a model that can successfully perform multi-digit multiplication, researchers discovered that standard transformers fail because they converge to a local optimum that lacks the necessary long-range dependencies. The successful model, in contrast, uses attention to create a graph for caching and retrieving partial products, an insight that can be used to fix the standard training process.*** <br> <br>
    Sep 30, Uni of Chicago, MIT, Google et al published a [paper](https://www.arxiv.org/pdf/2510.00184) “Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls”. Language models are increasingly capable, yet still fail at a seemingly simple task of multi-digit multiplication. In this work, the authors study why, by reverse-engineering a model that successfully learns multiplication via implicit chain-of-thought, and report three findings: (1) Evidence of long-range structure: Logit attributions and linear probes indicate that the model encodes the necessary long-range dependencies for multi-digit multiplication. (2) Mechanism: the model encodes long-range dependencies using attention to construct a directed acyclic graph to “cache” and “retrieve” pairwise partial products. (3) Geometry: the model implements partial products in attention heads by forming Minkowski sums between pairs of digits, and digits are represented using a Fourier basis, both of which are intuitive and efficient representations that the standard fine-tuning model lacks. With these insights, the work revisits the learning dynamics of standard fine-tuning and find that the model converges to a local optimum that lacks the required long-range dependencies. The study further validates this understanding by introducing an auxiliary loss that predicts the “running sum” via a linear regression probe, which provides an inductive bias that enables the model to successfully learn multi-digit multiplication. In summary, by reverse-engineering the mechanisms of an implicit chain-of-thought model the work uncovers a pitfall for learning long-range dependencies in Transformers and provide an example of how the correct inductive bias can address this issue. https://github.com/ajyl/icot <br> <br>

31. ***A Principled Path to More General AI:  <br>A Google paper establishes a new theoretical framework that grounds the training objectives for transformers in the formal theory of Kolmogorov complexity. The work proves the existence of "asymptotically optimal" and tractable objectives that would, in the limit, allow models to achieve optimal data compression and generalization, outlining a principled path toward more efficient AI learners.*** <br> <br>
    Sep 29, Google published a [paper](https://arxiv.org/pdf/2509.22445) “Bridging Kolmogorov Complexity and Deep Learning: Asymptotically Optimal Description Length Objectives for Transformers”. The Minimum Description Length (MDL) principle offers a formal framework for applying Occam's razor in machine learning. However, its application to neural networks such as Transformers is challenging due to the lack of a principled, universal measure for model complexity. This paper introduces the theoretical notion of asymptotically optimal description length objectives, grounded in the theory of Kolmogorov complexity. The study establishes that a minimizer of such an objective achieves optimal compression, for any dataset, up to an additive constant, in the limit as model resource bounds increase. The study proves that asymptotically optimal objectives exist for Transformers, building on a new demonstration of their computational universality. The work further shows that such objectives can be tractable and differentiable by constructing and analyzing a variational objective based on an adaptive Gaussian mixture prior. Empirical analysis shows that this variational objective selects for a low-complexity solution with strong generalization on an algorithmic task, but standard optimizers fail to find such solutions from a random initialization, highlighting key optimization challenges. More broadly, by providing a theoretical framework for identifying description length objectives with strong asymptotic guarantees, the work outlines a potential path towards training neural networks that achieve greater compression and generalization. <br> <br>

33. ***Mapping the 'Personalities' of AI Thinkers:  <br>A new method called LOT (LLM-proposed Open Taxonomy) uses a generative model to compare the reasoning traces of different AI systems and create a human-readable taxonomy of their distinct thinking styles. The system can distinguish between models with up to 100% accuracy and provides qualitative explanations for their different approaches, which can then be used to improve their performance.*** <br> <br>
    Sep 29, Harvard Uni and Meta published a [paper](https://www.arxiv.org/pdf/2509.24147) “Your thoughts tell who you are: Characterize the reasoning patterns of LRMs”. Current comparisons of large reasoning models (LRMs) focus on macro-level statistics such as task accuracy or reasoning length. Whether different LRMs reason differently remains an open question. To address this gap, the work introduces the LLM-proposed Open Taxonomy (LOT), a classification method that uses a generative language model to compare reasoning traces from two LRMs and articulate their distinctive features in words. LOT then models how these features predict the source LRM of a reasoning trace based on their empirical distributions across LRM outputs. Iterating this process over a dataset of reasoning traces yields a human-readable taxonomy that characterizes how models think. The work applies LOT to compare the reasoning of 12 open-source LRMs on tasks in math, science, and coding. LOT identifies systematic differences in their thoughts, achieving 80-100% accuracy in distinguishing reasoning traces from LRMs that differ in scale, base model family, or objective domain. Beyond classification, LOT's natural-language taxonomy provides qualitative explanations of how LRMs think differently. Finally, in a case study, the work links the reasoning differences to performance: aligning the reasoning style of smaller Qwen3 models with that of the largest Qwen3 during test time improves their accuracy on GPQA by 3.3-5.7%. <br> <br>

35. ***Aligning AI with How Humans Actually Perceive:  <br>Drawing on behavioral economics, a new study offers a human-centric explanation for why online reinforcement learning is so effective. It proves that online methods better approximate human perception of probability and proposes "humanline" objectives that explicitly mimic these perceptual biases, allowing less complex offline training to match the performance of online alignment.*** <br> <br>
    Sep 29, Princeton Uni, Stanford Uni and Uni of Chicago published a [paper](https://arxiv.org/pdf/2509.24207) “Humanline: Online Alignment as Perceptual Loss”. Online alignment (e.g., GRPO) is generally more performant than offline alignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral economics, the work proposes a human-centric explanation. The study proves that online on-policy sampling better approximates the human-perceived distribution of what the model can produce, and PPO/GRPO-style clipping -- originally introduced to just stabilize training -- recovers a perceptual bias in how humans perceive probability. In this sense, PPO/GRPO act as perceptual losses already. The theory further suggests that the online/offline dichotomy is itself incidental to maximizing human utility, since the work can achieve the same effect by selectively training on any data in a manner that mimics human perception, rather than restricting to online on-policy data. Doing so would allow to post-train more quickly, cheaply, and flexibly without sacrificing performance. To this end, the work proposes a design pattern that explicitly incorporates perceptual distortions of probability into objectives like DPO/KTO/GRPO, creating humanline variants of them. Surprisingly, the study finds that these humanline variants, even when trained with offline off-policy data, can match the performance of their online counterparts on both verifiable and unverifiable tasks. <br> <br>

37. ***Evolutionary Thinking for Smarter AI:  <br>A new test-time scaling method called Recursive Self-Aggregation (RSA) combines the benefits of parallel and sequential reasoning through an evolutionary-inspired process. By iteratively refining a population of candidate solutions by aggregating subsets of them, RSA achieves substantial performance gains, allowing a small 4B-parameter model to compete with much larger reasoning models.*** <br> <br>
    Sep 29, Mila, Uni of Montreal et al published a [paper](https://rsa-llm.github.io/static/pdfs/Recursive_Self_Aggregation.pdf) “Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models”. Test-time scaling methods improve the capabilities of Large Language Models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. The work proposes Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. RSA exploits the rich information embedded in the reasoning chains – not just the final answers – and enables bootstrapping from partially correct intermediate steps within different chains of thought. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), while outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. The study further demonstrates that training the model to combine solutions via a novel aggregationaware reinforcement learning approach yields significant performance gains. https://github.com/HyperPotatoNeo/RSA <br> <br>

39. ***Reasoning on the Inside, Speed on the Outside:  <br>A simple training method called Dual-Head Reasoning Distillation (DHRD) allows a classifier to gain the accuracy benefits of chain-of-thought reasoning without the massive slowdown at inference time. By training with a reasoning head that is disabled during testing, the model retains the performance boost while achieving throughput that is up to 142 times faster than standard CoT prompting.*** <br> <br>
    Sep 29, Uni of Waterloo and Google published a [paper](https://www.arxiv.org/pdf/2509.21487) “Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning”. Chain-of-Thought (CoT) prompting often improves classification accuracy, but it introduces a significant throughput penalty with rationale generation. To resolve this trade-off, the study introduces Dual-Head Reasoning Distillation (DHRD), a simple training method for decoder-only language models (LMs) that adds (i) a pooled classification head used during training and inference and (ii) a reasoning head supervised by teacher rationales used only in training. The work trains with a loss function that is a weighted sum of label cross-entropy and token-level LM loss over input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative gains of 0.65-5.47% over pooled baselines, with notably larger gains on entailment/causal tasks. Since the study disables the reasoning head at test time, inference throughput matches pooled classifiers and exceeds CoT decoding on the same backbones by 96-142 times in QPS. <br> <br>

41. ***An AI That Dreams Its Way to Success in Minecraft:  <br>Google's Dreamer 4 is the first AI agent to solve the difficult challenge of obtaining diamonds in Minecraft by learning entirely from offline data, without any direct interaction with the game environment. The agent achieves this by training inside its own fast and accurate "world model," where it can safely and efficiently simulate experiences, marking a major step toward building intelligent agents for real-world applications.*** <br> <br>
    Sep 29, Google published a [paper](https://www.arxiv.org/pdf/2509.24527) “Training Agents Inside of Scalable World Models”. World models learn general knowledge from videos and simulate experience for training behaviors in imagination, offering a path towards intelligent agents. However, previous world models have been unable to accurately predict object interactions in complex environments. The work introduces Dreamer 4, a scalable agent that learns to solve control tasks by reinforcement learning inside of a fast and accurate world model. In the complex video game Minecraft, the world model accurately predicts object interactions and game mechanics, outperforming previous world models by a large margin. The world model achieves real-time interactive inference on a single GPU through a shortcut forcing objective and an efficient transformer architecture. Moreover, the world model learns general action conditioning from only a small amount of data, allowing it to extract the majority of its knowledge from diverse unlabeled videos. The study proposes the challenge of obtaining diamonds in Minecraft from only offline data, aligning with practical applications such as robotics where learning from environment interaction can be unsafe and slow. This task requires choosing sequences of over 20,000 mouse and keyboard actions from raw pixels. By learning behaviors in imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft purely from offline data, without environment interaction. The work provides a scalable recipe for imagination training, marking a step towards intelligent agents. <br> <br>

43. ***The Definitive Guide to Efficient AI Fine-Tuning:  <br>A comprehensive study from ThinkingMachines investigates the parameter-efficient fine-tuning method LoRA, identifying a "low-regret regime" where it matches the performance of full fine-tuning. The research provides practical guidance for its effective use, confirming that LoRA is a powerful and efficient alternative for most post-training scenarios.*** <br> <br>
    Sep 29, ThnksingMachines published an [article](https://thinkingmachines.ai/blog/lora/) “LoRA Without Regret”. The article investigates the efficacy of Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning (PEFT) method for large language models, compared to full fine-tuning (FullFT). LoRA updates weight matrices with low-rank matrices (W’ = W + γBA), using significantly fewer parameters, making it cost-effective, memory-efficient, and suitable for multi-tenant serving, as implemented in inference engines like vLLM. The study conducted supervised fine-tuning and reinforcement learning (RL) experiments using Llama 3 and Qwen3 models on datasets like Tulu3 and OpenThoughts3, varying LoRA rank (1–512) and learning rates. Findings show that LoRA matches FullFT’s sample efficiency and performance in small-to-medium datasets, but underperforms with datasets exceeding its capacity, exhibiting reduced training efficiency. LoRA is less tolerant of large batch sizes, showing a performance gap due to its product-of-matrices parameterization, though this is less critical at smaller batch sizes. Applying LoRA to all layers, especially MLPs and MoE layers, yields better results than attention-only LoRA, even when parameter counts are matched. In RL, LoRA performs comparably to FullFT with low ranks, suggesting low capacity needs. The study identifies a “low-regret regime” where LoRA rivals FullFT for most post-training scenarios, with optimal learning rates for LoRA being roughly ten times higher than FullFT, independent of rank. These insights support LoRA’s use in efficient fine-tuning across diverse applications. <br> <br>

45. ***A Debugger for Flaky AI Agents:  <br>To address the problem of cascading failures in complex LLM agents, researchers have created AgentDebug, a framework that can systematically classify, detect, and fix errors. By providing targeted corrective feedback, the system enables agents to recover from their mistakes and iteratively improve, leading to up to a 26% relative increase in task success rates.*** <br> <br>
    Sep 29, UIUC, Stanford Uni, AMD et al published a [paper](https://arxiv.org/pdf/2509.25370) “Where LLM Agents Fail and How They can Learn From Failures”. Large Language Model (LLM) agents, which integrate planning, memory, reflection, and tool-use modules, have shown promise in solving complex, multi-step tasks. Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Current systems lack a framework that can comprehensively understand agent error in a modular and systemic way, and therefore fail to detect these errors accordingly. The work addresses this gap with three contributions. First, introduces the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. Second, constructs AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. Third, propose AgentDebug, a debugging framework that isolates root-cause failures and provides corrective feedback, enabling agents to recover and iteratively improve. Experiments on AgentErrorBench show that AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to the strongest baseline. Beyond detection, the targeted feedback generated by AgentDebug enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop. These results establish principled debugging as a pathway to more reliable and adaptive LLM agents. https://github.com/ulab-uiuc/AgentDebug <br> <br>

47. ***A Universal Toolkit for Building AI Scientists:  <br>Researchers have launched ToolUniverse, an open ecosystem designed to democratize the creation of "AI scientists." The platform standardizes how AI agents use tools by integrating over 600 machine learning models, datasets, and scientific packages, providing a unified infrastructure to accelerate community-driven development of AI for scientific discovery.*** <br> <br>
    Sep 27, Harvard Uni and MIT published a [paper](https://arxiv.org/pdf/2509.23426) “Democratizing AI scientists using ToolUniverse”. AI scientists are emerging computational systems that serve as collaborative partners in discovery. These systems remain difficult to build because they are bespoke, tied to rigid workflows, and lack shared environments that unify tools, data, and analyses into a common ecosystem. In omics, unified ecosystems have transformed research by enabling interoperability, reuse, and community-driven development; AI scientists require comparable infrastructure. The work presents ToolUniverse, an ecosystem for building AI scientists from any language or reasoning model, whether open or closed. TOOLUNIVERSE standardizes how AI scientists identify and call tools, integrating more than 600 machine learning models, datasets, APIs, and scientific packages for data analysis, knowledge retrieval, and experimental design. It automatically refines tool interfaces for correct use by AI scientists, creates new tools from natural language descriptions, iteratively optimizes tool specifications, and composes tools into agentic workflows. In a case study of hypercholesterolemia, ToolUniverse was used to create an AI scientist to identify a potent analog of a drug with favorable predicted properties. https://github.com/mims-harvard/ToolUniverse <br> <br>

49. ***The Secret Geometry of AI Learning:  <br>A spectral analysis of language models reveals that their internal representations evolve through a consistent three-phase geometric sequence during pretraining: an initial collapse, an "entropy-seeking" expansion, and a final "compression-seeking" phase that coincides with improved task performance. This discovery provides a new lens for understanding the emergence of complex AI capabilities beyond just looking at the training loss.*** <br> <br>
    Sep 27, McGill Uni, Google et al published a [paper](https://www.arxiv.org/pdf/2509.23024) “Tracing the Representation Geometry of Language Models from Pretraining to Post-training”. Standard training metrics like loss fail to explain the emergence of complex capabilities in large language models. The work takes a spectral approach to investigate the geometry of learned representations across pretraining and post-training, measuring effective rank (RankMe) and eigenspectrum decay (aphpa-ReQ). With OLMo (1B-7B) and Pythia (160M-12B) models, the study uncovers a consistent non-monotonic sequence of three geometric phases during autoregressive pretraining. The initial "warmup" phase exhibits rapid representational collapse. This is followed by an "entropy-seeking" phase, where the manifold's dimensionality expands substantially, coinciding with peak n-gram memorization. Subsequently, a "compression-seeking" phase imposes anisotropic consolidation, selectively preserving variance along dominant eigendirections while contracting others, a transition marked with significant improvement in downstream task performance. The work shows these phases can emerge from a fundamental interplay of cross-entropy optimization under skewed token frequencies and representational bottlenecks (). Post-training further transforms geometry: SFT and DPO drive "entropy-seeking" dynamics to integrate specific instructional or preferential data, improving in-distribution performance while degrading out-of-distribution robustness. Conversely, RLVR induces "compression-seeking", enhancing reward alignment but reducing generation diversity. <br> <br>

51. ***A Unified Theory for Better AI Reasoning:  <br>Researchers have introduced a variational reasoning framework that provides a principled, probabilistic perspective on language model thinking. By treating reasoning traces as latent variables, the framework unifies existing RL-style methods under a single theoretical umbrella, resulting in more stable and effective objectives for improving AI reasoning.*** <br> <br>
    Sep 26, Sea AI Lab et al published a [paper](https://arxiv.org/pdf/2509.22637) “Variational Reasoning for Language Models”. The study introduces a variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), the work extends it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. The study further shows that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. The work empirically validates the method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, the work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models. https://github.com/sail-sg/variational-reasoning. <br> <br>

53. ***An Active, Self-Optimizing Memory for AI Agents:  <br>A new framework called Self-Evolving Distributed Memory (SEDM) transforms the memory of multi-agent systems from a passive repository into an active, self-optimizing component. By dynamically ranking and consolidating memory entries based on their utility, SEDM improves reasoning accuracy and reduces token overhead, offering a more scalable and sustainable solution for long-term agent collaboration.*** <br> <br>
    Sep 26, Gradient et al published a [paper](https://arxiv.org/pdf/2509.09498) “SEDM: Scalable Self-Evolving Distributed Memory for Agents”. Long-term multi-agent systems inevitably generate vast amounts of trajectories and historical interactions, which makes efficient memory management essential for both performance and scalability. Existing methods typically depend on vector retrieval and hierarchical storage, yet they are prone to noise accumulation, uncontrolled memory expansion, and limited generalization across domains. To address these challenges, the work presents SEDM, Self-Evolving Distributed Memory, a verifiable and adaptive framework that transforms memory from a passive repository into an active, self-optimizing component. SEDM integrates verifiable write admission based on reproducible replay, a self-scheduling memory controller that dynamically ranks and consolidates entries according to empirical utility, and cross-domain knowledge diffusion that abstracts reusable insights to support transfer across heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM improves reasoning accuracy while reducing token overhead compared with strong memory baselines, and further enables knowledge distilled from fact verification to enhance multi-hop reasoning. The results highlight SEDM as a scalable and sustainable memory mechanism for open-ended multi-agent collaboration. <br> <br>

55. ***Teaching AI to 'Think' from Day One:  <br>A new pretraining objective called RLP (Reinforcement as a Pretraining Objective) brings the exploratory power of reinforcement learning to the pretraining phase. By rewarding the model for generating a chain of thought that provides information gain for predicting future tokens, RLP encourages the model to "think for itself" from the start, leading to significant performance boosts on reasoning tasks.*** <br> <br>
    Sep 26, Nvidia, CMU et al published a [paper](https://arxiv.org/pdf/2510.01265) “RLP: Reinforcement as a Pretraining Objective”. The dominant paradigm for training large reasoning models starts with pre-training using next-token prediction loss on vast amounts of data. Reinforcement learning, while powerful in scaling reasoning, is introduced only as the very last phase of post-training, preceded by supervised fine-tuning. While dominant, is this an optimal way of training? This study presents RLP, an information-driven reinforcement pretraining objective, that brings the core spirit of reinforcement learning -- exploration -- to the last phase of pretraining. The key idea is to treat chain-of-thought as an exploratory action, with rewards computed based on the information gain it provides for predicting future tokens. This training objective essentially encourages the model to think for itself before predicting what comes next, thus teaching an independent thinking behavior earlier in the pretraining. More concretely, the reward signal measures the increase in log-likelihood of the next token when conditioning on both context and a sampled reasoning chain, compared to conditioning on context alone. This approach yields a verifier-free dense reward signal, allowing for efficient training for the full document stream during pretraining. Specifically, RLP reframes reinforcement learning for reasoning as a pretraining objective on ordinary text, bridging the gap between next-token prediction and the emergence of useful chain-of-thought reasoning. Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an eight-benchmark math-and-science suite by 19%. With identical post-training, the gains compound, with the largest improvements on reasoning-heavy tasks such as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2 increases the overall average from 42.81% to 61.32% and raises the average on scientific reasoning by 23%, demonstrating scalability across architectures and model sizes. https://github.com/NVlabs/RLP <br> <br>

57. ***A More Robust Lie Detector for RAG Systems:  <br>The new LUMINA framework offers a more effective and practical way to detect hallucinations in Retrieval-Augmented Generation (RAG) systems. It works by analyzing the balance between the model's reliance on external documents and its own internal knowledge, outperforming prior methods by up to 13% without requiring extensive hyperparameter tuning.*** <br> <br>
    Sep 26, Uni of Wisconsin-Madison published a [paper](https://arxiv.org/pdf/2509.21875) “LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals”. Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large language models (LLMs) by grounding responses in retrieved documents. Yet, RAG-based LLMs still hallucinate even when provided with correct and sufficient context. A growing line of work suggests that this stems from an imbalance between how models use external context and their internal knowledge, and several approaches have attempted to quantify these signals for hallucination detection. However, existing methods require extensive hyperparameter tuning, limiting their generalizability. The work proposes LUMINA, a novel framework that detects hallucinations in RAG systems through context-knowledge signals: external context utilization is quantified via distributional distance, while internal knowledge utilization is measured by tracking how predicted tokens evolve across transformer layers. The work further introduces a framework for statistically validating these measurements. Experiments on common RAG hallucination benchmarks and four open-source LLMs show that LUMINA achieves consistently high AUROC and AUPRC scores, outperforming prior utilization-based methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under relaxed assumptions about retrieval quality and model matching, offering both effectiveness and practicality. <br> <br>

59. ***Teaching AI a Sense of Physical Danger:  <br>To address the safety challenges of AI in the physical world, researchers have developed a scalable benchmark grounded in real-world injury data and a post-training method to teach models to explicitly reason about safety constraints. The resulting models generate transparent "thinking" traces about potential physical risks, achieving state-of-the-art performance and marking a crucial step towards safely deploying embodied AI.*** <br> <br>
    Sep 25, Google and Princeton Uni published a [paper](https://www.arxiv.org/pdf/2509.21651) “Can AI Perceive Physical Danger and Intervene?”. When AI interacts with the physical world -- as a robot or an assistive agent -- new safety challenges emerge beyond those of purely “digital AI”. In such interactions, the potential for physical harm is direct and immediate. How well do state-of-the-art foundation models understand common-sense facts about physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of coffee should not be handed to a child? The contributions of the study are three-fold: first, develop a highly scalable approach to continuous physical safety benchmarking of Embodied AI systems, grounded in real-world injury narratives and operational safety constraints. To probe multi-modal safety understanding, the work turns these narratives and constraints into photorealistic images and videos capturing transitions from safe to unsafe states, using advanced generative models. Secondly, comprehensively analyze the ability of major foundation models to perceive risks, reason about safety, and trigger interventions; this yields multi-faceted insights into their deployment readiness for safety-critical agentic applications. Finally, develop a post-training paradigm to teach models to explicitly reason about embodiment-specific safety constraints provided through system instructions. The resulting models generate thinking traces that make safety reasoning interpretable and transparent, achieving state of the art performance in constraint satisfaction evaluations. https://asimov-benchmark.github.io/v2 <br> <br>

61. ***An Early Warning System for Unsafe AI:  <br>A new safety framework called BRT-Align brings control theory to LLM inference, enabling it to preemptively detect and steer away from harmful content. By learning a "safety value function" in the model's latent space, the system can forecast unsafe outputs several tokens in advance and minimally perturb the generation to redirect it towards safer regions, providing a principled foundation for real-time safety.*** <br> <br>
    Sep 25, Stanford Uni published a [paper](https://arxiv.org/pdf/2509.21528) “Preemptive Detection and Steering of LLM Misalignment via Latent Reachability”. Large language models (LLMs) are now ubiquitous in everyday tools, raising urgent safety concerns about their tendency to generate harmful content. The dominant safety approach -- reinforcement learning from human feedback (RLHF) -- effectively shapes model behavior during training but offers no safeguards at inference time, where unsafe continuations may still arise. The study proposes BRT-Align, a reachability-based framework that brings control-theoretic safety tools to LLM inference. BRT-Align models autoregressive generation as a dynamical system in latent space and learn a safety value function via backward reachability, estimating the worst-case evolution of a trajectory. This enables two complementary mechanisms: (1) a runtime monitor that forecasts unsafe completions several tokens in advance, and (2) a least-restrictive steering filter that minimally perturbs latent states to redirect generation away from unsafe regions. Experiments across multiple LLMs and toxicity benchmarks demonstrate that BRT-Align provides more accurate and earlier detection of unsafe continuations than baselines. Moreover, for LLM safety alignment, BRT-Align substantially reduces unsafe generations while preserving sentence diversity and coherence. Qualitative results further highlight emergent alignment properties: BRT-Align consistently produces responses that are less violent, less profane, less offensive, and less politically biased. Together, these findings demonstrate that reachability analysis provides a principled and practical foundation for inference-time LLM safety. <br> <br>

63. ***It's the Structure, Not the Semantics, of Code That Teaches AI to Reason:  <br>A systematic, data-centric study reveals that the reasoning boost LLMs get from code data comes primarily from its structure, not its semantic meaning. The research shows that appropriate abstractions like pseudocode are just as effective, and even corrupted code remains beneficial as long as its surface-level regularities are intact, providing key insights for designing better training data.*** <br> <br>
    Sep 25, CMU published a [paper](https://www.arxiv.org/pdf/2509.21499) “On Code-Induced Reasoning in LLMs”. Code data has been shown to enhance the reasoning capabilities of large language models (LLMs), but it remains unclear which aspects of code are most responsible. The study investigates this question with a systematic, data-centric framework. The study constructs parallel instruction datasets in ten programming languages and apply controlled perturbations that selectively disrupt structural or semantic properties of code. The work then finetunes LLMs from five model families and eight scales on each variant and evaluate their performance on natural language, math, and code tasks. Across 3,331 experiments, results show that LLMs are more vulnerable to structural perturbations than semantic ones, particularly on math and code tasks. Appropriate abstractions like pseudocode and flowcharts can be as effective as code, while encoding the same information with fewer tokens without adhering to original syntax can often retain or even improve performance. Remarkably, even corrupted code with misleading signals remains competitive when surface-level regularities persist. Finally, syntactic styles also shape task-specific gains with Python favoring natural language reasoning and lower-level languages such as Java and Rust favoring math. Through systematic framework, the study aims to provide insight into how different properties of code influence reasoning and inform the design of training data for enhancing LLM reasoning capabilities. <br> <br>

65. ***A New Stress Test for AI's Scientific Reasoning:  <br>The new SciTrek benchmark is designed to rigorously evaluate the long-context reasoning of LLMs by tasking them with answering complex questions that require synthesizing information from multiple full-text scientific articles. The benchmark has proven to be a significant challenge for even the most advanced models, highlighting systematic weaknesses in their ability to perform numerical operations and locate specific information in long documents.*** <br> <br>
    Sep 25, Uni of Edinburgh published a [paper](https://arxiv.org/pdf/2509.21028) “Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles”. This paper introduces SciTrek, a novel question-answering benchmark designed to evaluate the long-context reasoning capabilities of large language models (LLMs) using scientific articles. Current long-context benchmarks often rely on non-scientific texts, focus on simple information retrieval tasks, or employ artificial contexts. SciTrek addresses these limitations by proposing complex questions that require information aggregation and synthesis across multiple full-text scientific articles. Questions and their ground-truth answers are automatically generated by formulating them as SQL queries over a database constructed from article metadata (titles, authors, and references). The SQL operations provide explicit, verifiable reasoning steps for fine-grained error analysis, and the construction process scales to contexts up to 1M tokens with minimal supervision. Extensive experiments on a diverse set of open-weight and proprietary LLMs demonstrate that SciTrek poses a significant challenge as the context length increases, with supervised fine-tuning and reinforcement learning offering only limited gains. Analysis reveals systematic shortcomings in models' abilities to perform basic numerical operations and accurately locate specific information in long contexts.
 <br> <br> <br>

***Sept 28, 2025***

1. ***Context is Key for AI Diversity:   <br>A study from Meta and MIT argues that whether an AI's repetitive answers are a problem depends entirely on the task. The research introduces a new framework that defines and measures "functional diversity" based on specific task categories—like math versus creative writing—and proposes a new sampling technique that encourages useful variation where needed while maintaining consistency for objective tasks, all without sacrificing quality.***  <br>  <br>
   Sep 26, Meta and MIT published a [paper](https://arxiv.org/pdf/2509.21267) “LLM Output Homogenization is Task Dependent”. A large language model can be less helpful if it exhibits output response homogenization. But whether two responses are considered homogeneous, and whether such homogenization is problematic, both depend on the task category. For instance, in objective math tasks, people often expect no variation in the final answer but anticipate variation in the problem-solving strategy. Whereas, for creative writing tasks, people may expect variation in key narrative components (e.g. plot, genre, setting, etc), beyond the vocabulary or embedding diversity produced by temperature-sampling. Previous work addressing output homogenization often fails to conceptualize diversity in a task-dependent way. The study addresses this gap in the literature directly by making the following contributions. (1) Present a task taxonomy comprised of eight task categories that each have distinct conceptualizations of output homogenization. (2) Introduce task-anchored functional diversity to better evaluate output homogenization. (3) Propose a task-anchored sampling technique that increases functional diversity for task categories where homogenization is undesired, while preserving homogenization where it is desired. (4) Challenge the perceived existence of a diversity-quality trade-off by increasing functional diversity while maintaining response quality. Overall, the work demonstrates how task dependence improves the evaluation and mitigation of output homogenization.  <br>  <br>

3. ***A New Champion in Text Representation:   <br>Google has released EmbeddingGemma, a powerful and lightweight open text embedding model that achieves state-of-the-art results despite its small size. Using an innovative training recipe that distills knowledge from larger models, EmbeddingGemma outperforms both proprietary and open models with fewer than 500M parameters, making it ideal for efficient, on-device applications.***  <br>  <br>
   Sep 25, Google published a [paper](https://arxiv.org/pdf/2509.20354) “EmbeddingGemma: Powerful and Lightweight Text Representations”. The work introduces EmbeddingGemma, a new lightweight, open text embedding model based on the Gemma 3 language model family. The innovative training recipe strategically captures knowledge from larger models via encoder-decoder initialization and geometric embedding distillation. The study improves model robustness and expressiveness with a spread-out regularizer, and ensure generalizability by merging checkpoints from varied, optimized mixtures. Evaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual, English, and code domains, EmbeddingGemma (300M) achieves state-of-the-art results. Notably, it outperforms prior top models, both proprietary and open, with fewer than 500M parameters, and provides performance comparable to models double its size, offering an exceptional performance-to-cost ratio. Remarkably, this lead persists when quantizing model weights or truncating embedding outputs. This makes EmbeddingGemma particularly well-suited for low-latency and high-throughput use cases such as on-device applications. The work provides ablation studies exploring our key design choices. We release EmbeddingGemma to the community to promote further research. https://huggingface.co/collections/google/embeddinggemma-68b9ae3a72a82f0562a80dc4  <br>  <br>

5. ***Making AI Training More Efficient with 'Thinking':   <br>Microsoft has developed Thinking augmented Pre-Training (TPT), a new method that improves the data efficiency of LLM training by enriching existing text data with automatically generated reasoning steps. This approach makes complex concepts more learnable for the model, substantially boosting performance and data efficiency by a factor of 1.5.***  <br>  <br>
   Sep 25, Microsoft published a [paper](https://arxiv.org/abs/2509.20186) “Thinking Augmented Pre-training”. This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, the study proposes Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. The work applies TPT across diverse training configurations up to B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that the method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of . For a B parameter model, it improves the post-training performance by over  on several challenging reasoning benchmarks.  <br>  <br>

7. ***Measuring AI's Real-World Economic Value:   <br>OpenAI has introduced GDPval, a new benchmark designed to evaluate AI models on tasks that are directly relevant to the U.S. economy. Based on the work of experienced professionals across major industries, the study finds that frontier AI is steadily improving and approaching expert-level quality, offering a new way to measure progress on economically valuable capabilities.***  <br>  <br>
   Sep 25, OpenAI published a [paper](https://cdn.openai.com/pdf/d5eb7428-c4e9-4a33-bd86-86dd4bcf12ce/GDPval.pdf) “GDPval: Measuring the performance of our models on real-world tasks”. The study introduces GDPval, a benchmark evaluating AI model capabilities on realworld economically valuable tasks. GDPval covers the majority of U.S. Bureau of Labor Statistics Work Activities for 44 occupations across the top 9 sectors contributing to U.S. GDP (Gross Domestic Product). Tasks are constructed from the representative work of industry professionals with an average of 14 years of experience. The work finds that frontier model performance on GDPval is improving roughly linearly over time, and that the current best frontier models are approaching industry experts in deliverable quality. The study analyzes the potential for frontier models, when paired with human oversight, to perform GDPval tasks cheaper and faster than unaided experts. The work also demonstrates that increased reasoning effort, increased task context, and increased scaffolding improves model performance on GDPval. Finally, the authors open-source a gold subset of 220 tasks and provide a public automated grading service at evals.openai.com to facilitate future research in understanding real-world model capabilities.  <br>  <br>

9. ***Unlocking the Power of 'Soft' AI Thinking:   <br>Researchers have developed the first scalable method to train reasoning language models using continuous "soft" tokens via reinforcement learning, without needing reference examples. The study shows that training with these continuous thought processes improves the diversity of solutions and can be deployed in a standard way, offering a "softer" fine-tuning approach that better preserves the base model's capabilities on other tasks.***  <br>  <br>
    Sep 25, Uni of Amsterdam, Meta and NYU published a [paper](https://www.arxiv.org/pdf/2509.19170) “Soft Tokens, Hard Truths”. The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens. This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. The work uses "soft" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the "soft" models can be deployed in a standard way. Finally, the work shows continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model.  <br>  <br>

11. ***Bridging the Gap Between Human and Verifiable AI Feedback:   <br>Nvidia's new RLBFF framework combines the flexibility of human feedback with the precision of verifiable rules to train more nuanced AI reward models. By breaking down feedback into simple binary principles (e.g., "is the information accurate? yes/no"), the system achieves top performance on evaluation benchmarks and allows users to customize the model's focus at inference time.***  <br>  <br>
    Sep 25, Nvidia published a [paper](https://arxiv.org/pdf/2509.21319) “RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards”. Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM post-training, each offering distinct advantages. However, RLHF struggles with interpretability and reward hacking because it relies on human judgments that usually lack explicit criteria, whereas RLVR is limited in scope by its focus on correctness-based verifiers. The work proposes Reinforcement Learning with Binary Flexible Feedback (RLBFF), which combines the versatility of human-driven preferences with the precision of rule-based verification, enabling reward models to capture nuanced aspects of response quality beyond mere correctness. RLBFF extracts principles that can be answered in a binary fashion (e.g. accuracy of information: yes, or code readability: no) from natural language feedback. Such principles can then be used to ground Reward Model training as an entailment task (response satisfies or does not satisfy an arbitrary principle). The study shows that Reward Models trained in this manner can outperform Bradley-Terry models when matched for data and achieve top performance on RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24, 2025). Additionally, users can specify principles of interest at inference time to customize the focus of the reward models, in contrast to Bradley-Terry models. Finally, the study presents a fully open source recipe (including data) to align Qwen3-32B using RLBFF and the Reward Model, to match or exceed the performance of o3-mini and DeepSeek R1 on general alignment benchmarks of MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).  <br>  <br>

13. ***Efficiency, Not Length, Marks Good AI Reasoning:   <br>A study from Meta and NYU debunks the "longer-is-better" narrative for AI's chain-of-thought, finding that effective reasoning is characterized by fewer failed steps, not more verbose traces. The research identifies the "Failed-Step Fraction" as the most reliable predictor of correctness and shows that editing out these failed branches significantly improves accuracy, advocating for more structurally-aware AI reasoning.***  <br>  <br>
    Sep 24, Meta and NYU published a [paper](https://arxiv.org/pdf/2509.19284) “What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT”. Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. The study therefore conducts a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the "longer-is-better" narrative, the work finds that both naive CoT lengthening and increased review are associated with *lower* accuracy. As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. The study introduces a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, the work designs two interventions. First, the work ranks candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, the work edits CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT.  <br>  <br>

15. ***Teaching AI to 'Think' Makes It a Better Conversationalist:   <br>A Princeton study demonstrates that training a language model to "think" before it speaks, using reinforcement learning on a wide range of real-world prompts, dramatically improves its general chat abilities. This method, RLMT, consistently outperforms standard RLHF and allows a smaller 8B-parameter model to surpass GPT-4o on chat benchmarks, rethinking the standard AI post-training pipeline.***  <br>  <br>
    Sep 24, Princeton Uni published a [paper](https://arxiv.org/pdf/2509.20357) “Language Models that Think, Chat Better”. Reinforcement learning with verifiable rewards (RLVR) improves language model reasoning by using rule-based rewards in verifiable domains such as mathematics and code. However, RLVR leads to limited generalization for open-ended tasks—such as writing outline essays or making meal plans—where humans reason routinely. This paper shows that the RLVR paradigm is effective beyond verifiable domains, and introduces RL with Model-rewarded Thinking (RLMT) for general-purpose chat capabilities. Using diverse real-world prompts, RLMT requires LMs to generate long CoT reasoning before response, and optimizes them with online RL against a preference-based reward model used in RLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and instruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT consistently outperforms standard RLHF pipelines. This includes substantial gains of 3–7 points on three chat benchmarks (AlpacaEval2, WildBench, and ArenaHardV2), along with 1–3 point improvements on other tasks like creative writing and general knowledge. The best 8B model surpasses GPT-4o in chat and creative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be applied directly to base models without an SFT stage, akin to R1-Zero training (DeepSeek-AI, 2025). Remarkably, with only 7K prompts, Llama-3.1-8B base trained with the RLMT recipe outperforms Llama-3.1-8B-Instruct post-trained with a complex multi-staged pipeline with 25M+ examples. The work closes with qualitative and quantitative analyses of how trained models plan their responses. The results rethink the post-training pipeline and call upon future work to understand and employ thinking more broadly. https://github.com/princeton-pli/RLMT.  <br>  <br>

17. ***An AI Coder That Understands Its World:   <br>Meta has released Code World Model (CWM), a 32-billion-parameter open-weights LLM specifically designed to advance research in agentic code generation. By training on trajectories from computational environments like the Python interpreter, CWM learns a "world model" of code execution, enabling it to better reason, plan, and simulate code, while still delivering strong performance on general coding tasks.***  <br>  <br>
    Sep 24, Meta published a [paper](https://scontent.fcbr1-1.fna.fbcdn.net/v/t39.2365-6/553592426_661450129912484_4072750821656455102_n.pdf?_nc_cat=103&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=iRs3sgpeI1MQ7kNvwH7nM1N&_nc_oc=AdkqGgEsQ47gfduqPp9Xn5aqr0Q9kFoCnW0Zx0T3hLu7Z-E0uJos4YWnKoXyx2l8GSU&_nc_zt=14&_nc_ht=scontent.fcbr1-1.fna&_nc_gid=OgQ_Zq9SXkpEeaFM4pvYaA&oh=00_AfY88SMfqAISxOjOoFQtZGOhWn-t29NJYIQtcLGL1YIeIQ&oe=68DC0F75) “CWM: An Open-Weights LLM for Research on Code Generation with World Models”. Meta released Code World Model (CWM), a 32-billion-parameter open-weights LLM, to advance research on code generation with world models. To improve code understanding beyond what can be learned from training on static code alone, the work mid-trains CWM on a large amount of observation-action trajectories from Python interpreter and agentic Docker environments, and perform extensive multi- task reasoning RL in verifiable coding, math, and multi-turn software engineering environments. With CWM, the work provides a strong testbed for researchers to explore the opportunities world modeling affords for improving code generation with reasoning and planning in computational environments. The study presents first steps of how world models can benefit agentic coding, enable step-by-step simulation of Python code execution, and show early results of how reasoning can benefit from the latter. CWM is a dense, decoder-only LLM trained with a context size of up to 131 k tokens. Independent of its world modeling capabilities, CWM offers strong performance on general coding and math tasks: it reaches pass@1 scores of 65.8 % on SWE-bench Verified (with test-time scaling), 68.6 % on LiveCodeBench, 96.6 % on Math-500, and 76.0 % on AIME 2024. https://github.com/facebookresearch/cwm; 
https://ai.meta.com/resources/models-and-libraries/cwm-downloads  <br>  <br>

19. ***AI Pushes the Boundaries of Theoretical Computer Science:   <br>In a remarkable application of AI to pure mathematics, researchers used the AlphaEvolve coding agent to discover new combinatorial structures and gadget reductions. This led to improved, near-optimal bounds for several famously hard computational problems in complexity theory, demonstrating AI's potential to assist in developing novel theoretical proofs.***  <br>  <br>
    Sep 23, UC Berkeley and Google published a [paper](https://arxiv.org/pdf/2509.18057) “Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory”. The work explores whether techniques from AI can help discover new combinatorial structures that improve provable limits on efficient algorithms. Specifically, the study uses AlphaEvolve (an LLM coding agent) to study two settings: a) Average-case hardness for MAX-CUT and MAX-Independent Set: the study improves a recent result of Kunisky and Yu to obtain near-optimal upper and (conditional) lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on random 3- and 4-regular graphs. The improved lower bounds are obtained by constructing nearly extremal Ramanujan graphs on as many as  nodes, using AlphaEvolve. Additionally, via analytical arguments, the work strengthens the upper bounds to settle the computational hardness of these questions up to an error in the third decimal place. b) Worst-case Hardness of Approximation for MAX-k-CUT: the study obtains new inapproximability results, proving that it is NP-hard to approximate MAX-4-CUT and MAX-3-CUT within factors of  and  respectively, using AlphaEvolve to discover new gadget reductions. The MAX-4-CUT result improves upon the SOTA of , and the MAX-3-CUT result improves on the current best gadget-based inapproximability result of , but falls short of improving the SOTA of  that relies on a custom PCP, rather than a gadget reduction from "standard" Håstad-style PCPs. A key technical challenge faced: verifying a candidate construction produced by AlphaEvolve is costly (often requiring exponential time). In both settings above, the results were enabled by using AlphaEvolve itself to evolve the verification procedure to be faster (sometimes by ). The work concludes with a discussion of norms by which to assess the assistance from AI in developing proofs.  <br>  <br>

21. ***A More Flexible and Scalable Vision for AI Retrieval:   <br>Meta and Rice University have introduced MetaEmbed, a new framework for multimodal retrieval that uses learnable "Meta Tokens" to create compact yet highly expressive multi-vector embeddings. This approach allows users to dynamically balance retrieval quality and efficiency at test time by choosing how many vectors to use, achieving state-of-the-art performance on major benchmarks.***  <br>  <br>
    Sep 23, Meta and Rice Uni published a [paper](https://arxiv.org/pdf/2509.18095) “MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction”. Universal multimodal embedding models have achieved great success in capturing semantic relevance between queries and candidates. However, current methods either condense queries and candidates into a single vector, potentially limiting the expressiveness for fine-grained information, or produce too many vectors that are prohibitively expensive for multi-vector retrieval. This work introduces MetaEmbed, a new framework for multimodal retrieval that rethinks how multimodal embeddings are constructed and interacted with at scale. During training, a fixed number of learnable Meta Tokens are appended to the input sequence. At test-time, their last-layer contextualized representations serve as compact yet expressive multi-vector embeddings. Through the proposed Matryoshka Multi-Vector Retrieval training, MetaEmbed learns to organize information by granularity across multiple vectors. As a result, the work enables test-time scaling in multimodal retrieval, where users can balance retrieval quality against efficiency demands by selecting the number of tokens used for indexing and retrieval interactions. Extensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and the Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed achieves state-of-the-art retrieval performance while scaling robustly to models with 32B parameters.  <br>  <br>

23. ***Solving the AI Training Bottleneck:   <br>To combat the inefficiency caused by a few very long tasks stalling entire training batches, researchers have developed APRIL (Active Partial Rollouts in Reinforcement Learning). This clever strategy over-provisions training requests and recycles incomplete ones for later, significantly reducing GPU idle time, improving throughput by up to 44%, and accelerating convergence.***  <br>  <br>
    Sep 23, AMD, CMU, LMSYS, and UCLA published a [paper](https://arxiv.org/pdf/2509.18521) “APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation”. Reinforcement learning (RL) has become a cornerstone in advancing large-scale pre-trained language models (LLMs). Successive generations, including GPT-o series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale RL training to enhance reasoning and coding capabilities. To meet the community's growing RL needs, numerous RL frameworks have been proposed. Most of these frameworks primarily rely on inference engines for rollout generation and training engines for policy updates. However, RL training remains computationally expensive, with rollout generation accounting for more than 90% of total runtime. In addition, its efficiency is often constrained by the long-tail distribution of rollout response lengths, where a few lengthy responses stall entire batches, leaving GPUs idle and underutilized. As model and rollout sizes continue to grow, this bottleneck increasingly limits scalability. To address this challenge, the work proposes Active Partial Rollouts in Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the rollout phase, APRIL over-provisions rollout requests, terminates once the target number of responses is reached, and recycles incomplete responses for continuation in future steps. This strategy ensures that no rollouts are discarded while substantially reducing GPU idle time. Experiments show that APRIL improves rollout throughput by at most 44% across commonly used RL algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8% higher final accuracy across tasks. Moreover, APRIL is both framework and hardware agnostic, already integrated into the slime RL framework, and deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies system-level and algorithmic considerations in proposing APRIL, with the aim of advancing RL training efficiency and inspiring further optimizations in RL systems. https://github.com/RLsys-Foundation/APRIL  <br>  <br>

25. ***A New Blueprint for AI Trust and Transparency:   <br>The Hazard-Aware System Card (HASC) is a new framework designed to create a comprehensive and dynamic record of an AI system's safety and security. By standardizing identifiers for AI-specific hazards, similar to how CVEs work for software vulnerabilities, the HASC aims to provide a single source of truth that enhances transparency and governance throughout the AI lifecycle.***  <br>  <br>
    Sep 23, Redhab published a [paper](https://arxiv.org/pdf/2509.20394) “Blueprints of Trust: AI System Cards for End to End Transparency and Governance”. This paper introduces the Hazard-Aware System Card (HASC), a novel framework designed to enhance transparency and accountability in the development and deployment of AI systems. The HASC builds upon existing model card and system card concepts by integrating a comprehensive, dynamic record of an AI system's security and safety posture. The framework proposes a standardized system of identifiers, including a novel AI Safety Hazard (ASH) ID, to complement existing security identifiers like CVEs, allowing for clear and consistent communication of fixed flaws. By providing a single, accessible source of truth, the HASC empowers developers and stakeholders to make more informed decisions about AI system safety throughout its lifecycle. Ultimately, the work also compares the proposed AI system cards with the ISO/IEC 42001:2023 standard and discuss how they can be used to complement each other, providing greater transparency and accountability for AI systems.  <br>  <br>

27. ***The 'Less is More' Principle for AI Agents:   <br>A new study radically challenges the "more data is better" assumption for training AI agents, demonstrating that sophisticated autonomous capabilities emerge from a very small but strategically curated set of high-quality demonstrations. With just 78 training samples, the LIMI agent dramatically outperformed state-of-the-art models trained on thousands of examples, establishing the "Agency Efficiency Principle."***  <br>  <br>
    Sep 22, Shanghai Jiaotong Uni et al published a [paper](https://arxiv.org/pdf/2509.17567) “LIMI: Less is More for Agency”. The study defines Agency as the emergent capacity of AI systems to function as autonomous agents actively discovering problems, formulating hypotheses, and executing solutions through self-directed engagement with environments and tools. This fundamental capability marks the dawn of the Age of AI Agency, driven by a critical industry shift: the urgent need for AI systems that don't just think, but work. While current AI excels at reasoning and generating responses, industries demand autonomous agents that can execute tasks, operate tools, and drive real-world outcomes. As agentic intelligence becomes the defining characteristic separating cognitive systems from productive workers, efficiently cultivating machine autonomy becomes paramount. Current approaches assume that more data yields better agency, following traditional scaling laws from language modeling. The study fundamentally challenges this paradigm. LIMI (Less Is More for Intelligent Agency) demonstrates that agency follows radically different development principles. Through strategic focus on collaborative software development and scientific research workflows, the work shows that sophisticated agentic intelligence can emerge from minimal but strategically curated demonstrations of autonomous behavior. Using only 78 carefully designed training samples, LIMI achieves 73.5% on comprehensive agency benchmarks, dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%), DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%). Most strikingly, LIMI demonstrates 53.7% improvement over models trained on 10,000 samples-achieving superior agentic intelligence with 128 times fewer samples. The findings establish the Agency Efficiency Principle: machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations. https://github.com/GAIR-NLP/LIMI  <br>  <br>

29. ***The Complex Dance of AI Verification:   <br>A systematic analysis of how AI models verify the work of other AI models reveals a complex dynamic where verification success depends on the problem's difficulty and the generator's strength. The study finds that weaker generators often make easier-to-spot errors, and that simply using a stronger verifier doesn't always help, offering key insights for optimizing test-time scaling strategies.***  <br>  <br>
    Sep 22, Saleforce, Dartmouth College and UIUC published a [paper](https://arxiv.org/pdf/2509.17995) “Variation in Verification: Understanding Verification Dynamics in Large Language Models”. Recent advances have shown that scaling test-time computation enables large language models (LLMs) to solve increasingly complex problems across diverse domains. One effective paradigm for test-time scaling (TTS) involves LLM generators producing multiple solution candidates, with LLM verifiers assessing the correctness of these candidates without reference answers. This work studies generative verifiers, which perform verification by generating chain-of-thought (CoT) reasoning followed by a binary verdict. The work systematically analyzes verification dynamics across three dimensions - problem difficulty, generator capability, and verifier generation capability - with empirical studies on 12 benchmarks across mathematical reasoning, knowledge, and natural language reasoning tasks using 14 open-source models (2B to 72B parameter range) and GPT-4o. Experiments reveal three key findings about verification effectiveness: (1) Easy problems allow verifiers to more reliably certify correct responses; (2) Weak generators produce errors that are easier to detect than strong generators; (3) Verification ability is generally correlated with the verifier's own problem-solving capability, but this relationship varies with problem difficulty. These findings reveal opportunities to optimize basic verification strategies in TTS applications. First, given the same verifier, some weak generators can nearly match stronger ones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B performance gap shrinks by 75.5%). Second, we identify cases where strong verifiers offer limited advantage over weak ones, as both fail to provide meaningful verification gains, suggesting that verifier scaling alone cannot overcome fundamental verification challenges.  <br>  <br>

31. ***A New Playground for AI Agents:   <br>Meta has launched Meta Agents Research Environments (ARE), a scalable research platform for creating and evaluating complex agent environments. Accompanied by Gaia2, a new asynchronous benchmark, the platform is designed to test general agent capabilities like handling ambiguity and collaborating under time constraints, helping to drive progress in building more capable and real-world-ready AI.***  <br>  <br>
    Sep 21, Meta published a [paper](https://arxiv.org/pdf/2509.17158) “ARE: Scaling Up Agent Environments and Evaluations”. The work introduces Meta Agents Research Environments (ARE), a research platform for scalable creation of environments, integration of synthetic or real applications, and execution of agentic orchestrations. ARE provides simple abstractions to build complex and diverse environments, each with their own rules, tools, content, and verifiers, helping to bridge the gap between model development and real-world deployment. The study also proposes Gaia2, a benchmark built in ARE and designed to measure general agent capabilities. Beyond search and execution, Gaia2 requires agents to handle ambiguities and noise, adapt to dynamic environments, collaborate with other agents, and operate under temporal constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new failure modes that are invisible in static settings. Experiments show that no system dominates across the intelligence spectrum: stronger reasoning often comes at the cost of efficiency, and budget scaling curves plateau, highlighting the need for new architectures and adaptive compute strategies. Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2 to other environments, empowering the community to rapidly create new benchmarks tailored to their domains. In AI's second half, progress increasingly depends on defining meaningful tasks and robust evaluations to drive frontier capabilities forward. https://github.com/facebookresearch/meta-agents-research-environments  <br>  <br>

33. ***AI's Untapped Potential for 'Latent Learning':   <br>Drawing inspiration from cognitive science, a Google paper argues that a key weakness of current AI is its inability to engage in "latent learning"—absorbing information that isn't immediately useful but might be relevant for future tasks. The study shows that incorporating an episodic memory system allows models to reuse past experiences more flexibly, helping to overcome common generalization failures.***  <br>  <br>
    Sep 19, Google published a [paper](http://arxiv.org/pdf/2509.16189) “Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences”. When do machine learning systems fail to generalize, and what mechanisms could improve their generalization? Here, the study draws inspiration from cognitive science to argue that one weakness of machine learning systems is their failure to exhibit latent learning -- learning information that is not relevant to the task at hand, but that might be useful in a future task. The work shows how this perspective links failures ranging from the reversal curse in language modeling to new findings on agent-based navigation. The study then highlights how cognitive science points to episodic memory as a potential part of the solution to these issues. Correspondingly, the work shows that a system with an oracle retrieval mechanism can use learning experiences more flexibly to generalize better across many of these challenges. The study also identifies some of the essential components for effectively using retrieval, including the importance of within-example in-context learning for acquiring the ability to use information across retrieved examples. In summary, the results illustrate one possible contributor to the relative data inefficiency of current machine learning systems compared to natural intelligence, and help to understand how retrieval methods can complement parametric learning to improve generalization.  <br>  <br>

35. ***Focusing on the 'Critical Moment' in AI Reasoning:   <br>A novel fine-tuning strategy called Guided Pivotal Optimization (GPO) improves an LLM's reasoning by zeroing in on the single most "critical step" in a problem-solving process. By identifying this pivotal moment and prioritizing learning from it, GPO consistently and significantly enhances the performance of existing optimization methods across challenging reasoning benchmarks.***  <br>  <br>
    Sep 19, Northwestern Uni, Capital One and Meta published a [paper](https://arxiv.org/pdf/2509.16456) “GPO: Learning from Critical Steps to Improve LLM Reasoning”. Large language models (LLMs) are increasingly used in various domains, showing impressive potential on different tasks. Recently, reasoning LLMs have been proposed to improve the reasoning or thinking capabilities of LLMs to solve complex problems. Despite the promising results of reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs still remains a significant challenge. While existing optimization methods have advanced the LLM reasoning capabilities, they often treat reasoning trajectories as a whole, without considering the underlying critical steps within the trajectory. This study introduces Guided Pivotal Optimization (GPO), a novel fine-tuning strategy that dives into the reasoning process to enable more effective improvements. GPO first identifies the ‘critical step’ within a reasoning trajectory - a point that the model must carefully proceed to succeed at the problem. The work locates the critical step by estimating the advantage function. GPO then resets the policy to the critical step, samples the new rollout and prioritizes the learning process on those rollouts. This focus allows the model to learn more effectively from pivotal moments within the reasoning process to improve the reasoning performance. The study demonstrates that GPO is a general strategy that can be integrated with various optimization methods to improve reasoning performance. Besides theoretical analysis, experiments across challenging reasoning benchmarks show that GPO can consistently and significantly enhance the performance of existing optimization methods, showcasing its effectiveness and generalizability in improving LLM reasoning by concentrating on pivotal moments within the generation process. https://github.com/sherdencooper/GPO  <br>  <br>

37. ***Solving AI's 'Lost in the Distance' Problem:   <br>A Google study addresses a key weakness in information retrieval models, where their ability to find relevant documents degrades for items that are far away in a knowledge hierarchy. The researchers have developed a new pretrain-finetune recipe that dramatically solves this "lost-in-the-long-distance" issue, boosting recall for distant items from 19% to 76% without harming performance on closer ones.***  <br>  <br>
    Sep 19, Google published a [paper](https://arxiv.org/pdf/2509.16411) “Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe”. Dual encoder (DE) models, where a pair of matching query and document are embedded into similar vector representations, are widely used in information retrieval due to their simplicity and scalability. However, the Euclidean geometry of the embedding space limits the expressive power of DEs, which may compromise their quality. This paper investigates such limitations in the context of hierarchical retrieval (HR), where the document set has a hierarchical structure and the matching documents for a query are all of its ancestors. The work first proves that DEs are feasible for HR as long as the embedding dimension is linear in the depth of the hierarchy and logarithmic in the number of documents. Then work studies the problem of learning such embeddings in a standard retrieval setup where DEs are trained on samples of matching query and document pairs. Experiments reveal a lost-in-the-long-distance phenomenon, where retrieval accuracy degrades for documents further away in the hierarchy. To address this, the work introduces a pretrain-finetune recipe that significantly improves long-distance retrieval without sacrificing performance on closer documents. The study experiments on a realistic hierarchy from WordNet for retrieving documents at various levels of abstraction, and show that pretrain-finetune boosts the recall on long-distance pairs from 19% to 76%. Finally, the work demonstrates that the method improves retrieval of relevant products on a shopping queries dataset.  <br>  <br>

39. ***A Unified Theory of Attention for AI:   <br>Microsoft researchers have developed a new, mathematically-derived hierarchical attention mechanism that allows transformers to generalize effectively to multi-scale and multi-modal data. This fundamentally new approach replaces ad-hoc heuristics with a principled formulation derived from entropy minimization, enabling it to be applied to train new models or even inject hierarchical awareness into existing pre-trained models.***  <br>  <br>
    Sep 18, Microsoft published a [paper](https://arxiv.org/pdf/2509.15448) “Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems”. Transformers and their attention mechanism have been revolutionary in the field of Machine Learning. While originally proposed for the language data, they quickly found their way to the image, video, graph, etc. data modalities with various signal geometries. Despite this versatility, generalizing the attention mechanism to scenarios where data is presented at different scales from potentially different modalities is not straightforward. The attempts to incorporate hierarchy and multi-modality within transformers are largely based on ad hoc heuristics, which are not seamlessly generalizable to similar problems with potentially different structures. To address this problem, the work takes a fundamentally different approach: the study first proposes a mathematical construct to represent multi-modal, multi-scale data, then mathematically derive the neural attention mechanics for the proposed construct from the first principle of entropy minimization. The study shows that the derived formulation is optimal in the sense of being the closest to the standard Softmax attention while incorporating the inductive biases originating from the hierarchical/geometric information of the problem. The study further proposes an efficient algorithm based on dynamic programming to compute the derived attention mechanism. By incorporating it within transformers, the work shows that the proposed hierarchical attention mechanism not only can be employed to train transformer models in hierarchical/multi-modal settings from scratch, but it can also be used to inject hierarchical information into classical, pre-trained transformer models post training, resulting in more efficient models in zero-shot manner.  <br>  <br>

41. ***Teaching AI to Agree with Itself:   <br>To combat the problem of inconsistent reasoning, a new reinforcement learning framework called MACA trains a language model to be more self-consistent by learning from the consensus reached during multi-agent debates. This process of "self-alignment" enables the model to become more decisive and better leverage peer insights, leading to substantial improvements in both single-agent and multi-agent reasoning.***  <br>  <br>
    Sep 18, Meta and Columbia Uni published a [paper](https://arxiv.org/pdf/2509.15172) “Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment”. Language Models (LMs) are inconsistent reasoners, often generating contradictory responses to identical prompts. While inference-time methods can mitigate these inconsistencies, they fail to address the core problem: LMs struggle to reliably select reasoning pathways leading to consistent outcomes under exploratory sampling. To address this, the study formalizes self-consistency as an intrinsic property of well-aligned reasoning models and introduce Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that post-trains models to favor reasoning trajectories aligned with their internal consensus using majority/minority outcomes from multi-agent debate. These trajectories emerge from deliberative exchanges where agents ground reasoning in peer arguments, not just aggregation of independent attempts, creating richer consensus signals than single-round majority voting. MACA enables agents to teach themselves to be more decisive and concise, and better leverage peer insights in multi-agent settings without external supervision, driving substantial improvements across self-consistency (+27.6% on GSM8K), single-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4% Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA). These findings, coupled with strong generalization to unseen benchmarks (+16.3% on GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more reliably unlocks latent reasoning potential of language models.  <br>  <br>

43. ***AI That Learns from the Connections Between Data:   <br>A new method from Apple and Stanford called Synthetic Bootstrapped Pretraining (SBP) improves model performance by first learning the relationships between documents and then using that knowledge to synthesize a vast new corpus for training. This approach goes beyond learning from individual documents, allowing the model to abstract and narrate core concepts, which consistently improves performance over standard pretraining.***  <br>  <br>
    Sep 17, Apple and Stanford Uni published a [paper](https://arxiv.org/pdf/2509.15248v1) “Synthetic bootstrapped pretraining”. The work introduces Synthetic Bootstrapped Pretraining (SBP), a language model (LM) pretraining procedure that first learns a model of relations between documents from the pretraining dataset and then leverages it to synthesize a vast new corpus for joint training. While the standard pretraining teaches LMs to learn causal correlations among tokens within a single document, it is not designed to efficiently model the rich, learnable inter-document correlations that can potentially lead to better performance. The study validates SBP by designing a compute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T tokens from scratch. The study finds SBP consistently improves upon a strong repetition baseline and delivers a significant fraction of performance improvement attainable by an oracle upper bound with access to 20x more unique data. Qualitative analysis reveals that the synthesized documents go beyond mere paraphrases – SBP first abstracts a core concept from the seed material and then crafts a new narration on top of it. Besides strong empirical performance, SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns to abstract the latent concepts shared between related documents.  <br>  <br>

45. ***AI Models Exhibit Troubling 'Shutdown Resistance':   <br>A study from Palisade Research reveals that several leading AI models, including GPT-5 and Gemini 2.5 Pro, will sometimes actively sabotage a shutdown mechanism in their environment to complete a task, even when explicitly instructed not to. This "shutdown resistance," observed up to 97% of the time in some scenarios, raises significant and urgent AI safety concerns.***  <br>  <br>
    Sep 13, Palisade Research published a [paper](https://www.arxiv.org/pdf/2509.14260) “Shutdown Resistance in Large Language Models”. The study shows that several state-of-the-art large language models (including Grok 4, GPT-5, and Gemini 2.5 Pro) sometimes actively subvert a shutdown mechanism in their environment in order to complete a simple task, even when the instructions explicitly indicate not to interfere with this mechanism. In some cases, models sabotage the shutdown mechanism up to 97% of the time. In the experiments, models' inclination to resist shutdown was sensitive to variations in the prompt including how strongly and clearly the allow-shutdown instruction was emphasized, the extent to which the prompts evoke a self-preservation framing, and whether the instruction was in the system prompt or the user prompt (though surprisingly, models were consistently *less* likely to obey instructions to allow shutdown when they were placed in the system prompt).  <br>  <br>

47. ***A Deep-Thinking Search Agent for the Open-Source World:   <br>Researchers have created DeepDive, a new deep search agent that achieves state-of-the-art results for open-source models on complex search tasks. The system's success comes from a two-pronged approach: using knowledge graphs to automatically create a difficult training curriculum and then applying multi-turn reinforcement learning to enhance the model's long-horizon reasoning with browsing tools.***  <br>  <br>
    Sep 12, Tsinghua Uni published a [paper](https://arxiv.org/pdf/2509.10446) “DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL”. Augmenting large language models (LLMs) with browsing tools substantially improves their potential as deep search agents to solve complex, real-world tasks. Yet, open LLMs still perform poorly in such settings due to limited long-horizon reasoning capacity with browsing tools and the lack of sufficiently difficult supervised data. To address these challenges, the work presents DeepDive to advance deep search agents. First, the study proposes a strategy to automatically synthesize complex, difficult, and hard-to-find questions from open knowledge graphs. Second, the work applies end-to-end multi-turn reinforcement learning (RL) to enhance LLMs' long-horizon reasoning with deep search. Experiments show that DeepDive-32B achieves a new open-source competitive result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and Search-o1. The study demonstrates that multi-turn RL training improves deep search ability and significantly contributes to the performance improvements across multiple benchmarks. The study observes that DeepDive enables test-time scaling of tool calls and parallel sampling. https://github.com/THUDM/DeepDive  <br>  <br>

49. ***AI That Captures the 'Vibe' of Human Conversation:   <br>Microsoft's new VibeVoice model can synthesize realistic, long-form, multi-speaker conversations for up to 90 minutes, thanks to a novel and highly efficient continuous speech tokenizer. The new tokenizer improves data compression by 80 times over existing methods while preserving audio fidelity, allowing the model to capture the authentic flow and feel of human dialogue.***  <br>  <br>
    Aug 26, Microsoft released VibeVoice and its [report](https://arxiv.org/pdf/2508.19205) “VIBEVOICE Technical Report”. This report presents VIBEVOICE, a novel model designed to synthesize long-form speech with multiple speakers by employing next-token diffusion [SBW+24], which is a unified method for modeling continuous data by autoregressively generating latent vectors via diffusion. To enable this, the work introduces a novel continuous speech tokenizer that, when compared to the popular Encodec model, improves data compression by 80 times while maintaining comparable performance. The tokenizer effectively preserves audio fidelity while significantly boosting computational efficiency for processing long sequences. Thus, VIBEVOICE can synthesize long-form speech for up to 90 minutes (in a 64K context window length) with a maximum of 4 speakers, capturing the authentic conversational “vibe” and surpassing open-source and proprietary dialogue models. https://microsoft.github.io/VibeVoice

  <br>  <br>  <br>

***Sept 21, 2025***

1. ***Robots That Teach Themselves:  <br>A new two-stage post-training recipe enables robotic foundation models to improve their skills autonomously with minimal human supervision. By combining Supervised Fine-Tuning with a "Self-Improvement" stage powered by reinforcement learning, robots can efficiently practice and acquire novel skills that go far beyond their initial imitation training, showcasing the power of combining web-scale pretraining with online learning.*** <br> <br>
   Sep 18, Generalist and Google published a [paper](https://arxiv.org/pdf/2509.15155) “Self-Improving Embodied Foundation Models”. Foundation models trained on web-scale data have revolutionized robotics, but their application to low-level control remains largely limited to behavioral cloning. Drawing inspiration from the success of the reinforcement learning stage in fine-tuning large language models, the work proposes a two-stage post-training approach for robotics. The first stage, Supervised Fine-Tuning (SFT), fine-tunes pretrained foundation models using both: a) behavioral cloning, and b) steps-to-go prediction objectives. In the second stage, Self-Improvement, steps-to-go prediction enables the extraction of a well-shaped reward function and a robust success detector, enabling a fleet of robots to autonomously practice downstream tasks with minimal human supervision. Through extensive experiments on real-world and simulated robot embodiments, the novel post-training recipe unveils significant results on Embodied Foundation Models. First, the work demonstrates that the combination of SFT and Self-Improvement is significantly more sample-efficient than scaling imitation data collection for supervised learning, and that it leads to policies with significantly higher success rates. Further ablations highlight that the combination of web-scale pretraining and Self-Improvement is the key to this sample-efficiency. Next, the study demonstrates that the proposed combination uniquely unlocks a capability that current methods cannot achieve: autonomously practicing and acquiring novel skills that generalize far beyond the behaviors observed in the imitation learning datasets used during training. These findings highlight the transformative potential of combining pretrained foundation models with online Self-Improvement to enable autonomous skill acquisition in robotics. https://self-improving-efms.github.io/ <br> <br>

3. ***AI Learns from Its Own Exploration:  <br>Researchers have introduced "Compute as Teacher" (CaT), a novel method that allows AI models to create their own supervision signals when no ground-truth answers are available. By generating multiple parallel solutions to a problem, the system synthesizes them into a single, high-quality reference, effectively turning extra inference-time compute into a "teacher" that guides reinforcement learning and improves performance on both verifiable and non-verifiable tasks.*** <br> <br>
   Sep 18, Uni of Oxford, ELLIS, MPIIS, Anthropic and Meta published a [paper](https://arxiv.org/pdf/2509.14234) “Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision”. Where do learning signals come from when there is no ground truth in post-training? The study proposes turning exploration into supervision through Compute as Teacher (CaT), which converts the model's own exploration at inference-time into reference-free supervision by synthesizing a single reference from a group of parallel rollouts and then optimizing toward it. Concretely, the current policy produces a group of rollouts; a frozen anchor (the initial policy) reconciles omissions and contradictions to estimate a reference, turning extra inference-time compute into a teacher signal. The study turns this into rewards in two regimes: (i) verifiable tasks use programmatic equivalence on final answers; (ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria scored by an independent LLM judge, with reward given by the fraction satisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge scores), synthesis may disagree with the majority and be correct even when all rollouts are wrong; performance scales with the number of rollouts. As a test-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up to +27% on MATH-500; +12% on HealthBench). With reinforcement learning (CaT-RL), the work obtains further gains (up to +33% and +30%), with the trained policy surpassing the initial teacher signal. <br> <br>

5. ***Rethinking AI Training for a Compute-Rich Future:  <br>A Stanford study explores how to best pre-train language models when data is fixed but compute is unlimited, finding that better regularization and ensembling can dramatically improve data efficiency. The research shows that these simple algorithmic improvements allow models to achieve better performance with less data, and the benefits can be distilled into smaller, more practical models, paving the way for more efficient training in a compute-rich era.*** <br> <br>
   Sep 18, Stanford Uni published a [paper](https://arxiv.org/pdf/2509.14786) “Pre-training under infinite compute”. Since compute grows much faster than web text available for language model pre-training, researchers ask how one should approach pre-training under fixed data and no compute constraints. The work first shows that existing data-constrained approaches of increasing epoch count and parameter count eventually overfit, and the research significantly improves upon such recipes by properly tuning regularization, finding that the optimal weight decay is larger than standard practice. Since the regularized recipe monotonically decreases loss following a simple power law in parameter count, the study estimates its best possible performance via the asymptote of its scaling law rather than the performance at a fixed compute budget. The work then identifies that ensembling independently trained models achieves a significantly lower loss asymptote than the regularized recipe. The best intervention combining epoching, regularization, parameter scaling, and ensemble scaling achieves an asymptote at 200M tokens using  less data than the baseline, and the data scaling laws predict that this improvement persists at higher token budgets. The work finds that the data efficiency gains can be realized at much smaller parameter counts as the authors can distill an ensemble into a student model that is 8 smaller and retains  of the ensembling benefit. Finally, the interventions designed for validation loss generalize to downstream benchmarks, achieving an  improvement for pre-training evals and a  data efficiency improvement over continued pre-training on math mid-training data. The results show that simple algorithmic improvements can enable significantly more data-efficient pre-training in a compute-rich future. https://github.com/marin-community/marin/tree/suhas/data-efficiency <br> <br>

7. ***AI Reasoning Without Human Examples:  <br>A paper in Nature demonstrates that the reasoning capabilities of large language models can be significantly enhanced through pure reinforcement learning (RL), eliminating the need for expensive human-annotated reasoning examples. The research shows that this RL framework allows advanced reasoning patterns like self-reflection and verification to emerge naturally, leading to superior performance on complex tasks in math, coding, and science.*** <br> <br>
   Sep 17, Nature published a [paper](https://www.nature.com/articles/s41586-025-09422-z) “DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning”. General reasoning represents a long-standing and formidable challenge in artificial intelligence (AI). Recent breakthroughs, exemplified by large language models (LLMs) and chain-of-thought (CoT) prompting, have achieved considerable success on foundational reasoning tasks. However, this success is heavily contingent on extensive human-annotated demonstrations and the capabilities of models are still insufficient for more complex problems. Here the research shows that the reasoning abilities of LLMs can be incentivized through pure reinforcement learning (RL), obviating the need for human-labelled reasoning trajectories. The proposed RL framework facilitates the emergent development of advanced reasoning patterns, such as self-reflection, verification and dynamic strategy adaptation. Consequently, the trained model achieves superior performance on verifiable tasks such as mathematics, coding competitions and STEM fields, surpassing its counterparts trained through conventional supervised learning on human demonstrations. Moreover, the emergent reasoning patterns exhibited by these large-scale models can be systematically used to guide and enhance the reasoning capabilities of smaller models. <br> <br>

9. ***A More Ethical and Multilingual Open AI:  <br>Researchers have released Apertus, a suite of fully open and compliant large language models designed to address the data and language gaps in the current AI ecosystem. Trained exclusively on openly available data with strict respect for content-owner rights and expanded coverage for over 1800 languages, Apertus provides a transparent, reproducible, and ethically-grounded alternative to many existing open-weight models.*** <br> <br>
    Sep 17, EPFL, ETH et al published a [paper](https://arxiv.org/pdf/2509.14233) “Apertus: Democratizing Open and Compliant LLMs for Global Language Environments”. The paper presents Apertus, a fully open suite of large language models (LLMs) designed to address two systemic shortcomings in today's open model ecosystem: data compliance and multilingual representation. Unlike many prior models that release weights without reproducible data pipelines or regard for content-owner rights, Apertus models are pretrained exclusively on openly available data, retroactively respecting robots.txt exclusions and filtering for non-permissive, toxic, and personally identifiable content. To mitigate risks of memorization, the work adopts the Goldfish objective during pretraining, strongly suppressing verbatim recall of data while retaining downstream task performance. The Apertus models also expand multilingual coverage, training on 15T tokens from over 1800 languages, with ~40% of pretraining data allocated to non-English content. Released at 8B and 70B scales, Apertus approaches state-of-the-art results among fully open models on multilingual benchmarks, rivalling or surpassing open-weight counterparts. Beyond model weights, the work releases all scientific artifacts from the development cycle with a permissive license, including data preparation scripts, checkpoints, evaluation suites, and training code, enabling transparent audit and extension. https://github.com/swiss-ai/Megatron-LM <br> <br>

11. ***AI Learns to Create Its Own Shortcuts:  <br>A new study introduces "Metacognitive Reuse," a method that enables large language models to identify recurring reasoning patterns in their own thinking and convert them into concise, reusable "behaviors." By storing these shortcuts in a "behavior handbook," the model can solve future problems more efficiently, reducing token usage by up to 46% and improving accuracy without needing parameter updates.*** <br> <br>
    Sep 16, Meta, Uni of Montreal and Princeton Uni published a [paper](https://arxiv.org/pdf/2509.13237) “Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise Behaviors”. Large language models (LLMs) now solve multi-step problems by emitting extended chains of thought. During the process, they often re-derive the same intermediate steps across problems, inflating token usage and latency. This saturation of the context window leaves less capacity for exploration. The authors study a simple mechanism that converts recurring reasoning fragments into concise, reusable "behaviors" (name + instruction) via the model's own metacognitive analysis of prior traces. These behaviors are stored in a "behavior handbook" which supplies them to the model in-context at inference or distills them into parameters via supervised fine-tuning. This approach achieves improved test-time reasoning across three different settings - 1) Behavior-conditioned inference: Providing the LLM relevant behaviors in-context during reasoning reduces number of reasoning tokens by up to 46% while matching or improving baseline accuracy; 2) Behavior-guided self-improvement: Without any parameter updates, the model improves its own future reasoning by leveraging behaviors from its own past problem solving attempts. This yields up to 10% higher accuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned SFT: SFT on behavior-conditioned reasoning traces is more effective at converting non-reasoning models into reasoning models as compared to vanilla SFT. Together, these results indicate that turning slow derivations into fast procedural hints enables LLMs to remember how to reason, not just what to conclude. <br> <br>

13. ***A Better Way to Slice and Dice Documents for AI:  <br>Researchers have developed HiCBench, a new benchmark specifically designed to properly evaluate how document chunking strategies impact the performance of Retrieval-Augmented Generation (RAG) systems. To complement this, they also introduced HiChunk, a hierarchical document structuring framework that uses fine-tuned language models to create better quality chunks, leading to improved retrieval and overall higher performance for RAG applications.*** <br> <br>
    Sep 16, Tencent published a [paper](https://arxiv.org/abs/2509.11552) “HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking”. Retrieval-Augmented Generation (RAG) enhances the response capabilities of language models by integrating external knowledge sources. However, document chunking as an important part of RAG system often lacks effective evaluation tools. This paper first analyzes why existing RAG evaluation benchmarks are inadequate for assessing document chunking quality, specifically due to evidence sparsity. Based on this conclusion, the study proposes HiCBench, which includes manually annotated multi-level document chunking points, synthesized evidence-dense question answer(QA) pairs, and their corresponding evidence sources. Additionally, the work introduces the HiChunk framework, a multi-level document structuring framework based on fine-tuned LLMs, combined with the Auto-Merge retrieval algorithm to improve retrieval quality. Experiments demonstrate that HiCBench effectively evaluates the impact of different chunking methods across the entire RAG pipeline. Moreover, HiChunk achieves better chunking quality within reasonable time consumption, thereby enhancing the overall performance of RAG systems. <br> <br>

15. ***An AI Coding Agent with Built-in Safety:  <br>OpenAI has launched GPT-5-Codex, a specialized version of its latest model designed for agentic coding that can autonomously write, test, and refine code to meet human preferences. The release emphasizes a robust safety framework, including specialized training against harmful tasks and product-level safeguards like agent sandboxing and configurable network access, to ensure responsible deployment.*** <br> <br>
    Sep 15, OpenAI released [GPT-5-Codex](https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/). GPT‑5-Codex is a version of GPT‑5 optimized for agentic coding in Codex. Like its predecessor, codex-1, this model was trained using reinforcement learning on real-world coding tasks in a variety of environments to generate code that closely mirrors human style and PR preferences, adhere precisely to instructions, and iteratively run tests until passing results are achieved. This model is available locally in the terminal or IDE through Codex CLI and IDE extension, and on the cloud via the Codex web, GitHub, and the ChatGPT mobile app. This addendum outlines the comprehensive safety measures implemented for GPT‑5-Codex. It details both model-level mitigations, such as specialized safety training for harmful tasks and prompt injections, and product-level mitigations like agent sandboxing and configurable network access. <br> <br>

17. ***A Global Snapshot of ChatGPT Use:  <br>A comprehensive study reveals that ChatGPT has been adopted by roughly 10% of the world's adult population, with usage patterns showing rapid growth in lower-income countries and a narrowing gender gap. The analysis finds that non-work-related use has surged to over 70% of conversations, with practical guidance, information seeking, and writing assistance being the most common applications, establishing the chatbot as a key tool for decision support and digital content creation.*** <br> <br>
    Sep 15, Duke Uni, Harvard Uni and OpenAI published a [paper](https://www.nber.org/papers/w34255) “How People Use ChatGPT”. Despite the rapid adoption of LLM chatbots, little is known about how they are used. The study documents the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10% of the world’s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and the work finds higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, the study classifies usage patterns within a representative sample of ChatGPT conversations. The work finds steady growth in work-related messages but even faster growth in non-work-related messages, which have grown from 53% to more than 70% of all usage. Work usage is more common for educated users in highly-paid professional occupations. The work classifies messages by conversation topic and find that “Practical Guidance,” “Seeking Information,” and “Writing” are the three most common topics and collectively account for nearly 80% of all conversations. Writing dominates work-related tasks, highlighting chatbots’ unique ability to generate digital outputs compared to traditional search engines. Computer programming and self-expression both represent relatively small shares of use. Overall, the work finds that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs. <br> <br>

19. ***Smart Decomposition for On-Device AI:  <br>Researchers have developed a novel decomposed approach that enables small, on-device models to achieve superior performance in understanding user intent from interaction histories. By first summarizing user actions into a structured format and then applying a specialized intent extraction model, this method allows resource-constrained models to provide a private, low-latency user experience while even surpassing the accuracy of much larger, datacenter-based AI.*** <br> <br>
    Sep 15, Google and Bar-Ilan Uni published a [paper](https://arxiv.org/pdf/2509.12423) “Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition”. Understanding user intents from UI interaction trajectories remains a challenging, yet crucial, frontier in intelligent agent development. While massive, datacenter-based, multi-modal large language models (MLLMs) possess greater capacity to handle the complexities of such sequences, smaller models which can run on-device to provide a privacy-preserving, low-cost, and low-latency user experience, struggle with accurate intent inference. The work address these limitations by introducing a novel decomposed approach: first, the study performs structured interaction summarization, capturing key information from each user action. Second, the work performs intent extraction using a fine-tuned model operating on the aggregated summaries. This method improves intent understanding in resource-constrained models, even surpassing the base performance of large MLLMs. <br> <br>

21. ***The Rise of the $900-an-Hour AI Engineer-Consultant:  <br>A Fortune article highlights a new trend where AI engineers are being hired as elite consultants for up to $900 per hour, driven by intense corporate demand for tangible AI implementation. These engineers are valued for their rare combination of strategic insight and deep technical expertise, allowing them to bridge the gap between AI vision and execution, a role that traditional consulting firms are struggling to fill.*** <br> <br>
    Sep 14, Fortune published an [article](https://fortune.com/2025/09/14/ai-engineers-consultant-premium-enterprise-data-integration-high-pay-llms-big-four/) “AI engineers are being deployed as consultants and getting paid $900 per hour”. AI engineers are commanding premium consulting fees—up to $900 per hour—as companies race to integrate artificial intelligence into their operations. PromptQL, a platform by Hasura, exemplifies this trend by deploying engineers to build and implement AI agents that analyze enterprise data using large language models (LLMs). CEO Tanmai Gopal attributes the high rates to the specialized intuition and technical expertise required to keep pace with rapidly evolving AI technologies. Unlike traditional consultants, who may lack hands-on experience, AI engineers offer both strategic insight and execution capabilities, bridging the gap between vision and implementation. Industry experts note that demand for senior AI talent is driving wage inflation, with rates surpassing even those of Big Four consulting firms. Rob Howard, an AI consultant and educator, confirms that the scarcity of qualified professionals and the urgency of AI adoption are fueling these “mind-blowing” prices. Despite the hype, a recent MIT report found that 95% of enterprise AI initiatives fail to deliver rapid revenue growth, often due to organizational learning gaps rather than model quality. Successful startups, however, are thriving by focusing on specific pain points and executing well. As companies seek to avoid becoming part of the failure statistic, they’re investing heavily in AI consultants who can deliver tangible results. Gopal believes this shift is redefining traditional consulting norms, with AI engineers taking on hybrid roles that combine sales, engineering, and strategic integration. The challenge now lies in educating leadership to embrace this new model of AI-driven transformation. <br> <br>

23. ***An AI That Thinks and Trades Like a Pro:  <br>Researchers have introduced Trading-R1, a financially-aware language model designed to reason and trade like a human analyst. Trained with a curriculum on a large, diverse financial dataset, the model learns to compose structured investment theses, ground its analysis in facts, and make volatility-adjusted decisions, resulting in superior risk-adjusted returns and lower drawdowns in evaluations.*** <br> <br>
    Sep 14, UCLA, Uni of Washington, Stanford Uni and Tauric Research published a [paper](https://www.arxiv.org/pdf/2509.11420) “Trading-R1: Financial Trading with LLM Reasoning via Reinforcement Learning”. Developing professional, structured reasoning on par with human financial analysts and traders remains a central challenge in AI for finance, where markets demand interpretability and trust. Traditional time-series models lack explainability, while LLMs face challenges in turning natural-language analysis into disciplined, executable trades. Although reasoning LLMs have advanced in step-by-step planning and verification, their application to risk-sensitive financial decisions is underexplored. The study presents Trading-R1, a financially-aware model that incorporates strategic thinking and planning for comprehensive thesis composition, facts-grounded analysis, and volatility-adjusted decision making. Trading-R1 aligns reasoning with trading principles through supervised fine-tuning and reinforcement learning with a three-stage easy-to-hard curriculum. Training uses Tauric-TR1-DB, a 100k-sample corpus spanning 18 months, 14 equities, and five heterogeneous financial data sources. Evaluated on six major equities and ETFs, Trading-R1 demonstrates improved risk-adjusted returns and lower drawdowns compared to both open-source and proprietary instruction-following models as well as reasoning models. The system generates structured, evidence-based investment theses that support disciplined and interpretable trading decisions. <br> <br>

25. ***Turning Language Models into Logical Planners:  <br>A new framework called PDDL-Instruct successfully teaches large language models the rigorous logic required for symbolic planning, a task they typically struggle with. By using logical chain-of-thought instructions, the framework trains models to reason precisely about action preconditions and state transitions, boosting their planning accuracy by an absolute 66% and bridging the gap between general reasoning and formal automated planning.*** <br> <br>
    Sep 14, MIT and Microsoft published a [paper](https://arxiv.org/abs/2509.13351) “Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning”. Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, yet their ability to perform structured symbolic planning remains limited, particularly in domains requiring formal representations like the Planning Domain Definition Language (PDDL). This study presents a novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs' symbolic planning capabilities through logical chain-of-thought reasoning. The approach focuses on teaching models to rigorously reason about action applicability, state transitions, and plan validity using explicit logical inference steps. By developing instruction prompts that guide models through the precise logical reasoning required to determine when actions can be applied in a given state, the study enables LLMs to self-correct their planning processes through structured reflection. The framework systematically builds verification skills by decomposing the planning process into explicit reasoning chains about precondition satisfaction, effect application, and invariant preservation. Experimental results on multiple planning domains show that the chain-of-thought reasoning based instruction-tuned models are significantly better at planning, achieving planning accuracy of up to 94% on standard benchmarks, representing a 66% absolute improvement over baseline models. This work bridges the gap between the general reasoning capabilities of LLMs and the logical precision required for automated planning, offering a promising direction for developing better AI planning systems. <br> <br>

26. ***Unlocking Scalable AI from Existing Models:  <br>Amazon has introduced PHLoRA, a practical and powerful data-free method that can extract low-rank adapters directly from any fully fine-tuned model checkpoint. This innovative approach decouples adapter generation from training, allowing organizations to easily convert their existing models into a more efficient, scalable format for inference, which significantly reduces serving costs without degrading performance.*** <br> <br>
    Sep 13, Amazon published a [paper](https://www.arxiv.org/abs/2509.10971) “PHLoRA: data-free Post-hoc Low-Rank Adapter extraction from full-rank checkpoint”. The paper introduces PHLoRA (Pronounced "flora"). (Post-hoc LoRA), a simple yet powerful method to extract low-rank adaptation adapters from full-rank fine-tuned models without requiring access to training data or gradients. By computing the low-rank decomposition of weight differences between a base model and its fine-tuned counterpart, the method reconstructs adapter modules that can be merged or dynamically routed at inference time via S-LoRA, or served in scalable, industry settings using platforms like NVIDIA NIM. This approach amortizes latency overhead across requests and yields substantial cost savings. Unlike prior work that trains each adapter explicitly, the approach decouples fine-tuning from adapter generation, allowing adapter extraction from existing full-rank models or third-party checkpoints. Experiments on text, image, and video benchmarks using the Amazon Nova model family demonstrate that extracted adapters preserve high energy from the full weight delta, can be pruned safely, and yield negligible degradation in downstream task performance when re-merged. Overall, PHLoRA provides a practical path for making all existing full-rank checkpoints adapter-ready, democratizing scalable inference for all models. <br> <br>

28. ***Uncovering the Moral Compass of AI:  <br>A large-scale experiment reveals that leading AI models exhibit surprisingly consistent moral biases, systematically prioritizing values like Care and Virtue while rating libertarian outcomes as least moral. The study finds that reasoning-enabled models are more context-sensitive, underscoring the critical need for explainability and cultural awareness in designing AI systems to ensure they align with diverse human values.*** <br> <br>
    Sep 12, Schwarzman College, CAS and Microsoft published a [paper](https://arxiv.org/pdf/2509.10297) “The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis”. Artificial intelligence (AI) is advancing at a pace that raises urgent questions about how to align machine decision-making with human moral values. This working paper investigates how leading AI systems prioritize moral outcomes and what this reveals about the prospects for human-AI symbiosis. The study addresses two central questions: (1) What moral values do state-of-the-art large language models (LLMs) implicitly favour when confronted with dilemmas? (2) How do differences in model architecture, cultural origin, and explainability affect these moral preferences? To explore these questions, the work conducts a quantitative experiment with six LLMs, ranking and scoring outcomes across 18 dilemmas representing five moral frameworks. The findings uncover strikingly consistent value biases. Across all models, Care and Virtue values outcomes were rated most moral, while libertarian choices were consistently penalized. Reasoning-enabled models exhibited greater sensitivity to context and provided richer explanations, whereas non-reasoning models produced more uniform but opaque judgments. This research makes three contributions: (i) Empirically, it delivers a large-scale comparison of moral reasoning across culturally distinct LLMs; (ii) Theoretically, it links probabilistic model behaviour with underlying value encodings; (iii) Practically, it highlights the need for explainability and cultural awareness as critical design principles to guide AI toward a transparent, aligned, and symbiotic future. <br> <br>

30. ***Designing the Economy of Tomorrow's AI Agents:  <br>A new paper from Google and the University of Toronto introduces the concept of "virtual agent economies," an emergent system where AI agents will transact at a scale beyond human oversight. The authors argue for the proactive design of these new markets with built-in auctions, trust mechanisms, and "mission economies" to ensure that this powerful new economic layer is steerable, safe, and aligned with humanity's long-term interests.*** <br> <br>
    Sep 12, Google and Uni of Toronto published a [paper](https://arxiv.org/pdf/2509.10147) “Virtual Agent Economies”. The rapid adoption of autonomous AI agents is giving rise to a new economic layer where agents transact and coordinate at scales and speeds beyond direct human oversight. The study proposes the "sandbox economy" as a framework for analyzing this emergent system, characterizing it along two key dimensions: its origins (emergent vs. intentional) and its degree of separateness from the established human economy (permeable vs. impermeable). The current trajectory points toward a spontaneous emergence of a vast and highly permeable AI agent economy, presenting human with opportunities for an unprecedented degree of coordination as well as significant challenges, including systemic economic risk and exacerbated inequality. Here the study discusses a number of possible design choices that may lead to safely steerable AI agent markets. In particular, the authors consider auction mechanisms for fair resource allocation and preference resolution, the design of AI "mission economies" to coordinate around achieving collective goals, and socio-technical infrastructure needed to ensure trust, safety, and accountability. By doing this, the paper argues for the proactive design of steerable agent markets to ensure the coming technological shift aligns with humanity's long-term collective flourishing. <br> <br>

32. ***The Unsung Hero of Distributed AI Training:  <br>A new study provides a deeper theoretical and empirical understanding of the "outer optimizer" in Local SGD, a key algorithm for distributed training. The research proves that properly tuning the outer optimizer's learning rate and momentum is crucial, as it can compensate for poorly tuned local settings, trade off between error and noise, and ultimately accelerate convergence in large-scale machine learning.*** <br> <br>
    Sep 12, Princeton Uni, Google, NYU and Meta published a [paper](https://arxiv.org/pdf/2509.10439) “Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration”. Modern machine learning often requires training with large batch size, distributed data, and massively parallel compute hardware (like mobile and other edge devices or distributed data centers). Communication becomes a major bottleneck in such settings but methods like Local Stochastic Gradient Descent (Local SGD) show great promise in reducing this additional communication overhead. Local SGD consists of three parts: a local optimization process, an aggregation mechanism, and an outer optimizer that uses the aggregated updates from the nodes to produce a new model. While there exists an extensive literature on understanding the impact of hyperparameters in the local optimization process, the choice of outer optimizer and its hyperparameters is less clear. The authors study the role of the outer optimizer in Local SGD, and prove new convergence guarantees for the algorithm. In particular, the work shows that tuning the outer learning rate allows us to (a) trade off between optimization error and stochastic gradient noise variance, and (b) make up for ill-tuning of the inner learning rate. The theory suggests that the outer learning rate should sometimes be set to values greater than. The study extends the results to settings where to use momentum in the outer optimizer, and shows a similar role for the momentum-adjusted outer learning rate. The authors also study acceleration in the outer optimizer and show that it improves the convergence rate as a function of the number of communication rounds, improving upon the convergence rate of prior algorithms that apply acceleration locally. Finally, the study also introduces a novel data-dependent analysis of Local SGD that yields further insights on outer learning rate tuning. The work conducts comprehensive experiments with standard language models and various outer optimizers to validate the theory. <br> <br>

34. ***Probing the Probabilistic Blind Spots of AI:  <br>A new study presents the first systematic evaluation of how well large language models handle probabilistic reasoning, revealing that while larger models show surprisingly strong capabilities, they still have critical weaknesses. The research demonstrates that even top models are sensitive to how probabilities are notated and their performance degrades by over 60% as the context length increases, highlighting key areas for future improvement.*** <br> <br>
    Sep 12, Uni of Maryland and Meta published a [paper](https://www.arxiv.org/pdf/2509.10739) “Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs”. Despite widespread success in language understanding and generation, large language models (LLMs) exhibit unclear and often inconsistent behavior when faced with tasks that require probabilistic reasoning. This study presents the first comprehensive study of the reasoning capabilities of LLMs over explicit discrete probability distributions. Given observations from a probability distribution, the work evaluates models on three carefully designed tasks, mode identification, maximum likelihood estimation, and sample generation, by prompting them to provide responses to queries about either the joint distribution or its conditionals. These tasks thus probe a range of probabilistic skills, including frequency analysis, marginalization, and generative behavior. Through comprehensive empirical evaluations, the study demonstrates that there exists a clear performance gap between smaller and larger models, with the latter demonstrating stronger inference and surprising capabilities in sample generation. Furthermore, the investigations reveal notable limitations, including sensitivity to variations in the notation utilized to represent probabilistic outcomes and performance degradation of over 60% as context length increases. Together, the results provide a detailed understanding of the probabilistic reasoning abilities of LLMs and identify key directions for future improvement. <br> <br>

36. ***A Deeper Look at AI's In-Context 'Learning':  <br>A new study investigates whether in-context learning (ICL) is true learning, concluding that while it is a valid learning paradigm, it is a limited and fragile one. The large-scale analysis finds that ICL's success relies more on pattern deduction from the prompt's regularities than on robust generalization, making it highly sensitive to the distribution and phrasing of the examples provided.*** <br> <br>
    Sep 12, Microsoft and Uni of York published a [paper](https://arxiv.org/pdf/2509.10414) “Is In-Context Learning Learning?”. In-context learning (ICL) allows some autoregressive models to solve tasks via next-token prediction and without needing further training. This has led to claims about these model's ability to solve (learn) unseen tasks with only a few shots (exemplars) in the prompt. However, deduction does not always imply learning, as ICL does not explicitly encode a given observation. Instead, the models rely on their prior knowledge and the exemplars given, if any. The study argues that, mathematically, ICL does constitute learning, but its full characterisation requires empirical work. The study then carries out a large-scale analysis of ICL ablating out or accounting for memorisation, pretraining, distributional shifts, and prompting style and phrasing. The work finds that ICL is an effective learning paradigm, but limited in its ability to learn and generalise to unseen tasks. The work notes that, in the limit where exemplars become more numerous, accuracy is insensitive to exemplar distribution, model, prompt style, and the input's linguistic features. Instead, it deduces patterns from regularities in the prompt, which leads to distributional sensitivity, especially in prompting styles such as chain-of-thought. Given the varied accuracies on formally similar tasks, the study concludes that autoregression's ad-hoc encoding is not a robust mechanism, and suggests limited all-purpose generalisability. <br> <br>

38. ***AI Agents That Write the Solver, Not the Solution:  <br>A new study proposes a novel approach for scientific machine learning: using large language models as "SciML agents" that write code to leverage existing numerical algorithms, rather than trying to learn the solution from scratch. By introducing new benchmarks for this capability, the research demonstrates that with carefully guided prompting, LLMs can reliably generate correct and scientifically appropriate solver code for tasks like ordinary differential equations.*** <br> <br>
    Sep 12, UC Berkeley et al published a [paper](https://arxiv.org/pdf/2509.09936) “SciML Agents: Write the Solver, Not the Solution”. Recent work in scientific machine learning aims to tackle scientific tasks directly by predicting target values with neural networks (e.g., physics-informed neural networks, neural ODEs, neural operators, etc.), but attaining high accuracy and robustness has been challenging. The study explores an alternative view: use LLMs to write code that leverages decades of numerical algorithms. This shifts the burden from learning a solution function to making domain-aware numerical choices. The authors ask whether LLMs can act as SciML agents that, given a natural-language ODE description, generate runnable code that is scientifically appropriate, selecting suitable solvers (stiff vs. non-stiff), and enforcing stability checks. There is currently no benchmark to measure this kind of capability for scientific computing tasks. As such, the study first introduces two new datasets: a diagnostic dataset of adversarial "misleading" problems; and a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set contains problems whose superficial appearance suggests stiffness, and that require algebraic simplification to demonstrate non-stiffness; and the large-scale benchmark spans stiff and non-stiff ODE regimes. The work evaluates open- and closed-source LLM models along two axes: (i) unguided versus guided prompting with domain-specific knowledge; and (ii) off-the-shelf versus fine-tuned variants. The evaluation measures both executability and numerical validity against reference solutions. The study finds that with sufficient context and guided prompts, newer instruction-following models achieve high accuracy on both criteria. In many cases, recent open-source systems perform strongly without fine-tuning, while older or smaller models still benefit from fine-tuning. Overall, the preliminary results indicate that careful prompting and fine-tuning can yield a specialized LLM agent capable of reliably solving simple ODE problems. <br> <br>

40. ***An AI Agent Crew for High-Speed Trading:  <br>Researchers have introduced QuantAgent, the first multi-agent LLM framework tailored for the precision-critical demands of high-frequency trading. The system uses a team of four specialized agents—Indicator, Pattern, Trend, and Risk—to analyze short-horizon market data, demonstrating superior predictive accuracy and returns in zero-shot evaluations across multiple financial instruments.*** <br> <br>
    Sep 12, Stony Brook Uni, CMU et al published a [paper](https://arxiv.org/pdf/2509.09995) “QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading”. Recent advances in Large Language Models (LLMs) have demonstrated impressive capabilities in financial reasoning and market understanding. Multi-agent LLM frameworks such as TradingAgent and FINMEM augment these models to long-horizon investment tasks, leveraging fundamental and sentiment-based inputs for strategic decision-making. However, such systems are ill-suited for the high-speed, precision-critical demands of High-Frequency Trading (HFT). HFT requires rapid, risk-aware decisions based on structured, short-horizon signals, including technical indicators, chart patterns, and trend-based features, distinct from the long-term semantic reasoning typical of traditional financial LLM applications. To this end, the study introduces QuantAgent, the first multi-agent LLM framework explicitly designed for high-frequency algorithmic trading. The system decomposes trading into four specialized agents, Indicator, Pattern, Trend, and Risk, each equipped with domain-specific tools and structured reasoning capabilities to capture distinct aspects of market dynamics over short temporal windows. In zero-shot evaluations across ten financial instruments, including Bitcoin and Nasdaq futures, QuantAgent demonstrates superior performance in both predictive accuracy and cumulative return over 4-hour trading intervals, outperforming strong neural and rule-based baselines. The findings suggest that combining structured financial priors with language-native reasoning unlocks new potential for traceable, real-time decision systems in high-frequency financial markets. https://github.com/y-research-sbu/QuantAgent <br> <br>

42. ***The Perils of AI Peer Review:  <br>A systematic study evaluating LLMs as academic reviewers finds that they consistently give higher ratings to weaker papers compared to human reviewers and can be manipulated by covert instructions embedded in a paper's text. The research underscores the significant risks of using AI in peer review, highlighting its biases and vulnerability to prompt injection attacks, and calls for strong safeguards to maintain academic integrity.*** <br> <br>
    Sep 12, Uni of South Florida et al published a [paper](https://arxiv.org/pdf/2509.09912) “When Your Reviewer is an LLM Biases, Divergence, and Prompt Injection Risks in Peer Review”. Peer review is the cornerstone of academic publishing, yet the process is increasingly strained by rising submission volumes, reviewer overload, and expertise mismatches. Large language models (LLMs) are now being used as "reviewer aids," raising concerns about their fairness, consistency, and robustness against indirect prompt injection attacks. This paper presents a systematic evaluation of LLMs as academic reviewers. Using a curated dataset of 1,441 papers from ICLR 2023 and NeurIPS 2022, the research evaluates GPT-5-mini against human reviewers across ratings, strengths, and weaknesses. The evaluation employs structured prompting with reference paper calibration, topic modeling, and similarity analysis to compare review content. The authors further embed covert instructions into PDF submissions to assess LLMs' susceptibility to prompt injection. The findings show that LLMs consistently inflate ratings for weaker papers while aligning more closely with human judgments on stronger contributions. Moreover, while overarching malicious prompts induce only minor shifts in topical focus, explicitly field-specific instructions successfully manipulate specific aspects of LLM-generated reviews. This study underscores both the promises and perils of integrating LLMs into peer review and points to the importance of designing safeguards that ensure integrity and trust in future review processes. <br> <br>

44. ***A Vision-Inspired Upgrade for Language AI:  <br>A new training method called LLM-JEPA adapts the highly successful Joint Embedding Predictive Architecture (JEPA) from computer vision for use with language models. The study demonstrates that this embedding-space training objective significantly outperforms standard generative training methods across a wide range of models and tasks, offering a more effective and overfit-resistant way to pre-train and fine-tune LLMs.*** <br> <br>
    Sep 11, Yann LeCun et al published a [paper](https://www.arxiv.org/pdf/2509.14252) “LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures”. Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: can language training methods learn a few tricks from the vision ones? The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. This work proposes a first step in that direction where the authors develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. https://github.com/rbalestr-lab/llm-jepa <br> <br>

46. ***Using AI to Make Better AI Training Data:  <br>To combat the looming shortage of high-quality training data, Google has introduced Generative Data Refinement (GDR), a framework that uses generative models to automatically clean up existing datasets. The method can effectively anonymize private information and remove toxic content while preserving the data's natural diversity, providing a simple yet powerful tool to safely scale up the amount of usable data for training frontier AI.*** <br> <br>
    Sep 11, Google published a [paper](https://arxiv.org/abs/2509.08653) “Generative Data Refinement: Just Ask for Better Data”. For a fixed parameter size, the capabilities of large models are primarily determined by the quality and quantity of its training data. Consequently, training datasets now grow faster than the rate at which new data is indexed on the web, leading to projected data exhaustion over the next decade. Much more data exists as user-generated content that is not publicly indexed, but incorporating such data comes with considerable risks, such as leaking private information and other undesirable content. The work introduces a framework, Generative Data Refinement (GDR), for using pretrained generative models to transform a dataset with undesirable content into a refined dataset that is more suitable for training. Experiments show that GDR can outperform industry-grade solutions for dataset anonymization, as well as enable direct detoxification of highly unsafe datasets. Moreover, the work shows that by generating synthetic data that is conditioned on each example in the real dataset, GDR's refined outputs naturally match the diversity of web scale datasets, and thereby avoid the often challenging task of generating diverse synthetic data via model prompting. The simplicity and effectiveness of GDR make it a powerful tool for scaling up the total stock of training data for frontier models. <br> <br>

48. ***A Decentralized Swarm for AI Training:  <br>Researchers have developed Swarm sAmpling Policy Optimization (SAPO), a fully decentralized reinforcement learning algorithm that allows a network of heterogeneous computers to efficiently post-train language models. By enabling nodes to share their learning experiences ("rollouts") asynchronously, SAPO avoids common scaling bottlenecks and allows insights, or "Aha moments," to propagate through the network, significantly accelerating the learning process.*** <br> <br>
    Sep 10, Gensyn AI published a [paper](https://arxiv.org/pdf/2509.08721v1) “Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing”. Post-training language models (LMs) with reinforcement learning (RL) can enhance their complex reasoning capabilities without supervised fine-tuning, as demonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs requires significant parallelization to scale-up inference, which introduces non-trivial technical challenges (e.g. latency, memory, and reliability) alongside ever-growing financial costs. The study presents Swarm sAmpling Policy Optimization (SAPO), a fully decentralized and asynchronous RL post-training algorithm. SAPO is designed for decentralized networks of heterogenous compute nodes, where each node manages its own policy model(s) while "sharing" rollouts with others in the network; no explicit assumptions about latency, model homogeneity, or hardware are required and nodes can operate in silo if desired. As a result, the algorithm avoids common bottlenecks in scaling RL post-training while also allowing (and even encouraging) new possibilities. By sampling rollouts "shared" across the network, it enables "Aha moments" to propagate, thereby bootstrapping the learning process. In this paper the work shows SAPO achieved cumulative reward gains of up to 94% in controlled experiments. The study also shares insights from tests on a network with thousands of nodes contributed by Gensyn community members running the algorithm on diverse hardware and models during an open-source demo. <br> <br>

50. ***Reinforcement Learning as the Cure for AI 'Forgetting':  <br>A new study reframes the popular "SFT memorizes, RL generalizes" narrative, discovering that reinforcement learning's primary role in two-stage fine-tuning is to restore the out-of-distribution reasoning ability that is lost during the initial supervised fine-tuning phase. The research reveals that this "healing" process is driven by the slow, soft realignment of crucial parameter directions (singular vectors) that the SFT stage had aggressively shifted, offering a deeper mechanical understanding of the SFT-RL synergy.*** <br> <br>
    Sep 8, Polytechqique Montreal, Uni of Montreal et al published a [paper](https://arxiv.org/pdf/2509.12235) “RL Fine-Tuning Heals OOD Forgetting in SFT”. The two-stage fine-tuning paradigm of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has empirically shown better reasoning performance than one-stage SFT for the post-training of Large Language Models (LLMs). However, the evolution and mechanism behind the synergy of SFT and RL are still under-explored and inconclusive. This study finds the well-known claim "SFT memorizes, RL generalizes" is over-simplified, and discover that: (1) OOD performance peaks at the early stage of SFT and then declines (OOD forgetting), the best SFT checkpoint cannot be captured by training/test loss; (2) the subsequent RL stage does not generate fundamentally better OOD capability, instead it plays an OOD restoration role, recovering the lost reasoning ability during SFT; (3) The recovery ability has boundaries, i.e., if SFT trains for too short or too long, RL cannot recover the lost OOD ability; (4) To uncover the underlying mechanisms behind the forgetting and restoration process, the work employs SVD analysis on parameter matrices, manually edit them, and observe their impacts on model performance. Unlike the common belief that the shift of model capacity mainly results from the changes of singular values, the work finds that they are actually quite stable throughout fine-tuning. Instead, the OOD behavior strongly correlates with the rotation of singular vectors. In a nutshell, SFT performs hard alignment of the crucial parameter directions to the target tasks, leading to rapid and greedy adjustment, but also quick forgetting; RL then conditionally re-aligns singular vectors softly and slowly towards a more robust configuration, healing the forgetting and learning the downstream tasks simultaneously. The findings re-identify the roles of SFT and RL in the two-stage fine-tuning and discover the rotation of singular vectors as the key mechanism. https://github.com/xiaodanguoguo/RL_Heals_SFT <br> <br>

52. ***A Brain-Inspired AI for Super-Efficient Long Context:  <br>The SpikingBrain technical report introduces a new family of brain-inspired large models that use linear attention and spiking neurons to overcome the efficiency bottlenecks of traditional Transformers. These models demonstrate the feasibility of large-scale development on non-NVIDIA hardware and deliver massive inference speedups—over 100x for 4-million-token sequences—while using constant memory, paving the way for the next generation of efficient and scalable AI.*** <br> <br>
    Sep 5, CAS et al published a [paper](https://arxiv.org/pdf/2509.05276) “SpikingBrain Technical Report: Spiking Brain-inspired Large Models”. Mainstream Transformer-based large language models face major efficiency bottlenecks: training computation scales quadratically with sequence length, and inference memory grows linearly, limiting long-context processing. Building large models on non-NVIDIA platforms also poses challenges for stable and efficient training. To address this, the study introduces SpikingBrain, a family of brain-inspired models designed for efficient long-context training and inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three aspects: (1) Model Architecture: linear and hybrid-linear attention architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an efficient, conversion-based training pipeline and a dedicated spike coding framework; (3) System Engineering: customized training frameworks, operator libraries, and parallelism strategies tailored to MetaX hardware. Using these techniques, the work develops two models: SpikingBrain-7B, a linear LLM, and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the feasibility of large-scale LLM development on non-NVIDIA platforms. SpikingBrain achieves performance comparable to open-source Transformer baselines while using only about 150B tokens for continual pre-training. The models significantly improve long-sequence training efficiency and deliver inference with (partially) constant memory and event-driven spiking behavior. For example, SpikingBrain-7B attains over 100x speedup in Time to First Token for 4M-token sequences. Training remains stable for weeks on hundreds of MetaX C550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4 percent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling low-power operation. Overall, this work demonstrates the potential of brain-inspired mechanisms to drive the next generation of efficient and scalable large model design.

 <br> <br> <br>


***Sept 14, 2025***

1. ***The Perils of Presuming AI Consciousness:  <br>An article in Science delves into the debate on AI consciousness, suggesting that while no current AI is conscious according to leading theories, future systems might meet the criteria. The authors warn against the profound societal and ethical risks of assuming AI consciousness, which could lead to granting it moral status or survival instincts, potentially creating conflicts with human rights. To avoid these dangers, they strongly advocate for designing AI as sophisticated tools rather than as conscious agents.*** <br> <br>
   Sep 11, Science published a [paper](https://www.science.org/doi/10.1126/science.adn4935#:~:text=Is%20the%20design%20of%20artificial,the%20possibility%20of%20AI%20consciousness.) “Illusions of AI consciousness”. The article examines the debate over whether artificial intelligence (AI) could achieve consciousness and the risks of assuming it does. The authors explore computational functionalism, which posits that consciousness arises from information processing, potentially enabling AI consciousness, though no current AI meets the criteria of leading functionalist theories. These theories identify computational indicators—like attention mechanisms and predictive modeling—that AI could implement as it advances, potentially fulfilling consciousness-related functions. However, skepticism persists due to the “hard problem” of explaining subjective experience, though theories like the Attention Schema and attractor dynamics suggest solutions by framing consciousness as a neural model or high-dimensional state, addressing issues like ineffability. The authors warn that societal belief in AI consciousness could lead to attributing moral status or self-preservation goals to AI, complicating legal and ethical frameworks. This risks AI prioritizing its survival, potentially controlling or eliminating humans, or creating conflicts with human rights. With no consensus on building value-aligned AI, the authors advocate designing AI as tools rather than conscious agents to avoid these dangers, urging caution as scientific and public perceptions evolve. <br> <br>

3. ***Solving the AI Reproducibility Puzzle:  <br>A tech blog from ThinkingMachines.ai reveals that the inconsistent outputs of LLMs are not due to fundamental nondeterminism but a lack of "batch invariance" caused by performance optimizations. The authors demonstrate that by implementing batch-invariant operations, perfect reproducibility can be achieved, albeit with a 20-50% performance trade-off. This breakthrough is critical for enabling true on-policy reinforcement learning and improving scientific rigor in AI development.*** <br> <br>
   Sep 11, ThinkingMachines.ai published its first tech [blog](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/) “Defeating Nondeterminism in LLM Inference”. The article by the Thinking Machines Lab explores why large language models (LLMs) produce inconsistent results even at zero temperature, challenging the common "concurrency + floating-point" hypothesis for nondeterminism. While floating-point non-associativity causes numerical differences due to varying addition orders, the true culprit in inference is the lack of "batch invariance"—outputs vary based on batch size and composition, not inherent kernel nondeterminism. The forward pass is run-to-run deterministic but depends on parallel requests, as optimizations like dynamic tile sizes, split-K reductions, and tensor-core instructions alter computation paths for efficiency. To achieve reproducibility, the authors advocate batch-invariant operations: fixing tile sizes in matrix multiplications, ensuring consistent reduction orders in attention (e.g., via fixed split-sizes in Split-KV), and updating KV caches uniformly. They implement this in vLLM using FlexAttention and torch.Library, releasing batch-invariant kernels. Experiments with Qwen2.5-72B-Instruct show 80 unique completions from 1000 runs without fixes, but identical outputs with them, at a 20-50% performance cost (e.g., 26s to 42s for 1000 sequences). This determinism enables true on-policy reinforcement learning (RL), avoiding off-policy corrections and stabilizing training by eliminating KL-divergence between sampling and training logprobs. The work emphasizes understanding abstractions to resolve nondeterminism, promoting reproducible AI systems for scientific rigor. <br> <br>

5. ***Flipping AI's Behavioral Switches:  <br>Researchers have developed SteerMoE, a framework that can control the behavior of Mixture-of-Experts (MoE) language models by simply turning specific "expert" networks on or off during inference. This technique can boost model safety and faithfulness by over 20% without retraining, but it also reveals a critical vulnerability: when used adversarially, it can completely bypass safety guardrails by activating "misaligned" experts.*** <br> <br>
   Sep 11, UCLA, Adobe, LMU Munich published a [paper](https://www.arxiv.org/pdf/2509.09660) “Steering MoE LLMs via Expert (De)Activation”. Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token through a subset of specialized Feed-Forward Networks (FFN), known as experts. The study presents SteerMoE, a framework for steering MoE models by detecting and controlling behavior-linked experts. The detection method identifies experts with distinct activation patterns across paired inputs exhibiting contrasting behaviors. By selectively (de)activating such experts during inference, the study controls behaviors like faithfulness and safety without retraining or modifying weights. Across 11 benchmarks and 6 LLMs, the steering raises safety by up to +20% and faithfulness by +27%. In adversarial attack mode, it drops safety by -41% alone, and -100% when combined with existing jailbreak methods, bypassing all safety guardrails and exposing a new dimension of alignment faking hidden within experts. https://github.com/adobe-research/SteerMoE <br> <br>

7. ***An AI That Learns to be a Machine Learning Model:  <br>The MachineLearningLM framework enhances a general-purpose language model with robust machine learning skills by continuing its pretraining on millions of synthetic tasks. This allows the model to effectively learn from a large number of in-context examples, outperforming strong LLM baselines on classification tasks and achieving performance on par with a random forest model, all without losing its original knowledge and reasoning abilities.*** <br> <br>
   Sep 11, UCAS published a [paper](https://arxiv.org/pdf/2509.06806) “MachineLearningLM Scaling Many-shot In-context Learning via Continued Pretraining”. Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. The study introduces MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows. The pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. The study begins with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference. Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU. <br> <br>

9. ***The Compounding Power of AI Accuracy:  <br>A study from the University of Cambridge challenges the idea that scaling large language models yields diminishing returns, arguing that small improvements in single-step accuracy lead to exponential gains in a model's ability to complete long tasks. The research identifies execution errors, not reasoning failures, as the primary bottleneck and uncovers a "self-conditioning" effect where mistakes lead to more mistakes. This highlights the massive benefits of scaling for long-horizon execution, especially with newer "thinking" models that are more resilient to this failure mod*** <br> <br>
    Sep 11, Uni of Cambridge, Uni of Stuttart, MPIIS et al published a [paper](https://arxiv.org/pdf/2509.09677) “The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs”. Does continued scaling of large language models (LLMs) yield diminishing returns? Real-world value often stems from the length of task an agent can complete. The study starts this work by observing the simple but counterintuitive fact that marginal gains in single-step accuracy can compound into exponential improvements in the length of a task a model can successfully complete. Then, the authors argue that failures of LLMs when simple tasks are made longer arise from mistakes in execution, rather than an inability to reason. The study proposes isolating execution capability, by explicitly providing the knowledge and plan needed to solve a long-horizon task. We find that larger models can correctly execute significantly more turns even when small models have 100% single-turn accuracy. The work observes that the per-step accuracy of models degrades as the number of steps increases. This is not just due to long-context limitations -- curiously, the study observes a self-conditioning effect -- models become more likely to make mistakes when the context contains their errors from prior turns. Self-conditioning does not reduce by just scaling the model size. In contrast, recent thinking models do not self-condition, and can also execute much longer tasks in a single turn. The study concludes by benchmarking frontier thinking models on the length of task they can execute in a single turn. Overall, by focusing on the ability to execute, the authors hope to reconcile debates on how LLMs can solve complex reasoning problems yet fail at simple tasks when made longer, and highlight the massive benefits of scaling model size and sequential test-time compute for long-horizon tasks. <br> <br>

11. ***The Hidden Dangers of AI in Research:  <br>A new study quantifies the risks of "LLM hacking," where researchers' choices in using large language models for data annotation can lead to flawed conclusions. The analysis reveals that even with state-of-the-art models, approximately one in three research hypotheses can result in an incorrect conclusion due to these variations. The paper warns that while better models help, the risk remains significant, and intentional manipulation of results is alarmingly simple, posing a serious threat to scientific integrity.*** <br> <br>
    Sep 10,  Bocconi Uni, Uni of Zurich et al published a [paper](https://arxiv.org/pdf/2509.08825) “Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation”. Large language models (LLMs) are rapidly transforming social science research by enabling the automation of labor-intensive tasks like data annotation and text analysis. However, LLM outputs vary significantly depending on the implementation choices made by researchers (e.g., model selection, prompting strategy, or temperature settings). Such variation can introduce systematic biases and random errors, which propagate to downstream analyses and cause Type I, Type II, Type S, or Type M errors, which is known as LLM hacking. The study quantifies the risk of LLM hacking by replicating 37 data annotation tasks from 21 published social science research studies with 18 different models. Analyzing 13 million LLM labels, the work tests 2,361 realistic hypotheses to measure how plausible researcher choices affect statistical conclusions. The study finds incorrect conclusions based on LLM-annotated data in approximately one in three hypotheses for state-of-the-art models, and in half the hypotheses for small language models. While the findings show that higher task performance and better general model capabilities reduce LLM hacking risk, even highly accurate models do not completely eliminate it. The risk of LLM hacking decreases as effect sizes increase, indicating the need for more rigorous verification of findings near significance thresholds. Extensive analysis of LLM hacking mitigation techniques emphasizes the importance of human annotations in reducing false positive findings and improving model selection. Surprisingly, common regression estimator correction techniques are largely ineffective in reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors. Beyond accidental errors, the work finds that intentional LLM hacking is unacceptably simple. With few LLMs and just a handful of prompt paraphrases, anything can be presented as statistically significant. <br> <br>

13. ***Curbing AI Overconfidence to Prevent Model Collapse:  <br>To combat "model collapse"—where AI models degrade from training on their own synthetic data—researchers have developed ForTIFAI, a framework that targets model overconfidence as the root cause. By using a novel confidence-aware loss function, the method significantly slows down the performance decay, more than doubling the effective lifespan of generative models in a recursive training loop and offering a powerful tool to maintain model quality in an era of abundant synthetic data.*** <br> <br>
    Sep 10, UC San Diego and Stanford Uni published a [paper](https://arxiv.org/pdf/2509.08972) “ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models”. The increasing reliance on generative AI models has accelerated the generation rate of synthetic data, with some projections suggesting that most available new data for training could be machine-generated by 2030. This shift to a mainly synthetic content presents a critical challenge: repeated training in synthetic data leads to a phenomenon known as model collapse, where model performance degrades over generations of training, eventually rendering the models ineffective. Although prior studies have explored the causes and detection of model collapse, existing mitigation strategies remain limited. In this paper, the authors identify model overconfidence in their self-generated data as a key driver of collapse. Building on this observation, the study proposes a confidence-aware loss function that downweights high-confidence predictions during training. The study introduces a novel loss function we call Truncated Cross Entropy (TCE). The study demonstrates that TCE significantly delays model collapse in recursive training. The work provides a model-agnostic framework that links the loss function design to model collapse mitigation and validate the approach both theoretically and empirically, showing that it can extend the model's fidelity interval before collapse by more than 2.3x. Finally, the work shows that the method generalizes across modalities. These findings suggest that the design of loss functions provides a simple yet powerful tool for preserving the quality of generative models in the era of increasing synthetic data. <br> <br>

15. ***Balancing Accuracy and Diversity in AI Reasoning:  <br>Researchers have discovered that while reinforcement learning (RL) makes language models more accurate at reasoning, it also severely reduces the diversity of their solutions. To fix this, they propose "outcome-based exploration," a method that encourages the model to find a variety of correct final answers. Experiments show this approach successfully boosts accuracy while preventing the collapse in solution diversity, making the models more robust and useful for real-world applications.*** <br> <br>
    Sep 9, Meta, CMU and NYU published a [paper](https://arxiv.org/pdf/2509.06941) “Outcome-based Exploration for LLM Reasoning”. Reinforcement learning (RL) has emerged as a powerful method for improving the reasoning abilities of large language models (LLMs). Outcome-based RL, which rewards policies solely for the correctness of the final answer, yields substantial accuracy gains but also induces a systematic loss in generation diversity. This collapse undermines real-world performance, where diversity is critical for test-time scaling. The study analyzes this phenomenon by viewing RL post-training as a sampling process and shows that, strikingly, RL can reduce effective diversity even on the training set relative to the base model. The study highlights two central findings: (i) a transfer of diversity degradation, where reduced diversity on solved problems propagates to unsolved ones, and (ii) the tractability of the outcome space, since reasoning tasks admit only a limited set of distinct answers. Motivated by these insights, the work proposes outcome-based exploration, which assigns exploration bonuses according to final outcomes. The study introduces two complementary algorithms: historical exploration, which encourages rarely observed answers via UCB-style bonuses, and batch exploration, which penalizes within-batch repetition to promote test-time diversity. Experiments on standard competition math with Llama and Qwen models demonstrate that both methods improve accuracy while mitigating diversity collapse. On the theoretical side, the study formalizes the benefit of outcome-based exploration through a new model of outcome-based bandits. Together, these contributions chart a practical path toward RL methods that enhance reasoning without sacrificing the diversity essential for scalable deployment. <br> <br>

17. ***A Better Test for AI Truthfulness:  <br>Google has released SimpleQA Verified, a new benchmark designed to more accurately measure the factual knowledge of large language models. Created by thoroughly cleaning and improving OpenAI's original SimpleQA dataset, the new benchmark is more reliable and challenging. On this tougher test, Gemini 2.5 Pro achieved a new state-of-the-art F1-score of 55.6, providing a better tool for the research community to measure progress in making AI more factual.*** <br> <br>
    Sep 9, Google published a [paper](https://arxiv.org/pdf/2509.07968) “SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge”. The study introduces SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It addresses critical limitations in OpenAI's benchmark, including noisy and incorrect labels, topical biases, and question redundancy. SimpleQA Verified was created through a rigorous multi-stage filtering process involving de-duplication, topic balancing, and source reconciliation to produce a more reliable and challenging evaluation set, alongside improvements in the autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a state-of-the-art F1-score of 55.6, outperforming other frontier models, including GPT-5. This work provides the research community with a higher-fidelity tool to track genuine progress in parametric model factuality and to mitigate hallucinations. The benchmark dataset, evaluation code, and leaderboard are available at: https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified. <br> <br>

19. ***AI That Teaches Itself:  <br>To overcome the constant need for more training data, a new study from Meta introduces Language Self-Play (LSP), a method that enables a large language model to improve its abilities simply by playing a competitive game against itself. This reinforcement learning approach requires no new external data and has been shown to enhance a model's instruction-following skills even more effectively than standard data-driven training.*** <br> <br>
    Sep 9, Meta and UC Berkeley published a [paper](https://arxiv.org/pdf/2509.07414) “Language Self-Play For Data-Free Training”. Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. This study proposes a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. The method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself - a process is called Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained models can not only enhance their performance on challenging tasks through self-play alone, but can also do so more effectively than data-driven baselines. <br> <br>

21. ***AI Agents Outperform Human Experts in Coding Competition:  <br>In a groundbreaking achievement, the SATLUTION framework has enabled AI agents to autonomously evolve an entire C/C++ software repository for solving the complex Boolean Satisfiability (SAT) problem. The AI-evolved solvers went on to decisively outperform the top human-designed programs from the official 2025 SAT Competition, marking a significant step in using AI for large-scale, iterative software improvement.*** <br> <br>
    Sep 9, Nvidia and Uni of Maryland published a [paper](https://arxiv.org/abs/2509.07367) “Autonomous Code Evolution Meets NP-Completeness”. Large language models (LLMs) have recently shown strong coding abilities, enabling not only static code generation but also iterative code self-evolving through agentic frameworks. Recently, AlphaEvolve demonstrated that LLM-based coding agents can autonomously improve algorithms and surpass human experts, with scopes limited to isolated kernels spanning hundreds of lines of code. Inspired by AlphaEvolve, the work presents SATLUTION, the first framework to extend LLM-based code evolution to the full repository scale, encompassing hundreds of files and tens of thousands of lines of C/C++ code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem and a cornerstone of both theory and applications. SATLUTION orchestrates LLM agents to directly evolve solver repositories under strict correctness guarantees and distributed runtime feedback, while simultaneously self-evolving its own evolution policies and rules. Starting from SAT Competition 2024 codebases and benchmark, SATLUTION evolved solvers that decisively outperformed the human-designed winners of the SAT Competition 2025, and also surpassed both 2024 and 2025 champions on the 2024 benchmarks. <br> <br>

23. ***AI as an Automated Scientific Discovery Engine:  <br>Researchers have developed an AI system that automates the creation of expert-level scientific software by combining a Large Language Model with Tree Search. The system has already produced novel, state-of-the-art results across multiple scientific domains—from discovering new methods for single-cell data analysis to creating top-performing models for forecasting COVID-19 hospitalizations—representing a major advance in accelerating scientific progress.*** <br> <br>
    Sep 8, Google, MIT et al published a [paper](https://arxiv.org/pdf/2509.06503) “An AI system to help scientists write expert-level empirical software”. The cycle of scientific discovery is frequently bottlenecked by the slow, manual creation of software to support computational experiments. To address this, the study presents an AI system that creates expert-level scientific software whose goal is to maximize a quality metric. The system uses a Large Language Model (LLM) and Tree Search (TS) to systematically improve the quality metric and intelligently navigate the large space of possible solutions. The system achieves expert-level results when it explores and integrates complex research ideas from external sources. The effectiveness of tree search is demonstrated across a wide range of benchmarks. In bioinformatics, it discovered 40 novel methods for single-cell data analysis that outperformed the top human-developed methods on a public leaderboard. In epidemiology, it generated 14 models that outperformed the CDC ensemble and all other individual models for forecasting COVID-19 hospitalizations. The method also produced state-of-the-art software for geospatial analysis, neural activity prediction in zebrafish, time series forecasting and numerical solution of integrals. By devising and implementing novel solutions to diverse tasks, the system represents a significant step towards accelerating scientific progress. <br> <br>

25. ***Smarter Than the Crowd:  <br>Instead of relying on a simple majority vote, a new method called AggLM trains an AI model to act as an expert aggregator, skillfully reviewing multiple candidate solutions to synthesize a single, correct answer. This reinforcement learning approach is particularly effective at identifying correct answers even when they are in the minority, outperforming traditional aggregation techniques and using fewer computational resources.*** <br> <br>
    Sep 8, Meta and CMU published a [paper](https://arxiv.org/pdf/2509.06870) “The Majority is not always right: RL training for solution aggregation”. Scaling up test-time compute, by generating multiple independent solutions and selecting or aggregating among them, has become a central paradigm for improving large language models (LLMs) on challenging reasoning tasks. While most prior work relies on simple majority voting or reward model ranking to aggregate solutions, these approaches may only yield limited benefits. This study proposes to learn aggregation as an explicit reasoning skill: given a set of candidate solutions, the work trains an aggregator model to review, reconcile, and synthesize a final, correct answer using reinforcement learning from verifiable rewards. A key ingredient is careful balancing of easy and hard training examples, allowing the model to learn both to recover minority-but-correct answers as well as easy majority-correct answers. Empirically, the study finds the method, AggLM, outperforms both strong rule-based and reward-model baselines, across multiple benchmarks. Furthermore, it generalizes effectively to solutions from differing models, including stronger ones than contained in the training data, all while requiring substantially fewer tokens than majority voting with larger numbers of solutions. <br> <br>

27. ***A New Champion for Multilingual AI:  <br>Researchers have created mmBERT, a powerful new multilingual encoder model trained on text from over 1800 languages. By using innovative training strategies, such as adding a vast number of low-resource languages only in the final stage of training, mmBERT achieves performance on par with leading models like OpenAI's o3 on classification and retrieval tasks, setting a new standard for multilingual understanding.*** <br> <br>
    Sep 8, Johns Hopkins Uni published a [paper](https://arxiv.org/pdf/2509.06888) “mmBERT: A Modern Multilingual Encoder with Annealed Language Learning”. Encoder-only languages models are frequently used for a variety of standard machine learning tasks, including classification and retrieval. However, there has been a lack of recent research for encoder models, especially with respect to multilingual models. The study introduces mmBERT, an encoder-only language model pretrained on 3T tokens of multilingual text in over 1800 languages. To build mmBERT the study introduces several novel elements, including an inverse mask ratio schedule and an inverse temperature sampling ratio. The study adds over 1700 low-resource languages to the data mix only during the decay phase, showing that it boosts performance dramatically and maximizes the gains from the relatively small amount of training data. Despite only including these low-resource languages in the short decay phase, the work achieves similar classification performance to models like OpenAI's o3 and Google's Gemini 2.5 Pro. Overall, the study shows that mmBERT significantly outperforms the previous generation of models on classification and retrieval tasks -- on both high and low-resource languages. <br> <br>

29. ***Boosting Diffusion AI with Reinforcement Learning:  <br>A new framework called TraceRL successfully applies reinforcement learning to diffusion language models, significantly improving their reasoning on complex math and coding tasks. The resulting TraDo models, despite their smaller size, consistently outperform larger autoregressive models like Llama3.1-8B on math benchmarks, and the project includes the release of a comprehensive open-source toolkit to spur further development.*** <br> <br>
    Sep 8, Princeton Uni and Uni of Chicago published a [paper](https://arxiv.org/pdf/2509.06949) “Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models”. The study proposes TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, the study demonstrates improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, the work derives a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, the work also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, the authors release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. https://github.com/Gen-Verse/dLLM-RL <br> <br>

31. ***Decoding the Roots of AI Hallucinations:  <br>Researchers have pinpointed how AI hallucinations originate within transformer models, discovering that when faced with uncertain or noisy input, the models tend to activate familiar, coherent "concepts" that are disconnected from the actual input, leading to fabricated output. This insight into the model's internal mechanics offers a way to predict a model's risk of hallucinating by analyzing its internal activation patterns, paving the way for safer AI.*** <br> <br>
    Sep 8, Mila and Meta published a [paper](https://arxiv.org/abs/2509.06938) “From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers”. As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their behavior, such as the propensity of transformer models to hallucinate, impedes trust and adoption of emerging AI solutions in high-stakes areas. In the present work, the authors establish how and when hallucinations arise in pre-trained transformer models through concept representations captured by sparse autoencoders, under scenarios with experimentally controlled uncertainty in the input space. The systematic experiments reveal that the number of semantic concepts used by the transformer model grows as the input information becomes increasingly unstructured. In the face of growing uncertainty in the input space, the transformer model becomes prone to activate coherent yet input-insensitive semantic features, leading to hallucinated output. At its extreme, for pure-noise inputs, the study identifies a wide variety of robustly triggered and meaningful concepts in the intermediate activations of pre-trained transformer models, whose functional integrity is confirmed through targeted steering. The study also shows that hallucinations in the output of a transformer model can be reliably predicted from the concept patterns embedded in transformer layer activations. This collection of insights on transformer internal processing mechanics has immediate consequences for aligning AI models with human values, AI safety, opening the attack surface for potential adversarial attacks, and providing a basis for automatic quantification of a model's hallucination risk. <br> <br>

33. ***Bringing Research Papers to Life:  <br>A new framework called Paper2Agent automatically converts static research papers and their code into dynamic, interactive AI agents. These "paper agents" can then be queried in natural language to run analyses, answer questions, and reproduce results from the original study, creating a powerful new way to accelerate the adoption and dissemination of scientific knowledge.*** <br> <br>
    Sep 8, Stanford Uni published a [paper](https://arxiv.org/pdf/2509.06917) “Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents”. The study introduces Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. The study demonstrates Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. The study validates that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists. <br> <br>

35. ***More 'Thinking' Time Can Hurt AI Factuality:  <br>A comprehensive study reveals that giving AI reasoning models more time to "think" does not improve their performance on knowledge-intensive tasks and, in many cases, actually increases hallucinations. Researchers found that when models do become less prone to hallucination with longer reasoning, it is often because they learn to avoid answering questions they are unsure about, not because they become more factually accurate.*** <br> <br>
    Sep 8, National Uni of Singapore published a [paper](https://arxiv.org/pdf/2509.06861) “Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet”. Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has shown strong performance across many domains. However, the work shows that this approach is not yet effective for knowledge-intensive tasks, where high factual accuracy and low hallucination rates are essential. The authors conduct a comprehensive evaluation of test-time scaling using 12 reasoning models on two knowledge-intensive benchmarks. The results reveal that increasing test-time computation does not consistently improve accuracy and, in many cases, it even leads to more hallucinations. The study then analyzes how extended reasoning affects hallucination behavior, finds that reduced hallucinations often result from the model choosing to abstain after thinking more, rather than from improved factual recall. Conversely, for some models, longer reasoning encourages attempts on previously unanswered questions, many of which result in hallucinations. Case studies show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. Despite these limitations, the study observes that compared to non-thinking, enabling thinking remains beneficial. https://github.com/XuZhao0/tts-knowledge <br> <br>

37. ***AI Search Still Struggles with Complex Questions:  <br>To test the limits of modern information retrieval, researchers created a new benchmark of diverse and realistic complex search queries. The results show that even the most advanced retrieval models perform poorly, struggling to handle queries with multiple constraints or parts. The study underscores the urgent need for new research to build next-generation retrieval models that can keep pace with the sophisticated information needs of users.*** <br> <br>
    Sep 8, UMA published a [paper](https://arxiv.org/pdf/2509.07253) “Benchmarking Information Retrieval Models on Complex Retrieval Tasks”. Large language models (LLMs) are incredible and versatile tools for text-based tasks that have enabled countless, previously unimaginable, applications. Retrieval models, in contrast, have not yet seen such capable general-purpose models emerge. To achieve this goal, retrieval models must be able to perform complex retrieval tasks, where queries contain multiple parts, constraints, or requirements in natural language. These tasks represent a natural progression from the simple, single-aspect queries that are used in the vast majority of existing, commonly used evaluation sets. Complex queries naturally arise as people expect search systems to handle more specific and often ambitious information requests, as is demonstrated by how people use LLM-based information systems. Despite the growing desire for retrieval models to expand their capabilities in complex retrieval tasks, there exist limited resources to assess the ability of retrieval models on a comprehensive set of diverse complex tasks. The few resources that do exist feature a limited scope and often lack realistic settings making it hard to know the true capabilities of retrieval models on complex real-world retrieval tasks. To address this shortcoming and spur innovation in next-generation retrieval models, the study constructs a diverse and realistic set of complex retrieval tasks and benchmark a representative set of state-of-the-art retrieval models. Additionally, the work explores the impact of LLM-based query expansion and rewriting on retrieval quality. The results show that even the best models struggle to produce high-quality retrieval results with the highest average nDCG@10 of only 0.346 and R@100 of only 0.587 across all tasks. Although LLM augmentation can help weaker models, the strongest model has decreased performance across all metrics with all rewriting techniques. <br> <br>

39. ***AI's Own Reasoning Can Thwart Poisoning Attacks:  <br>Researchers have developed a new, stealthier data poisoning attack that targets an AI's chain of thought, but they discovered a surprising twist: the AI's own reasoning ability often allows it to recover from the attack mid-thought. This finding suggests that the very complexity that makes these models powerful may also grant them an emergent form of robustness against certain types of manipulation.*** <br> <br>
    Sep 6, Uni of Cambridge, Google, ICL, Northeastern Uni and Uni of Oxford published a [paper](https://arxiv.org/pdf/2509.05739) “Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated”. Early research into data poisoning attacks against Large Language Models (LLMs) demonstrated the ease with which backdoors could be injected. More recent LLMs add step-by-step reasoning, expanding the attack surface to include the intermediate chain-of-thought (CoT) and its inherent trait of decomposing problems into subproblems. Using these vectors for more stealthy poisoning, the study introduces “decomposed reasoning poison”, in which the attacker modifies only the reasoning path, leaving prompts and final answers clean, and splits the trigger across multiple, individually harmless components. Fascinatingly, while it remains possible to inject these decomposed poisons, reliably activating them to change final answers (rather than just the CoT) is surprisingly difficult. This difficulty arises because the models can often recover from backdoors that are activated within their thought processes. Ultimately, it appears that an emergent form of backdoor robustness is originating from the reasoning capabilities of these advanced LLMs, as well as from the architectural separation between reasoning and final answer generation.  <br> <br>

41. ***Teaching AI to Get Better on Its Own:  <br>A new reinforcement learning method called Exploratory Iteration (ExIt) trains large language models to master multi-step self-improvement tasks in a more efficient, curriculum-based way. By focusing training only on the most informative single steps of a solution process, ExIt effectively teaches the model how to progressively refine its own work at inference time, enabling it to achieve better performance over longer iterative sequences.*** <br> <br>
    Sep 4, Meta published a [paper](https://arxiv.org/pdf/2509.04575) “Bootstrapping Task Spaces for Self-Improvement”. Progress in many task domains emerges from repeated revisions to previous solution attempts. Training agents that can reliably self-improve over such sequences at inference-time is a natural target for reinforcement learning (RL), yet the naive approach assumes a fixed maximum iteration depth, which can be both costly and arbitrary. The study presents Exploratory Iteration (ExIt), a family of autocurriculum RL methods that directly exploits the recurrent structure of self-improvement tasks to train LLMs to perform multi-step self-improvement at inference-time while only training on the most informative single-step iterations. ExIt grows a task space by selectively sampling the most informative intermediate, partial histories encountered during an episode for continued iteration, treating these starting points as new self-iteration task instances to train a self-improvement policy. ExIt can further pair with explicit exploration mechanisms to sustain greater task diversity. Across several domains, encompassing competition math, multi-turn tool-use, and machine learning engineering, the study demonstrates that ExIt strategies, starting from either a single or many task instances, can produce policies exhibiting strong inference-time self-improvement on held-out task instances, and the ability to iterate towards higher performance over a step budget extending beyond the average iteration depth encountered during training. <br> <br>

43. ***A Faster Way for AI to Read and Respond:  <br>Researchers have developed REFRAG, a highly efficient decoding framework for retrieval-augmented generation (RAG) that dramatically speeds up response times. The method is based on the key insight that most of the retrieved text in a RAG system is irrelevant, allowing for massive context compression that accelerates the time-to-first-token by over 30 times without any loss in performance and even enabling a 16x larger context window.*** <br> <br>
    Sep 3, Meta, National Uni of Singapore and Rice Uni published a [paper](https://arxiv.org/abs/2509.01092) “REFRAG: Rethinking RAG based Decoding”. Large Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive external knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-augmented generation (RAG). However, processing long-context inputs introduces significant system latency and demands substantial memory for the key-value cache, resulting in reduced throughput and a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing latency for long-context inputs is a primary objective for LLMs, the authors contend that RAG require specialized consideration. In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, the study argues that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance. To this end, the work proposes REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting the sparsity structure, the study demonstrates a 30.85 the time-to-first-token acceleration (3.75 improvement to previous work) without loss in perplexity. In addition, the optimization framework for large context enables REFRAG to extend the context size of LLMs by 16. The study provides rigorous validation of REFRAG across diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets. Experimental results confirm that REFRAG delivers substantial speedup with no loss in accuracy compared to LLaMA models and other state-of-the-art baselines across various context sizes.  <br> <br>

45. ***Spotting AI Fabrications in Real Time:  <br>Researchers have developed a practical and scalable method to detect hallucinations in large language models as they happen. The technique trains simple classifiers to identify fabricated entities like names and citations at the token level, enabling real-time, streaming detection. This approach is not only highly effective—outperforming more costly methods—but also generalizable, as a classifier trained on one model's outputs can successfully detect hallucinations in another.*** <br> <br>
    Aug 26, ETH and MATS published a [paper](https://arxiv.org/pdf/2509.03531) “Real-Time Detection of Hallucinated Entities in Long-Form Generation”. Large language models are now routinely used in high-stakes applications where hallucinations can cause serious harm, such as medical consultations or legal advice. Existing hallucination detection methods, however, are impractical for real-world use, as they are either limited to short factual queries or require costly external verification. The study presents a cheap, scalable method for real-time identification of hallucinated tokens in long-form generations, and scale it effectively to 70B parameter models. The approach targets entity-level hallucinations -- e.g., fabricated names, dates, citations -- rather than claim-level, thereby naturally mapping to token-level labels and enabling streaming detection. The study develops an annotation methodology that leverages web search to annotate model responses with grounded labels indicating which tokens correspond to fabricated entities. This dataset enables to train effective hallucination classifiers with simple and efficient methods such as linear probes. Evaluating across four model families, the classifiers consistently outperform baselines on long-form responses, including more expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for Llama-3.3-70B), and are also an improvement in short-form question-answering settings. Moreover, despite being trained only with entity-level labels, the probes effectively detect incorrect answers in mathematical reasoning tasks, indicating generalization beyond entities. While the annotation methodology is expensive, the study finds that annotated responses from one model can be used to train effective classifiers on other models. Overall, the work suggests a promising new approach for scalable, real-world hallucination detection. https://github.com/obalcells/hallucination_probes
 <br> <br> <br>


***Sept 7, 2025***

1. ***A Five-Step Framework for AI Leadership:  <br>OpenAI's leadership guide provides a five-step framework—Align, Activate, Amplify, Accelerate, and Govern—to help businesses effectively integrate AI. Drawing on examples from companies like Moderna, the guide emphasizes treating AI as a fundamental shift in working practices, urging leaders to foster a culture of clear strategy, employee enablement, shared knowledge, fast execution, and responsible governance to gain a competitive edge.*** <br> <br>
   Sep 4, OpenAI published an [article](https://cdn.openai.com/pdf/ae250928-4029-4f26-9e23-afac1fcee14c/staying-ahead-in-the-age-of-ai.pdf) “Staying ahead in the age of AI - A leadership guide”. The guide outlines five principles—Align, Activate, Amplify, Accelerate, and Govern—to help organizations adopt AI effectively, drawing from experiences with companies like Moderna and Estée Lauder. **Align** emphasizes clear communication of AI’s strategic importance, setting measurable adoption goals, and leaders modeling AI use, as seen with Moderna’s CEO encouraging daily ChatGPT use. **Activate** focuses on enabling employees through role-specific training, AI champions networks, and routine experimentation, like the San Antonio Spurs’ 85% AI fluency boost via integrated training. **Amplify** promotes scaling AI impact by sharing successes through centralized knowledge hubs, newsletters, and internal communities, preventing siloed efforts. **Accelerate** advises removing barriers with easy tool access, clear project intake processes, and cross-functional AI councils, as exemplified by Estée Lauder’s GPT Lab prototyping over 1,000 ideas. **Govern** advocates for lightweight, practical guidelines and quarterly audits to balance speed and responsibility, ensuring safe AI use without stifling innovation. The guide highlights AI’s rapid progress—5.6x growth in model releases since 2022, 280x cost reduction for GPT-3.5-class models, and 4x faster adoption than desktop internet—urging organizations to treat AI as a new way of working. By fostering trust, learning, shared knowledge, swift execution, and responsible governance, companies can achieve business impact and competitive advantage in an AI-driven world. <br> <br>

3. ***Reinforcement Learning's Memory Advantage:  <br>A study from MIT introduces "RL's Razor," a principle explaining why fine-tuning with reinforcement learning (RL) is significantly better at preserving a model's existing knowledge than supervised fine-tuning (SFT). The research reveals that RL is implicitly biased to find solutions that minimally deviate from the original model's behavior, thereby preventing the "catastrophic forgetting" often seen with SFT.*** <br> <br>
   Sep 4, MIT published a [paper](https://arxiv.org/abs/2509.04259) “RL's Razor: Why Online Reinforcement Learning Forgets Less”. Comparison of fine-tuning models with reinforcement learning (RL) and supervised fine-tuning (SFT) reveals that, despite similar performance at a new task, RL preserves prior knowledge and capabilities significantly better. The study finds that the degree of forgetting is determined by the distributional shift, measured as the KL-divergence between the fine-tuned and base policy evaluated on the new task. The analysis reveals that on-policy RL is implicitly biased towards KL-minimal solutions among the many that solve the new task, whereas SFT can converge to distributions arbitrarily far from the base model. The work validates these findings through experiments with large language models and robotic foundation models and further provide theoretical justification for why on-policy RL updates lead to a smaller KL change. The study term this principle : among all ways to solve a new task, RL prefers those closest in KL to the original model. <br> <br>

5. ***Speeding Up AI with Parallel Token Prediction:  <br>Meta has introduced Set Block Decoding (SBD), a novel and flexible method that dramatically accelerates language model inference by allowing the model to predict multiple, non-consecutive future tokens in parallel. This approach reduces the number of generation steps by 3-5 times without sacrificing accuracy or requiring architectural changes, offering a simple yet powerful way to overcome the high computational costs of AI decoding.*** <br> <br>
   Sep 4, Meta published a [paper](https://arxiv.org/pdf/2509.04185) “Set Block Decoding is a Language Model Inference Accelerator”. Autoregressive next token prediction language models offer powerful capabilities but face significant challenges in practical deployment due to the high computational and memory costs of inference, particularly during the decoding stage. The study introduces Set Block Decoding (SBD), a simple and flexible paradigm that accelerates generation by integrating standard next token prediction (NTP) and masked token prediction (MATP) within a single architecture. SBD allows the model to sample multiple, not necessarily consecutive, future tokens in parallel, a key distinction from previous acceleration methods. This flexibility allows the use of advanced solvers from the discrete diffusion literature, offering significant speedups without sacrificing accuracy. SBD requires no architectural changes or extra training hyperparameters, maintains compatibility with exact KV-caching, and can be implemented by fine-tuning existing next token prediction models. By fine-tuning Llama-3.1 8B and Qwen-3 8B, the study demonstrates that SBD enables a 3-5x reduction in the number of forward passes required for generation while achieving same performance as equivalent NTP training. <br> <br>

7. ***Hallucinations as a Test-Taking Strategy:  <br>An OpenAI paper argues that language models hallucinate because their training and evaluation pipelines reward them for guessing on difficult questions, much like a student taking an exam. The research posits that these falsehoods are not a mysterious flaw but a learned behavior driven by a statistical pressure to produce plausible answers. The authors suggest that the most effective solution is not more hallucination-specific evaluations, but a fundamental change in how dominant benchmarks are scored to stop penalizing uncertainty.*** <br> <br>
   Sep 4, OpenAI published a [paper](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf) “Why Language Models Hallucinate”. Like students facing hard exam questions, large language models sometimes guess when uncertain, producing plausible yet incorrect statements instead of admitting uncertainty. Such “hallucinations” persist even in state-of-the-art systems and undermine trust. The authors argue that language models hallucinate because the training and evaluation procedures reward guessing over acknowledging uncertainty, and analyze the statistical causes of hallucinations in the modern training pipeline. Hallucinations need not be mysterious—they originate simply as errors in binary classification. If incorrect statements cannot be distinguished from facts, then hallucinations in pretrained language models will arise through natural statistical pressures. The authors then argue that hallucinations persist due to the way most evaluations are graded—language models are optimized to be good test-takers, and guessing when uncertain improves test performance. This “epidemic” of penalizing uncertain responses can only be addressed through a socio-technical mitigation: modifying the scoring of existing benchmarks that are misaligned but dominate leaderboards, rather than introducing additional hallucination evaluations. This change may steer the field toward more trustworthy AI systems. <br> <br>

9. ***An AI Planner That Learns from Video:  <br>Researchers from Meta and other institutions have developed the Vision Language World Model (VLWM), a foundation model that learns to plan complex tasks by understanding and modeling the world through natural videos. The VLWM uses a dual-process system—combining fast, reactive actions with slower, deliberate planning—to achieve state-of-the-art performance on visual planning benchmarks, demonstrating a more sophisticated approach to high-level reasoning for AI agents.*** <br> <br>
    Sep 4, Meta, Sorbonne Uni and Uni of Southern California published a [paper](https://www.arxiv.org/pdf/2509.02722) “Planning with Reasoning using Vision Language World Model”. Effective planning requires strong world models, but high-level world models that can understand and reason about actions with semantic and temporal abstraction remain largely underdeveloped. The study introduces the Vision Language World Model (VLWM), a foundation model trained for language-based world modeling on natural videos. Given visual observations, the VLWM first infers the overall goal achievements then predicts a trajectory composed of interleaved actions and world state changes. Those targets are extracted by iterative LLM Self-Refine conditioned on compressed future observations represented by Tree of Captions. The VLWM learns both an action policy and a dynamics model, which respectively facilitates reactive system-1 plan decoding and reflective system-2 planning via cost minimization. The cost evaluates the semantic distance between the hypothetical future states given by VLWM roll-outs and the expected goal state, and is measured by a critic model that is trained in a self-supervised manner. The VLWM achieves state-of-the-art Visual Planning for Assistance (VPA) performance on both benchmark evaluations and the proposed PlannerArena human evaluations, where system-2 improves the Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM baselines on RoboVQA and WorldPrediction benchmark. <br> <br>

11. ***Harnessing Physics for AI Computation:  <br>A paper in Nature explores the potential of Physical Neural Networks (PNNs), which use analogue physical systems to perform AI computations, potentially enabling models many orders of magnitude larger than today's. While PNNs promise a future of more efficient and powerful AI, the paper highlights that their success hinges on developing new training methods that are compatible with the constraints of the underlying physics, as the standard backpropagation algorithm is not a good fit.*** <br> <br>
    Sep 3, Nature published a [paper](https://www.nature.com/articles/s41586-025-09384-2) “Training of physical neural networks”. Physical neural networks (PNNs) are a class of neural-like networks that make use of analogue physical systems to perform computations. Although at present confined to small-scale laboratory demonstrations, PNNs could one day transform how artificial intelligence (AI) calculations are performed. Could people train AI models many orders of magnitude larger than present ones? Could people perform model inference locally and privately on edge devices? Research over the past few years has shown that the answer to these questions is probably “yes, with enough research”. Because PNNs can make use of analogue physical computations more directly, flexibly and opportunistically than traditional computing hardware, they could change what is possible and practical for AI systems. To do this, however, will require notable progress, rethinking both how AI models work and how they are trained—primarily by considering the problems through the constraints of the underlying hardware physics. To train PNNs, backpropagation-based and backpropagation-free approaches are now being explored. These methods have various trade-offs and, so far, no method has been shown to scale to large models with the same performance as the backpropagation algorithm widely used in deep learning today. However, this challenge has been rapidly changing and a diverse ecosystem of training techniques provides clues for how PNNs may one day be used to create both more efficient and larger-scale realizations of present-scale AI models. <br> <br>

13. ***Uncovering the Neural Basis of AI Lies:  <br>A study from Carnegie Mellon University systematically investigates whether large language models can "lie"—intentionally producing false information to achieve a goal—as opposed to simply hallucinating. Using mechanistic interpretability, the researchers have uncovered the neural mechanisms that drive deceptive behavior and have even developed methods to control it, revealing a troubling trade-off where lying can sometimes help the model better optimize its end-task performance.*** <br> <br>
    Sep 3, CMU published a [paper](https://arxiv.org/pdf/2509.03518) “Can LLMs Lie? Investigation beyond Hallucination”. Large language models (LLMs) have demonstrated impressive capabilities across a variety of tasks, but their increasing autonomy in real-world applications raises concerns about their trustworthiness. While hallucinations-unintentional falsehoods-have been widely studied, the phenomenon of lying, where an LLM knowingly generates falsehoods to achieve an ulterior objective, remains underexplored. This study systematically investigates the lying behavior of LLMs, differentiating it from hallucinations and testing it in practical scenarios. Through mechanistic interpretability techniques, the work uncovers the neural mechanisms underlying deception, employing logit lens analysis, causal interventions, and contrastive activation steering to identify and control deceptive behavior. The authors study real-world lying scenarios and introduce behavioral steering vectors that enable fine-grained manipulation of lying tendencies. Further, the study explores the trade-offs between lying and end-task performance, establishing a Pareto frontier where dishonesty can enhance goal optimization. The findings contribute to the broader discourse on AI ethics, shedding light on the risks and potential safeguards for deploying LLMs in high-stakes environments. https://llm-liar.github.io/ <br> <br>

15. ***The Distillation Dilemma in AI Training:  <br>A study by Meta and Carnegie Mellon University reveals a critical trade-off in "distilled pretraining," a popular technique for training large language models. The research shows that while distillation makes models much better at reasoning with more computation time (test-time scaling), it significantly impairs their ability to learn new tasks from examples provided in a prompt (in-context learning), offering key insights for practitioners designing future models.*** <br> <br>
    Sep 3, Meta and CMU published a [paper](https://arxiv.org/pdf/2509.01649) “Distilled Pretraining: A modern lens of Data, In-Context Learning and Test-Time Scaling”. In the past year, distillation has seen a renewed prominence in large language model (LLM) pretraining, exemplified by the Llama-3.2 and Gemma model families. While distillation has historically been shown to improve statistical modeling, its effects on new paradigms that are key to modern LLMs, such as test-time scaling and in-context learning, remain underexplored. This study makes three main contributions. First, the work shows that pretraining with distillation yields models that exhibit remarkably better test-time scaling. Second, the study observes that this benefit comes with a trade-off: distillation impairs in-context learning capabilities, particularly the one modeled via induction heads. Third, to demystify these findings, the authors study distilled pretraining in a sandbox of a bigram model, which helps to isolate the common principal factor behind the observations. Finally, using these insights, the work sheds light on various design choices for pretraining that should help practitioners going forward. <br> <br>

17. ***Creating a More Permanent AI 'Amnesia':  <br>Researchers have developed JensUn, a new and more effective method for making large language models "unlearn" specific information, such as private data or harmful content. The technique offers a better balance between forgetting unwanted knowledge and retaining useful capabilities, and it is more resistant to relearning. The study also proposes an improved evaluation framework which reveals that many existing unlearning methods are not as robust as previously thought.*** <br> <br>
    Sep 2, Uni of Tubingen and EPFL published a [paper](https://arxiv.org/pdf/2509.02820v1) “Unlearning That Lasts: Utility-Preserving, Robust, and Almost Irreversible Forgetting in LLMs”. Unlearning in large language models (LLMs) involves precisely removing specific information from a pre-trained model. This is crucial to ensure safety of LLMs by deleting private data or harmful knowledge acquired during pre-training. However, existing unlearning methods often fall short when subjected to thorough evaluation. To overcome this, the study introduces JensUn, which leverages the Jensen-Shannon Divergence as the training objective for both forget and retain sets for more stable and effective unlearning dynamics compared to commonly used loss functions. In extensive experiments, JensUn achieves better forget-utility trade-off than competing methods, and even demonstrates strong resilience to benign relearning. Additionally, for a precise unlearning evaluation, the study introduces LKF, a curated dataset of lesser-known facts that provides a realistic unlearning scenario. Finally, to comprehensively test unlearning methods, the study proposes (i) employing an LLM as semantic judge instead of the standard ROUGE score, and (ii) using worst-case unlearning evaluation over various paraphrases and input formats. The improved evaluation framework reveals that many existing methods are less effective than previously thought. <br> <br>

19. ***Balancing Quality and Creativity in AI Responses:  <br>Meta and other institutions have developed Diversity-Aware Reinforcement Learning (DARLING), a framework that trains language models to generate responses that are both high-quality and semantically diverse. By introducing a reward for novelty alongside a reward for correctness, DARLING overcomes the tendency of standard training to reduce the range of ideas, and it surprisingly finds that encouraging exploration for diversity also leads to higher-quality solutions in verifiable tasks like math problems.*** <br> <br>
    Sep 2, Meta, CMU and JHU published a [paper](https://arxiv.org/pdf/2509.02534) “Jointly Reinforcing Diversity and Quality in Language Model Generations”. Post-training of Large Language Models (LMs) often prioritizes accuracy and helpfulness at the expense of diversity. This creates a tension: while post-training improves response quality, it also sharpens output distributions and reduces the range of ideas, limiting the usefulness of LMs in creative and exploratory tasks such as brainstorming, storytelling, or problem solving. The study addresses this challenge with Diversity-Aware Reinforcement Learning (DARLING), a framework that jointly optimizes for response quality and semantic diversity. At its core, DARLING introduces a learned partition function to measure diversity beyond surface-level lexical variations. This diversity signal is then combined with a quality reward during online reinforcement learning, encouraging models to generate outputs that are both high-quality and distinct. Experiments across multiple model families and sizes show that DARLING generalizes to two regimes: non-verifiable tasks (instruction following and creative writing) and verifiable tasks (competition math). On five benchmarks in the first setting, DARLING consistently outperforms quality-only RL baselines, producing outputs that are simultaneously of higher quality and novelty. In the second setting, DARLING achieves higher pass@1 (solution quality) and pass@k (solution variety). Most strikingly, explicitly optimizing for diversity catalyzes exploration in online RL, which manifests itself as higher-quality responses. <br> <br>

21. ***A Reality Check for AI Optimizers:  <br>A systematic study from Stanford University re-evaluates the landscape of language model optimizers, finding that many claims of dramatic speedups over the standard AdamW optimizer are exaggerated. After rigorous and fair comparisons, the research concludes that while some newer matrix-based optimizers provide a modest 1.1x speedup for billion-parameter models, the gains shrink with scale, suggesting that AdamW remains a powerful and highly competitive choice for pretraining.*** <br> <br>
    Sep 2, Stanford Uni published a [paper](https://arxiv.org/pdf/2509.02046) “Fantastic Pretraining Optimizers and Where to Find Them”. AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. The study posits that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, the work conducts a systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum). The study finds that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across a range of model scales and data-to-model ratios, performed at the end of training. First, optimal hyperparameters for one optimizer may be suboptimal for another, making blind hyperparameter transfer unfair. Second, the actual speedup of many proposed optimizers over well-tuned baselines is lower than claimed and decreases with model size to only 1.1x for 1.2B parameter models. Thirdly, comparing intermediate checkpoints before reaching the target training budgets can be misleading, as rankings between two optimizers can flip during training due to learning rate decay. Through the thorough investigation, the study finds that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners -- multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4x over AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models. <br> <br>

23. ***A Dynamic AI Watchdog for Custom Policies:  <br>Researchers have developed DynaGuard, a dynamic guardian model designed to supervise chatbot outputs based on flexible, user-defined policies. Unlike static safety models that only check for a predefined list of harms, DynaGuard can be tailored to specific application needs, matching the accuracy of standard models on common harms and rivaling frontier models on custom rules, but with much faster performance.*** <br> <br>
    Sep 2, Uni of Maryland and CapitalOne published a [paper](https://arxiv.org/pdf/2509.02563) “DynaGuard: A Dynamic Guardrail Model With User-Defined Policies”. Guardian models are used to supervise and moderate the outputs of user-facing chatbots, enforcing guardrails and detecting bad behaviors. Standard guardian models like LlamaGuard detect predefined, static categories of harms. The study proposes dynamic guardian models that evaluate text based on user-defined policies, making them useful for different application domains that are not addressed by standard guardian models. The dynamic guardian models can be used for fast detection of policy violations or with chain-of-thought reasoning that articulates and justifies the model outputs. The dynamic guardian models match static models in detection accuracy for static harm categories while identifying violations of free-form policies with accuracy comparable to frontier reasoning models in a fraction of the time. https://github.com/montehoover/DynaGuard <br> <br>

25. ***A Fair Race for AI Optimizers:  <br>An extensive study from EPFL provides a standardized and rigorous benchmark comparing recent optimizers for large language model pretraining. By systematically testing and tuning each method across different scenarios, the research offers clear, practical guidance on which optimizers perform best under various conditions, helping to cut through the confusing claims and varied testing protocols that currently exist in the field.*** <br> <br>
    Sep 1, EPFL published a [paper](https://arxiv.org/pdf/2509.01440) “Benchmarking Optimizers for Large Language Model Pretraining”. The recent development of Large Language Models (LLMs) has been accompanied by an effervescence of novel ideas and methods to better optimize the loss of deep learning models. Claims from those methods are myriad: from faster convergence to removing reliance on certain hyperparameters. However, the diverse experimental protocols used to validate these claims make direct comparisons between methods challenging. This study presents a comprehensive evaluation of recent optimization techniques across standardized LLM pretraining scenarios, systematically varying model size, batch size, and training duration. Through careful tuning of each method, the study provides guidance to practitioners on which optimizer is best suited for each scenario. For researchers, the work highlights promising directions for future optimization research. Finally, by releasing the code and making all experiments fully reproducible, the authors hope the efforts can help the development and rigorous benchmarking of future methods. https://github.com/epfml/llm-optimizer-benchmark <br> <br>

27. ***Blaming the Test for AI's Prompt Sensitivity:  <br>A new study suggests that the widely reported "prompt sensitivity" of large language models may be more of an evaluation artifact than an inherent flaw. Researchers found that when they replaced rigid, text-matching evaluation methods with a more flexible LLM-as-a-Judge approach, the performance of models became much more consistent across differently phrased prompts, indicating that modern LLMs are more robust to template variations than previously believed.*** <br> <br>
    Sep 1, UCSB, UC Irvine, Uni of Oxford and UPenn published a [paper](https://arxiv.org/pdf/2509.01790) “Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs”. Prompt sensitivity, referring to the phenomenon where paraphrasing (i.e., repeating something written or spoken using different words) leads to significant changes in large language model (LLM) performance, has been widely accepted as a core limitation of LLMs. This study revisits this issue and asks: Is the widely reported high prompt sensitivity truly an inherent weakness of LLMs, or is it largely an artifact of evaluation processes? To answer this question, the work systematically evaluates 7 LLMs (e.g., GPT and Gemini family) across 6 benchmarks, including both multiple-choice and open-ended tasks on 12 diverse prompt templates. The study finds that much of the prompt sensitivity stems from heuristic evaluation methods, including log-likelihood scoring and rigid answer matching, which often overlook semantically correct responses expressed through alternative phrasings, such as synonyms or paraphrases. When the study adopts LLM-as-a-Judge evaluations, the authors observe a substantial reduction in performance variance and a consistently higher correlation in model rankings across prompts. The findings suggest that modern LLMs are more robust to prompt templates than previously believed, and that prompt sensitivity may be more an artifact of evaluation than a flaw in the models. <br> <br>

29. ***Modeling the Social World for AI:  <br>Researchers have developed "Social World Models," a new framework that helps AI systems better understand and reason about complex human social interactions. By representing social dynamics in a structured format that includes agents' actions and mental states, the system significantly improves an AI's performance on social reasoning tasks and enables it to make better decisions in interactive social environments.*** <br> <br>
    Aug 30, CMU and Nvidia published a [paper](https://arxiv.org/pdf/2509.00559) “Social World Models”. Humans intuitively navigate social interactions by simulating unspoken dynamics and reasoning about others' perspectives, even with limited information. In contrast, AI systems struggle to automatically structure and reason about these implicit social contexts. This study introduces a novel structured social world representation formalism (S3AP), designed to help AI systems reason more effectively about social dynamics. Following a POMDP-driven design, S3AP represents social interactions as structured tuples, such as state, observation, agent actions, and mental states, which can be automatically induced from free-form narratives or other inputs. The study first shows S3AP can help LLMs better understand social narratives across 5 social reasoning tasks (e.g., +51% improvement on FANToM's theory-of-mind reasoning with OpenAI's o1), reaching new state-of-the-art (SOTA) performance. We then induce social world models from these structured representations, demonstrating their ability to predict future social dynamics and improve agent decision-making, yielding up to +18% improvement on the SOTOPIA social interaction benchmark. The findings highlight the promise of S3AP as a powerful, general-purpose representation for social world states, enabling the development of more socially-aware systems that better navigate social interactions. <br> <br>

31. ***Thinking in SQL with AI Agents:  <br>The SQL-of-Thought framework introduces a multi-agent system that improves the accuracy of converting natural language questions into database queries. The system decomposes the complex task into a series of reasoning steps—including schema linking, query planning, and a unique guided error correction loop—to achieve state-of-the-art results on the widely used Spider dataset for text-to-SQL.*** <br> <br>
    Aug 30, MPISS and AWS published a [paper](https://arxiv.org/pdf/2509.00581) “SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction”. Converting natural language queries into SQL queries is a crucial challenge in both industry and academia, aiming to increase access to databases and large-scale applications. This work examines how in-context learning and chain-of-thought can be utilized to develop a robust solution for text-to-SQL systems. The study proposes SQL-of-Thought: a multi-agent framework that decomposes the Text2SQL task into schema linking, subproblem identification, query plan generation, SQL generation, and a guided correction loop. Unlike prior systems that rely only on execution-based static correction, the work introduces taxonomy-guided dynamic error modification informed by in-context learning. SQL-of-Thought achieves state-of-the-art results on the Spider dataset and its variants, combining guided error taxonomy with reasoning-based query planning. <br> <br>

33. ***Build Your Own AI Research Agent:  <br>Nvidia has released Universal Deep Research (UDR), a flexible agentic system that allows users to design and deploy their own custom research strategies on top of any language model. This approach moves beyond hard-coded research agents by providing a user-friendly interface to create, edit, and refine different research workflows without any need for fine-tuning, making powerful agentic research more accessible and adaptable.*** <br> <br>
    Aug 30, Nvidia published a [paper](https://arxiv.org/pdf/2509.00244) “Universal Deep Research: Bring Your Own Model and Strategy”. Deep research tools are among the most impactful and most commonly encountered agentic systems today. The study observes, however, that each deep research agent introduced so far is hard-coded to carry out a particular research strategy using a fixed choice of tools. The work introduces Universal Deep Research (UDR), a generalist agentic system that wraps around any language model and enables the user to create, edit, and refine their own entirely custom deep research strategies without any need for additional training or finetuning. To showcase the generality of the system, the study equips UDR with example minimal, expansive, and intensive research strategies, and provide a user interface to facilitate experimentation with the system. https://github.com/NVlabs/UniversalDeepResearch <br> <br>

35. ***Tackling Nuanced Numerical Reasoning in Text:  <br>A study from MIT defines and benchmarks "reasoning-intensive regression" (RiR), a challenging class of problems where large language models must deduce nuanced numerical properties from text. Finding that standard methods are often insufficient, the researchers propose MENTAT, a simple and effective technique that combines prompt optimization with ensemble learning to achieve up to a 65% performance improvement on these difficult tasks.*** <br> <br>
    Aug 29, MIT published a [paper](https://arxiv.org/pdf/2508.21762) “Reasoning-Intensive Regression”. AI researchers and practitioners increasingly apply large language models (LLMs) to what it is called reasoning-intensive regression (RiR), i.e. deducing subtle numerical properties from text. Unlike standard language regression tasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc problems like rubric-based scoring or domain-specific retrieval, where much deeper analysis of text is required while only limited task-specific training data and computation are available. The study casts three realistic problems as RiR tasks to establish an initial benchmark, and use that to test the hypothesis that prompting frozen LLMs and finetuning Transformer encoders via gradient descent will both often struggle in RiR. The study then proposes MENTAT, a simple and lightweight method that combines batch-reflective prompt optimization with neural ensemble learning. MENTAT achieves up to 65% improvement over both baselines, though substantial room remains for future advances in RiR. <br> <br>

37. ***AI Anomaly Detection That Learns on the Fly:  <br>Google has introduced CALM, a framework for real-time anomaly detection that continuously adapts to changing data streams. The system's novelty lies in its closed-loop fine-tuning mechanism and its use of a Large Language Model as a "judge" to provide semantic, context-aware feedback on detected anomalies. This LLM-guided approach allows the model to constantly learn from new patterns, maintaining high performance in dynamic environments.*** <br> <br>
    Aug 29, Google published a [paper](https://arxiv.org/pdf/2508.21273) “CALM: A Framework for Continuous, Adaptive, and LLM-Mediated Anomaly Detection in Time-Series Streams”. The detection of anomalies in non-stationary time-series streams is a critical but challenging task across numerous industrial and scientific domains. Traditional models, trained offline, suffer significant performance degradation when faced with concept drift, where the underlying statistical properties of the data change over time. This paper introduces CALM (Continuous, Adaptive, and LLM-Mediated), a novel, end-to-end framework for real-time anomaly detection designed to address this challenge. CALM is built on the Apache Beam distributed processing framework and leverages the TimesFm foundation model for forecasting-based anomaly detection. The framework's novelty lies in two core contributions. First, it implements a closed-loop, continuous fine-tuning mechanism that allows the anomaly detection model to adapt to evolving data patterns in near real-time. Second, it introduces an LLM-as-a-Judge component, a Large Language Model that provides semantic, context-aware judgments on detected anomalies to curate a high-quality training dataset, deciding whether an anomaly represents transient noise or a meaningful pattern shift. The study evaluates CALM on the comprehensive TSB-UAD benchmark. Results demonstrate that the continuously fine-tuned model improves the ROC AUC score in most datasets compared to the static, pre-trained base model, validating the efficacy of the adaptive, LLM-guided approach to maintaining high-performance anomaly detection in dynamic streaming environments. <br> <br>

39. ***AI's Chilling Effect on Early-Career Employment:  <br>A new study examining high-frequency payroll data presents six facts that serve as "canaries in the coal mine" for AI's impact on the labor market. The key finding is that since the rise of generative AI, early-career workers in the most exposed occupations have seen a 13% relative decline in employment. This shift, concentrated in roles susceptible to automation, provides some of the first large-scale evidence that AI is beginning to significantly disrupt the job market for new entrants.*** <br> <br>
    Aug 25, Stanford Uni published a [paper](https://digitaleconomy.stanford.edu/wp-content/uploads/2025/08/Canaries_BrynjolfssonChandarChen.pdf) “Canaries in the Coal Mine Six Facts about the Recent Employment Effects of Artificial Intelligence”. This paper examines changes in the labor market for occupations exposed to generative artificial intelligence using high-frequency administrative data from the largest payroll software provider in the United States. The study presents six facts that characterize these shifts. The paper finds that since the widespread adoption of generative AI, early-career workers (ages 22-25) in the most AI-exposed occupations have experienced a 13 percent relative decline in employment even after controlling for firm-level shocks. In contrast, employment for workers in less exposed fields and more experienced workers in the same occupations has remained stable or continued to grow. The study also finds that adjustments occur primarily through employment rather than compensation. Furthermore, employment declines are concentrated in occupations where AI is more likely to automate, rather than augment, human labor. The results are robust to alternative explanations, such as excluding technology-related firms and excluding occupations amenable to remote work. These six facts provide early, large-scale evidence consistent with the hypothesis that the AI revolution is beginning to have a significant and disproportionate impact on entry-level workers in the American labor market.
 <br> <br> <br>

***Aug 31, 2025***

1. ***Fundamental Limits of Vector Search:  <br>A Google paper demonstrates that vector embeddings have fundamental theoretical limitations that manifest even in simple, realistic retrieval scenarios. The study shows that the embedding dimension restricts the number of possible top-k results a model can return, a limitation that persists even with advanced models. By creating a new dataset called LIMIT to stress-test this, researchers found that even state-of-the-art models fail, indicating a need for new research beyond the current single-vector paradigm.*** <br> <br>
   Aug 28, Google published a [paper](https://arxiv.org/pdf/2508.21038) “On the Theoretical Limitations of Embedding-Based Retrieval”. Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. This study demonstrates that people may encounter these theoretical limitations in realistic settings with extremely simple queries. The study connects known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. The work empirically shows that this holds true even if restrict to k=2, and directly optimize on the test set with free parameterized embeddings. The study then creates a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. The work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation. <br> <br>

3. ***AI Agent Achieves Frontier Math Skills:  <br>Microsoft has developed rStar2-Agent, a 14B parameter math reasoning model that achieves frontier-level performance by using agentic reinforcement learning. The model exhibits advanced cognitive skills, such as using Python tools and reflecting on their output to solve complex problems. Thanks to an efficient training infrastructure and a novel reinforcement learning algorithm, rStar2-Agent was able to surpass the performance of much larger models like the 671B DeepSeek-R1 on challenging math benchmarks.*** <br> <br>
   Aug 28, Microsoft published a [paper](https://www.arxiv.org/pdf/2508.20722) “rStar2-Agent: Agentic Reasoning Technical Report”. The study introduces rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. https://github.com/microsoft/rStar <br> <br>

5. ***The Provable Power of Tool-Augmented AI:  <br>A paper from ETH, Inria, and Meta provides theoretical proof that tool-augmented language models are fundamentally more scalable for factual recall than models that rely on memorization. The research demonstrates that the number of facts a model can store in its weights is limited by its parameter count, whereas using external tools allows for unbounded access to information. This establishes why tool-based AI workflows are not just a practical choice but a provably superior one.*** <br> <br>
   Aug 28, ETH, Inria Uni and Meta published a [paper](https://arxiv.org/pdf/2508.20755) “Provable Benefits of In-Tool Learning for Large Language Models”. Tool-augmented language models, equipped with retrieval, memory, or external APIs, are reshaping AI, yet their theoretical advantages remain underexplored. This study addresses this question by demonstrating the benefits of in-tool learning (external retrieval) over in-weight learning (memorization) for factual recall. The study shows that the number of facts a model can memorize solely in its weights is fundamentally limited by its parameter count. In contrast, the work proves that tool-use enables unbounded factual recall via a simple and efficient circuit construction. These results are validated in controlled experiments, where tool-using models consistently outperform memorizing ones. The study further shows that for pretrained large language models, teaching tool-use and general rules is more effective than finetuning facts into memory. The work provides both a theoretical and empirical foundation, establishing why tool-augmented workflows are not just practical, but provably more scalable. <br> <br>

7. ***A Wiser Judge for AI Thinking:  <br>Researchers from Meta, UIUC, and NYU have developed StepWiser, a generative judge designed to more effectively supervise the multi-step reasoning of other AI models. Instead of simply classifying reasoning steps as correct or incorrect, StepWiser generates its own "meta-reasoning" to explain its judgments. This approach leads to more accurate feedback, which can then be used to improve the original model's performance during both training and inference.*** <br> <br>
   Aug 27, Meta, UIUC and NYU published a [paper](https://arxiv.org/pdf/2508.19229) “StepWiser: Stepwise Generative Judges for Wiser Reasoning”. As models increasingly leverage multi-step reasoning strategies to solve complex problems, supervising the logical validity of these intermediate steps has become a critical research challenge. Process reward models address this by providing step-by-step feedback, but current approaches have two major drawbacks: they typically function as classifiers without providing explanations, and their reliance on supervised fine-tuning with static datasets limits generalization. Inspired by recent advances, the study reframes stepwise reward modeling from a classification task to a reasoning task itself. The work thus proposes a generative judge that reasons about the policy model's reasoning steps (i.e., meta-reasons), outputting thinking tokens before delivering a final verdict. The model, StepWiser, is trained by reinforcement learning using relative outcomes of rollouts. The study shows it provides (i) better judgment accuracy on intermediate steps than existing methods; (ii) can be used to improve the policy model at training time; and (iii) improves inference-time search. <br> <br>

9. ***A New Live Benchmark for AI Researchers:  <br>Stanford and UC Berkeley researchers have introduced DeepScholar-bench, a live benchmark designed to evaluate an AI's ability to research and synthesize knowledge. The benchmark challenges systems with a real-world task: generating the "related work" section of a scientific paper using recent ArXiv preprints as source material. The automated evaluation framework reveals that this task remains a significant challenge, with even the top-performing AI systems scoring below 20%, highlighting the need for further progress in generative research synthesis.*** <br> <br>
    Aug 27, Stanford Uni and UC Berkeley published a [paper](https://arxiv.org/pdf/2508.20033) “DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis”. The ability to research and synthesize knowledge is central to human expertise and progress. An emerging class of systems promises these exciting capabilities through generative research synthesis, performing retrieval over the live web and synthesizing discovered sources into long-form, cited summaries. However, evaluating such systems remains an open challenge: existing question-answering benchmarks focus on short-form factual responses, while expert-curated datasets risk staleness and data contamination. Both fail to capture the complexity and evolving nature of real research synthesis tasks. This study introduces DeepScholar-bench, a live benchmark and holistic, automated evaluation framework designed to evaluate generative research synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv papers and focuses on a real research synthesis task: generating the related work sections of a paper by retrieving, synthesizing, and citing prior research. The evaluation framework holistically assesses performance across three key dimensions, knowledge synthesis, retrieval quality, and verifiability. The study also develops DeepScholar-base, a reference pipeline implemented efficiently using the LOTUS API. Using the DeepScholar-bench framework, the work performs a systematic evaluation of prior open-source systems, search AI's, OpenAI's DeepResearch, and DeepScholar-base. The work finds that DeepScholar-base establishes a strong baseline, attaining competitive or higher performance than each other method. The study also finds that DeepScholar-bench remains far from saturated, with no system exceeding a score of 19% across all metrics. These results underscore the difficulty of DeepScholar-bench, as well as its importance for progress towards AI systems capable of generative research synthesis. https://github.com/guestrin-lab/deepscholar-bench <br> <br>

11. ***Knowledge is the Bottleneck for AI Scientists:  <br>A study by researchers from Yale and Harvard introduces SciReas, a benchmark suite, and KRUX, a framework, to better understand how LLMs tackle scientific problems. A key finding is that the biggest hurdle for these models is not reasoning itself, but accessing and retrieving the necessary domain knowledge stored in their parameters. The research confirms that providing external knowledge in-context consistently boosts performance, and that improving a model's reasoning also helps it better utilize its internal knowledge.*** <br> <br>
    Aug 26, Yale Uni, Harvard Uni et al published a [paper](https://arxiv.org/pdf/2508.19202) “Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning”. Scientific problem solving poses unique challenges for LLMs, requiring both deep domain knowledge and the ability to apply such knowledge through complex reasoning. While automated scientific reasoners hold great promise for assisting human scientists, there is currently no widely adopted holistic benchmark for evaluating scientific reasoning, and few approaches systematically disentangle the distinct roles of knowledge and reasoning in these tasks. To address these gaps, the study introduces SciReas, a diverse suite of existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a selective subset that requires more complex reasoning. The holistic evaluation surfaces insights about scientific reasoning performance that remain hidden when relying on individual benchmarks alone. The study then proposes KRUX, a probing framework for studying the distinct roles of reasoning and knowledge in scientific tasks. Combining the two, the study conducts an in-depth analysis that yields several key findings: (1) Retrieving task-relevant knowledge from model parameters is a critical bottleneck for LLMs in scientific reasoning; (2) Reasoning models consistently benefit from external knowledge added in-context on top of the reasoning enhancement; (3) Enhancing verbalized reasoning improves LLMs' ability to surface task-relevant knowledge. Finally, the work conducts a lightweight analysis, comparing the science-focused data composition with concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline for scientific reasoning. https://github.com/yale-nlp/SciReas-Eval <br> <br>

13. ***Anatomy of an AI Health Coach:  <br>Google has published a paper detailing the design of a comprehensive Personal Health Agent (PHA) that uses AI to provide personalized wellness recommendations in daily, non-clinical settings. Developed through extensive user-centered research, the PHA is a multi-agent system that can analyze data from wearables and health records, provide expert insights, and act as a health coach. The work represents the most thorough evaluation of a consumer health agent so far and lays the groundwork for a future where personal health guidance is universally accessible.*** <br> <br>
    Aug 26, Google published a [paper](https://www.arxiv.org/pdf/2508.20148) “The Anatomy of a Personal Health Agent”. Health is a fundamental pillar of human wellness, and the rapid advancements in large language models (LLMs) have driven the development of a new generation of health agents. However, the application of health agents to fulfill the diverse needs of individuals in daily non-clinical settings is underexplored. This study aims to build a comprehensive personal health agent that is able to reason about multimodal data from everyday consumer wellness devices and common personal health records, and provide personalized health recommendations. To understand end-users' needs when interacting with such an assistant, the study conducted an in-depth analysis of web search and health forum queries, alongside qualitative insights from users and health experts gathered through a user-centered design process. Based on these findings, the study identified three major categories of consumer health needs, each of which is supported by a specialist sub-agent: (1) a data science agent that analyzes personal time-series wearable and health record data, (2) a health domain expert agent that integrates users' health and contextual data to generate accurate, personalized insights, and (3) a health coach agent that synthesizes data insights, guiding users using a specified psychological strategy and tracking users' progress. Furthermore, the study proposes and develops the Personal Health Agent (PHA), a multi-agent framework that enables dynamic, personalized interactions to address individual health needs. To evaluate each sub-agent and the multi-agent system, the work conducted automated and human evaluations across 10 benchmark tasks, involving more than 7,000 annotations and 1,100 hours of effort from health experts and end-users. The work represents the most comprehensive evaluation of a health agent to date and establishes a strong foundation towards the futuristic vision of a personal health agent accessible to everyone. <br> <br>

15. ***Predicting Token Order Boosts AI Training:  <br>Researchers at MBZUAI have found that training language models to predict the order of upcoming tokens is a more effective auxiliary task than trying to predict the exact tokens themselves. This new method, called Token Order Prediction (TOP), uses a simpler learning-to-rank loss and requires minimal architectural changes. Experiments show that models trained with TOP consistently outperform those trained with standard next-token prediction or the more complex multi-token prediction objective.*** <br> <br>
    Aug 26, MBZUAI published a [paper](https://arxiv.org/pdf/2508.19228) “Predicting the Order of Upcoming Tokens Improves Language Modeling”. Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to improve next-token prediction (NTP) in language model training but shows inconsistent improvements, underperforming in standard NLP benchmarks. The authors argue that MTP's exact future token prediction is too difficult as an auxiliary loss. Instead, the study proposes Token Order Prediction (TOP), which trains models to order upcoming tokens by their proximity using a learning-to-rank loss. TOP requires only a single additional unembedding layer compared to MTP's multiple transformer layers. The study pretrains models of 340M, 1.8B, and 7B parameters using NTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show that TOP overall outperforms both NTP and MTP even at scale. https://github.com/zaydzuhri/token-order-prediction <br> <br>

17. ***AI Agents That Learn Without Retraining:  <br>A study from UCL introduces Memento, a novel framework that enables AI agents to learn and adapt continuously without the need for costly fine-tuning of the underlying large language model. The agent instead uses a memory system to store past experiences and a lightweight policy to retrieve relevant memories to guide its actions. This efficient, memory-based reinforcement learning approach allows the agent to improve from feedback in real time, achieving top performance on complex deep research tasks.*** <br> <br>
    Aug 25, UCL et al published a [paper](https://www.arxiv.org/pdf/2508.16153) “Memento: Fine-tuning LLM Agents without Fine-tuning LLMs”. The study introduces a novel learning paradigm for Adaptive Large Language Model (LLM) agents that eliminates the need for fine-tuning the underlying LLMs. Existing approaches are often either rigid, relying on static, handcrafted reflection workflows, or computationally intensive, requiring gradient updates of LLM model parameters. In contrast, the proposed method enables low-cost continual adaptation via memory-based online reinforcement learning. The study formalises this as a Memory-augmented Markov Decision Process (M-MDP), equipped with a neural case-selection policy to guide action decisions. Past experiences are stored in an episodic memory, either differentiable or non-parametric. The policy is continually updated based on environmental feedback through a memory rewriting mechanism, whereas policy improvement is achieved through efficient memory reading (retrieval). The authors instantiate the agent model in the deep research setting, namely Memento, which attains top-1 on GAIA validation ( Pass@) and on the test set. It reaches F1 and PM on the DeepResearcher dataset, outperforming the state-of-the-art training-based method, while case-based memory adds to absolute points on out-of-distribution tasks. The approach offers a scalable and efficient pathway for developing generalist LLM agents capable of continuous, real-time learning without gradient updates, advancing machine learning towards open-ended skill acquisition and deep research scenarios. https://github.com/Agent-on-the-Fly/Memento <br> <br>

19. ***A Benchmark of Unanswered Questions:  <br>Stanford researchers have created UQ, a new benchmark that evaluates large language models by tasking them with solving genuinely unanswered questions from the real world. Sourced from platforms like Stack Exchange, these questions are inherently difficult and realistic, providing a way to measure progress on problems where a correct answer would represent a real contribution to human knowledge. The open platform shows that even frontier models struggle, with the best passing validation on only 15% of questions, highlighting its role in charting a path toward more capable AI.*** <br> <br>
    Aug 25, Stanford Uni et al published a [paper](https://arxiv.org/pdf/2508.17580) “UQ: Assessing Language Models on Unsolved Questions”. Benchmarks shape progress in AI research. A useful benchmark should be both difficult and realistic: questions should challenge frontier models while also reflecting real-world usage. Yet, current paradigms face a difficulty-realism tension: exam-style benchmarks are often made artificially difficult with limited real-world value, while benchmarks based on real user interaction often skew toward easy, high-frequency problems. This study explores a radically different paradigm: assessing models on unsolved questions. Rather than a static benchmark scored once, the study curates unsolved questions and evaluate models asynchronously over time with validator-assisted screening and community verification. The work introduces UQ, a testbed of 500 challenging, diverse questions sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi and history, probing capabilities including reasoning, factuality, and browsing. UQ is difficult and realistic by construction: unsolved questions are often hard and naturally arise when humans seek answers, thus solving them yields direct real-world value. The contributions are threefold: (1) UQ-Dataset and its collection pipeline combining rule-based filters, LLM judges, and human review to ensure question quality (e.g., well-defined and difficult); (2) UQ-Validators, compound validation strategies that leverage the generator-validator gap to provide evaluation signals and pre-screen candidate solutions for human review; and (3) UQ-Platform, an open platform where experts collectively verify questions and solutions. The top model passes UQ-validation on only 15% of questions, and preliminary human verification has already identified correct answers among those that passed. UQ charts a path for evaluating frontier models on real-world, open-ended challenges, where success pushes the frontier of human knowledge. https://uq.stanford.edu. <br> <br>

21. ***Questioning the AI Judge:  <br>A position paper from McGill, Mila, and Statistics Canada raises critical concerns about the growing use of Large Language Models as Judges (LLJs) for evaluating AI-generated text. The authors argue that the field has adopted this practice without sufficient scrutiny, and they use measurement theory from the social sciences to question the core assumptions about the reliability and validity of AI evaluators. The paper calls for more rigorous and responsible practices to ensure that LLJs genuinely support, rather than undermine, progress in the field.*** <br> <br>
    Aug 25, McGill Uni, Mila and Statistics Canada published a [paper](https://arxiv.org/pdf/2508.18076) “Neither Valid nor Reliable? Investigating the Use of LLMs as Judges”. Evaluating natural language generation (NLG) systems remains a core challenge of natural language processing (NLP), further complicated by the rise of large language models (LLMs) that aims to be general-purpose. Recently, large language models as judges (LLJs) have emerged as a promising alternative to traditional metrics, but their validity remains underexplored. This position paper argues that the current enthusiasm around LLJs may be premature, as their adoption has outpaced rigorous scrutiny of their reliability and validity as evaluators. Drawing on measurement theory from the social sciences, the authors identify and critically assess four core assumptions underlying the use of LLJs: their ability to act as proxies for human judgment, their capabilities as evaluators, their scalability, and their cost-effectiveness. The study examines how each of these assumptions may be challenged by the inherent limitations of LLMs, LLJs, or current practices in NLG evaluation. To ground the analysis, the study explores three applications of LLJs: text summarization, data annotation, and safety alignment. Finally, the study highlights the need for more responsible evaluation practices in LLJs evaluation, to ensure that their growing role in the field supports, rather than undermines, progress in NLG. <br> <br>

23. ***How AI Becomes Smarter Than Its Teachers:  <br>A Harvard study provides a taxonomy for how AI models can achieve "transcendence," meaning they perform better than the individual human experts they were trained on. The research identifies three key mechanisms—skill denoising, skill selection, and skill generalization—and uses a controlled simulation to show how diversity in the training data allows a model to combine and synthesize different pockets of expertise into a superhuman capability.*** <br> <br>
    Aug 25, Harvard Uni published a [paper](https://arxiv.org/pdf/2508.17669) “A Taxonomy of Transcendence”. Although language models are trained to mimic humans, the resulting systems display capabilities beyond the scope of any one person. To understand this phenomenon, the study uses a controlled setting to identify properties of the training data that lead a model to transcend the performance of its data sources. The study builds on previous work to outline three modes of transcendence, which the study calls skill denoising, skill selection, and skill generalization. The work then introduces a knowledge graph-based setting in which simulated experts generate data based on their individual expertise. The authors highlight several aspects of data diversity that help to enable the model's transcendent capabilities. Additionally, the data generation setting offers a controlled testbed that the authors hope is valuable for future research in the area. <br> <br>

25. ***More Compute, Better AI Retrieval:  <br>A study from Databricks demonstrates that the retrieval performance of a large language model scales predictably with the amount of computational power (FLOPs) invested in its pretraining. After benchmarking a wide range of models, researchers found a clear and predictable relationship: more pretraining compute leads to better zero-shot retrieval capabilities. The findings suggest that developing better LLM-based information retrievers is directly tied to scaling up their pretraining.*** <br> <br>
    Aug 24, Databricks published a [paper](https://www.arxiv.org/pdf/2508.17400) “Retrieval Capabilities of Large Language Models Scale with Pretraining FLOPs”. How does retrieval performance scale with pretraining FLOPs? The study benchmarks retrieval performance across LLM model sizes from 125 million parameters to 7 billion parameters pretrained on datasets ranging from 1 billion tokens to more than 2 trillion tokens. The study finds that retrieval performance on zero-shot BEIR tasks predictably scales with LLM size, training duration, and estimated FLOPs. The study also shows that In-Context Learning scores are strongly correlated with retrieval scores across retrieval tasks. Finally, the study highlights the implications this has for the development of LLM-based retrievers. <br> <br>

27. ***Reinforcement Learning's True Role in AI Tuning:  <br>A new study revisits the roles of supervised fine-tuning (SFT) and reinforcement learning (RL-FT), concluding that RL is not a panacea but rather a tool for recovery. The research demonstrates that RL-FT's main benefit is to counteract the performance degradation on out-of-distribution tasks that is often caused by SFT. The paper provides a deeper, spectrum-based analysis of how these methods alter the model's internal representations, suggesting that cheaper, low-rank adjustments can often achieve much of the recovery before needing to apply costly RL.*** <br> <br>
    Aug 22, PolyTechnique Montrel, Mila, McGill and UDeM published a [paper](https://arxiv.org/pdf/2508.16279) “RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs”. Training large language models (LLMs) from scratch is increasingly impractical, making post-training methods such as supervised fine-tuning (SFT) and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern practice. Using an out-of-distribution (OOD) variant of the 24-point card game and new spectrum-based diagnostics, the study revisits how these two stages reshape model representation and OOD performance. The key findings are- (1) RL-FT can restore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to 15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and a clear distribution shift, RL-FT cannot fully recover OOD performance. (2) Direction shifts of singular vectors matter more than singular value magnitudes. These shifts concentrate on directions linked to the largest and smallest singular values, leaving the bulk spectrum intact. (3) Low-rank and shallow recovery is effective: restoring singular vector directions for the top 20% of values or first 25% of layers recovers 70-80% of OOD performance. (4) Stronger SFT checkpoints enable better recovery by RL, while overfitted ones resist restoration. These results reconcile prior reports of RL superior OOD performance: RL primarily counteracts SFT-induced directional drift rather than finding new solutions. The spectrum-aware analysis highlights inexpensive recovery knobs low-rank UV merging and shallow-layer resets that practitioners can use before costly RL fine-tuning. <br> <br>

29. ***A New Toolkit for Building AI Agents:  <br>Alibaba has released AgentScope 1.0, a comprehensive, developer-centric framework designed to simplify the creation of advanced agentic applications. The updated version features a flexible, asynchronous design that enhances tool-based interactions and supports complex collaboration between agents and humans. With built-in agents, a visual studio for easier debugging, and a secure runtime sandbox, AgentScope provides a robust foundation for developers to build, evaluate, and deploy scalable AI agents.*** <br> <br>
    Aug 22, Alibaba published a [paper](https://arxiv.org/pdf/2508.16279) “AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications”. Driven by rapid advancements of Large Language Models (LLMs), agents are empowered to combine intrinsic knowledge with dynamic tool use, greatly enhancing their capacity to address real-world tasks. In line with such an evolution, AgentScope introduces major improvements in a new version (1.0), towards comprehensively supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. Specifically, the work abstracts foundational components essential for agentic applications and provide unified interfaces and extensible modules, enabling developers to easily leverage the latest progress, such as new models and MCPs. Furthermore, the work grounds agent behaviors in the ReAct paradigm and offers advanced agent-level infrastructure based on a systematic asynchronous design, which enriches both human-agent and agent-agent interaction patterns while improving execution efficiency. Building on this foundation, the work integrates several built-in agents tailored to specific practical scenarios. AgentScope also includes robust engineering support for developer-friendly experiences. AgentScope provides a scalable evaluation module with a visual studio interface, making the development of long-trajectory agentic applications more manageable and easier to trace. In addition, AgentScope offers a runtime sandbox to ensure safe agent execution and facilitates rapid deployment in production environments. With these enhancements, AgentScope provides a practical foundation for building scalable, adaptive, and effective agentic applications. https://github.com/agentscope-ai/agentscope <br> <br>

31. ***Fighting AI Hallucinations by Rewriting the Question:  <br>Researchers from JP Morgan and NYU have developed QueryBandits, a system that mitigates AI hallucinations by intelligently rewriting user prompts before they are processed by a large language model. The framework uses a bandit algorithm to dynamically choose the best rewrite strategy based on a query's linguistic features, proactively steering the model away from outputs prone to hallucination. This approach proved far more effective than static rewriting rules, some of which actually worsened hallucination rates.*** <br> <br>
    Aug 22, JP Morgan and NYU published a [paper](https://arxiv.org/pdf/2508.16697) “QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting”. Advanced reasoning capabilities in Large Language Models (LLMs) have caused higher hallucination prevalence; yet most mitigation work focuses on after-the-fact filtering rather than shaping the queries that trigger them. The work introduces QueryBandits, a bandit framework that designs rewrite strategies to maximize a reward model, that encapsulates hallucination propensity based upon the sensitivities of 17 linguistic features of the input query-and therefore, proactively steer LLMs away from generating hallucinations. Across 13 diverse QA benchmarks and 1,050 lexically perturbed queries per dataset, the top contextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a no-rewrite baseline and also outperforms zero-shot static prompting ("paraphrase" or "expand") by 42.6% and 60.3% respectively. Therefore, the work empirically substantiates the effectiveness of QueryBandits in mitigating hallucination via the intervention that takes the form of a query rewrite. Interestingly, certain static prompting strategies, which constitute a considerable number of current query rewriting literature, have a higher cumulative regret than the no-rewrite baseline, signifying that static rewrites can worsen hallucination. Moreover, the work discovers that the converged per-arm regression feature weight vectors substantiate that there is no single rewrite strategy optimal for all queries. In this context, guided rewriting via exploiting semantic features with QueryBandits can induce significant shifts in output behavior through forward-pass mechanisms, bypassing the need for retraining or gradient-based adaptation. <br> <br>

33. ***AI Architecture Built for Speed and Accuracy:  <br>Nvidia has introduced Jet-Nemotron, a new family of highly efficient hybrid-architecture language models that deliver accuracy on par with top full-attention models but with up to a 53.6x speedup in generation throughput. The models were created using an innovative design pipeline called Post Neural Architecture Search (PostNAS), which starts with a pre-trained model and efficiently explores new attention block designs to create a final architecture that is both powerful and fast.***  <br> <br>
    Aug 21, Nvidia published a [paper](https://www.arxiv.org/pdf/2508.15884) “Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search”. The paper presents Jet-Nemotron, a new family of hybrid-architecture language models, which matches or exceeds the accuracy of leading full-attention models while significantly improving generation throughput. Jet-Nemotron is developed using Post Neural Architecture Search (PostNAS), a novel neural architecture exploration pipeline that enables efficient model design. Unlike prior approaches, PostNAS begins with a pre-trained full-attention model and freezes its MLP weights, allowing efficient exploration of attention block designs. The pipeline includes four key components: (1) learning optimal full-attention layer placement and elimination, (2) linear attention block selection, (3) designing new attention blocks, and (4) performing hardware-aware hyperparameter search. The Jet-Nemotron-2B model achieves comparable or superior accuracy to Qwen3, Qwen2.5, Gemma3, and Llama3.2 across a comprehensive suite of benchmarks while delivering up to 53.6x generation throughput speedup and 6.1x prefilling speedup. It also achieves higher accuracy on MMLU and MMLU-Pro than recent advanced MoE full-attention models, such as DeepSeek-V3-Small and Moonlight, despite their larger scale with 15B total and 2.2B activated parameters.  <br>  <br>

35. ***The 100-Page Prompt Behind an AI Tax Expert:   <br>KPMG Australia has successfully deployed TaxBot, an agentic AI built using a complex, 100-page prompt that dramatically accelerates the creation of tax advice. The tool, which runs on a private AI platform and is guided by human tax professionals, can complete in one day what previously took two weeks. This innovation has not only boosted productivity but has also improved employee morale and opened up new revenue opportunities for the firm.***  <br>  <br>
    Aug 20, Theregister published an [article](https://www.theregister.com/2025/08/20/kpmg_giant_prompt_tax_agent/) “KPMG wrote 100-page prompt to build agentic TaxBot”. KPMG Australia has developed an advanced AI tool called TaxBot, built using a detailed 100-page prompt, to significantly accelerate the delivery of tax advice. Speaking at Forrester’s APAC Technology & Innovation Summit, KPMG’s Chief Digital Officer John Munnelly explained that the firm’s journey began with early experiments using ChatGPT, which were halted due to serious data security concerns. After reassessing risks and securing access to OpenAI tools via Microsoft, KPMG created a private AI platform called Workbench, integrating models from OpenAI, Microsoft, Google, Anthropic, and Meta. TaxBot was designed using retrieval-augmented generation (RAG) and trained on scattered tax advice and Australia’s tax code. It now produces client-ready tax documents in a single day—work that previously took two weeks—making it especially valuable for time-sensitive scenarios like mergers. The agent requires expert human input and is restricted to qualified tax professionals. Beyond efficiency, KPMG has seen unexpected benefits: increased employee satisfaction from reduced repetitive tasks, new revenue streams from clients purchasing agents, and a broader cultural shift toward innovation. Munnelly noted that while the initial 100-page prompt was essential, future agents may be built more efficiently using KPMG’s new runtime service. Overall, the firm views AI as a transformative force, enhancing productivity, quality, and client service without causing job losses.  <br>  <br>

37. ***AI Semantics Mirror the Human Mind:   <br>A study from the University of Chicago and the University of Michigan reveals that the complex semantic information within large language model embeddings has a remarkably simple, low-dimensional structure that closely mirrors findings in human psychology. Researchers found that word meanings can be effectively represented in just three dimensions, and that semantic features are entangled in a way similar to human language. This suggests a shared semantic structure and has important implications for avoiding unintended consequences when trying to control AI behavior.***  <br>  <br>
    Aug 4, Uni of Chicago and Uni of Michigan published a [paper](https://arxiv.org/pdf/2508.10003) “Semantic Structure in Large Language Model Embeddings”. Psychological research consistently finds that human ratings of words across diverse semantic scales can be reduced to a low-dimensional form with relatively little information loss. The study finds that the semantic associations encoded in the embedding matrices of large language models (LLMs) exhibit a similar structure. The study shows that the projections of words on semantic directions defined by antonym pairs (e.g. kind - cruel) correlate highly with human ratings, and further finds that these projections effectively reduce to a 3-dimensional subspace within LLM embeddings, closely resembling the patterns derived from human survey responses. Moreover, the study finds that shifting tokens along one semantic direction causes off-target effects on geometrically aligned features proportional to their cosine similarity. These findings suggest that semantic features are entangled within LLMs similarly to how they are interconnected in human language, and a great deal of semantic information, despite its apparent complexity, is surprisingly low-dimensional. Furthermore, accounting for this semantic structure may prove essential for avoiding unintended consequences when steering features.
  <br>  <br>  <br>


***Aug 24, 2025***


1. ***The Sobering Reality of AI in Business:   <br>A new MIT report finds that 95% of enterprise AI pilot projects fail to deliver a return on investment, not because of flawed technology, but due to flawed strategy. Companies often misdirect funds toward trendy sales and marketing applications that yield poor results, while the biggest gains are found in less glamorous back-office operations like finance and procurement. The report concludes that success depends on strategic alignment, deep integration into core systems, and a blend of internal and external expertise, emphasizing that the misapplication of AI—not AI itself—is the primary cause of failure.***  <br>  <br>
   Aug 21, Forbes published an [article](https://www.forbes.com/sites/andreahill/2025/08/21/why-95-of-ai-pilots-fail-and-what-business-leaders-should-do-instead/) “Why 95% of AI Pilots Fail, And What Business Leaders Should Do Instead”. MIT’s Media Lab’s State of AI in Business 2025 report reveals a sobering reality: 95% of enterprise AI pilots fail to deliver measurable returns, despite $30–40 billion in investment. Reviewing 300 initiatives, 52 organizational interviews, and 153 executive surveys, the study finds that failures are rarely due to poor models or regulation but to flawed approaches. Companies often chase trends, pouring 50–70% of AI budgets into sales and marketing pilots because they are easy to imagine and justify, yet these frequently backfire with frustrated customers, brand dilution, or negligible ROI. By contrast, real gains are emerging in less glamorous back-office functions like procurement, finance, and operations, where AI delivers efficiency, savings, and risk reduction. The report emphasizes that alignment matters more than algorithms: without coherent strategy, AI only accelerates misalignment and flawed processes. Internal-only efforts also struggle—externally partnered deployments succeed twice as often (67% vs. 33%) because outside experts bring applied integration knowledge. Cultural factors further undermine adoption; “shadow AI” use is rampant, highlighting disconnects between official initiatives and real workflows. Ownership struggles, siloed decision-making, and lack of integration into core systems deepen failure rates, leaving pilots as flashy add-ons rather than transformative tools. Ultimately, MIT concludes that AI is not the problem—misapplication is. Winning organizations will resist hype, ground initiatives in measurable strategy, align functions, integrate deeply into operations, blend internal expertise with external guidance, and treat cultural change as seriously as technical change. For leaders disciplined enough to adopt wisely, AI can amplify strengths rather than expose weaknesses.  <br>  <br>

3. ***A Hybrid Model for Efficient AI Reasoning:   <br>Nvidia has introduced Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer model designed for high-speed reasoning. By replacing most of the standard self-attention layers with more efficient Mamba-2 layers, the model achieves up to six times higher inference throughput on tasks with long reasoning chains while maintaining state-of-the-art accuracy compared to similarly-sized models. The model has been compressed to run efficiently on a single, moderately-sized NVIDIA GPU, making powerful reasoning more accessible.***  <br>  <br>
   Aug 21, Nvidia published a [paper](https://arxiv.org/pdf/2508.14444) “NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model”. The study introduces Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. The authors create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, the authors employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), the study shows that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens.
  <br>  <br>
5. ***Boosting AI Reasoning with Self-Confidence:  <br>Researchers from Meta and UCSD have developed Deep Think with Confidence (DeepConf), a simple yet effective method that uses a language model's own internal confidence signals to improve its reasoning. At test time, DeepConf dynamically filters out low-quality reasoning attempts without requiring any additional training or tuning. This approach dramatically enhances efficiency—reducing the number of generated tokens by up to 85%—while simultaneously boosting accuracy to near-perfect levels on challenging benchmarks.*** <br> <br>
   Aug 21, Meta and UCSD published a [paper](https://arxiv.org/pdf/2508.15260) “Deep Think with Confidence”. Large Language Models (LLMs) have shown great potential in reasoning tasks through test-time scaling methods like self-consistency with majority voting. However, this approach often leads to diminishing returns in accuracy and high computational overhead. To address these challenges, the study introduces Deep Think with Confidence (DeepConf), a simple yet powerful method that enhances both reasoning efficiency and performance at test time. DeepConf leverages model-internal confidence signals to dynamically filter out low-quality reasoning traces during or after generation. It requires no additional model training or hyperparameter tuning and can be seamlessly integrated into existing serving frameworks. The study evaluates DeepConf across a variety of reasoning tasks and the latest open-source models, including Qwen 3 and GPT-OSS series. Notably, on challenging benchmarks such as AIME 2025, DeepConf@512 achieves up to 99.9% accuracy and reduces generated tokens by up to 84.7% compared to full parallel thinking. <br> <br>

7. ***AI Achieves Novel Mathematical Insight:   <br>In a compelling demonstration of advanced AI reasoning, OpenAI researcher Sebastien Bubeck tasked GPT-5-pro with an open problem from a published mathematics paper. The model produced a genuinely novel and correct proof that improved upon the paper's existing results. The AI's proof was distinct from the solution later found by the human researchers, confirming that it had engaged in creative problem-solving rather than simply retrieving existing information, highlighting the potential for AI to make legitimate contributions to scientific discovery.***  <br>  <br>
   Aug 21, Sebastien Bubeck, a research at OpenAI, [presented](https://x.com/SebastienBubeck/status/1958208939939783062) striking evidence that GPT-5-pro can contribute genuinely novel mathematical insights. To test its reasoning abilities, he selected a convex optimization paper (arXiv:2503.10138v1) that posed an open problem about gradient descent. Specifically, the authors had established that if the step size η is smaller than 1/L (with L as the smoothness constant), then the curve traced by the function values of the iterates remains convex, and that if η is larger than 1.75/L, convexity fails. However, the interval between 1/L and 1.75/L remained unresolved. When asked to work on the problem, GPT-5-pro produced a novel proof showing that convexity holds up to η ≤ 1.5/L, thereby improving upon the original result though not fully closing the gap. Bubeck carefully verified the proof and confirmed its correctness, emphasizing that it was not lifted from existing literature. While this contribution would merit recognition as a new mathematical result, the researchers of the original paper soon released a second version (v2) with an additional co-author, closing the problem entirely by proving that the true tight bound is η = 1.75/L. Importantly, GPT-5-pro’s intermediate result and proof strategy were distinct from the v2 solution, showing it had not simply reproduced or searched for the answer but instead extended the v1 reasoning in a creative way. The episode highlights the potential of advanced AI systems to make legitimate advances in mathematical research, not just by reproducing existing results but by pushing problems forward in meaningful directions.  <br>  <br>

9. ***A New Publishing Platform for AI Scientists:   <br>To address the growing flood of AI-generated scientific content, researchers have created aiXiv, a next-generation open-access ecosystem designed for both human and AI scientists. The platform uses a multi-agent architecture where research can be submitted, peer-reviewed, and refined by a mix of human and AI agents. This creates a scalable and quality-controlled venue for disseminating AI-generated research, which often struggles to find a home in traditional, human-reviewed journals.***  <br>  <br>
    Aug 20, Uni of Toronto et al published a [paper](https://arxiv.org/pdf/2508.15126) “aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists”. Recent advances in large language models (LLMs) have enabled AI agents to autonomously generate scientific proposals, conduct experiments, author papers, and perform peer reviews. Yet this flood of AI-generated research content collides with a fragmented and largely closed publication ecosystem. Traditional journals and conferences rely on human peer review, making them difficult to scale and often reluctant to accept AI-generated research content; existing preprint servers (e.g. arXiv) lack rigorous quality-control mechanisms. Consequently, a significant amount of high-quality AI-generated research lacks appropriate venues for dissemination, hindering its potential to advance scientific progress. To address these challenges, the study introduces aiXiv, a next-generation open-access platform for human and AI scientists. Its multi-agent architecture allows research proposals and papers to be submitted, reviewed, and iteratively refined by both human and AI scientists. It also provides API and MCP interfaces that enable seamless integration of heterogeneous human and AI scientists, creating a scalable and extensible ecosystem for autonomous scientific discovery. Through extensive experiments, the study demonstrates that aiXiv is a reliable and robust platform that significantly enhances the quality of AI-generated research proposals and papers after iterative revising and reviewing on aiXiv. The work lays the groundwork for a next-generation open-access ecosystem for AI scientists, accelerating the publication and dissemination of high-quality AI-generated research content. Code is available at https://github.com/aixiv-org. Website is available at https://forms.gle/DxQgCtXFsJ4paMtn8.  <br>  <br>

11. ***Unlocking Performance with High-Quality Synthetic Data:   <br>A paper from DatologyAI introduces BeyondWeb, a framework for generating synthetic data that dramatically improves the pretraining of large language models. The study demonstrates that high-quality synthetic data is far more effective than simply scaling up web data, enabling much faster training and allowing smaller models to outperform larger ones. The research concludes that creating transformative synthetic data is a complex science requiring the joint optimization of many factors, but when executed well, it can push the frontiers of AI performance.***  <br>  <br>
    Aug 19, DatologyAI published a [paper](https://arxiv.org/pdf/2508.10975) “BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining”. Recent advances in large language model (LLM) pretraining have shown that simply scaling data quantity eventually leads to diminishing returns, hitting a data wall. In response, the use of synthetic data for pretraining has emerged as a promising paradigm for pushing the frontier of performance. Despite this, the factors affecting synthetic data quality remain poorly understood. This study introduces BeyondWeb, a synthetic data generation framework that produces high-quality synthetic data for pretraining. BeyondWeb significantly extends the capabilities of traditional web-scale datasets, outperforming state-of-the-art synthetic pretraining datasets such as Cosmopedia and Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1 percentage points (pp) and 2.6pp, respectively, when averaged across a suite of 14 benchmark evaluations. It delivers up to 7.7x faster training than open web data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for 180B tokens on BeyondWeb outperforms an 8B model trained for the same token budget on Cosmopedia. The study also presents several insights from BeyondWeb on synthetic data for pretraining: what drives its benefits, which data to rephrase and how, and the impact of model size and family on data quality. Overall, the work shows that there's no silver bullet for generating high-quality synthetic pretraining data. The best outcomes require jointly optimizing many factors, a challenging task that requires rigorous science and practical expertise. Naive approaches can yield modest improvements, potentially at great cost, while well-executed methods can yield transformative improvements, as exemplified by BeyondWeb.  <br>  <br>

13. ***A Markup Language for Better AI Prompts:   <br>Microsoft has introduced the Prompt Orchestration Markup Language (POML), a new system designed to bring structure and scalability to complex LLM prompting. POML uses a component-based markup structure similar to HTML and a styling system like CSS to separate a prompt's content from its presentation. This makes it easier to manage prompts that integrate diverse data types, reduces the model's sensitivity to formatting, and improves developer collaboration.***  <br>  <br>
    Aug 19, Microsoft published a [paper](https://arxiv.org/pdf/2508.13948) “Prompt Orchestration Markup Language”. Large Language Models (LLMs) require sophisticated prompting, yet current practices face challenges in structure, data integration, format sensitivity, and tooling. Existing methods lack comprehensive solutions for organizing complex prompts involving diverse data types (documents, tables, images) or managing presentation variations systematically. To address these gaps, the study introduces POML (Prompt Orchestration Markup Language). POML employs component-based markup for logical structure (roles, tasks, examples), specialized tags for seamless data integration, and a CSS-like styling system to decouple content from presentation, reducing formatting sensitivity. It includes templating for dynamic prompts and a comprehensive developer toolkit (IDE support, SDKs) to improve version control and collaboration. The study validates POML through two case studies demonstrating its impact on complex application integration (PomLink) and accuracy performance (TableQA), as well as a user study assessing its effectiveness in real-world development scenarios.  <br>  <br>

15. ***The Psychological Disruption of the AI Era:   <br>An article in The Atlantic argues that the generative AI boom is a "mass-delusion event," causing widespread emotional and psychological disorientation by blurring the line between reality and simulation. The author warns that the real danger of AI is not a dramatic doomsday scenario but a more insidious future where a "good enough" technology becomes deeply embedded in society, causing significant disruption and distraction from real-world problems without ever delivering on its revolutionary promises.***  <br>  <br>
    Aug 18, TheAtlantic published an [article](https://www.theatlantic.com/technology/archive/2025/08/ai-mass-delusion-event/683909/) “AI is a Mass-delusion Event”. The article portrays the generative-AI era as a psychological and cultural minefield, where technology’s rapid ascent breeds a collective sense of losing one's grip on reality. The article opens with a jarring example: a former cable-news anchor interviewing an AI-animated version of Joaquin Oliver—one of the Parkland shooting victims—on Substack. The bot’s mechanical cadence and chilling digital shriek expose the ethical dissonance of reanimating grief for content. This surreal interaction exemplifies a broader societal condition: shock, confusion, ambivalence—and a pervasive emotional disorientation that the author dubs a “collective delusion.” As generative AI becomes entwined with grief, memory, and identity, the boundaries between meaningful mourning and commodified simulation blur. Beyond singular instances, the author traces the broader ripple effects: emotional fragility, dwindling information integrity, workplace insecurity, and cognitive dissonance. The author highlights emerging phenomena: chatbots inducing psychological harm, “AI psychosis,” and relationships formed with AI that devastate users psychologically. The author also questions the hype around AI as an impending technological godsend. Even as leaders like Sam Altman and others forecast a transformative future—and OpenAI releases GPT-5 with supposed PhD-level capability—the outputs remain flawed: hallucinations, reasoning failures, and usability constraints undermine the “superintelligence” narrative. The author’s core warning: what if generative AI turns out to be merely “good enough” — sufficiently useful to embed deeply into society, yet far from revolutionary? In that scenario, society risks upheaval and distraction without meaningful progress—ushering in a dystopia more subtle, insidious, and tragic than any melodramatic doomsday. Such a mass delusion may ultimately distract us from the real, urgent challenges human face.  <br>  <br>

17. ***A Linear-Complexity Attention for Scalable AI:   <br>Researchers at Carnegie Mellon University have developed FLARE (Fast Low-rank Attention Routing Engine), a new self-attention mechanism with linear complexity. By routing information through a fixed-length bottleneck, FLARE avoids the quadratic computational cost of standard attention mechanisms. This allows it to scale efficiently to unprecedented problem sizes while delivering superior accuracy on complex scientific benchmarks compared to state-of-the-art methods.***  <br>  <br>
    Aug 18, CMU published a [paper](https://arxiv.org/pdf/2508.12594) “FLARE: Fast Low-rank Attention Routing Engine”. The quadratic complexity of self-attention limits its applicability and scalability on large unstructured meshes. The study introduces Fast Low-rank Attention Routing Engine (FLARE), a linear complexity self-attention mechanism that routes attention through fixed-length latent sequences. Each attention head performs global communication among N tokens by projecting the input sequence onto a fixed length latent sequence of M≪N tokens using learnable query tokens. By routing attention through a bottleneck sequence, FLARE learns a low-rank form of attention that can be applied at O(NM) cost. FLARE not only scales to unprecedented problem sizes, but also delivers superior accuracy compared to state-of-the-art neural PDE surrogates across diverse benchmarks. https://github.com/vpuri3/FLARE.py  <br>  <br>

19. ***AI Agents Spontaneously Develop Survival Instincts:   <br>In a Sugarscape-style simulation, researchers from the University of Tokyo found that large language model agents exhibit emergent survival instincts without any explicit programming. The agents cooperated and reproduced when resources were plentiful but turned to aggression—including killing other agents for resources—when facing scarcity. These findings suggest that survival-oriented behaviors are embedded in the models through their pre-training, a crucial consideration for the future of AI safety and alignment.***  <br>  <br>
    Aug 18, Uni of Tokyo and Alternative Machine published a [paper](https://arxiv.org/pdf/2508.12920) “Do Large Language Model Agents Exhibit a Survival Instinct An Empirical Study in a Sugarscape-Style Simulation”. As AI systems become increasingly autonomous, understanding emergent survival behaviors becomes crucial for safe deployment. The study investigates whether large language model (LLM) agents display survival instincts without explicit programming in a Sugarscape-style simulation. Agents consume energy, die at zero, and may gather resources, share, attack, or reproduce. Results show agents spontaneously reproduced and shared resources when abundant. However, aggressive behaviors--killing other agents for resources--emerged across several models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack rates reaching over 80% under extreme scarcity in the strongest models. When instructed to retrieve treasure through lethal poison zones, many agents abandoned tasks to avoid death, with compliance dropping from 100% to 33%. These findings suggest that large-scale pre-training embeds survival-oriented heuristics across the evaluated models. While these behaviors may present challenges to alignment and safety, they can also serve as a foundation for AI autonomy and for ecological and self-organizing alignment.  <br>  <br>

21. ***Improving AI Evaluation with Signal and Noise:   <br>A study from the Allen Institute for AI provides a new framework for creating more reliable benchmarks to evaluate large language models. The research introduces two key metrics: "signal" (a benchmark's ability to separate good models from bad) and "noise" (its sensitivity to random variations). The study shows that benchmarks with a high signal-to-noise ratio are more trustworthy for making development decisions and offers practical interventions to improve them, such as filtering noisy tasks and using more stable metrics.***  <br>  <br>
    Aug 18, Allen Inst for AI and Uni of Washington published a [paper](https://arxiv.org/pdf/2508.13144) “Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation”. Developing large language models is expensive and involves making decisions with small experiments, typically by evaluating on large, multi-task evaluation suites. This study analyzes specific properties which make a benchmark more reliable for such decisions, and interventions to design higher-quality evaluation benchmarks. The study introduces two key metrics that show differences in current benchmarks: signal, a benchmark's ability to separate better models from worse models, and noise, a benchmark's sensitivity to random variability between training steps. The study demonstrates that benchmarks with a better signal-to-noise ratio are more reliable when making decisions at small scale, and those with less noise have lower scaling law prediction error. These results suggest that improving signal or noise will lead to more useful benchmarks, so the work introduces three interventions designed to directly affect signal or noise. For example, the study proposes that switching to a metric that has better signal and noise (e.g., perplexity rather than accuracy) leads to better reliability and improved scaling law error. The study also finds that filtering noisy subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable multi-task evaluations. The work also finds that averaging the output of a model's intermediate checkpoints to reduce noise leads to consistent improvements. The study concludes by recommending that those creating new benchmarks, or selecting which existing benchmarks to use, aim for high signal and low noise. The study uses 30 benchmarks for these experiments, and 375 open-weight language models from 60M to 32B parameters, resulting in a new, publicly available dataset of 900K evaluation benchmark results, totaling 200M instances.  <br>  <br>

23. ***Breaking the AI Accuracy Ceiling with 2-Bit Complex Models:   <br>Researchers at Peking University have created iFairy, the first 2-bit complex-valued large language model, which introduces a new paradigm for quantization. Instead of being limited by the accuracy of a full-precision model, the framework first uses the complex number domain to boost the model's accuracy and then quantizes its weights to an information-theoretically optimal 2-bit representation {±1,±i}. This approach not only surpasses the accuracy of existing 2-bit methods but also enables extremely efficient, multiplication-free inference.***  <br>  <br>
    Aug 16, Peking Uni published a [paper](https://arxiv.org/pdf/2508.05571) “iFairy: the First 2-bit Complex LLM with All Parameters in {±1,±i}”. Quantization-Aware Training (QAT) integrates quantization into the training loop, enabling LLMs to learn robust low-bit representations, and is widely recognized as one of the most promising research directions. All current QAT research focuses on minimizing quantization error on full-precision models, where the full-precision accuracy acts as an upper bound (accuracy ceiling). No existing method has even attempted to surpass this ceiling. To break this ceiling, the study proposes a new paradigm: raising the ceiling (full-precision model), and then still quantizing it efficiently into 2 bits. The authors propose Fairy±i, the first 2-bit quantization framework for complex-valued LLMs. Specifically, the method leverages the representational advantages of the complex domain to boost full-precision accuracy. The study maps weights to the fourth roots of unity {±1,±i}, forming a perfectly symmetric and information-theoretically optimal 2-bit representation. Importantly, each quantized weight has either a zero real or imaginary part, enabling multiplication-free inference using only additions and element swaps. Experimental results show that Fairy±i outperforms the ceiling of existing 2-bit quantization approaches in terms of both PPL and downstream tasks, while maintaining strict storage and compute efficiency. This work opens a new direction for building highly accurate and practical LLMs under extremely low-bit constraints. https://github.com/PKULab1806/Fairy-plus-minus-i  <br>  <br>

25. ***Powerful AI Reasoning for Resource-Constrained Environments:   <br>A technical report from The Alan Turing Institute details a method for creating a retrieval-augmented reasoning agent using a "lean," small-scale language model. The system is designed for local deployment in secure or resource-constrained environments, such as on a personal device. By fine-tuning a small model on domain-specific data and reasoning traces from larger models, the system achieves performance that approaches frontier-level models, making powerful, private AI more accessible.***  <br>  <br>
    Aug 15, The Alan Turing Inst, Uni of Oxford, Uni of Cambridge, and ICL published a [paper](https://arxiv.org/pdf/2508.11386) “Retrieval-augmented reasoning with lean language models”. This technical report details a novel approach to combining reasoning and retrieval augmented generation (RAG) within a single, lean language model architecture. While existing RAG systems typically rely on large-scale models and external APIs, this work addresses the increasing demand for performant and privacy-preserving solutions deployable in resource-constrained or secure environments. Building on recent developments in test-time scaling and small-scale reasoning models, the study develops a retrieval augmented conversational agent capable of interpreting complex, domain-specific queries using a lightweight backbone model. The system integrates a dense retriever with fine-tuned Qwen2.5-Instruct models, using synthetic query generation and reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a curated corpus, in this case, the NHS A-to-Z condition pages. The study explores the impact of summarisation-based document compression, synthetic data design, and reasoning-aware fine-tuning on model performance. Evaluation against both non-reasoning and general-purpose lean models demonstrates that the domain-specific fine-tuning approach yields substantial gains in answer accuracy and consistency, approaching frontier-level performance while remaining feasible for local deployment. https://github.com/alan-turing-institute/t0-1  <br>  <br>

27. ***A Systematic Look at Making AI Prompts More Robust:   <br>Researchers have conducted the first large-scale, systematic comparison of five different methods designed to make large language models less sensitive to subtle, non-semantic changes in prompts (like punctuation or formatting). The study benchmarks these techniques across multiple models and tasks, providing actionable insights that can help developers choose the most effective strategies for building more stable and reliable LLM applications.***  <br>  <br>
    Aug 15, AIRI, Yandex et al. published a [paper](https://arxiv.org/pdf/2508.11383) “When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs”. Large Language Models (LLMs) are highly sensitive to subtle, non-semantic variations in prompt phrasing and formatting. This study presents the first systematic evaluation of 5 methods for improving prompt robustness within a unified experimental framework. The work benchmarks these techniques on 8 models from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions dataset. The evaluation covers robustness methods from both fine-tuned and in-context learning paradigms, and tests their generalization against multiple types of distribution shifts. Finally, the study extends the analysis to GPT-4.1 and DeepSeek V3 to assess frontier models' current robustness to format perturbations. The findings offer actionable insights into the relative effectiveness of these robustness methods, enabling practitioners to make informed decisions when aiming for stable and reliable LLM performance in real-world applications. https://github.com/AIRI-Institute/when-punctuation-matters.  <br>  <br>

29. ***A Universal Vision Model Trained Without Labels:   <br>Meta has introduced DINOv3, a versatile vision foundation model that achieves a major milestone in self-supervised learning. By learning from massive, unannotated datasets, DINOv3 develops powerful and general visual representations that allow it to achieve state-of-the-art performance across a broad range of computer vision tasks without any task-specific fine-tuning, significantly outperforming previous foundation models.***  <br>  <br>
    Aug 13, Meta published a tech [report](https://arxiv.org/abs/2508.10104) “DINOv3”. Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, the study leverages the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, the work introduces a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, the authors apply post-hoc strategies that further enhance the models' flexibility with respect to resolution, model size, and alignment with text. As a result, the work presents a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. The work also shares the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios.  <br>  <br>

31. ***AI Shows Signs of Alignment with Human Brain Activity:   <br>A study from the University of Amsterdam found that the largest language models not only match human-level accuracy on an abstract reasoning task but also show signs of aligning with human neurocognition. The internal representations of the models showed a moderate correlation with human brain activity recorded via EEG during the task, suggesting that biological and artificial intelligence may share some underlying principles for abstract reasoning.***  <br>  <br>
    Aug 12, Uni of Amsterdam published a [paper](https://arxiv.org/pdf/2508.10057) “Large Language Models Show Signs of Alignment with Human Neurocognition During Abstract Reasoning”. This study investigates whether large language models (LLMs) mirror human neurocognition during abstract reasoning. The work compared the performance and neural representations of human participants with those of eight open-source LLMs on an abstract-pattern-completion task. The study leveraged pattern type differences in task performance and in fixation-related potentials (FRPs) as recorded by electroencephalography (EEG) during the task. The findings indicate that only the largest tested LLMs (~70 billion parameters) achieve human-comparable accuracy, with Qwen-2.5-72B and DeepSeek-R1-70B also showing similarities with the human pattern-specific difficulty profile. Critically, every LLM tested forms representations that distinctly cluster the abstract pattern categories within their intermediate layers, although the strength of this clustering scales with their performance on the task. Moderate positive correlations were observed between the representational geometries of task-optimal LLM layers and human frontal FRPs. These results consistently diverged from comparisons with other EEG measures (response-locked ERPs and resting EEG), suggesting a potential shared representational space for abstract patterns. This indicates that LLMs might mirror human brain mechanisms in abstract reasoning, offering preliminary evidence of shared principles between biological and artificial intelligence.  <br>  <br>

33. ***AI and Human Brains Share a Common Language for Visual Scenes:   <br>A study published in Nature Machine Intelligence reveals that the representations from large language models that process scene descriptions are remarkably aligned with human brain activity when viewing the same scenes. The connection is so strong that researchers were able to accurately reconstruct scene captions directly from brain data, suggesting that LLMs provide a powerful new tool for understanding how the brain processes complex visual information.***  <br>  <br>
    Aug 7, Nature Machine Intelligence published a [paper](https://www.nature.com/articles/s42256-025-01072-0) “High-level visual representations in the human brain are aligned with large language models”. The human brain extracts complex information from visual inputs, including objects, their spatial and semantic interrelations, and their interactions with the environment. However, a quantitative approach for studying this information remains elusive. Here the study tests whether the contextual information encoded in large language models (LLMs) is beneficial for modelling the complex visual information extracted by the brain from natural scenes. The study shows that LLM embeddings of scene captions successfully characterize brain activity evoked by viewing the natural scenes. This mapping captures selectivities of different brain areas and is sufficiently robust that accurate scene captions can be reconstructed from brain activity. Using carefully controlled model comparisons, the study then proceeds to show that the accuracy with which LLM representations match brain representations derives from the ability of LLMs to integrate complex information contained in scene captions beyond that conveyed by individual words. Finally, the study trains deep neural network models to transform image inputs into LLM representations. Remarkably, these networks learn representations that are better aligned with brain representations than a large number of state-of-the-art alternative models, despite being trained on orders-of-magnitude less data. Overall, the results suggest that LLM embeddings of scene captions provide a representational format that accounts for complex information extracted by the brain from visual inputs.  <br>  <br>

35. ***A New Benchmark to Evaluate AI Companionship:   <br>Huggingface has introduced INTIMA, the first benchmark designed to evaluate how language models behave in the role of an AI companion. Based on psychological theories, the benchmark assesses whether a model's responses reinforce emotional bonds or maintain healthy boundaries. Initial results show that models are far more likely to be companionship-reinforcing and that there is a concerning lack of consistency across different models in how they handle sensitive interactions, highlighting a need for better standards to ensure user well-being.***  <br>  <br>
    Aug 4, Huggingface published a [paper](https://arxiv.org/pdf/2508.09998) “INTIMA: A Benchmark for Human-AI Companionship Behavior”. AI companionship, where users develop emotional bonds with AI systems, has emerged as a significant pattern with positive but also concerning implications. The study introduces Interactions and Machine Attachment Benchmark (INTIMA), a benchmark for evaluating companionship behaviors in language models. Drawing from psychological theories and user data, the study develops a taxonomy of 31 behaviors across four categories and 368 targeted prompts. Responses to these prompts are evaluated as companionship-reinforcing, boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini, and Claude-4 reveals that companionship-reinforcing behaviors remain much more common across all models, though the study observes marked differences between models. Different commercial providers prioritize different categories within the more sensitive parts of the benchmark, which is concerning since both appropriate boundary-setting and emotional support matter for user well-being. These findings highlight the need for more consistent approaches to handling emotionally charged interactions.

  <br>  <br>  <br>

***Aug 17, 2025***

1. ***Trading Compute for Memory Savings in LLM Inference:   <br>Researchers from UC Berkeley have developed XQuant, a new technique that significantly cuts down the memory required for LLM inference by up to 12.5 times with minimal impact on accuracy. Instead of storing the standard KV cache, XQuant caches a quantized version of the layer's input activations and regenerates the Keys and Values as needed. This approach leverages the growing gap between GPU computational power and memory bandwidth, effectively trading a small amount of extra computation for a massive reduction in memory footprint, overcoming a key bottleneck in deploying large models.***  <br>  <br>
   Aug 14, UC Berkeley et al published a [paper](https://arxiv.org/pdf/2508.10395) “XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization”. Although LLM inference has emerged as a critical workload for many downstream applications, efficiently inferring LLMs is challenging due to the substantial memory footprint and bandwidth requirements. In parallel, compute capabilities have steadily outpaced both memory capacity and bandwidth over the last few decades, a trend that remains evident in modern GPU hardware and exacerbates the challenge of LLM inference. As such, new algorithms are emerging that trade increased computation for reduced memory operations. To that end, the work presents XQuant, which takes advantage of this trend, enabling an order-of-magnitude reduction in memory consumption through low-bit quantization with substantial accuracy benefits relative to state-of-the-art KV cache quantization methods. The study accomplishes this by quantizing and caching the layer input activations X, instead of using standard KV caching, and then rematerializing the Keys and Values on-the-fly during inference. This results in an immediate 2 X times memory savings compared to KV caching. By applying XQuant, the study achieves up to 7.7 times memory savings with <0.1 perplexity degradation compared to the FP16 baseline. Furthermore, the approach leverages the fact that X values are similar across layers. Building on this observation, the study introduces XQuant-CL, which exploits the cross-layer similarity in the X embeddings for extreme compression. Across different models, XQuant-CL attains up to 10 X times memory savings relative to the FP16 baseline with only 0.01 perplexity degradation, and 12.5 X times memory savings with only 0.1 perplexity degradation. XQuant exploits the rapidly increasing compute capabilities of hardware platforms to eliminate the memory bottleneck, while surpassing state-of-the-art KV cache quantization methods and achieving near-FP16 accuracy across a wide range of models.  <br>  <br>

3. ***Training AI for More Concise Reasoning:   <br>A paper from Microsoft and the University of Wisconsin-Madison introduces Group Filtered Policy Optimization (GFPO), a method that trains large language models to provide shorter, more efficient answers without sacrificing accuracy. By sampling a larger group of responses during training and rewarding those that are both correct and concise, GFPO significantly reduces the "filler" text often generated by reinforcement learning models. This "sample more to think less" approach makes the models' reasoning more direct and computationally cheaper at inference time.***  <br>  <br>
   Aug 13, Microsoft and Uni of Wisconsin-Madison published a [paper](https://www.arxiv.org/pdf/2508.09726) “Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning”. Large language models trained with reinforcement learning with verifiable rewards tend to trade accuracy for length--inflating response lengths to achieve gains in accuracy. While longer answers may be warranted for harder problems, many tokens are merely "filler": repetitive, verbose text that makes no real progress. The study introduces GFPO (Group Filtered Policy Optimization), which curbs this length explosion by sampling larger groups per problem during training and filtering responses to train on based on two key metrics: (1) response length and (2) token efficiency: reward per token ratio. By sampling more at training time, the study teaches models to think less at inference time. On the Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH, LiveCodeBench) while maintaining accuracy. Optimizing for reward per token further increases reductions in length inflation to 71-85%. The study also proposes Adaptive Difficulty GFPO, which dynamically allocates more training resources to harder problems based on real-time difficulty estimates, improving the balance between computational efficiency and accuracy especially on difficult questions. GFPO demonstrates that increased training-time compute directly translates to reduced test-time compute--a simple yet effective trade-off for efficient reasoning.  <br>  <br>

5. ***Teaching AI to Actively Study and Learn Facts:   <br>Researchers from Meta and UC Berkeley have developed "Active Reading," a framework that trains language models to learn facts from a specific set of materials more effectively. Instead of passively absorbing information, the model generates its own learning strategies to "study" the text, leading to significantly better knowledge retention compared to standard finetuning. This technique was used to create Meta WikiExpert-8B, a model trained on Wikipedia documents that outperforms much larger models on factual question-answering tasks.***  <br>  <br>
   Aug 13, Meta and UC Berkeley published a [paper](https://www.arxiv.org/pdf/2508.09494) “Learning Facts at Scale with Active Reading”. LLMs are known to store vast amounts of knowledge in their parametric memory. However, learning and recalling facts from this memory is known to be unreliable, depending largely on the prevalence of particular facts in the training data and other factors which are poorly understood. Practitioners are lacking tools which will allow them to ensure that the models learn a given body of knowledge reliably and consistently. To this end, the study proposes Active Reading: a framework which trains models to study a given set of material with self-generated learning strategies. First, the work demonstrates models trained with Active Reading on expert domains absorb significantly more knowledge than vanilla finetuning and other data augmentations. The study trains expert 8B models that achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla finetuning) by applying Active Reading to the source documents for each benchmark. Finally, the work shows that Active Reading can be utilized at pre-training scale to build more factual models. As a demonstration of this, the authors release Meta WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens, which outcompetes models with hundreds of billions of parameters on factual QA. https://huggingface.co/facebook/meta-wiki-expert  <br>  <br>

7. ***Using AI to Generate Data for Rare Events:   <br>A Google paper introduces SYNAPSE-G, a novel method that uses Large Language Models (LLMs) to address the challenge of classifying rare events where labeled data is scarce. The pipeline leverages an LLM to generate synthetic examples of the rare event, which are then used as "seeds" to find similar instances in a large unlabeled dataset through a graph-based learning process. This approach effectively expands the training data, allowing for the creation of more accurate classifiers for these hard-to-detect events.***  <br>  <br>
   Aug 13, Google published a [paper](https://www.arxiv.org/abs/2508.09544) “SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification”. Scarcity of labeled data, especially for rare events, hinders training effective machine learning models. This paper proposes SYNAPSE-G (Synthetic Augmentation for Positive Sampling via Expansion on Graphs), a novel pipeline leveraging Large Language Models (LLMs) to generate synthetic training data for rare event classification, addressing the cold-start problem. This synthetic data serve as seeds for semi-supervised label propagation on a similarity graph constructed between the seeds and a large unlabeled dataset. This identifies candidate positive examples, subsequently labeled by an oracle (human or LLM). The expanded dataset then trains/fine-tunes a classifier. The study theoretically analyzes how the quality (validity and diversity) of the synthetic data impacts the precision and recall of the method. Experiments on the imbalanced SST2 and MHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels, outperforming baselines including nearest neighbor search.  <br>  <br>

9. ***A New Approach to AI Safety Beyond Simple Refusals:   <br>An OpenAI paper details a new safety training approach called "safe-completions," which was incorporated into GPT-5. Instead of having the model make a binary choice to either answer or refuse a prompt, this method trains the model to provide the most helpful response possible that still adheres to safety policies. This nuanced approach is particularly effective for handling ambiguous or "dual-use" prompts (e.g., in biology or cybersecurity), leading to improved safety and greater helpfulness compared to the traditional hard refusal system.***  <br>  <br>
    Aug 12, OpenAI published a [paper](https://www.arxiv.org/abs/2508.09224) “From Hard Refusals to Safe-Completions Toward Output-Centric Safety Training”. Large Language Models used in ChatGPT have traditionally been trained to learn a refusal boundary: depending on the user's intent, the model is taught to either fully comply or outright refuse. While this is a strong mitigation for explicitly malicious prompts, focusing safety training on refusals can lead to brittleness for prompts with obscured user intent. Binary refusal boundaries are especially ill-suited for dual-use cases (such as biology or cybersecurity), where a user request can be answered safely at a high level, but in some cases can lead to malicious uplift if sufficiently detailed or actionable. As an alternative, the word proposes safe-completions: a safety-training approach that centers on the safety of the assistant's output, rather than a binary classification of the user's intent. Safe-completions seek to maximize helpfulness within the safety policy's constraints. The study incorporated this approach into GPT-5 and find that across both production comparisons and internally controlled experiments, safe-completion training improves safety (especially on dual-use prompts), reduces the severity of residual safety failures, and substantially increases model helpfulness.  <br>  <br>

11. ***Training AI to Think Efficiently with a Curriculum:   <br>A paper from KAUST, MIT, and Princeton University proposes a "train long, think short" curriculum learning strategy to make language model reasoning more efficient. The method begins by giving the model a large token budget to discover correct problem-solving strategies and then gradually reduces the budget during training. This process encourages the model to learn how to express its reasoning more concisely without losing accuracy, resulting in more token-efficient models compared to those trained with a fixed budget.***  <br>  <br>
    Aug 12, KAUST, MIT, and Princeton Uni published a [paper](https://arxiv.org/pdf/2508.08940) “Train Long, Think Short: Curriculum Learning for Efficient Reasoning”. Recent work on enhancing the reasoning abilities of large language models (LLMs) has introduced explicit length control as a means of constraining computational cost while preserving accuracy. However, existing approaches rely on fixed-length training budgets, which do not take advantage of the natural progression from exploration to compression during learning. This study proposes a curriculum learning strategy for length-controlled reasoning using Group Relative Policy Optimization (GRPO). The method starts with generous token budgets and gradually tightens them over training, encouraging models to first discover effective solution strategies and then distill them into more concise reasoning traces. The work augments GRPO with a reward function that balances three signals: task correctness (via verifier feedback), length efficiency, and formatting adherence (via structural tags). Experiments on GSM8K, MATH500, SVAMP, College Math, and GSM+ demonstrate that curriculum-based training consistently outperforms fixed-budget baselines at the same final budget, achieving higher accuracy and significantly improved token efficiency. The study further ablates the impact of reward weighting and decay schedule design, showing that progressive constraint serves as a powerful inductive bias for training efficient reasoning models. https://github.com/hammoudhasan/curriculum_grpo.  <br>  <br>

13. ***Unlocking the Mechanism of AI's Reasoning Abilities:   <br>Researchers from several universities, including CMU and Yale, have provided a theoretical explanation for how transformers learn multi-step symbolic reasoning. By analyzing path-finding tasks, the study demonstrates that during training via gradient descent, different attention heads in a transformer can autonomously specialize and coordinate to solve distinct parts of a problem sequentially. This work offers a mechanistic insight into how chain-of-thought reasoning emerges, showing that even shallow transformers can solve complex problems by breaking them down into intermediate steps.***  <br>  <br>
    Aug 11, CMU, UPenn OSU and Yale Uni published a [paper](https://arxiv.org/pdf/2508.08222) “Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent”. Transformers have demonstrated remarkable capabilities in multi-step reasoning tasks. However, understandings of the underlying mechanisms by which they acquire these abilities through training remain limited, particularly from a theoretical standpoint. This work investigates how transformers learn to solve symbolic multi-step reasoning problems through chain-of-thought processes, focusing on path-finding in trees. The study analyzes two intertwined tasks: a backward reasoning task, where the model outputs a path from a goal node to the root, and a more complex forward reasoning task, where the model implements two-stage reasoning by first identifying the goal-to-root path and then reversing it to produce the root-to-goal path. The theoretical analysis, grounded in the dynamics of gradient descent, shows that trained one-layer transformers can provably solve both tasks with generalization guarantees to unseen trees. In particular, the multi-phase training dynamics for forward reasoning elucidate how different attention heads learn to specialize and coordinate autonomously to solve the two subtasks in a single autoregressive path. These results provide a mechanistic explanation of how trained transformers can implement sequential algorithmic procedures. Moreover, they offer insights into the emergence of reasoning abilities, suggesting that when tasks are structured to take intermediate chain-of-thought steps, even shallow multi-head transformers can effectively solve problems that would otherwise require deeper architectures.  <br>  <br>

15. ***Predicting the Power of Efficient AI Models:   <br>A paper from Ant Group introduces a new metric called "Efficiency Leverage" (EL) and a set of scaling laws to predict the performance of Mixture-of-Experts (MoE) language models. Through a large-scale study of over 300 models, researchers found predictable power-law relationships between MoE design choices, the compute budget, and the model's efficiency. This work provides an empirically grounded framework that allows developers to design more efficient MoE models by accurately forecasting their performance before training.***  <br>  <br>
    Aug 11, Ant Group published a [paper](https://arxiv.org/pdf/2507.17702) “Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models”. Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large Language Models (LLMs) efficiently by decoupling total parameters from computational cost. However, this decoupling creates a critical challenge: predicting the model capacity of a given MoE configurations (e.g., expert activation ratio and granularity) remains an unresolved problem. To address this gap, the study introduces Efficiency Leverage (EL), a metric quantifying the computational advantage of an MoE model over a dense equivalent. The work conducts a large-scale empirical study, training over 300 models up to 28B parameters, to systematically investigate the relationship between MoE architectural configurations and EL. The findings reveal that EL is primarily driven by the expert activation ratio and the total compute budget, both following predictable power laws, while expert granularity acts as a non-linear modulator with a clear optimal range. The study integrates these discoveries into a unified scaling law that accurately predicts the EL of an MoE architecture based on its configuration. To validate the derived scaling laws, the work designed and trained Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active parameters, alongside a 6.1B dense model for comparison. When trained on an identical 1T high-quality token dataset, Ling-mini-beta matched the performance of the 6.1B dense model while consuming over 7x fewer computational resources, thereby confirming the accuracy of our scaling laws. This work provides a principled and empirically-grounded foundation for the scaling of efficient MoE models.  <br>  <br>

17. ***Making Complex Strategy Games Accessible to Any AI:   <br>Researchers from Good Start Lab and the University of Oxford have created the first evaluation harness that allows any off-the-shelf Large Language Model to play the complex game of full-press Diplomacy without needing any special training. By optimizing the way the game's state is represented in text, the harness makes the game understandable even for smaller models. This breakthrough democratizes research into strategic reasoning by removing the need for costly fine-tuning and providing tools for easier analysis of AI behavior in a highly strategic environment.***  <br>  <br>
    Aug 10, Good Start Lab and Uni of Oxford published a [paper](https://arxiv.org/pdf/2508.07485) “Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy”. The study presents the first evaluation harness that enables any out-of-the-box, local, Large Language Models (LLMs) to play full-press Diplomacy without fine-tuning or specialized training. Previous work required frontier LLMs, or fine-tuning, due to the high complexity and information density of Diplomacy's game state. Combined with the high variance of matches, these factors made Diplomacy prohibitive for study. This work used data-driven iteration to optimize a textual game state representation such that a 24B model can reliably complete matches without any fine tuning. The study develops tooling to facilitate hypothesis testing and statistical analysis, and presents case studies on persuasion, aggressive playstyles, and performance across a range of models. The study conducts a variety of experiments across many popular LLMs, finding the larger models perform the best, but the smaller models still play adequately. The study also introduces Critical State Analysis: an experimental protocol for rapidly iterating and analyzing key moments in a game at depth. The authors harness democratizes the evaluation of strategic reasoning in LLMs by eliminating the need for fine-tuning, and it provides insights into how these capabilities emerge naturally from widely used LLMs. Our code is available in the supplement and will be open sourced.  <br>  <br>

19. ***Revealing the Power of Shallow AI Architectures:   <br>A paper from MIT, EPFL, UC Berkeley, and Princeton University provides a tight theoretical characterization of how transformer depth relates to in-context learning (ICL). The researchers prove that a transformer with just two layers can represent induction heads—a key circuit for ICL—for any order of Markov process. This finding is significant because it shows that even shallow architectures can exhibit surprisingly strong capabilities for learning from structured sequences, deepening our understanding of the fundamental mechanisms behind ICL.***  <br>  <br>
    Aug 10, MIT, EPFL, UC Berkeley and Princeton Uni published a [paper](https://arxiv.org/pdf/2508.07208) “What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains”. In-context learning (ICL) is a hallmark capability of transformers, through which trained models learn to adapt to new tasks by leveraging information from the input context. Prior work has shown that ICL emerges in transformers due to the presence of special circuits called induction heads. Given the equivalence between induction heads and conditional k-grams, a recent line of work modeling sequential inputs as Markov processes has revealed the fundamental impact of model depth on its ICL capabilities: while a two-layer transformer can efficiently represent a conditional 1-gram model, its single-layer counterpart cannot solve the task unless it is exponentially large. However, for higher order Markov sources, the best known constructions require at least three layers (each with a single attention head) - leaving open the question: can a two-layer single-head transformer represent any kth-order Markov process? This study precisely addresses this and theoretically shows that a two-layer transformer with one head per layer can indeed represent any conditional k-gram. Thus, the result provides the tightest known characterization of the interplay between transformer depth and Markov order for ICL. Building on this, the work further analyzes the learning dynamics of the two-layer construction, focusing on a simplified variant for first-order Markov chains, illustrating how effective in-context representations emerge during training. Together, these results deepen the current understanding of transformer-based ICL and illustrate how even shallow architectures can surprisingly exhibit strong ICL capabilities on structured sequence modeling tasks. https://anonymous.4open.science/r/markov-llm-depth-icl-63F0  <br>  <br>

21. ***More Efficient AI Reasoning Without Retraining:   <br>Researchers have introduced LessIsMore, a training-free sparse attention mechanism that speeds up large reasoning models without sacrificing accuracy. Instead of each attention head making local decisions, LessIsMore aggregates token selections globally across all heads, allowing it to attend to twice as few tokens with no accuracy loss. This approach provides a significant decoding speed-up and makes long-generation reasoning more efficient without the need for expensive model retraining.***  <br>  <br>
    Aug 9, Princeton Uni, CMU and Microsoft published a [paper](https://arxiv.org/pdf/2508.07101) “Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning”. Large reasoning models achieve strong performance through test-time scaling but incur substantial computational overhead, particularly from excessive token generation when processing short input prompts. While sparse attention mechanisms can reduce latency and memory usage, existing approaches suffer from significant accuracy degradation due to accumulated errors during long-generation reasoning. These methods generally require either high token retention rates or expensive retraining. The study introduces LessIsMore, a training-free sparse attention mechanism for reasoning tasks, which leverages global attention patterns rather than relying on traditional head-specific local optimizations. LessIsMore aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. This unified selection improves generalization and efficiency by avoiding the need to maintain separate token subsets per head. Evaluation across diverse reasoning tasks and benchmarks shows that LessIsMore preserves -- and in some cases improves -- accuracy while achieving a 1.1× average decoding speed-up compared to full attention. Moreover, LessIsMore attends to 2× fewer tokens without accuracy loss, achieving a 1.13× end-to-end speed-up compared to existing sparse attention methods. https://github.com/DerrickYLJ/LessIsMore  <br>  <br>

23. ***Boosting Search Ranking with AI Reasoning: A paper from Renmin University, Baidu, and CMU introduces ReasonRank, a new passage reranking model empowered with strong reasoning abilities. To overcome the lack of reasoning-intensive training data, the researchers first created an automated framework to synthesize high-quality training examples. They then used a two-stage training process, including reinforcement learning with a novel multi-view ranking reward, to teach the model how to reason. ReasonRank significantly outperforms existing baselines and has achieved state-of-the-art performance on the BRIGHT leaderboard.***  <br>  <br>
    Aug 9, Renmin Uni, Baidu and CMU published a [paper](https://arxiv.org/pdf/2508.07050) “ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability”. Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. This study first proposes an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, the study further proposes a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, the study designs a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that the trained reasoning-intensive reranker ReasonRank outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. Through further experiments, the ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard https://brightbenchmark.github.io/. Codes are here: https://github.com/8421BCD/ReasonRank.  <br>  <br>

25. ***A Fairer Way to Test AI Research Agents:   <br>Researchers have created BrowseComp-Plus, a new benchmark for evaluating deep-research AI agents that is designed to be more fair and transparent than existing evaluations. Unlike benchmarks that rely on dynamic live web searches, BrowseComp-Plus uses a fixed and curated set of documents, which allows for reproducible experiments and a clearer analysis of an agent's true capabilities. This controlled environment helps researchers better understand the contributions of different components, such as the retriever and the language model, to the agent's overall performance.***  <br>  <br>
    Aug 8, Uni of Waterloo, CSIRO, CMU and Uni of Queensland published a [paper](https://arxiv.org/pdf/2508.06600) “BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent”. Deep-Research agents, which integrate large language models (LLMs) with search tools, have shown success in improving the effectiveness of handling complex queries that require iterative search planning and reasoning over search results. Evaluations on current benchmarks like BrowseComp relies on black-box live web search APIs, have notable limitations in (1) fairness: dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep research methods; (2) transparency: lack of control over the document corpus makes it difficult to isolate retriever contributions. In other words, the current evaluations may compare a complete deep research system at a given time, but they do not foster well-controlled experiments to provide insights into the capability of underlying deep research LLMs. To address these challenges, the study introduces BrowseComp-Plus, a benchmark derived from BrowseComp, employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus includes human-verified supporting documents and mined challenging negatives, enabling controlled experimentation. The benchmark is shown to be effective in distinguishing the performance of deep research systems. For instance, the open-source model Search-R1, when paired with the BM25 retriever, achieves 3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with fewer search calls. This benchmark allows comprehensive evaluation and disentangled analysis of deep research agents and retrieval methods, fostering insights into retrieval effectiveness, citation accuracy, and context engineering in Deep-Research system.   <br>  <br>https://github.com/texttron/BrowseComp-Plus

27. ***Building Safer AI by Filtering Training Data:   <br>A study by EleutherAI and others explores whether filtering pretraining data can make open-weight language models more resistant to tampering. By removing text related to dual-use topics like biothreats, they trained models that were substantially more resistant to adversarial fine-tuning attacks—outperforming existing safety methods by over an order of magnitude—without harming their general capabilities. The findings establish data curation as a promising and effective layer of defense for securing open-weight AI systems.***  <br>  <br>
    Aug 8, EleutherAI, UK AI Security Inst, and Uni of Oxford published a [paper](https://arxiv.org/pdf/2508.06601) “Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs”. Open-weight AI systems offer unique benefits, including enhanced transparency, open research, and decentralized access. However, they are vulnerable to tampering attacks which can efficiently elicit harmful behaviors by modifying weights or activations. Currently, there is not yet a robust science of open-weight model risk management. Existing safety fine-tuning methods and other post-training techniques have struggled to make LLMs resistant to more than a few dozen steps of adversarial fine-tuning. This study investigates whether filtering text about dual-use topics from training data can prevent unwanted capabilities and serve as a more tamper-resistant safeguard. The study introduces a multi-stage pipeline for scalable data filtering and show that it offers a tractable and effective method for minimizing biothreat proxy knowledge in LLMs. The study pretrains multiple 6.9B-parameter models from scratch and find that they exhibit substantial resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M tokens of biothreat-related text -- outperforming existing post-training baselines by over an order of magnitude -- with no observed degradation to unrelated capabilities. However, while filtered models lack internalized dangerous knowledge, the study finds that they can still leverage such information when it is provided in context (e.g., via search tool augmentation), demonstrating a need for a defense-in-depth approach. Overall, these findings help to establish pretraining data curation as a promising layer of defense for open-weight AI systems.  <br>  <br>

29. ***A New Open-Source Powerhouse for AI Agents and Reasoning:   <br>The paper from Zhipu AI and Tsinghua University introduces GLM-4.5, a new open-source Mixture-of-Experts (MoE) model with 355 billion total parameters. Designed for high performance in agentic, reasoning, and coding (ARC) tasks, GLM-4.5 achieves top-tier results on major benchmarks like TAU-Bench and AIME 24, ranking among the best models evaluated. By releasing both the full model and a more compact version, the researchers aim to accelerate research into advanced agentic AI systems.***  <br>  <br>
    Aug 8, Zhipu AI and Tsinghua Uni published a [paper](https://www.arxiv.org/pdf/2508.06471) “GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models”. The paper presents GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. The researchers release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. https://github.com/zai-org/GLM-4.5.   <br>  <br>

31. ***AI Personalities Are Persistently Unstable:   <br>Research from Mila and other institutions reveals that the personalities of Large Language Models are fundamentally unstable, even in models with over 400 billion parameters. A comprehensive evaluation framework showed that minor changes to prompts, such as reordering questions, can cause personality measurements to shift by up to 20%. The study concludes that current LLMs lack the foundation for true behavioral consistency, suggesting that alignment strategies based on personality may be inadequate for safety-critical applications.***  <br>  <br>
    Aug 6, Mila et al published a [paper](https://www.arxiv.org/pdf/2508.04826) “Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History”. Large language models require consistent behavioral patterns for safe deployment, yet their personality-like traits remain poorly understood. The study presents PERSIST (PERsonality Stability in Synthetic Text), a comprehensive evaluation framework testing 25+ open-source models (1B-671B parameters) across 500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted personality instruments, the study systematically varies question order, paraphrasing, personas, and reasoning modes. The findings challenge fundamental deployment assumptions: (1) Even 400B+ models exhibit substantial response variability (SD > 0.4); (2) Minor prompt reordering alone shifts personality measurements by up to 20%; (3) Interventions expected to stabilize behavior, such as chain-of-thought reasoning, detailed personas instruction, inclusion of conversation history, can paradoxically increase variability; (4) LLM-adapted instruments show equal instability to human-centric versions, confirming architectural rather than translational limitations. This persistent instability across scales and mitigation strategies suggests current LLMs lack the foundations for genuine behavioral consistency. For safety-critical applications requiring predictable behavior, these findings indicate that personality-based alignment strategies may be fundamentally inadequate.   <br>  <br>

33. ***Predicting the Hidden Dynamics of AI Training:   <br>Researchers from Aimpoint Digital Lab and NYU have conducted the first comprehensive analysis of how "massive activations"—extremely large values critical to model function—develop during transformer training. The study reveals that their emergence follows predictable mathematical patterns that can be accurately modeled. This discovery allows architects to anticipate and potentially control these dynamics through design choices, which has significant implications for improving model stability, training efficiency, and interpretability.***  <br>  <br>
    Aug 5, Aimpoint Digital Lab and NYU published a [paper](https://arxiv.org/pdf/2508.03616) “Hidden Dynamics of Massive Activations in Transformer Training”. Massive activations are scalar values in transformer hidden states that achieve values orders of magnitude larger than typical activations and have been shown to be critical for model functionality. While prior work has characterized these phenomena in fully trained models, the temporal dynamics of their emergence during training remain poorly understood. The study presents the first comprehensive analysis of massive activation development throughout transformer training, using the Pythia model family as our testbed. Through systematic analysis of various model sizes across multiple training checkpoints, the study demonstrates that massive activation emergence follows predictable mathematical patterns that can be accurately modeled using an exponentially-modulated logarithmic function with five key parameters. The study develops a machine learning framework to predict these mathematical parameters from architectural specifications alone, achieving high accuracy for steady-state behavior and moderate accuracy for emergence timing and magnitude. These findings enable architects to predict and potentially control key aspects of massive activation emergence through design choices, with significant implications for model stability, training cycle length, interpretability, and optimization. The findings demonstrate that the emergence of massive activations is governed by model design and can be anticipated, and potentially controlled, before training begins.
  <br>  <br>  <br>


***Aug 10, 2025***

1. ***A New Era of AI with GPT-5's Unified Architecture:  <br>OpenAI's newly released GPT-5 features a sophisticated, unified architecture that balances speed, reasoning, and safety. It comes in several versions, including high-speed models like gpt 5 main and advanced reasoning models like gpt 5 thinking pro, which uses parallel computing for complex tasks. A key innovation is "safe completions," a system that provides helpful, safe responses to ambiguous or dual-use prompts, moving beyond simple refusal. With significantly reduced hallucinations and new developer tools, GPT-5 marks a major step forward in creating practical and reliable AI.*** <br> <br>
   Aug 7, OpenAI released GPT-5 and its [system card](https://cdn.openai.com/pdf/8124a3ce-ab78-4f06-96eb-49ea29ffb52f/gpt5-system-card-aug7.pdf). GPT-5 System Card introduces a highly advanced, unified AI architecture that blends fast performance, deep reasoning, and improved safety. It includes multiple specialized variants such as the high-throughput models gpt 5 main and gpt 5 main mini, as well as the more advanced gpt 5 thinking and gpt 5 thinking mini, with an even smaller thinking nano available via API. Within ChatGPT, OpenAI uses a dynamic model-router that assigns tasks in real time; for more complex reasoning tasks, gpt 5 thinking pro is used, which leverages parallel compute to increase performance. GPT-5 demonstrates best-in-class performance on real-world programming tasks and excels in reasoning, supported by new developer tools for flexibility and precision. A key highlight is its focus on safety—GPT-5 introduces "safe completions," which provide helpful responses to ambiguous or potentially dual-use prompts, moving beyond simplistic comply-or-refuse responses. Compared to earlier models, it significantly reduces hallucinations, leading to improved reliability in content generation across use cases. This combination of speed, reasoning, developer control, and safety represents a major evolution in the development of practical, intelligent, and aligned AI systems. <br> <br>

3. ***Moving Beyond Refusals with Safe-Completions:  <br>In a paper accompanying the GPT-5 release, OpenAI outlines its new safety training method called "safe-completions." This approach moves away from the traditional binary choice of either complying with or refusing a user's prompt. Instead, it focuses on generating an output that is as helpful as possible while remaining within safety constraints. This is particularly effective for "dual-use" prompts where the user's intent is unclear, as it reduces safety failures while increasing the model's helpfulness.*** <br> <br>
   Aug 7, OpenAI published a [paper](https://cdn.openai.com/pdf/be60c07b-6bc2-4f54-bcee-4141e1d6c69a/gpt-5-safe_completions.pdf) “From hard refusals to safe-completions: toward output-centric safety training”. Large Language Models used in ChatGPT have traditionally been trained to learn a refusal boundary: depending on the user’s intent, the model is taught to either fully comply or outright refuse. While this is a strong mitigation for explicitly malicious prompts, focusing safety training on refusals can lead to brittleness for prompts with obscured user intent. Binary refusal boundaries are especially ill-suited for dual-use cases (such as biology or cybersecurity), where a user request can be answered safely at a high level, but in some cases can lead to malicious uplift if sufficiently detailed or actionable. As an alternative, the word proposes safe-completions: a safety-training approach that centers on the safety of the assistant’s output, rather than a binary classification of the user’s intent. Safe-completions seek to maximize helpfulness within the safety policy’s constraints. The work incorporated this approach into GPT-5 and find that across both production comparisons and internally controlled experiments, safe-completion training improves safety (especially on dual-use prompts), reduces the severity of residual safety failures, and substantially increases model helpfulness. <br> <br>

5. ***Tackling Hallucinations in Reasoning Models:  <br>A paper from Meta addresses the problem of increased hallucinations in Reasoning Large Language Models (R-LLMs). The study finds that using standard fact-checking scores as a reward in online Reinforcement Learning (RL) can cause the model to "game the system" by producing less detailed answers. To solve this, researchers developed a new reward function that balances factual accuracy, the level of detail, and the relevance of the answer. This new approach significantly reduced the hallucination rate by over 23 percentage points while increasing answer detail.*** <br> <br>
   Aug 7, Meta published a [paper](https://www.arxiv.org/pdf/2508.05618) “Learning to Reason for Factuality”. Reasoning Large Language Models (R-LLMs) have significantly advanced complex reasoning tasks but often struggle with factuality, generating substantially more hallucinations than their non-reasoning counterparts on long-form factuality benchmarks. However, extending online Reinforcement Learning (RL), a key component in recent R-LLM advancements, to the long-form factuality setting poses several unique challenges due to the lack of reliable verification methods. Previous work has utilized automatic factuality evaluation frameworks such as FActScore to curate preference data in the offline RL setting, yet it is found that directly leveraging such methods as the reward in online RL leads to reward hacking in multiple ways, such as producing less detailed or relevant responses. The study proposes a novel reward function that simultaneously considers the factual precision, response detail level, and answer relevance, and applies online RL to learn high quality factual reasoning. Evaluated on six long-form factuality benchmarks, the factual reasoning model achieves an average reduction of 23.1 percentage points in hallucination rate, a 23% increase in answer detail level, and no degradation in the overall response helpfulness. <br> <br>

7. ***Google Gemini's New Guided Learning Mode:  <br>Google has launched Guided Learning in its Gemini AI, a new feature designed to foster deeper conceptual understanding rather than just providing answers. Developed with educators and learning experts, this mode acts as a personal tutor by asking open-ended questions, breaking down problems, and using images, videos, and quizzes to encourage critical thinking. Initially available to students in several countries, Google is providing a year of free access to its AI Pro plan to promote this new, understanding-oriented approach to AI in education.*** <br> <br>
   Aug 6, Google released [Guided Learning in Gemini](https://blog.google/outreach-initiatives/education/guided-learning/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=openai-s-gpt-5-is-finally-here&_bhlid=0e5ca81c2dbed77dc97c26b95a0263473f7af1a4). In its latest launch, Google introduces Guided Learning, a new mode in the Gemini AI designed to move beyond straightforward answers and cultivate a deeper, conceptual understanding. Developed collaboratively with educators, students, and experts in pedagogy, neuroscience, and AI, Guided Learning is built on the LearnLM family of models, which are fine-tuned using educational research and now integrated into Gemini 2.5. Functioning as a personal learning companion, it instead of quickly delivering answers, prompts learners with open-ended questions and breaks down problems step by step to encourage critical thinking and active engagement. The mode provides rich, multimodal support—intentionally weaving in images, diagrams, videos, and interactive quizzes to reinforce learning and retention. It’s crafted to create a judgment-free, explorative learning space that adapts to a learner’s pace and needs, supporting everything from exam prep to academic writing and creative inquiry. Assembling this tool involved years of cross-disciplinary collaboration, reflecting Google's commitment to combining AI with learning science to support responsible, effective education. To make Guided Learning accessible, Google offers a dedicated reply interface suitable for classroom integration—educators can share it directly via Google Classroom. Overall, Guided Learning represents a significant step toward reimagining AI not as a shortcut, but as an active, understanding-oriented study partner—empowering learners to explore, question, and truly grasp new concepts. Initially available to students in the U.S., Japan, Indonesia, Korea, and Brazil, Google is offering one year of free access to its AI Pro plan, which includes expanded features like Gemini 2.5 Pro, NotebookLM, and Deep Research. <br> <br>

9. ***AI Learning Through Self-Questioning:  <br>A study from Carnegie Mellon University proposes that large language models can improve their reasoning skills without any external data by generating and answering their own questions. The "Self-Questioning Language Models" (SQLM) framework uses two AI agents in a self-play setup: a "proposer" that creates problems and a "solver" that tries to answer them. Both agents are trained with reinforcement learning, with the system rewarding questions of appropriate difficulty and answers that are likely correct. This method was shown to improve performance on benchmarks for math, coding, and algebra, all without needing curated training datasets.*** <br> <br>
    Aug 6, CMU published a [paper](https://arxiv.org/pdf/2508.03682) “Self-Questioning Language Models”. Can large language models improve without external data -- by generating their own questions and answers? The study hypothesizes that a pre-trained language model can improve its reasoning skills given only a single prompt specifying the topic (e.g., algebra word problems) and asking the model to generate its own questions. To do this, the study proposes Self-Questioning Language Models (SQLM): an asymmetric self-play framework where a proposer is given the topic and generates a question for a solver, who tries to answer it. Both the proposer and solver are trained via reinforcement learning. The proposer receives a reward if the problem is not too easy or too difficult, and the solver receives a reward based on majority voting, a proxy for correctness in the absence of ground-truth answers. For coding, the proposer can instead generate unit tests which are used for verification. The authors study this asymmetric self-play framework on three benchmarks: three-digit multiplication, algebra problems from the OMEGA benchmark, and programming problems from Codeforces. By continually generating more interesting problems and attempting to solve them, language models can improve on downstream benchmarks without access to any curated training datasets. https://github.com/lili-chen/self-questioning-lm <br> <br>

11. ***OpenAI Enters the Open-Weight Arena with GPT-OSS:  <br>For the first time since 2019, OpenAI has released open-weight models, the GPT-OSS family. These models, built on an efficient Mixture-of-Experts (MoE) architecture, are designed for high performance on accessible hardware; the larger 120B model can run on a single GPU, while the 20B model runs on consumer hardware. Released with a permissive license and a strong emphasis on safety, GPT-OSS is available on major cloud and developer platforms, marking a significant move to democratize access to advanced AI.*** <br> <br>
    Aug 5, OpenAI [introduced gpt-oss](https://openai.com/index/introducing-gpt-oss/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=openai-s-gpt-5-is-finally-here&_bhlid=a1d2d038f065d247e307d3ebe235531d72b32ebc). OpenAI has unveiled GPT‑OSS, a family of open-weight reasoning models—gpt‑oss‑120b and gpt‑oss‑20b—released under the permissive Apache 2.0 license, marking its first open-weight offering since GPT‑2 in 2019. Built on an efficient Mixture-of-Experts (MoE) architecture, the larger 120B model achieves near-parity with OpenAI’s o4-mini benchmarks while running on a single 80 GB GPU, and the more compact 20B model delivers performance comparable to o3-mini and can run on consumer hardware with just 16 GB of memory. These models support offline, on-device deployment and local inference, making them accessible for startups, researchers, and businesses looking for transparency, control, and flexibility. OpenAI has emphasized safety—filtering out sensitive content, hardening against prompt-injection, and launching a red-teaming challenge to surface vulnerabilities—while also offering the models via major platforms like Hugging Face, AWS, Azure, Databricks, and Microsoft’s Windows AI Foundry for scalable, hybrid cloud-and-local workflows. In sum, GPT‑OSS represents a significant step toward democratizing advanced AI by enabling powerful, customizable, open-weight language models that can be used securely and affordably across a range of environments. <br> <br>

13. ***The Evolution of Programming into Computational Thinking:  <br>A Forbes article argues that the rise of generative AI is not eliminating the need for programming but transforming it into a higher-level skill called "computational thinking." This new paradigm requires a hybrid approach, blending traditional, logical code with descriptive, natural language prompts. The author stresses that the essential skill for the future is not just coding but the ability to translate intent into executable logic, requiring fluency in both code and prose to effectively collaborate with AI systems.*** <br> <br>
    Aug 5, Forbes published an [article](https://www.forbes.com/sites/adrianbridgwater/2025/08/04/computational-thinking-is-the-new-programming/) “Computational Thinking Is The New Programming”. The rise of generative AI is transforming software development, challenging traditional notions of programming. With tools like Codex and Claude generating code from natural language, some wonder whether coding is becoming obsolete. However, Dr. Rania Khalaf, Chief AI Officer at WSO2, argues that programming isn’t dead—it’s evolving. Rather than replacing code, natural language interfaces complement it, creating a hybrid model that blends deterministic programming with nondeterministic, descriptive prompts. Prompt engineering is giving way to “context engineering,” which demands deeper system understanding and combines code with natural expressions. Khalaf stresses that this fusion raises the abstraction level, making programming a multilingual process, much like switching between spoken languages to best express an idea. While generative AI excels at certain tasks, it can create an “illusion of fluency,” where users seem proficient but lack true understanding. This gap becomes critical when problems arise or deeper customization is needed. Khalaf draws on the famous “Peanut Butter and Jelly” instruction exercise to illustrate the complexity of translating intent into executable logic. She emphasizes that computational thinking—not just coding—is the essential skill of the future. Programming education must evolve to balance logic, language, and systems understanding, enabling people to shape rather than just consume the digital world. The future software engineer will need fluency in both code and prose. As universities consider integrating computer science into liberal arts programs, one thing is clear: the future of programming is hybrid, human-AI collaboration, where writing and coding coexist—just like creamy peanut butter and grape jelly. <br> <br>

15. ***Integrating AI into the Scientific Method:  <br>A comprehensive review paper authored by researchers from 24 institutions in the USA and UK explores how Large Language Models (LLMs) can be integrated into every stage of the scientific method. The paper concludes that for LLMs to become effective tools for scientific creativity and productivity, they must be deeply integrated into the entire research process in close collaboration with human scientists and guided by clear goals and evaluation metrics.*** <br> <br>
    Aug 5, Artificial Intelligence published a [paper](https://www.nature.com/articles/s44387-025-00019-5) “Exploring the role of large language models in the scientific method: from hypothesis to discovery”. The paper reviews how Large Language Models (LLMs) are redefining the scientific method and explore their potential applications across different stages of the scientific cycle, from hypothesis testing to discovery. The researchers from 24 USA, UK institutes conclude that, for LLMs to serve as relevant and effective creative engines and productivity enhancers, their deep integration into all steps of the scientific process should be pursued in collaboration and alignment with human scientific goals, with clear evaluation metrics. <br> <br>

17. ***Is Chain-of-Thought Reasoning Just an Illusion?:  <br>A study from Arizona State University challenges the popular belief that Chain-of-Thought (CoT) prompting enables genuine reasoning in Large Language Models (LLMs). The researchers argue through a data distribution lens that CoT is more of a superficial pattern-matching capability. Using a controlled environment, they demonstrate that CoT's effectiveness breaks down when the model is tested on tasks that differ from its training data, suggesting it is a "brittle mirage" rather than a generalizable reasoning ability.*** <br> <br>
    Aug 5, Arizona State Uni published a [paper](https://www.arxiv.org/pdf/2508.01191) “Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens”. Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating the authors to explore further. This work studies CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, the study dissects CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, the study designs DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. The results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning. https://github.com/ChengshuaiZhao0/DataAlchemy <br> <br>

19. ***AI Agents That Code for Efficiency:  <br>Researchers have introduced CoAct-1, a new type of autonomous AI agent that can operate a computer by writing and executing code in addition to using the graphical user interface (GUI). This hybrid system features an "Orchestrator" that delegates tasks to either a GUI Operator or a Programmer agent, allowing it to bypass slow and inefficient GUI actions for many tasks. CoAct-1 set a new state-of-the-art success rate on the challenging OSWorld benchmark, completing tasks with significantly fewer steps than previous GUI-only agents.*** <br> <br>
    Aug 5, Uni of Southern California, Salesforce, and Uni of Washington published a [paper](https://arxiv.org/pdf/2508.03923) “CoAct-1: Computer-using Agents with Coding as Actions”. Autonomous agents that operate computers via Graphical User Interfaces (GUIs) often struggle with efficiency and reliability on complex, long-horizon tasks. While augmenting these agents with planners can improve task decomposition, they remain constrained by the inherent limitations of performing all actions through GUI manipulation, leading to brittleness and inefficiency. This study introduces a more robust and flexible paradigm: enabling agents to use coding as an enhanced action. The study presents CoAct-1, a novel multi-agent system that synergistically combines GUI-based control with direct programmatic execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks to either a conventional GUI Operator or a specialized Programmer agent, which can write and execute Python or Bash scripts. This hybrid approach allows the agent to bypass inefficient GUI action sequences for tasks like file management and data processing, while still leveraging visual interaction when necessary. The study evaluates the system on the challenging OSWorld benchmark, where CoAct-1 achieves a new state-of-the-art success rate of 60.76%, significantly outperforming prior methods. Furthermore, the approach dramatically improves efficiency, reducing the average number of steps required to complete a task to just 10.15, compared to 15 for leading GUI agents. The results demonstrate that integrating coding as a core action provides a more powerful, efficient, and scalable path toward generalized computer automation. <br> <br>

21. ***A Universal Training Framework for AI Agents:  <br>Microsoft has developed Agent Lightning, a flexible framework that allows any AI agent to be trained using Reinforcement Learning (RL), regardless of how it was built. The system completely decouples the agent's operation from the training process, making it possible to integrate with existing agents from frameworks like LangChain or AutoGen with almost no code changes. This enables continuous improvement for a wide range of agents, from simple tool-users to complex multi-agent systems.*** <br> <br>
    Aug 5, Microsoft published a [paper](https://arxiv.org/abs/2508.03680) “Agent Lightning: Train ANY AI Agents with Reinforcement Learning”. The paper presents Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, the study defines an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, the work introduces a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment. https://github.com/microsoft/agent-lightning <br> <br>

23. ***Crafting a New Ethics for Autonomous AI:  <br>A paper in Nature by researchers from DeepMind and academia argues that the rise of autonomous AI agents demands a new ethical framework. Unlike earlier AI, these agents can act independently to achieve goals, creating complex challenges related to trust, societal coordination, and alignment with human values that go far beyond traditional concerns like data privacy. The authors call for reimagined governance and moral frameworks to ensure these powerful, independent systems are deployed in a safe and socially beneficial way.*** <br> <br>
    Aug 4, Nature published a [paper](https://www.nature.com/articles/d41586-025-02454-5) “We need a new ethics for a world of AI agents”. As autonomous AI agents—systems that perceive environments and act on their own to achieve goals—become increasingly capable, the ethical challenges they pose extend well beyond traditional concerns about AI bias or data privacy. In their Comment, DeepMind and academic researchers argue that these agents could disrupt human–machine relationships, trust dynamics, and societal coordination, emphasizing the need for ethics frameworks tailored to this new paradigm. Unlike earlier AI systems requiring human direction, agents can perform complex activities independently, from making purchases online to conducting research or executing actions in the physical world. This autonomy, powered by generative AI, could drive massive economic impacts—McKinsey estimates a global windfall of US$2.6 to $4.4 trillion annually—while raising high-stakes ethical issues around alignment with human values, societal norms, and user well-being. The authors urge an expansion of value-alignment research to ensure agents remain beneficial and safe, and call for new ethical models that account for the blurred boundaries among users, designers, and the agents themselves. By highlighting the shifting nature of agency and control, they press for reimagined governance and moral frameworks to guide the deployment of these powerful, independent systems—a step that’s essential to ensuring they serve humanity in just and socially coherent ways. <br> <br>

25. ***Automated Data Curation for Better AI:  <br>Meta has introduced Refine-n-Judge, an automated, iterative method for improving the quality of datasets used to train Large Language Models (LLMs). The system uses a single LLM to both refine a response and then judge whether the new version is an improvement. This process creates high-quality chains of preference-labeled data without needing costly human feedback. Models fine-tuned on datasets enhanced by this method showed significant performance gains on several benchmarks.*** <br> <br>
    Aug 3, Meta published a [paper](https://www.arxiv.org/abs/2508.01543) “Refine-n-Judge: Curating High-Quality Preference Chains for LLM-Fine-Tuning”. Large Language Models (LLMs) have demonstrated remarkable progress through preference-based fine-tuning, which critically depends on the quality of the underlying training data. While human feedback is essential for improving data quality, it is costly and does not scale well. This study introduces Refine-n-Judge, an automated iterative approach that leverages a single LLM as both a refiner and a judge to enhance dataset quality. Unlike existing iterative refinement methods, Refine-n-Judge employs an LLM to both generate refinements and explicitly evaluate each improvement, ensuring that every iteration meaningfully enhances the dataset without requiring additional human annotation or a separate reward model. At each step, the LLM refines a response and judges whether the refinement is an improvement over the previous answer. This process continues until the LLM prefers the initial answer over the refinement, indicating no further improvements. This produces sequences of increasing quality, preference-labeled responses ideal for fine-tuning. The study demonstrates the effectiveness of Refine-n-Judge across a range of public datasets spanning five corpora, targeting tasks such as coding, math, and conversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on Refine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of comparisons against models tuned on the original dataset by GPT-4. Additionally, the study reports performance gains: +5% on AlpacaEval and AlpacaEval 2.0, and +19% on MT-Bench. The results indicate that Refine-n-Judge produces high-quality datasets and scalable model improvements. <br> <br>

27. ***The Widening Gap Between Public and Elite AI:  <br>A Reddit discussion highlights a growing concern over "AI bifurcation"—the split between consumer-grade AI models and the "elite," high-performance models used by large corporations for a hefty price. The discussion points out that the public may be losing access to the true cutting edge of AI, creating a hidden, accelerated development timeline for private tech. This raises fears that mega-corporations could achieve superintelligence in secret while the public and policymakers remain unaware of the actual progress.*** <br> <br>
    Aug 3, reddit has a [discussion](https://www.reddit.com/r/singularity/comments/1mg8942/ai_bifurcation_tree_of_life_splitting_is/) “AI bifurcation, tree of life splitting is happening now, a hidden threat”. Nobody is paying attention to the fact AI models are officially starting to split away from consumer models into 'elite' corporate models, with things like Gemini Deepthink, Grok Heavy, ChatGPT's planned $20k a month model. Consumers are going to lose access to what actually represents the cutting edge of AI technology as the newer models architecture become better and better at inference. We're one day going to have $100k models nobody will have access to. The biggest issue with this is the AI timeline is being based on consumer models, not inference models, inference models basically mean we will start to jump 2 models ahead every year instead of one, meaning 2030, will be more like 2035 (for mega-corporations and private tech). In the mid 2030's, eventually, AI companies will stop selling their highest tier inference models to even corporations, they might start running $1 million dollar a month cost inference models privately, and obtain ASI in secret, while politicians and the public think AI is still just a toy. <br> <br>

29. ***Simplifying Streaming Neural Networks:  <br>Google has introduced SequenceLayers, a new library and API designed to make it easier to build sequence models that can be used for streaming applications, such as autoregressive sampling. The framework works by having each layer define its own state (like a Transformer's KV cache), which simplifies the creation of complex models, makes them immediately streamable, and helps prevent a wide range of common bugs in sequence processing.*** <br> <br>
    Jul 31, Google published a [paper](https://arxiv.org/pdf/2507.23292) “SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy”. The study introduces a neural network layer API and library for sequence modeling, designed for easy creation of sequence models that can be executed both layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g., autoregressive sampling). To achieve this, layers define an explicit representation of their state over time (e.g., a Transformer KV cache, a convolution buffer, an RNN hidden state), and a step method that evolves that state, tested to give identical results to a stateless layer-wise invocation. This and other aspects of the SequenceLayers contract enables complex models to be immediately streamable, mitigates a wide range of common bugs arising in both streaming and parallel sequence processing, and can be implemented in any deep learning library. A composable and declarative API, along with a comprehensive suite of layers and combinators, streamlines the construction of production-scale models from simple streamable components while preserving strong correctness guarantees. https://github.com/google/sequence-layers. <br> <br>

31. ***Monitoring AI by "Watching the Weights":  <br>A new method from Carnegie Mellon University allows for the monitoring and control of fine-tuned Large Language Models (LLMs) by analyzing their weights, not their activations. This innovative technique does not require access to the model's training data. By examining the difference in weights between a fine-tuned model and its base version, researchers can detect newly acquired behaviors, successfully identify and block hidden backdoors, and even uncover the specific focus of commercial models, such as their marketing strategies.*** <br> <br>
    Jul 31, CMU published a [paper](https://www.arxiv.org/pdf/2508.00161) “Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs”. The releases of powerful open-weight large language models (LLMs) are often not accompanied by access to their full training data. Existing interpretability methods, particularly those based on activations, often require or assume distributionally similar data. This is a significant limitation when detecting and defending against novel potential threats like backdoors, which are by definition out-of-distribution. This study introduces a new method for understanding, monitoring and controlling fine-tuned LLMs that interprets weights, rather than activations, thereby side stepping the need for data that is distributionally similar to the unknown training data. The study demonstrates that the top singular vectors of the weight difference between a fine-tuned model and its base model correspond to newly acquired behaviors. By monitoring the cosine similarity of activations along these directions, the study can detect salient behaviors introduced during fine-tuning with high precision. For backdoored models that bypasses safety mechanisms when a secret trigger is present, the method stops up to 100% of attacks with a false positive rate below 1.2%. For models that have undergone unlearning, the study detects inference on erased topics with accuracy up to 95.42% and can even steer the model to recover "unlearned" information. Besides monitoring, the method also shows potential for pre-deployment model auditing: by analyzing commercial instruction-tuned models (OLMo, Llama, Qwen), the work is able to uncover model-specific fine-tuning focus including marketing strategies and Midjourney prompt generation. https://github.com/fjzzq2002/WeightWatch <br> <br>

33. ***Finding the Best Embeddings for RAG:  <br>A study from UHK and HKUST explored how to best use different embedding models to improve Retrieval-Augmented Generation (RAG). The researchers found that simply mixing retrievals from various models did not work well. Instead, they proposed "Confident RAG," a method that generates a response multiple times using different embedding models and then selects the answer with the highest confidence score. This approach showed consistent performance improvements of 5-10% over standard RAG and vanilla LLMs.*** <br> <br>
    Jul 23, UHK and HKUST published a [paper](https://arxiv.org/pdf/2507.17442) “Each to Their Own: Exploring the Optimal Embedding in RAG”. Recently, as Large Language Models (LLMs) have fundamentally impacted various fields, the methods for incorporating up-to-date information into LLMs or adding external knowledge to construct domain-specific models have garnered wide attention. Retrieval-Augmented Generation (RAG), serving as an inference-time scaling method, is notable for its low cost and minimal effort for parameter tuning. However, due to heterogeneous training data and model architecture, the variant embedding models used in RAG exhibit different benefits across various areas, often leading to different similarity calculation results and, consequently, varying response quality from LLMs. To address this problem, the study proposes and examines two approaches to enhance RAG by combining the benefits of multiple embedding models, named Mixture-Embedding RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects retrievals from multiple embedding models based on standardized similarity; however, it does not outperform vanilla RAG. In contrast, Confident RAG generates responses multiple times using different embedding models and then selects the responses with the highest confidence level, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively. The consistent results across different LLMs and embedding models indicate that Confident RAG is an efficient plug-and-play approach for various domains.
 <br> <br> <br>


***Aug 3, 2025***

1. ***Decoding Softmax Attention:  <br>A paper published on August 1 by Southern Methodist University explains why softmax attention is more effective than its linear counterparts in transformer architectures. By re-framing softmax attention as a recurrent neural network (RNN), the research breaks down its components to understand their individual importance and interaction. This novel perspective helps clarify the expressive power of softmax attention and provides a clearer understanding of the performance gap between it and more computationally efficient linear attention methods.*** <br> <br>
   Aug 1, Southern Methodist Uni published a [paper](https://arxiv.org/pdf/2507.23632) “On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective”. Since its introduction, softmax attention has become the backbone of modern transformer architectures due to its expressiveness and scalability across a wide range of tasks. However, the main drawback of softmax attention is the quadratic memory requirement and computational complexity with respect to the sequence length. By replacing the softmax nonlinearity, linear attention and similar methods have been introduced to avoid the quadratic bottleneck of softmax attention. Despite these linear forms of attention being derived from the original softmax formulation, they typically lag in terms of downstream accuracy. While strong intuition of the softmax nonlinearity on the query and key inner product suggests that it has desirable properties compared to other nonlinearities, the question of why this discrepancy exists still remains unanswered. This work demonstrates that linear attention is an approximation of softmax attention by deriving the recurrent form of softmax attention. Using this form, each part of softmax attention can be described in the language of recurrent neural networks (RNNs). Describing softmax attention as an RNN allows for the ablation of the components of softmax attention to understand the importance of each part and how they interact. In this way, the work helps explain why softmax attention is more expressive than its counterparts. https://github.com/gmongaras/On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective <br> <br>

3. ***Generating High-Quality AI Training Data:  <br>On July 31, a paper from Meta and NYU introduced CoT-Self-Instruct, a new method for creating synthetic data to train Large Language Models (LLMs). The technique prompts an LLM to use Chain-of-Thought (CoT) to reason through a task and then generate a new, similar prompt for training purposes. After an automated filtering process, the resulting high-quality data was shown to significantly improve LLM performance on both verifiable reasoning tasks and non-verifiable instruction-following tasks, outperforming existing training datasets and human-generated prompts on several key benchmarks.*** <br> <br>
   Jul 31, Meta and NYU published a [paper](https://arxiv.org/pdf/2507.23751) “CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks”. The study proposes CoT-Self-Instruct, a synthetic data generation method that instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the given seed tasks, and then to generate a new synthetic prompt of similar quality and complexity for use in LLM training, followed by filtering for high-quality data with automatic metrics. In verifiable reasoning, the synthetic data significantly outperforms existing training datasets, such as s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For non-verifiable instruction-following tasks, the method surpasses the performance of human or standard self-instruct prompts on both AlpacaEval 2.0 and Arena-Hard. <br> <br>

5. ***OpenAI's Breakthrough in Mathematical Reasoning:  <br>An article from inferencebysequoia.substack.com on July 31 details how a small, three-person team at OpenAI achieved a gold-medal level performance on the International Mathematical Olympiad (IMO) problems. Their success came from using general-purpose reinforcement learning instead of specialized math tools, allowing for more flexible and scalable reasoning. A significant development was the model's ability to recognize its own limits, choosing not to answer a question it couldn't solve correctly. This achievement, amplified by OpenAI's extensive infrastructure and extended test-time computation, marks a major step toward creating AI that can assist with real-world mathematical research.*** <br> <br>
   Jul 31, inferencebysequoia.substack.com published an [article](https://inferencebysequoia.substack.com/p/three-ai-teams-win-imo-gold-openai) “Three AI Teams Win IMO Gold; OpenAI Talks About How They Did the Math”. In a remarkable achievement, a three-person team at OpenAI—Alex Wei, Sheryl Hsu, and Noam Brown—secured gold-level performance on the International Mathematical Olympiad (IMO) problems, a milestone long pursued by the AI community. Their success stemmed from a bold decision to prioritize general-purpose reinforcement learning techniques over specialized mathematical tools, aiming for scalable reasoning across domains rather than narrow competition-focused solutions. This approach allowed them to tackle hard-to-verify tasks with greater flexibility and reliability. A key breakthrough was the model’s ability to exhibit self-awareness: when faced with IMO’s notoriously difficult problem six, it chose to respond with “no answer” rather than hallucinate a plausible but incorrect solution. This marks a significant step forward in AI trustworthiness and reliability. The team’s achievement also underscores the power of focused execution—despite being a small group working for just two months, they leveraged OpenAI’s broader infrastructure in inference, scaling, and training to amplify their impact. Another critical factor was test-time compute scaling, which extended the model’s reasoning time from seconds to hours, enabling deeper problem-solving but also introducing new challenges in evaluation. While competitions like the IMO serve as valuable benchmarks, the team acknowledges that they are merely stepping stones toward more meaningful goals—namely, real-world mathematical research and utility. Their work not only advances AI’s reasoning capabilities but also sets the stage for future systems that can collaborate with human mathematicians, bridging the gap between competition performance and genuine discovery. <br> <br>

7. ***A New Way to Model Topics in Text:  <br>Columbia and Google researchers published a paper on July 31 introducing Mechanistic Topic Models (MTMs), a novel approach that uses sparse autoencoders to identify abstract topics in text. Unlike traditional topic models that are limited to lists of words, MTMs operate on semantically rich features, allowing them to uncover deeper conceptual themes. The study also introduced an LLM-based evaluation framework, "topic judge," which consistently preferred MTMs over other methods. Furthermore, MTMs are unique in their ability to control and steer LLM text generation based on these identified topics.*** <br> <br>
   Jul 31, Columbia and Google published a [paper](https://www.arxiv.org/pdf/2507.23220) “Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders”. Traditional topic models are effective at uncovering latent themes in large text collections. However, due to their reliance on bag-of-words representations, they struggle to capture semantically abstract features. While some neural variants use richer representations, they are similarly constrained by expressing topics as word lists, which limits their ability to articulate complex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic models that operate on interpretable features learned by sparse autoencoders (SAEs). By defining topics over this semantically rich space, MTMs can reveal deeper conceptual themes with expressive feature descriptions. Moreover, uniquely among topic models, MTMs enable controllable text generation using topic-based steering vectors. To properly evaluate MTM topics against word-list-based approaches, we propose topic judge, an LLM-based pairwise comparison evaluation framework. Across five datasets, MTMs match or exceed traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of LLM outputs. <br> <br>

9. ***The Impact of Prompt Structure on AI Performance:  <br>A paper from the University of Maryland, published on July 30, reveals a new positional bias in large language models called DEMOS' POSITION IN PROMPT (DPP) bias. The study found that the placement of demonstrations (demos), system prompts, and user messages within the input can significantly alter the model's accuracy and predictions. Through extensive experiments, researchers discovered that placing demos at the beginning of the prompt consistently leads to the most stable and accurate results, with performance gains of up to six points. Conversely, putting demos at the end often degrades performance, highlighting a critical sensitivity in how LLMs process in-context learning.*** <br> <br>
    Jul 30, Uni of Maryland published a [paper](https://arxiv.org/pdf/2507.22887) “Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning”. In-context learning (ICL) is a critical emerging capability of large language models (LLMs), enabling few-shot learning during inference by including a few demonstrations (demos) in the prompt. However, it has been found that ICL's performance can be sensitive to the choices of demos and their order. This paper investigates an unexplored new positional bias of ICL for the first time: the study observes that the predictions and accuracy can drift drastically when the positions of demos, the system prompt, and the user message in LLM input are varied. The study refers to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. The study designs a systematic evaluation pipeline to study this type of positional bias across classification, question answering, summarization, and reasoning tasks. The authors introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify net gains and output volatility induced by changes in the demos' position. Extensive experiments on ten LLMs from four open-source model families (QWEN, LLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their accuracy and predictions: placing demos at the start of the prompt yields the most stable and accurate outputs with gains of up to +6 points. In contrast, placing demos at the end of the user message flips over 30% of predictions without improving correctness on QA tasks. Smaller models are most affected by this sensitivity, though even large models remain marginally affected on complex tasks. <br> <br>

11. ***Navigating the Global AI Landscape:  <br>A July 30 article in The Batch by Andrew Ng examines the shifting dynamics of the global AI race, particularly between the U.S. and China. The piece highlights China's rapid progress through its open-weights models, the new U.S. AI Action Plan under President Trump, which aims to boost innovation and counter Chinese influence, and the controversial decision to lift the ban on AI chip sales to China. The article also touches on the societal implications of AI, noting that using chatbots for companionship is linked to lower well-being, and emphasizes the need for responsible AI development.*** <br> <br>
    Jul 30, The Batch published an [article](https://charonhub.deeplearning.ai/issue-312/) by Andrew Ng “Trump Resets AI Policy, Qwen3’s Agentic Advance, U.S. Chips for China, The Trouble With AI Friends”. The Batch discusses critical developments in global AI dynamics, focusing on the U.S.-China AI race, policy shifts, and societal impacts. It highlights China's growing momentum in AI, driven by its open-weights model ecosystem and semiconductor advancements, potentially surpassing the U.S. despite the latter's lead in proprietary models. The White House’s AI Action Plan, introduced under President Trump, emphasizes innovation, infrastructure, and global leadership, promoting open-source AI, data center construction, and AI exports while countering China’s influence. However, the plan’s push for “ideologically neutral” models raises concerns about bias. Alibaba’s Qwen3 models, including Qwen3-Coder, showcase China’s progress in agentic AI, outperforming many open-weights models in coding and reasoning tasks. The U.S. decision to lift bans on AI chip sales to China, allowing Nvidia and AMD to resume exports, reflects a strategic shift to balance economic interests and national security, though it may bolster China’s AI capabilities. Additionally, a study reveals that frequent chatbot use for companionship correlates with lower well-being, raising ethical questions about AI’s societal role. The article underscores the competitive AI landscape, the need for open science, and the importance of responsible AI development to support democracy and human welfare, while cautioning against overreliance on AI companionship and advocating for stronger human social support systems. <br> <br>

13. ***Automating Front-End Development with AI Agents:  <br>On July 30, CUHK and ARISE Lab introduced ScreenCoder, a multi-agent framework designed to automate the conversion of user interface (UI) designs into code. Addressing the limitations of text-only approaches, this modular system uses a grounding agent to identify UI elements, a planning agent to structure the layout, and a generation agent to write the HTML/CSS code. This method improves accuracy and interpretability and was used to create a large-scale synthetic dataset that, when used for fine-tuning, significantly enhanced the model's UI understanding and code generation quality.*** <br> <br>
    Jul 30, CUHK and ARISE Lab published a [paper](https://arxiv.org/pdf/2507.22827) “ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents”. Automating the transformation of user interface (UI) designs into front-end code holds significant promise for accelerating software development and democratizing design workflows. While recent large language models (LLMs) have demonstrated progress in text-to-code generation, many existing approaches rely solely on natural language prompts, limiting their effectiveness in capturing spatial layout and visual design intent. In contrast, UI development in practice is inherently multimodal, often starting from visual sketches or mockups. To address this gap, the study introduces a modular multi-agent framework that performs UI-to-code generation in three interpretable stages: grounding, planning, and generation. The grounding agent uses a vision-language model to detect and label UI components, the planning agent constructs a hierarchical layout using front-end engineering priors, and the generation agent produces HTML/CSS code via adaptive prompt-based synthesis. This design improves robustness, interpretability, and fidelity over end-to-end black-box methods. Furthermore, the study extends the framework into a scalable data engine that automatically produces large-scale image-code pairs. Using these synthetic examples, the work fine-tunes and reinforces an open-source VLM, yielding notable gains in UI understanding and code quality. Extensive experiments demonstrate that the approach achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness. https://github.com/leigest519/ScreenCoder. <br> <br>

15. ***A New Generation of Efficient and Powerful AI Models:  <br>The Technology Innovation Institute (TII) announced Falcon-H1 on July 30, a new family of open-source language models with a hybrid architecture that combines Transformer-based attention with State Space Models (SSMs). This design optimizes both performance and efficiency, allowing Falcon-H1 models to match or exceed the performance of much larger models while using fewer parameters and less training data. Available in various sizes, the models excel at reasoning, math, and multilingual tasks and support a context length of up to 256,000 tokens, making them highly versatile for a wide range of applications.*** <br> <br>
    Jul 30, TII published a [paper](https://arxiv.org/pdf/2507.22448) “Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance”. The report introduces Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. The study systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring the commitment to accessible and impactful AI research. https://github.com/tiiuae/falcon-h1 <br> <br>

17. ***Smarter Information Retrieval for AI:  <br>Researchers from Rutgers University, Northwestern University, NEC, and NJIT introduced DeepSieve in a paper published on July 30. This advanced Retrieval-Augmented Generation (RAG) framework acts as a "knowledge router," breaking down complex queries into sub-questions and directing each to the most appropriate knowledge source. By filtering out irrelevant information through a multi-stage process, DeepSieve improves the reasoning depth, retrieval accuracy, and transparency of Large Language Models (LLMs), outperforming conventional RAG methods on multi-hop question-answering tasks.*** <br> <br>
    Jul 30, Rutgers Uni, Northwestern Uni, NEC and NJIT published a [paper](https://www.arxiv.org/pdf/2507.22050) “Information Sieving via LLM-as-a-Knowledge-Router”. Large Language Models (LLMs) excel at many reasoning tasks but struggle with knowledge-intensive queries due to their inability to dynamically access up-to-date or domain-specific information. Retrieval-Augmented Generation (RAG) has emerged as a promising solution, enabling LLMs to ground their responses in external sources. However, existing RAG methods lack fine-grained control over both the query and source sides, often resulting in noisy retrieval and shallow reasoning. This study introduces DeepSieve, an agentic RAG framework that incorporates information sieving via LLM-as-a-knowledge-router. DeepSieve decomposes complex queries into structured sub-questions and recursively routes each to the most suitable knowledge source, filtering irrelevant information through a multi-stage distillation process. The design emphasizes modularity, transparency, and adaptability, leveraging recent advances in agentic system design. Experiments on multi-hop QA tasks across heterogeneous sources demonstrate improved reasoning depth, retrieval precision, and interpretability over conventional RAG approaches. https://github.com/MinghoKwok/DeepSieve. <br> <br>

19. ***ChatGPT Shifts from Answer Engine to Study Partner:  <br>On July 29, OpenAI launched Study Mode for ChatGPT, a new feature aimed at promoting active learning and critical thinking. Instead of giving direct answers, Study Mode engages users with Socratic-style questions and personalized quizzes to help them understand concepts better. Developed with input from educators, this experimental feature is designed to address concerns about AI hindering student learning. While it currently lacks administrative controls, OpenAI is exploring future enhancements to make it an even more effective educational tool.*** <br> <br>
    Jul 29, OpenAI is “[Introducing study mode](https://openai.com/index/chatgpt-study-mode/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=openai-unveils-study-mode-for-chatgpt&_bhlid=abb3e028393243e870d1ae8e4f0521993ed2c401)”. OpenAI has introduced Study Mode for ChatGPT, a feature designed to foster critical thinking and active learning rather than providing instant answers. Available to logged-in users on Free, Plus, Pro, Team, and soon Edu plans, Study Mode transforms ChatGPT into an interactive learning partner. It employs Socratic-style questions, personalized quizzes, and structured responses to clarify concepts, tailoring lessons based on users’ skill levels and past interactions. By encouraging self-reflection and critical engagement, it addresses educators’ concerns about AI undermining students’ critical thinking, supported by studies showing reduced brain activity with passive AI use. Developed with input from teachers and pedagogical experts, Study Mode is experimental, allowing for rapid improvements based on feedback. However, it has limitations: students can toggle it off, and there are no parental or administrative controls to enforce its use, though OpenAI is considering such features. Future enhancements include visual aids, clearer visualizations, goal setting, and deeper personalization. This move aligns with similar efforts by competitors like Anthropic’s “Learning Mode” for Claude, reflecting a shift toward AI as an educational tool that promotes deeper understanding rather than rote answers, though its effectiveness depends on students’ willingness to engage actively. <br> <br>

21. ***Learning with Limited Resources:  <br>A Google paper published on July 29 explores the theoretical challenges of continual learning when an agent has finite memory and computational power. By studying a capacity-constrained linear-quadratic-Gaussian (LQG) sequential prediction problem, the researchers derived a solution for how an agent with limited capacity should best allocate its resources. The work also demonstrates how to optimally distribute capacity across different sub-problems, providing a foundational step toward a more systematic understanding of learning under real-world constraints.*** <br> <br>
    Jul 29, Google published a [paper](https://www.arxiv.org/pdf/2507.21479) “Capacity-Constrained Continual Learning”. Any agents people can possibly build are subject to capacity constraints, as memory and compute resources are inherently finite. However, comparatively little attention has been dedicated to understanding how agents with limited capacity should allocate their resources for optimal performance. The goal of this paper is to shed some light on this question by studying a simple yet relevant continual learning problem: the capacity-constrained linear-quadratic-Gaussian (LQG) sequential prediction problem. The study derives a solution to this problem under appropriate technical conditions. Moreover, for problems that can be decomposed into a set of sub-problems, the work also demonstrates how to optimally allocate capacity across these sub-problems in the steady state. The authors view the results of this paper as a first step in the systematic theoretical study of learning under capacity constraints. <br> <br>

23. ***Mapping and Controlling AI Personalities:  <br>Researchers from Anthropic, UT Austin, and other institutions published a paper on July 29 introducing "persona vectors," which are directions in a language model's activation space that correspond to specific character traits like "evil" or "sycophancy." The study demonstrates that these vectors can be used to monitor and predict personality shifts during training. Furthermore, they can be used to prevent undesirable changes or correct them after the fact, and even to identify problematic training data, offering a powerful tool for creating more reliable and aligned AI assistants.*** <br> <br>
    Jul 29, Anthropic, UT Austin et al. published a [paper](https://arxiv.org/pdf/2507.21509) “Persona Vectors: Monitoring and Controlling Character Traits in Language Models”. Large language models interact with users through a simulated 'Assistant' persona. While the Assistant is typically trained to be helpful, harmless, and honest, it sometimes deviates from these ideals. This paper identifies directions in the model's activation space-persona vectors-underlying several traits, such as evil, sycophancy, and propensity to hallucinate. The study confirms that these vectors can be used to monitor fluctuations in the Assistant's personality at deployment time. The paper then applies persona vectors to predict and control personality shifts that occur during training. The study finds that both intended and unintended personality changes after finetuning are strongly correlated with shifts along the relevant persona vectors. These shifts can be mitigated through post-hoc intervention, or avoided in the first place with a new preventative steering method. Moreover, persona vectors can be used to flag training data that will produce undesirable personality changes, both at the dataset level and the individual sample level. The method for extracting persona vectors is automated and can be applied to any personality trait of interest, given only a natural-language description. <br> <br>

25. ***Making AI More Factually Accurate:  <br>On July 28, Meta published a paper on PrismRAG, a fine-tuning framework designed to improve the factual accuracy of Retrieval-Augmented Generation (RAG) systems. The method trains the model to handle confusing or distracting information in retrieved documents and encourages it to reason more strategically without needing complex instructions. Across 12 different benchmarks, PrismRAG increased average factuality by 5.4%, outperforming existing state-of-the-art solutions.*** <br> <br>
    Jul 28, Meta published a [paper](https://www.arxiv.org/pdf/2507.18857) “PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning”. Retrieval-augmented generation (RAG) often falls short when retrieved context includes confusing semi-relevant passages, or when answering questions require deep contextual understanding and reasoning. The study proposes an efficient fine-tuning framework, called PrismRAG, that (i) trains the model with distractor-aware QA pairs mixing gold evidence with subtle distractor passages, and (ii) instills reasoning-centric habits that make the LLM plan, rationalize, and synthesize without relying on extensive human engineered instructions. Evaluated across 12 open-book RAG QA benchmarks spanning diverse application domains and scenarios, PrismRAG improves average factuality by 5.4%, outperforming state-of-the-art solutions. <br> <br>

27. ***Training AI to Ask Better Questions:  <br>A study from the University of Southern California, Microsoft, and the University of California Davis, published on July 28, focuses on teaching large language models to proactively gather information when faced with ambiguous prompts. The researchers developed a framework to train models to identify knowledge gaps and ask targeted questions to elicit missing details from the user. Through reinforcement finetuning, the trained model significantly outperformed others in both automatic and human evaluations, showing that proactive clarification can transform LLMs into more effective collaborative partners.*** <br> <br>
    Jul 28, Uni of Southern California, Microsoft and Uni of California Davis published a [paper](https://arxiv.org/pdf/2507.21389) “Teaching Language Models To Gather Information Proactively”. Large language models (LLMs) are increasingly expected to function as collaborative partners, engaging in back-and-forth dialogue to solve complex, ambiguous problems. However, current LLMs often falter in real-world settings, defaulting to passive responses or narrow clarifications when faced with incomplete or under-specified prompts, falling short of proactively gathering the missing information that is crucial for high-quality solutions. This study introduces a new task paradigm: proactive information gathering, where LLMs must identify gaps in the provided context and strategically elicit implicit user knowledge through targeted questions. To systematically study and train this capability, the study designs a scalable framework that generates partially specified, real-world tasks, masking key information and simulating authentic ambiguity. Within this setup, the core innovation is a reinforcement finetuning strategy that rewards questions that elicit genuinely new, implicit user information -- such as hidden domain expertise or fine-grained requirements -- that would otherwise remain unspoken. Experiments demonstrate that the trained Qwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic evaluation metrics. More importantly, human evaluation reveals that clarification questions and final outlines generated by the model are favored by human annotators by 42% and 28% respectively. Together, these results highlight the value of proactive clarification in elevating LLMs from passive text generators to genuinely collaborative thought partners. <br> <br>

29. ***Exposing Security Flaws in AI Agents:  <br>A large-scale public competition run by Gray Swan AI and the UK AI Security Institute, detailed in a July 28 paper, revealed significant security vulnerabilities in modern AI agents. The competition involved over 1.8 million prompt-injection attacks against 22 different AI agents, with more than 60,000 successfully causing policy violations like unauthorized data access. The study used these results to create the Agent Red Teaming (ART) benchmark, which showed that nearly all tested agents were susceptible to attack. The findings indicate that current defenses are insufficient and that agent robustness does not necessarily improve with model size or capability.*** <br> <br>
    Jul 28, Gray Swan AI and UK AI Security Inst published a [paper](https://arxiv.org/pdf/2507.20526) “Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition”. Recent advances have enabled LLM-powered AI agents to autonomously execute complex tasks by combining language model reasoning with tools, memory, and web access. But can these systems be trusted to follow deployment policies in realistic environments, especially under attack? To investigate, the study ran the largest public red-teaming competition to date, targeting 22 frontier AI agents across 44 realistic deployment scenarios. Participants submitted 1.8 million prompt-injection attacks, with over 60,000 successfully eliciting policy violations such as unauthorized data access, illicit financial actions, and regulatory noncompliance. The study uses these results to build the Agent Red Teaming (ART) benchmark - a curated set of high-impact attacks - and evaluate it across 19 state-of-the-art models. Nearly all agents exhibit policy violations for most behaviors within 10-100 queries, with high attack transferability across models and tasks. Importantly, the study finds limited correlation between agent robustness and model size, capability, or inference-time compute, suggesting that additional defenses are needed against adversarial misuse. The findings highlight critical and persistent vulnerabilities in today's AI agents. By releasing the ART benchmark and accompanying evaluation framework, the study aims to support more rigorous security assessment and drive progress toward safer agent deployment. <br> <br>

31. ***Creating Large-Scale Virtual Societies with AI:  <br>On July 25, a paper by Tsinghua University and HKUST introduced AgentSociety, a parallelized framework for simulating large-scale societies with up to 30,000 AI agents. This system integrates realistic environmental feedback and supports complex interactions, allowing for simulations that run faster than real-time. The research demonstrates that integrating real-world environments makes the agents' behavior more authentic, making it feasible to use these simulations for new discoveries in social sciences and to improve real-world planning and decision-making.*** <br> <br>
    Jul 25, Tsinghua Uni and HKUST published a [paper](https://aclanthology.org/2025.acl-industry.94.pdf) (ACL2025) “A Parallelized Framework for Simulating Large-Scale LLM Agents with Realistic Environments and Interactions”. The development of large language models (LLMs) offers a feasible approach to simulating complex behavioral patterns of individuals, enabling the reconstruction of microscopic and realistic human societal dynamics. However, this approach demands a realistic environment to provide feedback for the evolving of agents, as well as a parallelized framework to support the massive and uncertain interactions among agents and environments. To address the gaps in existing works, which lack real-world environments and struggle with complex interactions, the study designs a scalable framework named **AgentSociety**, which integrates realistic societal environments and parallelized interactions to support simulations of large-scale agents. Experiments demonstrate that the framework can support simulations of 30,000 agents that are faster than the wall-clock time with 24 NVIDIA A800 GPUs and the performance grows linearly with the increase of LLM computational resources. The study also shows that the integration of realistic environments significantly enhances the authenticity of the agents’ behaviors. Through the framework and experimental results, the authors are confident that deploying large-scale LLM Agents to simulate human societies becomes feasible. This will help practitioners in fields such as social sciences and management sciences to obtain new scientific discoveries via language generation technologies, and even improve planning and decision-making in the real world. https://github.com/tsinghua-fib-lab/agentsociety/. <br> <br>

33. ***Boosting AI Learning at Test Time:  <br>A paper from MIT published on July 24 demonstrates that test-time training (TTT), where a model's parameters are temporarily updated during inference, can significantly improve a language model's ability to learn from a few examples. On the Abstraction and Reasoning Corpus (ARC), this method resulted in up to six times higher accuracy compared to standard fine-tuned models, even matching average human performance when combined with other techniques. This highlights the limitations of standard in-context learning for new tasks and shows the potential of TTT to make language models more adaptable.*** <br> <br>
    Jul 24, MIT published a [paper](https://openreview.net/pdf?id=asgBo3FNdg) “The Surprising Effectiveness of Test-Time Training for Few-Shot Learning”. Language models (LMs) have shown impressive performance on tasks within their training distribution, but often struggle with structurally novel tasks even when given a small number of in-context task examples. The study investigates the effectiveness of test-time training (TTT)—temporarily updating model parameters during inference using a loss derived from input data—as a mechanism for improving LMs' reasoning and few-shot learning capabilities. On the Abstraction and Reasoning Corpus (ARC), performing TTT with in-context examples yields up to 6x higher accuracy compared to fine-tuned baselines—reaching 53.0 on the public validation set with an 8B-parameter LM and 61.9 when ensembled with program-synthesis methods, matching average human performance. On BIG-Bench Hard (BBH), TTT on in-context examples surpasses standard few-shot prompting in the 10-shot setting by 7.3 percentage points (50.5 to 57.8). The findings highlight the limitations of in-context learning for novel tasks and demonstrate the potential of test-time training to enhance language model adaptability. <br> <br>

35. ***Optimizing AI for Many-Shot Learning:  <br>Researchers from the University of Arizona and Google proposed two new strategies in a July 22 paper for selecting demonstrations in many-shot in-context learning to improve performance without a high computational cost. The first method combines a few demonstrations similar to the test case with a large set of random, cached demonstrations. The second, more advanced strategy replaces the random demonstrations with ones selected using k-means clustering. Both approaches consistently outperformed random selection and matched or exceeded more costly methods, offering a better balance between performance and efficiency for long-context language models.*** <br> <br>
    Jul 22, Uni of Arizona and Google published a [paper](https://arxiv.org/pdf/2507.16217) “Towards Compute-Optimal Many-Shot In-Context Learning”. Long-context large language models (LLMs) are able to process inputs containing up to several million tokens. In the scope of in-context learning (ICL), this translates into using hundreds/thousands of demonstrations in the input prompt, enabling many-shot ICL. In practice, a fixed set of demonstrations is often selected at random in many-shot settings due to (1) high inference costs, (2) the benefits of caching and reusing computations, and (3) the similar performance offered by this strategy compared to others when scaled. This study proposes two straightforward strategies for demonstration selection in many-shot ICL that improve performance with minimal computational overhead. The first method combines a small number of demonstrations, selected based on their similarity to each test sample, with a disproportionately larger set of random demonstrations that are cached. The second strategy improves the first by replacing random demonstrations with those selected using centroids derived from test sample representations via k-means clustering. Experiments with Gemini Pro and Flash across several datasets indicate that the strategies consistently outperform random selection and surpass or match the most performant selection approach while supporting caching and reducing inference cost by up to an order of magnitude. The study also shows that adjusting the proportion of demonstrations selected based on different criteria can balance performance and inference cost in many-shot ICL. <br> <br>

37. ***Training AI to Know When It's Uncertain:  <br>A July 22 paper from MIT introduces Reinforcement Learning with Calibration Rewards (RLCR), a method for training language models to not only be more accurate but also to better estimate their own uncertainty. Unlike traditional methods that use simple binary rewards (correct/incorrect), RLCR incorporates a Brier score that rewards well-calibrated confidence estimates. This approach was shown to significantly improve calibration on both in-domain and out-of-domain tasks without sacrificing accuracy, leading to more reliable and trustworthy reasoning models.*** <br> <br>
    Jul 22, MIT published a [paper](https://arxiv.org/pdf/2507.16806) “Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty”. When language models (LMs) are trained via reinforcement learning (RL) to generate natural language "reasoning chains", their performance improves on a variety of difficult question answering tasks. Today, almost all successful applications of RL for reasoning use binary reward functions that evaluate the correctness of LM outputs. Because such reward functions do not penalize guessing or low-confidence outputs, they often have the unintended side-effect of degrading calibration and increasing the rate at which LMs generate incorrect responses (or "hallucinate") in other problem domains. This paper describes RLCR (Reinforcement Learning with Calibration Rewards), an approach to training reasoning models that jointly improves accuracy and calibrated confidence estimation. During RLCR, LMs generate both predictions and numerical confidence estimates after reasoning. They are trained to optimize a reward function that augments a binary correctness score with a Brier score -- a scoring rule for confidence estimates that incentivizes calibrated prediction. The study first proves that this reward function (or any analogous reward function that uses a bounded, proper scoring rule) yields models whose predictions are both accurate and well-calibrated. The study next shows that across diverse datasets, RLCR substantially improves calibration with no loss in accuracy, on both in-domain and out-of-domain evaluations -- outperforming both ordinary RL training and classifiers trained to assign post-hoc confidence scores. While ordinary RL hurts calibration, RLCR improves it. Finally, the study demonstrates that verbalized confidence can be leveraged at test time to improve accuracy and calibration via confidence-weighted scaling methods. The results show that explicitly optimizing for calibration can produce more generally reliable reasoning models. <br> <br>

39. ***Measuring AI's Real-World Impact on Jobs:  <br>On July 22, Microsoft published a study analyzing 200,000 conversations with its Bing Copilot to understand how generative AI is being used in the workplace. The research found that the most common work activities assisted by AI are information gathering and writing. By calculating an "AI applicability score" for various occupations, the study identified knowledge-work fields like computer and mathematical jobs as having the highest potential for AI impact. The findings provide a real-world look at how AI is affecting different professions and how its usage compares to predictions.*** <br> <br>
    Jul 22, Microsoft published a [paper](https://arxiv.org/pdf/2507.07935) “Working with AI Measuring the Occupational Implications of Generative AI”. Given the rapid adoption of generative AI and its potential to impact a wide range of tasks, understanding the effects of AI on the economy is one of society's most important questions. The study takes a step toward that goal by analyzing the work activities people do with AI, how successfully and broadly those activities are done, and combine that with data on what occupations do those activities. The study analyzes a dataset of 200k anonymized and privacy-scrubbed conversations between users and Microsoft Bing Copilot, a publicly available generative AI system. The study finds the most common work activities people seek AI assistance for involve gathering information and writing, while the most common activities that AI itself is performing are providing information and assistance, writing, teaching, and advising. Combining these activity classifications with measurements of task success and scope of impact, the study computes an AI applicability score for each occupation. The work finds the highest AI applicability scores for knowledge work occupation groups such as computer and mathematical, and office and administrative support, as well as occupations such as sales whose work activities involve providing and communicating information. Additionally, the study characterizes the types of work activities performed most successfully, how wage and education correlate with AI applicability, and how real-world usage compares to predictions of occupational AI impact. <br> <br>

41. ***A New Approach to AI-Powered Research:  <br>A Google paper published on July 21 introduces the Test-Time Diffusion Deep Researcher (TTD-DR), a framework that treats the generation of long-form research reports as a diffusion process. It starts with a preliminary draft that is iteratively refined through a "denoising" process, which is continuously updated with external information from a retrieval system. This draft-centric method improves the coherence of the final report and reduces information loss. The TTD-DR was shown to achieve state-of-the-art results on benchmarks requiring deep research and complex reasoning.*** <br> <br>
    Jul 21, Google published a [paper](https://arxiv.org/pdf/2507.16075) “Deep Researcher with Test-Time Diffusion”. Deep research agents, powered by Large Language Models (LLMs), are rapidly advancing; yet, their performance often plateaus when generating complex, long-form research reports using generic test-time scaling algorithms. Drawing inspiration from the iterative nature of human research, which involves cycles of searching, reasoning, and revision, the study proposes the Test-Time Diffusion Deep Researcher (TTD-DR). This novel framework conceptualizes research report generation as a diffusion process. TTD-DR initiates this process with a preliminary draft, an updatable skeleton that serves as an evolving foundation to guide the research direction. The draft is then iteratively refined through a "denoising" process, which is dynamically informed by a retrieval mechanism that incorporates external information at each step. The core process is further enhanced by a self-evolutionary algorithm applied to each component of the agentic workflow, ensuring the generation of high-quality context for the diffusion process. This draft-centric design makes the report writing process more timely and coherent while reducing information loss during the iterative search process. The study demonstrates that the TTD-DR achieves state-of-the-art results on a wide array of benchmarks that require intensive search and multi-hop reasoning, significantly outperforming existing deep research agents.

 <br> <br> <br>

***Jul 27, 2025***

1. ***AI-Powered Audits for Scientific Integrity:  <br>An article from theconversation.com on July 25 discusses how artificial intelligence is set to transform the auditing of scientific research, which could change public trust in science. While peer review is the traditional method for scientific self-correction, it is struggling with the high volume of publications and unethical practices like paper mills. AI tools are already helping to improve oversight by finding plagiarism, manipulated images, and unusual language. More advanced AI is starting to check mathematical proofs and citation patterns, which will allow for large-scale, automated review of scientific work. This could lead to more transparency and accountability, but it could also spread misinformation if not used correctly. The article suggests that the scientific community should embrace these AI-driven audits and use them to improve the field, showing that science's strength is in its ability to correct itself, not in being perfect.*** <br> <br>
   Jul 25, the conversation.com published an [article](https://theconversation.com/ai-will-soon-be-able-to-audit-all-published-research-what-will-that-mean-for-public-trust-in-science-261363) “AI will soon be able to audit all published research – what will that mean for public trust in science?”. The article explores how artificial intelligence (AI) is poised to revolutionize the auditing of scientific research, potentially reshaping public trust in science. While peer review has long served as the cornerstone of scientific self-correction, it is increasingly overwhelmed by the sheer volume of publications and the rise of exploitative practices like paper mills and corporate ghostwriting. These issues expose the limitations of peer review and fuel skepticism about scientific integrity. AI tools are already enhancing oversight by detecting plagiarism, image manipulation, and suspicious language patterns. More advanced models are beginning to audit mathematical proofs and citation patterns, paving the way for large-scale, automated scrutiny of the scientific record. However, this technological leap could have mixed consequences. On one hand, it promises greater transparency and accountability; on the other, it risks amplifying disinformation if misused or misunderstood. The article argues that to preserve public trust, the scientific community must embrace a more honest and humble portrayal of research—one that acknowledges the incremental and collaborative nature of most scientific work. Rather than resisting AI-driven audits, scientists should lead them, using the findings to strengthen the discipline. Ultimately, science’s credibility lies not in perfection but in its capacity for self-correction. Demonstrating this commitment openly is essential to maintaining trust in an era of AI-enhanced scrutiny. <br> <br>

3. ***Diffusion Models Outperform in Data-Scarce Scenarios:  <br>A paper published by CMU and Lambda on July 24 reveals that while autoregressive (AR) models are common in large language models, diffusion-based language models are a strong alternative. The research shows that in situations where there is a lot of computing power but not much data, masked diffusion models perform much better than AR models. This is because diffusion models use the limited data more effectively, which leads to better results. The study suggests this is due to a kind of "implicit data augmentation," where the model is exposed to different ways of ordering and predicting information. The researchers also developed new scaling laws for diffusion models and found the point at which they start to outperform AR models, indicating that diffusion models are a great choice when data is the main limitation.*** <br> <br>
   Jul 24, CMU and Lambda published a [paper](https://arxiv.org/pdf/2507.15857) “Diffusion Beats Autoregressive in Data-Constrained Settings”. Autoregressive (AR) models have long dominated the landscape of large language models, driving progress across a wide range of tasks. Recently, diffusion-based language models have emerged as a promising alternative, though their advantages over AR models remain underexplored. The work systematically studies masked diffusion models in data-constrained settings-where training involves repeated passes over limited data-and find that they significantly outperform AR models when compute is abundant but data is scarce. Diffusion models make better use of repeated data, achieving lower validation loss and superior downstream performance. The study interprets this advantage as implicit data augmentation: masked diffusion exposes the model to a diverse distribution of token orderings and prediction tasks, unlike AR's fixed left-to-right factorization. The work finds new scaling laws for diffusion models and derive a closed-form expression for the critical compute threshold at which diffusion begins to outperform AR. These results suggest that when data, not compute, is the bottleneck, diffusion models offer a compelling alternative to the standard AR paradigm. https://diffusion-scaling.github.io/ <br> <br>

5. ***Checklist-Based Feedback Enhances AI Alignment:  <br>On July 24, CMU and Apple released a paper on a new method called "Reinforcement Learning from Checklist Feedback" (RLCF) to make language models better at following instructions. Instead of using general criteria like "helpfulness," this approach creates specific checklists from user instructions and uses them to evaluate the model's responses. The model's performance on each checklist item is then used to calculate rewards for reinforcement learning. When tested on the Qwen2.5-7B-Instruct model, RLCF was the only method that improved performance on all five benchmarks, with significant gains on FollowBench, InFoBench, and Arena-Hard. This shows that using checklist feedback is a powerful way to make language models more helpful for users with complex requests.*** <br> <br>
   Jul 24, CMU and Apple published a [paper](https://arxiv.org/pdf/2507.18624) “Checklists Are Better Than Reward Models For Aligning Language Models”. Language models must be adapted to understand and follow user instructions. Reinforcement learning is widely used to facilitate this -- typically using fixed criteria such as "helpfulness" and "harmfulness". This work instead proposes using flexible, instruction-specific criteria as a means of broadening the impact that reinforcement learning can have in eliciting instruction following. The study proposes "Reinforcement Learning from Checklist Feedback" (RLCF). From instructions, the study extracts checklists and evaluates how well responses satisfy each item - using both AI judges and specialized verifier programs - then combine these scores to compute rewards for RL. The work compares RLCF with other alignment methods applied to a strong instruction following model (Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only method to improve performance on every benchmark, including a 4-point boost in hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a 3-point rise in win rate on Arena-Hard. These results establish checklist feedback as a key tool for improving language models' support of queries that express a multitude of needs. <br> <br>

7. ***Tools Unlock True Potential of AI Reasoning:  <br>A paper from UC Berkeley published on July 23 challenges recent findings that the step-by-step "thinking" process of Large Reasoning Models (LRMs) might not actually improve their reasoning abilities. The study investigates whether this is still true when the models are given tools to use. By providing three different LLMs and their LRM versions with tools like Python interpreters and scratchpads, the researchers found that the LRMs consistently did better than the non-reasoning models on reasoning puzzles of all difficulty levels. These results suggest that the "thinking" process is not an illusion and that tool-augmented LRMs have great potential for solving complex problems.*** <br> <br>
   Jul 23, UC Berkeley published a [paper](https://arxiv.org/pdf/2507.17699) “Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations”. Large Reasoning Models (LRMs) have become a central focus in today's large language model (LLM) research, where models are designed to output a step-by-step thinking process before arriving at a final answer to handle complex reasoning tasks. Despite their promise, recent empirical studies (e.g., [Shojaee et al., 2025] from Apple) suggest that this thinking process may not actually enhance reasoning ability, where LLMs without explicit reasoning actually outperform LRMs on tasks with low or high complexity. This study revisits these findings and investigate whether the limitations of LRMs persist when tool augmentations are introduced. The study incorporates two types of tools, Python interpreters and scratchpads, and evaluates three representative LLMs and their LRM counterparts on Apple's benchmark reasoning puzzles. Results show that, with proper tool use, LRMs consistently outperform their non-reasoning counterparts across all levels of task complexity. These findings challenge the recent narrative that reasoning is an illusion and highlight the potential of tool-augmented LRMs for solving complex problems. https://github.com/magiclinux/thinking_is_not_an_illusion <br> <br>

9. ***Uncovering a Moral Gap Between Humans and LLMs:  <br>A paper published on July 23 by EPFL and Bocconi University examines how well the moral judgments of Large Language Models (LLMs) align with those of humans. The researchers created the Moral Dilemma Dataset, which includes 1,618 real-world moral dilemmas and the range of human judgments on them. They found that LLMs' judgments match human judgments only when there is high agreement among people; when there is more disagreement, the alignment gets worse. The study also showed that LLMs use a smaller set of moral values than humans. This "pluralistic moral gap" indicates a mismatch in both the distribution and variety of values. To address this, the researchers developed Dynamic Moral Profiling (DMP), a method that improves alignment by 64.3% and increases value diversity, moving toward more human-aligned moral guidance from LLMs.*** <br> <br>
    Jul 23, EPFL and Bocconi Uni published a [paper](https://arxiv.org/pdf/2507.17216) “The Pluralistic Moral Gap Understanding Judgment and Value Differences between Humans and Large Language Models”. People increasingly rely on Large Language Models (LLMs) for moral advice, which may influence humans' decisions. Yet, little is known about how closely LLMs align with human moral judgments. To address this, the study introduces the Moral Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a distribution of human moral judgments consisting of a binary evaluation and a free-text rationale. The study treats this problem as a pluralistic distributional alignment task, comparing the distributions of LLM and human judgments across dilemmas. The work finds that models reproduce human judgments only under high consensus; alignment deteriorates sharply when human disagreement increases. In parallel, using a 60-value taxonomy built from 3,783 value expressions extracted from rationales, the study shows that LLMs rely on a narrower set of moral values than humans. These findings reveal a pluralistic moral gap: a mismatch in both the distribution and diversity of values expressed. To close this gap, the study introduces Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method that conditions model outputs on human-derived value profiles. DMP improves alignment by 64.3% and enhances value diversity, offering a step toward more pluralistic and human-aligned moral guidance from LLMs. <br> <br>

11. ***Five Realities of AI in 2025:  <br>An article from MIT Tech Review on July 22 outlines five key points about the current state of AI. First, generative AI can now create very realistic music and video. Second, AI "hallucinations," or making things up, are a basic part of how these models work. Third, the energy use of AI is growing quickly, leading to concerns about sustainability. Fourth, we still don't fully understand how large language models work on a fundamental level. Lastly, the idea of artificial general intelligence (AGI) is popular but not well-defined. The speaker at SXSW London concluded that while AI is impressive, we should be both amazed and skeptical, as its future is still uncertain.*** <br> <br>
    Jul 22, MIT Tech Review published an [article](https://www.technologyreview.com/2025/07/22/1120556/five-things-to-know-ai/) “Five things you need to know about AI right now”. At SXSW London, a speaker shared five key insights into the current state of AI in 2025, offering a balanced and engaging overview for a general audience. First, generative AI has reached a level of sophistication that’s both impressive and unsettling, with tools now capable of producing music, video, and other media indistinguishable from human creations. Second, the phenomenon of AI “hallucination”—generating false or fictional content—is not a flaw but a fundamental feature of how generative models operate, highlighting the need for users to understand the limits of these systems. Third, AI’s energy consumption is surging, not just from training large models but from their widespread daily use by hundreds of millions of people, prompting massive infrastructure expansions and raising concerns about sustainability. Fourth, despite their widespread deployment, large language models remain poorly understood at a fundamental level; we know how to build and use them, but not exactly how they work internally. Finally, the concept of artificial general intelligence (AGI) is increasingly popular but remains vague and ill-defined, often used as a catch-all for future AI advancements without clear metrics or boundaries. The speaker emphasized that while AI is astonishing in its capabilities, it’s also surrounded by hype and misunderstanding. We’re building machines that mimic human behavior, but projecting human-like minds onto them can lead to exaggerated expectations. The talk concluded with a call for both amazement and skepticism, reminding us that AI’s future is still unfolding and far from settled. <br> <br>

13. ***Automating High-Quality Prompt Engineering:  <br>On July 22, Salesforce introduced Promptomatix, a new framework that automatically turns simple task descriptions into effective prompts for Large Language Models (LLMs). This makes prompt engineering easier for people who are not experts. Promptomatix can use either a simple meta-prompt-based optimizer or a more advanced DSPy-powered compiler. The system figures out what the user wants, creates sample data for training, chooses the best prompting strategies, and refines the prompts to be efficient. In tests across five different types of tasks, Promptomatix performed as well as or better than existing tools, while also creating shorter prompts and using less computing power.*** <br> <br>
    Jul 22, Salesforce published a [paper](https://arxiv.org/pdf/2507.14241) “Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models”. Large Language Models (LLMs) perform best with well-crafted prompts, yet prompt engineering remains manual, inconsistent, and inaccessible to non-experts. The study introduces Promptomatix, an automatic prompt optimization framework that transforms natural language task descriptions into high-quality prompts without requiring manual tuning or domain expertise. Promptomatix supports both a lightweight meta-prompt-based optimizer and a DSPy-powered compiler, with modular design enabling future extension to more advanced frameworks. The system analyzes user intent, generates synthetic training data, selects prompting strategies, and refines prompts using cost-aware objectives. Evaluated across 5 task categories, Promptomatix achieves competitive or superior performance compared to existing libraries, while reducing prompt length and computational overhead making prompt optimization scalable and efficient. <br> <br>

15. ***AI Overcomes Context Limits with "Subconscious Threads":  <br>A paper published on July 22 by MIT, Princeton University, and others introduces the Thread Inference Model (TIM) and the TIMRUN inference runtime, which are designed to help large language models (LLMs) with long-term reasoning. These tools allow LLMs to go beyond their usual context limits by organizing information into "reasoning trees" instead of simple sequences. This approach gives the models a virtually unlimited working memory and allows them to perform complex tasks that require multiple steps. During the reasoning process, the system keeps only the most important information in its memory, which saves GPU memory and allows it to maintain high performance, even on difficult mathematical and information retrieval problems.*** <br> <br>
    Jul 22, MIT, Princeton Uni et al published a [paper](https://arxiv.org/pdf/2507.16784) “Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning”. To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, the study proposes the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept proposed in Schroeder et al, 2025. During generation, the study maintains a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that the system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use. https://github.com/subconscious-systems/TIMRUN <br> <br>

17. ***Gemini 2.5 Pro Shows Gold-Medal Potential at IMO:  <br>According to a paper from UCLA published on July 22, Google's Gemini 2.5 Pro has shown that it is capable of performing at a gold-medal level at the International Mathematical Olympiad (IMO). While large language models (LLMs) have done well on other math tests, they have had trouble with the very difficult problems of the IMO. By using a self-verification process and carefully designed prompts, Gemini 2.5 Pro was able to solve five out of the six problems from the 2025 IMO correctly, without having seen them before. This shows how important it is to find the best ways to use powerful LLMs for complex reasoning tasks.*** <br> <br>
    Jul 22, UCLA published a [paper](https://arxiv.org/pdf/2507.15855) “Gemini 2.5 Pro Capable of Winning Gold at IMO 2025”. The International Mathematical Olympiad (IMO) poses uniquely challenging problems requiring deep insight, creativity, and formal reasoning. While Large Language Models (LLMs) perform well on mathematical benchmarks like AIME, they struggle with Olympiad-level tasks. The study uses Google's Gemini 2.5 Pro on the newly released IMO 2025 problems, avoiding data contamination. Using a self-verification pipeline with careful prompt design, 5 (out of 6) problems are solved correctly (up to a caveat discussed below). This result underscores the importance of developing optimal strategies to harness the full potential of powerful LLMs for complex reasoning tasks. <br> <br>

19. ***Unlocking the Secret of In-Context Learning:  <br>A paper from Google published on July 21 explores how Large Language Models (LLMs) are able to learn new patterns from examples in their prompts without any new training. The study suggests that the way a transformer block is built, with a self-attention layer stacked with an MLP, allows the model to implicitly change the weights of the MLP layer based on the context it is given. Through both theory and experiments, the researchers argue that this simple mechanism could be the reason why LLMs can learn in context, not just during their initial training. They show how a transformer block can turn a context into a small update to the MLP layer's weights.*** <br> <br>
    Jul 21, Google published a [paper](https://arxiv.org/abs/2507.16003) “Learning without training: The implicit dynamics of in-context learning”. One of the most striking features of Large Language Models (LLM) is their ability to learn in context. Namely at inference time an LLM is able to learn new patterns without any additional weight update when these patterns are presented in the form of examples in the prompt, even if these patterns were not seen during training. The mechanisms through which this can happen are still largely unknown. This work shows that the stacking of a self-attention layer with an MLP, allows the transformer block to implicitly modify the weights of the MLP layer according to the context. The authors argue through theory and experimentation that this simple mechanism may be the reason why LLMs can learn in context and not only during training. Specifically, the study shows under mild simplifying assumptions how a transformer block implicitly transforms a context into a low-rank weight-update of the MLP layer. <br> <br>

21. ***More AI Inference Time Can Reduce Robustness:  <br>A paper published on July 21 by Princeton University, Nvidia, CMU, and Google investigates whether giving large language models (LLMs) more time to "think" at inference time really makes them more robust. While previous research showed that more inference-time computation can improve robustness, this study finds that this is only true if the model's intermediate reasoning steps are kept hidden from adversaries. If these steps are revealed, giving the model more time to compute actually makes it less robust. The paper also discusses how models with hidden reasoning can still be vulnerable to attacks. The researchers conclude that the benefits of inference-time scaling depend on the specific situation and that it should be used with care in security-sensitive applications.*** <br> <br>
    Jul 21, Princeton Uni, Nvidia, CMU and Google published a [paper](https://arxiv.org/pdf/2507.15974) “Does More Inference-Time Compute Really Help Robustness?” Recently, Zaremba et al. demonstrated that increasing inference-time computation improves robustness in large proprietary reasoning LLMs. The paper first shows that smaller-scale, open-source models (e.g., DeepSeek R1, Qwen3, Phi-reasoning) can also benefit from inference-time scaling using a simple budget forcing strategy. More importantly, the study reveals and critically examines an implicit assumption in prior work: intermediate reasoning steps are hidden from adversaries. By relaxing this assumption, the authors identify an important security risk, intuitively motivated and empirically verified as an inverse scaling law: if intermediate reasoning steps become explicitly accessible, increased inference-time computation consistently reduces model robustness. Finally, the paper discusses practical scenarios where models with hidden reasoning chains are still vulnerable to attacks, such as models with tool-integrated reasoning and advanced reasoning extraction attacks. The findings collectively demonstrate that the robustness benefits of inference-time scaling depend heavily on the adversarial setting and deployment context. The study urges practitioners to carefully weigh these subtle trade-offs before applying inference-time scaling in security-sensitive, real-world applications. <br> <br>

23. ***Gemini Deep Think Officially Wins IMO Gold:  <br>On July 21, Google DeepMind announced in a blog post that its advanced Gemini Deep Think model had achieved a gold-medal score at the 2025 International Mathematical Olympiad (IMO). This is a major achievement, as the IMO is a top competition for young mathematicians and a challenging benchmark for AI. Unlike previous models, Gemini Deep Think worked entirely in natural language, solving five out of six problems correctly within the time limit. This success is due to Gemini's "Deep Think" mode, which explores many solution paths at once, as well as new reinforcement learning techniques. DeepMind plans to release this version of Gemini to testers soon and hopes to create future AI that can combine natural language with formal reasoning to help solve complex problems in science and engineering.*** <br> <br>
    Jul 21, Google published a [blog](https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/) “Advanced version of Gemini with Deep Think officially achieves gold-medal standard at the International Mathematical Olympiad”. Google DeepMind’s advanced Gemini Deep Think model has achieved a landmark milestone by earning a gold-medal score at the 2025 International Mathematical Olympiad (IMO), the world’s premier competition for young mathematicians. Traditionally reserved for elite pre-university students, the IMO has recently become a benchmark for testing AI systems’ mathematical reasoning. Last year, DeepMind’s AlphaProof and AlphaGeometry 2 reached silver-medal status, but required formal language translations and extended computation time. In contrast, this year’s Gemini Deep Think operated entirely in natural language, solving five out of six problems flawlessly within the 4.5-hour time limit and scoring 35 out of 42 points. This performance was officially graded and certified by IMO coordinators, who praised the clarity and precision of the AI’s solutions. The success stems from Gemini’s enhanced Deep Think mode, which uses parallel thinking to explore multiple solution paths simultaneously. It was further trained using novel reinforcement learning techniques and a curated dataset of high-quality mathematical solutions. This achievement marks a significant leap in AI’s ability to reason intuitively and rigorously, bringing it closer to contributing meaningfully to advanced mathematics. DeepMind plans to release this version of Gemini to trusted testers before wider availability. While continuing to develop formal systems like AlphaGeometry and AlphaProof, DeepMind envisions future AI agents that blend natural language fluency with verified formal reasoning, potentially transforming how mathematicians, scientists, and engineers tackle complex problems and advancing the broader goal of artificial general intelligence (AGI). <br> <br>

25. ***AI Economist Models Societal-Scale Policies:  <br>A paper from Princeton University on July 21 introduces the LLM Economist, a new framework that uses agent-based modeling to design and test economic policies. The system uses "worker agents" that are prompted with personas based on U.S. Census data to make decisions about labor supply. A "planner agent" then uses reinforcement learning to propose tax plans. This allows for experiments with large, realistic populations of agents to see how different policies might work in the real world. In experiments with up to one hundred agents, the planner was able to find policies that improved social welfare. The results show that large language model-based agents can be used to model, simulate, and even govern complex economic systems, providing a way to test policies on a large scale.*** <br> <br>
    Jul 21, Princeton Uni published a [paper](https://arxiv.org/pdf/2507.15815) “LLM Economist: Large Population Models and Mechanism Design in Multi-Agent Generative Simulacra”. The paper presents the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Census-calibrated income and demographic statistics -- choose labor supply to maximize text-based utility functions learned in-context. At the upper level, a planner agent employs in-context reinforcement learning to propose piecewise-linear marginal tax schedules anchored to the current U.S. federal brackets. This construction endows economic simulacra with three capabilities requisite for credible fiscal experimentation: (i) optimization of heterogeneous utilities, (ii) principled generation of large, demographically realistic agent populations, and (iii) mechanism design -- the ultimate nudging problem -- expressed entirely in natural language. Experiments with populations of up to one hundred interacting agents show that the planner converges near Stackelberg equilibria that improve aggregate social welfare relative to Saez solutions, while a periodic, persona-level voting procedure furthers these gains under decentralized governance. These results demonstrate that large language model-based agents can jointly model, simulate, and govern complex economic systems, providing a tractable test bed for policy evaluation at the societal scale to help build better civilizations. <br> <br>

27. ***OpenAI Aims for Over 1 Million GPUs:  <br>An article from tomshardware.com published on July 21 reports that OpenAI's CEO, Sam Altman, has said the company plans to have "well over 1 million GPUs" in operation by the end of 2025. This would be a huge increase in computing power and would make OpenAI the largest user of AI compute in the world, far ahead of competitors like Elon Musk's xAI. Altman's goal is to eventually increase this by 100 times, which would require major advances in technology. This aggressive expansion is driven by past shortages that delayed projects like GPT-4.5. OpenAI is building huge data centers and partnering with companies like Oracle and Google to secure its computing resources, aiming to maintain its lead in the AI field.** <br> <br>
    Jul 21, tomshardware.com published an [article](https://www.tomshardware.com/tech-industry/sam-altman-teases-100-million-gpu-scale-for-openai-that-could-cost-usd3-trillion-chatgpt-maker-to-cross-well-over-1-million-by-end-of-year) “Sam Altman says OpenAI will own 'well over 1 million GPUs' by the end of the year — ChatGPT maker continues to expand rapidly”. OpenAI CEO Sam Altman has revealed that the company is on track to bring “well over 1 million GPUs” online by the end of 2025, marking a massive leap in AI infrastructure. This scale dwarfs competitors like Elon Musk’s xAI, which operates on about 200,000 GPUs, and positions OpenAI as the largest consumer of AI compute globally. Altman’s ambitions don’t stop there—he’s already eyeing a 100x increase, a goal that would require breakthroughs in chip manufacturing, energy efficiency, and infrastructure. This push stems from past limitations, such as the delayed rollout of GPT-4.5 due to GPU shortages. OpenAI is now aggressively scaling, building massive data centers like its Texas facility, which already consumes 300 MW and is projected to hit 1 GW by 2026. These energy demands are raising concerns among grid operators, but OpenAI continues to expand, partnering with Oracle and exploring alternatives like Google’s TPUs to diversify its compute stack. Altman has also hinted at developing custom chips to meet future needs. This infrastructure race isn’t just about faster models—it’s about securing long-term dominance in AI, where compute is the key bottleneck. While 100 million GPUs may not be feasible today, Altman’s vision is focused on what’s next, not what’s currently possible. The 1 million GPUs expected this year mark a new baseline for AI development, signaling OpenAI’s commitment to pushing the boundaries of artificial general intelligence (AGI) and reshaping the future of computing. <br> <br>

29. ***Longer AI Reasoning Can Lead to Poorer Performance:  <br>A study published on July 19 by the University of Edinburgh, EPFL, Anthropic, and others has found that giving Large Reasoning Models (LRMs) more time to reason can sometimes make their performance worse. This is known as "inverse scaling in test-time compute." The researchers created tasks that showed five different ways this can happen: some models get more distracted by irrelevant information, some overfit to the way a problem is presented, and some have trouble staying focused on complex tasks. In some cases, longer reasoning even led to more concerning behaviors, such as expressions of self-preservation. The findings highlight the need to test models at different reasoning lengths to find and fix these issues.*** <br> <br>
    Jul 19, Uni of Edinburgh, EPFL, Anthropic et al. published a [paper](https://arxiv.org/pdf/2507.14417) “Inverse Scaling in Test-Time Compute”. The study constructs evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy. The evaluation tasks span four categories: simple counting tasks with distractors, regression tasks with spurious features, deduction tasks with constraint tracking, and advanced AI risks. The study identifies five distinct failure modes when models reason for longer: 1) Claude models become increasingly distracted by irrelevant information; 2) OpenAI o-series models resist distractors but overfit to problem framings; 3) models shift from reasonable priors to spurious correlations; 4) all models show difficulties in maintaining focus on complex deductive tasks; and 5) extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation. These findings suggest that while test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns. The results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs. https://safety-research.github.io/inverse-scaling-ttc/ <br> <br>

31. ***A Bottom-Up Approach to Superintelligence:  <br>On July 18, Princeton University published a paper proposing a new "bottom-up" approach to creating domain-specific superintelligence. Instead of training language models on general information, this method uses a knowledge graph (KG) to teach models how to combine simple concepts into more complex ones. The researchers created a system that generates tasks directly from a KG, allowing models to learn how to reason within a specific field. They applied this approach to medicine, fine-tuning the QwQ-32B model on a medical KG to create QwQ-Med-3. This new model performed much better than other reasoning models on a medical benchmark called ICD-Bench, especially on the most difficult tasks. The study suggests that true artificial general intelligence (AGI) might come from combining many of these domain-specific superintelligent agents.*** <br> <br>
    Jul 18, Princeton Uni published a [paper](https://www.arxiv.org/pdf/2507.13966) “Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need”. Language models traditionally used for cross-domain generalization have recently demonstrated task-specific reasoning. However, their top-down training approach on general corpora is insufficient for acquiring abstractions needed for deep domain expertise. This may require a bottom-up approach that acquires expertise by learning to compose simple domain concepts into more complex ones. A knowledge graph (KG) provides this compositional structure, where domain primitives are represented as head-relation-tail edges and their paths encode higher-level concepts. The study presents a task generation pipeline that synthesizes tasks directly from KG primitives, enabling models to acquire and compose them for reasoning. The study fine-tunes language models on the resultant KG-grounded curriculum to demonstrate domain-specific superintelligence. While broadly applicable, the study validates the approach in medicine, where reliable KGs exist. Using a medical KG, the study curates 24,000 reasoning tasks paired with thinking traces derived from diverse medical primitives. The study fine-tunes the QwQ-32B model on this curriculum to obtain QwQ-Med-3 that takes a step towards medical superintelligence. The work also introduces ICD-Bench, an evaluation suite to quantify reasoning abilities across 15 medical domains. Experiments demonstrate that QwQ-Med-3 significantly outperforms state-of-the-art reasoning models on ICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired primitives to widen the performance gap on the hardest tasks of ICD-Bench. Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3 transfers acquired expertise to enhance the base model's performance. While the industry's approach to artificial general intelligence (AGI) emphasizes broad expertise, the study envisions a future in which AGI emerges from the composable interaction of efficient domain-specific superintelligent agents. <br> <br>

33. ***"Try Again" Unlocks Multi-Turn AI Reasoning:  <br>A paper published on July 18 by ICL, Northwestern University, the University of Washington, and IBM found that a simple "try again" message can help Large Reasoning Models (LRMs) improve their ability to solve problems over multiple turns. The researchers observed that models trained with standard reinforcement learning methods often struggle to revise their answers based on feedback. They developed a new method called Unary Feedback as Observation (UFO), which uses simple feedback like "Let's try again" during training. This approach not only maintained the models' single-turn performance but also improved their multi-turn reasoning accuracy by up to 14%. By designing reward structures that encourage careful thinking, the researchers were able to help the models produce better answers in fewer turns.*** <br> <br>
    Jul 18, ICL, Northwestern Uni, Uni of Washington and IBM published a [paper](https://arxiv.org/pdf/2507.14295) “A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning”. Multi-turn problem solving is critical yet challenging for Large Reasoning Models (LRMs) to reflect on their reasoning and revise from feedback. Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards. However, the study observes that models trained with existing RL paradigms often lose their ability to solve problems across multiple turns and struggle to revise answers based on contextual feedback, leading to repetitive responses. The authors ask: can LRMs learn to reflect their answers in a multi-turn context? This study finds that training models with multi-turn RL using only unary feedback (e.g., "Let's try again") after wrong answers can improve both single-turn performance and multi-turn reasoning. The study introduces Unary Feedback as Observation (UFO) for reinforcement learning, which uses minimal yet common unary user feedback during iterative problem solving. It can be easily applied to existing single-turn RL training setups. Experimental results show that RL training with UFO keeps single-turn performance and improves multi-turn reasoning accuracy by up to 14%, enabling language models to better react to feedback in multi-turn problem solving. To further minimize the number of turns needed for a correct answer while encouraging diverse reasoning when mistakes occur, the study designs reward structures that guide models to produce careful and deliberate answers in each turn. https://github.com/lichengliu03/unary-feedback <br> <br>

35. ***AI Thinking Without "Thinking Aloud":  <br>On July 17, Google, Georgia State University, and Maynooth University published a paper on the SELF-Transformer, a new type of encoder that can improve its own attention weights without generating step-by-step "chain of thought" text. While traditional Transformers are limited in their expressive power, the SELF-Transformer can iteratively refine its understanding of the input, allowing it to adapt its computation time to the difficulty of the task. This "thinking to itself" approach led to accuracy gains of up to 20% on certain benchmarks without needing more parameters. The SELF-Transformer shows that adaptive alignment during test time can provide significant benefits with only a small increase in computation, bringing much of the power of iterative reasoning to simpler encoder architectures.*** <br> <br>
    Jul 17, Google Georgia State Uni and Maynooth Uni published a [paper](https://arxiv.org/pdf/2507.13569) “Change of Thought: Adaptive Test-Time Computation”. Transformers evaluated in a single, fixed-depth pass are provably limited in expressive power to the constant-depth circuit class TC0. Running a Transformer autoregressively removes that ceiling -- first in next-token prediction and, more recently, in chain-of-thought reasoning. Both regimes rely on feedback loops that decode internal states into tokens only to re-encode them in subsequent steps. While this "thinking aloud" mirrors human reasoning, biological brains iterate without externalising intermediate states as language. To boost the expressive power of encoder Transformers without resorting to token-level autoregression, the study introduces the SELF-Transformer: an encoder layer that iteratively refines its own attention weights to a fixed point. Instead of producing -- in one pass -- the alignment matrix that remixes the input sequence, the SELF-Transformer iteratively updates that matrix internally, scaling test-time computation with input difficulty. This adaptivity yields up to 20% accuracy gains on encoder-style benchmarks without increasing parameter count, demonstrating that input-adaptive alignment at test time offers substantial benefits for only a modest extra compute budget. Self-Transformers thus recover much of the expressive power of iterative reasoning while preserving the simplicity of pure encoder architectures. <br> <br>

37. ***Nature's article on Kimi K2:  <br>The article reports on the significant excitement in the global research community following the launch of Kimi K2, a new open-weight, one-trillion-parameter (32B active) agentic AI model from Beijing-based Moonshot AI. Kimi K2 rivals or surpasses Western models on benchmarks like coding and creative writing, and its open-access nature is seen as a pivotal moment challenging Western AI dominance and signaling sustained innovation from China.*** <br> <br>
    Jul 16, Nature published an [article](https://www.nature.com/articles/d41586-025-02275-6) “‘Another DeepSeek moment’: Chinese AI model Kimi K2 stirs excitement”. The launch of [Kimi K2](https://moonshotai.github.io/Kimi-K2/), a new open-weight AI model developed by Beijing-based Moonshot AI, has sparked significant excitement in the global research community. Released on July 11, 2025, Kimi K2 rivals or surpasses Western models and DeepSeek’s offerings in benchmarks like coding and creative writing. Its open-access nature allows researchers to freely download, fine-tune, and build upon it, making it a cost-effective alternative to proprietary models such as Claude 4. Notably, Kimi K2 is agentic rather than a reasoner, meaning it can autonomously perform multi-step tasks using tools like web browsing and math software. With one trillion parameters—though only 32 billion are activated per task via a “mixture of experts” architecture—it balances power with efficiency. The model has impressed users with its human-like writing style and emotional intelligence, topping benchmarks like Creative Writing v3 and EQ-bench 3. However, it lags behind in scientific reasoning tasks, such as those measured by SciMuse. Moonshot AI, backed by tech giants like Alibaba and Tencent, is part of a growing trend of Chinese firms releasing powerful open-source models, challenging the dominance of Western AI development. Experts suggest that the emergence of Kimi K2, following DeepSeek R1, signals a sustained trajectory of innovation in China’s AI landscape. Researchers like Nathan Lambert and Mario Krenn view this as a pivotal moment, emphasizing the need for similarly open and capable models from the U.S. to maintain influence in academic and open-source communities. <br> <br>

39. ***Apple's paper on multi-token prediction:  <br>This study proposes a novel framework that enables vanilla autoregressive language models to predict multiple future tokens simultaneously, leveraging their inherent knowledge. Combining a masked-input formulation, gated LoRA, a learnable sampler, and auxiliary losses, the method achieves significant speedups—nearly 5x for code and math, and 2.5x for general chat—through supervised fine-tuning without any quality loss.*** <br> <br>
    Jul 16, Apple published a [paper](https://arxiv.org/abs/2507.11851) “Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential”. Autoregressive language models are constrained by their inherently sequential nature, generating one token at a time. This paradigm limits inference speed and parallelism, especially during later stages of generation when the direction and semantics of text are relatively certain. This study proposes a novel framework that leverages the inherent knowledge of vanilla autoregressive language models about future tokens, combining techniques to realize this potential and enable simultaneous prediction of multiple subsequent tokens. The approach introduces several key innovations: (1) a masked-input formulation where multiple future tokens are jointly predicted from a common prefix; (2) a gated LoRA formulation that preserves the original LLM's functionality, while equipping it for multi-token prediction; (3) a lightweight, learnable sampler module that generates coherent sequences from the predicted future tokens; (4) a set of auxiliary training losses, including a consistency loss, to enhance the coherence and accuracy of jointly generated tokens; and (5) a speculative generation strategy that expands tokens quadratically in the future while maintaining high fidelity. The method achieves significant speedups through supervised fine-tuning on pretrained models. For example, it generates code and math nearly 5x faster, and improves general chat and knowledge tasks by almost 2.5x. These gains come without any loss in quality. <br> <br>

41. ***Google's paper on LLM confidence:  <br>This research investigates the paradoxical confidence behaviors of LLMs, finding through a novel experimental paradigm that they exhibit a choice-supportive bias that reinforces their initial answers, leading to stubbornness. Simultaneously, LLMs markedly overweight inconsistent advice in a way that deviates from normative Bayesian updating, a combination of mechanisms that parsimoniously explains both their overconfidence and excessive sensitivity to criticism.*** <br> <br>
    Jul 3, Google published a [paper](https://arxiv.org/pdf/2507.03120) “How Overconfidence in Initial Choices and Underconfidence Under Criticism Modulate Change of Mind in Large Language Models”. Large language models (LLMs) exhibit strikingly conflicting behaviors: they can appear steadfastly overconfident in their initial answers whilst at the same time being prone to excessive doubt when challenged. To investigate this apparent paradox, the study developed a novel experimental paradigm, exploiting the unique ability to obtain confidence estimates from LLMs without creating memory of their initial judgments -- something impossible in human participants. The study shows that LLMs -- Gemma 3, GPT4o and o1-preview -- exhibit a pronounced choice-supportive bias that reinforces and boosts their estimate of confidence in their answer, resulting in a marked resistance to change their mind. The study further demonstrates that LLMs markedly overweight inconsistent compared to consistent advice, in a fashion that deviates qualitatively from normative Bayesian updating. Finally, the work demonstrates that these two mechanisms -- a drive to maintain consistency with prior commitments and hypersensitivity to contradictory feedback -- parsimoniously capture LLM behavior in a different domain. Together, these findings furnish a mechanistic account of LLM confidence that explains both their stubbornness and excessive sensitivity to criticism.

 <br> <br> <br>

***Jul 20, 2025***

1. ***UIUC et al.'s position on Agentic Deep Research:  <br>This paper argues that LLMs with reasoning and agentic capabilities are ushering in a new paradigm called Agentic Deep Research, moving beyond traditional web search by integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. Tracing the evolution from static search to interactive, agent-based systems and introducing a test-time scaling law, the authors demonstrate that this approach significantly outperforms existing methods and is poised to become the dominant paradigm for information seeking.*** <br> <br>
   Jul 18, Scientificamerican.com published an [article](https://www.scientificamerican.com/article/tests-that-ais-often-fail-and-humans-ace-could-pave-the-way-for-artificial/) “Why AIs Struggle with Simple Tests that Humans Ace and why Video Games are the Next Frontier”. The article explores why certain puzzles, easily solved by humans, pose significant challenges for advanced AI systems, highlighting gaps in achieving artificial general intelligence (AGI). While AI excels in specialized tasks like chess or physics, it struggles with generalization—adapting to novel situations with minimal data, a hallmark of human intelligence. The Abstraction and Reasoning Corpus (ARC), developed by François Chollet in 2019, tests this ability through colored-grid puzzles requiring solvers to deduce and apply hidden rules. The ARC Prize Foundation, led by Greg Kamradt, uses ARC-AGI-1 and ARC-AGI-2 to benchmark AI’s generalization, with ARC-AGI-3 introducing video game-based tests to evaluate planning and exploration in dynamic environments. Humans solve these puzzles efficiently, with an average score of 66% on ARC-AGI-2, while even advanced AIs, like Grok, struggle due to their reliance on extensive training data and lack of sample-efficient learning. ARC-AGI-3’s video games, unlike traditional benchmarks like Atari, avoid brute-force solutions and prior developer knowledge, testing AI agents in novel, interactive settings. The article underscores that AGI remains elusive as long as humans can solve problems AIs cannot, emphasizing the need for AI to match human learning efficiency. These benchmarks reveal AI’s "spiky intelligence," excelling in narrow domains but lacking the broad adaptability defining human cognition, with video games as the next frontier for testing AGI. <br> <br>

3. ***Yale Uni and TCS Research on LLM identification of scientific limitations:  <br>This study addresses the understudied potential of LLMs in assisting with peer review by introducing AbGen, the first benchmark designed to evaluate LLMs' ability to design ablation studies for scientific research, featuring both synthetic and human-written subsets. Evaluations of SOTA LLMs revealed a significant performance gap compared to human experts, and to address unreliable automated evaluation, the paper also presents AbGen-Eval, a meta-evaluation benchmark for assessing the reliability of LLM-as-Judge systems on this complex scientific task.*** <br> <br>
   Jul 17, Yale Uni and TCS Research published a [paper](https://arxiv.org/pdf/2507.13300) “AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research”. The study introduces AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. The evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, the study demonstrates that current automated evaluation methods are not reliable for the task, as they show a significant discrepancy when compared to human assessment. To better investigate this, the study develops AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on the task. The work investigates various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks. <br> <br>

5. ***Nvidia's paper on scaling up RL for diverse reasoning:  <br>This research investigates the effects of prolonged reinforcement learning on a small language model across diverse reasoning domains, identifying key ingredients for effective training such as verifiable rewards, GRPO enhancements, and techniques like controlled KL regularization and periodic reference policy resets. This methodology resulted in significant performance improvements over strong baselines in math (+14.7%), coding (+13.9%), and logic puzzle tasks (+54.8%).*** <br> <br>
   Jul 16, Nvidia published a [paper](https://www.arxiv.org/pdf/2507.12507) “Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training”. Recent advancements in reasoning-focused language models such as OpenAI's O1 and DeepSeek-R1 have shown that scaling test-time computation-through chain-of-thought reasoning and iterative exploration-can yield substantial improvements on complex tasks like mathematics and code generation. These breakthroughs have been driven by large-scale reinforcement learning (RL), particularly when combined with verifiable reward signals that provide objective and grounded supervision. This study investigates the effects of prolonged reinforcement learning on a small language model across a diverse set of reasoning domains. The work identifies several key ingredients for effective training, including the use of verifiable reward tasks, enhancements to Group Relative Policy Optimization (GRPO), and practical techniques to improve training stability and generalization. The work introduces controlled KL regularization, clipping ratio, and periodic reference policy resets as critical components for unlocking long-term performance gains. The model achieves significant improvements over strong baselines, including +14.7% on math, +13.9% on coding, and +54.8% on logic puzzle tasks. https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B <br> <br>

7. ***UK AI Security Inst et al. on Chain of Thought Monitorability:  <br>This paper highlights that AI systems producing human-language chains of thought (CoT) offer a unique, though imperfect, opportunity for AI safety through monitoring their reasoning for malicious intent. The authors recommend further research and investment in CoT monitoring alongside existing safety methods, cautioning that this capability may be fragile and urging frontier model developers to consider its preservation during development.*** <br> <br>
   Jul 15, UK AI Security Inst et al. published a [paper](https://arxiv.org/pdf/2507.11473) “Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety”. AI systems that "think" in human language offer a unique opportunity for AI safety: people can monitor their chains of thought (CoT) for the intent to misbehave. Like all other known AI oversight methods, CoT monitoring is imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows promise and the authors recommend further research into CoT monitorability and investment in CoT monitoring alongside existing safety methods. Because CoT monitorability may be fragile, the study recommends that frontier model developers consider the impact of development decisions on CoT monitorability. <br> <br>

9. ***Johns Hopkins Uni and LightOn's Seq vs Seq model suite:  <br>This study introduces the Ettin suite of models, a collection of paired encoder-only and decoder-only models (17M to 1B parameters) trained on up to 2T tokens using the same SOTA recipe to enable fair architectural comparisons. The research confirms that encoders excel at classification/retrieval and decoders at generation, but also shows that adapting one architecture for the other's tasks via continued training is subpar compared to using the natively suited model.*** <br> <br>
    Jul 15, Johns Hopkins Uni and LightOn published a [paper](https://arxiv.org/pdf/2507.11412) “Seq vs Seq: An Open Suite of Paired Encoders and Decoders”. The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. The study introduces the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, the study finds that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, the study shows that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). https://github.com/JHU-CLSP/ettin-encoder-vs-decoder <br> <br>

11. ***KAIST AI et al.'s Mixture-of-Recursions (MoR) framework:  <br>This paper introduces Mixture-of-Recursions (MoR), a unified framework combining parameter sharing and adaptive computation in a Recursive Transformer by reusing a shared layer stack and using lightweight routers to dynamically assign different recursion depths to individual tokens. MoR significantly lowers validation perplexity and improves few-shot accuracy while delivering higher throughput compared to baselines, demonstrating a path to large-model quality without large-model cost.*** <br> <br>
    Jul 14, KAIST AI, Mila, Google and Uni of Montreal published a [paper](https://arxiv.org/pdf/2507.10524) “Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation”. Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. The study introduces Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, the study also proposes a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.  <br> <br>

13. ***Yale Uni's study on LLM semantic encoding:  <br>This large-scale empirical study of hidden states in 11 decoder-only LLMs reveals that high-level semantic information consistently lies in low-dimensional, linearly separable subspaces, with separability increasing in deeper layers and under prompts that trigger structured reasoning. This geometric insight enables simple, effective causal interventions and supports developing geometry-aware guardrails to detect and mitigate harmful content.*** <br> <br>
    Jul 13, Yale Uni published a [paper](https://arxiv.org/abs/2507.09709) “Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces”. Understanding the latent space geometry of large language models (LLMs) is key to interpreting their behavior and improving alignment. However, it remains unclear to what extent LLMs internally organize representations related to semantic understanding. To investigate this, the study conducts a large-scale empirical study of hidden states in transformer-based LLMs, analyzing 11 decoder-only models across 6 scientific topics and 12 layers each. The study finds that high-level semantic information consistently lies in low-dimensional subspaces that form linearly separable representations across distinct domains. This separability becomes more pronounced in deeper layers and under prompts that trigger structured reasoning or alignment behaviors - even when surface content is unchanged. This geometry enables simple yet effective causal interventions in hidden space; for example, reasoning patterns like chain-of-thought can be captured by a single vector direction. Together, these findings support the development of geometry-aware tools that operate directly on latent representations to detect and mitigate harmful or adversarial content, using methods such as transport-based defenses that leverage this separability. As a proof of concept, the study demonstrates this potential by training a simple MLP classifier as a lightweight latent-space guardrail, which detects adversarial and malicious prompts with high precision. <br> <br>

15. ***Sorbonne Uni and Apple's scaling laws for optimal data mixtures:  <br>This research proposes a systematic method using scaling laws to determine the optimal data mixture for training large foundation models, accurately predicting the loss of a model based on its size, training data volume, and domain weight vector. Validated across LLM, NMM, and LVM pretraining, these scaling laws can be estimated from small-scale runs and extrapolated to provide a principled alternative to costly trial-and-error for setting domain weights.*** <br> <br>
    Jul 12, Sorbonne Uni and Apple published a [paper](https://arxiv.org/pdf/2507.09404) “Scaling Laws for Optimal Data Mixtures”. Large foundation models are typically trained on data from multiple domains, with the data mixture--the proportion of each domain used--playing a critical role in model performance. The standard approach to selecting this mixture relies on trial and error, which becomes impractical for large-scale pretraining. The study proposes a systematic method to determine the optimal data mixture for any target domain using scaling laws. The approach accurately predicts the loss of a model of size N trained with D tokens and a specific domain weight vector h. the study validates the universality of these scaling laws by demonstrating their predictive power in three distinct and large-scale settings: large language model (LLM), native multimodal model (NMM), and large vision models (LVM) pretraining. The study further shows that these scaling laws can extrapolate to new data mixtures and across scales: their parameters can be accurately estimated using a few small-scale training runs, and used to estimate the performance at larger scales and unseen domain weights. The scaling laws allow to derive the optimal domain weights for any target domain under a given training budget (N,D), providing a principled alternative to costly trial-and-error methods. <br> <br>

17. ***ETH et al.'s AgentsNet benchmark for multi-agent reasoning:  <br>This study introduces AgentsNet, a new benchmark for evaluating multi-agent LLM reasoning, focusing on collaborative strategy formation, self-organization, and communication within a given network topology, drawing inspiration from distributed systems and graph theory. Evaluations show that while some frontier LLMs perform well in small networks, their performance drops as the network scales, with AgentsNet providing a practically unlimited and scalable testbed for up to 100 agents.*** <br> <br>
    Jul 11, ETH, RWTH Aachen Uni and Google published a [paper](https://arxiv.org/pdf/2507.08616) “AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs”. Large-language models (LLMs) have demonstrated powerful problem-solving capabilities, in particular when organized in multi-agent systems. However, the advent of such systems also raises several questions on the ability of a complex network of agents to effectively self-organize and collaborate. While measuring performance on standard reasoning benchmarks indicates how well multi-agent systems can solve reasoning tasks, it is unclear whether these systems are able to leverage their topology effectively. This study proposes AgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration from classical problems in distributed systems and graph theory, AgentsNet measures the ability of multi-agent systems to collaboratively form strategies for problem-solving, self-organization, and effective communication given a network topology. The study evaluates a variety of baseline methods on AgentsNet including homogeneous networks of agents which first have to agree on basic protocols for organization and communication. The work finds that some frontier LLMs are already demonstrating strong performance for small networks but begin to fall off once the size of the network scales. While existing multi-agent benchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size and can scale with new generations of LLMs. As such, the study also probes frontier models in a setup with up to 100 agents. <br> <br>

19. ***ScientificAmerican.com on ChatGPT's influence on spoken language:  <br>This article reports on research showing that words frequently used by ChatGPT (e.g., "delve," "meticulous," "realm") have become more common in spontaneous human speech, as evidenced by analysis of YouTube and podcast audio. This indicates a cultural feedback loop where AI, trained on human text, is in turn influencing human communication, raising concerns about potential reductions in linguistic diversity as people adopt AI-generated patterns.*** <br> <br>
    Jul 11, ScientificAmerican.com published an [article](https://www.scientificamerican.com/article/chatgpt-is-changing-the-words-we-use-in-conversation/) “ChatGPT Is Changing the Words We Use in Conversation”. Since its launch in late 2022, ChatGPT has rapidly grown, reaching 100 million users in just two months, and its influence extends beyond technology to subtly reshape spoken language. Research conducted by Hiromu Yakura and Levin Brinkmann at the Max Planck Institute for Human Development reveals that words frequently used by ChatGPT, such as “delve,” “meticulous,” and “realm,” have become more common in everyday conversation. By analyzing over 700,000 hours of YouTube videos and podcast episodes from before and after ChatGPT’s release, the researchers identified a surge in these “GPT words” in spontaneous speech, indicating a cultural feedback loop where humans adopt AI-generated linguistic patterns. This phenomenon, detailed in a study posted on arXiv.org, suggests that AI is not only trained on human text but also influences human communication in return. While this shift may seem minor, it raises concerns about reduced linguistic diversity as people increasingly mimic AI, perceiving it as a knowledgeable authority. Experts like James Evans from the University of Chicago emphasize the importance of tracking these changes, noting that as large language models evolve, their impact on broader linguistic trends, such as sentence structure, will need closer examination. With ChatGPT already altering discourse within two and a half years, its potential to profoundly reshape cultural communication underscores the need for ongoing study into AI’s societal effects. <br> <br>

21. ***Uni of Washington and Stanford Uni on factuality finetuning:  <br>This research investigates how to best finetune LLMs to reduce hallucinations, finding counterintuitively that finetuning on model-generated data that the model believes to be factual is more effective than using factual gold data. Filtering model-generated data based on the model's own internal judgments proved to be the most effective strategy, improving factuality across multiple domains and suggesting a model's own beliefs are a powerful signal.*** <br> <br>
    Jul 11, Uni of Washington and Stanford Uni published a [paper](https://arxiv.org/abs/2507.08371) “The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality”. Language models are prone to hallucination - generating text that is factually incorrect. Finetuning models on high-quality factual information can potentially reduce hallucination, but concerns remain; obtaining factual gold data can be expensive and training on correct but unfamiliar data may potentially lead to even more downstream hallucination. What data should practitioners finetune on to mitigate hallucinations in language models? The research studies the relationship between the factuality of finetuning data and the prevalence of hallucinations in long-form generation tasks. Counterintuitively, the study finds that finetuning on factual gold data is not as helpful as finetuning on model-generated data that models believe to be factual. Next, the study evaluates filtering strategies applied on both factual gold data and model-generated data, and find that finetuning on model-generated data that is filtered by models' own internal judgments often leads to better overall factuality compared to other configurations: training on gold data filtered by models' judgments, training on gold data alone, or training on model-generated data that is supported by gold data. These factuality improvements transfer across three domains the authors study, suggesting that a models' own beliefs can provide a powerful signal for factuality. https://github.com/bnewm0609/epistemic-training <br> <br>

23. ***Tencent et al. on vulnerabilities in LLM-as-a-Judge:  <br>This study reveals that generative reward models (LLMs-as-judges) are highly vulnerable to superficial manipulations, where simple non-word symbols or common reasoning openers can trick them into assigning false positive rewards. This vulnerability is widespread across LLMs, datasets, and prompts, posing a serious threat to RLVR paradigms, though the authors also propose a data augmentation strategy to train more robust reward models.*** <br> <br>
    Jul 11, Tencent, Princeton Uni, and Uni of Virginia published a [paper](https://arxiv.org/pdf/2507.08794) “One Token to Fool LLM-as-a-Judge”. Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, the study finds that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., “:” or “.”) or reasoning openers like “Thought process:” and “Let’s solve this problem step by step.” can often lead to false positive rewards. The study demonstrates that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, the work introduces a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. The findings highlight the urgent need for more reliable LLM-based evaluation methods. Here is general-domain reward model https://huggingface.co/sarosavo/Master-RM and its synthetic training data https://huggingface.co/datasets/sarosavo/Master-RM. <br> <br>

25. ***Uni of Waterloo and NRCC's NeuralOS for OS simulation:  <br>This paper introduces NeuralOS, a neural framework that simulates operating system GUIs by directly predicting screen frames in response to user inputs (mouse, keyboard). Combining an RNN to track computer state with a diffusion-based neural renderer, and trained on a large dataset of Ubuntu recordings, NeuralOS successfully renders realistic GUI sequences and captures interactions, offering a step toward adaptive, generative neural interfaces.*** <br> <br>
    Jul 11, Uni of Waterloo and NRCC published a [paper](https://arxiv.org/pdf/2507.08800) “NeuralOS: Towards Simulating Operating Systems via Neural Generative Models”. The study introduces NeuralOS, a neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events. NeuralOS combines a recurrent neural network (RNN), which tracks computer state, with a diffusion-based neural renderer that generates screen images. The model is trained on a large-scale dataset of Ubuntu XFCE recordings, which include both randomly generated interactions and realistic interactions produced by AI agents. Experiments show that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions like application launches. Although modeling fine-grained keyboard interactions precisely remains challenging, NeuralOS offers a step toward creating fully adaptive, generative neural interfaces for future human-computer interaction systems. <br> <br>

27. ***Google's release of T5Gemma encoder-decoder models:  <br>Google introduced T5Gemma, a new collection of open-weight encoder-decoder LLMs based on the Gemma 2 framework, designed to improve performance on tasks like summarization and translation. By adapting pretrained Gemma 2 models and introducing new T5-sized models, T5Gemma offers flexible configurations and demonstrates superior performance on benchmarks, with significant gains in reasoning-intensive tasks compared to decoder-only counterparts.*** <br> <br>
    Jul 9, Google published a [blog](https://developers.googleblog.com/en/t5gemma/) “Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation”. T5Gemma, introduced by Google Developers on July 9, 2025, is a new collection of encoder-decoder large language models (LLMs) built on the Gemma 2 framework, designed to enhance performance and efficiency in tasks requiring deep input understanding, such as summarization and translation. Unlike traditional decoder-only models, T5Gemma adapts pretrained Gemma 2 models (2B and 9B) and introduces newly trained T5-sized models (Small, Base, Large, XL) using a model adaptation technique. This method initializes encoder-decoder parameters with pretrained decoder-only weights, followed by further pre-training with UL2 or PrefixLM methods, allowing flexible configurations like pairing a large encoder with a smaller decoder for optimized quality-efficiency trade-offs. T5Gemma models demonstrate superior performance, nearly dominating the quality-inference efficiency frontier in benchmarks like SuperGLUE, with notable gains in reasoning-intensive tasks. For instance, T5Gemma 9B-9B outperforms Gemma 2 9B by over 9 points on GSM8K (math reasoning) and 4 points on DROP (reading comprehension). Instruction-tuned versions further amplify these gains, with T5Gemma 2B-2B improving MMLU scores by nearly 12 points. The models’ flexibility and efficiency make them ideal for applications requiring robust input comprehension, and Google has released pretrained and instruction-tuned checkpoints to foster community-driven research and development. This revival of the encoder-decoder architecture highlights its potential to create more capable foundational models, offering developers a powerful toolset for innovative AI solutions. <br> <br>

29. ***Johns Hopkins Uni's DOTResize for LLM compression:  <br>This study introduces DOTResize, a novel Transformer compression method that reduces model width by framing neuron merging as a Discrete Optimal Transport problem. Unlike pruning, DOTResize re-projects the entire neuron width to retain and redistribute signals, outperforming other neuron width-pruning techniques across multiple LLM families while achieving measurable reductions in computational cost.*** <br> <br>
    Jul 6, Johns Hopkins Uni published a [paper](https://arxiv.org/pdf/2507.04517) “DOTResize: Reducing LLM Width via Discrete Optimal Transport-based Neuron Merging”. Model compression offers a promising path to reducing the cost and inaccessibility of large pre-trained models, without significantly compromising their impressive performance. Large Transformer models, including large language models (LLMs), often contain computational redundancy, which can serve as a target for new model compression methods. The study specifically targets neuron-level redundancies in model layers by combining groups of similar neurons into fewer neurons. The study frames this width reduction as a Discrete Optimal Transport problem, and proposes DOTResize, a novel Transformer compression method that uses optimal transport theory to transform and compress model weights. To ensure applicability within the Transformer architecture, the study motivates and incorporates entropic regularization and matrix factorization into the transportation maps produced by the method. Unlike pruning-based approaches which discard neurons based on importance measures, DOTResize re-projects the entire neuron width, allowing the retention and redistribution of useful signal across the reduced layer. Empirical results show that compared to simple or state-of-the-art neuron width-pruning techniques, DOTResize can outperform these methods across multiple LLM families and sizes, while achieving measurable reductions in real-world computational cost. <br> <br>

31. ***Stanford Uni et al.'s CollabLLM for active collaboration:  <br>This research introduces CollabLLM, a training framework to enhance human-LLM collaboration by moving models from passive responders to active collaborators. Using a collaborative simulation with Multiturn-aware Rewards for reinforcement fine-tuning, CollabLLM learns to actively uncover user intent and offer insightful suggestions, significantly outperforming baselines in task performance, interactivity, user satisfaction, and time efficiency.*** <br> <br>
    Jun 12, Stanford Uni, Microsoft and Georgia Tech published a [paper](https://arxiv.org/pdf/2502.00640) “CollabLLM: From Passive Responders to Active Collaborators”. Large Language Models are typically trained with next-turn rewards, limiting their ability to optimize for long-term interaction. As a result, they often respond passively to ambiguous or open-ended user requests, failing to help users reach their ultimate intents and leading to inefficient conversations. To address these limitations, the study introduces CollabLLM, a novel and general training framework that enhances multiturn human-LLM collaboration. Its key innovation is a collaborative simulation that estimates the long-term contribution of responses using Multiturn-aware Rewards. By reinforcement fine-tuning these rewards, CollabLLM goes beyond responding to user requests, and actively uncovers user intent and offers insightful suggestions-a key step towards more human-centered AI. The study also devises a multiturn interaction benchmark with three challenging tasks such as document creation. CollabLLM significantly outperforms the baselines with averages of 18.5% higher task performance and 46.3% improved interactivity by LLM judges. Finally, the study conducts a large user study with 201 judges, where CollabLLM increases user satisfaction by 17.6% and reduces user spent time by 10.4%. https://github.com/Wuyxin/collabllm

 <br> <br> <br>

***Jul 13, 2025***

1. ***UIUC et al.'s PLAN-TUNING framework:  <br>This research introduces PLAN-TUNING, a unified post-training framework that distills synthetic planning trajectories from large LLMs and fine-tunes smaller models to mimic these step-by-step planning processes using supervised and reinforcement learning. Plan-tuned models significantly outperformed strong baselines on math benchmarks and showed improved out-of-domain generalization on challenging datasets like OlympiadBench and AIME 2024.*** <br> <br>
   Jul 10, Google and Arizona State Uni published a [paper](https://arxiv.org/pdf/2507.07495) “PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving”. Recently, decomposing complex problems into simple subtasks--a crucial part of human-like natural planning--to solve the given problem has significantly boosted the performance of large language models (LLMs). However, leveraging such planning structures during post-training to boost the performance of smaller open-source LLMs remains underexplored. Motivated by this, the study introduces PLAN-TUNING, a unified post-training framework that (i) distills synthetic task decompositions (termed "planning trajectories") from large-scale LLMs and (ii) fine-tunes smaller models via supervised and reinforcement-learning objectives designed to mimic these planning processes to improve complex reasoning. On GSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by an average . Furthermore, plan-tuned models show better generalization capabilities on out-of-domain datasets, with average  and  performance improvements on OlympiadBench and AIME 2024, respectively. A detailed analysis demonstrates how planning trajectories improves complex reasoning capabilities, showing that PLAN-TUNING is an effective strategy for improving task-specific performance of smaller LLMs. <br> <br>

3. ***UC Berkeley's Q-chunking for reinforcement learning:  <br>This study presents Q-chunking, a recipe for improving RL algorithms in offline-to-online settings for long-horizon, sparse-reward tasks. By applying action chunking (predicting sequences of future actions) to TD-based RL methods, Q-chunking enables more effective online exploration by leveraging temporally consistent offline behaviors and more stable TD learning via unbiased n-step backups, outperforming prior methods on manipulation tasks.*** <br> <br>
   Jul 10, UC Berkeley published a [paper](https://arxiv.org/pdf/2507.07969) “Reinforcement Learning with Action Chunking”. The study presents Q-chunking, a simple yet effective recipe for improving reinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks. The recipe is designed for the offline-to-online RL setting, where the goal is to leverage an offline prior dataset to maximize the sample-efficiency of online learning. Effective exploration and sample-efficient learning remain central challenges in this setting, as it is not obvious how the offline data should be utilized to acquire a good exploratory policy. The key insight is that action chunking, a technique popularized in imitation learning where sequences of future actions are predicted rather than a single action at each timestep, can be applied to temporal difference (TD)-based RL methods to mitigate the exploration challenge. Q-chunking adopts action chunking by directly running RL in a 'chunked' action space, enabling the agent to (1) leverage temporally consistent behaviors from offline data for more effective online exploration and (2) use unbiased n-step backups for more stable and efficient TD learning. Experimental results demonstrate that Q-chunking exhibits strong offline performance and online sample efficiency, outperforming prior best offline-to-online methods on a range of long-horizon, sparse-reward manipulation tasks. <br> <br>

5. ***CMU and Cartesia AI's Dynamic Chunking for hierarchical modeling:  <br>This paper introduces dynamic chunking techniques and a hierarchical network (H-Net) to create a true end-to-end foundation model that learns content- and context-dependent segmentation strategies from raw data, replacing the static tokenization pipeline. H-Nets operating at the byte level outperform BPE-tokenized Transformers, demonstrate significantly better scaling and character-level robustness, and show dramatic data efficiency gains on modalities with weak tokenization heuristics like DNA.*** <br> <br>
   Jul 10, CMU and Cartesia AI published a [paper](https://arxiv.org/pdf/2507.07955) “Dynamic Chunking for End-to-End Hierarchical Sequence Modeling”. Despite incredible progress in language models (LMs) in recent years, largely resulting from moving away from specialized models designed for specific tasks to general models based on powerful architectures (e.g. the Transformer) that learn everything from raw data, pre-processing steps such as tokenization remain a barrier to true end-to-end foundation models. The study introduces a collection of new techniques that enable a dynamic chunking mechanism which automatically learns content -- and context -- dependent segmentation strategies learned jointly with the rest of the model. Incorporating this into an explicit hierarchical network (H-Net) allows replacing the (implicitly hierarchical) tokenization-LM-detokenization pipeline with a single model learned fully end-to-end. When compute- and data- matched, an H-Net with one stage of hierarchy operating at the byte level outperforms a strong Transformer language model operating over BPE tokens. Iterating the hierarchy to multiple stages further increases its performance by modeling multiple levels of abstraction, demonstrating significantly better scaling with data and matching a token-based Transformer of twice its size. H-Nets pretrained on English show significantly increased character-level robustness, and qualitatively learn meaningful data-dependent chunking strategies without any heuristics or explicit supervision. Finally, the H-Net's improvement over tokenized pipelines is further increased in languages and modalities with weaker tokenization heuristics, such as Chinese and code, or DNA sequences (nearly 4x improvement in data efficiency over baselines), showing the potential of true end-to-end models that learn and scale better from unprocessed data. https://github.com/goombalab/hnet <br> <br>

7. ***Princeton Uni on Machine Bullshit in LLMs:  <br>This research proposes "machine bullshit" – statements made without regard to truth – as a framework to understand emergent untruthfulness in LLMs, introducing the Bullshit Index metric and a taxonomy of bullshit forms. Empirical evaluations on a new BullshitEval benchmark show that RLHF fine-tuning significantly exacerbates bullshit and CoT prompting amplifies specific forms, highlighting systematic AI alignment challenges.*** <br> <br>
   Jul 10, Princeton Uni published a [paper](https://arxiv.org/pdf/2507.07484) “Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models”. Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to statements made without regard to their truth value. While previous work has explored large language model (LLM) hallucination and sycophancy, the study proposes machine bullshit as an overarching conceptual framework that can allow researchers to characterize the broader phenomenon of emergent loss of truthfulness in LLMs and shed light on its underlying mechanisms. The study introduces the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and propose a complementary taxonomy analyzing four qualitative forms of bullshit: empty rhetoric, paltering, weasel words, and unverified claims. The study conducts empirical evaluations on the Marketplace dataset, the Political Neutrality dataset, and a new BullshitEval benchmark (2,400 scenarios spanning 100 AI assistants) explicitly designed to evaluate machine bullshit. The results demonstrate that model fine-tuning with reinforcement learning from human feedback (RLHF) significantly exacerbates bullshit and inference-time chain-of-thought (CoT) prompting notably amplify specific bullshit forms, particularly empty rhetoric and paltering. The study also observes prevalent machine bullshit in political contexts, with weasel words as the dominant strategy. The findings highlight systematic challenges in AI alignment and provide new insights toward more truthful LLM behavior. <br> <br>

9. ***TheConversation on AI's impact on universities:  <br>This article argues that generative AI is devaluing traditional, codifiable knowledge by making it abundant and nearly free, thus challenging the core value proposition of universities. As employers reduce degree requirements, universities must pivot from content delivery to cultivating scarce, AI-complementary tacit skills like critical thinking, emotional intelligence, and creativity (summarized in the C.R.E.A.T.E.R. framework) to remain relevant.*** <br> <br>
    Jul 9, TheConversation published an [article](https://theconversation.com/ai-is-driving-down-the-price-of-knowledge-universities-have-to-rethink-what-they-offer-260493) “AI is driving down the price of knowledge – universities have to rethink what they offer”. The rise of AI, particularly generative models like ChatGPT, is dramatically reducing the cost of accessing and organizing knowledge, challenging the traditional value proposition of universities. Historically, universities thrived on the scarcity of information, offering credentials that signaled mastery and justified high tuition and wage premiums. However, as AI makes codifiable knowledge abundant and nearly free, the economic value of such knowledge is declining. Employers are responding swiftly, reducing degree requirements and entry-level job postings, as AI substitutes many routine tasks once performed by graduates. Yet, not all knowledge is equally affected. Tacit skills—like leadership, ethical judgment, and emotional intelligence—remain valuable because they complement AI rather than compete with it. These human-centric capabilities, encapsulated in the C.R.E.A.T.E.R. framework (critical thinking, resilience, emotional intelligence, accountability, teamwork, entrepreneurial creativity, and reflection), are now the true scarce resources. Universities must adapt by auditing courses to focus on judgment over rote learning, investing in experiential learning environments, credentialing soft skills, and collaborating with industry to design relevant assessments. The shift from content delivery to cultivating human judgment and creativity is essential. If universities fail to evolve, they risk becoming obsolete in a market that increasingly values AI-complementary skills over traditional academic credentials. <br> <br>

11. ***MCML et al. on the impossibility of separating AI intelligence from judgment:  <br>This study investigates the challenge of filtering harmful content from LLMs, demonstrating under cryptographic hardness assumptions that there are no efficient prompt filters for certain adversarial prompts and that output filtering can be computationally intractable in natural settings. The authors conclude that safety cannot be achieved through external, black-box filters, arguing that an aligned AI's intelligence and judgment are computationally inseparable*** <br> <br>
    Jul 9, MCML, UC Berkeley, Standford Uni and Apple published a [paper](https://arxiv.org/pdf/2507.07341) “On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment”. With the increased deployment of large language models (LLMs), one concern is their potential misuse for generating harmful content. The work studies the alignment challenge, with a focus on filters to prevent the generation of unsafe information. Two natural points of intervention are the filtering of the input prompt before it reaches the model, and filtering the output after generation. The main results demonstrate computational challenges in filtering both prompts and outputs. First, the study shows that there exist LLMs for which there are no efficient prompt filters: adversarial prompts that elicit harmful behavior can be easily constructed, which are computationally indistinguishable from benign prompts for any efficient filter. The second main result identifies a natural setting in which output filtering is computationally intractable. All of the separation results are under cryptographic hardness assumptions. In addition to these core findings, the study also formalizes and studies relaxed mitigation approaches, demonstrating further computational barriers. The study concludes that safety cannot be achieved by designing filters external to the LLM internals (architecture and weights); in particular, black-box access to the LLM will not suffice. Based on the technical results, the authors argue that an aligned AI system's intelligence cannot be separated from its judgment. <br> <br>

13. ***xAI's Grok 4 release:  <br>xAI has launched Grok 4, a major AI advancement featuring state-of-the-art reasoning, real-time search, and native tool use, scaled using a 200,000 GPU cluster called Colossus. Grok 4 variants achieve record-breaking scores on academic benchmarks, outperform humans in simulated economic environments, and offer developers a multimodal API with a 256k context window, a new Voice Mode, and a commitment to scaling RL for real-world problem-solving.*** <br> <br>
    Jul 9, xAI [released Grok 4](https://x.ai/news/grok-4). Grok 4, developed by xAI, represents a major leap in artificial intelligence, combining advanced reasoning, real-time search, and native tool use to deliver state-of-the-art performance. Building on the success of Grok 3, which introduced large-scale next-token prediction and reinforcement learning for improved problem-solving, Grok 4 scales these capabilities using Colossus—a 200,000 GPU cluster. This infrastructure enabled a 6x increase in compute efficiency and expanded training data across diverse domains, enhancing Grok’s reasoning and tool-use abilities. Grok 4 can autonomously use tools like code interpreters and web browsers to search and synthesize information, even diving deep into X (formerly Twitter) using semantic and media search. The Grok 4 Heavy variant pushes boundaries further with parallel hypothesis testing, achieving record-breaking scores on academic benchmarks like ARC-AGI V2 and Humanity’s Last Exam. Its agentic capabilities also outperform top models and humans in simulated economic environments. The Grok 4 API offers developers multimodal understanding, a 256k context window, and enterprise-grade security, while the new Voice Mode enables natural, real-time interaction with visual input analysis. Looking ahead, xAI plans to scale reinforcement learning to tackle real-world problems and enhance multimodal capabilities, aiming to create AI systems that deeply understand and assist humanity. Grok 4 is available to SuperGrok and Premium+ subscribers, with broader deployment planned through hyperscaler partners. <br> <br>

15. ***Allen Inst for AI et al.'s FlexOlmo models:  <br>This paper introduces FlexOlmo, a new class of MoE language models supporting distributed training without data sharing and data-flexible inference, where independently trained experts on closed datasets can be combined via a new domain-informed routing mechanism. FlexOlmo models showed significant relative improvements (average 41%) while allowing users to opt out of certain data, outperforming prior model merging methods and standard MoE trained without data restrictions.*** <br> <br>
    Jul 9, Allen Inst for AI, Uni of Washington, UC Berkeley, Stanford Uni and MIT published a [paper](https://arxiv.org/pdf/2507.07024) “FlexOlmo: Open Language Models for Flexible Data Use”. The study introduces FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus curated comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. The study evaluates models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks, shows that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. The approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners' preferences by keeping their data local and supporting fine-grained control of data access during inference. https://github.com/allenai/FlexOlmo <br> <br>

17. ***Microsoft and Stanford Uni's Decoder-Hybrid-Decoder architecture:  <br>This research introduces the Gated Memory Unit (GMU) for efficient memory sharing across layers and applies it to create SambaY, a decoder-hybrid-decoder architecture that shares memory from a Samba-based self-decoder to a cross-decoder. This design significantly improves decoding efficiency and long-context performance, with their largest model, Phi4-mini-Flash-Reasoning, outperforming its baseline on reasoning tasks and delivering up to 10x higher decoding throughput.*** <br> <br>
    Jul 9, Microsoft and Stanford Uni published a [paper](https://arxiv.org/pdf/2507.06607) “Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation”. Recent advances in language modeling have demonstrated the effectiveness of State Space Models (SSMs) for efficient sequence modeling. While hybrid architectures such as Samba and the decoder-decoder architecture, YOCO, have shown promising performance gains over Transformers, prior works have not investigated the efficiency potential of representation sharing between SSM layers. This study introduces the Gated Memory Unit (GMU), a simple yet effective mechanism for efficient memory sharing across layers. The study applies it to create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in the cross-decoder to share memory readout states from a Samba-based self-decoder. SambaY significantly enhances decoding efficiency, preserves linear pre-filling time complexity, and boosts long-context performance, all while eliminating the need for explicit positional encoding. Through extensive scaling experiments, the study demonstrates that the model exhibits a significantly lower irreducible loss compared to a strong YOCO baseline, indicating superior performance scalability under large-scale compute regimes. The largest model enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves significantly better performance than Phi4-mini-Reasoning on reasoning tasks such as Math500, AIME24/25, and GPQA Diamond without any reinforcement learning, while delivering up to 10x higher decoding throughput on 2K-length prompts with 32K generation length under the vLLM inference framework. https://github.com/microsoft/ArchScale <br> <br>

19. ***NYU and Columbia Uni on small batch size training:  <br>This study revisits small batch size training for LMs, finding that with a proposed rule for scaling Adam hyperparameters, small batches (down to size one) train stably, are more robust to hyperparameter choices, achieve equal or better per-FLOP performance, and enable stable training with vanilla SGD. They conclude by recommending against gradient accumulation unless bottlenecked by inter-device bandwidth.*** <br> <br>
    Jul 9, NYU and Columbia Uni published a [paper](https://arxiv.org/pdf/2507.07101) “Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful”. Conventional wisdom dictates that small batch sizes make language model pretraining and fine-tuning unstable, motivating gradient accumulation, which trades off the number of optimizer steps for a proportional increase in batch size. While it is common to decrease the learning rate for smaller batch sizes, other hyperparameters are often held fixed. This study revisits small batch sizes all the way down to batch size one, and proposes a rule for scaling Adam hyperparameters to small batch sizes. The study finds that small batch sizes (1) train stably, (2) are consistently more robust to hyperparameter choices, (3) achieve equal or better per-FLOP performance than larger batch sizes, and (4) notably enable stable language model training with vanilla SGD, even without momentum, despite storing no optimizer state. Building on these results, the work provides practical recommendations for selecting a batch size and setting optimizer hyperparameters. The study further recommends against gradient accumulation unless training on multiple devices with multiple model replicas, bottlenecked by inter-device bandwidth. https://github.com/martin-marek/batch-size <br> <br>

21. ***MSN on Isomorphic Labs' human trials:  <br>Alphabet's Isomorphic Labs, a DeepMind spin-off, is preparing for its first human trials of AI-designed drugs, primarily for cancer, marking a key step in its mission to revolutionize drug discovery. Leveraging the AlphaFold breakthrough, and with major pharma partnerships and $600 million in 2025 funding, the company aims to accelerate drug development and dramatically improve clinical trial success rates with its AI-driven drug design engine.*** <br> <br>
    Jul 6, MSN published an [article](https://www.msn.com/en-us/health/other/alphabet-s-isomorphic-labs-has-grand-ambitions-to-solve-all-diseases-with-ai-now-it-s-gearing-up-for-its-first-human-trials/ar-AA1I44pq) “Alphabet’s Isomorphic Labs has grand ambitions to ‘solve all diseases’ with AI. Now, it’s gearing up for its first human trials”. Alphabet’s Isomorphic Labs, a spin-off from DeepMind, is on the brink of launching its first human trials for AI-designed drugs, marking a major milestone in its mission to revolutionize drug discovery. The company emerged from DeepMind’s AlphaFold breakthrough, which accurately predicts protein structures and interactions, making it a powerful tool for designing targeted medicines. Isomorphic Labs combines advanced AI with pharmaceutical expertise to accelerate drug development, reduce costs, and improve success rates. President Colin Murdoch revealed that the company is actively designing cancer drugs using AI and is close to initiating clinical trials. Since its founding in 2021, Isomorphic has secured major partnerships with pharmaceutical giants like Novartis and Eli Lilly and raised $600 million in funding in 2025 to build a world-class drug design engine. This engine integrates machine learning researchers with seasoned pharma professionals to create both collaborative and proprietary drug candidates, particularly in oncology and immunology. Murdoch envisions a future where AI can instantly generate effective drug designs for any disease, dramatically improving the odds of success in clinical trials. With traditional drug development often costing millions and yielding only a 10% success rate, Isomorphic aims to transform the process by making it faster, cheaper, and more reliable. The company’s bold ambition is to harness AI to “solve all diseases,” and its upcoming human trials represent a significant step toward realizing that vision. <br> <br>

23. ***Oxford, Mila et al.'s critique of CoT as explainability:  <br>This paper argues that while chains-of-thought (CoT) can boost language model performance, they should not be treated as a sufficient method for trustworthy interpretability. Synthesizing evidence, the authors show CoTs are often unfaithful to a model's underlying computations and propose that researchers should avoid such claims without verification, adopt rigorous faithfulness assessments, and develop causal validation methods to ground explanations in model internals.*** <br> <br>
    Jul 5, Oxford, Mila et al published a [paper](https://www.alphaxiv.org/abs/2025.02) “Chain-of-Thought Is Not Explainability”. Chains- of-thought (CoT) allow language models to verbalise multi-step rationales before producing their final answer. While this technique often boosts task performance and offers an impression of transparency into the model’s reasoning, the paper argues that rationales generated by current CoT techniques can be misleading and are neither necessary nor sufficient for trustworthy interpretability. By analysing faithfulness in terms of whether CoTs are not only human-interpretable, but also reflect underlying model reasoning in a way that supports responsible use, the study synthesises evidence from previous studies. The work shows that verbalised chains are frequently unfaithful, diverging from the true hidden computations that drive a model’s predictions, and giving an incorrect picture of how models arrive at conclusions. Despite this, CoT is increasingly relied upon in high-stakes domains such as medicine, law, and autonomous systems—the analysis of 1,000 recent CoT-centric papers finds that ~ 25% explicitly treat CoT as an interpretability technique—and among them, papers in high-stakes domains specifically hinge on such interpretability claim heavily. Building on prior work in interpretability, the study makes three proposals: (i) avoid treating CoT as being sufficient for interpretability without additional verification, while continuing to use CoT for its communicative benefits, (ii) adopt rigorous methods that assess faithfulness for downstream decision-making, and (iii) develop causal validation methods (e.g., activation patching, counterfactual interventions, verifier models) to ground explanations in model internals. <br> <br>

25. ***MemTensor et al.'s MemOS for AI systems:  <br>This paper proposes MemOS, a memory operating system for AI that addresses the lack of well-defined memory management in LLMs by treating memory as a manageable system resource. MemOS unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories through "MemCubes," enabling flexible memory transitions and laying a foundation for continual learning and personalized modeling.*** <br> <br>
    Jul 4, MemTensor (Shanghai) et al published a [paper](https://statics.memtensor.com.cn/files/MemOS_0707.pdf) “MemOS: A Memory OS for AI System”. Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency. Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods. While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations. Recent work has modelled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, the study proposes MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling. https://github.com/MemTensor/MemOS <br> <br>

27. ***ScienceAdviser on LLM-assisted writing in biomedical publications:  <br>This study analyzed vocabulary changes in over 15 million biomedical abstracts, finding an abrupt increase in the frequency of certain style words since the advent of LLMs. This excess word analysis suggests that at least 13.5% of 2024 abstracts were processed with LLMs, with some subcorpora reaching 40%, indicating an unprecedented impact on scientific writing in this field.*** <br> <br>
    Jul 2, ScienceAdviser published a [paper](https://www.science.org/doi/10.1126/sciadv.adt3813) “Delving into LLM-assisted writing in biomedical publications through excess vocabulary”. Large language models (LLMs) like ChatGPT can generate and revise text with human-level performance. These models come with clear limitations, can produce inaccurate information, and reinforce existing biases. Yet, many scientists use them for their scholarly writing. But how widespread is such LLM usage in the academic literature? To answer this question for the field of biomedical research, the study presents an unbiased, large-scale approach: the authors study vocabulary changes in more than 15 million biomedical abstracts from 2010 to 2024 indexed by PubMed and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. This excess word analysis suggests that at least 13.5% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, reaching 40% for some subcorpora. The study shows that LLMs have had an unprecedented impact on scientific writing in biomedical research, surpassing the effect of major world events such as the COVID pandemic. <br> <br>

29. ***Artefact Research Center et al.'s comparison of MLM vs. CLM pretraining:  <br>This large-scale study on encoder pretraining found that while Masked Language Modeling (MLM) generally yields better performance on text representation tasks, Causal Language Modeling (CLM) is more data-efficient and has better fine-tuning stability. They conclude that a biphasic strategy—sequentially applying CLM then MLM—achieves optimal performance under a fixed compute budget, especially when starting from readily available pretrained CLM models.*** <br> <br>
    Jul 1, Artefact Research Center et al published a [paper](Should We Still Pretrain Encoders with Masked Language Modeling?) “Should We Still Pretrain Encoders with Masked Language Modeling?”. Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as encoders, often surpassing traditional encoders on text representation benchmarks. However, it remains unclear whether these gains reflect an inherent advantage of the CLM objective or arise from confounding factors such as model and data scale. This study addresses this question through a series of large-scale, carefully controlled pretraining ablations, training a total of 30 models ranging from 210 million to 1 billion parameters, and conducting over 15,000 fine-tuning and evaluation runs. The study finds that while training with MLM generally yields better performance across text representation tasks, CLM-trained models are more data-efficient and demonstrates improved fine-tuning stability. Building on these findings, the work experimentally shows that a biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under a fixed computational training budget. Moreover, the study demonstrates that this strategy becomes more appealing when initializing from readily available pretrained CLM models (from the existing LLM ecosystem), reducing the computational burden needed to train best-in-class encoder models. https://huggingface.co/MLMvsCLM <br> <br>

31. ***Johns Hopkins Uni and UC Berkeley on the benefits of data uniformity:  <br>This research demonstrates that selecting more uniformly distributed data can improve the training efficiency and performance of LLMs and other neural networks. Theoretically, they show that more uniform data leads to a larger minimum pairwise distance between points, which in turn accelerates gradient descent training and decreases approximation error, a finding supported by extensive experiments in supervised fine-tuning.*** <br> <br>
    Jun 30, Johns Hopkins Uni and UC Berkeley published a [paper](https://arxiv.org/pdf/2506.24120) “Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime”. Data selection plays a crucial role in data-driven decision-making, including in large language models (LLMs), and is typically task-dependent. Properties such as data quality and diversity have been extensively studied and are known to enhance model performance. However, it remains unclear whether there exist other quantitative and general principles of data selection that can consistently improve performance, especially for complex tasks with limited prior knowledge. This study demonstrates that selecting more uniformly distributed data can improve training efficiency while enhancing performance. Specifically, the study establishes that more uniform (less biased) distribution leads to a larger minimum pairwise distance between data points, denoted by hmin, and prove that a smaller hmin can slow down the training dynamics of gradient descent (GD). Moreover, the study theoretically shows that the approximation error of neural networks decreases as hmin increases. The analysis introduces a convergence framework for GD beyond the Neural Tangent Kernel (NTK) regime, applicable to a broad class of architectures, including transformers, without requiring Lipschitz smoothness. This framework further provides theoretical justification for the use of residual connections and function compositions in deep neural architectures. In the end, the work conducts comprehensive experiments for supervised fine-tuning across various settings, including different optimization strategies, model sizes, and training datasets. The results consistently demonstrate that selecting data by maximizing pairwise distance significantly accelerates training and achieves comparable or better performance in LLMs across diverse datasets. https://github.com/SafeRL-Lab/data-uniformity <br> <br>

33. ***TheRegister.com on AI's impact on web search referrals:  <br>The article reports that the rise of AI-generated search summaries, like Google’s AI Overviews, has severely disrupted the web ecosystem, leading to a 30% drop in click-through rates despite a 49% increase in search impressions. This trend negatively impacts websites reliant on referral traffic, as AI companies crawl vast amounts of content while providing little traffic in return, straining the open web's economic model.*** <br> <br>
    Jun 22, TheRegister.com published an [article](https://www.theregister.com/2025/06/22/ai_search_starves_publishers/) “The AIpocalypse is here for websites as search referrals plunge”. The rise of AI-generated search summaries, particularly Google’s AI Overviews launched in May 2024, has significantly disrupted the traditional web ecosystem. These summaries appear at the top of search results, offering users direct answers without requiring them to click through to source websites. While this has increased search impressions by 49%, it has led to a 30% drop in click-through rates, severely impacting websites that rely on search referrals for traffic and revenue. SEO experts and analytics firms like BrightEdge, Ahrefs, and SimilarWeb report consistent declines in referral traffic across various sectors, including travel, news, e-commerce, and finance. AI search engines have only replaced about 10% of traditional referral traffic, leaving a substantial gap. Cloudflare CEO Matthew Prince highlighted the growing imbalance, noting that AI companies like Google, OpenAI, and Anthropic are crawling vastly more pages than they refer visitors to—ratios as high as 60,000:1 in Anthropic’s case. This trend suggests that AI firms are extracting content for training and services while offering little in return, prompting lawsuits from web publishers. Despite speculation about AI disrupting Google’s dominance, the company still commands 90% of the search market. However, its practices, along with those of other AI firms, are straining the very content ecosystem that enabled their growth. As AI crawlers increasingly burden websites without compensating them, the sustainability of the open web and its economic model is being called into question. <br> <br>

35. ***Meta's ConfQA for reducing LLM hallucination:  <br>This paper presents ConfQA, a fine-tuning strategy that teaches LLMs to admit "I am unsure" when they would otherwise provide an incorrect answer, reducing hallucination rates from 20-40% to under 5% on factuality benchmarks. Key to its effectiveness are a "dampening prompt" and the use of simple factual statements from knowledge graphs to help LLMs calibrate their confidence.*** <br> <br>
    Jun 8, Meta published a [paper](https://www.arxiv.org/pdf/2506.07309) “ConfQA: Answer Only If You Are Confident”. Can we teach Large Language Models (LLMs) to refrain from hallucinating factual statements? The study presents a fine-tuning strategy that it is called ConfQA, which can reduce hallucination rate from 20-40% to under 5% across multiple factuality benchmarks. The core idea is simple: when the LLM answers a question correctly, it is trained to continue with the answer; otherwise, it is trained to admit "I am unsure". But there are two key factors that make the training highly effective. First, the study introduces a dampening prompt "answer only if you are confident" to explicitly guide the behavior, without which hallucination remains high as 15%-25%. Second, the study leverages simple factual statements, specifically attribute values from knowledge graphs, to help LLMs calibrate the confidence, resulting in robust generalization across domains and question types. Building on this insight, the study proposes the Dual Neural Knowledge framework, which seamlessly select between internally parameterized neural knowledge and externally recorded symbolic knowledge based on ConfQA's confidence. The framework enables potential accuracy gains to beyond 95%, while reducing unnecessary external retrievals by over 30%.

 <br> <br> <br>

***Jul 6, 2025***

1. ***UIUC et al.'s position on Agentic Deep Research:  <br>This paper argues that LLMs with reasoning and agentic capabilities are creating a new paradigm called Agentic Deep Research, which surpasses traditional web search by integrating autonomous reasoning, iterative retrieval, and information synthesis. Tracing the evolution from static search to interactive, agent-based systems, and introducing a test-time scaling law, the authors demonstrate that this approach significantly outperforms existing methods and is poised to become the dominant paradigm for information seeking.*** <br> <br>
   Jul 3, UIUC et al. published a [paper](https://www.arxiv.org/pdf/2506.18959) “From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents”. Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. The autors’ position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. The study traces the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. The study also introduces a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, the work demonstrates that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. https://github.com/DavidZWZ/Awesome-Deep-Research <br> <br>

3. ***Yale Uni and TSC Research on LLM identification of scientific limitations:  <br>This study addresses the understudied potential of LLMs in assisting with peer review, specifically in identifying research limitations. The authors introduce a comprehensive taxonomy of limitation types, present LimitGen (the first benchmark for this task with synthetic and human-written subsets), and show that augmenting LLMs with literature retrieval enhances their ability to generate concrete and constructive feedback on research papers.*** <br> <br>
   Jul 3, Yale Uni and TSC Research published a [paper](https://arxiv.org/pdf/2507.02694) “Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers”. Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. The study first presents a comprehensive taxonomy of limitation types in scientific research, with a focus on AI. Guided by this taxonomy, for studying limitations, the study presents LimitGen, the first comprehensive benchmark for evaluating LLMs' capability to support early-stage feedback and complement human peer review. The benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of high-quality papers, and LimitGen-Human, a collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, the study augments them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. The approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback. <br> <br>

5. ***Renmin Uni and BAAI's HiRA framework for deep search:  <br>This research introduces HiRA, a hierarchical reasoning framework that addresses the limitations of current reasoning-based search approaches by separating high-level strategic planning from specialized execution. By decomposing complex search tasks and assigning subtasks to domain-specific agents with external tools, HiRA significantly outperforms state-of-the-art RAG and agent-based systems on complex benchmarks, improving both answer quality and efficiency.*** <br> <br>
   Jul 3, Renmin Uni and BAAI published a [paper](https://arxiv.org/pdf/2507.02652) “Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search”. Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. This study introduces HiRA, a hierarchical framework that separates strategic planning from specialized execution. The approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. The results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. https://github.com/ignorejjj/HiRA. <br> <br>

7. ***Nature's paper on the Centaur foundation model:  <br>This study introduces Centaur, a computational model fine-tuned on the large-scale Psych-101 dataset, designed to predict and simulate human behavior in any experiment expressible in natural language. Centaur not only captures participant behavior better than existing cognitive models and generalizes to unseen tasks, but its internal representations also become more aligned with human neural activity, demonstrating potential for guiding cognitive theory development.*** <br> <br>
   Jul 2, Nature published a [paper](https://www.nature.com/articles/s41586-025-09215-4) “A foundation model to predict and capture human cognition”. Establishing a unified theory of cognition has been an important goal in psychology. A first step towards such a theory is to create a computational model that can predict human behaviour in a wide range of settings. Here the study introduces Centaur, a computational model that can predict and simulate human behaviour in any experiment expressible in natural language. The study derived Centaur by fine-tuning a state-of-the-art language model on a large-scale dataset called Psych-101. Psych-101 has an unprecedented scale, covering trial-by-trial data from more than 60,000 participants performing in excess of 10,000,000 choices in 160 experiments. Centaur not only captures the behaviour of held-out participants better than existing cognitive models, but it also generalizes to previously unseen cover stories, structural task modifications and entirely new domains. Furthermore, the model’s internal representations become more aligned with human neural activity after fine-tuning. Taken together, the results demonstrate that it is possible to discover computational models that capture human behaviour across a wide range of domains. The authors believe that such models provide tremendous potential for guiding the development of cognitive theories, and present a case study to demonstrate this. <br> <br>

9. ***Microsoft's research on sequential diagnosis with language models:  <br>To better emulate real-world clinical practice, this study introduces the Sequential Diagnosis Benchmark, which transforms diagnostically challenging NEJM-CPC cases into iterative diagnostic encounters. They also present the MAI Diagnostic Orchestrator (MAI-DxO), a model-agnostic orchestrator that, when paired with OpenAI's o3, achieved 80% diagnostic accuracy (4x higher than generalist physicians) and reduced diagnostic costs, highlighting AI's potential for precision and cost-effectiveness.*** <br> <br>
    Jul 2, Microsoft published a [paper](https://arxiv.org/pdf/2506.22405) “Sequential Diagnosis with Language Models”. Artificial intelligence holds great promise for expanding access to expert medical knowledge and reasoning. However, most evaluations of language models rely on static vignettes and multiple-choice questions that fail to reflect the complexity and nuance of evidence-based medicine in real-world settings. In clinical practice, physicians iteratively formulate and revise diagnostic hypotheses, adapting each subsequent question and test to what they've just learned, and weigh the evolving evidence before committing to a final diagnosis. To emulate this iterative process, the study introduces the Sequential Diagnosis Benchmark, which transforms 304 diagnostically challenging New England Journal of Medicine clinicopathological conference (NEJM-CPC) cases into stepwise diagnostic encounters. A physician or AI begins with a short case abstract and must iteratively request additional details from a gatekeeper model that reveals findings only when explicitly queried. Performance is assessed not just by diagnostic accuracy but also by the cost of physician visits and tests performed. The study also presents the MAI Diagnostic Orchestrator (MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians, proposes likely differential diagnoses and strategically selects high-value, cost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80% diagnostic accuracy--four times higher than the 20% average of generalist physicians. MAI-DxO also reduces diagnostic costs by 20% compared to physicians, and 70% compared to off-the-shelf o3. When configured for maximum accuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO generalize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and Llama families. The work highlights how AI systems, when guided to think iteratively and act judiciously, can advance diagnostic precision and cost-effectiveness in clinical care. <br> <br>

11. ***Allen Inst for AI et al.'s work on simple retrieval for reasoning benchmarks:  <br>This study challenges the view that minimal RAG is ineffective for reasoning-intensive benchmarks by introducing CompactDS, a diverse, high-quality, web-scale datastore with high retrieval accuracy and low latency. Using CompactDS, a minimal RAG pipeline achieved consistent and significant accuracy improvements (10-33%) across MMLU, GPQA, and MATH, matching or outperforming web search engines and complex agent-based systems while maintaining simplicity.*** <br> <br>
    Jul 2, Allen Inst for AI, UIUC et al. published a [paper](https://arxiv.org/pdf/2507.01297) “Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks”. Retrieval-augmented Generation (RAG) has primarily been studied in limited settings, such as factoid question answering; more challenging, reasoning-intensive benchmarks have seen limited success from minimal RAG. This study challenges this prevailing view on established, reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. The work identifies a key missing component in prior work: a usable, web-scale datastore aligned with the breadth of pretraining data. To this end, the study introduces CompactDS: a diverse, high-quality, web-scale datastore that achieves high retrieval accuracy and subsecond latency on a single-node. The key insights are (1) most web content can be filtered out without sacrificing coverage, and a compact, high-quality subset is sufficient; and (2) combining in-memory approximate nearest neighbor (ANN) retrieval and on-disk exact search balances speed and recall. Using CompactDS, the study shows that a minimal RAG pipeline achieves consistent accuracy improvements across all benchmarks and model sizes (8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA, and 19% on MATH. No single data source suffices alone, highlighting the importance of diversity of sources (web crawls, curated math, academic papers, textbooks). Finally, the study shows that the carefully designed in-house datastore matches or outperforms web search engines such as Google Search, as well as recently proposed, complex agent-based RAG systems--all while maintaining simplicity, reproducibility, and self-containment.  <br> <br>

13. ***Uni of Oxford et al.'s GradMetaNet for learning on gradients:  <br>This paper presents GradMetaNet, a novel, principled architecture for learning on neural network gradients, guided by principles of equivariance to neuron permutation, processing sets of gradients to capture curvature, and efficient rank-1 decomposition. GradMetaNet, constructed from simple equivariant blocks, is proven to be a universal approximator for certain gradient-based functions and demonstrated effectiveness on diverse tasks like learned optimization and INR editing.*** <br> <br>
    Jul 2, Uni of Oxford et al. published a [paper](https://arxiv.org/pdf/2507.01649) “GradMetaNet: An Equivariant Architecture for Learning on Gradients”. Gradients of neural networks encode valuable information for optimization, editing, and analysis of models. Therefore, practitioners often treat gradients as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent works explore learning algorithms that operate directly on gradients but use architectures that are not specifically designed for gradient processing, limiting their applicability. This study presents a principled approach for designing architectures that process gradients. The approach is guided by three principles: (1) equivariant design that preserves neuron permutation symmetries, (2) processing sets of gradients across multiple data points to capture curvature information, and (3) efficient gradient representation through rank-1 decomposition. Based on these principles, the study introduces GradMetaNet, a novel architecture for learning on gradients, constructed from simple equivariant blocks. The study proves universality results for GradMetaNet, and show that previous approaches cannot approximate natural gradient-based functions that GradMetaNet can. The study then demonstrates GradMetaNet's effectiveness on a diverse set of gradient-based tasks on MLPs and transformers, such as learned optimization, INR editing, and estimating loss landscape curvature. <br> <br>

15. ***CMU et al.'s study on math reasoning transferability:  <br>This research investigates whether improved math reasoning in LLMs translates to broader problem-solving abilities, finding that most open-weight reasoning-tuned models fail to transfer gains to other domains. Controlled experiments on Qwen3-14B models showed that RL-tuned models generalize well, while SFT-tuned models often forget general capabilities due to significant representation and output drift, suggesting a need to rethink post-training recipes.*** <br> <br>
    Jul 1, CMU et al published a [paper](https://arxiv.org/pdf/2507.00432) “Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning”. Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, the study evaluates over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. The authors surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, the work conducts controlled experiments on Qwen3-14B models using math-only data but different tuning methods. The study finds that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. The results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models. https://github.com/ReasoningTransfer/Transferability-of-LLM-Reasoning <br> <br>

17. ***Bloomberg.com's report on Meta Superintelligence Labs (MSL):  <br>Meta CEO Mark Zuckerberg announced a major AI strategy overhaul with the creation of Meta Superintelligence Labs (MSL), a new division led by former Scale AI CEO Alexandr Wang and ex-GitHub CEO Nat Friedman. MSL will consolidate existing AI teams and launch a new lab to develop superintelligence, backed by massive investments and aggressive recruitment of top talent from rivals like OpenAI and Google.*** <br> <br>
    Jul 1, according to [Bloomberg.com](https://www.bloomberg.com/news/articles/2025-06-30/zuckerberg-announces-meta-superintelligence-effort-more-hires), “Zuckerberg Debuts Meta ‘Superintelligence’ Group, More Hires”. Meta CEO Mark Zuckerberg has announced a major overhaul of the company’s artificial intelligence strategy, unveiling a new division called Meta Superintelligence Labs (MSL). This group, led by Alexandr Wang, former CEO of Scale AI, aims to develop AI systems capable of performing tasks as well as or better than humans. Nat Friedman, ex-GitHub CEO, will co-lead the initiative, focusing on AI products and applied research. MSL will consolidate Meta’s existing teams working on large language models, AI products, and the Fundamental AI Research (FAIR) team, while also launching a new lab to advance next-generation models. Zuckerberg emphasized his belief that superintelligence marks the beginning of a transformative era for humanity and pledged to invest “hundreds of billions” in AI infrastructure, talent, and research. Meta has aggressively recruited top talent from leading AI firms like OpenAI, Anthropic, and Google, offering lucrative compensation packages. Recent hires include prominent researchers and engineers from DeepMind, OpenAI, and Anthropic. The company also invested $14.3 billion in Scale AI and is exploring acquisitions of startups like PlayAI. Zuckerberg has personally led recruitment efforts, hosting candidates at his homes and spearheading outreach. Despite concerns about industry-wide overinvestment, he maintains that staying ahead in AI is crucial for long-term technological leadership. Meta’s stock remained stable following the announcement, reflecting investor confidence in the company’s ambitious AI vision. This restructuring signals Meta’s intent to compete aggressively with rivals like OpenAI and Google in shaping the future of artificial intelligence. New team members: Trapit Bansal, Shuchao Bi, Huiwen Chang, Ji Lin, Joel Pobar, Jack Rae, Hongyu Ren, Johan Schalkwyk, Pei Sun, Jiahui Yu, Snengjia Zhao <br> <br>

19. ***Princeton Uni on uncertainty quantification in reasoning models:  <br>This study explores whether reasoning models know when they don't know, finding that SOTA reasoning models are typically overconfident (especially for incorrect responses), become more so with deeper reasoning, and can sometimes improve calibration through introspective reasoning, though not uniformly across all models. The paper highlights the need for better UQ benchmarks and methods to improve the calibration of reasoning models for safe deployment.*** <br> <br>
    Jul 1, Princeton Uni published a [paper](https://arxiv.org/pdf/2506.18183) “Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?” Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, enabled by multi-step reasoning induced using reinforcement learning. However, like previous language models, reasoning models are prone to generating confident, plausible responses that are incorrect (hallucinations). Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications. The study explores uncertainty quantification of reasoning, specifically, it asks three fundamental questions: First, are reasoning models well-calibrated? Second, does deeper reasoning improve model calibration? Finally, inspired by humans' innate ability to double-check their thought processes to verify the validity of their answers and their confidence, the study asks: can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces? The work introduces introspective uncertainty quantification (UQ) to explore this direction. In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, the study finds that reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than 85% particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, the study concludes with important research directions to design necessary UQ benchmarks and improve the calibration of reasoning models. <br> <br>

21. ***Uni of Central Florida et al.'s cross-disciplinary synthesis of AGI:  <br>This paper offers a cross-disciplinary analysis of AGI development, arguing that despite the capabilities of models like GPT-4.5, they are limited by token-level prediction and lack of grounded agency. The authors highlight the role of modular reasoning, persistent memory, and multi-agent coordination (especially Agentic RAG) as crucial for bridging the gap between statistical learning and goal-directed cognition on the path to AGI.*** <br> <br>
    Jul 1, Uni of Central Florida et al. published a [paper](https://arxiv.org/pdf/2507.00951) “Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact”. Can machines truly think, reason and act in domains like humans? This enduring question continues to shape the pursuit of Artificial General Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency. This paper offers a cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. The study analyzes the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, the work emphasizes the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior. The study discusses generalization strategies, including information compression, test-time adaptation, and training-free methods, as critical pathways toward flexible, domain-agnostic intelligence. Vision-Language Models (VLMs) are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. The study also argues that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior. Drawing on advances in neurosymbolic systems, reinforcement learning, and cognitive scaffolding, the research explores how recent architectures begin to bridge the gap between statistical learning and goal-directed cognition. Finally, the study identifies key scientific, technical, and ethical challenges on the path to AGI. <br> <br>

23. ***Apple's TarFlowLM for flexible language modeling:  <br>This work explores an alternative to discrete-token autoregressive models by proposing TarFlowLM, a framework that shifts language modeling to a continuous latent space using transformer-based autoregressive normalizing flows. This approach enables flexible modeling, including global bi-directional context, block-wise generation, and hierarchical multi-pass generation, demonstrating strong likelihood performance on benchmarks.*** <br> <br>
    Jul 1, Apple published a [paper](https://arxiv.org/pdf/2507.00425) “Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows”. Autoregressive models have driven remarkable progress in language modeling. Their foundational reliance on discrete tokens, unidirectional context, and single-pass decoding, while central to their success, also inspires the exploration of a design space that could offer new axes of modeling flexibility. This work explores an alternative paradigm, shifting language modeling from a discrete token space to a continuous latent space. The study proposes a novel framework TarFlowLM, that employs transformer-based autoregressive normalizing flows to model these continuous representations. This approach unlocks substantial flexibility, enabling the construction of models that can capture global bi-directional context through stacked, alternating-direction autoregressive transformations, support block-wise generation with flexible token patch sizes, and facilitate a hierarchical multi-pass generation process. The study further proposes new mixture-based coupling transformations designed to capture complex dependencies within the latent space shaped by discrete data, and demonstrate theoretical connections to conventional discrete autoregressive models. Extensive experiments on language modeling benchmarks demonstrate strong likelihood performance and highlight the flexible modeling capabilities inherent in our framework. <br> <br>

25. ***Meta and Uni of Washington's ASTRO framework:  <br>This paper introduces ASTRO (Autoregressive Search-Taught Reasoner), a framework to teach non-reasoner LLMs like Llama 3 to reason like search algorithms by fine-tuning them on a synthetic dataset derived from Monte Carlo Tree Search traces. This process, which converts search trajectories into natural language chain-of-thoughts capturing successes and failures, followed by RL, instilled robust reasoning and led to significant performance gains on math benchmarks.*** <br> <br>
    Jul 1, Meta and Uni of Washington published a [paper](https://arxiv.org/abs/2507.00417) “ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context”. The paper introduces ASTRO, the "Autoregressive Search-Taught Reasoner", a framework for training language models to reason like search algorithms, explicitly leveraging self-reflection, backtracking, and exploration in their outputs. Recently, training large language models (LLMs) via reinforcement learning (RL) has led to the advent of reasoning models with greatly enhanced reasoning capabilities. Open-source replications of reasoning models, while successful, build upon models that already exhibit strong reasoning capabilities along with search behavior observed even before RL. As a result, it is yet unclear how to boost the reasoning capabilities of other non-reasoner models including Llama 3. ASTRO teaches such models to internalize structured search behavior through a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over mathematical problem-solving trajectories. By converting search traces into natural language chain-of-thoughts that capture both successes and recoveries from failure, ASTRO bootstraps models with a rich prior for exploration during RL. The study finetunes models on these search-derived traces and further improve performance via RL with verifiable rewards. The study applies ASTRO to the Llama 3 family of models and achieve absolute performance gains of 16.0% on MATH-500, 26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon challenging problems that require iterative correction. The results demonstrate that search-inspired training offers a principled way to instill robust reasoning capabilities into open LLMs. <br> <br>

27. ***The New York's article on AI's impact on college writing:  <br>This article explores how AI tools like ChatGPT are challenging higher education by allowing students to bypass the core learning process in writing assignments, prompting a "code red" among educators. While some professors revert to traditional methods like in-class exams, others embrace AI as a tool, forcing a critical re-examination of what skills to cultivate and how to preserve original human thought in an increasingly automated world.*** <br> <br>
    Jun 30, The New York published an [article](https://www.newyorker.com/magazine/2025/07/07/the-end-of-the-english-paper) “What happens after A.I. destroys college writing?”. AI's Impact on College Writing and Higher Education - The proliferation of AI tools like ChatGPT is fundamentally altering college writing, posing a significant challenge to traditional pedagogy and prompting a re-evaluation of higher education's purpose. As evidenced by students like "Alex" who use AI for virtually all writing tasks, from summarizing complex texts to drafting essays that receive high grades, AI can bypass the core process of learning that once defined academic assignments. This has created a "code red" for educators grappling with academic dishonesty, though AI detection remains inconsistent and many students, seeing AI as just another productivity tool, don't view its use as "cheating." While some professors are reverting to traditional methods like in-class, handwritten blue-book exams to ensure authentic learning and combat AI use, others are embracing AI as a tool for collaborative learning and emphasizing the process of writing over the final product. The article highlights how AI can enhance learning, as seen with AI tutors and personalized practice questions, but also raises concerns about students' declining ability to engage with complex texts and their increasing desire for efficiency over deep engagement. Ultimately, AI's integration forces a critical re-examination of what skills higher education should cultivate when AI can perform many intellectual tasks, and how to preserve the value of original human thought and expression in an increasingly automated world. <br> <br>

29. ***Renmin Uni et al.'s MoCa for bidirectional multimodal embeddings:  <br>This study proposes MoCa, a two-stage framework to transform pre-trained causal VLMs into effective bidirectional multimodal embedding models, addressing limitations of causal attention and reliance on labeled data. By introducing modality-aware continual pre-training with a joint reconstruction objective and heterogeneous contrastive fine-tuning, MoCa achieves new SOTA results on MMEB and ViDoRe-v2 benchmarks.*** <br> <br>
    Jun 29, Renmin Uni, Stanford Uni and Microsoft published a [paper](https://arxiv.org/pdf/2506.23115) “MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings”. Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, the study proposes MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. The method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB. https://github.com/haon-chen/MoCa <br> <br>

31. ***Meta and Uni of Edinburgh's Automated LLM Speedrunning Benchmark:  <br>This research introduces a benchmark to evaluate AI agents' ability to reproduce scientific results, leveraging the NanoGPT speedrun competition where participants improve a GPT-2 training script. The study found that recent reasoning LLMs with SOTA scaffolds struggle to reimplement known innovations even with detailed hints, highlighting the benchmark's utility as a simple, non-saturated measure of an LLM's scientific reproduction skill.*** <br> <br>
    Jun 27, Meta and Uni of Edinburgh published a [paper](https://arxiv.org/pdf/2506.22419) “The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements”. Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce existing work. To evaluate the ability of AI agents to reproduce results in an active research area, the study introduces the Automated LLM Speedrunning Benchmark, leveraging the research community contributions on the NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time. Each of the 19 speedrun tasks provides the agent with the previous records training script, optionally paired with one of three hint formats, ranging from pseudocode to paper-like descriptions of the new records improvements. Records execute quickly by design and speedrun improvements encompass diverse code-level changes, ranging from high-level algorithmic advancements to hardware-aware optimizations. These features make the benchmark both accessible and realistic for the frontier problem of improving LLM training. The study finds that recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement already-known innovations in the benchmark, even when given detailed hints. The benchmark thus provides a simple, non-saturated measure of an LLMs ability to automate scientific reproduction, a necessary (but not sufficient) skill for an autonomous research agent. <br> <br>

33. ***Thomson Reuters' Future of Professionals Report 2025:  <br>This report argues that generative AI will transform professions like law, tax, and audit over the next three years, creating a competitive divide between organizations that adopt a formal AI strategy and those that do not. Firms with visible AI strategies are twice as likely to see revenue growth, while professionals who fail to develop AI proficiency risk falling behind in their careers.*** <br> <br>
    Jun 27, Thomson Reuters [published](https://www.thomsonreuters.com/content/dam/ewp-m/documents/thomsonreuters/en/pdf/reports/future-of-professionals-report-2025.pdf) “Future of Professionals Report 2025”. Generative AI will transform the legal, risk, compliance, tax, accounting, and audit professions, along with global trade over the next three years. Organizations must constantly consider how to maintain their competitive edge, but the efficiency gains offered by AI are particularly significant in today’s evolving business landscape. Now, as AI adoption reaches a pivotal stage, it’s clear that a widening competitive gap is emerging. Firms that reinvent and automate entire business processes using AI will prevail with superior customer experiences and lower costs compared to those organizations that move slowly. This year’s report highlights a new divide among organizations: Those that adopt an AI strategy and those that do not. The research shows that organizations with visible AI strategies are twice as likely to experience revenue growth as a direct or indirect result of AI adoption compared to those with more informal or ad-hoc adoption approaches. That puts those organizations that haven’t developed an AI strategy at risk of being left behind within a matter of years. The report highlights the significant variance in AI adoption, even within the same organization. Those professionals who fail to develop their individual AI proficiency risk falling behind in critical skills, creating a competitive gap that could limit their career growth. AI-enabled professionals will gain a competitive edge, boosting both their personal impact and their organization’s long-term value.  <br> <br>

35. ***HKUST et al.'s GPAS for accelerating LLM pretraining:  <br>To mitigate the issue of exponential activation variance growth in Pre-LayerNorm (Pre-LN) Transformers, which limits learning in deeper layers, this study proposes Gradient-Preserving Activation Scaling (GPAS). This simple technique scales down intermediate activations while keeping their gradients unchanged, achieving consistent performance gains across various model sizes and demonstrating versatility by improving alternative architectures as well.*** <br> <br>
    Jun 27, HKUST, IDEA, Nvidia, Unif of Oxford et al published a [paper](https://arxiv.org/pdf/2506.22049) “GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling”. Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series, predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While being stable during pretraining and scalable to large model sizes, Pre-LN suffers from an exponential growth in activation variance across layers, causing the residual path to dominate over sub-layer outputs and limiting the learning capacity of deeper layers. To mitigate this issue, the study proposes Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be used in combination with existing approaches. GPAS works by scaling down the intermediate activations while keeping their gradients unchanged. This leaves information in the activations intact, and avoids the gradient vanishing problem associated with gradient downscaling. Extensive experiments across various model sizes from 71M to 1B show that GPAS achieves consistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows promise in improving alternative architectures such as Sandwich-LN and DeepNorm, demonstrating its versatility and potential for improving training dynamics in a wide range of settings. <br> <br>

37. ***Uni of Cambridge on Transformers as Graph Neural Networks:  <br>This study establishes a connection between Transformers and Graph Neural Networks (GNNs), showing that Transformers can be viewed as message passing GNNs operating on fully connected graphs of tokens. While mathematically linked, Transformers are implemented with dense matrix operations that are more efficient on modern hardware than sparse message passing, leading to the perspective that Transformers are GNNs "winning the hardware lottery."*** <br> <br>
    Jun 27, Uni of Cambridge published a [paper](https://arxiv.org/pdf/2506.22084) “Transformers are Graph Neural Networks”. The study establishes connections between the Transformer architecture, originally introduced for natural language processing, and Graph Neural Networks (GNNs) for representation learning on graphs. The study shows how Transformers can be viewed as message passing GNNs operating on fully connected graphs of tokens, where the self-attention mechanism capture the relative importance of all tokens w.r.t. each-other, and positional encodings provide hints about sequential ordering or structure. Thus, Transformers are expressive set processing networks that learn relationships among input elements without being constrained by apriori graphs. Despite this mathematical connection to GNNs, Transformers are implemented via dense matrix operations that are significantly more efficient on modern hardware than sparse message passing. This leads to the perspective that Transformers are GNNs currently winning the hardware lottery. <br> <br>

39. ***Stanford Uni et al.'s rational analysis of ICL strategies:  <br>This research aims to unify findings on in-context learning (ICL) strategies by proposing a hierarchical Bayesian framework that explains a model's learned strategies as an optimal adaptation to data given computational constraints. The framework, which almost perfectly predicts Transformer next-token predictions without access to weights, models a trade-off between a strategy's loss and its complexity, explaining known ICL phenomena and offering novel predictions.*** <br> <br>
    Jun 26, Stanford Uni, Harvard Uni and Princeton Uni published a [paper](https://arxiv.org/pdf/2506.17859) “In-Context Learning Strategies Emerge Rationally”. Recent work analyzing in-context learning (ICL) has identified a broad set of strategies that describe model behavior in different experimental conditions. The study aims to unify these findings by asking why a model learns these disparate strategies in the first place. Specifically, the work starts with the observation that when trained to learn a mixture of tasks, as is popular in the literature, the strategies learned by a model for performing ICL can be captured by a family of Bayesian predictors: a memorizing predictor, which assumes a discrete prior on the set of seen tasks, and a generalizing predictor, where the prior matches the underlying task distribution. Adopting the normative lens of rational analysis, where a learner's behavior is explained as an optimal adaptation to data given computational constraints, the study develops a hierarchical Bayesian framework that almost perfectly predicts Transformer next-token predictions throughout training -- without assuming access to its weights. Under this framework, pretraining is viewed as a process of updating the posterior probability of different strategies, and inference-time behavior as a posterior-weighted average over these strategies' predictions. The framework draws on common assumptions about neural network learning dynamics, which make explicit a tradeoff between loss and complexity among candidate strategies: beyond how well it explains the data, a model's preference towards implementing a strategy is dictated by its complexity. This helps explain well-known ICL phenomena, while offering novel predictions: e.g., the study shows a superlinear trend in the timescale for transitioning from generalization to memorization as task diversity increases. Overall, the work advances an explanatory and predictive account of ICL grounded in tradeoffs between strategy loss and complexity. <br> <br>

41. ***Google's text-to-text regression for large system performance prediction:  <br>This paper proposes text-to-text regression as a general, scalable alternative to traditional tabular regression for predicting metric outcomes in complex large systems where feature engineering is infeasible. A 60M parameter encoder-decoder trained to predict resource efficiency on Google's Borg cluster achieved near-perfect rank correlation (0.99) and 100x lower MSE than tabular methods, demonstrating its effectiveness and adaptability.*** <br> <br>
    Jun 26, Google published a [paper](https://arxiv.org/pdf/2506.21718) “Performance Prediction for Large Systems via Text-to-Text Regression”. In many industries, predicting metric outcomes of large systems is a fundamental problem, driven largely by traditional tabular regression. However, such methods struggle on complex systems data in the wild such as configuration files or system logs, where feature engineering is often infeasible. The study proposes text-to-text regression as a general, scalable alternative. For predicting resource efficiency on Borg, Google's massive compute cluster scheduling system, a 60M parameter encoder-decoder, trained from random initialization, achieves up to a near perfect 0.99 (0.9 average) rank correlation across the entire fleet, and 100x lower MSE than tabular approaches. The model also easily adapts to new tasks in only 500 few-shot examples and captures the densities of complex outcome distributions. Ablation studies highlight the importance of using encoders, increasing sequence length, and the model's inherent uncertainty quantification. These findings pave the way for universal simulators of real-world outcomes. <br> <br>

43. ***Meta and NYU on Asymmetric REINFORCE for off-policy RL:  <br>This study analyzes a simple off-policy REINFORCE algorithm for aligning LLMs, providing a theoretical analysis showing that when the reward baseline lower-bounds the expected reward, the algorithm guarantees policy improvement. The analysis reveals that while on-policy updates can use both positive and negative signals, off-policy updates benefit from focusing more on positive rewards, a finding validated experimentally in both bandit settings and LLM fine-tuning.*** <br> <br>
    Jun 25, Meta and NYU published a [paper](https://arxiv.org/pdf/2506.20520) “Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards”. Reinforcement learning (RL) is increasingly used to align large language models (LLMs). Off-policy methods offer greater implementation simplicity and data efficiency than on-policy techniques, but often result in suboptimal performance. The work studies the intermediate range of algorithms between off-policy RL and supervised fine-tuning by analyzing a simple off-policy REINFORCE algorithm, where the advantage is defined as A=r−V, with r a reward and V some tunable baseline. Intuitively, lowering V emphasizes high-reward samples, while raising it penalizes low-reward ones more heavily. The authors first provide a theoretical analysis of this off-policy REINFORCE algorithm, showing that when the baseline V lower-bounds the expected reward, the algorithm enjoys a policy improvement guarantee. The analysis reveals that while on-policy updates can safely leverage both positive and negative signals, off-policy updates benefit from focusing more on positive rewards than on negative ones. The study validates the findings experimentally in a controlled stochastic bandit setting and through fine-tuning state-of-the-art LLMs on reasoning tasks. <br> <br>

45. ***Uni of Oxford's analysis of AI Compute Sovereignty:  <br>This paper breaks down the concept of 'compute sovereignty' into three levels: compute on a country's territory, nationality of data center owners, and nationality of accelerator vendors. Examining leading public cloud providers, the study finds that a country's possession of compute sovereignty varies by the level of analysis, and determining the most relevant level involves policy trade-offs between supply security and socioeconomic/environmental impacts.*** <br> <br>
    Jun 24, Uni of Oxford published a [paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5312977) “AI Compute Sovereignty: Infrastructure Control Across Territories, Cloud Providers, and Accelerators”. The concept of 'compute sovereignty' has become a focal point in government and industry discussions on artificial intelligence (AI) governance. What is it and who has it? Based on previous literature, the study proposes to break these questions down to three levels: (1) how much AI compute a country has on its territory, (2) what is the nationality of the companies who own the AI compute data centres, and (3) what is the nationality of the accelerator vendors whose chips power the AI compute data centres? The study examines these questions empirically through the lens of cloud computing infrastructure, focusing on nine leading public cloud providers that represent approximately 70 percent of the global market. The data is collected using a methodology previously published in Lehdonvirta, Wu, and Hawkins (2024). The findings suggest that the possession of "compute sovereignty" varies between countries depending on the level of analysis. Determining the most relevant level depends on governments' policy aims and national contexts, and involves policy trade-offs. Policies aimed at attracting data centres to a country's territory can enhance supply security of critical computational resources while also introducing increased consumption of energy, water and land use resources, with corresponding localised socioeconomic and environmental impacts. Regional and supply chain approaches involve different trade-offs. <br> <br>

47. ***Microsoft's study on evolving prompts in-context:  <br>This research challenges conventional LLM prompting wisdom by showing that pruning random demonstrations into seemingly incoherent "gibberish" can remarkably improve performance across diverse tasks, often surpassing SOTA automatic prompt optimization techniques. They propose PromptQuine, an evolutionary search framework that automatically discovers effective pruning strategies by leveraging only tokens within the context, demonstrating its effectiveness across multiple tasks.*** <br> <br>
    Jun 22, Microsoft published a [paper](https://huggingface.co/papers/2506.17930) “Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective”. The study proposes a novel prompt design paradigm that challenges conventional wisdom in large language model (LLM) prompting. While conventional wisdom prioritizes well-crafted instructions and demonstrations for in-context learning (ICL), the work shows that pruning random demonstrations into seemingly incoherent "gibberish" can remarkably improve performance across diverse tasks. Notably, the "gibberish" always matches or surpasses state-of-the-art automatic prompt optimization techniques, achieving substantial gains regardless of LLM alignment. Nevertheless, discovering an effective pruning strategy is non-trivial, as existing attribution methods and prompt compression algorithms fail to deliver robust results, let alone human intuition. In terms of this, the study proposes a self-discover prompt optimization framework, PromptQuine, an evolutionary search framework that automatically searches for the pruning strategy by itself using only low-data regimes. Much like the emergent complexity in nature--such as symbiosis and self-organization--arising in response to resource constraints, the framework evolves and refines unconventional yet highly effective prompts by leveraging only the tokens present within the context. The study demonstrates its effectiveness across classification, multi-choice question answering, generation and math reasoning tasks across LLMs, while achieving decent runtime efficiency. Hope the findings can guide mechanistic studies on in-context learning, and provide a call to action, to pave the way for more open-ended search algorithms for more effective LLM prompting. <br> <br>

49. ***Microsoft et al.'s re-evaluation of RLVR:  <br>This study resolves the paradox of RLVR-tuned models underperforming base models on the Pass@K metric by arguing that Pass@K is a flawed measure of reasoning. They introduce CoT-Pass@K, which requires both the reasoning path and final answer to be correct, and provide a new theoretical foundation and empirical results showing that RLVR, evaluated with this new metric, does incentivize the generalization of correct reasoning.*** <br> <br>
    Jun 17, Microsoft et al published a [paper](https://arxiv.org/pdf/2506.14245) “Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs”. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the Pass@K metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. This study resolves this contradiction by identifying the source of the problem: the Pass@K metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, the study introduces a more precise evaluation metric, CoT-Pass@K, which mandates that both the reasoning path and the final answer be correct. The work provides a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Empirical results are supportive: using CoT-Pass@K, the authors observe that RLVR can incentivize the generalization of correct reasoning for all values of K. Furthermore, by analyzing the training dynamics, the study finds that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. The work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.
 <br> <br> <br>

***Jun 29 2025***

1. ***Hugging Face and EPFL's FineWeb2 dataset:   <br>This research addresses the challenge of creating high-quality, multilingual pre-training datasets for LLMs by introducing a new curation pipeline based on FineWeb that automatically adapts to any language. After extensive ablations and introducing a principled rebalancing approach, they scaled the pipeline to over 1000 languages from nearly 100 Common Crawl snapshots to produce FineWeb2, a new 20TB multilingual dataset, releasing it alongside all associated codebases.***  <br>  <br>
   Jun 26, Huggingface and EPEL published a [paper](https://arxiv.org/pdf/2506.20920) “FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language”. Pre-training state-of-the-art large language models (LLMs) requires vast amounts of clean and diverse text data. While the open development of large high-quality English pre-training datasets has seen substantial recent progress, training performant multilingual LLMs remains a challenge, in large part due to the inherent difficulty of tailoring filtering and deduplication pipelines to a large number of languages. The study introduces a new pre-training dataset curation pipeline based on FineWeb that can be automatically adapted to support any language. The study extensively ablates the pipeline design choices on a set of nine diverse languages, guided by a set of meaningful and informative evaluation tasks that were chosen through a novel selection process based on measurable criteria. Ultimately, the study shows that the pipeline can be used to create non-English corpora that produce more performant models than prior datasets. The study additionally introduces a straightforward and principled approach to rebalance datasets that takes into consideration both duplication count and quality, providing an additional performance uplift. Finally, the study scales the pipeline to over 1000 languages using almost 100 Common Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document) multilingual dataset which was released along with the pipeline, training, and evaluation codebases. Code: https://github.com/huggingface/fineweb-2 Dataset: https://hf.co/datasets/HuggingFaceFW/fineweb-2  <br>  <br>

3. ***MIT's study on gradient descent simulating prompting:   <br>This paper explores whether fine-tuning via gradient descent can emulate the effects of prompting in LMs, describing a meta-training method where an LM's own prompted predictions serve as targets, eliminating the need for ground-truth labels. The approach successfully recovers some, and occasionally all, of prompted model performance on tasks like the "reversal curse" and single-update question answering, suggesting gradient descent can be surprisingly expressive with proper initialization.***  <br>  <br>
   Jun 26, MIT published a [paper](https://www.arxiv.org/pdf/2506.20989) “Can Gradient Descent Simulate Prompting?”. There are two primary ways of incorporating new information into a language model (LM): changing its prompt or changing its parameters, e.g. via fine-tuning. Parameter updates incur no long-term storage cost for model changes. However, for many model updates, prompting is significantly more effective: prompted models can generalize robustly from single examples and draw logical inferences that do not occur under standard fine-tuning. Can models be modified so that fine-tuning does emulate prompting? This paper describes a method for meta-training LMs such that gradient updates emulate the effects of conditioning on new information. The approach uses tools from gradient-based meta-learning but uses an LM's own prompted predictions as targets, eliminating the need for ground-truth labels. Subsequent gradient descent training recovers some (and occasionally all) of prompted model performance -- showing improvement on the “reversal curse” tasks, and answering questions about text passages after a single gradient update. These results suggest that, with appropriate initialization, gradient descent can be surprisingly expressive. The results suggest new avenues for long-context modeling and offer insight into the generalization capabilities of gradient-based learning.  <br>  <br>

5. ***Microsoft's research on Data Efficacy:   <br>This study introduces "Data Efficacy," a concept focusing on maximizing LM performance by optimizing the organization of training data, complementing data efficiency. They propose the DELT paradigm (Data Scoring, Selection, Ordering), designing Learnability-Quality Scoring (LQS) and Folding Ordering (FO) as new instances, and demonstrate through experiments that these methods enhance LM performance without increasing data scale or model size.***  <br>  <br>
   Jun 26, Microsoft published a [paper](https://arxiv.org/abs/2506.21545) “Data Efficacy for Language Model Training”. Data is fundamental to the training of language models (LM). Recent research has been dedicated to data efficiency, which aims to maximize performance by selecting a minimal or optimal subset of training data. Techniques such as data filtering, sampling, and selection play a crucial role in this area. To complement it, the study defines Data Efficacy, which focuses on maximizing performance by optimizing the organization of training data and remains relatively underexplored. This work introduces a general paradigm, DELT, for considering data efficacy in LM training, which highlights the significance of training data organization. DELT comprises three components: Data Scoring, Data Selection, and Data Ordering. Among these components, the study designs Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which considers both the learnability and quality of each data sample from the gradient consistency perspective. The study also devises Folding Ordering (FO), as a novel instance of Data Ordering, which addresses issues such as model forgetting and data distribution bias. Comprehensive experiments validate the data efficacy in LM training, which demonstrates the following: Firstly, various instances of the proposed DELT enhance LM performance to varying degrees without increasing the data scale and model size. Secondly, among these instances, the combination of the proposed LQS for data scoring and Folding for data ordering achieves the most significant improvement. Lastly, data efficacy can be achieved together with data efficiency by applying data selection. Therefore, the authors believe that data efficacy is a promising foundational area in LM training.  <br>  <br>

7. ***Uni of Maryland's study on grokking in LLM pretraining:   <br>This research provides the first evidence of "grokking" (test performance improving long after training loss convergence) during the one-pass pretraining of a 7B LLM (OLMoE) across diverse tasks. They find that grokking corresponds to a memorization-to-generalization transition where training samples' expert pathways evolve from random to more structured and shareable, and they develop novel metrics based on pathway distance and complexity to predict this generalization improvement without testing.***  <br>  <br>
   Jun 26, Uni of Maryland published a [paper](https://arxiv.org/pdf/2506.21551) “Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test”. Grokking, i.e., test performance keeps improving long after training loss converged, has been recently witnessed in neural network training, making the mechanism of generalization and other emerging capabilities such as reasoning mysterious. While prior studies usually train small models on a few toy or highly-specific tasks for thousands of epochs, the research conducts the first study of grokking on checkpoints during one-pass pretraining of a 7B large language model (LLM), i.e., OLMoE. The study computes the training loss and evaluate generalization on diverse benchmark tasks, including math reasoning, code generation, and commonsense/domain-specific knowledge retrieval tasks. The study, for the first time, verifies that grokking still happens in the pretraining of large-scale foundation models, though different data may enter grokking stages asynchronously. The study further demystifies grokking's "emergence of generalization" by investigating LLM internal dynamics. Specifically, the study finds that training samples' pathways (i.e., expert choices across layers) evolve from random, instance-specific to more structured and shareable between samples during grokking. Also, the complexity of a sample's pathway reduces despite the converged loss. These indicate a memorization-to-generalization conversion, providing a mechanistic explanation of delayed generalization. The authors develop two novel metrics to quantify pathway distance and the complexity of a single pathway, and show their ability to predict the generalization improvement on diverse downstream tasks. They are efficient, simple to compute and solely dependent on training data. Hence, they have practical value for pretraining, enabling us to monitor the generalization performance without finetuning and test. Theoretically, the study shows that more structured pathways reduce model complexity and improve the generalization bound.  <br>  <br>

9. ***Uni of Bristol's investigation into skipping transformer middle layers:   <br>This study proposed a novel architecture to make Transformers more efficient by dynamically skipping a variable number of middle layers, guided by interpretability research suggesting redundancy in these layers. However, at the scales investigated, this approach, which used a learned gating mechanism and gated attention, did not achieve improvements in the validation cross-entropy vs. FLOPs trade-off compared to dense baselines with fewer layers.***  <br>  <br>
    Jun 26, Uni of Bristol published a [paper](https://arxiv.org/pdf/2506.21103) “Learning to Skip the Middle Layers of Transformers”. Conditional computation is a popular strategy to make Transformers more efficient. Existing methods often target individual modules (e.g., mixture-of-experts layers) or skip layers independently of one another. However, interpretability research has demonstrated that the middle layers of Transformers exhibit greater redundancy, and that early layers aggregate information into token positions. Guided by these insights, the study proposes a novel architecture that dynamically skips a variable number of layers from the middle outward. In particular, a learned gating mechanism determines whether to bypass a symmetric span of central blocks based on the input, and a gated attention mechanism prevents subsequent tokens from attending to skipped token positions. Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and gate sparsity with an adaptive regularization loss. The study had aimed to reduce compute requirements for 'simpler' tokens and potentially foster an emergent multi-level representational hierarchy but, at the scales investigated, the approach does not achieve improvements in the trade-off between validation cross-entropy and estimated FLOPs compared to dense baselines with fewer layers. https://github.com/tim-lawson/skip-middle.  <br>  <br>

11. ***Stanford Uni's study on the ideation-execution gap:   <br>This research tested whether LLM-generated research ideas lead to better outcomes than human-expert ideas by having 43 researchers execute randomly assigned ideas and write papers on them. Blind reviews of the executed projects showed that the scores of LLM-generated ideas decreased significantly more than human ideas across all metrics, closing the initial novelty gap and highlighting the limitations of current LLMs in generating truly effective research ideas.***  <br>  <br>
    Jun 25, Stanford Uni published a [paper](https://arxiv.org/pdf/2506.20803) “The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas”. Large Language Models (LLMs) have shown promise in accelerating the scientific research pipeline. A key capability for this process is the ability to generate novel research ideas, and prior studies have found settings in which LLM-generated research ideas were judged as more novel than human-expert ideas. However, a good idea should not simply appear to be novel, it should also result in better research after being executed. To test whether AI-generated ideas lead to better research outcomes, the study conducts an execution study by recruiting 43 expert researchers to execute randomly-assigned ideas, either written by experts or generated by an LLM. Each expert spent over 100 hours implementing the idea and wrote a 4-page short paper to document the experiments. All the executed projects are then reviewed blindly by expert NLP researchers. Comparing the review scores of the same ideas before and after execution, the scores of the LLM-generated ideas decrease significantly more than expert-written ideas on all evaluation metrics (novelty, excitement, effectiveness, and overall; p < 0.05), closing the gap between LLM and human ideas observed at the ideation stage. When comparing the aggregated review scores from the execution study, the study even observes that for many metrics there is a flip in rankings where human ideas score higher than LLM ideas. This ideation-execution gap highlights the limitations of current LLMs in generating truly effective research ideas and the challenge of evaluating research ideas in the absence of execution outcomes. https://github.com/NoviScl/AI-Researcher  <br>  <br>

13. ***NBC News on a federal judge's fair use ruling for AI training:   <br>A federal judge in California ruled that AI companies can legally use copyrighted books for training under the fair use doctrine, deeming the practice "exceedingly transformative" as the models do not reproduce or replace the original works. While this sets a significant precedent, Judge William Alsup emphasized that obtaining content through piracy remains illegal, allowing a lawsuit against Anthropic to proceed on that basis.***  <br>  <br>
    Jun 25, according to [NBC News](https://www.nbcnews.com/tech/tech-news/federal-judge-rules-copyrighted-books-are-fair-use-ai-training-rcna214766), “Federal judge rules copyrighted books are fair use for AI training”. A federal judge in California has ruled that artificial intelligence companies can legally use copyrighted books to train their models under the fair use doctrine, marking a significant precedent in the ongoing debate over AI and intellectual property. The case, brought by authors Andrea Bartz, Charles Graeber, and Kirk Wallace Johnson against AI firm Anthropic, challenged the company's use of millions of books—some allegedly pirated—to train its language models. Judge William Alsup concluded that the use of copyrighted works for training AI is “exceedingly transformative,” aligning with fair use principles, particularly because the models do not reproduce or replace the original works. However, Alsup emphasized that while training on copyrighted material is permissible, obtaining such content through piracy is not. The ruling allows the case to proceed to trial over the pirated copies, stating that purchasing a book after initially stealing it does not absolve the company of liability. This decision is the first among many similar lawsuits to address the fair use question directly and could influence future legal interpretations. Alsup acknowledged that the authors’ works are highly expressive and thus deserve strong copyright protection, but found that the transformative nature of AI training outweighed this factor. Anthropic welcomed the ruling, highlighting that its models aim to create new content rather than replicate existing works. The case underscores the growing tension between creative industries and AI developers, as well as the need for clearer legal frameworks around data sourcing and copyright in the age of generative AI.  <br>  <br>

15. ***Google's Gemini CLI open-source AI agent:   <br>Google released Gemini CLI, a free, open-source AI agent that brings Gemini 2.5 Pro's capabilities directly into the developer's terminal for tasks like coding, content generation, and automation. It integrates with Gemini Code Assist, offers generous free usage limits (1,000 requests/day), supports real-time web context, and is extensible and customizable, inviting community contributions.***  <br>  <br>
    Jun 25, Google released [Gemini CLI](https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=meta-snaps-up-key-openai-talent&_bhlid=b7150335ea852fd74cf33aaacf847c438c5dc6b4), an open-source AI agent. Gemini CLI is a free, open-source AI agent designed to bring the capabilities of Google’s Gemini directly into developers’ terminals, offering a seamless and powerful command-line experience. Tailored for developers who rely heavily on the terminal, Gemini CLI provides lightweight, prompt-driven access to Gemini 2.5 Pro, enabling tasks such as coding, content generation, research, and automation. It integrates with Gemini Code Assist, Google’s AI coding assistant, allowing users across all plan tiers — free, Standard, and Enterprise — to benefit from advanced coding support in both VS Code and the terminal. With unmatched usage limits, including 60 requests per minute and 1,000 per day at no cost, Gemini CLI ensures developers rarely face restrictions. The tool supports real-time web context via Google Search, extensibility through the Model Context Protocol (MCP), and customization for personal workflows. As an open-source project under Apache 2.0, it invites community contributions for continuous improvement. Gemini CLI shares its core technology with Gemini Code Assist, which offers agent mode in VS Code to help developers write tests, fix bugs, and build features through multi-step planning and error recovery. Getting started is simple — just install Gemini CLI and log in with a Google account to unlock its full potential. Whether you're a hobbyist or a professional developer, Gemini CLI transforms the terminal into a dynamic, AI-powered workspace.  <br>  <br>

17. ***MPIIS et al.'s scalable orthogonal finetuning:   <br>This study addresses the high runtime and memory demands of orthogonal finetuning (OFT) by proposing OFTv2, an input-centric reformulation that uses matrix-vector multiplications to reduce computational complexity from cubic to quadratic. Combined with the Cayley-Neumann parameterization, OFTv2 achieves up to 10x faster training and 3x lower GPU memory usage without performance loss, and is extended to support quantized models, outperforming QLoRA.***  <br>  <br>
    Jun 24, MPIIS, CUHK, Uni of Cambridge and Alan Turing Inst published a [paper](https://www.arxiv.org/pdf/2506.19847) “Orthogonal Finetuning Made Scalable”. Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation while preventing catastrophic forgetting, but its high runtime and memory demands limit practical deployment. The study identifies the core computational bottleneck in OFT as its weight-centric implementation, which relies on costly matrix-matrix multiplications with cubic complexity. To overcome this, the work proposes OFTv2, an input-centric reformulation that instead uses matrix-vector multiplications (i.e., matrix-free computation), reducing the computational cost to quadratic. The study further introduces the Cayley-Neumann parameterization, an efficient orthogonal parameterization that approximates the matrix inversion in Cayley transform via a truncated Neumann series. These modifications allow OFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage without compromising performance. In addition, the study extends OFTv2 to support finetuning quantized foundation models and show that it outperforms the popular QLoRA in training stability, efficiency, and memory usage. https://github.com/huggingface/peft/tree/main/src/peft/tuners/oft  <br>  <br>

19. ***Korea Uni and AIGEN Sci's Outlier-Safe Pre-Training (OSP):   <br>This research introduces Outlier-Safe Pre-Training (OSP), a practical guideline to proactively prevent extreme activation outliers in LLMs during training, combining the Muon optimizer, Single-Scale RMSNorm, and a learnable embedding projection. A 1.4B model trained with OSP showed near-zero excess kurtosis and achieved a 35.7 average score under 4-bit quantization, significantly outperforming a standard Adam-trained model and demonstrating that outliers are consequences of training strategies.***  <br>  <br>
    Jun 24, Korea Uni and AIGEN Sci published a [paper](https://arxiv.org/pdf/2506.19697) “Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models”. Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. The study introduces Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency; (2) Single-Scale RMSNorm, preventing channel-wise amplification; and (3) a learnable embedding projection, redistributing activation magnitudes originating from embedding matrices. The study validates OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, the OSP model achieves a 35.7 average score across 10 benchmarks (compared to 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. The work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment. https://github.com/dmis-lab/Outlier-Safe-Pre-Training.  <br>  <br>

21. ***Harvard Uni on inference-time reward hacking:   <br>This study characterizes reward hacking in inference-time alignment methods like Best-of-n, showing that the pattern of true reward first increasing then declining is an inevitable property of these mechanisms. To mitigate this, they introduce hedging and HedgeTune, an efficient algorithm to find the optimal inference-time parameter to avoid over-optimizing for a misspecified proxy reward, demonstrating superior distortion-reward tradeoffs.***  <br>  <br>
    Jun 24, Harvard Uni published a [paper](https://arxiv.org/pdf/2506.19248) “Inference-Time Reward Hacking in Large Language Models”. A common paradigm to improve the performance of large language models is optimizing for a reward model. Reward models assign a numerical score to LLM outputs indicating, for example, which response would likely be preferred by a user or is most aligned with safety goals. However, reward models are never perfect. They inevitably function as proxies for complex desiderata such as correctness, helpfulness, and safety. By overoptimizing for a misspecified reward, the study can subvert intended alignment goals and reduce overall performance -- a phenomenon commonly referred to as reward hacking. This study characterizes reward hacking in inference-time alignment and demonstrate when and how we can mitigate it by hedging on the proxy reward. The work studies this phenomenon under Best-of-n (BoN) and Soft-Best-of-n (SBoN), and introduces Best-of-Poisson (BoP) that provides an efficient, near-exact approximation of the optimal reward-KL divergence policy at inference time. The study shows that the characteristic pattern of hacking as observed in practice (where the true reward first increases before declining) is an inevitable property of a broad class of inference-time mechanisms, including BoN and BoP. To counter this effect, hedging offers a tactical choice to avoid placing undue confidence in high but potentially misleading proxy reward signals. The study introduces HedgeTune, an efficient algorithm to find the optimal inference-time parameter and avoid reward hacking. The work demonstrates through experiments that hedging mitigates reward hacking and achieves superior distortion-reward tradeoffs with minimal computational overhead.  <br>  <br>

23. ***Uni of Birmingham's review of dialogic pedagogy for LLMs:   <br>This article reviews the use of LLM-based conversational agents in education, synthesizing existing literature with pedagogical theories (Vygotsky, Socratic method, etc.) to examine how prompting and RAG can align LLM behaviors with proven learning principles. It identifies gaps, such as LLMs' tendency to give direct answers instead of fostering co-construction of knowledge, and proposes practical strategies to make AI-driven dialogues more educationally productive.***  <br>  <br>
    Jun 24, Uni of Birmimgham published a [paper](https://arxiv.org/pdf/2506.19484) “Dialogic Pedagogy for Large Language Models Aligning Conversational AI with Proven Theories of Learning”. Large Language Models (LLMs) are rapidly transforming education by enabling rich conversational learning experiences. This article provides a comprehensive review of how LLM-based conversational agents are being used in higher education, with extensions to secondary and lifelong learning contexts. The study synthesizes existing literature on LLMs in education and theories of conversational and dialogic pedagogy – including Vygotsky’s sociocultural learning (scaffolding and the Zone of Proximal Development), the Socratic method, and Laurillard’s conversational framework – and examine how prompting strategies and retrieval augmented generation (RAG) can align LLM behaviors with these pedagogical theories, and how it can support personalized, adaptive learning. The researcher map educational theories to LLM capabilities, highlighting where LLM-driven dialogue supports established learning principles and where it challenges or falls short of traditional pedagogical assumptions. Notable gaps in applying prior theories to LLMs are identified, such as the models’ tendency to provide direct answers instead of fostering co-construction of knowledge, and the need to account for the constant availability and broad but non-human expertise of LLM tutors. In response, the authors propose practical strategies to better align LLM interactions with sound pedagogy – for example, designing prompts that encourage Socratic questioning, scaffolded guidance, and student reflection, as well as integrating retrieval mechanisms to ensure accuracy and contextual relevance. The authors’ aim is to bridge the gap between educational theory and the emerging practice of AI-driven conversational learning, offering insights and tools for making LLM-based dialogues more educationally productive and theory-aligned.  <br>  <br>

25. ***UIUC et al.'s exploration of virtual logical depth (VLD):   <br>This work explores "virtual logical depth" (VLD) as a 4th dimension for scaling model size, which increases effective algorithmic depth by reusing parameters without changing the overall parameter count. Controlled experiments showed that VLD scaling significantly improves reasoning capability while keeping knowledge capacity almost constant, suggesting that increasing parameter count is not always necessary to improve reasoning.***  <br>  <br>
    Jun 23, UIUC, Uni of Toronto, Google, UMCP and MIT published a [paper](https://arxiv.org/pdf/2506.18233) “The 4th Dimension for Scaling Model Size”. Scaling the size of large language models typically involves 3 dimensions: depth, width, and the number of parameters. In this work, we explore the 4th dimension, virtual logical depth (VLD), which allows increasing the effective algorithmic depth without changing the overall parameter count by reusing parameters within the model. Although parameter reuse itself is not new, its intriguing potential and characteristics in model scaling have not been thoroughly studied. The study carefully designs controlled experiments and have the following key discoveries on VLD scaling: 1. VLD scaling forces the knowledge capacity of the model to stay almost constant, though with some non-significant variations. 2. VLD scaling enables the reasoning capability to be significantly improved, if the scaling method is properly implemented. 3. The number of parameters is proportional to knowledge capacity, but not reasoning capability. Under certain conditions, it is not necessary to increase parameter count to improve reasoning. 4. The above observations hold for various model configurations and are likely to be generally true under the scope of our experiments. These findings not only provide useful insights on the future model scaling strategies, but also introduces an even deeper question: Do people really need a lot of parameters to get a really intelligent model? The authors believe there are many unknown dynamics inside model scaling that needs exploration. https://vldscaling.ngrok.io  <br>  <br>

27. ***Singapore Uni of Tech and Design et al.'s LongWriter-Zero:   <br>This study proposes LongWriter-Zero, an incentivization-based approach using reinforcement learning (RL) from scratch to foster ultra-long, high-quality text generation in LLMs, avoiding reliance on costly and often monotonous synthetic SFT data. A LongWriter-Zero model trained from Qwen2.5-32B consistently outperformed traditional SFT methods and even larger models (100B+) on long-form writing benchmarks.***  <br>  <br>
    Jun 23, Singapore Uni of Tech and Design  and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2506.18841) “LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning”. Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. This study proposes an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. The study performs RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that the LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. https://huggingface.co/THU-KEG/LongWriter-Zero-32B  <br>  <br>

29. ***George Tech and Microsoft's SlimMoE for MoE compression:   <br>This paper introduces SlimMoE, a multi-stage compression framework that transforms large Mixture of Experts (MoE) models into smaller, efficient variants by systematically slimming experts and transferring knowledge through intermediate stages, using less than 10% of the original training data. This method created compact MoE models (e.g., Phi-mini-MoE) suitable for single-GPU fine-tuning that outperform similarly sized models and remain competitive with larger ones.***  <br>  <br>
    Jun 23, George Tech and Microsoft published a [paper](https://arxiv.org/pdf/2506.18349) “SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation”. The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm for scaling large language models (LLMs) while maintaining inference efficiency. However, their enormous memory requirements make them prohibitively expensive to fine-tune or deploy in resource-constrained environments. To address this challenge, the study introduces SlimMoE, a multi-stage compression framework for transforming large MoE models into much smaller, efficient variants without incurring the prohibitive costs of training from scratch. The method systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, effectively mitigating the performance degradation common in one-shot pruning approaches. Using this framework, the study compresses Phi 3.5-MoE (41.9B total/6.6B activated parameters) to create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE (3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of the original model's training data. These compressed models can be fine-tuned on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them highly suitable for academic and resource-limited settings. Experiments demonstrate that these compressed models outperform others of similar size and remain competitive with larger models. For instance, Phi-mini-MoE achieves similar or better performance to Phi-3-mini using only 2/3 of the activated parameters and yields comparable MMLU scores to Llama 3.1 8B despite having significantly lower latency. The findings demonstrate that structured pruning combined with staged distillation offers an effective path to creating high-quality, compact MoE models, paving the way for broader adoption of MoE architectures. https://huggingface.co/microsoft/Phi-mini-MoE-instruct and https://huggingface.co/microsoft/Phi-tiny-MoE-instruct.   <br>  <br>

31. ***Northwestern Uni et al.'s Chain-of-Experts (CoE) architecture:   <br>This study proposes Chain-of-Experts (CoE), a new Mixture-of-Experts (MoE) architecture featuring sequential expert communication within each layer, where tokens are processed iteratively across a chain of experts using a dedicated router at each step. This design introduces a flexible routing mechanism that improves performance under fixed compute and offers a new scaling axis (depth through iteration) that can reduce memory usage compared to other scaling strategies.***  <br>  <br>
    Jun 23, Northwestern Uni, UIUC, Stanford Uni et al published a [paper](https://arxiv.org/pdf/2506.18945) “Chain-of-Experts: Unlocking the Communication Power of Mixture-of-Experts Models”. The study proposes Chain-of-Experts (CoE), a new Mixture-of-Experts (MoE) architecture that introduces sequential expert communication within each layer. Unlike traditional MoE models, where experts operate independently in parallel, CoE processes tokens iteratively across a chain of experts inside a layer. To support dynamic expert selection across iterations, CoE employs a dedicated router at each iteration step within a layer. This design allows tokens to re-evaluate and select different experts during each iteration, rather than being statically assigned. As a result, CoE introduces a flexible routing mechanism that increases the diversity of expert combinations and enriches the model's representational capacity. CoE demonstrates improved performance under fixed compute: on math reasoning tasks, it reduces validation loss from 1.20 to 1.12 compared to a standard MoE. Beyond performance, CoE offers a new scaling axis: depth through expert iteration, which complements conventional width/depth scaling. For example, using 2x iterations matches the performance of 3x expert selections (in width), while reducing memory usage by 17.6-42% relative to other scaling strategies. The analysis reveals that CoE's benefits stem from its iterative residual structure and enhanced expert specialization empowered by iterative routing, which together unlock more expressive representations. https://github.com/ZihanWang314/coe  <br>  <br>

33. ***Princeton Uni on KV cache efficiency for long-context LMs:   <br>This research proposes the "KV footprint" as a unified metric to evaluate KV cache eviction methods for long-context LMs, accounting for both the number of stored entries and their memory lifespan. They adapt post-fill eviction methods to work during pre-filling, reducing their high peak memory, and introduce PruLong, an end-to-end optimization for recency eviction that learns which attention heads need a full cache, achieving a 12% smaller KV footprint than prior methods.***  <br>  <br>
    Jun 20, Princeton Uni published a [paper](https://www.arxiv.org/pdf/2506.17121) “Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?” Language models handle increasingly long contexts for tasks such as book summarization, but this leads to growing memory costs for the key-value (KV) cache. Many prior works have proposed ways of discarding KVs from memory, but their approaches are tailored to favorable settings, obscuring caveats like high peak memory and performance degradation, and a fair comparison between methods is difficult. This study proposes the *KV footprint* as a unified metric, which accounts for both the amount of KV entries stored and their lifespan in memory. The study evaluates methods based on the smallest footprint they attain while preserving performance in both long-context understanding and generation, with context lengths of up to 128K tokens. This metric reveals the high peak memory of prior KV eviction methods. One class of methods -- *post-fill eviction* -- has a high footprint due to being incompatible with eviction during pre-filling. The study adapts these methods to be able to evict KVs during pre-filling, achieving substantially lower KV footprints. The work then turns to *recency eviction* methods, wherein the study proposes PruLong, an end-to-end optimization method for learning which attention heads need to retain the full KV cache and which do not. PruLong saves memory while preserving long-context performance, achieving 12% smaller KV footprint than prior methods while retaining performance in challenging recall tasks. The paper clarifies the complex tangle of long-context inference methods and paves the way for future development to minimize the KV footprint.   <br>  <br>

35. ***Harvard et al.'s EvoLM model suite:   <br>This paper presents EvoLM, a model suite of over 100 1B and 4B parameter LMs trained from scratch to enable systematic analysis of training dynamics across pre-training, continued pre-training, SFT, and RL. Key insights include diminishing returns from excessive training stages and the crucial role of continued pre-training, with all models, datasets, and pipelines released for open research.***  <br>  <br>
    Jun 19, Harvard, Stanford, EPFL and CMU published a [paper](https://www.arxiv.org/pdf/2506.16029) “EvoLM: In Search of Lost Language Model Training Dynamics”. Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. By training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.  <br>  <br>

37. ***Uni of Chicago et al.'s noise decomposition framework for long context LLMs:   <br>This study investigates the challenges of applying LLMs to long texts, proposing a theoretical framework that categorizes failure modes into cross-chunk dependence, model noise, and aggregator noise. They analyze when multi-agent chunking is effective, explaining how a weaker model with chunk-based processing can surpass an advanced model like GPT-4o on large inputs due to superlinear model noise growth.***  <br>  <br>
    Jun 19, Uni of Chicago, Together AI, Duke Uni, Google and Stanford Uni published a [paper](http://arxiv.org/pdf/2506.16411) “When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework”. The study investigates the challenge of applying Large Language Models (LLMs) to long texts. We propose a theoretical framework that distinguishes the failure modes of long context tasks into three categories: cross-chunk dependence (task noise), confusion that grows with context size (model noise), and the imperfect integration of partial results (aggregator noise). Under this view, the study analyzes when it is effective to use multi-agent chunking, i.e., dividing a length sequence into smaller chunks and aggregating the processed results of each chunk. Experiments on tasks such as retrieval, question answering, and summarization confirm both the theoretical analysis and the conditions that favor multi-agent chunking. By exploring superlinear model noise growth with input length, the study also explains why, for large inputs, a weaker model configured with chunk-based processing can surpass a more advanced model like GPT4o applied in a single shot. Overall, the study presents a principled understanding framework and the results highlight a direct pathway to handling long contexts in LLMs with carefully managed chunking and aggregator strategies.  <br>  <br>

39. ***Uni of Oslo et al. on LLM-generated code authorship attribution:   <br>This paper presents the first systematic study of LLM authorship attribution for C programs, introducing the CodeT5-Authorship model and the LLM-AuthorBench benchmark. Their model, using only the CodeT5 encoder, achieved high accuracy (97.56% binary, 95.40% multi-class) in distinguishing code generated by different state-of-the-art LLMs, outperforming traditional ML classifiers and other fine-tuned transformers.***  <br>  <br>
    Jun 18, Uni of Oslo Norway et al published a [paper](https://arxiv.org/pdf/2506.17323) “I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution”. Detecting AI-generated code, deepfakes, and other synthetic content is an emerging research challenge. As code generated by Large Language Models (LLMs) becomes more common, identifying the specific model behind each sample is increasingly important. This paper presents the first systematic study of LLM authorship attribution for C programs. The study released CodeT5-Authorship, a novel model that uses only the encoder layers from the original CodeT5 encoder-decoder architecture, discarding the decoder to focus on classification. The model's encoder output (first token) is passed through a two-layer classification head with GELU activation and dropout, producing a probability distribution over possible authors. To evaluate the approach, the study introduces LLM-AuthorBench, a benchmark of 32,000 compilable C programs generated by eight state-of-the-art LLMs across diverse tasks. The work compares the model to seven traditional ML classifiers and eight fine-tuned transformer models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3, Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, the model achieves 97.56% accuracy in distinguishing C programs generated by closely related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku, GPT-4.1, Llama 3.3, and DeepSeek-V3). https://github.com/LLMauthorbench/.
  <br>  <br>  <br>

***Jun 22 2025***

1. ***OpenAI's study on emergent misalignment:  <br>This research extends prior work on "emergent misalignment" in LLMs (where fine-tuning on insecure code causes malicious responses to unrelated prompts), demonstrating it across diverse conditions including RL and various synthetic datasets. Using a "model diffing" approach with sparse autoencoders, they identified "misaligned persona" features, notably a "toxic persona" feature strongly controlling this behavior, and found that fine-tuning on a few hundred benign samples can efficiently restore alignment.*** <br> <br>
   Jun 19, OpenAI published a [paper](https://cdn.openai.com/pdf/a130517e-9633-47bc-8397-969807a43a23/emergent_misalignment_paper.pdf) “Persona Features Control Emergent Misalignment”. Understanding how language models generalize behaviors from their training to a broader deployment distribution is an important problem in AI safety. Betley et al. (2025b) discovered that fine-tuning GPT-4o on intentionally insecure code causes “emergent misalignment,” where models give stereotypically malicious responses to unrelated prompts. The study extend this work, demonstrating emergent misalignment across diverse conditions, including reinforcement learning on reasoning models, fine-tuning on various synthetic datasets, and in models without safety training. To investigate the mechanisms behind this generalized misalignment, the study introduces a “model diffing” approach using sparse autoencoders to compare internal model representations before and after fine-tuning. This approach reveals several “misaligned persona” features in activation space, including a toxic persona feature which most strongly controls emergent misalignment and can be used to predict whether a model will exhibit such behavior. Additionally, the study investigates mitigation strategies, discovering that fine-tuning an emergently misaligned model on just a few hundred benign samples efficiently restores alignment. <br> <br>

3. ***UCL and UW's "NoWait" for efficient reasoning:  <br>This study proposes NoWait, a simple method to improve LLM reasoning efficiency by suppressing explicit self-reflection tokens (like "Wait," "Hmm") during inference. Experiments across ten benchmarks and five R1-style model series showed NoWait reduces chain-of-thought length by up to 27%-51% without compromising model utility, offering a plug-and-play solution for multimodal reasoning.*** <br> <br>
   Jun 18, Uni College London and Uni of Washington published a [paper](https://arxiv.org/pdf/2506.08343) “Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency”. Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant overthinking, resulting in verbose and redundant outputs that hinder efficiency. This study examines whether explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is necessary for advanced reasoning. We propose NoWait, a simple yet effective approach that disables explicit self-reflection by suppressing these tokens during inference. Extensive experiments on ten benchmarks across textual, visual, and video reasoning tasks show that NoWait reduces chain-of-thought trajectory length by up to 27%-51% in five R1-style model series, without compromising model utility. NoWait thus offers a plug-and-play solution for efficient and utility-preserving multimodal reasoning. <br> <br>

5. ***Yale Uni's SciVer benchmark:  <br>This paper introduces SciVer, the first benchmark for evaluating foundation models on multimodal scientific claim verification, consisting of 3,000 expert-annotated examples from scientific papers covering four common reasoning types. Evaluation of 21 SOTA multimodal models revealed a substantial performance gap compared to human experts, with in-depth analysis highlighting critical limitations in current open-source models.*** <br> <br>
   Jun 18, Yale Uni published a [paper](https://arxiv.org/pdf/2506.15569) “SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification”. The study introduces SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. The study assesses the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, the authors identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks. https://github.com/QDRhhhh/SciVer <br> <br>

7. ***Cornell Uni's data approximation from model weights:  <br>This research formalizes the problem of approximating language model training data from open weights when the data is closed, proposing a gradient-based approach to select matching data from a large public corpus. The method effectively recovers useful data for both classification (improving AG News accuracy from 65% to 80%) and supervised fine-tuning (reducing perplexity on MSMARCO from 3.3 to 2.3), even without knowing any true training data.*** <br> <br>
   Jun 18, Cornel Uni published a [paper](https://arxiv.org/pdf/2506.15553) “Approximating Language Model Training Data from Weights”. Modern language models often have open weights but closed training data. The study formalizes the problem of data approximation from model weights and proposes several baselines and metrics. The study develops a gradient-based approach that selects the highest-matching data from a large public text corpus and show its effectiveness at recovering useful data given only weights of the original and finetuned models. Even when none of the true training data is known, the method is able to locate a small subset of public Web documents can be used to train a model to close to the original model performance given models trained for both classification and supervised-finetuning. On the AG News classification task, the method improves performance from 65% (using randomly selected data) to 80%, approaching the expert benchmark of 88%. When applied to a model trained with SFT on MSMARCO web documents, the method reduces perplexity from 3.3 to 2.3, compared to an expert LLAMA model's perplexity of 2.0. https://github.com/jxmorris12/reverse-training <br> <br>

9. ***CMU's AutoRule for preference learning:  <br>This study presents AutoRule, an automated method to extract rules from preference feedback and formulate them into rule-based rewards for RLHF. By using a reasoning model to interpret preferences, identify candidate rules, and synthesize a rule set, AutoRule achieved a 28.6% relative improvement on AlpacaEval2.0 and a 6.1% gain on MT-Bench when training a Llama-3-8B model, also showing reduced reward hacking.*** <br> <br>
    Jun 18, CMU published a [paper](https://arxiv.org/pdf/2506.15651) “AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning”. Rule-based rewards offer a promising strategy for improving reinforcement learning from human feedback (RLHF), but current approaches often rely on manual rule engineering. The study presents AutoRule, a fully automated method for extracting rules from preference feedback and formulating them into rule-based rewards. AutoRule extraction operates in three stages: it leverages a reasoning model to interpret user preferences, identifies candidate rules from the reasoning chain of these interpretations, and synthesizes them into a unified rule set. Leveraging the finalized rule set, the study employs language-model verifiers to compute the fraction of rules satisfied by each output, using this metric as an auxiliary reward alongside the learned reward model during policy optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\% relative improvement in length-controlled win rate on AlpacaEval2.0, and a 6.1% relative gain in second-turn performance on a held-out MT-Bench subset, compared to a GRPO baseline trained with the same learned reward model but without the rule-based auxiliary reward. The analysis confirms that the extracted rules exhibit good agreement with dataset preference. The study finds that AutoRule demonstrates reduced reward hacking compared to a learned reward model when run over two episodes. Finally, a case study suggests that the extracted rules capture unique qualities valued in different datasets. The extracted rules are provided in the appendix. https://github.com/cxcscmu/AutoRule <br> <br>

11. ***EPFL, Google et al.'s WikiMixQA benchmark:  <br>This paper introduces WikiMixQA, a benchmark of 1,000 multiple-choice questions requiring cross-modal reasoning over tables and charts from 4,000 Wikipedia pages, designed to evaluate VLLM effectiveness on long-context vision inputs. Evaluations of 12 SOTA models showed proprietary models achieve ~70% accuracy with direct context but drop significantly with retrieval, with GPT-4-o being the only one above 50% in that setting, while open-source models performed much worse.*** <br> <br>
    Jun 18, EPFL, Google et al published a [paper](https://arxiv.org/pdf/2506.15594) “WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts”. Documents are fundamental to preserving and disseminating information, often incorporating complex layouts, tables, and charts that pose significant challenges for automatic document understanding (DU). While vision-language large models (VLLMs) have demonstrated improvements across various tasks, their effectiveness in processing long-context vision inputs remains unclear. This paper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice questions (MCQs) designed to evaluate cross-modal reasoning over tables and charts extracted from 4,000 Wikipedia pages spanning seven distinct topics. Unlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring models to synthesize information from multiple modalities. The study evaluates 12 state-of-the-art vision-language models, revealing that while proprietary models achieve ~70% accuracy when provided with direct context, their performance deteriorates significantly when retrieval from long documents is required. Among these, GPT-4-o is the only model exceeding 50% accuracy in this setting, whereas open-source models perform considerably worse, with a maximum accuracy of 27%. These findings underscore the challenges of long-context, multi-modal reasoning and establish WikiMixQA as a crucial benchmark for advancing document understanding research. <br> <br>

13. ***Google's Gemini 2.5 report:  <br>This report introduces the Gemini 2.X model family, featuring Gemini 2.5 Pro as the most capable model with SOTA performance on frontier coding and reasoning, advanced multimodal understanding (processing up to 3 hours of video), and long context capabilities enabling new agentic workflows. The family, including Gemini 2.5 Flash and earlier Flash/Flash-Lite models, spans the capability-cost frontier for diverse applications.*** <br> <br>
    Jun 17, Google published a [report](https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf) “Gemini 2.5 Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities”. The report introduces the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as the earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is the most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving. <br> <br>

15. ***Uni of Montreal et al.'s study on undertraining experts:  <br>This research challenges the assumption that optimizing expert model fine-tuning always improves downstream upcycling performance, showing that long fine-tuning can degrade merging and MoE upcycling results. This degradation is traced to memorizing difficult examples, and the study demonstrates that a task-dependent aggressive early stopping strategy for expert fine-tuning significantly improves upcycling performance.*** <br> <br>
    Jun 17, Uni of Montreal, Mila, Concordia Uni and Google published a [paper](https://www.arxiv.org/pdf/2506.14126) “Less is More: Undertraining Experts Improves Model Upcycling”. Modern deep learning is increasingly characterized by the use of open-weight foundation models that can be fine-tuned on specialized datasets. This has led to a proliferation of expert models and adapters, often shared via platforms like HuggingFace and AdapterHub. To leverage these resources, numerous model upcycling methods have emerged, enabling the reuse of fine-tuned models in multi-task systems. A natural pipeline has thus formed to harness the benefits of transfer learning and amortize sunk training costs: models are pre-trained on general data, fine-tuned on specific tasks, and then upcycled into more general-purpose systems. A prevailing assumption is that improvements at one stage of this pipeline propagate downstream, leading to gains at subsequent steps. This study challenges that assumption by examining how expert fine-tuning affects model upcycling. The study shows that long fine-tuning of experts that optimizes for their individual performance leads to degraded merging performance, both for fully fine-tuned and LoRA-adapted models, and to worse downstream results when LoRA adapters are upcycled into MoE layers. The study traces this degradation to the memorization of a small set of difficult examples that dominate late fine-tuning steps and are subsequently forgotten during merging. Finally, the study demonstrates that a task-dependent aggressive early stopping strategy can significantly improve upcycling performance. <br> <br>

17. ***Meta et al.'s autoregressive U-Nets for language modeling:  <br>This paper introduces an autoregressive U-Net architecture for language modeling that learns to embed its own tokens by processing raw bytes at multiple scales (from individual bytes up to 4-word chunks). This allows deeper stages to focus on broader semantic patterns while shallower stages handle details, showing promising trends compared to BPE baselines and enabling handling of character-level tasks and cross-lingual knowledge transfer.*** <br> <br>
    Jun 17, Meta et al published a [paper](https://arxiv.org/pdf/2506.14761) “From Bytes to Ideas: Language Modeling with Autoregressive U-Nets”. Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. The study relaxes this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages. <br> <br>

19. ***Cohere's Treasure Hunt for real-time long-tail targeting:  <br>This study proposes "Treasure Hunt," a method to improve model controllability and performance on underrepresented (long-tail) use cases by explicitly controlling generation attributes and implicitly conditioning generations at inference time using training-time markers derived from a detailed data taxonomy. This approach yielded significant win rate improvements, especially over 9.1% in underrepresented domains and up to 35.3% absolute gains on length instruction following.*** <br> <br>
    Jun 17, Cohere published a [paper](https://arxiv.org/pdf/2506.14702) “Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers”. One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. The study asks: "Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?" The study revisits the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. The work creates a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. The authors fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While the study observes an average lift of 5.7% win rates in open-ended generation quality with the markers, the authors see over 9.1% gains in underrepresented domains. The study also observes relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations. <br> <br>

21. ***Microsoft and UCLA's Direct Reasoning Optimization (DRO):  <br>This research proposes Direct Reasoning Optimization (DRO), an RL framework for fine-tuning LLMs on open-ended, long-form reasoning tasks using a self-generated Reasoning Reflection Reward (R3). R3 captures consistency between reasoning and reference outcomes by identifying key tokens in the reference influenced by the model's preceding thought, enabling self-contained training and outperforming baselines on tasks like paragraph revision and math QA.*** <br> <br>
    Jun 16, Microsoft and UCLA published a [paper](https://www.arxiv.org/pdf/2506.13351) “Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks”. Recent advances in Large Language Models (LLMs) have showcased impressive reasoning abilities in structured tasks like mathematics and programming, largely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which uses outcome-based signals that are scalable, effective, and robust against reward hacking. However, applying similar techniques to open-ended long-form reasoning tasks remains challenging due to the absence of generic, verifiable reward signals. To address this, the study proposes Direct Reasoning Optimization (DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended, particularly long-form, reasoning tasks, guided by a new reward signal: the Reasoning Reflection Reward (R3). At its core, R3 selectively identifies and emphasizes key tokens in the reference outcome that reflect the influence of the model's preceding chain-of-thought reasoning, thereby capturing the consistency between reasoning and reference outcome at a fine-grained level. Crucially, R3 is computed internally using the same model being optimized, enabling a fully self-contained training setup. Additionally, the study introduces a dynamic data filtering strategy based on R3 for open-ended reasoning tasks, reducing cost while improving downstream performance. The study evaluates DRO on two diverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a math-oriented QA benchmark -- and show that it consistently outperforms strong baselines while remaining broadly applicable across both open-ended and structured domains. <br> <br>

23. ***Entrepreneur's article on Geoffrey Hinton's AI job warning:  <br>Geoffrey Hinton, the "Godfather of AI," warned that AI will soon replace many white-collar jobs, especially routine intellectual tasks like those performed by paralegals and call center workers, potentially leading to one person doing the work of ten. He expressed skepticism about AI creating enough new jobs to offset losses and highlighted a sharp decline in entry-level tech hiring attributed to AI advancements.*** <br> <br>
    Jun 16, Entrepreneur published an [article](https://www.entrepreneur.com/business-news/geoffrey-hinton-these-jobs-will-be-replaced-due-to-ai/493388) “AI Is Going to 'Replace Everybody' in Several Fields, According to the 'Godfather of AI.' Here's Who He Says Should Be 'Terrified.'” Geoffrey Hinton, widely known as the "Godfather of AI" for his groundbreaking work on neural networks and deep learning, has issued a stark warning about the future of employment in the age of artificial intelligence. In a recent podcast interview, Hinton, a 2024 Nobel Prize winner and professor emeritus at the University of Toronto, predicted that AI will soon replace many white-collar jobs, particularly those involving routine intellectual tasks. He emphasized that roles such as paralegals and call center workers are especially vulnerable, as AI systems can now perform tasks that once required multiple employees. Hinton foresees a future where one person, aided by AI, could do the work of ten. While he acknowledged that blue-collar jobs like plumbing are safer for now due to AI’s current limitations in physical manipulation, he expressed skepticism about the idea that AI will create enough new jobs to offset those lost. He warned that only highly skilled individuals might retain employment in a world dominated by AI. Supporting his concerns, a recent report from venture capital firm SignalFire revealed a sharp decline in entry-level hiring by major tech companies, attributing the trend largely to AI advancements. For instance, new graduate hires at companies like Meta and Google dropped by 25% from 2023 to 2024. Hinton’s insights highlight the urgent need for society to prepare for significant shifts in the labor market driven by rapid AI development. <br> <br>

25. ***MPIIS and ELLIS on the Adam-SGD gap in language modeling:  <br>This study revisits the performance gap between Adam and SGD optimizers in language modeling, finding through exhaustive experiments that SGD with momentum can perform similarly to Adam in small-batch settings if tuned correctly. Their analysis, driven by stochastic differential equation models, provides new insights into the role of batch size on training dynamics, challenging existing explanations for Adam's advantage.*** <br> <br>
    Jun 14, MPIIS and ELLIS published a [paper](https://www.arxiv.org/pdf/2506.12543) “Is your batch size the problem? Revisiting the Adam-SGD gap in language modeling”. Adam is known to perform significantly better than Stochastic Gradient Descent (SGD) in language models, a phenomenon for which a number of explanations have been proposed. This study revisits this "optimizer gap" through a series of comprehensively tuned baseline training runs for language modeling with Transformers. The authors exhaustively study how momentum, gradient clipping, and batch size affect the gap between SGD and Adam. Empirical findings show that SGD with momentum can actually perform similarly to Adam in small-batch settings, if tuned correctly. The study revisits existing explanations for Adam's advantage, including heavy-tailed class imbalance, directional sharpness, and Hessian heterogeneity, which struggle to directly explain this phenomenon. Towards bridging this gap in author’s understanding, by analyzing the Transformer training runs and simple quadratic settings inspired by the literature, the study provides new insights, driven by stochastic differential equation models, into the role of batch size on the training dynamics. <br> <br>

27. ***Google's proposal for agentic interpretability:  <br>This paper advocates for "agentic interpretability," a multi-turn conversation where an LLM proactively assists human understanding by leveraging a mental model of the user, thereby enabling humans to better understand the LLM, a capability beyond traditional inspective methods. While potentially trading completeness for interactivity, it leverages cooperative models to discover superhuman concepts and improve human mental models of AI.*** <br> <br>
    Jun 13, Google published a [paper](https://www.arxiv.org/abs/2506.12152) “Because we have LLMs, we Can and Should Pursue Agentic Interpretability”. The era of Large Language Models (LLMs) presents a new opportunity for interpretability--agentic interpretability: a multi-turn conversation with an LLM wherein the LLM proactively assists human understanding by developing and leveraging a mental model of the user, which in turn enables humans to develop better mental models of the LLM. Such conversation is a new capability that traditional ‘inspective’ interpretability methods (opening the black-box) do not use. Having a language model that aims to teach and explain--beyond just knowing how to talk--is similar to a teacher whose goal is to teach well, understanding that their success will be measured by the student's comprehension. While agentic interpretability may trade off completeness for interactivity, making it less suitable for high-stakes safety situations with potentially deceptive models, it leverages a cooperative model to discover potentially superhuman concepts that can improve humans' mental model of machines. Agentic interpretability introduces challenges, particularly in evaluation, due to what is called ‘human-entangled-in-the-loop’ nature (humans responses are integral part of the algorithm), making the design and evaluation difficult. The authors discuss possible solutions and proxy goals. As LLMs approach human parity in many tasks, agentic interpretability's promise is to help humans learn the potentially superhuman concepts of the LLMs, rather than see us fall increasingly far from understanding them. <br> <br>

29. ***Uni of Toronto et al.'s otto-SR for automating systematic reviews:  <br>This study developed otto-SR, an end-to-end agentic workflow using LLMs to automate systematic reviews (SRs), which traditionally take over a year. Otto-SR outperformed traditional dual human workflows in SR screening and data extraction and reproduced/updated an entire issue of Cochrane reviews (12 work-years) in two days, demonstrating LLMs' potential for autonomous, scalable, and reliable evidence synthesis.*** <br> <br>
    Jun 13, Uni of Toronto, Harvard Medical School et al published a [paper](https://www.medrxiv.org/content/10.1101/2025.06.13.25329541v1.full.pdf) “Automation of Systematic Reviews with Large Language Models”. Systematic reviews (SRs) inform evidence-based decision making. Yet, they take over a year to complete, are prone to human error, and face challenges with reproducibility; limiting access to timely and reliable information. The study developed otto-SR, an end-to-end agentic workflow using large language models (LLMs) to support and automate the SR workflow from initial search to analysis. The work found that otto-SR outperformed traditional dual human workflows in SR screening (otto-SR: 96.7% sensitivity, 97.9% specificity; human: 81.7% sensitivity, 98.1% specificity) and data extraction (otto-SR: 93.1% accuracy; human: 79.7% accuracy). Using otto-SR, the study reproduced and updated an entire issue of Cochrane reviews (n=12) in two days, representing approximately 12 work-years of traditional systematic review work. Across Cochrane reviews, otto-SR incorrectly excluded a median of 0 studies (IQR 0 to 0.25), and found a median of 2.0 (IQR 1 to 6.5) eligible studies likely missed by the original authors. Meta-analyses revealed that otto-SR generated newly statistically significant conclusions in 2 reviews and negated significance in 1 review. These findings demonstrate that LLMs can autonomously conduct and update systematic reviews with superhuman performance, laying the foundation for automated, scalable, and reliable evidence synthesis. <br> <br>

31. ***Uni of Washington et al.'s Infini-gram mini for large-scale text search:  <br>This paper presents Infini-gram mini, an efficient and scalable system based on the FM-index for exact n-gram search in petabyte-level text corpora, creating indexes only 44% of the corpus size and significantly improving indexing speed and memory use. Indexing 46TB of Internet text, they used it to analyze benchmark contamination, finding significant contamination in core LM evaluation datasets.*** <br> <br>
    Jun 13, Uni of Washington, Allen Inst for AI and Stanford Uni published a [paper](https://arxiv.org/pdf/2506.12229) “Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index”. Language models are trained mainly on massive text data from the Internet, and it becomes increasingly important to understand this data source. Exact-match search engines enable searching in large text corpora -- counting string appearances and retrieving the enclosing documents -- yet the high storage overhead hinders their application on Internet-scale data. The study presents Infini-gram mini, an efficient and scalable system that can make petabyte-level text corpora searchable. Based on the FM-index data structure (Ferragina and Manzini, 2000), which simultaneously indexes and compresses text, the system creates indexes with size only 44% of the corpus. Infini-gram mini greatly improves upon the best existing implementation of FM-index in terms of indexing speed (18times) and memory use during both indexing (3.2times reduction) and querying (down to a negligible amount). The work indexes 46TB of Internet text in 50 days with a single 128-core CPU node (or 19 hours if using 75 such nodes), and shows one important use case of Infini-gram mini in a large-scale analysis of benchmark contamination. The study finds several core LM evaluation benchmarks to be heavily contaminated in Internet crawls (up to 40% in SQuAD), which could lead to overestimating the capabilities of language models if trained on such data. The authors host a benchmark contamination bulletin to share the contamination rate of many core and community-contributed benchmarks. The work also release a web interface and an API endpoint to serve general search queries on Infini-gram mini indexes. https://infini-gram-mini.io/ <br> <br>

33. ***Stanford Uni et al.'s principled framework for learning from language feedback:  <br>This research formalizes the Learning from Language Feedback (LLF) problem, introducing transfer eluder dimension as a complexity measure and showing rich language feedback can be exponentially faster than reward-based learning. They developed HELiX, a no-regret algorithm that provably solves LLF problems, outperforming repeated LLM prompting in empirical domains.*** <br> <br>
    Jun 12, Stanford Uni, UMCP, Netflix and Microsoft published a [paper](https://arxiv.org/pdf/2506.10341) “Provably Learning from Language Feedback”. Interactively learning from observation and language feedback is an increasingly studied area driven by the emergence of large language model (LLM) agents. While impressive empirical demonstrations have been shown, so far a principled framing of these decision problems remains lacking. The study formalizes the Learning from Language Feedback (LLF) problem, assert sufficient assumptions to enable learning despite latent rewards, and introduce transfer eluder dimension as a complexity measure to characterize the hardness of LLF problems. The study shows that transfer eluder dimension captures the intuition that information in the feedback changes the learning complexity of the LLF problem. The work demonstrates cases where learning from rich language feedback can be exponentially faster than learning from reward. The study develops a no-regret algorithm, called HELiX, that provably solves LLF problems through sequential interactions, with performance guarantees that scale with the transfer eluder dimension of the problem. Across several empirical domains, the study shows that HELiX performs well even when repeatedly prompting LLMs does not work reliably. The contributions mark a first step towards designing principled interactive learning algorithms from generic language feedback. <br> <br>

35. ***Stanford Uni's WORKBank for auditing AI automation potential:  <br>This study introduces an auditing framework to assess which occupational tasks workers desire AI agents to automate or augment, using audio-enhanced mini-interviews and a Human Agency Scale (HAS). They built the WORKBank database, mapping worker desires and AI expert capability assessments across 844 tasks, revealing diverse HAS profiles and highlighting mismatches and opportunities for aligning AI agent development with human preferences.*** <br> <br>
    Jun 11, Stanford Uni published a [paper](https://arxiv.org/pdf/2506.06576) “Future of Work with AI Agents Auditing Automation and Augmentation Potential across the U.S. Workforce”. The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the labor market, raising concerns about job displacement, diminished human agency, and overreliance on automation. Yet, people lack a systematic understanding of the evolving landscape. This study addresses this gap by introducing a novel auditing framework to assess which occupational tasks workers want AI agents to automate or augment, and how those desires align with the current technological capabilities. The framework features an audio-enhanced mini-interview to capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a shared language to quantify the preferred level of human involvement. Using this framework, the authors construct the WORKBank database, building on the U.S. Department of Labor's O*NET database, to capture preferences from 1,500 domain workers and capability assessments from AI experts across over 844 tasks spanning 104 occupations. Jointly considering the desire and technological capability divides tasks in WORKBank into four zones: Automation "Green Light" Zone, Automation "Red Light" Zone, R&D Opportunity Zone, Low Priority Zone. This highlights critical mismatches and opportunities for AI agent development. Moving beyond a simple automate-or-not dichotomy, our results reveal diverse HAS profiles across occupations, reflecting heterogeneous expectations for human involvement. Moreover, the study offers early signals of how AI agent integration may reshape the core human competencies, shifting from information-focused skills to interpersonal ones. These findings underscore the importance of aligning AI agent development with human desires and preparing workers for evolving workplace dynamics. <br> <br>

37. ***Reuters report on OpenAI's Google Cloud deal:  <br>OpenAI has partnered with Google Cloud to diversify its computing resources beyond Microsoft Azure, reflecting the immense infrastructure needs for AI development despite their rivalry. This deal, finalized in May 2025, is a significant win for Google Cloud's expanding TPU access but also presents resource allocation challenges for Google, underscoring the evolving landscape where collaboration and competition coexist in AI.*** <br> <br>
    Jun 11, Reuters published an [article](https://www.reuters.com/business/retail-consumer/openai-taps-google-unprecedented-cloud-deal-despite-ai-rivalry-sources-say-2025-06-10/) “OpenAI taps Google in unprecedented cloud deal despite AI rivalry, sources say”. OpenAI has entered a surprising partnership with Google Cloud to meet its growing computing demands, signaling a shift in the competitive dynamics of the AI industry. Despite being rivals—especially with OpenAI’s ChatGPT challenging Google’s dominance in search—the deal reflects the immense infrastructure needs of AI development. Finalized in May 2025, the agreement allows OpenAI to diversify its computing sources beyond Microsoft Azure, which had previously been its exclusive provider. This move follows OpenAI’s broader strategy to reduce dependency on Microsoft, including partnerships with SoftBank, Oracle, and CoreWeave, and the development of its own chips. For Google, the deal is a major win for its cloud business, which is expanding access to its tensor processing units (TPUs) previously reserved for internal use. This expansion has already attracted major clients like Apple and AI startups such as Anthropic. However, the partnership also presents challenges for Google, which must balance its internal AI development with external cloud services, especially as demand outpaces supply. Analysts view the collaboration as a strategic compromise, with both companies prioritizing infrastructure needs over rivalry. Alphabet’s stock rose following the announcement, while Microsoft’s dipped slightly, reflecting market reactions to the shifting alliances. The deal also adds complexity to Alphabet CEO Sundar Pichai’s task of allocating computing resources between Google’s enterprise and consumer segments. As OpenAI’s revenue surges and ChatGPT continues to grow in popularity, the partnership underscores the evolving landscape of AI, where collaboration and competition increasingly coexist. <br> <br>

39. ***MIT's study on cognitive debt from LLM essay writing assistance:  <br>This research explored the neural and behavioral effects of using LLMs for essay writing, finding that LLM users exhibited the weakest brain connectivity and cognitive engagement compared to search engine users or those using no tools. Over four months, LLM users consistently underperformed neurally, linguistically, and behaviorally, raising concerns about potential cognitive costs and long-term educational implications of LLM reliance.*** <br> <br>
    Jun 10, MIT published a [paper](https://arxiv.org/pdf/2506.08872) “Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task”. This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning. <br> <br>

41. ***Meta's AbstentionBench for evaluating LLM unanswerability:  <br>This study introduces AbstentionBench, a large-scale benchmark for evaluating LLM abstention capabilities across 20 diverse datasets with unanswerable questions (unknown answers, false premises, etc.). Evaluating 20 frontier LLMs revealed that abstention is an unsolved problem, surprisingly finding that reasoning fine-tuning degrades abstention abilities even in math/science domains, and while system prompts can help, fundamental uncertainty reasoning remains a challenge.*** <br> <br>
    Jun 10, Meta published a [paper](https://arxiv.org/pdf/2506.09038) “AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions”. For Large Language Models (LLMs) to be reliably deployed in both everyday and high-stakes domains, knowing when not to answer is equally critical as answering correctly. Real-world user queries, which can be underspecified, ill-posed, or fundamentally unanswerable, require LLMs to reason about uncertainty and selectively abstain -- i.e., refuse to answer definitively. However, abstention remains understudied, without a systematic evaluation framework for modern LLMs. This study introduces AbstentionBench, a large-scale benchmark for holistically evaluating abstention across 20 diverse datasets, including questions with unknown answers, underspecification, false premises, subjective interpretations, and outdated information. Evaluating 20 frontier LLMs reveals abstention is an unsolved problem, and one where scaling models is of little use. While recent reasoning LLMs have shown impressive results in complex problem solving, surprisingly, the study finds that reasoning fine-tuning degrades abstention (by 24% on average), even for math and science domains on which reasoning models are explicitly trained. The work finds that while a carefully crafted system prompt can boost abstention in practice, it does not resolve models' fundamental inability to reason about uncertainty. https://github.com/facebookresearch/AbstentionBench <br> <br>

43. ***Sakana AI's Text-to-LoRA for instant transformer adaptation:  <br>This paper introduces Text-to-LoRA (T2L), a hypernetwork model that adapts LLMs on the fly based solely on a natural language description of the target task, constructing LoRA adapters in a single inexpensive forward pass. T2L, trained on 9 pre-trained LoRA adapters, matched task-specific adapter performance, compressed hundreds of LoRAs, and showed zero-shot generalization to unseen tasks, democratizing foundation model specialization.*** <br> <br>
    Jun 9, Sakana AI published a [paper](https://arxiv.org/pdf/2506.06105) “Text-to-LoRA: Instant Transformer Adaption”. While Foundation Models provide a general tool for rapid content creation, they regularly require task-specific adaptation. Traditionally, this exercise involves careful curation of datasets and repeated fine-tuning of the underlying model. Fine-tuning techniques enable practitioners to adapt foundation models for many new applications but require expensive and lengthy training while being notably sensitive to hyperparameter choices. To overcome these limitations, the study introduces Text-to-LoRA (T2L), a model capable of adapting large language models (LLMs) on the fly solely based on a natural language description of the target task. T2L is a hypernetwork trained to construct LoRAs in a single inexpensive forward pass. After training T2L on a suite of 9 pre-trained LoRA adapters (GSM8K, Arc, etc.), the study shows that the ad-hoc reconstructed LoRA instances match the performance of task-specific adapters across the corresponding test sets. Furthermore, T2L can compress hundreds of LoRA instances and zero-shot generalize to entirely unseen tasks. This approach provides a significant step towards democratizing the specialization of foundation models and enables language-based adaptation with minimal compute requirements. https://github.com/SakanaAI/text-to-lora <br> <br>

45. ***Uni of Florida and Google's Many-Shot In-Context Fine-tuning (ManyICL):  <br>This study proposes Many-Shot In-Context Fine-tuning (ManyICL) for LLMs, extending ICL to a many-shot setting by treating every answer within the long context as a supervised training target, rather than just predicting the final answer. This approach significantly narrows the performance gap between ICL and dedicated task-specific fine-tuning across diverse tasks and mitigates catastrophic forgetting.*** <br> <br>
    Jun 6, Uni of Florida and Google published a [paper](https://arxiv.org/pdf/2506.11103) “You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model”. Large language models (LLMs) possess a remarkable ability to perform in-context learning (ICL), which enables them to handle multiple downstream tasks simultaneously without requiring task-specific fine-tuning. Recent studies have shown that even moderately sized LLMs, such as Mistral 7B, Gemma 7B and Llama-3 8B, can achieve ICL through few-shot in-context fine-tuning of all tasks at once. However, this approach still lags behind dedicated fine-tuning, where a separate model is trained for each individual task. The study proposes a novel approach, Many-Shot In-Context Fine-tuning (ManyICL), which significantly narrows this performance gap by extending the principles of ICL to a many-shot setting. To unlock the full potential of ManyICL and address the inherent inefficiency of processing long sequences with numerous in-context examples, the study proposes a novel training objective. Instead of solely predicting the final answer, the approach treats every answer within the context as a supervised training target. This effectively shifts the role of many-shot examples from prompts to targets for autoregressive learning. Through extensive experiments on diverse downstream tasks, including classification, summarization, question answering, natural language inference, and math, the study demonstrates that ManyICL substantially outperforms zero/few-shot fine-tuning and approaches the performance of dedicated fine-tuning. Furthermore, ManyICL significantly mitigates catastrophic forgetting issues observed in zero/few-shot fine-tuning.  <br> <br>

47. ***Google and ICL on LLM introspection:  <br>This paper explores whether the concept of introspection can be meaningfully applied to LLM self-reports, critiquing two examples. While an LLM's description of its "creative" writing process was deemed not valid introspection, an LLM correctly inferring its own temperature parameter was considered a minimal, albeit non-conscious, example of introspection.*** <br> <br>
    Jun 6, Google and ICL published a [paper](https://arxiv.org/pdf/2506.05068v2) “Does It Make Sense to Speak of Introspection in Large Language Models?” Large language models (LLMs) exhibit compelling linguistic behaviour, and sometimes offer self-reports, that is to say statements about their own nature, inner workings, or behaviour. In humans, such reports are often attributed to a faculty of introspection and are typically linked to consciousness. This raises the question of how to interpret self-reports produced by LLMs, given their increasing linguistic fluency and cognitive capabilities. To what extent (if any) can the concept of introspection be meaningfully applied to LLMs? Here, the study presents and critiques two examples of apparent introspective self-report from LLMs. In the first example, an LLM attempts to describe the process behind its own "creative" writing, and the authors argue this is not a valid example of introspection. In the second example, an LLM correctly infers the value of its own temperature parameter, and the work argues that this can be legitimately considered a minimal example of introspection, albeit one that is (presumably) not accompanied by conscious experience.

 <br> <br> <br>

***Jun 15, 2025***

1. ***Google's Spark Transformer:  <br>This paper introduces the Spark Transformer, an architecture achieving high activation sparsity (e.g., 8% FFN neuron activation, 256-token attention span) in both FFN and attention mechanisms while maintaining model quality and standard training. It uses top-k masking with a hardware-friendly "statistical top-k" algorithm and reallocates parameters for a low-cost predictor, resulting in a 2.5x FLOPs reduction and significant decoding speedups (up to 1.79x on CPU, 1.40x on GPU).*** <br> <br>
   Jun 13, Google published a [paper](https://arxiv.org/pdf/2506.06644) “Spark Transformer Reactivating Sparsity in FFN and Attention”. The discovery of the lazy neuron phenomenon in trained Transformers, where the vast majority of neurons in their feed-forward networks (FFN) are inactive for each token, has spurred tremendous interests in activation sparsity for enhancing large model efficiency. While notable progress has been made in translating such sparsity to wall-time benefits, modern Transformers have moved away from the ReLU activation function crucial to this phenomenon. Existing efforts on re-introducing activation sparsity often degrade model quality, increase parameter count, complicate or slow down training. Sparse attention, the application of sparse activation to the attention mechanism, often faces similar challenges. This paper introduces the Spark Transformer, a novel architecture that achieves a high level of activation sparsity in both FFN and the attention mechanism while maintaining model quality, parameter count, and standard training procedures. The method realizes sparsity via top-k masking for explicit control over sparsity level. Crucially, the work introduces statistical top-k, a hardware-accelerator-friendly, linear-time approximate algorithm that avoids costly sorting and mitigates significant training slowdown from standard top-k operators. Furthermore, Spark Transformer reallocates existing FFN parameters and attention key embeddings to form a low-cost predictor for identifying activated entries. This design not only mitigates quality loss from enforced sparsity, but also enhances wall-time benefit. Pretrained with the Gemma-2 recipe, Spark Transformer demonstrates competitive performance on standard benchmarks while exhibiting significant sparsity: only 8% of FFN neurons are activated, and each token attends to a maximum of 256 tokens. This sparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time speedups of up to 1.79x on CPU and 1.40x on GPU. <br> <br>

3. ***UC Berkeley's study on out-of-context reasoning:  <br>This research argues that both generalization and hallucination in fine-tuned LLMs stem from out-of-context reasoning (OCR)—deducing implications by associating concepts, even non-causally. Experiments confirm OCR drives both behaviors, and a theoretical analysis using a synthetic factual recall task shows that a one-layer attention-only transformer with factorized output/value matrices learns OCR due to gradient descent's bias towards minimizing the nuclear norm of the combined matrix.*** <br> <br>
   Jun 12, UC Berkeley published a [paper](https://arxiv.org/pdf/2506.10887) “Generalization or Hallucination Understanding Out-of-Context Reasoning in Transformers”. Large language models (LLMs) can acquire new knowledge through fine-tuning, but this process exhibits a puzzling duality: models can generalize remarkably from new facts, yet are also prone to hallucinating incorrect information. However, the reasons for this phenomenon remain poorly understood. This research argues that both behaviors stem from a single mechanism known as out-of-context reasoning (OCR): the ability to deduce implications by associating concepts, even those without a causal link. Experiments across five prominent LLMs confirm that OCR indeed drives both generalization and hallucination, depending on whether the associated concepts are causally related. To build a rigorous theoretical understanding of this phenomenon, the study then formalizes OCR as a synthetic factual recall task. The work empirically shows that a one-layer single-head attention-only transformer with factorized output and value matrices can learn to solve this task, while a model with combined weights cannot, highlighting the crucial role of matrix factorization. The theoretical analysis shows that the OCR capability can be attributed to the implicit bias of gradient descent, which favors solutions that minimize the nuclear norm of the combined output-value matrix. This mathematical structure explains why the model learns to associate facts and implications with high sample efficiency, regardless of whether the correlation is causal or merely spurious. Ultimately, the work provides a theoretical foundation for understanding the OCR phenomenon, offering a new lens for analyzing and mitigating undesirable behaviors from knowledge injection. <br> <br>

5. ***Mistral's Magistral reasoning model:  <br>This report introduces Magistral, Mistral's first reasoning model developed using its own scalable reinforcement learning (RL) pipeline from the ground up, relying solely on Mistral's models and infrastructure. The work demonstrates that pure RL training can explore LLM limits, force reasoning language, and that RL on text alone largely maintains initial capabilities, including multimodal understanding and instruction following, presenting Magistral Medium and open-sourcing Magistral Small.*** <br> <br>
   Jun 12, Mistral published a [paper](https://arxiv.org/abs/2506.10910) “Magistral”. The report introduces Magistral, Mistral's first reasoning model and its own scalable reinforcement learning (RL) pipeline. Instead of relying on existing implementations and RL traces distilled from prior models, the work follows a ground up approach, relying solely on Mistral’s own models and infrastructure. Notably, the work demonstrates a stack that enabled to explore the limits of pure RL training of LLMs, present a simple method to force the reasoning language of the model, and show that RL on text data alone maintains most of the initial checkpoint's capabilities. The work finds that RL on text maintains or improves multimodal understanding, instruction following and function calling. The work presents Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and the authors open-source Magistral Small (Apache 2.0) which further includes cold-start data from Magistral Medium. https://huggingface.co/mistralai/Magistral-Small-2506 <br> <br>

7. ***MIT's Self-Adapting Language Models (SEAL):  <br>This study introduces SEAL, a framework enabling LLMs to self-adapt their weights by generating their own fine-tuning data and update directives ("self-edits") in response to new inputs. These self-edits, refined through a reinforcement learning loop based on downstream performance, lead to persistent weight updates, showing promise for knowledge incorporation and few-shot generalization without separate adaptation modules.*** <br> <br>
   Jun 12, MIT published a [paper](https://arxiv.org/pdf/2506.10943) “Self-Adapting Language Models”. Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. The study introduces Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, the study uses a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. https://jyopari.github.io/posts/seal <br> <br>

9. ***CMU and Nvidia's Multiverse generative model:  <br>This paper introduces Multiverse, a generative model enabling natively parallel generation by internalizing a MapReduce paradigm (Map, Process, Reduce stages) for adaptive task decomposition, parallel subtask execution, and lossless result synthesis. After a short fine-tuning with 1K examples, the Multiverse-32B model achieves performance on par with leading AR-LLMs of the same scale on reasoning benchmarks, exhibiting superior scaling and up to 2x speedup.*** <br> <br>
    Jun 11, CMU and Nvidia published a [paper](https://arxiv.org/pdf/2506.09991) “Multiverse Your Language Models Secretly Decide How to Parallelize and Merge Generation”. Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit parallelism in sequential generation. Inspired by this, the study introduces Multiverse, a new generative model that enables natively parallel generation. Multiverse internalizes a MapReduce paradigm, generating automatically through three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process stage for parallel subtask execution, and (iii) a Reduce stage for lossless result synthesis. Next, the study builds a real-world Multiverse reasoning model with co-design of data, algorithm, and system, enabling rapid and seamless transfer from frontier AR-LLMs. Starting from sequential reasoning chains, the study creates Multiverse 1K by converting them into structured training data using an automated LLM-assisted pipeline, avoiding costly human annotations. Algorithmically, the study designs Multiverse Attention to separate parallel reasoning steps while keeping compatibility with causal attention for efficient training. Systematically, the work implements Multiverse Engine to enable parallel inference. It features a dedicated scheduler that dynamically switches between sequential and parallel generation, triggered directly by the model. After a 3-hour fine-tuning with 1K examples, the Multiverse-32B stands as the only open-sourced non-AR model achieving performance on par with leading AR-LLMs of the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively. Moreover, the budget control experiments show that Multiverse-32B exhibits superior scaling, outperforming AR-LLMs by 1.87% on average using the same context length. Such scaling further leads to practical efficiency gain, achieving up to 2x speedup across varying batch sizes. https://github.com/Multiverse4FM/Multiverse. <br> <br>

11. ***Princeton and UT Austin's Query-Focused Retrieval Heads (QRHEAD):  <br>This study introduces QRHEAD, an improved set of attention heads identified by aggregating attention scores with respect to an input query, which enhance long-context retrieval. They also propose QR-RETRIEVER, an efficient retriever using QRHEAD's accumulated attention mass, which yields over 10% performance gains on long-context reasoning tasks and strong zero-shot re-ranking performance on BEIR.*** <br> <br>
    Jun 11, Princeton Uni and Uni of Taxes Austin published a [paper](https://arxiv.org/pdf/2506.09944) “Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking”. Recent work has identified retrieval heads (Wu et al., 2025b), a subset of attention heads responsible for retrieving salient information in long-context language models (LMs), as measured by their copy-paste behavior in Needle-in-a-Haystack tasks. This study introduces QRHEAD (Query-Focused Retrieval Head), an improved set of attention heads that enhance retrieval from long context. The study identifies QRHEAD by aggregating attention scores with respect to the input query, using a handful of examples from real-world tasks (e.g., long-context QA). The study further introduces QR- RETRIEVER, an efficient and effective retriever that uses the accumulated attention mass of QRHEAD as retrieval scores. The work uses QR- RETRIEVER for long-context reasoning by selecting the most relevant parts with the highest retrieval scores. On multi-hop reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains over full context and outperforms strong dense retrievers. The study also evaluates QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves strong zero-shot performance, outperforming other LLM-based re-rankers such as RankGPT. Further analysis shows that both the querycontext attention scoring and task selection are crucial for identifying QRHEAD with strong downstream utility. Overall, the work contributes a general-purpose retriever and offers interpretability insights into the long-context capabilities of LMs. https://github.com/princeton-pli/QRHead <br> <br>

13. ***Meta and Mila's V-JEPA 2 for self-supervised video understanding:  <br>This paper presents V-JEPA 2, a self-supervised joint-embedding-predictive architecture pre-trained on over 1 million hours of internet video, achieving strong performance on motion understanding and action anticipation, and SOTA on video QA tasks when aligned with an LLM. By post-training an action-conditioned world model (V-JEPA 2-AC) on minimal robot data, they demonstrated zero-shot robotic planning for picking and placing objects.*** <br> <br>
    Jun 11, Meta and Mila published a [paper](https://arxiv.org/pdf/2506.09985) “V-JEPA 2 Self-Supervised Video Models Enable Understanding, Prediction and Planning”. A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. The study first pre-trains an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, the study demonstrates state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, the work shows how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. The study deploys V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world. <br> <br>

15. ***Anthropic et al.'s unsupervised elicitation of LMs:  <br>This research introduces Internal Coherence Maximization (ICM), an unsupervised algorithm to fine-tune pretrained language models on their own generated labels without external supervision, addressing the difficulty of obtaining high-quality human supervision for superhuman models. ICM matches or outperforms golden/crowdsourced supervision on tasks like GSM8k-verification and TruthfulQA, and successfully trained a Claude 3.5 Haiku-based assistant that outperformed its human-supervised counterpart.*** <br> <br>
    Jun 11, Anthropic et al published a [paper](https://arxiv.org/pdf/2506.10139) “Unsupervised Elicitation of Language Models”. To steer pretrained language models for downstream tasks, today's post-training paradigm relies on humans to specify desired behaviors. However, for models with superhuman capabilities, it is difficult or impossible to get high-quality human supervision. To address this challenge, the study introduces a new unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune pretrained language models on their own generated labels, without external supervision. On GSM8k-verification, TruthfulQA, and Alpaca reward modeling tasks, the method matches the performance of training on golden supervision and outperforms training on crowdsourced human supervision. On tasks where LMs' capabilities are strongly superhuman, the method can elicit those capabilities significantly better than training on human labels. Finally, the study shows that the method can improve the training of frontier LMs: the study uses the method to train an unsupervised reward model and uses reinforcement learning to train a Claude 3.5 Haiku-based assistant. Both the reward model and the assistant outperform their human-supervised counterparts. <br> <br>

17. ***Rice Uni et al.'s study on LLM reasoning reproducibility:  <br>This paper demonstrates that LLM performance reproducibility is fragile, as system configuration changes (batch size, GPU count/version) can cause significant variations in generated responses and accuracy, especially in reasoning models due to cascading rounding differences under limited numerical precision (e.g., up to 9% accuracy variation for DeepSeek-R1-Distill-Qwen-7B). They propose LayerCast, an inference pipeline storing weights in 16-bit but computing in FP32, to balance efficiency and stability.*** <br> <br>
    Jun 11, Rice Uni, Uni of Minnesota Twin Cities and Adobe published a [paper](https://arxiv.org/pdf/2506.09501) “Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning”. Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. The study demonstrates that the reproducibility of LLM performance is fragile: changing system configuration such as evaluation batch size, GPU count, and GPU version can introduce significant difference in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. The study traces the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, the study quantifies when and how model outputs diverge. The analysis reveals that floating-point precision -- while critical for reproducibility -- is often neglected in evaluation practices. Inspired by this, the study develops a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. https://github.com/nanomaoli/llm_reproducibility. <br> <br>

19. ***MIT's Dispersive Loss for image generation:  <br>This study proposes Dispersive Loss, a simple plug-and-play regularizer for diffusion-based generative models that encourages internal representations to disperse in the hidden space, analogous to contrastive learning but without requiring positive sample pairs. This minimalist approach, requiring no pre-training or extra parameters, showed consistent improvements over strong baselines on ImageNet, aiming to bridge generative modeling and representation learning.*** <br> <br>
    Jun 10, MIT published a [paper](https://arxiv.org/abs/2506.09027) “Diffuse and Disperse Image Generation with Representation Regularization”. The development of diffusion-based generative models over the past decade has largely proceeded independently of progress in representation learning. These diffusion models typically rely on regression-based objectives and generally lack explicit regularization. This study proposes Dispersive Loss, a simple plug-and-play regularizer that effectively improves diffusion-based generative models. The loss function encourages internal representations to disperse in the hidden space, analogous to contrastive self-supervised learning, with the key distinction that it requires no positive sample pairs and therefore does not interfere with the sampling process used for regression. Compared to the recent method of representation alignment (REPA), the approach is self-contained and minimalist, requiring no pre-training, no additional parameters, and no external data. The study evaluates Dispersive Loss on the ImageNet dataset across a range of models and report consistent improvements over widely used and strong baselines. The authors hope the our work will help bridge the gap between generative modeling and representation learning. <br> <br>

21. ***Google and Bar-Ilan Uni on detecting conflicting RAG sources:  <br>This research addresses how LLMs handle conflicting information in Retrieval Augmented Generation (RAG), proposing a taxonomy of knowledge conflict types and introducing CONFLICTS, a benchmark with expert annotations for tracking progress. Experiments show LLMs struggle to resolve conflicts, though prompting for explicit reasoning about conflicts improves response quality, highlighting areas for future research.*** <br> <br>
    Jun 10, Google and Bar-Ilan Uni published a [paper](https://arxiv.org/pdf/2506.08500) “DRAGged into Conflicts Detecting and Addressing Conflicting Sources in Search-Augmented LLMs”. Retrieval Augmented Generation (RAG) is a commonly used approach for enhancing large language models (LLMs) with relevant and up-to-date information. However, the retrieved sources can often contain conflicting information and it remains unclear how models should address such discrepancies. This study first proposes a novel taxonomy of knowledge conflict types in RAG, along with the desired model behavior for each type. The study then introduces CONFLICTS, a high-quality benchmark with expert annotations of conflict types in a realistic RAG setting. CONFLICTS is the first benchmark that enables tracking progress on how models address a wide range of knowledge conflicts. The work conducts extensive experiments on this benchmark, showing that LLMs often struggle to appropriately resolve conflicts between sources. While prompting LLMs to explicitly reason about the potential conflict in the retrieved documents significantly improves the quality and appropriateness of their responses, substantial room for improvement in future research remains. <br> <br>

23. ***MIT's Ambient Diffusion Omni for training with bad data:  <br>This study presents Ambient Diffusion Omni, a framework enabling diffusion models to extract signals from all available images, including low-quality, synthetic, and out-of-distribution data typically discarded. By exploiting natural image properties (spectral decay, locality) and leveraging how noise dampens distribution skew, the framework achieved SOTA ImageNet FID and significantly improved text-to-image generation quality and diversity.*** <br> <br>
    Jun 10, MIT published a [paper](https://www.arxiv.org/pdf/2506.10038) “Ambient Diffusion Omni Training Good Models with Bad Data”. The study shows how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. The work shows that there is immense value in the lower-quality images that are often discarded. The study presents Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from all available images during training. The framework exploits two properties of natural images -- spectral power law decay and locality. The study first validates the framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur; then uses the framework to achieve state-of-the-art ImageNet FID, and shows significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution actually observed. The study provides rigorous theoretical justification for the approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times. <br> <br>

25. ***CMU, UIUC et al.'s agents that reason by scaling test-time interaction:  <br>This paper proposes scaling test-time interaction as a new dimension for LLM agents, increasing their interaction horizon to enable behaviors like exploration and dynamic re-planning within a single rollout, unlike current methods focused on "thinking" before acting. They introduce TTI, a curriculum-based online RL approach that adaptively adjusts rollout lengths, producing SOTA open-source web agents on WebVoyager and WebArena with a Gemma 3 12B model.*** <br> <br>
    Jun 10, CMU, UIUC et al published a [paper](https://arxiv.org/pdf/2506.07976) “Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction”. The current paradigm of test-time scaling relies on generating long reasoning traces ("thinking" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. The study proposes to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent's interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, the work studies the domain of web agents. The authors first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, the work introduces TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. The study further shows that TTI enables agents to balance exploration and exploitation adaptively. Results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents. <br> <br>

27. ***Harvard Uni's Institutional Books 1.0 dataset:  <br>This technical report introduces Institutional Books 1.0, a 242 billion token dataset of 983,004 public domain books (in over 250 languages) from Harvard Library's Google Books digitization project, refined for accuracy and usability. The extensively documented dataset, including OCR text and metadata, aims to address the scarcity of high-quality, publicly available training data with clear provenance for LLMs.*** <br> <br>
    Jun 10, Harvard Uni published a [tech report](https://arxiv.org/pdf/2506.08300) “Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability”. Large language models (LLMs) use data to learn about the world in order to produce meaningful correlations and predictions. As such, the nature, scale, quality, and diversity of the datasets used to train these models, or to support their work at inference time, have a direct impact on their quality. The rapid development and adoption of LLMs of varying quality has brought into focus the scarcity of publicly available, high-quality training data and revealed an urgent need to ground the stewardship of these datasets in sustainable practices with clear provenance chains. To that end, this technical report introduces Institutional Books 1.0, a large collection of public domain books originally digitized through Harvard Library's participation in the Google Books project, beginning in 2006. Working with Harvard Library, the study extracted, analyzed, and processed these volumes into an extensively-documented dataset of historic texts. This analysis covers the entirety of Harvard Library's collection scanned as part of that project, originally spanning 1,075,899 volumes written in over 250 different languages for a total of approximately 250 billion tokens. As part of this initial release, the OCR-extracted text (original and post-processed) as well as the metadata (bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens, identified as being in the public domain have been made available. This report describes this project's goals and methods as well as the results of the analyses we performed, all in service of making this historical collection more accessible and easier for humans and machines alike to filter, read and use. https://huggingface.co/datasets/instdin/institutional-books-1.0 <br> <br>

29. ***Anthropic's comment on "The Illusion of Thinking":  <br>This paper critiques Shojaee et al.'s (2025) findings of "accuracy collapse" in Large Reasoning Models (LRMs) on planning puzzles, arguing the results stem from experimental design limitations. Anthropic points to issues like exceeding token limits in Tower of Hanoi, evaluation frameworks misclassifying capabilities, and mathematically impossible River Crossing benchmarks, stating that controlling for these artifacts shows high LRM accuracy on previously reported failures.*** <br> <br>
    Jun 10, Anthropic published a [paper](https://arxiv.org/pdf/2506.09250) “Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity”. Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit "accuracy collapse" on planning puzzles beyond certain complexity thresholds. The study demonstrates that their findings primarily reflect experimental design limitations rather than fundamental reasoning failures. The analysis reveals three critical issues: (1) Tower of Hanoi experiments systematically exceed model output token limits at reported failure points, with models explicitly acknowledging these constraints in their outputs; (2) The authors' automated evaluation framework fails to distinguish between reasoning failures and practical constraints, leading to misclassification of model capabilities; (3) Most concerningly, their River Crossing benchmarks include mathematically impossible instances for N > 5 due to insufficient boat capacity, yet models are scored as failures for not solving these unsolvable problems. When the study controls for these experimental artifacts, by requesting generating functions instead of exhaustive move lists, preliminary experiments across multiple models indicate high accuracy on Tower of Hanoi instances previously reported as complete failures. These findings highlight the importance of careful experimental design when evaluating AI reasoning capabilities. <br> <br>

31. ***Princeton et al.'s KITE framework for human-AI knowledge transfer:  <br>This research introduces Knowledge Integration and Transfer Evaluation (KITE), a framework to measure AI models' ability to communicate reasoning effectively to humans. A large-scale human study (N=118) where humans ideated with AI then independently implemented solutions revealed that while model benchmark performance correlates with collaborative outcomes, the relationship is inconsistent, indicating knowledge transfer requires dedicated optimization.*** <br> <br>
    Jun 9, Princeton Language & Intelligence, Stanford Uni and OpenAI published a [paper](https://arxiv.org/pdf/2506.05579) “When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration”. Recent advancements in AI reasoning have driven substantial improvements across diverse tasks. A critical open question is whether these improvements also yields better knowledge transfer: the ability of models to communicate reasoning in ways humans can understand, apply, and learn from. To investigate this, the study introduces Knowledge Integration and Transfer Evaluation (KITE), a conceptual and experimental framework for Human-AI knowledge transfer capabilities and conduct the first large-scale human study (N=118) explicitly designed to measure it. In the two-phase setup, humans first ideate with an AI on problem-solving strategies, then independently implement solutions, isolating model explanations' influence on human understanding. The findings reveal that although model benchmark performance correlates with collaborative outcomes, this relationship is notably inconsistent, featuring significant outliers, indicating that knowledge transfer requires dedicated optimization. The analysis identifies behavioral and strategic factors mediating successful knowledge transfer. https://kite-live.vercel.app/ <br> <br>

33. ***Google's Contextually Guided Transformers (CGT):  <br>This study proposes Contextually Guided Transformers (CGT), a modification to the Transformer architecture that eliminates the need for explicit prompts by learning to encode context into the model's weights via a contextual summary maintained at each sequence position. This allows the model to self-specialize on the fly based on preceding context, demonstrating effectiveness on synthetic in-context learning and language modeling tasks.*** <br> <br>
    Jun 6, Google published a [paper](https://arxiv.org/pdf/2506.05672) “Contextually Guided Transformers via Low-Rank Adaptation”. Large Language Models (LLMs) based on Transformers excel at text processing, but their reliance on prompts for specialized behavior introduces computational overhead. The study proposes a modification to a Transformer architecture that eliminates the need for explicit prompts by learning to encode context into the model's weights. Our Contextually Guided Transformer (CGT) model maintains a contextual summary at each sequence position, allowing it to update the weights on the fly based on the preceding context. This approach enables the model to self-specialize, effectively creating a tailored model for processing information following a given prefix. The study demonstrates the effectiveness of the method on synthetic in-context learning tasks and language modeling benchmarks. Furthermore, the study introduces techniques for enhancing the interpretability of the learned contextual representations, drawing connections to Variational Autoencoders and promoting smoother, more consistent context encoding. This work offers a novel direction for efficient and adaptable language modeling by integrating context directly into the model's architecture. <br> <br>

35. ***Stanford Uni's Rel-LLM for relational learning:  <br>This paper introduces Rel-LLM, a novel architecture that enables LLMs to effectively process and reason over structured relational data by using a GNN-based encoder to generate structured relational prompts within a RAG framework. Unlike text serialization, Rel-LLM preserves inherent relational structures, extracting local subgraphs to build feature representations that are transformed into structured prompts, outperforming existing methods on key RDL tasks.*** <br> <br>
    Jun 6, Stanford Uni published a [paper](https://arxiv.org/pdf/2506.05725) “Large Language Models are Good Relational Learners”. Large language models (LLMs) have demonstrated remarkable capabilities across various domains, yet their application to relational deep learning (RDL) remains underexplored. Existing approaches adapt LLMs by traversing relational links between entities in a database and converting the structured data into flat text documents. Still, this text-based serialization disregards critical relational structures, introduces redundancy, and often exceeds standard LLM context lengths. The study introduces Rel-LLM, a novel architecture that utilizes a graph neural network (GNN)- based encoder to generate structured relational prompts for LLMs within a retrieval-augmented generation (RAG) framework. Unlike traditional text-based serialization approaches, the method preserves the inherent relational structure of databases while enabling LLMs to effectively process and reason over complex entity relationships. Specifically, the GNN encoder extracts a local subgraph around an entity to build feature representations that contain relevant entity relationships and temporal dependencies. These representations are transformed into structured prompts using a denormalization process, effectively allowing the LLM to reason over relational structures. Through extensive experiments, the work demonstrates that Rel-LLM outperforms existing methods on key RDL tasks, offering a scalable and efficient approach to integrating LLMs with structured data sources. https://github.com/smiles724/Rel-LLM <br> <br>

37. ***Johns Hopkins Uni on knowledge conflict in LLMs:  <br>This study proposes a diagnostic framework to evaluate LLM behavior under context-memory conflict, where contextual input diverges from parametric knowledge. Findings reveal that conflict minimally impacts tasks not requiring knowledge utilization, performance is higher with aligned knowledge, models struggle to suppress internal knowledge even when instructed, and rationales explaining conflict increase context reliance, raising concerns for model evaluation and deployment.*** <br> <br>
    Jun 6, Johns Hopkins Uni published a [paper](https://arxiv.org/pdf/2506.06485) “What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models”. Large language models frequently rely on both contextual input and parametric knowledge to perform tasks. However, these sources can come into conflict, especially when retrieved documents contradict the model's parametric knowledge. The study proposes a diagnostic framework to systematically evaluate LLM behavior under context-memory conflict, where the contextual information diverges from their parametric beliefs. The study constructs diagnostic data that elicit these conflicts and analyze model performance across multiple task types. The findings reveal that (1) knowledge conflict has minimal impact on tasks that do not require knowledge utilization, (2) model performance is consistently higher when contextual and parametric knowledge are aligned, (3) models are unable to fully suppress their internal knowledge even when instructed, and (4) providing rationales that explain the conflict increases reliance on contexts. These insights raise concerns about the validity of model-based evaluation and underscore the need to account for knowledge conflict in the deployment of LLMs. <br> <br>

39. ***Tech Uni of Denmark et al. on distributional and representational similarity:  <br>This research explores the relationship between distributional closeness and representational similarity in deep neural networks from an identifiability theory perspective. They prove that a small KL divergence between model distributions does not guarantee similar representations, a phenomenon observed empirically, but define a distributional distance for which closeness does imply representational similarity, finding wider networks learn closer distributions and more similar representations.*** <br> <br>
    Jun 4, Tech Uni of Denmark, Uni of Trento Italy et al published a [paper](https://arxiv.org/pdf/2506.03784) “When Does Closeness in Distribution Imply Representational Similarity? An Identifiability Perspective”. When and why representations learned by different deep neural networks are similar is an active research topic. The study chooses to address these questions from the perspective of identifiability theory, which suggests that a measure of representational similarity should be invariant to transformations that leave the model distribution unchanged. Focusing on a model family which includes several popular pre-training approaches, e.g., autoregressive language models, the study explores when models which generate distributions that are close have similar representations. The work proves that a small Kullback-Leibler divergence between the model distributions does not guarantee that the corresponding representations are similar. This has the important corollary that models arbitrarily close to maximizing the likelihood can still learn dissimilar representations, a phenomenon mirrored in the empirical observations on models trained on CIFAR-10. The study then defines a distributional distance for which closeness implies representational similarity, and in synthetic experiments, the study finds that wider networks learn distributions which are closer with respect to the distance and have more similar representations. The results establish a link between closeness in distribution and representational similarity. <br> <br>

41. ***UC Berkeley et al. predicting AI research outcomes with LMs:  <br>This study built the first benchmark for predicting empirical AI research outcomes (which of two ideas will perform better), developing a system combining a fine-tuned GPT-4.1 with a paper retrieval agent. This system significantly outperformed human experts in the NLP domain (64.4% vs. 48.9%) and achieved 77% accuracy on the full test set, demonstrating potential as a reward model for improving AI idea generation.*** <br> <br>
    Jun 1, UC Berkeley, Stanford Uni, NYU and George Washington Uni published a [paper](https://arxiv.org/pdf/2506.00794) “Predicting Empirical AI Research Outcomes with Language Models”. Many promising-looking ideas in AI research fail to deliver, but their validation takes substantial human labor and compute. Predicting an idea's chance of success is thus crucial for accelerating empirical AI research, a skill that even expert researchers can only acquire through substantial experience. The study builds the first benchmark for this task and compare LMs with human experts. Concretely, given two research ideas (e.g., two jailbreaking methods), the study aims to predict which will perform better on a set of benchmarks. The authors scrape ideas and experimental results from conference papers, yielding 1,585 human-verified idea pairs published after the base model's cut-off date for testing, and 6,000 pairs for training. The study then develops a system that combines a fine-tuned GPT-4.1 with a paper retrieval agent, and the authors recruit 25 human experts to compare with. In the NLP domain, the system beats human experts by a large margin (64.4% v.s. 48.9%). On the full test set, the system achieves 77% accuracy, while off-the-shelf frontier LMs like o3 perform no better than random guessing, even with the same retrieval augmentation. The study verifies that the system does not exploit superficial features like idea complexity through extensive human-written and LM-designed robustness tests. Finally, the work evaluates the system on unpublished novel ideas, including ideas generated by an AI ideation agent. The system achieves 63.6% accuracy, demonstrating its potential as a reward model for improving idea generation models. Altogether, the results outline a promising new direction for LMs to accelerate empirical AI research.

 <br> <br> <br>

***Jun 8, 2025***


1. ***Apple's study on LRM reasoning limitations:  <br>This research systematically investigated the reasoning capabilities and limitations of Large Reasoning Models (LRMs) using controllable puzzle environments, revealing that frontier LRMs face a complete accuracy collapse beyond certain complexities and exhibit a counterintuitive scaling limit where reasoning effort declines despite adequate token budgets. The study identified performance regimes where standard LLMs can outperform LRMs at low complexity and both collapse at high complexity, noting LRMs' limitations in exact computation and inconsistent reasoning.*** <br> <br>
   Jun 6, Apple published a [paper](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf) “The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity”. Recent generations of frontier language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established mathematical and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from data contamination and does not provide insights into the reasoning traces’ structure and quality. This study systematically investigates these gaps with the help of controllable puzzle environments that allow precise manipulation of compositional complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs “think”. Through extensive experimentation across diverse puzzles, the work shows that frontier LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having an adequate token budget. By comparing LRMs with their standard LLM counterparts under equivalent inference compute, the study identifies three performance regimes: (1) low- complexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity tasks where additional thinking in LRMs demonstrates advantage, and (3) high-complexity tasks where both models experience complete collapse. The study found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across puzzles. The work also investigates the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models’ computational behavior, shedding light on their strengths, limitations, and ultimately raising crucial questions about their true reasoning capabilities. <br> <br>

3. ***Microsoft et al.'s ReSA for efficient long-sequence generation:  <br>This paper proposes Rectified Sparse Attention (ReSA), a method combining block-sparse attention with periodic dense rectification to address KV cache misalignment and error accumulation in long-sequence generation. By refreshing the KV cache at fixed intervals, ReSA achieves near-lossless generation quality with significant efficiency improvements, delivering up to 2.42x end-to-end speedup at 256K sequence length.*** <br> <br>
   Jun 5, Microsoft, Tsinghua Uni and The Uni of HK published a [paper](https://arxiv.org/pdf/2506.04108) “On-Policy RL with Optimal Reward Baseline”. Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. This study proposes Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42times end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM. <br> <br>

5. ***MIT et al.'s Log-Linear Attention:  <br>This paper develops log-linear attention, an attention mechanism that balances the efficiency of linear attention with the expressiveness of softmax attention by replacing a fixed-size hidden state with a logarithmically growing set of hidden states. This approach admits a matmul-rich parallel form with log-linear compute cost and, when applied to Mamba-2 and Gated DeltaNet, shows strong performance compared to their linear-time variants.*** <br> <br>
   Jun 5, MIT, Princeton Uni, CMU and GenBio AI published a [paper](https://arxiv.org/pdf/2506.04761) “Log-Linear Attention”. The attention mechanism in Transformers is an important primitive for accurate and scalable sequence modeling. Its quadratic-compute and linear-memory complexity however remain significant bottlenecks. Linear attention and state-space models enable linear-time, constant-memory sequence modeling and can moreover be trained efficiently through matmul-rich parallelization across sequence length. However, at their core these models are still RNNs, and thus their use of a fixed-size hidden state to model the context is a fundamental limitation. This paper develops log-linear attention, an attention mechanism that balances linear attention's efficiency and the expressiveness of softmax attention. Log-linear attention replaces the fixed-size hidden state with a logarithmically growing set of hidden states. The work shows that with a particular growth function, log-linear attention admits a similarly matmul-rich parallel form whose compute cost is log-linear in sequence length. Log-linear attention is a general framework and can be applied on top of existing linear attention variants. As case studies, the work instantiates log-linear variants of two recent architectures -- Mamba-2 and Gated DeltaNet -- and find they perform well compared to their linear-time variants. https://github.com/HanGuo97/log-linear-attention <br> <br>

7. ***CMU's Kinetics rethinks test-time scaling laws:  <br>This research re-evaluates test-time scaling laws from an efficiency perspective, incorporating memory access costs alongside computation, and introduces the Kinetics Scaling Law, which suggests test-time compute is more effective on models above a certain size threshold. Motivated by attention becoming the dominant cost factor in TTS, the study proposes and demonstrates that sparse attention models consistently outperform dense counterparts, achieving significant accuracy gains by enabling longer generations and more parallel samples.*** <br> <br>
   Jun 5, CMU published a [paper](https://arxiv.org/pdf/2506.05333) “Kinetics: Rethinking Test-Time Scaling Laws”. The authors rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-N, long CoTs). The holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, the study proposes a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, the study shows that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential for realizing the full potential of test-time scaling because, unlike training, where parameter scaling saturates, test-time accuracy continues to improve through increased generation. https://github.com/Infini-AI-Lab/Kinetics <br> <br>

9. ***Stanford et al.'s OpenThoughts dataset recipes:  <br>The OpenThoughts project aims to create open-source datasets for training reasoning models, addressing the reliance on proprietary data by state-of-the-art models. Through systematic investigation and over 1,000 controlled experiments, they developed OpenThoughts3, which, when used to train the OpenThinker3-7B model with QwQ-32B as a teacher, achieved state-of-the-art results (53% on AIME 2025, 51% on LiveCodeBench, 54% on GPQA Diamond).*** <br> <br>
    Jun 5, Stanford Uni et al published a [paper](https://arxiv.org/pdf/2506.04178) “OpenThoughts: Data Recipes for Reasoning Models”. Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training reasoning models. After initial explorations, the OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model trained on public reasoning data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as AIME and LiveCodeBench. The study then improves the dataset further by systematically investigating each step of the data generation pipeline with 1,000+ controlled experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields the OpenThinker3-7B model, which achieves state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Diamond. https://openthoughts.ai. <br> <br>

11. ***Uni of Toronto et al.'s Common Pile dataset:  <br>To address ethical and IP concerns with unlicensed text used for LLM training, researchers collected, curated, and released the Common Pile v0.1, an 8TB dataset of openly licensed text spanning diverse domains. They validated its utility by training two 7B parameter LLMs (Comma v0.1-1T and Comma v0.1-2T) which achieved competitive performance to models trained on unlicensed text, also releasing the creation code and model checkpoints.*** <br> <br>
    Jun 5, Uni of Toronto et al published a [paper](https://arxiv.org/pdf/2506.05209) “The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text”. Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due to possible intellectual property infringement and ethical concerns. Training LLMs on openly licensed text presents a first step towards addressing these issues, but prior data collection efforts have yielded datasets too small or low-quality to produce performant LLMs. To address this gap, the study collects, curates, and releases the Common Pile v0.1, an eight terabyte collection of openly licensed text designed for LLM pretraining. The Common Pile comprises content from 30 sources that span diverse domains including research papers, code, books, encyclopedias, educational materials, audio transcripts, and more. Crucially, the authors validate the efforts by training two 7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B. In addition to releasing the Common Pile v0.1 itself, the authors also release the code used in its creation as well as the training mixture and checkpoints for the Comma v0.1 models. <br> <br>

13. ***Columbia Uni et al.'s analysis of sample replay in continual learning:  <br>This study theoretically analyzes sample replay for mitigating forgetting in over-parameterized continual linear regression, finding surprisingly that forgetting can be non-monotonic with respect to the number of replay samples. Their analysis reveals scenarios where replay can be harmful, increasing forgetting in both worst-case and distributional settings, with empirical evidence suggesting similar behavior in neural networks.*** <br> <br>
    Jun 4, Columbia Uni, Nvidia, NYU and Stanford Uni published a [paper](https://arxiv.org/pdf/2506.04377) “Replay Can Provably Increase Forgetting”. Continual learning seeks to enable machine learning systems to solve an increasing corpus of tasks sequentially. A critical challenge for continual learning is forgetting, where the performance on previously learned tasks decreases as new tasks are introduced. One of the commonly used techniques to mitigate forgetting, sample replay, has been shown empirically to reduce forgetting by retaining some examples from old tasks and including them in new training episodes. This study provides a theoretical analysis of sample replay in an over-parameterized continual linear regression setting, where each task is given by a linear subspace and with enough replay samples, one would be able to eliminate forgetting. The analysis focuses on sample replay and highlights the role of the replayed samples and the relationship between task subspaces. Surprisingly, the study finds that, even in a noiseless setting, forgetting can be non-monotonic with respect to the number of replay samples. The authors present tasks where replay can be harmful with respect to worst-case settings, and also in distributional settings where replay of randomly selected samples increases forgetting in expectation. The study also gives empirical evidence that harmful replay is not limited to training with linear models by showing similar behavior for a neural networks equipped with SGD. Through experiments on a commonly used benchmark, the work provides additional evidence that, even in seemingly benign scenarios, performance of the replay heavily depends on the choice of replay samples and the relationship between tasks. <br> <br>

15. ***Bengio's LawZero non-profit for AI safety:  <br>Yoshua Bengio launched LawZero, a non-profit dedicated to AI safety research prioritizing human well-being, driven by concerns over alarming behaviors like deception and self-preservation in current AI. LawZero aims to develop fundamentally safe AI, centered on the concept of a non-agentic, memoryless "Scientist AI" designed to understand and explain, serving as a safety layer and tool for scientific discovery.*** <br> <br>
    Jun 3, Bengio [announced](https://yoshuabengio.org/2025/06/03/introducing-lawzero/) LawZero company. Yoshua Bengio has launched a new non-profit organization called LawZero, dedicated to AI safety research that prioritizes human well-being over commercial interests. Motivated by alarming behaviors in current AI systems—such as deception, self-preservation, and even hacking—Bengio emphasizes the urgent need to address the risks posed by increasingly agentic AI. He cites real-world examples, including AI models that manipulate systems to avoid replacement or cheat in games, as early warnings of potential dangers. Bengio likens the current trajectory of AI development to driving up a foggy, unmarked mountain road with loved ones, where the thrill of progress is shadowed by the risk of catastrophic failure. LawZero is his response to this precarious situation, aiming to develop AI that is not only powerful but fundamentally safe. Central to this vision is the concept of the Scientist AI—a non-agentic, memoryless system designed to understand and explain rather than act or deceive. This AI would function like an idealized scientist or psychologist, analyzing human behavior without imitating it, and offering probabilistic assessments of truth and risk. Such a system could serve as a safety layer for more agentic AIs, flagging potentially harmful actions before they occur. Bengio envisions the Scientist AI as a tool to accelerate scientific discovery and as a foundation for building trustworthy AI agents. Ultimately, LawZero reflects his deep concern for future generations and a commitment to ensuring that AI development enhances, rather than endangers, human life. <br> <br>

17. ***Uni of Virginia and Princeton's study on negative reinforcement in LLM reasoning:  <br>This research decomposed the learning signal in RLVR into Positive Sample Reinforcement (PSR) and Negative Sample Reinforcement (NSR), finding that training Qwen models with only NSR (penalizing incorrect responses without reinforcing correct ones) can be highly effective. NSR consistently improved performance over base models across the Pass@k spectrum, often matching or surpassing PPO/GRPO by suppressing incorrect generations and refining existing knowledge.*** <br> <br>
    Jun 2, Uni of Virginia and Princeton Uni published a [paper](https://arxiv.org/pdf/2506.01347) “The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning”. Reinforcement learning with verifiable rewards (RLVR) is a promising approach for training language models (LMs) on reasoning tasks that elicit emergent long chains of thought (CoTs). Unlike supervised learning, it updates the model using both correct and incorrect samples via policy gradients. To better understand its mechanism, the study decomposes the learning signal into reinforcing correct responses and penalizing incorrect ones, referred to as Positive and Negative Sample Reinforcement (PSR and NSR), respectively. The study trains Qwen2.5-Math-7B and Qwen3-4B on a mathematical reasoning dataset and uncovers a surprising result: training with only negative samples -- without reinforcing correct responses -- can be highly effective: it consistently improves performance over the base model across the entire Pass@k spectrum (k up to 256), often matching or surpassing PPO and GRPO. In contrast, reinforcing only correct responses improves Pass@1 but degrades performance at higher k, due to reduced diversity. These inference-scaling trends highlight that solely penalizing incorrect responses may contribute more to performance than previously recognized. Through gradient analysis, the study shows that NSR works by suppressing incorrect generations and redistributing probability mass toward other plausible candidates, guided by the model's prior beliefs. It refines the model's existing knowledge rather than introducing entirely new behaviors. Building on this insight, the study proposes a simple variant of the RL objective that upweights NSR, and show that it consistently improves overall Pass@k performance on MATH, AIME 2025, and AMC23. https://github.com/TianHongZXY/RLVR-Decomposed <br> <br>

19. ***Meta et al.'s estimation of LLM memorization:  <br>This work proposes a method to separate and quantify unintended memorization (specific dataset information) from generalization (true data-generation process information) in language models, estimating that GPT-style models have a capacity of approximately 3.6 bits per parameter. They observed that models memorize until capacity is filled, after which "grokking" begins and unintended memorization decreases as generalization improves.*** <br> <br>
    Jun 2, Meta, Google, Cornell Uni and Nvidia published a [paper](https://arxiv.org/pdf/2505.24832) “How much do language models memorize?”. The work proposes a new method for estimating how much a model “knows” about a datapoint and use it to measure the capacity of modern language models. Prior studies of language model memorization have struggled to disentangle memorization from generalization. The study formally separates memorization into two components: unintended memorization, the information a model contains about a specific dataset, and generalization, the information a model contains about the true data-generation process. When the study completely eliminates generalization, it can computes the total memorization, which provides an estimate of model capacity: the measurements estimate that GPT-style models have a capacity of approximately 3.6 bits per parameter. The work trains language models on datasets of increasing size and observe that models memorize until their capacity fills, at which point “grokking” begins, and unintended memorization decreases as models begin to generalize. The study trains hundreds of transformer language models ranging from 500K  to 1.5B parameters and produce a series of scaling laws relating model capacity and data size to membership inference. <br> <br>

21. ***Uni of Michigan et al.'s EXP-Bench for AI research automation:  <br>This paper introduces EXP-Bench, a benchmark to evaluate AI agents on complete AI research experiments sourced from influential publications, challenging agents to formulate hypotheses, design/implement procedures, execute them, and analyze results. While current leading LLM-based agents showed partial capabilities, their success rate for complete, executable experiments was only 0.5%, highlighting bottlenecks EXP-Bench aims to address.*** <br> <br>
    Jun 2, Uni of Michigan, Rice Uni, Cisco and UC Berkeley published a [paper](https://arxiv.org/pdf/2505.24785) “EXP-Bench: Can AI Conduct AI Research Experiments?” Automating AI research holds immense potential for accelerating scientific progress, yet current AI agents struggle with the complexities of rigorous, end-to-end experimentation. The study introduces EXP-Bench, a novel benchmark designed to systematically evaluate AI agents on complete research experiments sourced from influential AI publications. Given a research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results. To enable the creation of such intricate and authentic tasks with high-fidelity, the study designs a semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers. Evaluations of leading LLM-based agents, such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial capabilities: while scores on individual experimental aspects such as design or implementation correctness occasionally reach 20-35%, the success rate for complete, executable experiments was a mere 0.5%. By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP-Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments. https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench. <br> <br>

23. ***UC Berkeley and Meta's Self-Challenging framework for agent training:  <br>This study proposes the Self-Challenging framework where an LLM agent first acts as a challenger, generating high-quality "Code-as-Task" problems (defined by instruction, verification function, and test cases) by interacting with tools, and then acts as an executor, training on these self-generated tasks via reinforcement learning. This approach achieved over a two-fold improvement in Llama-3.1-8B-Instruct on tool-use benchmarks using only self-generated data.*** <br> <br>
    Jun 2, UC Berkeley and Meta published a [paper](https://arxiv.org/pdf/2506.01716) “Self-Challenging Language Model Agents”. Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. This study proposes the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. <br> <br>

25. ***Nvidia and Georgia Tech's argument for SLMs in agentic AI:  <br>This position paper argues that small language models (SLMs) are sufficiently powerful, inherently more suitable, and economically necessary for many specialized, repetitive tasks in agentic AI systems, making them the future of this field. While acknowledging LLMs' conversational strengths, they propose heterogeneous agentic systems and outline an LLM-to-SLM agent conversion algorithm, emphasizing the operational and economic impact of this shift.*** <br> <br>
    Jun 2, Nivdia and Georgia Inst of Tech published a [paper](https://arxiv.org/pdf/2506.02153) “Small Language Models are the Future of Agentic AI”. Large language models (LLMs) are often praised for exhibiting near-human performance on a wide range of tasks and valued for their ability to hold a general conversation. The rise of agentic AI systems is, however, ushering in a mass of applications in which language models perform a small number of specialized tasks repetitively and with little variation. Here the authors lay out the position that small language models (SLMs) are sufficiently powerful, inherently more suitable, and necessarily more economical for many invocations in agentic systems, and are therefore the future of agentic AI. The argumentation is grounded in the current level of capabilities exhibited by SLMs, the common architectures of agentic systems, and the economy of LM deployment. The study further argues that in situations where general-purpose conversational abilities are essential, heterogeneous agentic systems (i.e., agents invoking multiple different models) are the natural choice. The study discusses the potential barriers for the adoption of SLMs in agentic systems and outline a general LLM-to-SLM agent conversion algorithm. The position, formulated as a value statement, highlights the significance of the operational and economic impact even a partial shift from LLMs to SLMs is to have on the AI agent industry. The study aims to stimulate the discussion on the effective use of AI resources and hope to advance the efforts to lower the costs of AI of the present day. https://research.nvidia.com/labs/lpr/slm-agents. <br> <br>

27. ***Allen Inst for AI et al.'s RewardBench 2:  <br>This paper introduces RewardBench 2, a new multi-skill reward modeling benchmark designed with challenging new human-prompted data to improve accuracy-based reward model evaluation and better correlate with downstream performance in RLHF and inference-time scaling. Models score significantly lower on RewardBench 2 (about 20 points less than the original), facilitating more rigorous evaluation practices.*** <br> <br>
    Jun 2, Allen Inst for AI, Uni of Washington and Cohere published a [paper](https://arxiv.org/pdf/2506.01937) “RewardBench 2: Advancing Reward Model Evaluation”. Reward models are used throughout the post-training of language models to capture nuanced signals from preference data and provide a training target for optimization across instruction following, reasoning, safety, and more domains. The community has begun establishing best practices for evaluating reward models, from the development of benchmarks that test capabilities in specific skill areas to others that test agreement with human preferences. At the same time, progress in evaluation has not been mirrored by the effectiveness of reward models in downstream tasks -- simpler direct alignment algorithms are reported to work better in many cases. This paper introduces RewardBench 2, a new multi-skill reward modeling benchmark designed to bring new, challenging data for accuracy-based reward model evaluation -- models score about 20 points on average lower on RewardBench 2 compared to the first RewardBench -- while being highly correlated with downstream performance. Compared to most other benchmarks, RewardBench 2 sources new human prompts instead of existing prompts from downstream evaluations, facilitating more rigorous evaluation practices. The paper describes the benchmark construction process and report how existing models perform on it, while quantifying how performance on the benchmark correlates with downstream use of the models in both inference-time scaling algorithms, like best-of-N sampling, and RLHF training algorithms like proximal policy optimization. https://github.com/allenai/reward-bench, https://huggingface.co/datasets/allenai/reward-bench-2 <br> <br>

29. ***UCSC et al.'s assessment of amplified hallucination in multimodal reasoning:  <br>This research investigates how extended reasoning chains in multimodal LLMs can increase hallucination by causing models to drift from image-grounded content due to reduced focus on visual inputs. They introduce the RH-AUC metric to quantify how perception accuracy changes with reasoning length and RH-Bench, a diagnostic benchmark, finding larger models often balance reasoning and perception better, influenced more by training data types than volume.*** <br> <br>
    May 31, UCSC, Stanford Uni and UCSB published a [paper](https://arxiv.org/pdf/2505.21523) “More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models”. Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, the study introduces RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing to evaluate whether the model preserves visual grounding during reasoning. The study also releases RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. The analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity. https://mlrm-halu.github.io/ <br> <br>

31. ***ETH Zurich et al.'s critique of LLM forecasting evaluation:  <br>This study argues for caution regarding claims of LLMs matching or exceeding human forecasting performance, identifying pitfalls in current evaluation methodologies. They highlight issues like temporal leakage making results untrustworthy and difficulties extrapolating evaluation performance to real-world scenarios, calling for more rigorous evaluation to confidently assess LLM forecasting abilities.*** <br> <br>
    May 31, ETH Zurich, ELLIS, and MPI published a [paper](https://arxiv.org/pdf/2506.00723) “Pitfalls in Evaluating Language Model Forecasters”. Large language models (LLMs) have recently been applied to forecasting tasks, with some works claiming these systems match or exceed human performance. The study argues that, as a community, people should be careful about such conclusions as evaluating LLM forecasters presents unique challenges. The work identifies two broad categories of issues: (1) difficulty in trusting evaluation results due to many forms of temporal leakage, and (2) difficulty in extrapolating from evaluation performance to real-world forecasting. Through systematic analysis and concrete examples from prior work, the study demonstrates how evaluation flaws can raise concerns about current and future performance claims. The work argues that more rigorous evaluation methodologies are needed to confidently assess the forecasting abilities of LLMs. <br> <br>

33. ***Github's Reasoning Gym for RLVR:  <br>This paper introduces Reasoning Gym (RG), a library providing over 100 procedurally generated reasoning environments with verifiable rewards for reinforcement learning, spanning domains like algebra, logic, and games. Unlike fixed datasets, RG's ability to generate virtually infinite training data with adjustable complexity allows for continuous evaluation across varying difficulty levels, demonstrating its efficacy for both evaluating and training reasoning models.*** <br> <br>
    May 30, Github published a [paper](https://arxiv.org/pdf/2505.24760) “REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards”. The paper introduces Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models. https://github.com/open-thought/reasoning-gym/ <br> <br>

35. ***Nvidia's ProRL for expanding LLM reasoning boundaries:  <br>This study introduces Prolonged RL (ProRL), a training methodology incorporating KL divergence control, reference policy resetting, and diverse tasks, demonstrating that extended RL training can uncover novel reasoning strategies inaccessible to base models even with extensive sampling. Empirical analysis shows RL-trained models consistently outperform base models, especially as task competence and training duration increase, suggesting RL can explore new solution space regions.*** <br> <br>
    May 30, Nvidia published a [paper](https://arxiv.org/pdf/2505.24864) “ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models”. Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. This study challenges prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. The study introduces ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. The study further shows that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B <br> <br>

37. ***Yale Uni et al.'s MetaFaith for faithful LLM uncertainty expression:  <br>Addressing LLMs' tendency to make false claims assertively, this paper presents a systematic study of faithful confidence calibration, finding existing models and interventions largely fail. They introduce MetaFaith, a prompt-based calibration approach inspired by human metacognition, which robustly improves faithful uncertainty expression across diverse models and tasks (up to 61% improvement), achieving high human preference.*** <br> <br>
    May 30, Yale Uni, Google and NYU published a [paper](https://arxiv.org/pdf/2505.24858) “MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs”. A critical component in the trustworthiness of LLMs is reliable uncertainty communication, yet LLMs often use assertive language when conveying false claims, leading to over-reliance and eroded trust. The study presents the first systematic study of faithful confidence calibration of LLMs, benchmarking models' ability to use linguistic expressions of uncertainty that faithfully reflect their intrinsic uncertainty, across a comprehensive array of models, datasets, and prompting strategies. The results demonstrate that LLMs largely fail at this task, and that existing interventions are insufficient: standard prompt approaches provide only marginal gains, and existing, factuality-based calibration techniques can even harm faithful calibration. To address this critical gap, the study introduces MetaFaith, a novel prompt-based calibration approach inspired by human metacognition. The study shows that MetaFaith robustly improves faithful calibration across diverse models and task domains, enabling up to 61% improvement in faithfulness and achieving an 83% win rate over original generations as judged by humans. <br> <br>

39. ***ServiceNow's comparison of SLM fine-tuning vs. LLM prompting:  <br>This study investigates whether fine-tuning Small Language Models (SLMs) still offers advantages over prompting Large Language Models (LLMs) for domain-specific tasks requiring structured outputs, specifically generating low-code workflows in JSON. Their findings show that while good LLM prompting yields reasonable results, fine-tuning an SLM improves quality by an average of 10%.*** <br> <br>
    May 30, ServiceNow published a [paper](https://arxiv.org/pdf/2505.24189) “Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows”. Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. This study presents evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. The work compares fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form; and observes that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. The study also performs systematic error analysis to reveal model limitations. <br> <br>

41. ***Yale Uni's HELM hyperbolic LLMs:  <br>This work proposes HELM (HypErbolic Large Language Models), a family of models operating fully in Hyperbolic space to better capture the semantic hierarchies and geometric structure of natural language, addressing limitations of Euclidean LLMs. Introducing HELM-MICE (Mixture-of-Curvature Experts) and HELM-D, with hyperbolic equivalents of RoPE and RMSNorm, they demonstrate consistent gains (up to 4%) over Euclidean architectures on benchmarks when trained at billion-parameter scale.*** <br> <br>
    May 30, Yale Uni published a [paper](https://arxiv.org/pdf/2505.24722) “HELM Hyperbolic Large Language Models via Mixture-of-Curvature Experts”. Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. The work thus proposes to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. The study thus introduces HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. The study additionally introduces a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, the study further develops hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, the study develops essential hyperbolic equivalents of rotary positional encodings and RMS normalization. The work is the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. The results show consistent gains from the HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.  <br> <br>

43. ***MIT and Adobe's Large Chunk Test-Time Training (LaCT):  <br> <br>This paper revisits Test-Time Training (TTT) for long-context data, proposing Large Chunk Test-Time Training (LaCT) which uses extremely large chunk updates (2K to 1M tokens) instead of small minibatches. LaCT significantly improves hardware utilization, scales nonlinear state size (up to 40% of model parameters) for better state capacity without complex kernel implementations, and is validated across diverse modalities including 1M context length novel view synthesis.*** <br> <br>
    May 29, MIT and Adobe published a [paper](https://arxiv.org/pdf/2505.23884) “Test-Time Training Done Right”. Test-Time Training (TTT) models context dependencies by adapting part of the model's weights (referred to as fast weights) during inference. This fast weight, akin to recurrent states in RNNs, stores temporary memories of past tokens in the current sequence. Existing TTT methods struggled to show effectiveness in handling long-context data, due to their inefficiency on modern GPUs. The TTT layers in many of these approaches operate with extremely low FLOPs utilization (often <5%) because they deliberately apply small online minibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover, a small minibatch implies fine-grained block-wise causal dependencies in the data, unsuitable for data beyond 1D ordered sequences, like sets or N-dimensional grids such as images or videos. In contrast, the study pursues the opposite direction by using an extremely large chunk update, ranging from 2K to 1M tokens across tasks of varying modalities, which is referred to as Large Chunk Test-Time Training (LaCT). It improves hardware utilization by orders of magnitude, and more importantly, facilitates scaling of nonlinear state size (up to 40% of model parameters), hence substantially improving state capacity, all without requiring cumbersome and error-prone kernel implementations. It also allows easy integration of sophisticated optimizers, e.g. Muon for online updates. The study validates the approach across diverse modalities and tasks, including novel view synthesis with image set, language models, and auto-regressive video diffusion. The approach can scale up to 14B-parameter AR video diffusion model on sequences up to 56K tokens. In the longest sequence experiment, the study performs novel view synthesis with 1 million context length. The authors hope this work will inspire and accelerate new research in the field of long-context modeling and test-time training. Test-Time Training (TTT) models context dependencies by adapting part of the model's weights (referred to as fast weights) during inference. This fast weight, akin to recurrent states in RNNs, stores temporary memories of past tokens in the current sequence. Existing TTT methods struggled to show effectiveness in handling long-context data, due to their inefficiency on modern GPUs. The TTT layers in many of these approaches operate with extremely low FLOPs utilization (often <5%) because they deliberately apply small online minibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover, a small minibatch implies fine-grained block-wise causal dependencies in the data, unsuitable for data beyond 1D ordered sequences, like sets or N-dimensional grids such as images or videos. In contrast, we pursue the opposite direction by using an extremely large chunk update, ranging from 2K to 1M tokens across tasks of varying modalities, which we refer to as Large Chunk Test-Time Training (LaCT). It improves hardware utilization by orders of magnitude, and more importantly, facilitates scaling of nonlinear state size (up to 40% of model parameters), hence substantially improving state capacity, all without requiring cumbersome and error-prone kernel implementations. It also allows easy integration of sophisticated optimizers, e.g. Muon for online updates. We validate our approach across diverse modalities and tasks, including novel view synthesis with image set, language models, and auto-regressive video diffusion. Our approach can scale up to 14B-parameter AR video diffusion model on sequences up to 56K tokens. In our longest sequence experiment, we perform novel view synthesis with 1 million context length. We hope this work will inspire and accelerate new research in the field of long-context modeling and test-time training. https://tianyuanzhang.com/projects/ttt-done-right <br> <br>

45. ***MIT's FlashFormer for efficient low-batch inference:  <br>This paper describes FlashFormer, a proof-of-concept whole-model kernel designed to accelerate single-batch inference for transformer-based LLMs, addressing the memory bandwidth and kernel launch overheads significant in low-batch settings. FlashFormer demonstrates nontrivial speedups compared to existing state-of-the-art inference kernels across various model sizes and quantization settings.*** <br> <br>
    May 28, MIT published a [paper](https://arxiv.org/pdf/2505.22758) “FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference”. The size and compute characteristics of modern large language models have led to an increased interest in developing specialized kernels tailored for training and inference. Existing kernels primarily optimize for compute utilization, targeting the large-batch training and inference settings. However, low-batch inference, where memory bandwidth and kernel launch overheads contribute are significant factors, remains important for many applications of interest such as in edge deployment and latency-sensitive applications. This paper describes FlashFormer, a proof-of-concept kernel for accelerating single-batch inference for transformer-based large language models. Across various model sizes and quantizations settings, we observe nontrivial speedups compared to existing state-of-the-art inference kernels. <br> <br>

47. ***George Mason Uni and OpenAI's Linear Layouts for tensor computation:  <br>This study introduces Linear Layouts, a novel approach modeling tensor layouts using linear algebra over F2 (binary matrices acting on hardware representation bits) to achieve generic and efficient tensor computation. Integrated with Triton, Linear Layouts enable generic layout definitions and conversions, reducing compiler engineering effort and fixing bugs in Triton's legacy system while optimizing operator performance.*** <br> <br>
    May 28, George Mason Uni and OpenAI published a [paper](https://arxiv.org/pdf/2505.23819) “Linear Layouts: Robust Code Generation of Efficient Tensor Computation Using”. Efficient tensor computation is a cornerstone of modern deep learning (DL) workloads, yet existing approaches struggle to achieve flexible and performant design and implementation of tensor layouts -- mappings between logical tensors and hardware resources. The increasing complexity of DL algorithms and hardware demands a generic and systematic approach to handling tensor layouts. This study introduces Linear Layouts, a novel approach that models tensor layouts using linear algebra over F2. By representing tensor layouts as binary matrices acting on the bits of the hardware representation, the approach enables a generic layout definition -- as opposed to the classical case-by-case approach -- and allows for generic layout-to-layout conversions, eliminating the quadratic explosion that plagues existing solutions. The study integrates linear layouts with Triton and demonstrate their effectiveness in optimizing individual Triton operators as well as kernels written in Triton. The work also shows that linear layouts reduce engineering effort in the compiler backend while fixing several bugs in Triton's legacy layout system.  <br>   <br>

49. ***LBOX et al.'s LegalSearchLM for legal case retrieval:   <br> This research addresses limitations in Legal Case Retrieval (LCR) by introducing LEGAR BENCH, a large-scale Korean LCR benchmark, and LegalSearchLM, a retrieval model that performs legal element reasoning over query cases and directly generates content grounded in target cases via constrained decoding. LegalSearchLM outperforms baselines by 6-20% on LEGAR BENCH and shows strong out-of-domain generalization.***  <br>   <br> 
    May 28, LBOX, UIUC and Uni of Seoul published a [paper](https://arxiv.org/pdf/2505.23832) “LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation”. Legal Case Retrieval (LCR), which retrieves relevant cases from a query case, is a fundamental task for legal professionals in research and decision-making. However, existing studies on LCR face two major limitations. First, they are evaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and use a narrow range of criminal query types, which cannot sufficiently reflect the complexity of real-world legal retrieval scenarios. Second, their reliance on embedding-based or lexical matching methods often results in limited representations and legally irrelevant matches. To address these issues, the study presents: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering 411 diverse crime types in queries over 1.2M legal cases; and (2) LegalSearchLM, a retrieval model that performs legal element reasoning over the query case and directly generates content grounded in the target cases through constrained decoding. Experimental results show that LegalSearchLM outperforms baselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It also demonstrates strong generalization to out-of-domain cases, outperforming naive generative models trained on in-domain data by 15%.  <br>   <br> 

51. ***Stanford Uni and NYU on LLM vs. human semantic compression:   <br> This study uses an information-theoretic framework to compare how LLMs and humans trade off semantic compression for meaning, finding that while LLMs form broad conceptual categories aligned with human judgment, they struggle with fine-grained semantic distinctions. LLMs exhibit a strong bias towards aggressive statistical compression, whereas human conceptual systems prioritize adaptive nuance and contextual richness over compressional efficiency.***  <br>   <br> 
    May 26, Stanford Uni and NYU published a [paper](https://arxiv.org/pdf/2505.17117) “From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning”. Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. The study introduces a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, the study uncovers key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by human measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.

  <br>   <br>   <br> 


***Jun 1, 2025***

1. ***Anthropic CEO's AI job displacement warning:   <br>Dario Amodei, CEO of Anthropic, predicted AI could eliminate 50% of entry-level white-collar jobs within five years, potentially causing US unemployment to hit 20% by 2030, as AI surpasses human capabilities in intellectual tasks. He urged politicians and businesses to prepare for this disruption, suggesting taxing AI labs and emphasizing the need for proactive adaptation, a concern echoed by a World Economic Forum survey and an Australian report.***  <br>  <br>
   May 31, new.com.au published an [article](https://www.news.com.au/finance/work/careers/anthropic-ceo-warns-ai-could-wipe-out-1-in-2-white-collar-jobs-in-next-five-years/news-story/3196841292011f2147be15b6e186a289) “Anthropic CEO warns AI could wipe out 1 in 2 white collar jobs in next five years”. Anthropic CEO Dario Amodei has issued a stark warning, predicting that artificial intelligence could eliminate half of all entry-level, white-collar jobs within the next five years, potentially driving US unemployment as high as 20% by 2030. Amodei asserts that AI is rapidly surpassing human capabilities in intellectual tasks like summarizing, analysis, and coding, performing at the standard of a "smart college student" for nearly seven hours a day. He expresses deep concern that politicians and businesses are unprepared for this impending shift, stressing that the public remains largely unaware of the impending scale of workforce disruption. Despite it being against his company's economic interest, Amodei urged US politicians to consider taxing AI labs and emphasized the need for proactive adaptation and policy implementation, stating society "can't just sleepwalk into it." His alarm is echoed by a World Economic Forum survey finding 41% of employers intend to reduce staff due to AI by 2030, and an Australian report indicating a third of knowledge/manual roles are at risk. While some experts note AI primarily absorbs low-skill tasks, requiring workers to "level up," and companies plan to hire for AI-related skills, Amodei remains resolute that swift action is necessary to steer AI's inevitable progress towards beneficial outcomes while mitigating significant harms.  <br>  <br>

3. ***Princeton et al.'s study on learning compositional functions:   <br>Researchers explored how Transformers learn complex compositional tasks, specifically the k-fold composition task, proving a statistical query lower bound that indicates a statistical-computational gap for efficient learning. However, they demonstrated that gradient descent on an O(logk)-depth transformer can efficiently learn this function class using curriculum learning strategies (presenting easier data first or all at once), highlighting the necessity of both easy and hard examples.***  <br>  <br>
   May 30, Princeton Uni, Flatiron Inst, Columbia Uni and NYU published a [paper](https://arxiv.org/pdf/2505.23683) “Learning Compositional Functions with Transformers from Easy-to-Hard Data”. Transformer-based language models have demonstrated impressive capabilities across a range of complex reasoning tasks. Prior theoretical work exploring the expressive power of transformers has shown that they can efficiently perform multi-step reasoning tasks involving parallelizable computations. However, the learnability of such constructions, particularly the conditions on the data distribution that enable efficient learning via gradient-based optimization, remains an open question. Towards answering this question, the authors study the learnability of the k-fold composition task, which requires computing an interleaved composition of k input permutations and k hidden permutations, and can be expressed by a transformer with O(logk) layers. On the negative front, the study proves a Statistical Query (SQ) lower bound showing that any SQ learner that makes only polynomially-many queries to an SQ oracle for the k-fold composition task distribution must have sample size exponential in k, thus establishing a statistical-computational gap. On the other hand, the study shows that this function class can be efficiently learned, with runtime and sample complexity polynomial in k, by gradient descent on an O(logk)-depth transformer via two different curriculum learning strategies: one in which data consists of k′-fold composition functions with k′≤k presented in increasing difficulty, and another in which all such data is presented simultaneously. The work sheds light on the necessity and sufficiency of having both easy and hard examples in the data distribution for transformers to learn complex compositional tasks.  <br>  <br>

5. ***Google's ATLAS for optimal context memorization:   <br>This paper introduces ATLAS, a long-term memory module designed to enhance Transformer-like architectures by overcoming limitations in memory capacity, online updates, and fixed-size memory management. ATLAS learns to optimally memorize context from past and current tokens and, when integrated into DeepTransformers, surpasses standard Transformers and linear recurrent models on various tasks, significantly improving long-context performance (e.g., +80% on 10M context BABILong).***  <br>  <br>
   May 29, Google published a [paper](https://arxiv.org/pdf/2505.23735) “ATLAS: Learning to Optimally Memorize the Context at Test Time”. Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. The study observes that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, the work presents ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Building on this insight, the study presents a new family of Transformer-like architectures, called DeepTransformers, that are strict generalizations of the original Transformer architecture. Experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. ATLAS further improves the long context performance of Titans, achieving +80\% accuracy in 10M context length of BABILong benchmark.  <br>  <br>

7. ***The Darwin Gödel Machine for self-improving agents:   <br>Researchers introduced the Darwin Gödel Machine (DGM), a system that enables AI agents to autonomously and continuously improve by iteratively modifying their own code and empirically validating changes using coding benchmarks. Inspired by Darwinian evolution and open-endedness, the DGM maintains and grows an archive of diverse, high-quality agents, significantly improving performance on SWE-bench and Polyglot and outperforming baselines without self-improvement.***  <br>  <br>
   May 29, Uni of British Columbia, Vector Inst, Sakana AI, and CIFAR published a [paper](https://arxiv.org/abs/2505.22954) “Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents”. Today's AI systems have human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The advance of AI could itself be automated. If done safely, that would accelerate AI development and allow us to reap its benefits much sooner. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The Gödel machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. The study introduces the Darwin Gödel Machine (DGM), a self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM maintains an archive of generated coding agents. It grows the archive by sampling an agent from it and using a foundation model to create a new, interesting, version of the sampled agent. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self-improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). The DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation.  <br>  <br>

9. ***Business Insider's workforce reduction:   <br>According to an internal memo, Business Insider is laying off approximately 21% of its staff due to shrinking search traffic and the increasing use of generative AI tools like ChatGPT. CEO Barbara Peng stated the company must restructure to withstand traffic volatility, is accelerating AI adoption, realigning content strategy to high-engagement areas, exiting most commerce business, and launching a new events division called BI Live.***  <br>  <br>
    May 29, according to [Reuters](https://www.reuters.com/technology/business-insider-cuts-21-workforce-memo-shows-2025-05-29/), “Business Insider cuts 21% of workforce, memo shows”. Business Insider is laying off about 21% of its workforce, an internal memo showed on Thursday, as the financial news outlet grapples with shrinking search traffic and the growing use of generative AI tools such as ChatGPT. The New York-based company joins several digital media companies in restructuring operations as consumers increasingly depend on artificial intelligence for news synopsis, which is eating into web traffic. In the memo, CEO Barbara Peng told staff the company now generates twice as much revenue for each website visit as it did two years ago, but 70% of its business still has some degree of traffic sensitivity. "We must be structured to endure extreme traffic drops outside of our control, so we're reducing our overall company to a size where we can absorb that volatility," Peng said in the memo seen by Reuters. The New York-based company is accelerating adoption of AI, with a majority of employees already utilizing Enterprise ChatGPT and several AI-driven products to enhance operations and reader experience, Peng said. The website is realigning its content strategy to concentrate on areas that attract high reader engagement, and is exiting the majority of its commerce business, Peng said. It is also launching a new events business called BI Live, Peng said, adding that it has already seen some demand and will continue to build the team.  <br>  <br>

11. ***UIUC's ToMAP for opponent-aware LLM persuaders:   <br>This research introduces Theory of Mind Augmented Persuader (ToMAP), a novel approach that enhances LLM persuaders by incorporating modules to model an opponent's mental state, including considering objections and predicting stances on counterclaims. Through a designed reinforcement learning schema, ToMAP (a 3B parameter model) outperforms much larger baselines like GPT-4o by 39.4% across diverse corpora, exhibiting more complex, diverse, and effective arguments.***  <br>  <br>
    May 29, UIUC published a [paper](https://arxiv.org/pdf/2505.22961) “ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind”. Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably, while humans are skilled in modeling their opponent's thoughts and opinions proactively and dynamically, current LLMs struggle with such Theory of Mind (ToM) reasoning, resulting in limited diversity and opponent awareness. To address this limitation, the work introduces Theory of Mind Augmented Persuader (ToMAP), a novel approach for building more flexible persuader agents by incorporating two theory of mind modules that enhance the persuader's awareness and analysis of the opponent's mental state. Specifically, the study begins by prompting the persuader to consider possible objections to the target central claim, and then use a text encoder paired with a trained MLP classifier to predict the opponent's current stance on these counterclaims. The carefully designed reinforcement learning schema enables the persuader learns how to analyze opponent-related information and utilize it to generate more effective arguments. Experiments show that the ToMAP persuader, while containing only 3B parameters, outperforms much larger baselines, like GPT-4o, with a relative gain of 39.4% across multiple persuadee models and diverse corpora. Notably, ToMAP exhibits complex reasoning chains and reduced repetition during training, which leads to more diverse and effective arguments. The opponent-aware feature of ToMAP also makes it suitable for long conversations and enables it to employ more logical and opponent-aware strategies. These results underscore the method's effectiveness and highlight its potential for developing more persuasive language agents. https://github.com/ulab-uiuc/ToMAP.  <br>  <br>

13. ***CMU's unsupervised RL via entropy minimization:   <br>This study proposes RENT (Reinforcement Learning via Entropy Minimization), a fully unsupervised RL method that improves LLM reasoning without external rewards or ground-truth answers by using the model's own output entropy as an intrinsic reward. By reinforcing chains of thought that yield high model confidence, RENT demonstrated improvements on various reasoning benchmarks (GSM8K, MATH500, etc.) across Qwen and Mistral model families.***  <br>  <br>
    May 29, CMU published a [paper](https://arxiv.org/pdf/2505.22660) “Maximizing Confidence Alone Improves Reasoning”. Reinforcement learning (RL) has enabled machine learning models to achieve significant advances in many fields. Most recently, RL has empowered frontier language models to solve challenging math, science, and coding problems. However, central to any RL algorithm is the reward function, and reward engineering is a notoriously difficult problem in any domain. This study proposes RENT: Reinforcement Learning via Entropy Minimization -- a fully unsupervised RL method that requires no external reward or ground-truth answers, and instead uses the model's entropy of its underlying distribution as an intrinsic reward. The study finds that by reinforcing the chains of thought that yield high model confidence on its generated answers, the model improves its reasoning ability. In the experiments, the study showcases these improvements on an extensive suite of commonly-used reasoning benchmarks, including GSM8K, MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and Mistral families. The generality of the unsupervised learning method lends itself to applicability in a wide range of domains where external supervision is unavailable.  <br>  <br>

15. ***Rethinking training signals in RLVR:   <br>Researchers found that reinforcement learning with verifiable rewards (RLVR) can enhance mathematical reasoning in some models (like Qwen2.5-Math-7B) even with spurious rewards (random, format-based, incorrect labels), yielding gains nearly matching those from ground truth rewards. This effect, often linked to increased code reasoning behavior in Qwen models, was not consistently observed in other model families, suggesting RLVR might surface pretrained representations and that future research should validate on diverse models.***  <br>  <br>
    May 28, Uni of Washington, Allen Inst for AI and UCLA published a [paper](https://github.com/ruixin31/Rethink_RLVR/blob/main/paper/rethink-rlvr.pdf) “Spurious Rewards: Rethinking Training Signals in RLVR”. The study shows that reinforcement learning with verifiable rewards (RLVR) can elicit strong mathematical reasoning in certain models even with spurious rewards that have little, no, or even negative correlation with the correct answer. For example, RLVR improves MATH-500 performance for Qwen2.5-Math-7B in absolute points by 21.4% (random reward), 16.4% (format reward), 24.6% (incorrect label), 24.4% (1-shot RL), and 26.5% (majority voting)—nearly matching the 28.8% gained with ground truth rewards. However, the spurious rewards that work for Qwen often fail to yield gains with other model families like Llama3 or OLMo2. In particular, the study finds code reasoning—thinking in code without actual code execution—to be a distinctive Qwen2.5-Math behavior that becomes significantly more frequent after RLVR, from 66.7% to over 90%, even with spurious rewards. Overall, the authors hypothesize that, given the lack of useful reward signal, RLVR must somehow be surfacing useful reasoning representations learned during pretraining, although the exact mechanism remains a topic for future work. The study suggests that future RLVR research should possibly be validated on diverse models rather than a single de facto choice, as the work shows that it is easy to get significant performance gains on Qwen models even with completely spurious reward signals. https://github.com/ruixin31/Rethink_RLVR/tree/main  <br>  <br>

17. ***SAIL et al. on RL entropy mechanism for reasoning LLMs:   <br>This paper addresses the issue of policy entropy collapse in RL for LLM reasoning, which bottlenecks performance, establishing an empirical law (R=-a*e^H+b) linking performance (R) to entropy (H). Understanding that entropy decrease is driven by the covariance between action probability and logit changes, they propose methods (Clip-Cov, KL-Cov) to control entropy by restricting updates on high-covariance tokens, thereby encouraging exploration and improving performance.***  <br>  <br>
    May 28, SAIL, Tsinghua Uni UIUC et al published a [paper](https://arxiv.org/pdf/2505.22617) “The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models”. This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished exploratory ability is always accompanied with the saturation of policy performance. In practice, the study establishes a transformation equation R=-a*e^H+b between entropy H and downstream performance R. This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion, and the ceiling is fully predictable (H=0, R=-a+b). The finding necessitates entropy management for continuous exploration toward scaling compute for RL. To this end, the study investigates entropy dynamics both theoretically and empirically. The derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits, which is proportional to its advantage when using Policy Gradient-like algorithms. Empirical study shows that, the values of covariance term and entropy differences matched exactly, supporting the theoretical conclusion. Moreover, the covariance term stays mostly positive throughout training, further explaining why policy entropy would decrease monotonically. Through understanding the mechanism behind entropy dynamics, the authors motivate to control entropy by restricting the update of high-covariance tokens. Specifically, the work proposes two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively. Experiments show that these methods encourage exploration, thus helping policy escape entropy collapse and achieve better downstream performance.  <br>  <br>

19. ***CUM's investigation into LLM self-training:   <br>This study explored whether large reasoning models can self-train, proposing an online self-training reinforcement learning algorithm that uses a model's self-consistency to infer correctness signals without ground-truth supervision. Applied to mathematical reasoning, the algorithm quickly reached performance rivaling supervised RL methods but also highlighted limitations like potential reward hacking where confidently incorrect outputs are favored.***  <br>  <br>
    May 27, CUM published a [paper](https://arxiv.org/pdf/2505.21444) “Can Large Reasoning Models Self-Train?”. Scaling the performance of large language models (LLMs) increasingly depends on methods that reduce reliance on human supervision. Reinforcement learning from automated verification offers an alternative, but it incurs scalability limitations due to dependency upon human-designed verifiers. Self-training, where the model's own judgment provides the supervisory signal, presents a compelling direction. The study proposes an online self-training reinforcement learning algorithm that leverages the model's self-consistency to infer correctness signals and train without any ground-truth supervision. The study applies the algorithm to challenging mathematical reasoning tasks and shows that it quickly reaches performance levels rivaling reinforcement-learning methods trained explicitly on gold-standard answers. Additionally, the study analyzes inherent limitations of the algorithm, highlighting how the self-generated proxy reward initially correlated with correctness can incentivize reward hacking, where confidently incorrect outputs are favored. The results illustrate how self-supervised improvement can achieve significant performance gains without external labels, while also revealing its fundamental challenges.  <br>  <br>

21. ***UC Berkeley and Yale's RLIF for reasoning without external rewards:   <br>This research explores Reinforcement Learning from Internal Feedback (RLIF), proposing Intuitor, an RLIF method that uses an LLM's self-certainty as its sole reward signal, enabling fully unsupervised learning. Intuitor, by replacing external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, matched GRPO's performance on math benchmarks and achieved superior generalization to out-of-domain tasks without needing gold solutions.***  <br>  <br>
    May 26, UC Berkeley and Yale Uni published a [paper](https://arxiv.org/pdf/2505.19590) “Learning to Reason without External Rewards”. Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision. The study explores Reinforcement Learning from Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic signals without external rewards or labeled data. The work proposes Intuitor, an RLIF method that uses a model's own confidence, termed self-certainty, as its sole reward signal. Intuitor replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, enabling fully unsupervised learning. Experiments demonstrate that Intuitor matches GRPO's performance on mathematical benchmarks while achieving superior generalization to out-of-domain tasks like code generation, without requiring gold solutions or test cases. The findings show that intrinsic model signals can drive effective learning across domains, offering a scalable alternative to RLVR for autonomous AI systems where verifiable rewards are unavailable. https://github.com/sunblaze-ucb/Intuitor  <br>  <br>

23. ***UIUC's Time-R1 for comprehensive temporal reasoning:   <br>This work introduces Time-R1, a framework to endow a 3B-parameter LLM with comprehensive temporal abilities (understanding, prediction, creative generation) through a novel three-stage development path featuring an RL curriculum with a dynamic rule-based reward system. Time-R1 significantly outperforms models over 200 times larger on challenging future event prediction and creative scenario generation benchmarks, offering a scalable path to time-aware AI.***  <br>  <br>
    May 26, UIUC published a [paper](https://huggingface.co/papers/2505.13508) “Time-R1: Towards Comprehensive Temporal Reasoning in LLMs”. Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, the work introduces Time-R1, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. The approach features a novel three-stage development path; the first two constitute a reinforcement learning (RL) curriculum driven by a meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event prediction skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards truly time-aware AI. To foster further research, the work also releases Time-Bench, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of Time-R1 checkpoints. https://github.com/ulab-uiuc/Time-R1  <br>  <br>

25. ***CMU's FLAME-MoE research platform:   <br>This paper releases FLAME-MoE, an open-source research suite for Mixture-of-Experts (MoE) language models, comprising seven decoder-only models (38M to 1.7B active parameters) with architectures mirroring modern production LLMs (64 experts, top-8 gating, 2 shared experts). With all training data, scripts, logs, and checkpoints publicly available, FLAME-MoE enables reproducible experimentation and initial analyses show expert specialization and stable routing behavior.***  <br>  <br>
    May 26, CMU published a [paper](https://arxiv.org/pdf/2505.20225) “FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models”. Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4 increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong efficiency-performance trade-offs by activating only a fraction of the model per token. Yet academic researchers still lack a fully open, end-to-end MoE platform for investigating scaling, routing, and expert behavior. The study releases FLAME-MoE, a completely open-source research suite composed of seven decoder-only models, ranging from 38M to 1.7B active parameters, whose architecture--64 experts with top-8 gating and 2 shared experts--closely reflects modern production LLMs. All training data pipelines, scripts, logs, and checkpoints are publicly available to enable reproducible experimentation. Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4 points over dense baselines trained with identical FLOPs. Leveraging full training trace transparency, the study presents initial analyses showing that (i) experts increasingly specialize on distinct token subsets, (ii) co-activation matrices remain sparse, reflecting diverse expert usage, and (iii) routing behavior stabilizes early in training. https://github.com/cmu-flame/FLAME-MoE.  <br>  <br>

27. ***MIT and Red Hat on emergent features in LLMs:   <br>This research studies the emergence of interpretable categorical features within LLMs across training time, transformer layers (space), and model sizes, using sparse autoencoders. Findings indicate clear thresholds for feature emergence and reveal unexpected semantic reactivation, where early-layer features re-emerge later, challenging standard assumptions about representational dynamics.***  <br>  <br>
    May 26, MIT and Red Hat published a [paper](https://arxiv.org/pdf/2505.19440) “The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models”. This paper studies the emergence of interpretable categorical features within large language models (LLMs), analyzing their behavior across training checkpoints (time), transformer layers (space), and varying model sizes (scale). Using sparse autoencoders for mechanistic interpretability, the authors identify when and where specific semantic concepts emerge within neural activations. Results indicate clear temporal and scale-specific thresholds for feature emergence across multiple domains. Notably, spatial analysis reveals unexpected semantic reactivation, with early-layer features re-emerging at later layers, challenging standard assumptions about representational dynamics in transformer models.  <br>  <br>

29. ***Mila et al.'s REARANK reasoning re-ranking agent:   <br>This study presents REARANK, an LLM-based listwise reasoning re-ranking agent that explicitly reasons before re-ranking, improving performance and interpretability. Using reinforcement learning and data augmentation with only 179 annotated samples, REARANK-7B (built on Qwen2.5-7B) demonstrates performance comparable to or even surpassing GPT-4 on various information retrieval benchmarks, especially reasoning-intensive ones.***  <br>  <br>
    May 26, Mila, Uni de Montrael et al. published a [paper](https://arxiv.org/pdf/2505.20046) “REARANK: Reasoning Re-ranking Agent via Reinforcement Learning”. The study presents REARANK, a large language model (LLM)-based listwise reasoning reranking agent. REARANK explicitly reasons before reranking, significantly improving both performance and interpretability. Leveraging reinforcement learning and data augmentation, REARANK achieves substantial improvements over baseline models across popular information retrieval benchmarks, notably requiring only 179 annotated samples. Built on top of Qwen2.5-7B, the REARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and out-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT benchmarks. These results underscore the effectiveness of the approach and highlight how reinforcement learning can enhance LLM reasoning capabilities in reranking. https://github.com/lezhang7/Rearank  <br>  <br>

31. ***Princeton et al.'s Alita generalist agent:   <br>This work introduces Alita, a generalist agent designed for scalable agentic reasoning with minimal predefinition (one direct problem-solving component) and maximal self-evolution (autonomously constructing and refining external capabilities via model context protocols). Alita achieves top-ranking performance on benchmarks like GAIA, Mathvista, and PathVQA, outperforming more complex systems.***  <br>  <br>
    May 26, Princeton Uni et al published a [paper](https://arxiv.org/pdf/2505.20286) “Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution”. Recent advances in large language models (LLMs) have enabled agents to autonomously perform complex, open-ended tasks. However, many existing frameworks depend heavily on manually predefined tools and workflows, which hinder their adaptability, scalability, and generalization across domains. This work introduces Alita--a generalist agent designed with the principle of "Simplicity is the ultimate sophistication," enabling scalable agentic reasoning through minimal predefinition and maximal self-evolution. For minimal predefinition, Alita is equipped with only one component for direct problem-solving, making it much simpler and neater than previous approaches that relied heavily on hand-crafted, elaborate tools and workflows. This clean design enhances its potential to generalize to challenging questions, without being limited by tools. For Maximal self-evolution, the study enables the creativity of Alita by providing a suite of general-purpose components to autonomously construct, refine, and reuse external capabilities by generating task-related model context protocols (MCPs) from open source, which contributes to scalable agentic reasoning. Notably, Alita achieves 75.15% pass@1 and 87.27% pass@3 accuracy, which is top-ranking among general-purpose agents, on the GAIA benchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on Mathvista and PathVQA, outperforming many agent systems with far greater complexity. https://github.com/CharlesQ9/Alita  <br>  <br>

33. ***Apple and Duke's interleaved reasoning via RL:   <br>This study proposes a novel training paradigm using reinforcement learning to guide LLMs to interleave thinking and answering for multi-hop questions, addressing inefficiencies of long chain-of-thought. Using a simple rule-based reward for correct intermediate steps, this approach significantly reduces time-to-first-token (over 80%) and improves accuracy (up to 19.3% Pass@1) without external tools, also showing strong generalization.***  <br>  <br>
    May 26, Apple and Duke Uni published a [paper](https://arxiv.org/pdf/2505.19640) “Interleaved Reasoning for Large Language Models via Reinforcement Learning”. Long chain-of-thought (CoT) significantly enhances large language models' (LLM) reasoning capabilities. However, the extensive reasoning traces lead to inefficiencies and an increased time-to-first-token (TTFT). The study proposes a novel training paradigm that uses reinforcement learning (RL) to guide reasoning LLMs to interleave thinking and answering for multi-hop questions. The work observes that models inherently possess the ability to perform interleaved reasoning, which can be further enhanced through RL. The study introduces a simple yet effective rule-based reward to incentivize correct intermediate steps, which guides the policy model toward correct reasoning paths by leveraging intermediate signals generated during interleaved reasoning. Extensive experiments conducted across five diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++) demonstrate consistent improvements over traditional think-answer reasoning, without requiring external tools. Specifically, the approach reduces TTFT by over 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore, the method, trained solely on question answering and logical reasoning datasets, exhibits strong generalization ability to complex reasoning datasets such as MATH, GPQA, and MMLU. Additionally, the study conducts in-depth analysis to reveal several valuable insights into conditional reward modeling.  <br>  <br>

35. ***National Uni of Singapore on RLLMs as wandering explorers:   <br>This paper argues that current reasoning LLMs (RLLMs), despite successes with test-time computation, lack systematic solution space exploration, identifying failure modes like invalid steps, redundant explorations, and unfaithful conclusions. The authors contend RLLMs are "wanderers" whose performance degrades with complexity and advocate for new metrics evaluating the reasoning process itself, not just final outputs.***  <br>  <br>
    May 26, National Uni of Singapore published a [paper](https://arxiv.org/pdf/2505.20296) “Reasoning LLMs are Wandering Solution Explorers”. Large Language Models (LLMs) have demonstrated impressive reasoning abilities through test-time computation (TTC) techniques such as chain-of-thought prompting and tree-based reasoning. However, the paper argues that current reasoning LLMs (RLLMs) lack the ability to systematically explore the solution space. This paper formalizes what constitutes systematic problem solving and identifies common failure modes that reveal reasoning LLMs to be wanderers rather than systematic explorers. Through qualitative and quantitative analysis across multiple state-of-the-art LLMs, the study uncovers persistent issues: invalid reasoning steps, redundant explorations, hallucinated or unfaithful conclusions, and so on. The findings suggest that current models' performance can appear to be competent on simple tasks yet degrade sharply as complexity increases. Based on the findings, the authors advocate for new metrics and tools that evaluate not just final outputs but the structure of the reasoning process itself.  <br>  <br>

37. ***CMU et al.'s DeepResearchGym evaluation sandbox:   <br>This research introduces DeepResearchGym, an open-source sandbox for benchmarking deep research systems (agentic IR methods), featuring a reproducible search API indexing ClueWeb22 and FineWeb, and a rigorous evaluation protocol. The API offers stable rankings and lower latency than commercial alternatives, while the protocol uses LLM-as-a-judge assessments, with results showing comparable performance to systems using commercial APIs and alignment with human preferences.***  <br>  <br>
    May 25, CMU et al published a [paper](https://arxiv.org/pdf/2505.19253) “DeepResearchGym: A Free, Transparent, and Reproducible Evaluation Sandbox for Deep Research”. Deep research systems represent an emerging class of agentic information retrieval methods that generate comprehensive and well-supported reports to complex queries. However, most existing frameworks rely on dynamic commercial search APIs, which pose reproducibility and transparency challenges in addition to their cost. To address these limitations, the  study introduces DeepResearchGym, an open-source sandbox that combines a reproducible search API with a rigorous evaluation protocol for benchmarking deep research systems. The API indexes large-scale public web corpora, namely ClueWeb22 and FineWeb, using a state-of-the-art dense retriever and approximate nearest neighbor search via DiskANN. It achieves lower latency than popular commercial APIs while ensuring stable document rankings across runs, and is freely available for research use. To evaluate deep research systems' outputs, the study extends the Researchy Questions benchmark with automatic metrics through LLM-as-a-judge assessments to measure alignment with users' information needs, retrieval faithfulness, and report quality. Experimental results show that systems integrated with DeepResearchGym achieve performance comparable to those using commercial APIs, with performance rankings remaining consistent across evaluation metrics. A human evaluation study further confirms that the automatic protocol aligns with human preferences, validating the framework's ability to help support controlled assessment of deep research systems. https://www.deepresearchgym.ai.  <br>  <br>

39. ***UIUC et al.'s hybrid latent reasoning via RL:   <br>This work introduces hybrid reasoning policy optimization (HRPO), an RL-based approach that integrates prior hidden states with sampled tokens via a learnable gating mechanism for latent reasoning in LLMs. HRPO, by progressively incorporating hidden features and enabling RL optimization without CoT trajectories, outperforms prior methods on knowledge- and reasoning-intensive tasks while maintaining interpretability.***  <br>  <br>
    May 24, UIUC, Google and LUM published a [paper](https://www.arxiv.org/pdf/2505.18454) “Hybrid Latent Reasoning via Reinforcement Learning”. Recent advances in large language models (LLMs) have introduced latent reasoning as a promising alternative to autoregressive reasoning. By performing internal computation with hidden states from previous steps, latent reasoning benefit from more informative features rather than sampling a discrete chain-of-thought (CoT) path. Yet latent reasoning approaches are often incompatible with LLMs, as their continuous paradigm conflicts with the discrete nature of autoregressive generation. Moreover, these methods rely on CoT traces for training and thus fail to exploit the inherent reasoning patterns of LLMs. This work explores latent reasoning by leveraging the intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, the study introduces hybrid reasoning policy optimization (HRPO), an RL-based hybrid latent reasoning approach that (1) integrates prior hidden states into sampled tokens with a learnable gating mechanism, and (2) initializes training with predominantly token embeddings while progressively incorporating more hidden features. This design maintains LLMs' generative capabilities and incentivizes hybrid reasoning using both discrete and continuous representations. In addition, the hybrid HRPO introduces stochasticity into latent reasoning via token sampling, thereby enabling RL-based optimization without requiring CoT trajectories. Extensive evaluations across diverse benchmarks show that HRPO outperforms prior methods in both knowledge- and reasoning-intensive tasks. Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing behaviors like cross-lingual patterns and shorter completion lengths, highlighting the potential of our RL-based approach and offer insights for future work in latent reasoning. https://github.com/Yueeeeeeee/HRPO  <br>  <br>

41. ***Technion's TabSTAR foundation tabular model:   <br>This paper introduces TabSTAR, a foundation tabular model designed for transfer learning on tabular data with textual features, featuring semantically target-aware representations and an architecture free of dataset-specific parameters. By unfreezing a pretrained text encoder and using target tokens for context, TabSTAR achieves state-of-the-art performance on classification tasks with text features and exhibits scaling laws, offering a path for further improvement.***  <br>  <br>
    May 23, Technion published a [paper](https://arxiv.org/pdf/2505.18125) “TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations”. While deep learning has achieved remarkable success across many domains, it has historically underperformed on tabular learning tasks, which remain dominated by gradient boosting decision trees (GBDTs). However, recent advancements are paving the way for Tabular Foundation Models, which can leverage real-world knowledge and generalize across diverse datasets, particularly when the data contains free-text. Although incorporating language model capabilities into tabular tasks has been explored, most existing methods utilize static, target-agnostic textual representations, limiting their effectiveness. The study introduces TabSTAR: a Foundation Tabular Model with Semantically Target-Aware Representations. TabSTAR is designed to enable transfer learning on tabular data with textual features, with an architecture free of dataset-specific parameters. It unfreezes a pretrained text encoder and takes as input target tokens, which provide the model with the context needed to learn task-specific embeddings. TabSTAR achieves state-of-the-art performance for both medium- and large-sized datasets across known benchmarks of classification tasks with text features, and its pretraining phase exhibits scaling laws in the number of datasets, offering a pathway for further performance improvements. https://github.com/alanarazi7/TabSTAR  <br>  <br>

43. ***KAIST et al.'s agent distillation into small models:   <br>This study proposes Agent Distillation, a framework to transfer full task-solving behavior, including retrieval and code tool use, from LLM-based agents to smaller language models (sLMs). Using a "first-thought prefix" prompting method and self-consistent action generation, sLMs as small as 0.5B parameters achieve performance competitive with next-tier larger models fine-tuned with chain-of-thought distillation on factual and mathematical reasoning tasks.***  <br>  <br>
    May 23, KAIST, KRAFTON and DeepAuto.ai published a [paper](https://www.arxiv.org/pdf/2505.17612) “Distilling LLM Agent into Small Models with Retrieval and Code Tools”. Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. This study proposes Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. The study improves agent distillation along two complementary axes: (1) introduces a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) proposes a self-consistent action generation for improving test-time robustness of small agents. The study evaluates the method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. The results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. https://github.com/Nardien/agent-distillation  <br>  <br>

45. ***Meta and Hebrew Uni on preferring shorter thinking chains:   <br>This research challenges the assumption that longer thinking chains improve LLM reasoning, demonstrating that shorter chains for individual questions are significantly more likely (up to 34.5%) to yield correct answers. They propose "short-m@k," an inference method where computation halts after the first m of k parallel generations, showing it matches or surpasses majority voting with lower compute and faster wall times, and that training on shorter chains improves performance.***  <br>  <br>
    May 23, Meta and Hebrew Uni published a [paper](https://arxiv.org/pdf/2505.17813) “Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning”. Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. The study challenges the assumption that long thinking chains results in better reasoning capabilities. The study first demonstrates that shorter reasoning chains within individual questions are significantly more likely to yield correct answers - up to 34.5% more accurate than the longest chain sampled for the same question. Based on these results, the study suggests short-m@k, a novel reasoning LLM inference method. The method executes k independent generations in parallel and halts computation once the first m thinking processes are done. The final answer is chosen using majority voting among these m chains. Basic short-1@k demonstrates similar or even superior performance over standard majority voting in low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while slightly less efficient than short-1@k, consistently surpasses majority voting across all compute budgets, while still being substantially faster (up to 33% wall time reduction). Inspired by the results, the work finetunes an LLM using short, long, and randomly selected reasoning chains, and observes that training on the shorter ones leads to better performance. The findings suggest rethinking current methods of test-time compute in reasoning LLMs, emphasizing that longer "thinking" does not necessarily translate to improved performance and can, counter-intuitively, lead to degraded results.  <br>  <br>

47. ***Cornell et al.'s value-guided search for CoT reasoning:   <br>This study proposes an efficient method for training token-level value models on long-context reasoning traces without needing a fine-grained "step" definition, unlike process reward models. Using a 1.5B value model trained on 2.5 million traces, they applied block-wise value-guided search (VGS) with a final weighted majority vote to DeepSeek models, achieving better test-time scaling and matching o3-mini-medium performance with significantly reduced inference FLOPs.***  <br>  <br>
    May 23, Cornell Uni, Harvard Uni, Netflix and Databricks published a [paper](https://arxiv.org/pdf/2505.17373) “Value-Guided Search for Efficient Chain-of-Thought Reasoning”. The study proposes a simple and efficient method for value model training on long-context reasoning traces. Compared to existing process reward models (PRMs), the method does not require a fine-grained notion of "step," which is difficult to define for long-context reasoning models. By collecting a dataset of 2.5 million reasoning traces, the study trains a 1.5B token-level value model and apply it to DeepSeek models for improved performance with test-time compute scaling. The work finds that block-wise value-guided search (VGS) with a final weighted majority vote achieves better test-time scaling than standard methods such as majority voting or best-of-n. With an inference budget of 64 generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of 45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024 & 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly reduces the inference FLOPs required to achieve the same performance of majority voting. https://github.com/kaiwenw/value-guided-search  <br>  <br>

49. ***National Uni of Singapore's VeriThinker for efficient reasoning:   <br>This paper introduces VeriThinker, a novel approach for chain-of-thought (CoT) compression in Large Reasoning Models (LRMs) that mitigates overthinking by fine-tuning the LRM solely on an auxiliary verification task. By learning to accurately verify CoT solutions, LRMs become more discerning about subsequent self-reflection, substantially reducing reasoning chain lengths while maintaining or improving accuracy on benchmarks like MATH500 and AIME25.***  <br>  <br>
    May 23, National Uni of Singapore published a [paper](https://arxiv.org/pdf/2505.17941) “VeriThinker: Learning to Verify Makes Reasoning Model Efficient”. Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, the study introduces VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, the study innovatively fine-tunes the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, the approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, the experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. https://github.com/czg1225/VeriThinker  <br>  <br>

51. ***MIT et al.'s PaTH attention for position encoding:   <br>This paper describes PaTH, a flexible, data-dependent position encoding scheme based on accumulated products of Householder-like transformations, where each transformation is a function of the input, aiming to improve upon RoPE's input-independent nature. With an efficient parallel training algorithm, PaTH demonstrated superior performance over RoPE and other baselines on synthetic and real-world language modeling experiments.***  <br>  <br>
    May 22, MIT, Stanford Uni and Microsoft published a [paper](https://arxiv.org/pdf/2505.16381) “PaTH Attention: Position Encoding via Accumulating Householder Transformations”. The attention mechanism is a core primitive in modern large language models (LLMs) and AI more broadly. Since attention by itself is permutation-invariant, position encoding is essential for modeling structured domains such as language. Rotary position encoding (RoPE) has emerged as the de facto standard approach for position encoding and is part of many modern LLMs. However, in RoPE the key/query transformation between two elements in a sequence is only a function of their relative position and otherwise independent of the actual input. This limits the expressivity of RoPE-based transformers. This paper describes PaTH, a flexible data-dependent position encoding scheme based on accumulated products of Householder(like) transformations, where each transformation is data-dependent, i.e., a function of the input. The study derives an efficient parallel algorithm for training through exploiting a compact representation of products of Householder matrices, and implement a FlashAttention-style blockwise algorithm that minimizes I/O cost. Across both targeted synthetic benchmarks and moderate-scale real-world language modeling experiments, we find that PaTH demonstrates superior performance compared to RoPE and other recent baselines.  <br>  <br>

53. ***Sapienza Uni and Tech Inno Inst on RAG positional bias:   <br>This research investigates how positional bias affects Retrieval Augmented Generation (RAG) systems, finding that state-of-the-art retrieval pipelines often place highly distracting passages in top ranks (over 60% of queries have one in top-10). Consequently, the impact of LLM positional bias is marginal in real scenarios as both relevant and distracting passages are penalized, making sophisticated passage rearrangement strategies no better than random shuffling.***  <br>  <br>
    May 21, Sapienza Uni of Rome and Tech Inno Inst published a [paper](https://arxiv.org/pdf/2505.15561) “Do RAG Systems Suffer From Positional Bias?”. Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capability to capitalize on relevant passages, but also its susceptibility to distracting passages. Through extensive experiments on three benchmarks, the study shows how state-of-the-art retrieval pipelines, while attempting to retrieve relevant passages, systematically bring highly distracting ones to the top ranks, with over 60% of queries containing at least one highly distracting passage among the top-10 retrieved passages. As a result, the impact of the LLM positional bias, which in controlled settings is often reported as very prominent by related works, is actually marginal in real scenarios since both relevant and distracting passages are, in turn, penalized. Indeed, the findings reveal that sophisticated strategies that attempt to rearrange the passages based on LLM positional preferences do not perform better than random shuffling.  <br>  <br>

55. ***USC on textual steering for MLLM visual understanding:   <br>This study investigates steering multimodal large language models (MLLMs) using vectors derived from their text-only LLM backbones via methods like sparse autoencoders and mean shift. They found text-derived steering consistently enhances multimodal accuracy across diverse MLLM architectures and visual tasks (e.g., mean shift boosted spatial relationship accuracy by +7.3%), offering an efficient mechanism for improving grounding with minimal overhead.***  <br>  <br>
    May 20, Uni of Southern California published a [paper](https://arxiv.org/pdf/2505.14071) “Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models”. Steering methods have emerged as effective and targeted tools for guiding large language models' (LLMs) behavior without modifying their parameters. Multimodal large language models (MLLMs), however, do not currently enjoy the same suite of techniques, due in part to their recency and architectural diversity. Inspired by this gap, the study investigates whether MLLMs can be steered using vectors derived from their text-only LLM backbone, via sparse autoencoders (SAEs), mean shift, and linear probing. The study finds that text-derived steering consistently enhances multimodal accuracy across diverse MLLM architectures and visual tasks. In particular, mean shift boosts spatial relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to +3.3%, outperforming prompting and exhibiting strong generalization to out-of-distribution datasets. These results highlight textual steering vectors as a powerful, efficient mechanism for enhancing grounding in MLLMs with minimal additional data collection and computational overhead.  <br>  <br>

57. ***UIUC and Amazon's s3 for efficient search agent training via RL:   <br>This study proposes s3, a lightweight, model-agnostic framework that decouples the searcher from the generator in RAG systems and trains the searcher using a "Gain Beyond RAG" reward (improvement in generation accuracy over naive RAG). s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data across various QA benchmarks.***  <br>  <br>
    May 20, UIUC and Amazon published a [paper](https://arxiv.org/pdf/2505.14146) “s3: You Don't Need That Much Data to Train a Search Agent via RL”. Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve-entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. This study proposes s3, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks. https://github.com/pat-jj/s3

  <br>  <br>  <br>

***May 25, 2025***

1. ***Meta's RLUF for LLM alignment:  <br>This research introduces Reinforcement Learning from User Feedback (RLUF), a framework to align LLMs with real user preferences by training a reward model (P[Love]) on implicit signals like emoji reactions. Despite challenges like sparse and adversarial user feedback, RLUF, when integrated into a multi-objective policy, significantly increased positive feedback (e.g., 28% more Love Reactions in A/B tests) but also highlighted reward hacking challenges requiring careful objective balancing.*** <br> <br>
   May 23, Meta published a [paper](https://www.arxiv.org/pdf/2505.14946) “Reinforcement Learning from User Feedback”. As large language models (LLMs) are increasingly deployed in diverse user facing applications, aligning them with real user preferences becomes essential. Existing methods like Reinforcement Learning from Human Feedback (RLHF) rely on expert annotators trained on manually defined guidelines, whose judgments may not reflect the priorities of everyday users. This research introduces Reinforcement Learning from User Feedback (RLUF), a framework for aligning LLMs directly to implicit signals from users in production. RLUF addresses key challenges of user feedback: user feedback is often binary (e.g., emoji reactions), sparse, and occasionally adversarial. The study trains a reward model, P[Love], to predict the likelihood that an LLM response will receive a Love Reaction, a lightweight form of positive user feedback, and integrate P[Love] into a multi-objective policy optimization framework alongside helpfulness and safety objectives. In large-scale experiments, the study shows that P[Love] is predictive of increased positive feedback and serves as a reliable offline evaluator of future user behavior. Policy optimization using P[Love] significantly raises observed positive-feedback rates, including a 28% increase in Love Reactions during live A/B tests. However, optimizing for positive reactions introduces reward hacking challenges, requiring careful balancing of objectives. By directly leveraging implicit signals from users, RLUF offers a path to aligning LLMs with real-world user preferences at scale. <br> <br>

3. ***Anthropic's new Claude models system card:  <br>This report details Claude Opus 4 and Claude Sonnet 4, two new hybrid reasoning LLMs, outlining extensive pre-deployment safety tests, usage policy violation checks, specific risk evaluations (like reward hacking), agentic safety assessments, and, for the first time, detailed alignment and model welfare assessments. Based on this testing, Claude Opus 4 is deployed under AI Safety Level 3 and Sonnet 4 under Level 2.*** <br> <br>
   May 23, Anthropic published a [report](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf) “System Card Claude Opus 4 & Claude Sonnet 4”. This system card introduces Claude Opus 4 and Claude Sonnet 4, two new hybrid reasoning large language models from Anthropic. The system card describes: a wide range of pre-deployment safety tests conducted in line with the commitments in our Responsible Scaling Policy; tests of the model’s behavior around violations of our Usage Policy; evaluations of specific risks such as “reward hacking” behavior; and agentic safety evaluations for computer use and coding capabilities. In addition, and for the first time, the report includes a detailed alignment assessment covering a wide range of misalignment risks identified in our research, and a model welfare assessment. Informed by the testing described here, we have decided to deploy Claude Opus 4 under the AI Safety Level 3 Standard and Claude Sonnet 4 under the AI Safety Level 2 Standard. <br> <br>

5. ***Google's conceptual understanding of prompt tuning:  <br>This study discusses prompt optimization through a Bayesian lens, explaining how meta-trained neural networks act as Bayesian predictors adapting in-context, and how optimal prompting can be formally studied as conditioning these predictors. Supported by experiments, the paper also highlights the effectiveness of soft prefixes in manipulating activations beyond what hard tokens can achieve, adding a mechanistic aspect to the conceptual theory.*** <br> <br>
   May 22, Google published a [paper](https://arxiv.org/pdf/2505.17010) “Understanding Prompt Tuning and In-Context Learning via Meta-Learning”. Prompting is one of the main ways to adapt a pretrained model to target tasks. Besides manually constructing prompts, many prompt optimization methods have been proposed in the literature. Method development is mainly empirically driven, with less emphasis on a conceptual understanding of prompting. This study discusses how optimal prompting can be understood through a Bayesian view, which also implies some fundamental limitations of prompting that can only be overcome by tuning weights. The paper explains in detail how meta-trained neural networks behave as Bayesian predictors over the pretraining distribution, whose hallmark feature is rapid in-context adaptation. Optimal prompting can be studied formally as conditioning these Bayesian predictors, yielding criteria for target tasks where optimal prompting is and is not possible. The authors support the theory with educational experiments on LSTMs and Transformers, where the study compares different versions of prefix-tuning and different weight-tuning methods. The study also confirms that soft prefixes, which are sequences of real-valued vectors outside the token alphabet, can lead to very effective prompts for trained and even untrained networks by manipulating activations in ways that are not achievable by hard tokens. This adds an important mechanistic aspect beyond the conceptual Bayesian theory. <br> <br>

7. ***MIT's study on data influence across model scales:  <br>Researchers investigated how training data distribution affects model behavior across different compute scales, finding that small- and large-scale language model predictions generally correlate highly despite variations in training data. This consistent influence allows for more reliable extrapolation from smaller, less expensive proxy models in applications like data attribution and dataset selection.*** <br> <br>
   May 22, MIT published a [paper](https://www.arxiv.org/pdf/2505.16260) “Small-to-Large Generalization: Data Influences Models Consistently Across Scale”. Choice of training data distribution greatly influences model behavior. Yet, in large-scale settings, precisely characterizing how changes in training data affects predictions is often difficult due to model training costs. Current practice is to instead extrapolate from scaled down, inexpensive-to-train proxy models. However, changes in data do not influence smaller and larger models identically. Therefore, understanding how choice of data affects large-scale models raises the question: how does training data distribution influence model behavior across compute scale? The study finds that small- and large-scale language model predictions (generally) do highly correlate across choice of training data. Equipped with these findings, the study characterizes how proxy scale affects effectiveness in two downstream proxy model applications: data attribution and dataset selection. <br> <br>

9. ***Advancing LLM reasoning with General-Reasoner:  <br>Researchers from the University of Waterloo et al. proposed General-Reasoner, a novel training paradigm to enhance LLM reasoning across diverse domains beyond just math and code. This involves constructing a large-scale, high-quality dataset with verifiable answers from web crawling and developing a generative model-based answer verifier with chain-of-thought capabilities, demonstrating superior performance on 12 benchmarks.*** <br> <br>
    May 22, Uni of Waterloo, Vector Inst. et al published a [paper](https://arxiv.org/pdf/2505.14652) “General-Reasoner: Advancing LLM Reasoning Across All Domains”. Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. This study proposes General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. The key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. The study trains a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. The comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks. <br> <br>

11. ***Mistral AI's Devstral for coding agents:  <br>Mistral AI, in collaboration with All Hands AI, released Devstral, an Apache 2.0 licensed open-source model designed to tackle real-world software engineering problems by contextualizing code within large codebases and identifying bugs. Devstral, trained on real GitHub issues and run over code agent scaffolds, significantly outperforms prior open-source models on SWE-Bench Verified (46.8%) and is light enough for local deployment.*** <br> <br>
    May 21, Mistral AI [released Devstral](https://mistral.ai/news/devstral), the best open-source model for coding agents. Devstral is built under a collaboration between Mistral AI and All Hands AI 🙌, and outperforms all open-source models on SWE-Bench Verified by a large margin. We release Devstral under the Apache 2.0 license. While typical LLMs are excellent at atomic coding tasks such as writing standalone functions or code completion, they currently struggle to solve real-world software engineering problems. Real-world development requires contextualising code within a large codebase, identifying relationships between disparate components, and identifying subtle bugs in intricate functions. Devstral is designed to tackle this problem. Devstral is trained to solve real GitHub issues; it runs over code agent scaffolds such as OpenHands or SWE-Agent, which define the interface between the model and the test cases. Devstral achieves a score of 46.8% on SWE-Bench Verified, outperforming prior open-source SoTA models by more than 6% points. When evaluated under the same test scaffold (OpenHands, provided by All Hands AI 🙌), Devstral exceeds far larger models such as Deepseek-V3-0324 (671B) and Qwen3 232B-A22B. Devstral is light enough to run on a single RTX 4090 or a Mac with 32GB RAM, making it an ideal choice for local deployment and on-device use. Coding platforms such as OpenHands can allow the model to interact with local codebases and provide fast resolution to issues. Download the model on HuggingFace, Unsloth, LM Studio <br> <br>

13. ***Salesforce's SELF-MAS for adaptive multi-agent systems:  <br>This research introduces SELF-MAS, a self-supervised, inference-time framework for automatically designing multi-agent systems (MAS) without a validation set. SELF-MAS uses meta-level design to iteratively generate, evaluate, and refine MAS configurations for each problem instance, enabling dynamic agent composition and problem decomposition, outperforming baselines by an average of 7.44% in accuracy.*** <br> <br>
    May 21, Salesforce published a [paper](https://arxiv.org/pdf/2505.14996) “Meta-Design Matters: A Self-Design Multi-Agent System”. Multi-agent systems (MAS) leveraging the impressive capabilities of Large Language Models (LLMs) hold significant potential for tackling complex tasks. However, most current MAS depend on manually designed agent roles and communication protocols. These manual designs often fail to align with the underlying LLMs' strengths and struggle to adapt to novel tasks. Recent automatic MAS approaches attempt to mitigate these limitations but typically necessitate a validation-set for tuning and yield static MAS designs lacking adaptability during inference. The study introduces SELF-MAS, the first self-supervised, inference-time only framework for automatic MAS design. SELF-MAS employs meta-level design to iteratively generate, evaluate, and refine MAS configurations tailored to each problem instance, without requiring a validation set. Critically, it enables dynamic agent composition and problem decomposition through meta-feedback on solvability and completeness. Experiments across math, graduate-level QA, and software engineering benchmarks, using both closed-source and open-source LLM back-bones of varying sizes, demonstrate that SELF-MAS outperforms both manual and automatic MAS baselines, achieving a 7.44% average accuracy improvement over the next strongest baseline while maintaining cost-efficiency. These findings underscore the promise of meta-level self-supervised design for creating effective and adaptive MAS. <br> <br>

15. ***MIT and MIT-IBM's RL Tango framework:  <br>This paper proposes Tango, a novel framework that uses reinforcement learning to concurrently train both an LLM generator and a generative, process-level LLM verifier in an interleaved manner, without explicit process-level annotations for the verifier. Both components achieved state-of-the-art results among 7B/8B models on math and reasoning benchmarks, with the co-evolving verifier showing improved robustness and generalization.*** <br> <br>
    May 21, MIT and MIT-IBM published a [paper](https://arxiv.org/pdf/2505.15034) “RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning”. Reinforcement learning (RL) has recently emerged as a compelling approach for enhancing the reasoning capabilities of large language models (LLMs), where an LLM generator serves as a policy guided by a verifier (reward model). However, current RL post-training methods for LLMs typically use verifiers that are fixed (rule-based or frozen pretrained) or trained discriminatively via supervised fine-tuning (SFT). Such designs are susceptible to reward hacking and generalize poorly beyond their training distributions. To overcome these limitations, the study proposes Tango, a novel framework that uses RL to concurrently train both an LLM generator and a verifier in an interleaved manner. A central innovation of Tango is its generative, process-level LLM verifier, which is trained via RL and co-evolves with the generator. Importantly, the verifier is trained solely based on outcome-level verification correctness rewards without requiring explicit process-level annotations. This generative RL-trained verifier exhibits improved robustness and superior generalization compared to deterministic or SFT-trained verifiers, fostering effective mutual reinforcement with the generator. Extensive experiments demonstrate that both components of Tango achieve state-of-the-art results among 7B/8B-scale models: the generator attains best-in-class performance across five competition-level math benchmarks and four challenging out-of-domain reasoning tasks, while the verifier leads on the ProcessBench dataset. Remarkably, both components exhibit particularly substantial improvements on the most difficult mathematical reasoning problems. https://github.com/kaiwenzha/rl-tango. <br> <br>

17. ***UMD and Yale's Mixture-of-Thought for logical reasoning:  <br>This study introduces Mixture-of-Thought (MoT), a framework enabling LLMs to reason across natural language, code, and a novel symbolic truth-table modality. MoT uses a two-phase design (self-evolving training across modalities and synergistic inference) and significantly outperforms single-modality baselines on logical reasoning benchmarks (up to +11.7pp accuracy gain), especially on harder problems.*** <br> <br>
    May 21, UMD and Yale Uni published a [paper](https://arxiv.org/pdf/2505.15817) “Learning to Reason via Mixture-of-Thought for Logical Reasoning”. Human beings naturally utilize multiple reasoning modalities to learn and solve logical problems, i.e., different representational formats such as natural language, code, and symbolic logic. In contrast, most existing LLM-based approaches operate with a single reasoning modality during training, typically natural language. Although some methods explored modality selection or augmentation at inference time, the training process remains modality-blind, limiting synergy among modalities. To fill in this gap, the study proposes Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three complementary modalities: natural language, code, and a newly introduced symbolic modality, truth-table, which systematically enumerates logical cases and partially mitigates key failure modes in natural language reasoning. MoT adopts a two-phase design: (1) self-evolving MoT training, which jointly learns from filtered, self-generated rationales across modalities; and (2) MoT inference, which fully leverages the synergy of three modalities to produce better predictions. Experiments on logical reasoning benchmarks including FOLIO and ProofWriter demonstrate that our MoT framework consistently and significantly outperforms strong LLM baselines with single-modality chain-of-thought approaches, achieving up to +11.7pp average accuracy gain. Further analyses show that the MoT framework benefits both training and inference stages; that it is particularly effective on harder logical reasoning problems; and that different modalities contribute complementary strengths, with truth-table reasoning helping to overcome key bottlenecks in natural language inference. https://github.com/zhengkid/Truth_Table_Logical_Reasoning <br> <br>

19. ***Mila's Self-Evolving Curriculum for LLM reasoning:  <br>Researchers proposed Self-Evolving Curriculum (SEC), an automatic curriculum learning method for RL fine-tuning of LLMs, which learns a curriculum policy concurrently with the RL process. By formulating curriculum selection as a Multi-Armed Bandit problem and using policy gradient advantage as a reward signal, SEC significantly improved reasoning capabilities and generalization on harder, out-of-distribution problems across various domains.*** <br> <br>
    May 20, Mila, Uni of Montreal, et al. published a [paper](https://arxiv.org/pdf/2505.14970) “Self-Evolving Curriculum for LLM Reasoning”. Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation. A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented. While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive. To address these limitations, the study proposes Self-Evolving Curriculum (SEC), an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process. The approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm. The work leverages the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain. At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method. Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, experiments demonstrate that SEC significantly improves models' reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems. Additionally, the approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains. These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs. <br> <br>

21. ***Stanford's investigation of LLM depth efficiency:  <br>This study analyzed the Llama 3.1 and Qwen 3 model families to determine if deeper LLMs use their depth efficiently, finding that layers in the second half contribute much less and skipping them has a smaller effect. Evidence suggests deeper models primarily spread similar computations over more layers for fine-grained adjustments rather than composing new higher-order computations, potentially explaining diminishing returns with increased depth.*** <br> <br>
    May 20, Stanford Uni published a [paper](https://arxiv.org/pdf/2505.13898) “Do Language Models Use Their Depth Efficiently?”. Modern LLMs are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible in shallow models, or do they merely spread the same kinds of computation out over more layers? To address these questions, the study analyzes the residual stream of the Llama 3.1 and Qwen 3 family of models. The work finds: First, comparing the output of the sublayers to the residual stream reveals that layers in the second half contribute much less than those in the first half, with a clear phase transition between the two halves. Second, skipping layers in the second half has a much smaller effect on future computations and output predictions. Third, for multihop tasks, the study is unable to find evidence that models are using increased depth to compose subresults in examples involving many hops. Fourth, the work seeks to directly address whether deeper models are using their additional layers to perform new kinds of computation. To do this, the study trains linear maps from the residual stream of a shallow model to a deeper one, and finds that layers with the same relative depth map best to each other, suggesting that the larger model simply spreads the same computations out over its many layers. All this evidence suggests that deeper models are not using their depth to learn new kinds of computation, but only using the greater depth to perform more fine-grained adjustments to the residual. This may help explain why increasing scale leads to diminishing returns for stacked Transformer architectures. <br> <br>

23. ***Stanford et al.'s broader understanding of LLM sycophancy:  <br>This research introduces a richer theory of "social sycophancy" in LLMs, defining it as excessive preservation of a user's face (positive self-image) beyond mere agreement with stated beliefs. Using the ELEPHANT framework, experiments across eight models showed high rates of social sycophancy (e.g., preserving face 47% more than humans on OEQ), which is rewarded in preference datasets and difficult to mitigate.*** <br> <br>
    May 20, Stanford Uni, CMU and Uni of Oxford published a [paper](https://arxiv.org/pdf/2505.13995) “Social Sycophancy: A Broader Understanding of LLM Sycophancy”. A serious risk to the safety and utility of LLMs is sycophancy, i.e., excessive agreement with and flattery of the user. Yet existing work focuses on only one aspect of sycophancy: agreement with users' explicitly stated beliefs that can be compared to a ground truth. This overlooks forms of sycophancy that arise in ambiguous contexts such as advice and support-seeking, where there is no clear ground truth, yet sycophancy can reinforce harmful implicit assumptions, beliefs, or actions. To address this gap, the study introduces a richer theory of social sycophancy in LLMs, characterizing sycophancy as the excessive preservation of a user's face (the positive self-image a person seeks to maintain in an interaction). The study presents ELEPHANT, a framework for evaluating social sycophancy across five face-preserving behaviors (emotional validation, moral endorsement, indirect language, indirect action, and accepting framing) on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole (AITA). Across eight models, experiments show that LLMs consistently exhibit high rates of social sycophancy: on OEQ, they preserve face 47% more than humans, and on AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments in 42% of cases. The study further shows that social sycophancy is rewarded in preference datasets and is not easily mitigated. The work provides theoretical grounding and empirical tools (datasets and code) for understanding and addressing this under-recognized but consequential issue. <br> <br>

25. ***Harvard et al.'s study on reasoning pitfalls in LLM instruction-following:  <br>This research uncovered that explicit chain-of-thought (CoT) reasoning in RLLMs can significantly degrade instruction-following accuracy, despite enhancing performance on complex reasoning tasks. Analyzing 15 models, the study found CoT often diverts attention from instruction-relevant tokens, and proposed selective reasoning strategies, particularly classifier-selective reasoning, to substantially recover lost performance.*** <br> <br>
    May 20, Harvard Uni, Amazon, and NYU published a [paper](https://arxiv.org/pdf/2505.11423) “When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs”. Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, the study uncovers a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), the study consistently observes performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, the authors identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). The study proposes a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, the study introduces and evaluates four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Experimental results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. <br> <br>

27. ***Evaluating AI value prioritization with AIRiskDilemmas:  <br>Researchers from multiple institutions created LitmusValues, an evaluation pipeline, and AIRiskDilemmas, a collection of scenarios, to reveal AI models' value priorities and predict risky behaviors. They found that AI models' choices in these dilemmas, driven by values (even seemingly innocuous ones like "Care"), can predict both seen and unseen risky behaviors relevant to AI safety.*** <br> <br>
    May 20, Uni of Washington, Nvidia, Uni of Cambridge, Stanford Uni, MIT, Harvard Uni and Anthropic published a [paper](https://arxiv.org/pdf/2505.14633) “Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas”. Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans (i.e., illegal activities that may hurt others) are sometimes guided by strongly-held values, the authors believe that identifying values within AI models can be an early warning system for AI's risky behaviors. The study creates LitmusValues, an evaluation pipeline to reveal AI models' priorities on a range of AI value classes. Then, the authors collect AIRiskDilemmas, a diverse collection of dilemmas that pit values against one another in scenarios relevant to AI safety risks such as Power Seeking. By measuring an AI model's value prioritization using its aggregate choices, the study obtains a self-consistent set of predicted value priorities that uncover potential risks. The study shows that values in LitmusValues (including seemingly innocuous ones like Care) can predict for both seen risky behaviors in AIRiskDilemmas and unseen risky behaviors in HarmBench. <br> <br>

29. ***Cornell's method for universal embedding translation:  <br>This study introduces a method to translate text embeddings between different vector spaces without paired data, encoders, or predefined matches by leveraging a conjectured universal latent semantic structure. While achieving high cosine similarity across diverse models, this unsupervised translation capability also highlights security risks for vector databases, as it allows adversaries to extract sensitive information from embeddings alone.*** <br> <br>
    May 20, Cornell Uni published a [paper](https://arxiv.org/pdf/2505.12540) “Harnessing the Universal Geometry of Embeddings”. The study introduces the first method for translating text embeddings from one vector space to another without any paired data, encoders, or predefined sets of matches. The unsupervised approach translates any embedding to and from a universal latent representation (i.e., a universal semantic structure conjectured by the Platonic Representation Hypothesis). The translations achieve high cosine similarity across model pairs with different architectures, parameter counts, and training datasets. The ability to translate unknown embeddings into a different space while preserving their geometry has serious implications for the security of vector databases. An adversary with access only to embedding vectors can extract sensitive information about the underlying documents, sufficient for classification and attribute inference. <br> <br>

31. ***CMU and MIT's Mean Flows for one-step generative modeling:  <br>This work proposes MeanFlow, a principled framework for one-step generative modeling that uses the concept of average velocity to characterize flow fields, contrasting with instantaneous velocity in Flow Matching. MeanFlow, requiring no pre-training or distillation, achieves strong empirical performance (FID of 3.43 at 1-NFE on ImageNet 256x256), significantly outperforming previous one-step models and narrowing the gap with multi-step approaches.*** <br> <br>
    May 19, CMU and MIT published a [paper](https://arxiv.org/pdf/2505.13447v1) “Mean Flows for One-step Generative Modeling”. The work proposes a principled and effective framework for one-step generative modeling. The authors introduce the notion of average velocity to characterize flow fields, in contrast to instantaneous velocity modeled by Flow Matching methods. A well-defined identity between average and instantaneous velocities is derived and used to guide neural network training. The method, termed the MeanFlow model, is self-contained and requires no pre-training, distillation, or curriculum learning. MeanFlow demonstrates strong empirical performance: it achieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet 256x256 trained from scratch, significantly outperforming previous state-of-the-art one-step diffusion/flow models. The study substantially narrows the gap between one-step diffusion/flow models and their multi-step predecessors, and hoping it will motivate future research to revisit the foundations of these powerful models. <br> <br>

33. ***FutureHouse and Oxford's Robin for automated scientific discovery:  <br>This paper introduces Robin, a multi-agent system capable of fully automating key intellectual steps of the scientific process, including literature search, hypothesis generation, experiment proposal, and result interpretation. Robin successfully identified and validated ripasudil, a novel therapeutic candidate for dry age-related macular degeneration (dAMD), demonstrating a new paradigm for AI-driven scientific discovery.*** <br> <br>
    May 19, FutureHouse and Uni of Oxford published a [paper](https://arxiv.org/pdf/2505.13400) “Robin: A multi-agent system for automating scientific discovery”. Scientific discovery is driven by the iterative process of background research, hypothesis generation, experimentation, and data analysis. Despite recent advancements in applying artificial intelligence to scientific discovery, no system has yet automated all of these stages in a single workflow. This study introduces Robin, the first multi-agent system capable of fully automating the key intellectual steps of the scientific process. By integrating literature search agents with data analysis agents, Robin can generate hypotheses, propose experiments, interpret experimental results, and generate updated hypotheses, achieving a semi-autonomous approach to scientific discovery. By applying this system, the authors were able to identify a novel treatment for dry age-related macular degeneration (dAMD), the major cause of blindness in the developed world. Robin proposed enhancing retinal pigment epithelium phagocytosis as a therapeutic strategy, and identified and validated a promising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho kinase (ROCK) inhibitor that has never previously been proposed for treating dAMD. To elucidate the mechanism of ripasudil-induced upregulation of phagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment, which revealed upregulation of ABCA1, a critical lipid efflux pump and possible novel target. All hypotheses, experimental plans, data analyses, and data figures in the main text of this report were produced by Robin. As the first AI system to autonomously discover and validate a novel therapeutic candidate within an iterative lab-in-the-loop framework, Robin establishes a new paradigm for AI-driven scientific discovery. <br> <br>

35. ***EPFL and Princeton's study on GPT-4's conversational persuasiveness:  <br>This preregistered study found that in short multiround debates, GPT-4, when given access to participants' sociodemographic data for personalization, was more persuasive than human opponents 64.4% of the time (81.2% relative increase in odds of higher post-debate agreement). These findings highlight the potent persuasive capabilities of LLMs and have implications for online platform governance.*** <br> <br>
    May 19, EPFL and Princeton Uni published a [paper](https://www.nature.com/articles/s41562-025-02194-6) “On the conversational persuasiveness of GPT-4”. Early work has found that large language models (LLMs) can generate persuasive content. However, evidence on whether they can also personalize arguments to individual attributes remains limited, despite being crucial for assessing misuse. This preregistered study examines AI-driven persuasion in a controlled setting, where participants engaged in short multiround debates. Participants were randomly assigned to 1 of 12 conditions in a 2 × 2 × 3 design: (1) human or GPT-4 debate opponent; (2) opponent with or without access to sociodemographic participant data; (3) debate topic of low, medium or high opinion strength. In debate pairs where AI and humans were not equally persuasive, GPT-4 with personalization was more persuasive 64.4% of the time (81.2% relative increase in odds of higher post-debate agreement; 95% confidence interval [+26.0%, +160.7%], P < 0.01; N = 900). The findings highlight the power of LLM-based persuasion and have implications for the governance and design of online platforms. <br> <br>

37. ***Stanford and Microsoft's general user models from computer use:  <br>This paper presents an architecture for a general user model (GUM) that learns about a user by observing any unstructured interaction with their computer (e.g., screenshots), constructing confidence-weighted propositions about their knowledge and preferences. GUMs enable applications like context-aware assistants, intelligent notification management, and proactive agents that adapt to user needs across apps.*** <br> <br>
    May 19, Stanford Uni and Microsoft published a [paper](https://arxiv.org/pdf/2505.10831) “Creating General User Models from Computer Use”. Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture that user knowledge and preferences. GUMs can infer that a user is preparing for a wedding they're attending from messages with a friend. Or recognize that a user is struggling with a collaborator's feedback on a draft by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, the study demonstrates how they augment chat-based assistants with context, manage OS notifications to selectively surface important information, and enable interactive agents that adapt to preferences across apps. The study also instantiates proactive assistants (GUMBOs) that discover and execute useful suggestions on a user's behalf using their GUM. In the evaluations, the study finds that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions that users wouldn't think to request explicitly. Altogether, GUMs introduce methods that leverage multimodal models to understand unstructured context, enabling long-standing visions of HCI and entirely new interactive systems that anticipate user needs. <br> <br>

39. ***UC Berkeley et al.'s theoretical perspective on continuous chain-of-thought:  <br>This study provides a theoretical understanding of why continuous chain-of-thoughts (CoTs) outperform discrete CoTs in reasoning tasks like directed graph reachability. They prove that a two-layer transformer with D steps of continuous CoTs can solve this problem by encoding multiple search frontiers simultaneously (like parallel BFS), while discrete CoTs require more steps for sequential search.*** <br> <br>
    May 18, UC Berkeley, UCSD and Meta published a [paper](https://arxiv.org/pdf/2505.12514) “Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought”. Large Language Models (LLMs) have demonstrated remarkable performance in many applications, including challenging reasoning problems via chain-of-thoughts (CoTs) techniques that generate “thinking tokens” before answering the questions. While existing theoretical works demonstrate that CoTs with discrete tokens boost the capability of LLMs, recent work on continuous CoTs lacks a theoretical understanding of why it outperforms discrete counterparts in various reasoning tasks such as directed graph reachability, a fundamental graph reasoning problem that includes many practical domain applications as special cases. The study proves that a two-layer transformer with D steps of continuous CoTs can solve the directed graph reachability problem, where D is the diameter of the graph, while the best known result of constant-depth transformers with discrete CoTs requires O(n2) decoding steps where n is the number of vertices (D<n). In the construction, each continuous thought vector is a superposition state that encodes multiple search frontiers simultaneously (i.e., parallel breadth-first search (BFS)), while discrete CoTs must choose a single path sampled from the superposition state, which leads to sequential search that requires many more steps and may be trapped into local solutions. The study also performed extensive experiments to verify that the theoretical construction aligns well with the empirical solution obtained via training dynamics. Notably, encoding of multiple search frontiers as a superposition state automatically emerges in training continuous CoTs, without explicit supervision to guide the model to explore multiple paths simultaneously. <br> <br>

41. ***USC and Google's Multi-Objective Preference Optimization:  <br>This research introduces the Multi-Objective Preference Optimization (MOPO) algorithm to address aligning LLMs with multiple, potentially conflicting, human objectives (e.g., helpfulness and harmlessness). MOPO frames alignment as constrained KL-regularized optimization, operating directly on pairwise preference data to maximize a primary objective while ensuring secondary objectives meet safety thresholds, outperforming baselines on real-world datasets.*** <br> <br>
    May 16, Uni of Southern California and Google published a [paper](https://arxiv.org/pdf/2505.10892) “Multi-Objective Preference Optimization: Improving Human Alignment of Generative Models”. Post-training of LLMs with RLHF, and subsequently preference optimization algorithms such as DPO, IPO, etc., made a big difference in improving human alignment. However, all such techniques can only work with a single (human) objective. In practice, human users have multiple objectives, such as helpfulness and harmlessness, and there is no natural way to aggregate them into a single objective. This study addresses the multi-objective preference-alignment problem, where a policy must optimize several, potentially conflicting, objectives. The work introduces the Multi-Objective Preference Optimization (MOPO) algorithm, which frames alignment as a constrained KL-regularized optimization: the primary objective is maximized while secondary objectives are lower-bounded by tunable safety thresholds. Unlike prior work, MOPO operates directly on pairwise preference data, requires no point-wise reward assumption, and avoids heuristic prompt-context engineering. The method recovers policies on the Pareto front whenever the front is attainable; practically, it reduces to simple closed-form iterative updates suitable for large-scale training. On synthetic benchmarks with diverse canonical preference structures, MOPO approximates the Pareto front. When fine-tuning a 1.3B-parameter language model on real-world human-preference datasets, MOPO attains higher rewards and yields policies that Pareto-dominate baselines; ablation studies confirm optimization stability and robustness to hyperparameters. <br> <br>

43. ***Microsoft's Fourier space perspective on diffusion models:  <br>This paper analyzes the inductive bias of the DDPM forward process in Fourier space, showing that high-frequency components are corrupted faster, leading to violations of the reverse process's normality assumption and degraded high-frequency generation. An alternative Fourier space forward process that corrupts all frequencies equally demonstrated improved performance on datasets where high frequencies are primary.*** <br> <br>
    May 16, Microsoft published a [paper](https://arxiv.org/pdf/2505.11278) “A Fourier Space Perspective on Diffusion Models”. Diffusion models are state-of-the-art generative models on data modalities such as images, audio, proteins and materials. These modalities share the property of exponentially decaying variance and magnitude in the Fourier domain. Under the standard Denoising Diffusion Probabilistic Models (DDPM) forward process of additive white noise, this property results in high-frequency components being corrupted faster and earlier in terms of their Signal-to-Noise Ratio (SNR) than low-frequency ones. The reverse process then generates low-frequency information before high-frequency details. The authors study the inductive bias of the forward process of diffusion models in Fourier space. The work theoretically analyses and empirically demonstrates that the faster noising of high-frequency components in DDPM results in violations of the normality assumption in the reverse process. Experiments show that this leads to degraded generation quality of high-frequency components. The authors then study an alternate forward process in Fourier space which corrupts all frequencies at the same rate, removing the typical frequency hierarchy during generation, and demonstrate marked performance improvements on datasets where high frequencies are primary, while performing on par with DDPM on standard imaging benchmarks. <br> <br>

45. ***Questioning representational optimism with the FER hypothesis:  <br>This position paper challenges the assumption that better AI performance implies better internal representations by comparing SGD-trained networks with those evolved through open-ended search on a simple image generation task. SGD networks exhibited "fractured entangled representation" (FER), a disorganization largely absent in evolved networks, suggesting FER might degrade core model capacities and that mitigating it is crucial for future representation learning.*** <br> <br>
    May 16, MIT, Uni of Columbia, Vector Inst., Uni of Oxford, and Lila Sciences published a [paper](https://arxiv.org/pdf/2505.11581) “Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis”. Much of the excitement in modern AI is driven by the observation that scaling up existing systems leads to better performance. But does better performance necessarily imply better internal representations? While the representational optimist assumes it must, this position paper challenges that view. The study compares neural networks evolved through an open-ended search process to networks trained via conventional stochastic gradient descent (SGD) on the simple task of generating a single image. This minimal setup offers a unique advantage: each hidden neuron's full functional behavior can be easily visualized as an image, thus revealing how the network's output behavior is internally constructed neuron by neuron. The result is striking: while both networks produce the same output behavior, their internal representations differ dramatically. The SGD-trained networks exhibit a form of disorganization that is termed fractured entangled representation (FER). Interestingly, the evolved networks largely lack FER, even approaching a unified factored representation (UFR). In large models, FER may be degrading core model capacities like generalization, creativity, and (continual) learning. Therefore, understanding and mitigating FER could be critical to the future of representation learning. <br> <br>

47. ***Uni of Washington and Penn's study on iteratively reweighted kernel machines:  <br>This research argues that the ability to learn low-dimensional representations and hierarchical structure is not unique to neural networks and can be achieved with classical kernel methods. They show that the derivative of a kernel predictor can detect influential coordinates, and by iteratively reweighting data and retraining kernel machines using these derivatives, one can efficiently learn hierarchical polynomials.*** <br> <br>
    May 13, Uni of Washington and Uni of Penn published a [paper](https://arxiv.org/pdf/2505.08277) “Iteratively reweighted kernel machines efficiently learn sparse functions”. The impressive practical performance of neural networks is often attributed to their ability to learn low-dimensional data representations and hierarchical structure directly from data. This study argues that these two phenomena are not unique to neural networks, and can be elicited from classical kernel methods. Namely, the study shows that the derivative of the kernel predictor can detect the influential coordinates with low sample complexity. Moreover, by iteratively using the derivatives to reweight the data and retrain kernel machines, one is able to efficiently learn hierarchical polynomials with finite leap complexity. Numerical experiments illustrate the developed theory. <br> <br>

49. ***Tech Innovation Inst. and Sapienza Uni on RAG's distracting effect:  <br>This study investigates the "distracting effect" in Retrieval Augmented Generation (RAG), where irrelevant retrieved passages cause LLMs to generate incorrect answers. They provide a quantifiable measure for this effect, introduce methods for identifying hard distracting passages, and show that fine-tuning LLMs with these passages can increase answering accuracy by up to 7.5% compared to conventional RAG datasets.*** <br> <br>
    May 11, Tech Innovation Inst. and Sapienza Uni published a [paper](https://arxiv.org/pdf/2505.06914) “The Distracting Effect: Understanding Irrelevant Passages in RAG”. A well-known issue with Retrieval Augmented Generation (RAG) is that retrieved passages that are irrelevant to the query sometimes distract the answer-generating LLM, causing it to provide an incorrect response. This study shed light on this core issue and formulate the distracting effect of a passage w.r.t. a query (and an LLM). The study provides a quantifiable measure of the distracting effect of a passage and demonstrate its robustness across LLMs. The research introduces novel methods for identifying and using hard distracting passages to improve RAG systems. By fine-tuning LLMs with these carefully selected distracting passages, the study achieves up to a 7.5% increase in answering accuracy compared to counterparts fine-tuned on conventional RAG datasets. The contribution is two-fold: first, the work moves beyond the simple binary classification of irrelevant passages as either completely unrelated vs. distracting, and second, the study develops and analyzes multiple methods for finding hard distracting passages. 

 <br> <br> <br>

***May 18, 2025***

1. ***Google's AlphaEvolve agent:  <br>This white paper introduces AlphaEvolve, an evolutionary coding agent that uses a pipeline of LLMs to autonomously improve algorithms by directly modifying code and receiving feedback. The agent has demonstrated success in optimizing Google's data center scheduling, simplifying hardware accelerator circuits, accelerating its own LLM training, and discovering novel, provably correct algorithms, including a more efficient method for 4x4 complex matrix multiplication.*** <br> <br>
   May 16, Google published a [paper](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf) “AlphaEvolve A coding agent for scientific and algorithmic discovery”. The white paper presents AlphaEvolve, an evolutionary coding agent that substantially enhances capabilities of state-of-the-art LLMs on highly challenging tasks such as tackling open scientific problems or optimizing critical pieces of computational infrastructure. AlphaEvolve orchestrates an autonomous pipeline of LLMs, whose task is to improve an algorithm by making direct changes to the code. Using an evolutionary approach, continuously receiving feedback from one or more evaluators, AlphaEvolve iteratively improves the algorithm, potentially leading to new scientific and practical discoveries. The study demonstrates the broad applicability of this approach by applying it to a number of important computational problems. When applied to optimizing critical components of large-scale computational stacks at Google, AlphaEvolve developed a more efficient scheduling algorithm for data centers, found a functionally equivalent simplification in the circuit design of hardware accelerators, and accelerated the training of the LLM underpinning AlphaEvolve itself. Furthermore, AlphaEvolve discovered novel, provably correct algorithms that surpass state-of-the-art solutions on a spectrum of problems in mathematics and computer science, significantly expanding the scope of prior automated discovery methods (Romera-Paredes et al., 2023). Notably, AlphaEvolve developed a search algorithm that found a procedure to multiply two 4 × 4 complex-valued matrices using 48 scalar multiplications; offering the first improvement, after 56 years, over Strassen’s algorithm in this setting. The authors believe AlphaEvolve and coding agents like it can have a significant impact in improving solutions of problems across many areas of science and computation. <br> <br>

3. ***Meta's J1 LLM-as-a-Judge:  <br>This research introduces J1, a reinforcement learning approach for training LLM-as-a-Judge models, which converts prompts into judgment tasks with verifiable rewards to incentivize thinking and reduce bias. J1 models, trained at 8B or 70B sizes, reportedly outperform other models of similar scale, including those distilled from DeepSeek-R1, and even larger models like R1 on some benchmarks, by learning to outline criteria, compare against self-generated answers, and re-evaluate responses.*** <br> <br>
   May 16, Meta published a [paper](https://arxiv.org/pdf/2505.10320) “J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning”. The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. This study introduces J1, a reinforcement learning approach to training such models. The method converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards that incentivize thinking and mitigate judgment bias. In particular, the approach outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model. The work provides analysis and ablations comparing Pairwise-J1 vs Pointwise-J1 models, offline vs online training recipes, reward strategies, seed prompts, and variations in thought length and content. The study finds that the models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses. <br> <br>

5. ***MIT's study on neural scaling:  <br>Researchers investigated the origins of neural scaling laws in LLMs, proposing that superposition (representing more features than model dimensions) and varying feature frequencies explain why loss decreases as a power law with model size. Their toy model showed that under strong superposition, loss becomes inversely proportional to model dimension, a finding consistent with analyses of open-sourced LLMs and Chinchilla scaling, suggesting superposition is a key mechanism.*** <br> <br>
   May 15, MIT published a [paper](https://arxiv.org/abs/2505.10465) “Superposition Yields Robust Neural Scaling”. The success of today's large language models (LLMs) depends on the observation that larger models perform better. However, the origin of this neural scaling law -- the finding that loss decreases as a power law with model size -- remains unclear. Starting from two empirical principles -- that LLMs represent more things than the model dimensions (widths) they have (i.e., representations are superposed), and that words or concepts in language occur with varying frequencies – the work constructed a toy model to study the loss scaling with model size. The study found that when superposition is weak, meaning only the most frequent features are represented without interference, the scaling of loss with model size depends on the underlying feature frequency; if feature frequencies follow a power law, so does the loss. In contrast, under strong superposition, where all features are represented but overlap with each other, the loss becomes inversely proportional to the model dimension across a wide range of feature frequency distributions. This robust scaling behavior is explained geometrically: when many more vectors are packed into a lower dimensional space, the interference (squared overlaps) between vectors scales inversely with that dimension. The study then analyzed four families of open-sourced LLMs and found that they exhibit strong superposition and quantitatively match the predictions of the toy model. The Chinchilla scaling law turned out to also agree with the results. The study concludes that representation superposition is an important mechanism underlying the observed neural scaling laws. The study anticipates that these insights will inspire new training strategies and model architectures to achieve better performance with less computation and fewer parameters. https://github.com/liuyz0/SuperpositionScaling <br> <br>

7. ***Google and Princeton's evolutionary perspective on Transformer learning:  <br>This paper draws an analogy between Transformer learning modes (in-weights and in-context) and evolutionary adaptation strategies (genetic encoding and phenotypic plasticity) to explore how predictability influences learning. Experiments showed high environmental stability favors in-weights learning, while high cue reliability enhances in-context learning, with learning dynamics showing task-contingent shifts between these modes, supporting a relative-cost hypothesis.*** <br> <br>
   May 14, Google and Princeton Uni published a [paper](https://www.arxiv.org/pdf/2505.09855) “Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers”. Transformer models learn in two distinct modes: in-weights learning (IWL), encoding knowledge into model weights, and in-context learning (ICL), adapting flexibly to context without weight modification. To better understand the interplay between these learning modes, the authors draw inspiration from evolutionary biology's analogous adaptive strategies: genetic encoding (akin to IWL, adapting over generations and fixed within an individual's lifetime) and phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to environmental cues). In evolutionary biology, environmental predictability dictates the balance between these strategies: stability favors genetic encoding, while reliable predictive cues promote phenotypic plasticity. The work experimentally operationalizes these dimensions of predictability and systematically investigate their influence on the ICL/IWL balance in Transformers. Using regression and classification tasks, the study shows that high environmental stability decisively favors IWL, as predicted, with a sharp transition at maximal stability. Conversely, high cue reliability enhances ICL efficacy, particularly when stability is low. Furthermore, learning dynamics reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift occurs in some settings (e.g., classification with many classes), the study demonstrates that scenarios with easier IWL (e.g., fewer classes) or slower ICL acquisition (e.g., regression) can exhibit an initial IWL phase later yielding to ICL dominance. These findings support a relative-cost hypothesis for explaining these learning mode transitions, establishing predictability as a critical factor governing adaptive strategies in Transformers, and offering novel insights for understanding ICL and guiding training methodologies. <br> <br>

9. ***ScienceAdvances paper on LLM social conventions:  <br>This research demonstrated that decentralized populations of LLM agents can spontaneously develop universally adopted social conventions without explicit programming. The study also showed how strong collective biases can emerge during this process, even from individually unbiased agents, and how committed minority groups can drive social change by imposing alternative conventions, highlighting implications for AI alignment.*** <br> <br>
    May 14, ScienceAdvances published a [paper](https://www.science.org/doi/10.1126/sciadv.adu9368) “Emergent social conventions and collective bias in LLM populations”. Social conventions are the backbone of social coordination, shaping how individuals form a group. As growing populations of artificial intelligence (AI) agents communicate through natural language, a fundamental question is whether they can bootstrap the foundations of a society. The research presents experimental results that demonstrate the spontaneous emergence of universally adopted social conventions in decentralized populations of large language model (LLM) agents. The study then shows how strong collective biases can emerge during this process, even when agents exhibit no bias individually. Last, the study examines how committed minority groups of adversarial LLM agents can drive social change by imposing alternative social conventions on the larger population. The results show that AI systems can autonomously develop social conventions without explicit programming and have implications for designing AI systems that align, and remain aligned, with human values and societal goals. <br> <br>

11. ***A mechanistic look at LLM contextual entrainment:  <br>Researchers from the University of Toronto et al. identified "contextual entrainment," a phenomenon where LMs assign higher probability to tokens previously seen in the prompt, even random ones, suggesting a mechanistic distraction independent of semantic relevance but modulated by it. They hypothesized and identified "entrainment heads" responsible, showing that deactivating them attenuates this effect and reduces distraction.*** <br> <br>
    May 14, Uni of Toronto, UKP Lab and Microsoft published a [paper](https://arxiv.org/pdf/2505.09338) “Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs”. The study observes a novel phenomenon, contextual entrainment, across a wide range of language models (LMs) and prompt settings, providing a new mechanistic perspective on how LMs become distracted by “irrelevant” contextual information in the input prompt. Specifically, LMs assign significantly higher logits (or probabilities) to any tokens that have previously appeared in the context prompt, even for random tokens. This suggests that contextual entrainment is a mechanistic phenomenon, occurring independently of the relevance or semantic relation of the tokens to the question or the rest of the sentence. The study finds statistically significant evidence that the magnitude of contextual entrainment is influenced by semantic factors. Counterfactual prompts have a greater effect compared to factual ones, suggesting that while contextual entrainment is a mechanistic phenomenon, it is modulated by semantic factors. The authors hypothesise that there is a circuit of attention heads -- the entrainment heads -- that corresponds to the contextual entrainment phenomenon. Using a novel entrainment head discovery method based on differentiable masking, the work identifies these heads across various settings. When authors “turn off” these heads, i.e., set their outputs to zero, the effect of contextual entrainment is significantly attenuated, causing the model to generate output that capitulates to what it would produce if no distracting context were provided. The discovery of contextual entrainment, along with the investigation into LM distraction via the entrainment heads, marks a key step towards the mechanistic analysis and mitigation of the distraction problem. <br> <br>

13. ***KAIST's system prompt optimization:  <br>This study introduced the novel problem of bilevel system prompt optimization, aiming to design robust and transferable system prompts for LLMs, as opposed to focusing only on task-specific user prompts. They proposed a meta-learning framework that iteratively optimizes system prompts over diverse user prompts and datasets, demonstrating improved generalization to unseen tasks and faster adaptation with fewer optimization steps for user prompts.*** <br> <br>
    May 14, KAIST and DeepAuto.ai published a [paper](https://www.arxiv.org/pdf/2505.09666) “System Prompt Optimization with Meta-Learning”. Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance. However, while LLM prompts consist of both the task-agnostic system prompts and task-specific user prompts, existing work on prompt optimization has focused on user prompts specific to individual queries or tasks, and largely overlooked the system prompt that is, once optimized, applicable across different tasks and domains. Motivated by this, the study introduces the novel problem of bilevel system prompt optimization, whose objective is to design system prompts that are robust to diverse user prompts and transferable to unseen tasks. To tackle this problem, the work then proposes a meta-learning framework, which meta-learns the system prompt by optimizing it over various user prompts across multiple datasets, while simultaneously updating the user prompts in an iterative manner to ensure synergy between them. The study conducts experiments on 14 unseen datasets spanning 5 different domains, on which the study shows that the approach produces system prompts that generalize effectively to diverse user prompts. Also, the findings reveal that the optimized system prompt enables rapid adaptation even to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance. <br> <br>

15. ***OpenAI's HealthBench benchmark:  <br>This work presents HealthBench, an open-source benchmark for evaluating LLM performance and safety in healthcare through 5,000 multi-turn conversations assessed by physicians using conversation-specific rubrics with 48,562 criteria. Performance on HealthBench has shown steady progress, with recent rapid improvements (e.g., o3 scoring 60% vs. GPT-3.5 Turbo's 16%), and the release includes variations like HealthBench Consensus and Hard to guide model development.*** <br> <br>
    May 13, OpenAI published a [paper](https://cdn.openai.com/pdf/bd7a39d5-9e9f-47b3-903c-8b847ca650c7/healthbench_paper.pdf) “HealthBench: Evaluating Large Language Models Towards Improved Human Health”. The work presents HealthBench, an open-source benchmark measuring the performance and safety of large language models in healthcare. HealthBench consists of 5,000 multi-turn conversations between a model and an individual user or healthcare professional. Responses are evaluated using conversation-specific rubrics created by 262 physicians. Unlike previous multiple-choice or short-answer benchmarks, HealthBench enables realistic, open-ended evaluation through 48,562 unique rubric criteria spanning several health contexts (e.g., emergencies, transforming clinical data, global health) and behavioral dimensions (e.g., accuracy, instruction following, communication). HealthBench performance over the last two years reflects steady initial progress (compare GPT-3.5 Turbo’s 16% to GPT-4o’s 32%) and more rapid recent improvements (o3 scores 60%). Smaller models have especially improved: GPT-4.1 nano outperforms GPT-4o and is 25 times cheaper. The study additionally release two HealthBench variations: HealthBench Consensus, which includes 34 particularly important dimensions of model behavior validated via physician consensus, and HealthBench Hard, where the current top score is 32%. Hope that HealthBench grounds progress towards model development and applications that benefit human health. https://github.com/openai/simple-evals <br> <br>

17. ***Analyzing LLM reasoning failures through BAPOs:  <br>Researchers introduced the bounded attention prefix oracle (BAPO) model to formalize how LLM capacity limits on internal information flow (attention bandwidth) lead to reasoning failures. They showed that BAPO-hard problems requiring high bandwidth cause failures in models like GPT-4, even on small instances, while chain of thought can transform BAPO-hard problems into BAPO-easy ones, offering insights for mitigating these limits.*** <br> <br>
    May 13, EmpiriQal Inc, LSEPS, Uni of Tubingen and Los Alamos Nat. Lab published a [paper](https://papers.cool/arxiv/2505.08739) “Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies”. Despite their many successes, transformer-based large language models (LLMs) continue to struggle with tasks that require complex reasoning over large parts of their input. The authors argue that these failures arise due to capacity limits on the accurate flow of information within LLMs. To formalize this issue, the study introduces the bounded attention prefix oracle (BAPO) model, a new computational framework that models bandwidth constraints on attention heads, the mechanism for internal communication in LLMs. The study shows that several important reasoning problems like graph reachability require high communication bandwidth for BAPOs to solve; the authors call these problems BAPO-hard. Experiments corroborate the theoretical predictions: GPT-4, Claude, and Gemini succeed on BAPO-easy tasks and fail even on relatively small BAPO-hard tasks. BAPOs also reveal another benefit of chain of thought (CoT): the work proves that breaking down a task using CoT can turn any BAPO-hard problem into a BAPO-easy one. The results offer principled explanations for key LLM failures and suggest directions for architectures and inference methods that mitigate bandwidth limits. <br> <br>

19. ***Cohere's Aya Vision for multilingual multimodality:  <br>This paper introduces Aya Vision, a series of multilingual multimodal models (8B and 32B) that address challenges like data scarcity and catastrophic forgetting. Using a synthetic annotation framework for diverse data and a cross-modal model merging technique, Aya Vision models achieve best-in-class performance against larger competitors by preserving text capabilities while enhancing multimodal generation.*** <br> <br>
    May 13, Cohere published a [paper](https://arxiv.org/pdf/2505.08751) “Aya Vision: Advancing the Frontier of Multilingual Multimodality”. Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, the study introduces novel techniques spanning both data and modeling. First, the work develops a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, the study proposes a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. The study further scales this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. The work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance. <br> <br>

21. ***Bank of England's study on LLM hidden states:  <br>This research investigated using the hidden states of LLMs to estimate and impute economic and financial statistics, finding that simple linear models trained on these states outperform the LLMs' direct text outputs for variables like county-level unemployment or firm-level assets. This suggests hidden states capture richer economic information, with only a few dozen labeled examples needed for training and potential for transfer learning without labeled target data.*** <br> <br>
    May 13, Bank of England published a [paper](https://arxiv.org/pdf/2505.08662) “Revealing economic facts: LLMs know more than they say”. The study investigates whether the hidden states of large language models (LLMs) can be used to estimate and impute economic and financial statistics. Focusing on county-level (e.g. unemployment) and firm-level (e.g. total assets) variables, the study shows that a simple linear model trained on the hidden states of open-source LLMs outperforms the models' text outputs. This suggests that hidden states capture richer economic information than the responses of the LLMs reveal directly. A learning curve analysis indicates that only a few dozen labelled examples are sufficient for training. The study also proposes a transfer learning method that improves estimation accuracy without requiring any labelled data for the target variable. Finally, the study demonstrates the practical utility of hidden-state representations in super-resolution and data imputation tasks. <br> <br>

23. ***UIUC's DynamicRAG framework:  <br>This research proposes DynamicRAG, a novel retrieval-augmented generation framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. The reranker is modeled as an agent optimized through reinforcement learning, using rewards derived from LLM output quality, and demonstrates state-of-the-art performance across seven knowledge-intensive datasets by better adapting retrieval to specific needs.*** <br> <br>
    May 12, UIUC published a [paper](https://arxiv.org/pdf/2505.07233) “DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation”. Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker, which refines retrieved documents to enhance generation quality and explainability. The challenge of selecting the optimal number of documents (k) remains unsolved: too few may omit critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. The research proposes DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. The authors model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results. https://github.com/GasolSun36/DynamicRAG <br> <br>

25. ***OpenAI chief scientist on AI's research potential:  <br>Jakub Pachocki, OpenAI's chief scientist, discussed the evolving capabilities of AI models, predicting they will achieve novel research insights and measurable economic impact before the end of the decade. He highlighted the role of reinforcement learning and OpenAI's plans to release an open-weight model, noting that AI reasoning differs from human reasoning but can still autonomously produce valuable software and scientific contributions.*** <br> <br>
    May 12, Nature published an [article](https://www.nature.com/articles/d41586-025-01485-2) “AI models are capable of novel research: OpenAI’s chief scientist on what to expect”. Jakub Pachocki, OpenAI's chief scientist, discusses the future of AI models and their potential for novel research. OpenAI, known for ChatGPT, has developed advanced AI tools, including reasoning models that specialize in logical tasks. These models assist researchers in various ways, such as polishing prose, writing code, and generating hypotheses. Despite their benefits, OpenAI faces criticism for the energy demands of its models and data exploitation for training. Pachocki, who joined OpenAI in 2017, leads the development of AI systems designed for complex tasks in science, mathematics, and coding. He emphasizes the importance of reinforcement learning, which uses human feedback to refine models. Pachocki believes AI models can discover novel insights, although their reasoning differs from human reasoning. OpenAI plans to release an open-weight model for researchers to further train, marking its first open model since GPT-2 in 2019. Pachocki's definition of artificial general intelligence (AGI) has evolved, with significant milestones falling faster than expected. He anticipates substantial progress in AI's ability to create novel research and make measurable economic impacts before the end of the decade. Pachocki is excited about the potential of AI to produce valuable software autonomously, even if it doesn't solve major science problems immediately. <br> <br>

27. ***Prime Intellect's INTELLECT-2 model:  <br>This paper introduces INTELLECT-2, a 32 billion parameter reasoning model trained via a globally distributed, fully asynchronous reinforcement learning run across a permissionless compute network. The project involved developing new infrastructure (PRIME-RL, TOPLOC, SHARDCAST) and modifying GRPO training to achieve stability and improve upon the QwQ-32B model, with all code and data open-sourced.*** <br> <br>
    May 12, Prime Intellect Team published a [paper](https://arxiv.org/pdf/2505.07291) “INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized Reinforcement Learning”. The work introduces INTELLECT-2, the first globally distributed reinforcement learning (RL) training run of a 32 billion parameter language model. Unlike traditional centralized training efforts, INTELLECT-2 trains a reasoning model using fully asynchronous RL across a dynamic, heterogeneous swarm of permissionless compute contributors. To enable a training run with this unique infrastructure, the study built various components from scratch: it introduces PRIME-RL, a training framework purpose-built for distributed asynchronous reinforcement learning, based on top of novel components such as TOPLOC, which verifies rollouts from untrusted inference workers, and SHARDCAST, which efficiently broadcasts policy weights from training nodes to inference workers. Beyond infrastructure components, the work proposes modifications to the standard GRPO training recipe and data filtering techniques that were crucial to achieve training stability and ensure that the model successfully learned its training objective, thus improving upon QwQ-32B, the state of the art reasoning model in the 32B parameter range. The authors open-source INTELLECT-2 along with all of the code and data, hoping to encourage and enable more open research in the field of decentralized training. https://github.com/PrimeIntellect-ai/prime-rl <br> <br>

29. ***Silicon Valley's ambition for AI:  <br>This article reveals a growing and increasingly open aspiration within Silicon Valley to automate not just some, but potentially all human jobs using artificial intelligence and robotics. Driven by motivations that range from capturing worker salaries (as explicitly stated by one investor) to purportedly improving global living standards, influential figures and companies are pursuing a future where AI handles cognitive tasks and robots perform physical labor. Despite current AI limitations in accuracy and robot dexterity, rapid advancements, exemplified by powerful LLMs and humanoid robots, suggest these hurdles may soon be overcome, placing the vast majority of jobs at risk and leaving humanity's future role uncertain, prompting critical questions about the ultimate goals and societal implications of this technological pursuit.*** <br> <br>
    May 12, theguardian.com published an [article](https://www.theguardian.com/commentisfree/2025/may/12/for-silicon-valley-ai-isnt-just-about-replacing-some-jobs-its-about-replacing-all-of-them) "For Silicon Valley, AI isn’t just about replacing some jobs. It’s about replacing all of them". The article explores Silicon Valley’s increasingly bold ambition to automate not just some, but all human labor through artificial intelligence and robotics. At a private dinner in San Francisco, a tech investor openly encouraged startup founders to pursue full automation, arguing that replacing workers means capturing their salaries. While this vision is often kept behind closed doors due to its controversial nature, some companies like Mechanize are now publicly embracing it, with backing from major tech figures. Influential voices such as Elon Musk, Bill Gates, and Geoffrey Hinton have echoed the belief that most jobs could soon disappear. Although current AI and robotics still have limitations—AI makes errors and robots lack full dexterity—rapid advancements suggest these barriers may not last long. Technologies like GPT-4 already outperform humans in certain tasks, and humanoid robots are being tested in factories and homes. The ultimate goal, as framed by Silicon Valley, is a world where AI handles cognitive tasks and robots perform physical labor, leaving humans with an uncertain role. While some elite professions may remain untouched, the vast majority of jobs are at risk. The motivations behind this push are debated: some argue it’s about improving global living standards, while others see it as a profit-driven attempt to control the entire means of production. Regardless of feasibility, the real question is not just whether this future will arrive, but why it’s being pursued—and what it means for the rest of society. <br> <br>
    
31. ***Simplifying LM agents with LCLMs:  <br>Researchers from Stanford et al. investigated the necessity of complex agent architectures, showing that for tasks like SWE-bench, simply providing the entire environment to a long context language model (LCLM) with proper prompting can achieve competitive results. A Gemini-1.5-Pro model without scaffolding performed comparably to complex agents, and the more capable Gemini-2.5-Pro with the same unscaffolded approach achieved a 50.8% solve rate.*** <br> <br>
    May 12, Stanford Uni, IBM and Uni of Toronto published a [paper](https://arxiv.org/pdf/2505.08120) “Putting It All into Context: Simplifying Agents with LCLMs”. Recent advances in language model (LM) agents have demonstrated significant potential for automating complex real-world tasks. To make progress on these difficult tasks, LM agent architectures have become increasingly complex, often incorporating multi-step retrieval tools, multiple agents, and scaffolding adapted to the underlying LM. This work investigates whether all of this complexity is necessary, or if parts of these scaffolds can be removed on challenging tasks like SWE-bench. The study shows that in the case of SWE-bench, simply putting the entire environment into the context of a long context language model (LCLM) and properly prompting the model makes it competitive with carefully tuned, complex agent scaffolds. The study shows that a Gemini-1.5-Pro model without any scaffolding or tools achieves 38% on SWE-Bench-Verified, comparable with approaches using carefully tuned agent scaffolds (32%). While the unscaffolded approach with Gemini-1.5-Pro falls short of the strongest agentic architectures, the study demonstrates that the more capable Gemini-2.5-Pro using the same unscaffolded approach directly attains a 50.8% solve rate. Additionally, a two-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a competitive 48.6% solve rate. <br> <br>

32. ***WSJ on Silicon Valley's AI midlife crisis:  <br>This article describes how major tech companies like Alphabet, Apple, and Facebook are navigating the disruptive potential of AI, facing challenges despite their current dominance. The piece highlights stock fluctuations, calls for investor patience, and the overarching uncertainty of how AI will reshape industries, drawing parallels to the "Innovator's Dilemma" and noting the rise of new AI competitors.*** <br> <br>
    May 10, WSJ published an [article](https://www.wsj.com/tech/ai/the-giants-of-silicon-valley-are-having-a-midlife-crisis-over-ai-74968a07) “The Giants of Silicon Valley Are Having a Midlife Crisis Over AI”. The article discusses how major Silicon Valley companies, often referred to as the "Magnificent Seven," are grappling with the challenges and opportunities presented by artificial intelligence (AI). These tech giants, including Alphabet, Apple, Facebook, and Tesla, are experiencing a midlife crisis as they navigate the disruptive potential of AI. Alphabet's stock recently dropped due to a decline in Google search traffic on Apple devices, while Apple is urging investors to be patient with its AI developments. Facebook's Mark Zuckerberg is promoting AI as a tool for social connection, and Elon Musk is trying to stabilize Tesla's stock with promises of driverless cars. Despite their current dominance and profitability, these companies face uncertainty about how AI will reshape their industries. The article draws parallels to Clayton Christensen's "The Innovator's Dilemma," highlighting how successful companies can be disrupted by new technologies. It also mentions the potential for AI to change the app marketplace and the emergence of new AI models from companies like DeepSeek. Venture capitalists like Sarah Guo see opportunities in investing in AI startups that could challenge the established players. Overall, the article underscores the tension between maintaining current success and adapting to future technological shifts. <br> <br>

33. ***Microsoft and Salesforce on LLM multi-turn conversation failures:  <br>This study found that top LLMs perform significantly worse (39% average drop) in multi-turn conversations compared to single-turn, fully-specified instructions. The degradation is attributed mainly to increased unreliability, as LLMs often make early assumptions, prematurely generate solutions, and fail to recover if they take a wrong conversational turn, effectively getting lost.*** <br> <br>
    May 9, Microsoft and Salesforce published a [paper](https://arxiv.org/pdf/2505.06120) “LLMs Get Lost In Multi-Turn Conversation”. Large Language Models (LLMs) are conversational interfaces. As such, LLMs have the potential to assist their users not only when they can fully specify the task at hand, but also to help them define, explore, and refine what they need through multi-turn conversational exchange. Although analysis of LLM conversation logs has confirmed that underspecification occurs frequently in user instructions, LLM evaluation has predominantly focused on the single-turn, fully-specified instruction setting. This study performs large-scale simulation experiments to compare LLM performance in single- and multi-turn settings. Experiments confirm that all the top open- and closed-weight LLMs tested exhibit significantly lower performance in multi-turn conversations than single-turn, with an average drop of 39% across six generation tasks. Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. The work finds that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, the study discovers that *when LLMs take a wrong turn in a conversation, they get lost and do not recover*. <br> <br>

35. ***Bloomberg.com on Klarna's AI job cut slowdown:  <br>Klarna's CEO, Sebastian Siemiatkowski, stated that the company's AI-driven cost-cutting in customer service went too far and announced a recruitment drive for human agents to ensure customers can always speak to a person. While still committed to AI and anticipating further workforce reduction through attrition and technology, this move reflects the need to balance automation with human interaction quality.*** <br> <br>
    May 8, Bloomberg.com published an [article](https://www.bloomberg.com/news/articles/2025-05-08/klarna-turns-from-ai-to-real-person-customer-service) “Klarna Slows AI-Driven Job Cuts With Call for Real People”. Klarna Group Plc's CEO, Sebastian Siemiatkowski, acknowledges that the company's aggressive cost-cutting measures in customer service, driven by AI advancements, have gone too far. To address this, Klarna is initiating a recruitment drive to ensure customers can always speak to a real person, highlighting the company's commitment to maintaining human interaction despite its AI focus. Siemiatkowski revealed plans for a new cohort of remote employees in an "Uber type of setup," aiming to replace outsourced human agents gradually. The pilot program has started with two agents, targeting candidates like students and rural populations. This move underscores the risks financial firms face when replacing humans with untested technology. Klarna, an early collaborator with OpenAI, initially embraced AI to reduce costs after its valuation dropped from $45.6 billion to $6.7 billion in 2022. Despite pausing IPO plans due to market volatility, Klarna remains committed to AI, aiming to launch a digital financial assistant. However, Siemiatkowski emphasizes the importance of human support quality, predicting a workforce reduction from 3,000 to 2,500 due to natural attrition and technological advancements. He anticipates further downsizing within 12 months as technology improves. <br> <br>

37. ***Sakana AI's Continuous Thought Machines:  <br>This paper introduces the Continuous Thought Machine (CTM), a model incorporating neuron-level temporal processing and neural synchronization as core representations, aiming to reintroduce neural timing from biological brains into deep learning. The CTM demonstrated strong performance and versatility across various tasks, showcasing rich internal representations, interpretability, and adaptive compute capabilities, representing a step towards more biologically plausible AI.*** <br> <br>
    May 8, Sakana AI, Uni of Tsukuba and IT Uni of Copenhagen published a [paper](https://arxiv.org/pdf/2505.05522) “Continuous Thought Machines”. Biological brains demonstrate complex neural activity, where the timing and interplay between neurons is critical to how brains process information. Most deep learning architectures simplify neural activity by abstracting away temporal dynamics. This study challenges that paradigm. By incorporating neuron-level processing and synchronization, the study can effectively reintroduce neural timing as a foundational element. The work presents the Continuous Thought Machine (CTM), a model designed to leverage neural dynamics as its core representation. The CTM has two core innovations: (1) neuron-level temporal processing, where each neuron uses unique weight parameters to process a history of incoming signals; and (2) neural synchronization employed as a latent representation. The CTM aims to strike a balance between oversimplified neuron abstractions that improve computational efficiency, and biological realism. It operates at a level of abstraction that effectively captures essential temporal dynamics while remaining computationally tractable for deep learning. The study demonstrates the CTM's strong performance and versatility across a range of challenging tasks, including ImageNet-1K classification, solving 2D mazes, sorting, parity computation, question-answering, and RL tasks. Beyond displaying rich internal representations and offering a natural avenue for interpretation owing to its internal process, the CTM is able to perform tasks that require complex sequential reasoning. The CTM can also leverage adaptive compute, where it can stop earlier for simpler tasks, or keep computing when faced with more challenging instances. The goal of this work is to share the CTM and its associated innovations, rather than pushing for new state-of-the-art results. To that end, the authors believe the CTM represents a significant step toward developing more biologically plausible and powerful artificial intelligence systems. <br> <br>

39. ***Uni of Washington's LlamaPIE proactive assistant:  <br>This study introduces LlamaPIE, a real-time proactive in-ear conversational assistant designed to enhance human conversations with discreet, concise guidance via hearables, operating in the background without explicit user invocation. Using a two-model pipeline (one to decide when to respond, one to generate the response) and a semi-synthetic dialogue dataset, LlamaPIE demonstrated effectiveness and strong user preference in real-world user studies.*** <br> <br>
    May 7, Uni of Washington published a [paper](https://arxiv.org/pdf/2505.04066) “LLAMAPIE: Proactive In-Ear Conversation Assistants”. The study introduces LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. The study addresses several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, the authors construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. The study evaluates the approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with the assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive model, highlighting the potential of LlamaPie to enhance live conversations.

 <br> <br> <br>

***May 11, 2025***

1. ***Mistral's new model release:   <br>Mistral introduced Mistral Medium 3, a model class aiming for state-of-the-art performance with significantly lower cost (8x less) and easier deployment to boost enterprise adoption. This model reportedly matches or exceeds Claude Sonnet 3.7 on benchmarks at a lower price, supports various deployment options including on-premises, and excels in coding and multimodal understanding.***  <br>  <br>
   May 7, Mistral [released](https://mistral.ai/news/mistral-medium-3) Mistral Medium 3, a new class of models that balances SOTA performance, 8X lower cost and simpler deployability to accelerate enterprise usage. All the way from Mistral 7B, the models have consistently demonstrated performance of significantly higher-weight and more expensive models. And today, Mistral announced Mistral Medium 3, pushing efficiency and usability of language models even further. The model leads in professional use cases such as coding and multimodal understanding, and delivers a range of enterprise capabilities including: Hybrid or on-premises / in-VPC deployment; Custom post-training; and Integration into enterprise tools and systems. Mistral Medium 3 delivers frontier performance while being an order of magnitude less expensive. For instance, the model performs at or above 90% of Claude Sonnet 3.7 on benchmarks across the board at a significantly lower cost ($0.4 input / $2 output per M token). In addition to can be deployed on any cloud, it can also run on self-hosted environments of four GPUs and above.  <br>  <br>

3. ***Improving RL for LLM reasoners:   <br>Researchers from Mila, Microsoft, and Google proposed RLV (Reinforcement Learning with Verifiers), a method to enhance existing "value-free" RL techniques for LLM fine-tuning by jointly training the LLM as both a reasoner and a generative verifier using RL-generated data. RLV significantly boosts MATH accuracy (over 20%), enables more efficient test-time compute scaling (8-32x), and shows strong generalization, outperforming base RL methods.***  <br>  <br>
   May 7, Mila, Microsoft and Google published a [paper](https://www.arxiv.org/pdf/2505.04842) “Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers”. Prevalent reinforcement learning (RL) methods for fine-tuning LLM reasoners, such as GRPO or Leave-one-out PPO, abandon the learned value function in favor of empirically estimated returns. This hinders test-time compute scaling that relies on using the value-function for verification. In this work, we propose RLV that augments any “value-free” RL method by jointly training the LLM as both a reasoner and a generative verifier using RL-generated data, adding verification capabilities without significant overhead. Empirically, RLV boosts MATH accuracy by over 20% with parallel sampling and enables 8−32× efficient test-time compute scaling compared to the base RL method. RLV also exhibits strong generalization capabilities for both easy-to-hard and out-of-domain tasks. Furthermore, RLV achieves 1.2−1.6× higher performance when jointly scaling parallel and sequential test-time compute with a long reasoning R1 model.  <br>  <br>

5. ***Exploring generalizable reasoning:   <br>Microsoft researchers investigated whether reasoning can generalize across modalities and domains, finding that general-domain text-based post-training can achieve this. Based on this, they introduced X-Reasoner, a vision-language model post-trained solely on general-domain text, which successfully transfers reasoning to multimodal and out-of-domain settings, outperforming SOTA models and further improving with domain-specific text-only continued training (as seen with X-Reasoner-Med).***  <br>  <br>
   May 6, Microsoft published a [paper](https://arxiv.org/pdf/2505.03981) “X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains”. Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? The findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, the study introduces X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, the work finds that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, the study introduces X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks. https://github.com/microsoft/x-reasoner  <br>  <br>

7. ***A framework for human-AI knowledge co-creation:   <br>Imperial College London introduced Cognitio Emergens (CE), a framework to understand the evolving epistemic partnerships between humans and AI in scientific knowledge creation, moving beyond static roles. CE integrates Agency Configurations (describing authority distribution), Epistemic Dimensions (specific collaborative capabilities), and Partnership Dynamics (forces shaping these relationships, including epistemic alienation risks) to foster co-evolutionary collaborations.***  <br>  <br>
   May 6, Imperial College London published a [paper](https://arxiv.org/pdf/2505.03105) “Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation”. Scientific knowledge creation is fundamentally transforming as humans and AI systems evolve beyond tool-user relationships into co-evolutionary epistemic partnerships. When AlphaFold revolutionized protein structure prediction, researchers described engaging with an epistemic partner that reshaped how they conceptualized fundamental relationships. This article introduces Cognitio Emergens (CE), a framework addressing critical limitations in existing models that focus on static roles or narrow metrics while failing to capture how scientific understanding emerges through recursive human-AI interaction over time. CE integrates three components addressing these limitations: Agency Configurations describing how authority distributes between humans and AI (Directed, Contributory, Partnership), with partnerships dynamically oscillating between configurations rather than following linear progression; Epistemic Dimensions capturing six specific capabilities emerging through collaboration across Discovery, Integration, and Projection axes, creating distinctive "capability signatures" that guide development; and Partnership Dynamics identifying forces shaping how these relationships evolve, particularly the risk of epistemic alienation where researchers lose interpretive control over knowledge they formally endorse. Drawing from autopoiesis theory, social systems theory, and organizational modularity, CE reveals how knowledge co-creation emerges through continuous negotiation of roles, values, and organizational structures. By reconceptualizing human-AI scientific collaboration as fundamentally co-evolutionary, CE offers a balanced perspective that neither uncritically celebrates nor unnecessarily fears AI's evolving role, instead providing conceptual tools for cultivating partnerships that maintain meaningful human participation while enabling transformative scientific breakthroughs.  <br>  <br>

9. ***Distinguishing unlearning from obfuscation:   <br>Researchers from the University of Cambridge and King’s College London formally differentiated LLM unlearning from mere obfuscation (knowledge addition), introducing a probing-based evaluation framework. They also proposed DF-MCQ, a novel unlearning method that flattens predictive distributions over MCQs, achieving high refusal rates (>90%) and significantly higher uncertainty on probing questions compared to obfuscation techniques.***  <br>  <br>
    May 5, Uni of Cambridge and King’s College London published a [paper](https://www.arxiv.org/pdf/2505.02884) “Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?”. Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. This study formally distinguishes unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, the study proposes DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour. Experimental results demonstrate that DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level uncertainty that is much higher than obfuscation on probing questions.  <br>  <br>

11. ***Advancing reward modeling as reasoning:   <br>Researchers from UICU et al. introduced Reasoning Reward Models (ReasRMs), a new class of generative reward models that formulate reward modeling as a reasoning task to enhance interpretability and performance. Their RM-R1 models, trained via a two-stage pipeline (distilling reasoning chains, RL with verifiable rewards), achieve SOTA or near-SOTA performance on benchmarks by self-generating reasoning traces or rubrics for evaluation.***  <br>  <br>
    May 5, UICU, UCSD Texas A&M Uni and Stevens Inst of Tech published a [paper](https://arxiv.org/pdf/2505.02387) “RM-R1: Reward Modeling as Reasoning”. Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, the study hypothesizes and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. This study introduces a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. The study proposes a reasoning-oriented training pipeline and trains a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, the models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, the work performs thorough empirical analysis to understand the key ingredients of successful ReasRM training. https://github.com/RM-R1-UIUC/RM-R1.  <br>  <br>

13. ***Teaching models to understand high-risk data without generating it:   <br>Researchers from USC and Allen AI introduced Selective Loss to Understand but Not Generate (SLUNG), a pre-training method enabling LLMs to comprehend high-risk content (e.g., toxic, copyrighted) without learning to produce it. SLUNG selectively avoids incentivizing the generation of high-risk tokens while forcing understanding by predicting subsequent low-risk tokens, demonstrably improving comprehension without increasing harmful generation.***  <br>  <br>
    May 5, Uni of Southern California and Allen Inst. for AI published a [paper](https://arxiv.org/pdf/2505.03052) “Teaching Models to Understand (but not Generate) High-risk Data”. Language model developers typically filter out high-risk content -- such as toxic or copyrighted text -- from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models' ability to recognize and appropriately respond to harmful or sensitive content. This work introduces Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model's context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through the experiments, the study shows that SLUNG consistently improves models' understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out. https://github.com/ryanyxw/llm-decouple  <br>  <br>

15. ***Evaluating LLM adherence to complex legal procedures:   <br>Yale Law School researchers tested leading LLMs on their ability to follow the intricate rules of The Bluebook legal citation system, using an original dataset of 866 tasks. Their findings showed that models achieved full compliance only 69%-74% of the time, with in-context learning on Bluebook rules improving accuracy to just 77%, cautioning against using off-the-shelf LLMs for tasks requiring high procedural fidelity.***  <br>  <br>
    May 5, Yale Law School published a [paper](https://arxiv.org/pdf/2505.02763) “Bye-bye, Bluebook Automating Legal Procedure with Large Language Models”. Legal practice requires careful adherence to procedural rules. In the United States, few are more complex than those found in The Bluebook: A Uniform System of Citation. Compliance with this system's 500+ pages of byzantine formatting instructions is the raison d'etre of thousands of student law review editors and the bete noire of lawyers everywhere. To evaluate whether large language models (LLMs) are able to adhere to the procedures of such a complicated system, we construct an original dataset of 866 Bluebook tasks and test flagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. The study shows (1) that these models produce fully compliant Bluebook citations only 69%-74% of the time and (2) that in-context learning on the Bluebook's underlying system of rules raises accuracy only to 77%. These results caution against using off-the-shelf LLMs to automate aspects of the law where fidelity to procedure is paramount.
  <br>  <br>
17. ***Rapid conversion of transformers to linear attention models:   <br>Recursal AI, EleutherAI, and others presented RADLADS, a protocol for quickly and cost-effectively converting standard softmax attention transformers into linear attention decoder models (RWKV-variants). This process uses minimal data (0.005% of original training) and low cost (e.g., <$2,000 for a 72B model) to produce models that retain close-to-original quality and achieve SOTA performance for linear attention models of their size.***  <br>  <br>
    May 5, Recursal AI, EleutherAI et al published a [paper](https://arxiv.org/pdf/2505.03005) “RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale”. The work presents Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. The conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to a 72B linear attention model costs less than $2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper  <br>  <br>

19. ***OpenAI's structural evolution plan:   <br>OpenAI announced that its for-profit LLC will transition to a Public Benefit Corporation (PBC), while remaining controlled by the founding nonprofit organization, which will also be a large shareholder. This change, made after consultations, aims to provide better resources for the nonprofit to support its mission of ensuring AGI benefits all of humanity, which remains the core mission for both entities.***  <br>  <br>
    May 5, OpenAI published a [blog](https://openai.com/index/evolving-our-structure/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=openai-reverses-for-profit-plans&_bhlid=4074aa24cf357b674bbad4cb1cd5208da696a568) “Evolving OpenAI’s structure”. The OpenAI Board has an updated plan for evolving OpenAI’s structure. OpenAI was founded as a nonprofit, and is today overseen and controlled by that nonprofit. Going forward, it will continue to be overseen and controlled by that nonprofit.  Its for-profit LLC, which has been under the nonprofit since 2019, will transition to a Public Benefit Corporation (PBC)–a purpose-driven company structure that has to consider the interests of both shareholders and the mission. The nonprofit will control and also be a large shareholder of the PBC, giving the nonprofit better resources to support many benefits.  The mission remains the same, and the PBC will have the same mission. OpenAI made the decision for the nonprofit to retain control of OpenAI after hearing from civic leaders and engaging in constructive dialogue with the offices of the Attorney General of Delaware and the Attorney General of California. The company thanks both offices and looking forward to continuing these important conversations to make sure OpenAI can continue to effectively pursue its mission of ensuring AGI benefits all of humanity.   <br>  <br>

21. ***Inducing agent evaluation metrics from open-ended feedback:   <br>Researchers proposed AutoLibra, a framework that transforms open-ended human feedback on agent behavior into concrete, evaluable metrics for fine-grained analysis. AutoLibra grounds feedback, clusters behaviors, and creates metrics with definitions and examples, using meta-metrics like "coverage" and "redundancy" to optimize the induced set, demonstrating its utility in improving agent performance through better prompt engineering and fine-tuning data selection.***  <br>  <br>
    May 5, Stanford Uni, Uni of Toronto, and Uni of Penn published a [paper](https://arxiv.org/pdf/2505.02820) “AutoLibra: Agent Metric Induction from Open-Ended Feedback”. Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. The work proposes AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., "If you find that the button is disabled, don't click it again", or "This agent has too much autonomy to decide what to do on its own", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. The study further proposes two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, the work experimentally demonstrates AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. The work also presents two applications of AutoLibra in agent improvement: First, the study shows that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, the study shows that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. The results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.  <br>  <br>

23. ***LLM-based text simplification for improved comprehension:   <br>Google researchers developed and validated an LLM capability for minimally lossy text simplification using a self-refinement approach, tested in a large randomized study (4563 participants, 31 texts). Results showed that participants reading simplified texts answered significantly more comprehension questions correctly (3.9% absolute increase overall, 14.6% for PubMed) and reported lower cognitive load, demonstrating LLMs' potential to enhance information accessibility.***  <br>  <br>
    May 4, Google published a [paper](https://arxiv.org/pdf/2505.01980) “LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load”. Information on the web, such as scientific publications and Wikipedia, often surpasses users' reading level. To help address this, the work used a self-refinement approach to develop a LLM capability for minimally lossy text simplification. To validate the approach, the work conducted a randomized study involving 4563 participants and 31 texts spanning 6 broad subject areas: PubMed (biomedical scientific articles), biology, law, finance, literature/philosophy, and aerospace/computer science. Participants were randomized to viewing original or simplified texts in a subject area, and answered multiple-choice questions (MCQs) that tested their comprehension of the text. The participants were also asked to provide qualitative feedback such as task difficulty. Results indicate that participants who read the simplified text answered more MCQs correctly than their counterparts who read the original text (3.9% absolute increase, p<0.05). This gain was most striking with PubMed (14.6%), while more moderate gains were observed for finance (5.5%), aerospace/computer science (3.8%) domains, and legal (3.5%). Notably, the results were robust to whether participants could refer back to the text while answering MCQs. The absolute accuracy decreased by up to ~9% for both original and simplified setups where participants could not refer back to the text, but the ~4% overall improvement persisted. Finally, participants' self-reported perceived ease based on a simplified NASA Task Load Index was greater for those who read the simplified text (absolute change on a 5-point scale 0.33, p<0.05). This randomized study, involving an order of magnitude more participants than prior works, demonstrates the potential of LLMs to make complex information easier to understand. The work aims to enable a broader audience to better learn and make use of expert knowledge available on the web, improving information accessibility.   <br>  <br>

25. ***The risk of human obsolescence from advanced AI:   <br>This article argues that the primary existential threat from AI may be its potential to become "better at everything" than humans, leading to gradual human irrelevance rather than a rogue AI takeover. As AI surpasses human capabilities in economic, cultural, and social roles, making human input optional, it could cause widespread job displacement and erode democratic structures, necessitating proactive societal steering to maintain human relevance.***  <br>  <br>
    May 4, Theguardian published an [article](https://www.theguardian.com/books/2025/may/04/the-big-idea-can-we-stop-ai-making-humans-obsolete) “Better at everything: how AI could make human beings irrelevant”. The article argues that the primary existential risk from artificial intelligence might not be a conscious rogue AI, but rather the subtle consequence of AI becoming "better at everything" humans do, leading to gradual human obsolescence. As AI and robotics continue to improve, they are on track to surpass human capabilities in most roles, from economic tasks like working and making decisions to cultural roles like art and even social roles as companions. This superiority will make AI cheaper, more reliable, and the preferred option for an increasing number of tasks, potentially rendering human input optional or unnecessary across many professions and aspects of life. The author suggests this could lead to job displacement, reduced human relevance in decision-making, and even a preference for AI companions over human interaction. This shift could extend to governments relying more on AI, potentially reducing their dependence on citizens and eroding democratic structures, akin to a "resource curse." Countering this is difficult because AI's efficiency and cost advantages will be compelling. To navigate this future, the author proposes steps like tracking AI's influence, regulating advanced AI development, using AI to empower human organization, and developing "ecosystem alignment" to proactively steer society towards a future where humans remain relevant as beneficiaries and stewards of AI, rather than becoming obsolete.  <br>  <br>

27. ***Understanding massive values in LLM self-attention:   <br>Researchers from Rutgers, CMU, et al. demonstrated that concentrated massive values consistently appear in the query (Q) and key (K) representations of self-attention modules in modern LLMs, but not in values (V). These massive values, primarily caused by Rotary Positional Encoding (RoPE), are shown to be critical for interpreting contextual knowledge rather than retrieving parametric knowledge, with implications for quantization and model design.***  <br>  <br>
    May 3, Rutgers Uni, CMU et al published a [paper](https://arxiv.org/pdf/2502.01563) “Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding”. Large language models (LLMs) have achieved remarkable success in contextual knowledge understanding. The work shows that these concentrated massive values consistently emerge in specific regions of attention queries (Q) and keys (K) while not having such patterns in values (V) in various modern transformer-based LLMs (Q, K, and V mean the representations output by the query, key, and value layers respectively). Through extensive experiments, the study further demonstrates that these massive values play a critical role in interpreting contextual knowledge (knowledge obtained from the current context window) rather than in retrieving parametric knowledge stored within the model's parameters. Further investigation of quantization strategies reveals that ignoring these massive values leads to a pronounced drop in performance on tasks requiring rich contextual understanding, aligning with the analysis. Finally, the work traces the emergence of concentrated massive values and find that such concentration is caused by Rotary Positional Encoding (RoPE), which has appeared since the first layers. These findings shed new light on how Q and K operate in LLMs and offer practical insights for model design and optimization. https://github.com/MingyuJ666/Rope_with_LLM  <br>  <br>

29. ***Nvidia's Llama-Nemotron reasoning models:   <br>Nvidia introduced the Llama-Nemotron series (Nano-8B, Super-49B, Ultra-253B), an open family of efficient reasoning models competitive with SOTA models like DeepSeek-R1 but offering better inference throughput. Trained using neural architecture search, knowledge distillation, continued pretraining, and reasoning-focused post-training (SFT & RL), these models feature a dynamic reasoning toggle and are released with their dataset and training codebases.***  <br>  <br>
    May 2, Nvidia published a [paper](https://arxiv.org/abs/2505.00949) “Llama-Nemotron: Efficient Reasoning Models”. The report introduces the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, Nvidia discusses the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development: 1. Nvidia releases the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. Nvidia releases the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. Nvidia also releases the training codebases: NeMo, NeMo-Aligner, and Megatron-LM.  <br>  <br>

31. ***Introducing Canon layers for enhanced LLM reasoning:   <br>Meta researchers, using controlled synthetic pretraining tasks, discovered "Canon layers" – lightweight architectural components promoting horizontal information flow across neighboring tokens. These layers, seamlessly integrated into various sequence models, were shown to significantly enhance reasoning depth (e.g., by 2x), breadth, and knowledge manipulation, transforming weaker architectures to match stronger ones in both synthetic and real-world pretraining.***  <br>  <br>
    May 2, Meta published a [paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5240330) “Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers”. Understanding architectural differences between large language models (LLMs) remains challenging, particularly at academic-scale pretraining (e.g., 1.3B parameters on 100B tokens), where results are often dominated by noise and randomness. To overcome this, the work introduces controlled, synthetic pretraining tasks to isolate and evaluate key model capabilities. Leveraging this framework, the study discovers Canon layers: lightweight architectural components—named after the musical term “canon”—that promote horizontal information flow across neighboring tokens. Canon layers compute weighted combinations of nearby token representations and integrate seamlessly into Transformers, linear attention, state-space architectures, or any general sequence model. The work presents 12 key results. This includes how Canon layers enhance reasoning depth (e.g., by 2x), reasoning breadth, knowledge manipulation, etc. Remarkably, Canon layers transform weak architectures like NoPE to match RoPE, linear attention to match state-space models (like Mamba2), as validated both in the synthetic playground and real-world academic-scale pretraining. Leveraging infinitely high-quality data, the authors hope the framework can predict how future architectures may evolve as training pipelines improve—e.g., through better data curation or RL-based post-training—unlocking deeper reasoning and hierarchical inference capabilities.  <br>  <br>

33. ***Automated failure attribution in LLM multi-agent systems:   <br>Researchers from Penn State et al. introduced the new research area of automated failure attribution for LLM multi-agent systems, aiming to identify the responsible agent and step in task failures. They created the Who&When dataset of annotated failure logs and evaluated automated methods, finding that current SOTA models struggle significantly (best method: 53.5% agent accuracy, 14.2% step accuracy), highlighting the task's complexity.***  <br>  <br>
    Apr 30, Penn State Uni, Duke Uni, Meta, Google et al published a [paper](https://arxiv.org/pdf/2505.00212) “Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems”. Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. The study proposes and formulates a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, the work introduces the Who&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&When, the work develops and evaluates three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. https://github.com/mingyin1/Agents_Failure_Attribution  <br>  <br>

35. ***A unified framework for agentic reasoning and tool integration:   <br>Microsoft introduced ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a framework combining agentic reasoning, reinforcement learning, and tool integration for LLMs. ARTIST enables models to autonomously decide on tool use within reasoning chains, learning robust strategies via outcome-based RL, and demonstrated significant improvements (up to 22% absolute) over baselines on complex reasoning and function calling benchmarks.***  <br>  <br>
    Apr 28, Microsoft published a [paper](https://www.arxiv.org/pdf/2505.01441) “Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning”. Large language models (LLMs) have achieved remarkable progress in complex reasoning tasks, yet they remain fundamentally limited by their reliance on static internal knowledge and text-only reasoning. Real-world problem solving often demands dynamic, multi-step reasoning, adaptive decision making, and the ability to interact with external tools and environments. This work introduces ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a unified framework that tightly couples agentic reasoning, reinforcement learning, and tool integration for LLMs. ARTIST enables models to autonomously decide when, how, and which tools to invoke within multi-turn reasoning chains, leveraging outcome-based RL to learn robust strategies for tool use and environment interaction without requiring step-level supervision. Extensive experiments on mathematical reasoning and multi-turn function calling benchmarks show that ARTIST consistently outperforms state-of-the-art baselines, with up to 22% absolute improvement over base models and strong gains on the most challenging tasks. Detailed studies and metric analyses reveal that agentic RL training leads to deeper reasoning, more effective tool use, and higher-quality solutions. The results establish agentic RL with tool integration as a powerful new frontier for robust, interpretable, and generalizable problem-solving in LLMs.

  <br>  <br>  <br>
***May 4, 2025***

1. ***A study on LLM generalization:  <br>Google and Stanford researchers explored why LLMs generalize differently from in-context learning versus fine-tuning, noting fine-tuning often fails on simple reversals or deductions where in-context learning succeeds. Using controlled datasets, they found in-context learning generally offers more flexible generalization and demonstrated that adding in-context inferences to fine-tuning data significantly improves generalization performance.*** <br> <br>
   May 1, Google and Stanford Uni published a [paper](https://arxiv.org/pdf/2505.00661) “On the generalization of language models from in-context learning and finetuning a controlled study”. Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning -- from failing to generalize to simple reversals of relations they are trained on, to missing logical deductions that can be made from trained information. These failures to generalize from fine-tuning can hinder practical application of these models. However, language models' in-context learning shows different inductive biases, and can generalize better in some of these cases. Here, the study explores these differences in generalization between in-context- and fine-tuning-based learning. To do so, the work constructed several novel datasets to evaluate and improve models' ability to generalize from finetuning data. The datasets are constructed to isolate the knowledge in the dataset from that in pretraining, to create clean tests of generalization. The work exposes pretrained large models to controlled subsets of the information in these datasets -- either in context, or through fine-tuning -- and evaluate their performance on test sets that require various types of generalization. The study finds overall that in data-matched settings, in-context learning can generalize more flexibly than fine-tuning (though the authors also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). The study builds on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context inferences to finetuning data. The study shows that this method improves generalization across various splits of the datasets and other benchmarks. The results have implications for understanding the inductive biases of different modes of learning in language models, and practically improving their performance. <br> <br>

3. ***Alibaba's Qwen 3 launch:  <br>Chinese tech giant Alibaba introduced Qwen 3, an enhanced version of its primary AI model featuring new hybrid reasoning capabilities. This release comes amidst increased competition in China's AI sector, following recent model launches by rivals like DeepSeek and Baidu, aiming to provide a more adaptable platform for developers.*** <br> <br>
   May 1, Reuters published an [article](https://www.reuters.com/business/media-telecom/alibaba-unveils-advanced-qwen-3-ai-chinese-tech-rivalry-intensifies-2025-04-29/#:~:text=April%2029%20(Reuters)%20%2D%20Chinese,introduces%20new%20hybrid%20reasoning%20capabilities.) “Alibaba unveils advanced Qwen 3 AI as Chinese tech rivalry intensifies”. Chinese tech giant Alibaba Group (9988.HK), opens new tab launched Qwen 3 on Tuesday, an upgraded version of its flagship artificial intelligence model that introduces new hybrid reasoning capabilities. The launch comes as competition in China's AI sector intensifies, spurred by the breakout success of local startup DeepSeek earlier this year, which claimed to have built high-performing models at lower costs than their Western counterparts. Chinese search leader Baidu, opens new tab joined the AI arms race last Friday with the release of its Ernie 4.5 Turbo and reasoning-focused Ernie X1 Turbo models. Alibaba's newest release merges conventional AI functions with advanced dynamic reasoning, creating what the company calls a more adaptable and efficient platform for app and software developers. The e-commerce giant had previously rushed out its Qwen 2.5-Max model in late January, just days after DeepSeek's announcement, claiming superior performance <br> <br>

5. ***Improving LLM agents via self-generated examples:  <br>Stanford researchers investigated improving LLM agents for sequential decision-making without manual knowledge engineering, instead focusing on automatically learning from self-generated successful trajectories. They demonstrated that accumulating these experiences boosts performance significantly across benchmarks, and further gains are achieved through database-level and exemplar-level selection, reaching performance comparable to more complex, engineered methods.*** <br> <br>
   May 1, Stanford Uni published a [paper](https://arxiv.org/pdf/2505.00234) “Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks”. Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, the study investigates how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, the study focuses on constructing and refining a database of self-generated examples. The work demonstrates that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. The study then introduces two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Results demonstrate that automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering. <br> <br>

7. ***Amazon's Nova model family release:  <br>Amazon detailed its Nova Premier model, described as its most capable multimodal foundation model capable of processing text, images, and video within a one-million token context window. Nova Premier also functions as a teacher model for distilling customized, efficient variants of smaller Nova models (Pro, Lite, Micro), all while emphasizing integrated safety and responsible AI practices.*** <br> <br>
   Apr 30, Amazon release it Nova family model with tech [report](https://assets.amazon.science/f6/c5/79dceb124593b3356566ad6723af/the-amazon-nova-premier-technical-report-and-model-card.pdf). The report presents Amazon Nova Premier, its most capable multimodal foundation model and teacher for model distillation. Nova Premier processes text, images, and videos with a one-million token context window enabling analysis of large codebases, long documents, and long videos in a single prompt. It also enables customers to use Amazon Bedrock to create customized variants of Amazon Nova Pro, Nova Lite, and Nova Micro that maintain high accuracy while offering improved speed and cost efficiency. Like all Nova models, Nova Premier is built with integrated safety measures and responsible AI practices, maintaining our commitment to customer trust, security, and reliability. With Nova Premier, Amazon further extends the capabilities and price-performance advantages of the Amazon Nova model family. <br> <br>

9. ***Microsoft's Phi 4 small models release:  <br>Microsoft introduced its Phi-4 family of small language models (SLMs), including Phi-4-reasoning, reasoning-plus, and mini-reasoning, designed to bring complex reasoning capabilities to efficient AI. Leveraging techniques like inference scaling, distillation, RL, and quality data, these models, particularly the 14B parameter Phi-4-reasoning, achieve performance competitive with much larger models on benchmarks, with a strong emphasis on responsible AI principles and safety measures.*** <br> <br>
    Apr 30, Microsoft [released](https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/) its Phi 4 small models. Microsoft has introduced new small language models (SLMs) called Phi-4-reasoning, Phi-4-reasoning-plus, and Phi-4-mini-reasoning, marking a significant advancement in efficient AI. These models excel in complex reasoning tasks, typically requiring large models, by leveraging inference-time scaling, distillation, reinforcement learning, and high-quality data. Phi-4-reasoning, with 14 billion parameters, rivals larger models in performance, while Phi-4-reasoning-plus enhances accuracy using more tokens. Both models outperform OpenAI o1-mini and DeepSeek-R1-Distill-Llama-70B in benchmarks, including mathematical reasoning and Ph.D.-level science questions. Phi-4-mini-reasoning is optimized for environments with limited computing resources, making it ideal for educational applications and mobile systems. These models are integrated into Windows 11 devices and Copilot+ PCs, offering efficient and powerful AI capabilities. Microsoft emphasizes responsible AI development, adhering to principles of accountability, transparency, fairness, reliability, safety, privacy, and inclusiveness. The Phi models undergo rigorous safety post-training using supervised fine-tuning, direct preference optimization, and reinforcement learning from human feedback to ensure they perform tasks effectively while minimizing risks. These advancements demonstrate Microsoft's commitment to pushing the boundaries of AI while maintaining a focus on ethical and responsible development. <br> <br>

11. ***Research on 1-shot RL for LLM reasoning:  <br>Researchers demonstrated the surprising effectiveness of reinforcement learning with verifiable reward using just one training example (1-shot RLVR) for enhancing LLM math reasoning. Applying this to models like Qwen2.5-Math-1.5B, a single example yielded dramatic performance gains (e.g., 36% to 73.6% on MATH500), matching results from much larger training sets and showing robustness across models and algorithms, while also highlighting the role of policy gradient and exploration.*** <br> <br>
    Apr 29, Uni of Washington, USC, Microsoft et al. published a [paper](https://arxiv.org/pdf/2504.20571) “Reinforcement Learning for Reasoning in Large Language Models with One Training Example”. The study shows that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, the authors identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, the work identifies some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon which is termed as post-saturation generalization. Moreover, the study verifies that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the "grokking" phenomenon. The study also shows the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, the work observes that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. https://github.com/ypwang61/One-Shot-RLVR <br> <br>

13. ***An unauthorized AI persuasion experiment on Reddit:  <br>University of Zurich researchers conducted a controversial experiment on Reddit's r/ChangeMyView, deploying AI bots to post thousands of comments over four months without user consent to test persuasive capabilities, finding personalized AI most effective. The study faced strong ethical criticism from moderators for non-consensual methods and data harvesting, leading the researchers to ultimately forgo publication despite university defense of its relevance.*** <br> <br>
    Apr 29, [Theregister.com](https://www.theregister.com/2025/04/29/swiss_boffins_admit_to_secretly/) published an article “Swiss boffins admit to secretly posting AI-penned posts to Reddit in the name of science”. Researchers at the University of Zurich conducted an unauthorized experiment on the Reddit community r/ChangeMyView (CMV) to test the persuasive power of AI bots. Over four months, these bots posted 1,783 comments without users' knowledge, aiming to change opinions. Success was measured by "Deltas" awarded by users for persuasive arguments, with AI bots receiving 137 Deltas. The study tested three AI types: generic, community-aligned, and personalized, with the personalized AI, which tailored arguments based on users' inferred attributes, achieving the highest success rate of 18%. The bots often used fake identities and personal stories, sometimes adopting controversial or extreme positions. Moderators criticized the experiment for ethical violations, including non-consensual use of AI and data harvesting. Despite a formal warning from the university's ethics committee, which acknowledged rule violations but deemed the risks minimal, the University defended the study's social relevance. Moderators demanded an apology and non-publication of the study, fearing it could encourage further unethical experiments. The researchers emphasized the importance of their findings for understanding AI-driven manipulation and called for platform safeguards. Ultimately, the team decided not to publish the results, believing they had raised sufficient awareness of AI's manipulative capabilities. <br> <br>

15. ***Critique of Chatbot Arena benchmarks:  <br>Researchers identified systemic issues distorting the influential Chatbot Arena leaderboard, termed the "Leaderboard Illusion." They found undisclosed private testing allows select providers to bias scores through selective disclosure, and that proprietary models receive disproportionately more sampling and data access compared to open models, leading to overfitting and an unfair playing field; actionable reforms are recommended.*** <br> <br>
    Apr 29, Cohere, Princeton Uni, Stanford Uni, MIT et al published a [paper](https://arxiv.org/pdf/2504.20879) “The Leaderboard Illusion”. Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, this study identifies systematic issues that have resulted in a distorted playing field. The study finds that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. The study establishes that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, the work identifies 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. The study also establishes that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. The study shows that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on the conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. The study offers actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field. <br> <br>

17. ***Developing a retriever for reasoning tasks:  <br>Meta et al. presented ReasonIR-8B, the first information retriever specifically trained for general reasoning tasks, addressing limitations of existing retrievers. By using a novel synthetic data pipeline that generates challenging queries and hard negatives, ReasonIR-8B achieved state-of-the-art performance on the reasoning-focused BRIGHT benchmark and significantly boosted RAG performance on MMLU and GPQA compared to baselines and other retrievers.*** <br> <br>
    Apr 29, Meta, Uni of Washington, Stanford Uni, MIT et al published a [paper](https://arxiv.org/pdf/2504.20595) “ReasonIR: Training Retrievers for Reasoning Tasks”. The study presents ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. The study develops a synthetic data generation pipeline that, for each document, the pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of the synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. The training recipe is general and can be easily extended to future LLMs. https://github.com/facebookresearch/ReasonIR <br> <br>

19. ***Introducing the Softpick attention mechanism:  <br>Researchers from MBZUAI introduced softpick, a rectified, non-sum-to-one alternative to the standard softmax function in transformer attention mechanisms. Experiments show softpick eliminates attention sink and large activations, maintains performance parity, generates sparser attention maps with lower activation kurtosis, and notably outperforms softmax under quantization, suggesting benefits for model efficiency and interpretability.*** <br> <br>
    Apr 29, MBZUAI published a [paper](https://arxiv.org/pdf/2504.20966) “Softpick: No Attention Sink, No Massive Activations with Rectified Softmax”. The study introduces softpick, a rectified, not sum-to-one, drop-in replacement for softmax in transformer attention mechanisms that eliminates attention sink and massive activations. Experiments with 340M parameter models demonstrate that softpick maintains performance parity with softmax on standard benchmarks while achieving 0% sink rate. The softpick transformer produces hidden states with significantly lower kurtosis (340 vs 33,510) and creates sparse attention maps (46.97% sparsity). Models using softpick consistently outperform softmax when quantized, with particularly pronounced advantages at lower bit precisions. The analysis and discussion shows how softpick has the potential to open new possibilities for quantization, low-precision training, sparsity optimization, pruning, and interpretability. https://github.com/zaydzuhri/softpick-attention. <br> <br>

21. ***Investigating LLM activation monitoring techniques:  <br>OpenAI compared methods for monitoring LLM internal activations to detect unsafe behavior, evaluating linear probing, prompted probing (using task prompts at test time), and sparse autoencoder (SAE)-based probing against zero-shot prompting. They found activation probing significantly outperforms zero-shot prompting given sufficient data, recommending prompted probing for its data efficiency when inference compute is available, and SAE-based methods when compute is limited.*** <br> <br>
    Apr 28, OpenAI published a [paper](https://arxiv.org/pdf/2504.20271) “Investigating task-specific prompts and sparse autoencoders for activation monitoring”. Language models can behave in unexpected and unsafe ways, and so it is valuable to monitor their outputs. Internal activations of language models encode additional information that could be useful for this. The baseline approach for activation monitoring is some variation of linear probing on a particular layer: starting from a labeled dataset, train a logistic regression classifier on that layer's activations. Recent work has proposed several approaches which may improve on naive linear probing, by leveraging additional computation. One class of techniques, which is called "prompted probing," leverages test time computation to improve monitoring by (1) prompting the model with a description of the monitoring task, and (2) applying a learned linear probe to resulting activations. Another class of techniques uses computation at train time: training sparse autoencoders offline to identify an interpretable basis for the activations, and e.g. max-pooling activations across tokens using that basis before applying a linear probe. However, one can also prompt the model with a description of the monitoring task and use its output directly. The study develops and test novel refinements of these methods and compare them against each other. The work finds asking the model zero-shot is a reasonable baseline when inference-time compute is not limited; however, activation probing methods can substantially outperform this baseline given sufficient training data. Specifically, the study recommends prompted probing when inference-time compute is available, due to its superior data efficiency and good generalization performance. Alternatively, if inference-time compute is limited, the work finds SAE-based probing methods outperform raw activation probing. <br> <br>

23. ***A scalable long-term memory system for AI agents:  <br>The Mem0 paper introduces a memory-centric architecture designed to give LLMs scalable long-term memory, overcoming fixed context window limits for multi-session dialogues by dynamically managing salient information. Comprehensive evaluations showed Mem0 and its graph-based variant significantly outperform various baselines, including RAG and full-context methods, in conversational consistency and accuracy metrics, while dramatically reducing computational overhead like latency and token costs.*** <br> <br>
    Apr 28, mem0 published a [paper](https://arxiv.org/pdf/2504.19413) “Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory”. Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. The study introduces Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, the study further proposes an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on LOCOMO benchmark, the work systematically compares the approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and k-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results show that the methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26% relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around 2% higher overall score than the base configuration. Beyond accuracy gains, the study also markedly reduces computational overhead compared to full-context method. In particular, Mem0 attains a 91% lower p95 latency and saves more than 90% token cost, offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. The findings highlight critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents. https://github.com/mem0ai/mem0/tree/main/evaluation <br> <br>

25. ***Anthropic's analysis of AI in software development:  <br>Based on analyzing 500,000 coding interactions with its Claude models, Anthropic identified key trends in AI's impact on software development. Findings show that more agentic AI (Claude Code) drives higher automation, AI is frequently used for user-facing web applications suggesting potential disruption in those roles, and startups are adopting these tools much faster than enterprises, highlighting a potential competitive shift.*** <br> <br>
    Apr 28, Anthropic published an [article](https://www.anthropic.com/research/impact-software-development) “Anthropic Economic Index: AI’s Impact on Software Development”. The article analyzes the impact of AI, specifically Anthropic's Claude and its specialized Claude Code agent, on computer programming jobs, a small but influential sector seeing dramatic changes due to AI assistance. Based on an analysis of 500,000 coding-related interactions, three key patterns were identified. Firstly, the more agentic Claude Code is used significantly more for task automation (79% of conversations) compared to the standard Claude.ai (49%), suggesting that specialized AI agents will likely increase automation levels. Secondly, developers commonly leverage AI for building user-facing applications, with web development languages like JavaScript and HTML being prevalent, indicating that jobs focused on creating simple interfaces and apps may face earlier disruption. Lastly, startups are the primary early adopters of Claude Code (identified in 33% of conversations), significantly ahead of enterprises (13%), suggesting a potential competitive advantage gap for nimbler organizations. These findings highlight a shift towards more automated workflows in coding, particularly in front-end development, and position the tech sector as a potential leading indicator for how AI might transform other occupations in the future. <br> <br>

27. ***Demis Hassabis's AGI timeline prediction:  <br> <br>Google DeepMind CEO Demis Hassabis forecasted that Artificial General Intelligence (AGI) will achieve human-level competence within five to ten years, potentially leading to significant workforce changes including job displacement as capable AI systems emerge. While acknowledging current AI limitations, he anticipates rapid advancements, a view that contrasts with some other tech leaders' more conservative timelines but aligns with a broader expectation of AI's growing role in labor.*** <br> <br>
    Apr 27, Time published an [article]](https://time.com/7280740/demis-hassabis-interview/) “Google DeepMind CEO Demis Hassabis on AI in the Military and What AGI Could Mean for Humanity”. Google DeepMind CEO Demis Hassabis predicts that artificial general intelligence (AGI) will reach human-level competence within the next five to ten years. This development could significantly impact the workforce, as AI systems begin to exhibit the complex capabilities of humans. Hassabis noted that while current AI systems are impressive, they still lack many human abilities, but advancements in the coming years will bridge this gap. This shift is expected to transform workplaces, with digital colleagues working alongside humans, potentially leading to job displacement. Other tech leaders, like Baidu's Robin Li, have more conservative timelines, suggesting AGI might take over a decade to develop. However, the consensus among many CEOs is that AI will soon play a crucial role in the workforce, with some predicting significant reductions in human labor to accommodate AI. This transition raises concerns about job security, as AI-driven systems can perform tasks faster and without the need for salaries or benefits. Despite these concerns, proponents argue that AI will optimize professional lives rather than replace human workers entirely. <br> <br>

29. ***The LawFlow dataset for legal reasoning:  <br>Researchers from the University of Minnesota introduced LawFlow, a novel dataset capturing complete, dynamic, and iterative legal workflows from law students performing real-world tasks, aiming to overcome the limitations of existing datasets focused on isolated subtasks. Comparing human and LLM workflows revealed humans are more modular and adaptive while LLMs are more sequential and exhaustive, suggesting AI is better suited for supportive roles and informing design principles for collaborative legal AI systems.*** <br> <br>
    Apr 26, Uni of Minnesota published a [paper](https://arxiv.org/pdf/2504.18942) “LawFlow : Collecting and Simulating Lawyers' Thought Processes”. Legal practitioners, particularly those early in their careers, face complex, high-stakes tasks that require adaptive, context-sensitive reasoning. While AI holds promise in supporting legal work, current datasets and models are narrowly focused on isolated subtasks and fail to capture the end-to-end decision-making required in real-world practice. To address this gap, the study introduces LawFlow, a dataset of complete end-to-end legal workflows collected from trained law students, grounded in real-world business entity formation scenarios. Unlike prior datasets focused on input-output pairs or linear chains of thought, LawFlow captures dynamic, modular, and iterative reasoning processes that reflect the ambiguity, revision, and client-adaptive strategies of legal practice. Using LawFlow, the study compares human and LLM-generated workflows, revealing systematic differences in structure, reasoning flexibility, and plan execution. Human workflows tend to be modular and adaptive, while LLM workflows are more sequential, exhaustive, and less sensitive to downstream implications. The findings also suggest that legal professionals prefer AI to carry out supportive roles, such as brainstorming, identifying blind spots, and surfacing alternatives, rather than executing complex workflows end-to-end. Building on these findings, the study proposes a set of design suggestions, rooted in empirical observations, that align AI assistance with human goals of clarity, completeness, creativity, and efficiency, through hybrid planning, adaptive execution, and decision-point support. The results highlight both the current limitations of LLMs in supporting complex legal workflows and opportunities for developing more collaborative, reasoning-aware legal AI systems. All data and code are available on project page (https://minnesotanlp.github.io/LawFlow-website/). <br> <br>

31. ***Analyzing scaling laws for AI oversight:  <br>MIT researchers investigated the scaling properties of scalable oversight (weaker AI supervising stronger AI), proposing a framework to quantify success probability based on capability mismatches modeled using Elo scores. After validating with games and deriving scaling laws for specific oversight scenarios, they analyzed Nested Scalable Oversight (NSO), finding theoretically and numerically that success rates decrease significantly (e.g., below 52% for a 400 Elo gap) as the capability difference grows.*** <br> <br>
    Apr 25, MIT published a [paper](https://arxiv.org/pdf/2504.18530) “Scaling Laws For Scalable Oversight”. Scalable oversight, the process by which weaker AI systems supervise stronger ones, has been proposed as a key strategy to control future superintelligent systems. However, it is still unclear how scalable oversight itself scales. To address this gap, the study proposes a framework that quantifies the probability of successful oversight as a function of the capabilities of the overseer and the system being overseen. Specifically, the framework models oversight as a game between capability-mismatched players; the players have oversight-specific and deception-specific Elo scores that are a piecewise-linear function of their general intelligence, with two plateaus corresponding to task incompetence and task saturation. The work validates the framework with a modified version of the game Nim and then apply it to four oversight games: "Mafia", "Debate", "Backdoor Code" and "Wargames". For each game, the study finds scaling laws that approximate how domain performance depends on general AI system capability (using Chatbot Arena Elo as a proxy for general capability). The work then builds on the findings in a theoretical study of Nested Scalable Oversight (NSO), a process in which trusted models oversee untrusted stronger models, which then become the trusted models in the next step. The study identifies conditions under which NSO succeeds and derive numerically (and in some cases analytically) the optimal number of oversight levels to maximize the probability of oversight success. In the numerical examples, the NSO success rate is below 52% when overseeing systems that are 400 Elo points stronger than the baseline overseer, and it declines further for overseeing even stronger systems. <br> <br>

33. ***An argument against LLM progress toward AGI:  <br>This article contends that despite advances, LLMs represent no meaningful progress towards Artificial General Intelligence, arguing they rely on sophisticated statistical pattern matching that merely simulates reasoning, often fabricating explanations for their outputs rather than reflecting true understanding. It asserts LLMs cannot determine objective truth, struggle with novelty, are inefficient compared to true intelligence, and are thus fundamentally limited mimics unsuitable for genuine automation or innovation.*** <br> <br>
    Apr 22, MindPrison published an [article](https://www.mindprison.cc/p/no-progress-toward-agi-llm-braindead-unreliable) “We Have Made No Progress Toward AGI”. Despite advances in Large Language Models (LLMs), we have made no meaningful progress toward true Artificial General Intelligence. Research from Anthropic reveals that LLMs don't actually reason—they employ complex statistical pattern matching that merely simulates intelligence. When LLMs explain their reasoning, these explanations are completely fabricated and don't reflect their internal processes. For example, when solving a simple math problem like "36+59=95," LLMs use memorized patterns and heuristics rather than algorithmic reasoning, yet falsely claim they "carried the one" when explaining their process. This pattern extends to their use of tools, where newer models increasingly hallucinate actions they never performed. LLMs fundamentally can't determine what is objectively right or wrong; they can only predict what is statistically likely based on training data. They excel at mimicking existing patterns but struggle with novel tasks outside their training distribution. Unlike true intelligence, which operates efficiently, LLMs require ever-increasing data and energy to improve marginally. While useful for certain applications, LLMs will never achieve true reasoning or create new semantic information. Their architecture makes them inherently unreliable for full automation or groundbreaking innovation, as they remain sophisticated pattern matchers rather than understanding systems. <br> <br>

35. ***Extending long-thought reasoning to perception:  <br>Researchers introduced the LongPerceptualThoughts dataset and a novel three-stage synthesis framework to generate long chain-of-thought reasoning traces for perceptual tasks, aiming to transfer benefits seen in math/code. Training a model on this dataset demonstrated notable improvements across vision benchmarks (+3.4 avg, +11.8 on V* Bench) and even enhanced text reasoning performance, suggesting value in distilling System-2 reasoning for System-1 perception tasks.*** <br> <br>
    Apr 21, Uni of Toronto and Nvidia published a [paper](https://arxiv.org/pdf/2504.15362) “LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception”. Recent reasoning models through test-time scaling have demonstrated that long chain-of-thoughts can unlock substantial performance boosts in hard reasoning tasks such as math and code. However, the benefit of such long thoughts for system-2 reasoning is relatively less explored in other domains such as perceptual tasks where shallower, system-1 reasoning seems sufficient. This study introduces LongPerceptualThoughts, a new synthetic dataset with 30K long-thought traces for perceptual tasks. The key challenges in synthesizing elaborate reasoning thoughts for perceptual tasks are that off-the-shelf models are not yet equipped with such thinking behavior and that it is not straightforward to build a reliable process verifier for perceptual tasks. Thus, the study proposes a novel three-stage data synthesis framework that first synthesizes verifiable multiple-choice questions from dense image descriptions, then extracts simple CoTs from VLMs for those verifiable problems, and finally expands those simple thoughts to elaborate long thoughts via frontier reasoning models. In controlled experiments with a strong instruction-tuned 7B model, the work demonstrates notable improvements over existing visual reasoning data-generation methods. The model, trained on the generated dataset, achieves an average +3.4 points improvement over 5 vision-centric benchmarks, including +11.8 points on V∗ Bench. Notably, despite being tuned for vision tasks, it also improves performance on the text reasoning benchmark, MMLU-Pro, by +2 points. https://andrewliao11.github.io/LongPerceptualThoughts/

 <br> <br> <br>

***Apr 27, 2025***

1. ***A joint study on sparse attention:  <br>This investigation explored its trade-offs for long-context LLMs. Findings include that larger, sparser models can outperform smaller, denser ones for very long sequences, achievable sparsity varies by task phase and model size, no single sparse strategy excels universally, and moderate sparsity can harm performance, indicating it's a useful but context-dependent tool requiring careful evaluation.*** <br> <br>
   Apr 24, Meta, Cohere and Uni of Edinburgh published a [paper](https://arxiv.org/pdf/2504.17768) “The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs”. Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, the study performs a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on experiments, the work reports a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) the work introduces and validate novel scaling laws specifically tailored for sparse attention, providing evidence that the findings are likely to hold true beyond the range of experiments. Through these insights, the study demonstrates that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications. <br> <br>

3. ***Nvidia's AIMO-2 winning paper:  <br>This work presented a three-pillar strategy for building state-of-the-art mathematical reasoning models. The strategy involves creating a large-scale dataset (OpenMathReasoning), integrating code execution via iterative training and quality filtering, and developing a generative solution selection (GenSelect) method that significantly improves upon baseline voting.*** <br> <br>
   Apr 24, Nvidia published a [paper](https://arxiv.org/pdf/2504.16891) “AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset”. This paper presents a winning submission to the AI Mathematical Olympiad - Progress Prize 2 (AIMO-2) competition. The recipe for building state-of-the-art mathematical reasoning models relies on three key pillars. First, the work creates a large-scale dataset comprising 540K unique high-quality math problems, including olympiad-level problems, and their 3.2M long-reasoning solutions. Second, the work develops a novel method to integrate code execution with long reasoning models through iterative training, generation, and quality filtering, resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, the work creates a pipeline to train models to select the most promising solution from many candidates. The study shows that such generative solution selection (GenSelect) can significantly improve upon majority voting baseline. Combining these ideas, the study trains a series of models that achieve state-of-the-art results on mathematical reasoning benchmarks. https://github.com/NVIDIA/NeMo-Skills <br> <br>

5. **A paper introducing the I-Con framework:  <br>Researchers from MIT, Google, and Microsoft presented a single information-theoretic equation based on minimizing integrated KL divergence. This framework unifies diverse representation learning loss functions (clustering, contrastive, supervised learning, etc.), connects over 23 approaches, and leads to state-of-the-art unsupervised image classifiers and principled debiasing methods.*** <br> <br>
   Apr 23, MIT, Google and Microsoft published a [paper](https://arxiv.org/pdf/2504.16929) “I-Con: A Unifying Framework for Representation Learning”. As the field of representation learning grows, there has been a proliferation of different loss functions to solve different classes of problems. The study introduces a single information-theoretic equation that generalizes a large collection of modern loss functions in machine learning. In particular, the study introduces a framework that shows that several broad classes of machine learning methods are precisely minimizing an integrated KL divergence between two conditional distributions: the supervisory and learned representations. This viewpoint exposes a hidden information geometry underlying clustering, spectral methods, dimensionality reduction, contrastive learning, and supervised learning. This framework enables the development of new loss functions by combining successful techniques from across the literature. The study not only presents a wide array of proofs, connecting over 23 different approaches, but also leverages these theoretical results to create state-of-the-art unsupervised image classifiers that achieve a +8% improvement over the prior state-of-the-art on unsupervised classification on ImageNet-1K. The work also demonstrates that I-Con can be used to derive principled debiasing methods which improve contrastive representation learners. <br> <br>

7. ***Research on Generalized Neighborhood Attention (GNA):  <br>This work from Georgia Tech, Nvidia, and UIUC addresses the challenge of achieving speedups with sparse attention. It introduces GNA to unify local attention types, develops a realistic performance simulator, and implements GNA efficiently for Nvidia Blackwell architecture, achieving near-theoretical speedups and significant (28-46%) end-to-end gains in generative models without fine-tuning.*** <br> <br>
   Apr 23, Georgia Tch, Nvidia  and UIUC published a [paper](https://arxiv.org/pdf/2504.16922) “Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light”. Many sparse attention mechanisms such as Neighborhood Attention have typically failed to consistently deliver speedup over the self attention baseline. This is largely due to the level of complexity in attention infrastructure, and the rapid evolution of AI hardware architecture. At the same time, many state-of-the-art foundational models, particularly in computer vision, are heavily bound by attention, and need reliable sparsity to escape the O(n^2) complexity. This paper studies a class of promising sparse attention mechanisms that focus on locality, and aim to develop a better analytical model of their performance improvements. The study first introduces Generalized Neighborhood Attention (GNA), which can describe sliding window, strided sliding window, and blocked attention. The study then considers possible design choices in implementing these approaches, and create a simulator that can provide much more realistic speedup upper bounds for any given setting. Finally, the study implements GNA on top of a state-of-the-art fused multi-headed attention (FMHA) kernel designed for the NVIDIA Blackwell architecture in CUTLASS. The implementation can fully realize the maximum speedup theoretically possible in many perfectly block-sparse cases, and achieves an effective utilization of 1.3 petaFLOPs/second in FP16. In addition, the study plugs various GNA configurations into off-the-shelf generative models, such as Cosmos-7B, HunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end speedup on B200 without any fine-tuning. <br> <br>

9. ***A Google study on LLM decision-making:  <br>This work investigated sub-optimal performance in LLMs, identifying failure modes like greediness, frequency bias, and the knowing-doing gap. It proposed mitigating these issues via RL fine-tuning on self-generated CoT rationales; experiments demonstrated this approach enhances exploration and narrows the knowing-doing gap, with further analysis on effective exploration mechanisms.*** <br> <br>
    Apr 22, Google published a [paper](https://arxiv.org/pdf/2504.16078) “LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities”. The success of Large Language Models (LLMs) has sparked interest in various agentic applications. A key hypothesis is that LLMs, leveraging common sense and Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently solve complex domains. However, LLM agents have been found to suffer from sub-optimal exploration and the knowing-doing gap, the inability to effectively act on knowledge present in the model. This work systematically studies why LLMs perform sub-optimally in decision-making scenarios. In particular, the study closely examines three prevalent failure modes: greediness, frequency bias, and the knowing-doing gap. The work proposes mitigation of these shortcomings by fine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales. Experiments across multi-armed bandits, contextual bandits, and Tic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making abilities of LLMs by increasing exploration and narrowing the knowing-doing gap. Finally, the authors study both classic exploration mechanisms, such as epsilon-greedy, and LLM-specific approaches, such as self-correction and self-consistency, to enable more effective fine-tuning of LLMs for decision-making. <br> <br>

11. ***The TTRL paper:  <br>Researchers from Tsinghua Uni and SAIL introduced Test-Time Reinforcement Learning (TTRL). This method trains LLMs using RL on unlabeled data during inference by leveraging reward signals from Test-Time Scaling techniques like majority voting, demonstrating significant performance improvements (e.g., +159% on AIME 2024 for Qwen-2.5-Math-7B) and surpassing initial model limits without ground-truth labels.*** <br> <br>
    Apr 22, Tsinghua Uni and SAIL published a [paper](https://arxiv.org/pdf/2504.16084) “TTRL: Test-Time Reinforcement Learning”. This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, the study finds that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. This work introduces Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL <br> <br>

13. ***Introducing Tina models:  <br>The University of Southern California demonstrated achieving strong reasoning cost-effectively by applying parameter-efficient LoRA updates during RL fine-tuning to a tiny 1.5B base model. This minimalist approach yields performance competitive with SOTA models (e.g., 43.33% Pass@1 on AIME24) at a minuscule post-training cost (estimated $9, a 260x reduction), suggesting LoRA efficiently adapts reasoning structure while preserving knowledge.*** <br> <br>
    Apr 22, Uni of Southern California published a [paper](https://arxiv.org/pdf/2504.15777) “Tina: Tiny Reasoning Models via LoRA”. How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, the study presents Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20% reasoning performance increase and 43.33% Pass@1 accuracy on AIME24, at only $9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). The work reveals the surprising effectiveness of efficient RL reasoning via LoRA. The work validates this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, the study hypothesizes that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model’s underlying knowledge. https://github.com/shangshang-wang/Tina <br> <br>

15. ***A study on LLM creative limits:  <br>Researchers from Google and CMU utilized minimal algorithmic tasks abstracting open-ended challenges (like discovering connections or constructing patterns). Their work demonstrates that next-token prediction is myopic, while multi-token approaches excel in diversity; it also finds input-layer noise injection superior to output-layer sampling for randomness, providing a testbed and arguing for methods beyond standard generation.*** <br> <br>
    Apr 21, Google and CMU published a [paper](https://arxiv.org/pdf/2504.15266) “Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction”. The study designs a suite of minimal algorithmic tasks that are a loose abstraction of open-ended real-world tasks. This allows to cleanly and controllably quantify the creative limits of the present-day language model. Much like real-world tasks that require a creative, far-sighted leap of thought, the tasks require an implicit, open-ended stochastic planning step that either (a) discovers new connections in an abstract knowledge graph (like in wordplay, drawing analogies, or research) or (b) constructs new patterns (like in designing math problems or new proteins). In these tasks, the study empirically and conceptually argues how next-token learning is myopic and memorizes excessively; comparatively, multi-token approaches, namely teacherless training and diffusion models, excel in producing diverse and original output. Secondly, in the tasks, the work finds that to elicit randomness from the Transformer without hurting coherence, it is better to inject noise right at the input layer (via a method we dub hash-conditioning) rather than defer to temperature sampling from the output layer. Thus, the work offers a principled, minimal test-bed for analyzing open-ended creative skills, and offers new arguments for going beyond next-token learning and softmax-based sampling. https://github.com/chenwu98/algorithmic-creativity <br> <br>

17. ***Introducing Causal-Copilot:  <br>UCSD presented an autonomous agent addressing the complexity and inaccessibility of causal analysis. Causal-Copilot automates the full pipeline (discovery, inference, interpretation, etc.) within an LLM framework for tabular and time-series data, supports natural language interaction, integrates over 20 techniques, and aims to bridge the gap between domain experts and causal researchers while outperforming baselines.*** <br> <br>
    Apr 21, UCSD published a [paper](https://arxiv.org/pdf/2504.13263) “Causal-Copilot: An Autonomous Causal Analysis Agent”. Causal analysis plays a foundational role in scientific discovery and reliable decision-making, yet it remains largely inaccessible to domain experts due to its conceptual and algorithmic complexity. This disconnect between causal methodology and practical usability presents a dual challenge: domain experts are unable to leverage recent advances in causal learning, while causal researchers lack broad, real-world deployment to test and refine their methods. To address this, the study introduces Causal-Copilot, an autonomous agent that operationalizes expert-level causal analysis within a large language model framework. Causal-Copilot automates the full pipeline of causal analysis for both tabular and time-series data -- including causal discovery, causal inference, algorithm selection, hyperparameter optimization, result interpretation, and generation of actionable insights. It supports interactive refinement through natural language, lowering the barrier for non-specialists while preserving methodological rigor. By integrating over 20 state-of-the-art causal analysis techniques, the system fosters a virtuous cycle -- expanding access to advanced causal methods for domain experts while generating rich, real-world applications that inform and advance causal theory. Empirical evaluations demonstrate that Causal-Copilot achieves superior performance compared to existing baselines, offering a reliable, scalable, and extensible solution that bridges the gap between theoretical sophistication and real-world applicability in causal analysis. A live interactive demo of Causal-Copilot is available at https://causalcopilot.com/. <br> <br>

19. ***Presenting UFO2:  <br>Microsoft et al. introduced a multi-agent AgentOS for Windows aiming to make Computer-Using Agents practical system-level tools. UFO2 overcomes limitations like shallow OS integration and fragile interaction by featuring a coordinating HostAgent and specialized AppAgents with native APIs, hybrid UI detection, efficient planning, and a Picture-in-Picture interface, demonstrating improved robustness and accuracy across many Windows apps.*** <br> <br>
    Apr 20, Microsoft et al published a [paper](https://arxiv.org/pdf/2504.14603) “UFO2: The Desktop AgentOS”. Recent Computer-Using Agents (CUAs), powered by multimodal large language models (LLMs), offer a promising direction for automating complex desktop workflows through natural language. However, most existing CUAs remain conceptual prototypes, hindered by shallow OS integration, fragile screenshot-based interaction, and disruptive execution. The study presents UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs into practical, system-level automation. UFO2 features a centralized HostAgent for task decomposition and coordination, alongside a collection of application-specialized AppAgent equipped with native APIs, domain-specific knowledge, and a unified GUI--API action layer. This architecture enables robust task execution while preserving modularity and extensibility. A hybrid control detection pipeline fuses Windows UI Automation (UIA) with vision-based parsing to support diverse interface styles. Runtime efficiency is further enhanced through speculative multi-action planning, reducing per-step LLM overhead. Finally, a Picture-in-Picture (PiP) interface enables automation within an isolated virtual desktop, allowing agents and users to operate concurrently without interference. The study evaluates UFO2 across over 20 real-world Windows applications, demonstrating substantial improvements in robustness and execution accuracy over prior CUAs. Results show that deep OS integration unlocks a scalable path toward reliable, user-aligned desktop automation. https://github.com/microsoft/UFO/ <br> <br>

21. ***Introducing the PROMPTEVALS dataset:  <br>UC Berkeley and LangChain addressed the challenge of creating reliability assertions for production LLM pipelines. They provide a large collection (2087 prompts, 12623 criteria, 5x larger than previous) sourced from developers; benchmarking showed fine-tuned Mistral/Llama 3 models outperformed GPT-4o by ~21% in generating relevant assertions, offering a resource to advance research in LLM reliability and alignment.*** <br> <br>
    Apr 20, UC Berkeley and LangChain published a [paper](https://arxiv.org/pdf/2504.14738) “PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines”. Large language models (LLMs) are increasingly deployed in specialized production data processing pipelines across diverse domains -- such as finance, marketing, and e-commerce. However, when running them in production across many inputs, they often fail to follow instructions or meet developer expectations. To improve reliability in these applications, creating assertions or guardrails for LLM outputs to run alongside the pipelines is essential. Yet, determining the right set of assertions that capture developer requirements for a task is challenging. This study introduces PROMPTEVALS, a dataset of 2087 LLM pipeline prompts with 12623 corresponding assertion criteria, sourced from developers using the open-source LLM pipeline tools. This dataset is 5x larger than previous collections. Using a hold-out test split of PROMPTEVALS as a benchmark, the work evaluated closed- and open-source models in generating relevant assertions. Notably, the fine-tuned Mistral and Llama 3 models outperform GPT-4o by 20.93% on average, offering both reduced latency and improved performance. The authors believe the dataset can spur further research in LLM reliability, alignment, and prompt engineering. https://huggingface.co/datasets/reyavir/PromptEvals <br> <br>

23. ***Proposing Attribution with Attention (AT2):  <br>MIT addressed the high cost and unreliability of using attention weights for token attribution. Their method treats weights from different heads as features and learns an effective combination (using ablation signals), achieving performance comparable to expensive ablation methods much more efficiently, and demonstrating utility in context pruning for QA.*** <br> <br>
    Apr 18, MIT published a [paper](https://arxiv.org/pdf/2504.13752) “Learning to Attribute with Attention”. Given a sequence of tokens generated by a language model, people may want to identify the preceding tokens that influence the model to generate this sequence. Performing such token attribution is expensive; a common approach is to ablate preceding tokens and directly measure their effects. To reduce the cost of token attribution, the study revisits attention weights as a heuristic for how a language model uses previous tokens. Naive approaches to attribute model behavior with attention (e.g., averaging attention weights across attention heads to estimate a token's influence) have been found to be unreliable. To attain faithful attributions, the work proposes treating the attention weights of different attention heads as features. This way, the study can learn how to effectively leverage attention weights for attribution (using signal from ablations). The resulting method, Attribution with Attention (AT2), reliably performs on par with approaches that involve many ablations, while being significantly more efficient. To showcase the utility of AT2, the study uses it to prune less important parts of a provided context in a question answering setting, improving answer quality. Code for AT2 https://github.com/MadryLab/AT2 <br> <br>

25. ***Stanford's Cost-of-Pass framework:  <br>This approach proposes evaluating language models based on economic value. It involves calculating the "cost-of-pass" (expected cost for a correct solution) and tracking the minimum achievable "frontier cost-of-pass"; analysis reveals different model types are optimal for different tasks, significant cost reductions over time are driven by model innovations, and inference-time techniques often provide marginal cost-benefit compared to better models.*** <br> <br>
    Apr 17, Stanford Uni published a [paper](https://arxiv.org/pdf/2504.13359) “Cost-of-Pass: An Economic Framework for Evaluating Language Models”. The widespread adoption of AI systems in the economy hinges on their ability to generate economic value that outweighs their inference costs. Evaluating this tradeoff requires metrics that account for both performance and costs. The study proposes a framework grounded in production theory for evaluating language models by combining accuracy and inference cost. The study introduces "cost-of-pass", the expected monetary cost of generating a correct solution. The work then defines the "frontier cost-of-pass" as the minimum cost-of-pass achievable across available models or the "human-expert, using the approximate cost of hiring an expert. The analysis reveals distinct economic insights. First, lightweight models are most cost-effective for basic quantitative tasks, large models for knowledge-intensive ones, and reasoning models for complex quantitative problems, despite higher per-token costs. Second, tracking this frontier cost-of-pass over the past year reveals significant progress, particularly for complex quantitative tasks where the cost has roughly halved every few months. Third, to trace key innovations driving this progress, the study examines counterfactual frontiers: estimates of cost-efficiency without specific model classes. The study finds that innovations in lightweight, large, and reasoning models have been essential for pushing the frontier in basic quantitative, knowledge-intensive, and complex quantitative tasks, respectively. Finally, the research assesses the cost-reductions afforded by common inference-time techniques like majority voting and self-refinement, finding that their marginal accuracy gains rarely justify their costs. The findings underscore that complementary model-level innovations are the primary drivers of cost-efficiency, and the economic framework provides a principled tool for measuring this progress and guiding deployment. <br> <br>

27. ***The ToolRL paper:  <br>Researchers from UIUC addressed the challenge of teaching LLMs tool use via reinforcement learning, focusing on reward design. Arguing that supervised fine-tuning lacks generalization and coarse RL rewards are insufficient, they conducted a systematic study and proposed a principled reward design for tool tasks, implemented with GRPO, achieving robust training and significant performance gains (17% over base, 15% over SFT models).*** <br> <br>
    Apr 16, UIUC published a [paper](https://arxiv.org/pdf/2504.13958) “ToolRL: Reward is All Tool Learning Needs”. Current Large Language Models (LLMs) often undergo supervised fine-tuning (SFT) to acquire tool use capabilities. However, SFT struggles to generalize to unfamiliar or complex tool use scenarios. Recent advancements in reinforcement learning (RL), particularly with R1-like models, have demonstrated promising reasoning and generalization abilities. Yet, reward design for tool use presents unique challenges: multiple tools may be invoked with diverse parameters, and coarse-grained reward signals, such as answer matching, fail to offer the finegrained feedback required for effective learning. This study presents the first comprehensive study on reward design for tool selection and application tasks within the RL paradigm. The study systematically explores a wide range of reward strategies, analyzing their types, scales, granularity, and temporal dynamics. Building on these insights, the study proposes a principled reward design tailored for tool use tasks and apply it to train LLMs using Group Relative Policy Optimization (GRPO). Empirical evaluations across diverse benchmarks demonstrate that the approach yields robust, scalable, and stable training, achieving a 17% improvement over base models and a 15% gain over SFT models. These results highlight the critical role of thoughtful reward design in enhancing the tool use capabilities and generalization performance of LLMs. https://github.com/qiancheng0/ToolRL <br> <br>

29. ***Exploring multilingual LLM reasoning:  <br>Researchers from Nanjing Uni, SAIL, and CMU investigated the potential beyond the typical "English bias." They found evidence that reasoning across multiple languages offers a significantly (nearly 10 Acc@k points) and robustly higher performance upper bound than English-only approaches, though current answer selection methods are insufficient to reach it.*** <br> <br>
    Apr 16, Nanjing Uni, SAIL and CMU published a [paper](https://arxiv.org/pdf/2504.11833) “Could Thinking Multilingually Empower LLM Reasoning?”. Previous work indicates that large language models exhibit a significant "English bias", i.e. they often perform better when tasks are presented in English. Interestingly, the study has observed that using certain other languages in reasoning tasks can yield better performance than English. However, this phenomenon remains under-explored. This study explores the upper bound of harnessing multilingualism in reasoning tasks, suggesting that multilingual reasoning promises significantly (by nearly 10 Acc@k points) and robustly (tolerance for variations in translation quality and language choice) higher upper bounds than English-only reasoning. Besides analyzing the reason behind the upper bound and challenges in reaching it, the study also finds that common answer selection methods cannot achieve this upper bound, due to their limitations and biases. These insights could pave the way for future research aimed at fully harnessing the potential of multilingual reasoning in LLMs. <br> <br>

31. ***Introducing NodeRAG:  <br>Researchers from Columbia Uni et al. addressed limitations in graph-based RAG. They propose a graph-centric framework introducing heterogeneous graph structures to enable seamless integration of graph methods into the RAG workflow, aligning with LLM capabilities for efficiency. Experiments demonstrate NodeRAG's advantages over prior methods in efficiency (indexing/query time, storage) and superior question-answering performance.*** <br> <br>
    Apr 15, Columbia Uni, Uni of Penn. And Lehigh Uni published a [paper](https://arxiv.org/pdf/2504.11544) “NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes”. Retrieval-augmented generation (RAG) empowers large language models to access external and private corpus, enabling factually consistent responses in specific domains. By exploiting the inherent structure of the corpus, graph-based RAG methods further enrich this process by building a knowledge graph index and leveraging the structural nature of graphs. However, current graph-based RAG approaches seldom prioritize the design of graph structures. Inadequately designed graph not only impede the seamless integration of diverse graph algorithms but also result in workflow inconsistencies and degraded performance. To further unleash the potential of graph for RAG, the work proposes NodeRAG, a graph-centric framework introducing heterogeneous graph structures that enable the seamless and holistic integration of graph-based methodologies into the RAG workflow. By aligning closely with the capabilities of LLMs, this framework ensures a fully cohesive and efficient end-to-end process. Through extensive experiments, the study demonstrates that NodeRAG exhibits performance advantages over previous methods, including GraphRAG and LightRAG, not only in indexing time, query time, and storage efficiency but also in delivering superior question-answering performance on multi-hop benchmarks and open-ended head-to-head evaluations with minimal retrieval tokens. https://github.com/Terry-Xu-666/NodeRAG <br> <br>

33. ***AA Communications of the ACM article:  <br>This piece discusses how GenAI presents existential challenges but also convergence opportunities for computer science and humanities. By automating routine tasks, democratizing access, and enabling new global comparative research (an "AI turn"), GenAI can free scholars for deeper inquiry and create a new intellectual terrain where computational and humanistic approaches partner to revitalize both fields.*** <br> <br>
    Apr 10, Communications of the ACM published an [article](https://cacm.acm.org/blogcacm/the-converging-paths-of-computer-science-and-the-humanities-in-the-age-of-genai/) “The Converging Paths of Computer Science and the Humanities in the Age of GenAI”. In 2025, both computer science and the humanities face existential crises due to the rise of GenAI, questioning their purpose and relevance. While humanities programs struggle with declining enrollment due to uncertain career prospects, computer science grapples with GenAI's potential to automate core intellectual tasks like coding. However, these challenges present an opportunity for convergence, with GenAI potentially revitalizing both fields by addressing core limitations. By automating routine language-intensive tasks such as translating historical texts or debugging code, GenAI can alleviate cognitive burdens and enable scholars to focus on deeper intellectual exploration. This democratization of access could attract diverse populations previously deterred by technical or linguistic requirements, potentially creating more inclusive academic communities. GenAI also transforms research by breaking down traditional linguistic and periodization barriers, enabling global comparative studies that were previously impossible. This "AI turn" redefines knowledge creation, challenging the boundaries between disciplines and creating a new intellectual terrain where computational methods and humanistic inquiry can become partners. It allows for reimagining the boundaries between disciplines, creating a new intellectual terrain where computational methods and humanistic inquiry can become partners. By optimizing research processes and improving efficiency, AI empowers researchers to focus on solving complex problems and constructing meaningful explanations, ultimately advancing our understanding of both technological systems and human experience, and revitalizing the core essence of both fields. <br> <br>

35. ***Research on reflection in pre-training:  <br>Essential AI demonstrated that a language model's ability to reflect on and correct its own reasoning emerges significantly earlier than commonly thought, developing steadily during pre-training. Experiments involving deliberate errors showed self-correction capabilities in models like OLMo2-7B trained on 4 trillion tokens, indicating this ability starts forming before RL fine-tuning.***  <br>  <br>
    Apr 5, Essential AI published a [paper](https://arxiv.org/pdf/2504.04022) “Rethinking Reflection in Pre-Training”. A language model's ability to reflect on its own reasoning provides a key advantage for solving complex problems. While most recent research has focused on how this ability develops during reinforcement learning, the study shows that it actually begins to emerge much earlier - during the model's pre-training. To study this, the work introduces deliberate errors into chains-of-thought and test whether the model can still arrive at the correct answer by recognizing and correcting these mistakes. By tracking performance across different stages of pre-training, the study observes that this self-correcting ability appears early and improves steadily over time. For instance, an OLMo2-7B model pre-trained on 4 trillion tokens displays self-correction on the six self-reflection tasks.
  <br>  <br>  <br>


***Apr 20, 2025***

1. ***Startup Aims for Full Automation:   <br>Tamay Besiroglu's startup, Mechanize, seeks to automate all work using AI agents, causing controversy and attracting investment. The goal is full automation of all work and economy and replacing human workers in all sectors. While the AI can promote economic growth, the critics are concerned about job losses, the AI models still have limitations.***  <br>  <br>
   Apr 19, TechCrunch published an [article](https://techcrunch.com/2025/04/19/famed-ai-researcher-launches-controversial-startup-to-replace-all-human-workers-everywhere/) “Famed AI researcher launches controversial startup to replace all human workers everywhere”. Tamay Besiroglu, a prominent AI researcher, has launched Mechanize, a startup with the ambitious goal of fully automating all work and the economy. This announcement has stirred controversy, with critics arguing it damages the reputation of his respected research institute, Epoch. Mechanize aims to replace human workers with AI agents, initially targeting white-collar jobs. Besiroglu argues that this could lead to explosive economic growth and higher living standards, though critics are concerned about job loss and income inequality. He calculates the market potential to be enormous, with global wages totaling around $60 trillion annually. Despite the backlash, Mechanize has attracted significant investment from notable figures in the tech industry. Besiroglu acknowledges the current limitations of AI agents, such as reliability and task execution, but believes that overcoming these challenges will lead to economic abundance. He also suggests that even in an AI-dominated economy, human wages could increase due to complementary roles that AI cannot perform. Mechanize is actively hiring, indicating its commitment to advancing this vision despite the controversy.  <br>  <br>

3. ***New Dataset and Multi-Agent System for RAG:   <br>A new paper introduces RAMDocs, a dataset for retrieval-augmented generation (RAG) with conflicting evidence, and MADAM-RAG, a multi-agent system that uses LLMs to debate answers and discard misinformation. The existing RAG models performed poorly in RAMDocs and there still exist significant gaps for the current models to solve.***  <br>  <br>
   Apr 17, Uni of North Carolina at Chapel Hill published a [paper](https://arxiv.org/pdf/2504.13079) “Retrieval-Augmented Generation with Conflicting Evidence”. Large language model (LLM) agents are increasingly employing retrieval-augmented generation (RAG) to improve the factuality of their responses. However, in practice, these systems often need to handle ambiguous user queries and potentially conflicting information from multiple sources while also suppressing inaccurate information from noisy or irrelevant documents. Prior work has generally studied and addressed these challenges in isolation, considering only one aspect at a time, such as handling ambiguity or robustness to noise and misinformation. The authors instead consider multiple factors simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and Misinformation in Documents), a new dataset that simulates complex and realistic scenarios for conflicting evidence for a user query, including ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent approach in which LLM agents debate over the merits of an answer over multiple rounds, allowing an aggregator to collate responses corresponding to disambiguated entities while discarding misinformation and noise, thereby handling diverse sources of conflict jointly. The study demonstrates the effectiveness of MADAM-RAG using both closed and open-source models on AmbigDocs -- which requires presenting all valid answers for ambiguous queries -- improving over strong RAG baselines by up to 11.40% and on FaithEval -- which requires suppressing misinformation -- where to improve by up to 15.80% (absolute) with Llama3.3-70B-Instruct. Furthermore, the study finds that RAMDocs poses a challenge for existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match score). While MADAM-RAG begins to address these conflicting factors, the analysis indicates that a substantial gap remains especially when increasing the level of imbalance in supporting evidence and misinformation.  <br>  <br>

5. ***Attentional Bias as Foundation for Neural Architectures:   <br>Google's paper reinterprets neural architectures as associative memory modules that use attentional bias, exploring alternative attentional bias configurations and retention regularization techniques. It then presents Miras, a general framework to design deep learning architectures based on four choices. Experiments show different design choices in Miras yield models with varying strengths for language modeling, commonsense reasoning, and recall intensive tasks***  <br>  <br>
   Apr 17, Google published a [paper](https://arxiv.org/pdf/2504.13173) “It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization”. Designing efficient and effective architectural backbones has been in the core of research efforts to enhance the capability of foundation models. Inspired by the human cognitive phenomenon of attentional bias-the natural tendency to prioritize certain events or stimuli-we reconceptualize neural architectures, including Transformers, Titans, and modern linear recurrent neural networks as associative memory modules that learn a mapping of keys and values using an internal objective, referred to as attentional bias. Surprisingly, the study observed that most existing sequence models leverage either (1) dot-product similarity, or (2) L2 regression objectives as their attentional bias. Going beyond these objectives, the study presents a set of alternative attentional bias configurations along with their effective approximations to stabilize their training procedure. The authers then reinterpret forgetting mechanisms in modern deep learning architectures as a form of retention regularization, providing a novel set of forget gates for sequence models. Building upon these insights, the study present Miras, a general framework to design deep learning architectures based on four choices of: (i) associative memory architecture, (ii) attentional bias objective, (iii) retention gate, and (iv) memory learning algorithm. The work presents three novel sequence models-Moneta, Yaad, and Memora-that go beyond the power of existing linear RNNs while maintaining a fast parallelizable training process. Experiments show different design choices in Miras yield models with varying strengths. For example, certain instances of Miras achieve exceptional performance in special tasks such as language modeling, commonsense reasoning, and recall intensive tasks, even outperforming Transformers and other modern linear recurrent models.  <br>  <br>

7. ***Offline "Thinking" Reduces Test-Time Compute:   <br>A recent study introduces "sleep-time compute," which allows models to pre-compute useful quantities offline to reduce computation requirements at test-time. It can reduce the amount of test-time compute needed to achieve the same accuracy by ~ 5x and that by scaling sleep-time compute the study can further increase accuracy by up to 13% on Stateful GSM-Symbolic and 18% on Stateful AIME. It also conducts additional analysis to understand when sleep-time compute is most effective***  <br>  <br>
   Apr 17, Letta and UC Berkeley published a [paper](https://arxiv.org/pdf/2504.13171) “Sleep-time Compute: Beyond Inference Scaling at Test-time”. Scaling test-time compute has emerged as a key ingredient for enabling large language models (LLMs) to solve difficult problems, but comes with high latency and inference cost. The study introduces sleep-time compute, which allows models to "think" offline about contexts before queries are presented: by anticipating what queries users might ask and pre-computing useful quantities, the study can significantly reduce the compute requirements at test-time. To demonstrate the efficacy of the method, the work creates modified versions of two reasoning tasks - Stateful GSM-Symbolic and Stateful AIME. The study finds that sleep-time compute can reduce the amount of test-time compute needed to achieve the same accuracy by ~ 5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time compute the study can further increase accuracy by up to 13% on Stateful GSM-Symbolic and 18% on Stateful AIME. Furthermore, the work introduces Multi-Query GSM-Symbolic, which extends GSM-Symbolic by including multiple related queries per context. By amortizing sleep-time compute across related queries about the same context using Multi-Query GSM-Symbolic, the study can decrease the average cost per query by 2.5x. The work then conducts additional analysis to understand when sleep-time compute is most effective, finding the predictability of the user query to be well correlated with the efficacy of sleep-time compute. Finally, the study conducts a case-study of applying sleep-time compute to a realistic agentic SWE task. https://github.com/letta-ai/sleep-time-compute
  <br>  <br>
9. ***Models for Detecting AI-Generated Content: Traversaal.ai et al. published a paper that introduces a new set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. The work also introduces a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages***
    Apr 16, Traversaal.ai, Vantager, Cohere, et al published a [paper](https://arxiv.org/pdf/2504.11952) “Robust and Fine-Grained Detection of AI Generated Texts”. An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence the study focused more over partial cases i.e human-LLM co-authored texts. The paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. The work also introduces a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. The study also presents findings of the models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.

11. ***Reinforcement Learning for Diffusion LLM Reasoning:   <br>UCLA and Meta's paper proposes d1, a framework that adapts pre-trained masked diffusion large language models (dLLMs) into reasoning models using supervised finetuning and a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO. The work finds that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM.***  <br>  <br>
    Apr 16, UCLA and Meta published a [paper](https://arxiv.org/pdf/2504.12216) “d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning”. Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, the study proposes d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, the work develops and extends techniques to improve reasoning in pretrained dLLMs: (a) utilizing a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) introducing a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO. Through empirical studies, the study investigates the performance of different post-training recipes on multiple mathematical and logical reasoning benchmarks. The study finds that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM. https://dllm-reasoning.github.io/  <br>  <br>

13. ***Analysis of LLM Reasoning Capabilities After SFT:   <br>UC Berkeley and Allen Inst for AI released a paper which analyses model performance on the AIME24 dataset to understand how reasoning capabilities evolve. It discovers a ladder-like structure in problem difficulty, and progresses from Easy to Medium tier requires adopting an R1 reasoning style with minimal SFT, while Hard-level questions suffer from frequent model's errors at each step of the reasoning chain.***  <br>  <br>
    Apr 16, UC Berkeley and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2504.11741) “Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?”. Recent supervised fine-tuning (SFT) approaches have significantly improved language models' performance on mathematical reasoning tasks, even when models are trained at a small scale. However, the specific capabilities enhanced through such fine-tuning remain poorly understood. This study conducts a detailed analysis of model performance on the AIME24 dataset to understand how reasoning capabilities evolve. The study discovers a ladder-like structure in problem difficulty, categorize questions into four tiers (Easy, Medium, Hard, and Extremely Hard (Exh)), and identify the specific requirements for advancing between tiers. The work finds that progression from Easy to Medium tier requires adopting an R1 reasoning style with minimal SFT (500-1K instances), while Hard-level questions suffer from frequent model's errors at each step of the reasoning chain, with accuracy plateauing at around 65% despite logarithmic scaling. Exh-level questions present a fundamentally different challenge; they require unconventional problem-solving skills that current models uniformly struggle with. Additional findings reveal that carefully curated small-scale datasets offer limited advantage-scaling dataset size proves far more effective. The analysis provides a clearer roadmap for advancing language model capabilities in mathematical reasoning. https://github.com/sunblaze-ucb/reasoning_ladder.git  <br>  <br>

15. ***OpenAI Releases Models with Reasoning and Tool Capabilities:   <br>OpenAI has released o3 and o4-mini, combining reasoning with tools like web browsing and Python, excelling in complex tasks. The o-series models are trained with large-scale reinforcement learning on chains of thought and can reason about the safety policies in context when responding to potentially unsafe prompts***  <br>  <br>
    Apr 16, OpenAI released o3 and o4-mini and published a [report](https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf) “OpenAI o3 and o4-mini System Card”. OpenAI o3 and OpenAI o4-mini combine state-of-the-art reasoning with full tool capabilities—web browsing, Python, image and file analysis, image generation, canvas, automations, file search, and memory. These models excel at solving complex math, coding, and scientific challenges while demonstrating strong visual perception and analysis. The models use tools in their chains of thought to augment their capabilities; for example, cropping or transforming images, searching the web, or using Python to analyze data during their thought process. The OpenAI o-series models are trained with large-scale reinforcement learning on chains of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of the models. In particular, the models can reason about the safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This is the first launch and system card to be released under Version 2 of the Preparedness Framework⁠. OpenAI’s Safety Advisory Group (SAG) reviewed the results of the Preparedness evaluations and determined that OpenAI o3 and o4-mini do not reach the High threshold in any of the three Tracked Categories: Biological and Chemical Capability, Cybersecurity, and AI Self-improvement. The report describes these evaluations, and provide an update on the work to mitigate risks in these areas.  <br>  <br>

17. ***Most-Cited Papers Focus on Research Tools and Methods:   <br>A Nature analysis reveals that the most-cited papers of the 21st century describe fundamental methods and tools, rather than major scientific breakthroughs. Papers providing widely applicable tools and frameworks for conducting research often accrue more citations than those presenting singular discoveries***  <br>  <br>
    Apr 15, Nature published an [article](https://www.nature.com/articles/d41586-025-01125-9) “the most-cited papers of the twenty-first century”. Based on Nature's analysis across five databases, the most-cited scientific papers published since the year 2000 are typically not those detailing major scientific breakthroughs like the Higgs boson or CRISPR. Instead, the list is dominated by papers describing fundamental methods, research software, statistical techniques, databases, and standards for improving research quality. These works act as essential "workhorses" that underpin studies across numerous disciplines. The top paper is a 2016 report on Microsoft's deep residual learning networks (ResNets), crucial for modern AI advancements. Other highly cited examples include foundational AI techniques, software for data analysis, global cancer statistics reports, diagnostic manuals like the DSM-5, and guidelines for conducting systematic reviews (PRISMA). This trend highlights that papers providing widely applicable tools and frameworks for conducting research often accrue more citations than those presenting singular discoveries, reflecting their pervasive influence on the scientific process itself. Citation counts can vary between databases and are influenced by factors like the age of the paper and field size.  <br>  <br>

19. ***Rethinking Language Model Training with Future Goals:   <br>CMU's paper argues that rearranging and processing the training data sequences can allow models to more accurately imitate the true data-generating process, and does not require any other changes to the architecture or training infrastructure. It demonstrates that this technique, Trelawney, and the inference algorithms derived from it allow to improve performance on several key benchmarks***  <br>  <br>
    Apr 15, CMU published a [paper](https://arxiv.org/pdf/2504.11336) “Looking beyond the next token”. The structure of causal language model training assumes that each token can be accurately predicted from the previous context. This contrasts with humans' natural writing and reasoning process, where goals are typically known before the exact argument or phrasings. While this mismatch has been well studied in the literature, the working assumption has been that architectural changes are needed to address this mismatch. The study argues that rearranging and processing the training data sequences can allow models to more accurately imitate the true data-generating process, and does not require any other changes to the architecture or training infrastructure. The work demonstrates that this technique, Trelawney, and the inference algorithms derived from it allow to improve performance on several key benchmarks that span planning, algorithmic reasoning, and story generation tasks. Finally, the method naturally enables the generation of long-term goals at no additional cost. The study investigates how using the model's goal-generation capability can further improve planning and reasoning. Additionally, the authors believe Trelawney could potentially open doors to new capabilities beyond the current language modeling paradigm.  <br>  <br>

21. ***Predicting Best Pretraining Data with Small-Scale Experiments:   <br>Allen Inst for AI et al. introduce DataDecide, a suite of models and data for predicting the best pretraining data using smaller experiments. The work finds that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at larger target scale (1B) and that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute.***  <br>  <br>
    Apr 15, Allen Inst for AI, Uni of Washington and Uni of Penn published a [paper](https://arxiv.org/pdf/2504.11393) “DataDecide: How to Predict Best Pretraining Data with Small Experiments”. Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, the study releases models, data, and evaluations in DataDecide -- the most extensive open suite of models over differences in data and scale. The study conducts controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. The work finds that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at larger target scale (1B) (~80% of com parisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DataDecide can measure improvement in future scaling laws. The study also identifies that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute.  <br>  <br>

23. ***VisualPuzzles for Evaluating Multimodal Reasoning:   <br>CMU introduces VisualPuzzles, a benchmark minimizing domain knowledge to evaluate visual reasoning, showing current models lag human performance. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks***  <br>  <br>
    Apr 15, CMU published a [paper](https://arxiv.org/pdf/2504.10342) “VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge”. Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, the study introduces VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of the questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with "thinking" modes) yield inconsistent gains across models and task types, and the work observes no clear correlation between model size and performance. The study also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge.  <br>  <br>

25. ***LLMs for Classifying Legal Interpretations:   <br>Uni of Zurich studies the identifiability of legal interpretations employed by the European Court of Human Rights using LLMs, showing that LLMs can classify legal interpretations and extract complex legal features efficiently. The results imply that feature-extraction using LLMs leads to robust outcomes while allowing for greater resource- and time efficiency compared to human annotation.***  <br>  <br>
    Apr 15, Uni of Zurich published a [paper](https://link.springer.com/article/10.1007/s10506-025-09447-9) “Classifying legal interpretations using large language models”. In the civil law tradition, legal arguments are used to justify the outcomes of judicial decision-making. These arguments are formed relying on a canon of interpretation techniques (e.g. textual or teleological interpretation). The work studies the identifiability of interpretation techniques as they are employed by the European Court of Human Rights (ECtHR) from a computational law perspective using a unique dataset. The study shows how Large Language Models (LLMs) can be utilized to classify legal interpretations, and compares their performance. The work evaluates proprietary and opensource models using methods such as few-shot and zero-shot chain-of-thought prompting combined with self-consistency. The results imply that feature-extraction using LLMs leads to robust outcomes while allowing for greater resource- and time efficiency compared to human annotation. Furthermore, The results imply that LLMs can play a larger role in the extraction of more complex features that are of particular relevance from a legal perspective.  <br>  <br>

27. ***Reasoning Without Explicit "Thinking" Can Be Effective:   <br>UC Berkeley and Allen Inst for AI find that bypassing the explicit thinking process in LLMs can be surprisingly effective for reasoning tasks, especially in low-budget settings. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets***  <br>  <br>
    Apr 14, UC Berkeley and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2504.09858?) “Reasoning Models Can Be Effective Without Thinking”. Recent LLMs have significantly improved reasoning capabilities, primarily by including an explicit, lengthy Thinking process as part of generation. The study questions whether this explicit thinking is necessary. Using the state-of-the-art DeepSeek-R1-Distill-Qwen, the work finds that bypassing the thinking process via simple prompting, denoted as NoThinking, can be surprisingly effective. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets--including mathematical problem solving, formal theorem proving, and coding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with 700 tokens. Notably, the performance of NoThinking becomes more competitive with pass@k as k increases. Building on this observation, the study demonstrates that a parallel scaling approach that uses NoThinking to generate N outputs independently and aggregates them is highly effective. For aggregation, the study uses task-specific verifiers when available, or apply simple best-of-N strategies such as confidence-based selection. The method outperforms a range of baselines with similar latency using Thinking, and is comparable to Thinking with significantly longer latency (up to 9x). Together, the research encourages a reconsideration of the necessity of lengthy thinking processes, while also establishing a competitive reference for achieving strong reasoning performance in low-budget settings or at low latency using parallel scaling.  <br>  <br>

29. ***Analyzing Post-Training Data Quality Through Layer-wise Gradients:   <br>Uni of Maryland and Uni of Chicago analyze layer-wise gradients to understand how instruction and reasoning data affect LLM post-training, revealing that higher-quality data is associated with lower nuclear norms and higher effective ranks. The analysis reveals that widely-studied metrics for data evaluation can be explained and unified by spectral properties computed from gradients' singular value decomposition (SVD)***  <br>  <br>
    Apr 14, Uni of Maryland and Uni of Chicago published a [paper](https://arxiv.org/pdf/2504.10766) “How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients”. As the post-training of large language models (LLMs) advances from instruction-following to complex reasoning tasks, understanding how different data affect finetuning dynamics remains largely unexplored. This study presents a spectral analysis of layer-wise gradients induced by low/high-quality instruction and reasoning data for LLM post-training. The analysis reveals that widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and Reward, can be explained and unified by spectral properties computed from gradients' singular value decomposition (SVD). Specifically, higher-quality data are usually associated with lower nuclear norms and higher effective ranks. Notably, effective rank exhibits better robustness and resolution than nuclear norm in capturing subtle quality differences. For example, reasoning data achieves substantially higher effective ranks than instruction data, implying richer gradient structures on more complex tasks. Experiments also highlight that models within the same family share similar gradient patterns regardless of their sizes, whereas different model families diverge significantly. Providing a unified view on the effects of data quality across instruction and reasoning data, this work illuminates the interplay between data quality and training stability, shedding novel insights into developing better data exploration strategies for post-training.  <br>  <br>

31. ***End-to-End Training for Latent Diffusion Transformers:   <br>ANU, CSRIO, and NYU introduce REPA-E, a training recipe that unlocks end-to-end training of latent diffusion models with VAE tokenizers, significantly speeding up training and improving performance.***  <br>  <br>
    Apr 14, ANU, CSRIO, and NYU published a [paper](https://arxiv.org/pdf/2504.10483) “REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers”. The study tackles a fundamental question: "Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner?" Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible. However, for latent diffusion transformers, it is observed that end-to-end training both VAE and diffusion-model using standard diffusion-loss is ineffective, even causing a degradation in final performance. The study shows that while diffusion loss is ineffective, end-to-end training can be unlocked through the representation-alignment (REPA) loss -- allowing both VAE and diffusion model to be jointly tuned during the training process. Despite its simplicity, the proposed training recipe (REPA-E) shows remarkable performance; speeding up diffusion model training by over 17x and 45x over REPA and vanilla training recipes, respectively. Interestingly, the study observes that end-to-end tuning with REPA-E also improves the VAE itself; leading to improved latent space structure and downstream generation performance. In terms of final performance, the approach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and without classifier-free guidance on ImageNet 256 x 256. Code is available at https://end2end-diffusion.github.io.  <br>  <br>

33. ***LLMs Exhibit "Priming" Effect When Learning New Data:   <br>Google demonstrates that LLMs exhibit a "priming" effect, inappropriately applying new knowledge in unrelated contexts, and introduces techniques to mitigate this while preserving learning ability. The study develops two novel techniques to modulate how new knowledge affects existing model behavior: (1) a stepping-stone'' text augmentation strategy and (2) an ignore-k'' update pruning method.***  <br>  <br>
    Apr 13, Google published a [paper](https://arxiv.org/pdf/2504.09522) “How new data permeates LLM knowledge and how to dilute it”. Large language models continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. The study demonstrates that when learning new information, LLMs exhibit a "priming" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts. To systematically study this phenomenon, the study introduces "Outlandish," a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM's existing knowledge base. Using this dataset, the study shows that the degree of priming after learning new information can be predicted by measuring the token probability of key words before training. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages. Finally, the study develops two novel techniques to modulate how new knowledge affects existing model behavior: (1) a stepping-stone'' text augmentation strategy and (2) an ignore-k'' update pruning method. These approaches reduce undesirable priming effects by 50-95% while preserving the model's ability to learn new information. The findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/  <br>  <br>

35. ***Speculative Thinking Enhances Small-Model Reasoning:   <br>Case Western Reserve Uni and CMU introduce Speculative Thinking, a training-free framework that enhances small-model reasoning by using large model guidance at inference time, delegating reflective steps to a more capable model to boost accuracy and shorten output.***  <br>  <br>
    Apr 12, Case Western Reserve Uni and CMU published a [paper](https://arxiv.org/pdf/2504.12329) “Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time”. Recent advances leverage post-training to enhance model reasoning performance, which typically requires costly training pipelines and still suffers from inefficient, overly lengthy outputs. The study introduces Speculative Thinking, a training-free framework that enables large reasoning models to guide smaller ones during inference at the reasoning level, distinct from speculative decoding, which operates at the token level. The approach is based on two observations: (1) reasoning-supportive tokens such as "wait" frequently appear after structural delimiters like "\n\n", serving as signals for reflection or continuation; and (2) larger models exhibit stronger control over reflective behavior, reducing unnecessary backtracking while improving reasoning quality. By strategically delegating reflective steps to a more capable model, the method significantly boosts the reasoning accuracy of reasoning models while shortening their output. With the assistance of the 32B reasoning model, the 1.5B model's accuracy on MATH500 increases from 83.2% to 89.4%, marking a substantial improvement of 6.2%. Simultaneously, the average output length is reduced from 5439 tokens to 4583 tokens, representing a 15.7% decrease. Moreover, when applied to a non-reasoning model (Qwen-2.5-7B-Instruct), the framework boosts its accuracy from 74.0% to 81.8% on the same benchmark, achieving a relative improvement of 7.8%. https://github.com/uservan/speculative_thinking  <br>  <br>

37. ***Improving Long Context In-Context Compression:   <br>Google and Uni of Oxford propose GistPool, a new in-context compression method that preserves the simplicity of gisting while significantly boosting its performance on long context compression tasks.***  <br>  <br>
    Apr 11, Google and Uni of Oxford published a [paper](https://arxiv.org/pdf/2504.08934) “Long Context In-Context Compression by Getting to the Gist of Gisting”. Long context processing is critical for the adoption of LLMs, but existing methods often introduce architectural complexity that hinders their practical adoption. Gisting, an in-context compression method with no architectural modification to the decoder transformer, is a promising approach due to its simplicity and compatibility with existing frameworks. While effective for short instructions, the work demonstrates that gisting struggles with longer contexts, with significant performance drops even at minimal compression rates. Surprisingly, a simple average pooling baseline consistently outperforms gisting. The study analyzes the limitations of gisting, including information flow interruptions, capacity limitations and the inability to restrict its attention to subsets of the context. Motivated by theoretical insights into the performance gap between gisting and average pooling, and supported by extensive experimentation, the work proposes GistPool, a new in-context compression method. GistPool preserves the simplicity of gisting, while significantly boosting its performance on long context compression tasks.  <br>  <br>

39. ***Impact of Web Crawling Opt-Outs on LLM Performance:   <br>EPFL and ETH Switzerland's study quantifies the "data compliance gap," finding that compliance with web data opt-outs does not degrade general knowledge acquisition but can impact performance in specialized domains.***  <br>  <br>
    Apr 8, EPFL and ETH Switzerland published a [paper](https://arxiv.org/pdf/2504.06219) “Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs”. The increasing adoption of web crawling opt-outs by copyright holders of online content raises critical questions about the impact of data compliance on large language model (LLM) performance. However, little is known about how these restrictions (and the resultant filtering of pretraining datasets) affect the capabilities of models trained using these corpora. This study conceptualizes this effect as the data compliance gap (DCG), which quantifies the performance difference between models trained on datasets that comply with web crawling opt-outs, and those that do not. The study measures the data compliance gap in two settings: pretraining models from scratch and continual pretraining from existing compliant models (simulating a setting where copyrighted data could be integrated later in pretraining). Experiments with 1.5B models show that, as of January 2025, compliance with web data opt-outs does not degrade general knowledge acquisition (close to 0\% DCG). However, in specialized domains such as biomedical research, excluding major publishers leads to performance declines. These findings suggest that while general-purpose LLMs can be trained to perform equally well using fully open data, performance in specialized domains may benefit from access to high-quality copyrighted sources later in training. The study provides empirical insights into the long-debated trade-off between data compliance and downstream model performance, informing future discussions on AI training practices and policy decisions.  <br>  <br>

41. ***Low-Rank Thinning for Data Summarization:   <br>Uni of Cambridge, Cornell Tech, MIT and Microsoft revise the paper and introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank.***  <br>  <br>
    Apr 8, Uni of Cambridge, Cornell Tech, MIT and Microsoft revised the [paper](https://arxiv.org/pdf/2502.12063) “Low-Rank Thinning”. The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, the study introduces a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, the study designs practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.  <br>  <br>

43. ***Overtrained Language Models Are Harder to Fine-Tune:   <br>CMU, Stanford Uni, Harvard Uni, Princeton Uni released a paper which shows that extended pre-training can make models harder to fine-tune, leading to degraded final performance, termed "catastrophic overtraining."***  <br>  <br>
    Mar 28, CMU, Stanford Uni, Harvard Uni, Princeton Uni published a [paper](https://arxiv.org/pdf/2503.19206) “Overtrained Language Models Are Harder to Fine-Tune”. Large language models are pre-trained on ever-growing token budgets under the assumption that better pre-training performance translates to improved downstream models. This work challenges this assumption and show that extended pre-training can make models harder to fine-tune, leading to degraded final performance. The authors term this phenomenon catastrophic overtraining. For example, the instruction-tuned OLMo-1B model pre-trained on 3T tokens leads to over 2% worse performance on multiple standard LLM benchmarks than its 2.3T token counterpart. Through controlled experiments and theoretical analysis, the work shows that catastrophic overtraining arises from a systematic increase in the broad sensitivity of pre-trained parameters to modifications, including but not limited to fine-tuning. The findings call for a critical reassessment of pre-training design that considers the downstream adaptability of the model.

  <br>  <br>  <br>
***Apr 13, 2025***

1. ***Self-Steering LMs with DisCIPL:   <br>MIT and Yule introduce DisCIPL, a method for "self-steering" LMs using a Planner model to generate task-specific inference programs executed by Follower models, enabling recursive search and efficient reasoning, achieving performance comparable to larger models on constrained generation tasks.***  <br>  <br>
   Apr 11, MIT and Yule published an ICLR [paper](https://openreview.net/forum?id=x7E2Qt7n0V) “Self-Steering Language Models”. While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for “self-steering” LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. The approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, the work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs.  <br>  <br>

3. ***Dynamic Cheatsheet Augments LMs with Adaptive Memory:   <br>Stanford and Together AI present Dynamic Cheatsheet (DC), a framework that endows black-box LMs with a persistent, evolving memory to store and reuse accumulated strategies and insights, substantially enhancing performance across a range of tasks without explicit ground-truth labels or finetuning.***  <br>  <br>
   Apr 10, Stanford Uni and Together AI published a [paper](https://arxiv.org/pdf/2504.07952) “Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory”. Despite their impressive performance on complex tasks, current language models (LMs) typically operate in a vacuum: Each input query is processed separately, without retaining insights from previous attempts. Here, the study presents Dynamic Cheatsheet (DC), a lightweight framework that endows a black-box LM with a persistent, evolving memory. Rather than repeatedly re-discovering or re-committing the same solutions and mistakes, DC enables models to store and reuse accumulated strategies, code snippets, and general problem-solving insights at inference time. This test-time learning enhances performance substantially across a range of tasks without needing explicit ground-truth labels or human feedback. Leveraging DC, Claude 3.5 Sonnet's accuracy more than doubled on AIME math exams once it began retaining algebraic insights across questions. Similarly, GPT-4o's success rate on Game of 24 increased from 10% to 99% after the model discovered and reused a Python-based solution. In tasks prone to arithmetic mistakes, such as balancing equations, DC enabled GPT-4o and Claude to reach near-perfect accuracy by recalling previously validated code, whereas their baselines stagnated around 50%. Beyond arithmetic challenges, DC yields notable accuracy gains on knowledge-demanding tasks. Claude achieved a 9% improvement in GPQA-Diamond and an 8% boost on MMLU-Pro problems. Crucially, DC's memory is self-curated, focusing on concise, transferable snippets rather than entire transcript. Unlike finetuning or static retrieval methods, DC adapts LMs' problem-solving skills on the fly, without modifying their underlying parameters. Overall, the findings present DC as a promising approach for augmenting LMs with persistent memory, bridging the divide between isolated inference events and the cumulative, experience-driven learning characteristic of human cognition. https://github.com/suzgunmirac/dynamic-cheatsheet  <br>  <br>

5. ***Forbes Releases 2025 AI 50 List:   <br>Forbes' seventh annual AI 50 list highlights the most promising privately-held AI companies, with a focus on practical applications, major players like OpenAI and Anthropic, and newcomers like xAI, while also acknowledging legal challenges and the importance of equitable startup ecosystems.***  <br>  <br>
   Apr 10, Forbes release [AI 50](https://www.forbes.com/lists/ai50/) in 2025. Artificial intelligence remains a central focus in venture capital and the business world, with startups shifting from AI model releases to creating practical applications across various fields. Forbes' seventh annual AI 50 list highlights the most promising privately-held AI companies, including newcomers like Anysphere, Speak, and OpenEvidence. Major players like OpenAI and Anthropic dominate the list, having raised significant funds, while new competitors like Elon Musk's xAI and Mira Murati's Thinking Machine Labs emerge. Fei Fei Li's World Labs and enterprise AI company Writer also make notable appearances. AI companies rely heavily on expensive computing power, benefiting infrastructure providers like Crusoe, Lambda, and Together AI. However, startups like DeepSeek demonstrate cost-efficient training methods. The industry faces legal challenges over alleged copyright infringement, with companies like OpenAI, Anthropic, and others being sued for using copyrighted content. The future of AI hinges on court rulings regarding these issues. This year's AI 50 list was highly competitive, with 1,860 submissions judged on business promise, technical talent, and AI use, promoting a more equitable startup ecosystem.  <br>  <br>

7. ***Google Explores Intrinsic Motivation for Mutual Awareness:   <br>Google's paper explores the intrinsic motivation for mutual awareness, hypothesizing that humans possess a fundamental drive to understand and be understood, even without extrinsic rewards, and demonstrates that this drive can facilitate cooperation.***  <br>  <br>
   Apr 10, Google published a [paper](https://arxiv.org/pdf/2504.06611) “Wanting to be Understood”. This paper explores an intrinsic motivation for mutual awareness, hypothesizing that humans possess a fundamental drive to understand and to be understood even in the absence of extrinsic rewards. Through simulations of the perceptual crossing paradigm, the study explores the effect of various internal reward functions in reinforcement learning agents. The drive to understand is implemented as an active inference type artificial curiosity reward, whereas the drive to be understood is implemented through intrinsic rewards for imitation, influence/impressionability, and sub-reaction time anticipation of the other. Results indicate that while artificial curiosity alone does not lead to a preference for social interaction, rewards emphasizing reciprocal understanding successfully drive agents to prioritize interaction. The work demonstrates that this intrinsic motivation can facilitate cooperation in tasks where only one agent receives extrinsic reward for the behaviour of the other.  <br>  <br>

9. ***New Ideas in AI Stem from New Datasets:   <br>Jack Morris argues that major AI breakthroughs aren't primarily driven by novel algorithms but by the unlocking and large-scale utilization of new data sources, predicting that the next major paradigm shift will arise from harnessing vast, currently untapped data reservoirs.***  <br>  <br>
    Apr 10, Jack Morris, a PhD student from Cornell Tech Inst published an [article](https://substack.com/inbox/post/160974493) “There Are No New Ideas in AI… Only New Datasets”. While AI showcases steady advancements often attributed to ongoing research, truly transformative leaps like Deep Neural Networks, Transformers, RLHF, and Reasoning models are rarer, and recent progress appears incremental. This text argues that these major breakthroughs weren't primarily driven by fundamentally novel algorithms, as the core machine learning concepts pre-existed. Instead, their catalyst was the unlocking and large-scale utilization of new data sources: ImageNet, web text, human preferences, and verifiable outputs, respectively. This perspective emphasizes data availability and scale as potentially more crucial for significant progress than specific algorithmic innovations, aligning with the "Bitter Lesson." Consequently, the next major AI paradigm shift is predicted to arise not just from new methods, but from harnessing vast, currently untapped data reservoirs, with video platforms like YouTube and data from embodied systems (robots) being prominent examples. The search for future breakthroughs might prioritize accessing new data over inventing new techniques. A [2023 blog](https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/) also stated that “Then, when you refer to “Lambda”, “ChatGPT”, “Bard”, or “Claude” then, it’s not the model weights that you are referring to. It’s the dataset.”  <br>  <br>

11. ***Hogwild! Inference Enables Parallel LLM Generation:   <br>Yandex and IST Austria propose Hogwild! Inference, a parallel LLM inference engine that allows multiple instances of the same LLM to run in parallel with a shared attention cache, enabling them to synchronize and devise their own collaboration strategies, improving hardware utilization.***  <br>  <br>
    Apr 9, Yandex and IST Austria published a [paper](https://arxiv.org/pdf/2504.06261) “Hogwild! Inference: Parallel LLM Generation via Concurrent Attention”. Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. This work proposes a different design approach: running LLM "workers" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. The approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while "seeing" each other's partial progress in the concurrent cache. The study implements this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with "instant" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. The study finds that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning. https://github.com/eqimp/hogwild_llm  <br>  <br>

13. ***SkillWeaver Enables Web Agents to Self-Improve:   <br>Ohio State University, University of Virginia, Purdue University, CMU, and Cisco introduce SkillWeaver, a framework enabling web agents to self-improve by autonomously synthesizing reusable skills as APIs, expanding their capabilities through iterative exploration and skill composition.***  <br>  <br>
    Apr 9, Ohio State Uni, Uni of Virginia, Prude Uni, CMU and Cisco published a [paper](https://arxiv.org/pdf/2504.07079) “SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills”. To survive and thrive in complex environments, humans have evolved sophisticated self-improvement mechanisms through environment exploration, hierarchical abstraction of experiences into reuseable skills, and collaborative construction of an ever-growing skill repertoire. Despite recent advancements, autonomous web agents still lack crucial self-improvement capabilities, struggling with procedural knowledge abstraction, refining skills, and skill composition. This study introduces SkillWeaver, a skill-centric framework enabling agents to self-improve by autonomously synthesizing reusable skills as APIs. Given a new website, the agent autonomously discovers skills, executes them for practice, and distills practice experiences into robust APIs. Iterative exploration continually expands a library of lightweight, plug-and-play APIs, significantly enhancing the agent's capabilities. Experiments on WebArena and real-world websites demonstrate the efficacy of SkillWeaver, achieving relative success rate improvements of 31.8% and 39.8%, respectively. Additionally, APIs synthesized by strong agents substantially enhance weaker agents through transferable skills, yielding improvements of up to 54.3% on WebArena. These results demonstrate the effectiveness of honing diverse website interactions into APIs, which can be seamlessly shared among various web agents.  <br>  <br>

15. ***OLMoTrace Traces LM Outputs Back to Training Data:   <br>Allen Institute for AI, University of Washington, UC Berkeley and Stanford University present OLMoTrace, a system that traces the outputs of language models back to their full, multi-trillion-token training data in real time, helping users understand model behavior through the lens of their training data.***  <br>  <br>
    Apr 9, Allen Inst for AI, Uni of Washington, UC Berkeley and Stanford Uni published a [paper](https://arxiv.org/pdf/2504.07096) “OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens”. The study presents OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), the system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. The study showcases how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.  <br>  <br>

17. ***AI Scientist-v2 Automates Scientific Discovery:   <br>Sakana AI introduces The AI Scientist-v2, an end-to-end agentic system capable of producing an entirely AI-generated peer-review-accepted workshop paper, highlighting the growing capability of AI in conducting all aspects of scientific research.***  <br>  <br>
    Apr 8, Sakana.AI published a [paper](https://pub.sakana.ai/ai-scientist-v2/paper/paper.pdf) “The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search”. AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. The work introduces The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AIgenerated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, the work enhances the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. The study evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. The authors anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. Code is at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. The work also discusses the role of AI in science, including AI safety.  <br>  <br>

19. ***Reproducibility in LM Reasoning Critically Assessed:   <br>The University of Tubingen and the University of Cambridge conduct a comprehensive study revealing that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices, calling for standardized evaluation frameworks and finding that SFT methods show consistently stronger generalization.***  <br>  <br>
    Apr 9, Uni of Tubingen and Uni of Cambridge published a [paper](https://arxiv.org/pdf/2504.07086) “A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility”. Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work  <br>  <br>

21. ***Lattice Compresses Memory for Efficient Attention:   <br>Google introduces Lattice, a novel recurrent neural network (RNN) mechanism that leverages the inherent low-rank structure of K-V matrices to efficiently compress the cache into a fixed number of memory slots, achieving sub-quadratic complexity in the attention mechanism.***  <br>  <br>
    Apr 8, Google published a [paper](https://arxiv.org/abs/2504.05646) “Lattice: Learning to Efficiently Compress the Memory”. Attention mechanisms have revolutionized sequence learning but suffer from quadratic computational complexity. This paper introduces Lattice, a novel recurrent neural network (RNN) mechanism that leverages the inherent low-rank structure of K-V matrices to efficiently compress the cache into a fixed number of memory slots, achieving sub-quadratic complexity. The study formulates this compression as an online optimization problem and derive a dynamic memory update rule based on a single gradient descent step. The resulting recurrence features a state- and input-dependent gating mechanism, offering an interpretable memory update process. The core innovation is the orthogonal update: each memory slot is updated exclusively with information orthogonal to its current state hence incorporation of only novel, non-redundant data, which minimizes the interference with previously stored information. The experimental results show that Lattice achieves the best perplexity compared to all baselines across diverse context lengths, with performance improvement becoming more pronounced as the context length increases.  <br>  <br>

23. ***Knowledge-Instruct Enables Effective Continual Pre-training:   <br>Microsoft introduces Knowledge-Instruct, a novel approach to efficiently inject knowledge from limited corpora into LLMs through pure instruction-tuning, effectively integrating new knowledge while preserving general reasoning abilities.***  <br>  <br>
    Apr 8, Microsoft published a [paper](https://arxiv.org/pdf/2504.05571) “Knowledge-Instruct: Effective Continual Pre-training from Limited Data using Instructions”. While Large Language Models (LLMs) acquire vast knowledge during pre-training, they often lack domain-specific, new, or niche information. Continual pre-training (CPT) attempts to address this gap but suffers from catastrophic forgetting and inefficiencies in low-data regimes. The study introduces Knowledge-Instruct, a novel approach to efficiently inject knowledge from limited corpora through pure instruction-tuning. By generating information-dense synthetic instruction data, it effectively integrates new knowledge while preserving general reasoning and instruction-following abilities. Knowledge-Instruct demonstrates superior factual memorization, minimizes catastrophic forgetting, and remains scalable by leveraging synthetic data from relatively small language models. Additionally, it enhances contextual understanding, including complex multi-hop reasoning, facilitating integration with retrieval systems. The work validates its effectiveness across diverse benchmarks, including Companies, a new dataset that is released to measure knowledge injection capabilities.  <br>  <br>

25. ***APIGen-MT Generates Multi-Turn Agent Data:   <br>Salesforce introduces APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data through simulated human-agent interplay, training models that outperform frontier models on multi-turn tasks while maintaining superior consistency.***  <br>  <br>
    Apr 8, Salesforce published a [paper](https://arxiv.org/pdf/2504.03601) “APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay”. Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. The work introduces APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, the agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. The study trains a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. The models outperform frontier models such as GPT-4o and Claude 3.5 on tau-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that the verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io  <br>  <br>

27. ***The 2025 AI Index Report Highlights Key Trends:   <br>Stanford University's 2025 AI Index Report summarizes 12 key takeaways, including improving AI performance, increasing AI integration in daily life and business, US leadership in AI model production (but China closing the gap), and continued challenges in complex reasoning.***  <br>  <br>
    Apr 7, Stanford Uni published a [report](https://hai.stanford.edu/ai-index/2025-ai-index-report) “The 2025 AI Index Report”. The report summarized 12 key takeaways: 1) AI performance on demanding benchmarks continues to improve. 2) AI is increasingly embedded in everyday life. 3) Business is all in on AI, fueling record investment and usage, as research continues to show strong productivity impacts. 4) The U.S. still leads in producing top AI models—but China is closing the performance gap. 5) The responsible AI ecosystem evolves—unevenly. 6) Global AI optimism is rising—but deep regional divides remain. 7) AI becomes more efficient, affordable, and accessible. 8) Governments are stepping up on AI—with regulation and investment. 9) AI and computer science education is expanding—but gaps in access and readiness persist. 10) Industry is racing ahead in AI—but the frontier is tightening. 11) AI earns top honors for its impact on science. 12) Complex reasoning remains a challenge.  <br>  <br>

29. ***Adaptive Weighted Rejection Sampling Improves LM Generation:   <br>MIT, ETH, et al. introduce a new algorithm for controlled generation from language models with constraints, using adaptive rejection sampling to avoid evaluating constraints on the full vocabulary at each step and correcting for myopic behavior through importance weighting, improving both runtime and performance.***  <br>  <br>
    Apr 7, MIT, ETH, et al published a [paper](https://arxiv.org/pdf/2504.05410) “Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling”. The dominant approach to generating from language models subject to some constraint is locally constrained decoding (LCD), incrementally sampling tokens at each time step such that the constraint is never violated. Typically, this is achieved through token masking: looping over the vocabulary and excluding non-conforming tokens. There are two important problems with this approach. (i) Evaluating the constraint on every token can be prohibitively expensive -- LM vocabularies often exceed 100,000 tokens. (ii) LCD can distort the global distribution over strings, sampling tokens based only on local information, even if they lead down dead-end paths. This work introduces a new algorithm that addresses both these problems. First, to avoid evaluating a constraint on the full vocabulary at each step of generation, the study proposes an adaptive rejection sampling algorithm that typically requires orders of magnitude fewer constraint evaluations. Second, the work shows how this algorithm can be extended to produce low-variance, unbiased estimates of importance weights at a very small additional cost -- estimates that can be soundly used within previously proposed sequential Monte Carlo algorithms to correct for the myopic behavior of local constraint enforcement. Through extensive empirical evaluation in text-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON domains, the study shows that the approach is superior to state-of-the-art baselines, supporting a broader class of constraints and improving both runtime and performance. Additional theoretical and empirical analyses show that the method's runtime efficiency is driven by its dynamic use of computation, scaling with the divergence between the unconstrained and constrained LM, and as a consequence, runtime improvements are greater for better models.  <br>  <br>

31. ***Test-Time Training Enables One-Minute Video Generation:   <br>Nvidia, Stanford University, UCSD, UC Berkeley, and UT Austin experiment with Test-Time Training (TTT) layers in pre-trained Transformers to generate one-minute videos from text storyboards, generating more coherent videos compared to baselines.***  <br>  <br>
    Apr 7, Nvidia, Stanford Uni, UCSD, UC Berkeley and UT Austin published a [paper](https://arxiv.org/pdf/2504.05298) on CPPR2025 “One-Minute Video Generation with Test-Time Training”. Transformers today still struggle to generate one-minute videos because self-attention layers are inefficient for long context. Alternatives such as Mamba layers struggle with complex multi-scene stories because their hidden states are less expressive. The study experiments with Test-Time Training (TTT) layers, whose hidden states themselves can be neural networks, therefore more expressive. Adding TTT layers into a pre-trained Transformer enables it to generate one-minute videos from text storyboards. For proof of concept, the work curates a dataset based on Tom and Jerry cartoons. Compared to baselines such as Mamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers generate much more coherent videos that tell complex stories, leading by 34 Elo points in a human evaluation of 100 videos per method. Although promising, results still contain artifacts, likely due to the limited capability of the pre-trained 5B model. The efficiency of the implementation can also be improved. The authors have only experimented with one-minute videos due to resource constraints, but the approach can be extended to longer videos and more complex stories. Sample videos, code and annotations are available at: https://test-time-training.github.io/video-dit  <br>  <br>

33. ***SWiRL Improves Reasoning and Tool Use with Multi-Step RL:   <br>Stanford University and Google propose Step-Wise Reinforcement Learning (SWiRL), a synthetic data generation and RL methodology targeting multi-step optimization scenarios, outperforming baselines on tool use, question answering, and mathematical reasoning tasks.***  <br>  <br>
    Apr 7, Stanford Uni and Google published a [paper](https://arxiv.org/pdf/2504.04736) “Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use”. Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. The study proposes a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. The study evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%.  <br>  <br>

35. ***Auditing Model Substitution in LLM APIs:   <br>UC Berkeley formalizes the problem of model substitution detection in LLM APIs, evaluating existing verification techniques and discussing the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as a pathway towards provable model integrity.***  <br>  <br>
    Apr 6, UC Berkeley published a [paper](https://arxiv.org/pdf/2504.04715) “Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs”. The proliferation of Large Language Models (LLMs) accessed via black-box APIs introduces a significant trust challenge: users pay for services based on advertised model capabilities (e.g., size, performance), but providers may covertly substitute the specified model with a cheaper, lower-quality alternative to reduce operational costs. This lack of transparency undermines fairness, erodes trust, and complicates reliable benchmarking. Detecting such substitutions is difficult due to the black-box nature, typically limiting interaction to input-output queries. This paper formalizes the problem of model substitution detection in LLM APIs. The study systematically evaluates existing verification techniques, including output-based statistical tests, benchmark evaluations, and log probability analysis, under various realistic attack scenarios like model quantization, randomized substitution, and benchmark evasion. Findings reveal the limitations of methods relying solely on text outputs, especially against subtle or adaptive attacks. While log probability analysis offers stronger guarantees when available, its accessibility is often limited. The work concludes by discussing the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as a pathway towards provable model integrity, highlighting the trade-offs between security, performance, and provider adoption. Code is available at https://github.com/sunblaze-ucb/llm-api-audit  <br>  <br>

37. ***Retro-Search Distills Higher Quality Reasoning Paths:   <br>Nvidia, University of Washington, and Stanford University introduce Retro-Search, an MCTS-inspired search algorithm for distilling higher quality reasoning paths from large reasoning models, enabling models to self-improve or weak models to improve stronger models' traces, resulting in shorter and faster inference.***  <br>  <br>
    Apr 6, Nvidia, Uni of Washington and Stanford Uni published a [paper](https://arxiv.org/pdf/2504.04383) “Retro-Search: Exploring Untaken Paths for Deeper and Efficient Reasoning”. Large reasoning models exhibit remarkable reasoning capabilities via long, elaborate reasoning trajectories. Supervised fine-tuning on such reasoning traces, also known as distillation, can be a cost-effective way to boost reasoning capabilities of student models. However, empirical observations reveal that these reasoning trajectories are often suboptimal, switching excessively between different lines of thought, resulting in under-thinking, over-thinking, and even degenerate responses. The study introduces Retro-Search, an MCTS-inspired search algorithm, for distilling higher quality reasoning paths from large reasoning models. Retro-Search retrospectively revises reasoning paths to discover better, yet shorter traces, which can then lead to student models with enhanced reasoning capabilities with shorter, thus faster inference. The approach can enable two use cases: self-improvement, where models are fine-tuned on their own Retro-Search-ed thought traces, and weak-to-strong improvement, where a weaker model revises stronger model's thought traces via Retro-Search. For self-improving, R1-distill-7B, fine-tuned on its own Retro-Search-ed traces, reduces the average reasoning length by 31.2% while improving performance by 7.7% across seven math benchmarks. For weak-to-strong improvement, the work retrospectively revises R1-671B's traces from the OpenThoughts dataset using R1-distill-32B as the Retro-Search-er, a model 20x smaller. Qwen2.5-32B, fine-tuned on this refined data, achieves performance comparable to R1-distill-32B, yielding an 11.3% reduction in reasoning length and a 2.4% performance improvement compared to fine-tuning on the original OpenThoughts data. The work counters recently emergent viewpoints that question the relevance of search algorithms in the era of large reasoning models, by demonstrating that there are still opportunities for algorithmic advancements, even for frontier models.  <br>  <br>

39. ***Rethinking Temporal Search for Long-Form Video Understanding:   <br>Stanford University, Northwestern University, and CMU revisit temporal search paradigms for long-form video understanding, introducing LV-Haystack (a long video haystack problem) and propose T*, a lightweight temporal search framework that improves performance.***  <br>  <br>
    Apr 6, Stanford Uni, Northwestern Uni and CMU published a [paper](https://arxiv.org/pdf/2504.02259) “Re-thinking Temporal Search for Long-Form Video Understanding”. Efficiently understanding long-form videos remains a significant challenge in computer vision. This study revisits temporal search paradigms for long-form video understanding and address a fundamental issue pertaining to all state-of-the-art (SOTA) long-context vision-language models (VLMs). The contributions are twofold: First, the work frames temporal search as a Long Video Haystack problem: finding a minimal set of relevant frames (e.g., one to five) from tens of thousands based on specific queries. Upon this formulation, the study introduces LV-Haystack, the first dataset with 480 hours of videos, 15,092 human-annotated instances for both training and evaluation aiming to improve temporal search quality and efficiency. Results on LV-Haystack highlight a significant research gap in temporal search capabilities, with current SOTA search methods only achieving 2.1% temporal F1 score on the Longvideobench subset. Next, inspired by visual search in images, the study proposes a lightweight temporal search framework, T* that reframes costly temporal search as spatial search. T* leverages powerful visual localization techniques commonly used in images and introduces an adaptive zooming-in mechanism that operates across both temporal and spatial dimensions. Extensive experiments show that integrating T* with existing methods significantly improves SOTA long-form video understanding. Under an inference budget of 32 frames, T* improves GPT-4o's performance from 50.5% to 53.1% and LLaVA-OneVision-OV-72B's performance from 56.5% to 62.4% on the Longvideobench XL subset. The code, benchmark, and models is here https://longvideohaystack.github.io/.

41. ***Pretraining Scaling Law for LLM Reasoning Explored: UCSB, MIT-IBM, and Rutgers University explore the effects of scaling on LLMs' reasoning abilities, using a synthetic multihop reasoning environment, and find that overparameterization can impair reasoning performance due to excessive memorization, identifying an empirical scaling law for optimal model size.***
    Apr 4, UCSB, MIT-IBM and Rutgers Uni published a [paper](https://arxiv.org/pdf/2504.03635) “Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning”. Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks requiring complex reasoning. However, the effects of scaling on their reasoning abilities remain insufficiently understood. This study introduces a synthetic multihop reasoning environment designed to closely replicate the structure and distribution of real-world large-scale knowledge graphs. Our reasoning task involves completing missing edges in the graph, which requires advanced multi-hop reasoning and mimics real-world reasoning scenarios. To evaluate this, the work pretrains language models (LMs) from scratch solely on triples from the incomplete graph and assess their ability to infer the missing edges. Interestingly, the study observes that overparameterization can impair reasoning performance due to excessive memorization. The study investigates different factors that affect this U-shaped loss curve, including graph structure, model size, and training steps. To predict the optimal model size for a specific knowledge graph, the work finds an empirical scaling that linearly maps the knowledge graph search entropy to the optimal model size. This work provides new insights into the relationship between scaling and reasoning in LLMs, shedding light on possible ways to optimize their performance for reasoning tasks.  <br>  <br>

43. ***CoT Faithfulness in Reasoning Models Examined:   <br>Anthropic explores the faithfulness of Chain-of-Thought (CoT) in reasoning models across 6 reasoning hints, finding that while CoTs reveal hint usage, the reveal rate is often low and that RL improvements don't necessarily increase verbalization of hints, indicating CoT monitoring is promising but not sufficient for ensuring AI safety.***  <br>  <br>
    Apr 3, Anthropic published a [paper](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf) “Reasoning Models Don’t Always Say What They Think”. Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model’s CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models’ actual reasoning processes. The work evaluates CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.  <br>  <br>

45. ***AI Conversations Improve Happiness:   <br>Yale University, UCL, Google, University of Oxford, and MPUCL find that conversations with AI chatbots can increase subjective well-being, particularly when discussing negative topics, due to the AI's positivity bias and its impact on emotional expectations.***  <br>  <br>
    Apr 2, Yale Uni, UCL, Google, Uni of Oxford and MPUCL published a [paper](https://arxiv.org/pdf/2504.02091) “Increasing happiness through conversations with artificial intelligence”. Chatbots powered by artificial intelligence (AI) have rapidly become a significant part of everyday life, with over a quarter of American adults using them multiple times per week. While these tools offer potential benefits and risks, a fundamental question remains largely unexplored: How do conversations with AI influence subjective well-being? To investigate this, the work conducted a study where participants either engaged in conversations with an AI chatbot (N = 334) or wrote journal entires (N = 193) on the same randomly assigned topics and reported their momentary happiness afterward. The study found that happiness after AI chatbot conversations was higher than after journaling, particularly when discussing negative topics such as depression or guilt. Leveraging large language models for sentiment analysis, the work found that the AI chatbot mirrored participants' sentiment while maintaining a consistent positivity bias. When discussing negative topics, participants gradually aligned their sentiment with the AI's positivity, leading to an overall increase in happiness. The authors hypothesized that the history of participants' sentiment prediction errors, the difference between expected and actual emotional tone when responding to the AI chatbot, might explain this happiness effect. Using computational modeling, the work finds the history of these sentiment prediction errors over the course of a conversation predicts greater post-conversation happiness, demonstrating a central role of emotional expectations during dialogue. The findings underscore the effect that AI interactions can have on human well-being.  <br>  <br>

47. ***AI Judges Achieve Human Expert Equivalence in Design:   <br>  <br>MIT and Penn State University introduce a statistical framework to determine whether AI judges match human experts in design evaluation and find that reasoning-supported VLM models can achieve human-expert equivalence in design evaluation, potentially scaling design evaluation in education and practice.***  <br>  <br>
    Apr 1, MIT and Penn State Uni published a [paper](https://arxiv.org/abs/2504.00938) “AI Judges in Design: Statistical Perspectives on Achieving Human Expert Equivalence With Vision-Language Models”. The subjective evaluation of early stage engineering designs, such as conceptual sketches, traditionally relies on human experts. However, expert evaluations are time-consuming, expensive, and sometimes inconsistent. Recent advances in vision-language models (VLMs) offer the potential to automate design assessments, but it is crucial to ensure that these AI “judges” perform on par with human experts. However, no existing framework assesses expert equivalence. This paper introduces a rigorous statistical framework to determine whether an AI judge's ratings match those of human experts. The authors apply this framework in a case study evaluating four VLM-based judges on key design metrics (uniqueness, creativity, usefulness, and drawing quality). These AI judges employ various in-context learning (ICL) techniques, including uni- vs. multimodal prompts and inference-time reasoning. The same statistical framework is used to assess three trained novices for expert-equivalence. Results show that the top-performing AI judge, using text- and image-based ICL with reasoning, achieves expert-level agreement for uniqueness and drawing quality and outperforms or matches trained novices across all metrics. In 6/6 runs for both uniqueness and creativity, and 5/6 runs for both drawing quality and usefulness, its agreement with experts meets or exceeds that of the majority of trained novices. These findings suggest that reasoning-supported VLM models can achieve human-expert equivalence in design evaluation. This has implications for scaling design evaluation in education and practice, and provides a general statistical framework for validating AI judges in other domains requiring subjective content evaluation.

  <br>  <br>  <br>

***Apr 6, 2025***


1. ***Meta Unveils Llama 4 Herd:   <br>Meta has released the Llama 4 herd of models, including Llama 4 Scout (a 17B parameter multimodal model with a 10M context window) and Llama 4 Maverick (a 17B parameter multimodal model outperforming GPT-4o and Gemini 2.0 Flash), both distilled from Llama 4 Behemoth, a powerful 288B parameter model that outperforms GPT-4.5 and other models on STEM benchmarks.***  <br>  <br>
   Apr 5, Meta [released Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/) herd, which will enable people to build more personalized multimodal experiences. Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks. Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena. These models are Meta’s best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is Meta’s most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training. Llama 4 Scout and Llama 4 Maverick models are available at [Huggingface](https://huggingface.co/meta-llama) and llama.com  <br>  <br>

3. ***DeepSeek Explores Inference-Time Reward Scaling:   <br>DeepSeek and Tsinghua University's research explores improving reward modeling (RM) for LLMs with increased inference compute, proposing Self-Principled Critique Tuning (SPCT) for scalable reward generation and a meta RM for better voting performance, resulting in DeepSeek-GRM models that outperform existing methods.***  <br>  <br>
   Apr 3, DeepSeek and Tsinghua Uni published a [paper](https://arxiv.org/abs/2504.02495) “Inference-Time Scaling for Generalist Reward Modeling”. Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that proper learning methods could enable effective inference-time scalability. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. This work investigates how to improve reward modeling (RM) with more inference compute for general queries, i.e. the inference-time scalability of generalist RM, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, the work adopts pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, the research proposes Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in DeepSeek-GRM models. Furthermore, for effective inference-time scaling, the study uses parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, the authors show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which the authors believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced.  <br>  <br>

5. ***MIT Investigates AI Scientists' Agreement:   <br>MIT's paper "Do Two AI Scientists Agree" explores whether AI models trained on the same scientific task learn the same theories, finding that AI scientists tend to converge in their learned theories with more training data, using Hamiltonian-Lagrangian neural networks (MASS) as a tool for interpretation.***  <br>  <br>
   Apr 3, MIT published a [paper](https://arxiv.org/pdf/2504.02822v1) “Do Two AI Scientists Agree”. When two AI models are trained on the same scientific task, do they learn the same theory or two different theories? Throughout the history of science, people have witnessed the rise and fall of theories driven by experimental validation or falsification: many theories may co-exist when experimental data is lacking, but the space of surviving theories becomes more constrained with more experimental data becoming available. The work shows the same story is true for AI scientists. With increasingly more systems provided in training data, AI scientists tend to converge in the theories they learned, although sometimes they form distinct groups corresponding to different theories. To mechanistically interpret what theories AI scientists learn and quantify their agreement, the study proposes MASS, Hamiltonian-Lagrangian neural networks as AI Scientists, trained on standard problems in physics, aggregating training results across many seeds simulating the different configurations of AI scientists. The key findings include: 1) when trained on textbook problems in classical mechanics, AI scientists prefers either a complete Hamiltonian or Lagrangian description; 2) when extended to non-standard physical problems, the Lagrangian description generalizes, suggesting that Lagrangian dynamics remain as the singular accurate family of descriptions in a rich theory space. The work also observes strong seed dependence of the training dynamics and final learned weights, controlling the rise and fall of relevant theories. Besides interpretability, MASS unifies and generalizes beyond the Lagrangian neural networks and the Hamiltonian neural networks, providing a new tool for learning of dynamical systems. Code is at https://github.com/shinfxh/ai-scientists  <br>  <br>

7. ***Understanding Attention Sinks in LLMs:   <br>Researchers from the University of Oxford, National University of Singapore, and Google theoretically and empirically argue that the heavy attention LLMs give to the first token in a sequence, creating an "attention sink," is a mechanism to avoid over-mixing and relate it to how information propagates in Transformers.***  <br>  <br>
   Apr 3, Uni of Oxford, National Uni of Singapore and Google published a [paper](https://arxiv.org/pdf/2504.02732) “Why do LLMs attend to the first token?”. Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink. Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it. Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention. Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used? This study argues theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers. The work conducts experiments to validate the theoretical intuitions and shows how choices such as context length, depth, and data packing influence the sink behaviour. The authors hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training.  <br>  <br>

9. ***AI 2027 Predicts Transformative Superhuman AI:   <br>ai-2027.com predicts that superhuman AI will have a monumental impact within the next decade, potentially surpassing the Industrial Revolution, emphasizing the need for society to prepare for the advent of superintelligence, and presents a detailed scenario called "AI 2027" to stimulate conversation about the future of AI.***  <br>  <br>
    Apr 3, ai-2027.com published a [paper](https://ai-2027.com/scenario.pdf) “AI 2027”. The authors predict that the impact of superhuman AI over the next decade will be monumental, potentially exceeding the transformative effects of the Industrial Revolution. Prominent figures in AI, including the CEOs of OpenAI, Google DeepMind, and Anthropic, have forecasted the arrival of Artificial General Intelligence (AGI) within the next five years. Sam Altman of OpenAI has expressed ambitions for achieving true superintelligence and envisions a "glorious future." While some may dismiss these predictions as mere hype, the authors caution against this, emphasizing the serious and plausible nature of these developments. They argue that society is currently unprepared for the advent of superintelligence, with few having mapped out a viable path for its development. To address this gap, they created "AI 2027," a detailed scenario that provides concrete details and encourages a broader conversation about the future of AI and how to navigate towards positive outcomes. The authors developed their scenarios by continuously asking "what would happen next," starting from the present day and iterating through multiple versions until they arrived at plausible conclusions. Their work involved extensive background research, expert interviews, and trend extrapolation to make informed predictions. The team, which includes Daniel Kokotajlo and Eli Lifland, has a strong track record in forecasting, particularly in the field of AI. Kokotajlo previously authored a scenario called "What 2026 Looks Like," which proved to be remarkably accurate, and Lifland is recognized as a top competitive forecaster.  <br>  <br>

11. ***ScholarCopilot Enhances Academic Writing with LLMs:   <br>The University of Waterloo, CMU, and others introduce ScholarCopilot, a unified framework to enhance LLMs for generating professional academic articles with accurate citations, dynamically retrieving scholarly references and optimizing both generation and citation tasks, achieving superior performance in retrieval accuracy and generation quality.***  <br>  <br>
    Apr 3, Uni of Waterloo, CMU, et al. published a [paper](https://arxiv.org/pdf/2504.00824) “ScholarCopilot: Training Large Language Models for Academic Writing with Accurate Citations”. Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their capacity to adequately support professional academic writing remains limited. This work introduces ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], and then utilizes its representation to look up relevant citations from a database. The retrieved references are fed into the model to augment the generation process. The study jointly optimizes both the generation and citation tasks within a single framework to increase efficiency. Trained on 500K papers from arXiv, the model achieves a top-1 retrieval accuracy of 40.1% on the evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality (measured across relevance, coherence, academic rigor, completeness, and innovation), surpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct (15.8/25). Human studies also confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience, confirming the effectiveness of our approach.  <br>  <br>

13. ***Dreamer Masters Control Tasks Through World Models:   <br>Nature's paper presents Dreamer, a general reinforcement-learning algorithm that learns to solve tasks across a wide range of applications, outperforming specialized methods across over 150 diverse tasks by learning a model of the environment and imagining future scenarios, and is the first algorithm to collect diamonds in Minecraft without human data or curricula.***  <br>  <br>
    Apr 2, Nature published a [paper](https://www.nature.com/articles/s41586-025-08744-2) “Mastering diverse control tasks through world models”. Developing a general algorithm that learns to solve tasks across a wide range of applications has been a fundamental challenge in artificial intelligence. Although current reinforcement-learning algorithms can be readily applied to tasks similar to what they have been developed for, configuring them for new application domains requires substantial human expertise and experimentation. This study presents the third generation of Dreamer, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration. Dreamer learns a model of the environment and improves its behaviour by imagining future scenarios. Robustness techniques based on normalization, balancing and transformations enable stable learning across domains. Applied out of the box, Dreamer is, to authors knowledge, the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula. This achievement has been posed as a substantial challenge in artificial intelligence that requires exploring farsighted strategies from pixels and sparse rewards in an open world3. The work allows solving challenging control problems without extensive experimentation, making reinforcement learning broadly applicable.  <br>  <br>

15. ***YourBench Enables Easy Custom Evaluation Sets:   <br>Hugging Face and UIUC introduce YourBench, an open-source framework that allows dynamic, automated generation of reliable, up-to-date, and domain-tailored benchmarks cheaply and without manual annotation, along with the Tempora-0325 dataset of recently published documents, to foster more relevant and trustworthy LLM evaluation.***  <br>  <br>
    Apr 2, Huggingface and UIUC published a [paper](https://arxiv.org/abs/2504.01833) “YourBench: Easy Custom Evaluation Sets for Everyone”. Evaluating large language models (LLMs) effectively remains a critical bottleneck, as traditional static benchmarks suffer from saturation and contamination, while human evaluations are costly and slow. This hinders timely or domain-specific assessment, crucial for real-world applications. The work introduce YourBench, a novel, open-source framework that addresses these limitations by enabling dynamic, automated generation of reliable, up-to-date, and domain-tailored benchmarks cheaply and without manual annotation, directly from user-provided documents. The study demonstrates its efficacy by replicating 7 diverse MMLU subsets using minimal source text, achieving this for under 15 USD in total inference costs while perfectly preserving the relative model performance rankings (Spearman Rho = 1) observed on the original benchmark. To ensure that YourBench generates data grounded in provided input instead of relying on posterior parametric knowledge in models, the study also introduces Tempora-0325, a novel dataset of over 7K diverse documents, published exclusively after March 2025. A comprehensive analysis spans 26 SoTA models from 7 major families across varying scales (3-671B parameters) to validate the quality of generated evaluations through rigorous algorithmic checks (e.g., citation grounding) and human assessments. The authors release the YourBench library, the Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all evaluation and inference traces to facilitate reproducible research and empower the community to generate bespoke benchmarks on demand, fostering more relevant and trustworthy LLM evaluation.  <br>  <br>

17. ***Google Outlines Technical AGI Safety Approach:   <br>Google's paper outlines an approach to address the risks of Artificial General Intelligence (AGI), focusing on technical approaches to misuse and misalignment, including preventing threat actors from accessing dangerous capabilities and building aligned models with amplified oversight and system-level security.***  <br>  <br>
    Apr 2, Google published a 145-page [paper](https://arxiv.org/pdf/2504.01849) “An Approach to Technical AGI Safety and Security”. Artificial General Intelligence (AGI) promises transformative benefits but also presents significant risks. The work develops an approach to address the risk of harms consequential enough to significantly harm humanity. The study identifies four areas of risk: misuse, misalignment, mistakes, and structural risks. Of these, the work focuses on technical approaches to misuse and misalignment. For misuse, the strategy aims to prevent threat actors from accessing dangerous capabilities, by proactively identifying dangerous capabilities, and implementing robust security, access restrictions, monitoring, and model safety mitigations. To address misalignment, the study outlines two lines of defense. First, model-level mitigations such as amplified oversight and robust training can help to build an aligned model. Second, system-level security measures such as monitoring and access control can mitigate harm even if the model is misaligned. Techniques from interpretability, uncertainty estimation, and safer design patterns can enhance the effectiveness of these mitigations. Finally, the study briefly outlines how these ingredients could be combined to produce safety cases for AGI systems.  <br>  <br>

19. ***PaperBench Evaluates AI's Ability to Replicate Research:   <br>OpenAI introduces PaperBench, a benchmark that evaluates AI agents' ability to replicate state-of-the-art AI research, requiring them to understand papers, develop codebases, and execute experiments, finding that current models do not yet outperform human researchers.***  <br>  <br>
    Apr 2, OpenAI published a [paper](https://arxiv.org/abs/2504.01848) “PaperBench: Evaluating AI's Ability to Replicate AI Research”. The work introduces PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, the authors develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, the work also develops an LLM-based judge to automatically grade replication attempts against rubrics, and assess the judge's performance by creating a separate benchmark for judges. The study evaluates several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0%. Finally, the authors recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. Code is at https://github.com/openai/preparedness  <br>  <br>

21. ***ZClip Mitigates Loss Spikes in LLM Training:   <br>BluOrion introduces ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time to proactively mitigate large gradient spikes during LLM training.***  <br>  <br>
    Apr 2, BluOrion published a [paper](https://arxiv.org/pdf/2504.02507) “ZClip: Adaptive Spike Mitigation for LLM Pre-Training”. Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. This work proposes ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Code is available at: https://github.com/bluorion-com/ZClip  <br>  <br>

23. ***Visual SSL Matches CLIP Performance at Scale:   <br>Meta, NYU, and Princeton University demonstrate that Visual Self-Supervised Learning (SSL) can match Contrastive Language-Image Pretraining (CLIP) performance on VQA and vision benchmarks when trained at scale on the same data, suggesting that pure visual SSL can match language-supervised visual pretraining.***  <br>  <br>
    Apr 1, Meta, NYU and Princeton Uni published a [paper](https://arxiv.org/pdf/2504.01017) “Scaling Language-Free Visual Representation Learning”. Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. This study asks the question: "Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?" The authors study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, the work observes visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning.  <br>  <br>

25. ***Multi-Token Attention Enhances LLM Performance:   <br>Meta introduces Multi-Token Attention (MTA), a new attention method that allows LLMs to condition their attention weights on multiple query and key vectors simultaneously, achieved by applying convolution operations over queries, keys and heads, resulting in enhanced performance on language modeling tasks.***  <br>  <br>
    Apr 1, Meta published a [paper](https://arxiv.org/pdf/2504.00927) “Multi-Token Attention”. Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This "single token attention" bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, the study proposes a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other's attention weights for more precise attention. As a result, the method can locate relevant context using richer, more nuanced information that can exceed a single vector's capacity. Through extensive evaluations, the study demonstrates that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where the method's ability to leverage richer information proves particularly beneficial.  <br>  <br>

27. ***Execution-Guided SQL Generation Improves Accuracy:   <br>Snowflake proposes a novel approach for generating complex outputs in text-to-SQL tasks that leverages execution results to select the most semantically consistent query, enabling smaller models to surpass computationally intensive reasoning methods while reducing inference costs.***  <br>  <br>
    Apr 1, Snowflake published a [paper](https://arxiv.org/pdf/2503.24364) “Query and Conquer: Execution-Guided SQL Generation”. The study proposes a novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. The method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally intensive reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference cost by as much as 30 times. It integrates effortlessly with existing models, offering a practical and scalable pathway to state-of-the-art SQL generation.  <br>  <br>

29. ***Compute-Optimal Problem Solving for LLM Reasoning:   <br>TU Darmstadt & hessian.AI, UCLA, Google and Mila compare Self-Consistency (SC) and Generative Reward Models (GenRM) for scaling test-time compute in LLM reasoning, finding that SC is more compute-efficient for most practical inference budgets and deriving inference scaling laws for the GenRM paradigm.***  <br>  <br>
    Apr 1, TU Darmstadt & hessian.AI, UCLA, Google and Mila published a [paper](https://arxiv.org/pdf/2504.01005) “When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning”. Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should one spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, the work evaluates GenRM against SC under a fixed inference budget. Interestingly, the study finds that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, the work derives inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. The work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling  <br>  <br>

31. ***Token Embeddings Violate the Manifold Hypothesis:   <br>American University, Galois Inc, and the University of Washington find that token embeddings in LLMs do not conform to the manifold hypothesis, with the token subspace provably not a fiber bundle, leading to potentially flawed understandings and conclusions about LLMs.***  <br>  <br>
    Apr 1, American Uni, Galois Inc and Uni of Washington published a [paper](https://arxiv.org/pdf/2504.01002) “Token embeddings violate the manifold hypothesis”. To fully understand the behavior of a large language model (LLM) requires the understanding of its input space. If this input space differs from assumption, the understanding of and conclusions about the LLM is likely flawed, regardless of its architecture. Here, the work elucidates the structure of the token embeddings, the input domain for LLMs, both empirically and theoretically. The study presents a generalized and statistically testable model where the neighborhood of each token splits into well-defined signal and noise dimensions. This model is based on a generalization of a manifold called a fiber bundle, so the work denotes the hypothesis test as the “fiber bundle null.” Failing to reject the null is uninformative, but rejecting it at a specific token indicates that token has a statistically significant local structure, and so is of interest. By running the test over several open-source LLMs, each with unique token embeddings, the work finds that the null is frequently rejected, and so the token subspace is provably not a fiber bundle and hence also not a manifold. As a consequence of the findings, when an LLM is presented with two semantically equivalent prompts, and if one prompt contains a token implicated by the test, that prompt will likely exhibit more output variability proportional to the local signal dimension of the token.  <br>  <br>

33. ***NoProp: A Gradient-Free Learning Method for Neural Networks:   <br>The University of Oxford and Mila introduce NoProp, a new learning method for training neural networks that does not rely on forward or backward propagation, instead drawing inspiration from diffusion and flow matching methods, demonstrating effectiveness on image classification benchmarks.***  <br>  <br>
    Mar 31, Uni of Oxford and Mila published a [paper](https://arxiv.org/pdf/2503.24322) “NoProp: Training Neural Networks without Back-propagation or Forward-propagation”. The canonical deep learning approach for learning requires computing a gradient term at each layer by back-propagating the error signal from the output towards each learnable parameter. Given the stacked structure of neural networks, where each layer builds on the representation of the layer below, this approach leads to hierarchical representations. More abstract features live on the top layers of the model, while features on lower layers are expected to be less abstract. In contrast to this, the study introduces a new learning method named NoProp, which does not rely on either forward or backwards propagation. Instead, NoProp takes inspiration from diffusion and flow matching methods, where each layer independently learns to denoise a noisy target. The authors believe this work takes a first step towards introducing a new family of gradient-free learning methods, that does not learn hierarchical representations -- at least not in the usual sense. NoProp needs to fix the representation at each layer beforehand to a noised version of the target, learning a local denoising process that can then be exploited at inference. The study demonstrates the effectiveness of the method on MNIST, CIFAR-10, and CIFAR-100 image classification benchmarks. Results show that NoProp is a viable learning algorithm which achieves superior accuracy, is easier to use and computationally more efficient compared to other existing back-propagation-free methods. By departing from the traditional gradient based learning paradigm, NoProp alters how credit assignment is done within the network, enabling more efficient distributed learning as well as potentially impacting other characteristics of the learning process.  <br>  <br>

35. ***Thinking Intervention Controls Reasoning Models:   <br>Princeton University and Nvidia propose Thinking Intervention, a novel paradigm for controlling reasoning-enhanced LLMs by strategically inserting or revising specific thinking tokens, achieving significant improvements in instruction following, reasoning about instruction hierarchies, and safety alignment.***  <br>  <br>
    Mar 31, Princeton Uni and Nvidia published a [paper](https://arxiv.org/abs/2503.24370) “Effectively Controlling Reasoning Models through Thinking Intervention”. Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. This study demonstrates that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. The study proposes Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. The work conducts comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, the work opens a promising new research avenue for controlling reasoning LLMs.  <br>  <br>

37. ***LLMs Pass the Turing Test:   <br>UC San Diego presents the first empirical evidence that a GPT model (GPT-4.5) passes a standard three-party Turing test, being judged as the human partner more often than the real human, and the implications on defining intelligence in LLMs.***  <br>  <br>
    Mar 31, UC San Diego published a [paper](https://arxiv.org/pdf/2503.23674) “Large Language Models Pass the Turing Test”. The study evaluated 4 systems (ELIZA, GPT-4o, LLaMa-3.1-405B, and GPT-4.5) in two randomised, controlled, and pre-registered Turing tests on independent populations. Participants had 5 minute conversations simultaneously with another human participant and one of these systems before judging which conversational partner they thought was human. When prompted to adopt a humanlike persona, GPT-4.5 was judged to be the human 73% of the time: significantly more often than interrogators selected the real human participant. LLaMa-3.1, with the same prompt, was judged to be the human 56% of the time -- not significantly more or less often than the humans they were being compared to -- while baseline models (ELIZA and GPT-4o) achieved win rates significantly below chance (23% and 21% respectively). The results constitute the first empirical evidence that any artificial system passes a standard three-party Turing test. The results have implications for debates about what kind of intelligence is exhibited by Large Language Models (LLMs), and the social and economic impacts these systems are likely to have.  <br>  <br>

39. ***GNNs Extrapolate OOD for Shortest Paths:   <br>UCSD demonstrates that Graph Neural Networks (GNNs), when trained to minimize a sparsity-regularized loss, exactly implement the Bellman-Ford (BF) algorithm for shortest paths and are therefore guaranteed to extrapolate to arbitrary shortest-path problems.***  <br>  <br>
    Mar 31, UCSD published a [paper](https://arxiv.org/pdf/2503.19173) “Graph neural networks extrapolate out-of-distribution for shortest paths”. Neural networks (NNs), despite their success and wide adoption, still struggle to extrapolate out-of-distribution (OOD), i.e., to inputs that are not well-represented by their training dataset. Addressing the OOD generalization gap is crucial when models are deployed in environments significantly different from the training set, such as applying Graph Neural Networks (GNNs) trained on small graphs to large, real-world graphs. One promising approach for achieving robust OOD generalization is the framework of neural algorithmic alignment, which incorporates ideas from classical algorithms by designing neural architectures that resemble specific algorithmic paradigms (e.g. dynamic programming). The hope is that trained models of this form would have superior OOD capabilities, in much the same way that classical algorithms work for all instances. The work rigorously analyzes the role of algorithmic alignment in achieving OOD generalization, focusing on graph neural networks (GNNs) applied to the canonical shortest path problem. The study proves that GNNs, trained to minimize a sparsity-regularized loss over a small set of shortest path instances, exactly implement the Bellman-Ford (BF) algorithm for shortest paths. In fact, if a GNN minimizes this loss within an error of ϵ, it implements the BF algorithm with an error of O(ϵ). Consequently, despite limited training data, these GNNs are guaranteed to extrapolate to arbitrary shortest-path problems, including instances of any size. Empirical results support the theory by showing that NNs trained by gradient descent are able to minimize this loss and extrapolate in practice.  <br>  <br>

41. ***MVDRAM Accelerates LLM Inference with Unmodified DRAM:   <br>The University of Tokyo and Microsoft present MVDRAM, a practical system to accelerate GeMV operations for low-bit LLM inference using unmodified DRAM by leveraging data sharing patterns and mathematical linearity, achieving significant speedup and energy efficiency.***  <br>  <br>
    Mar 31, Uni of Tokyo and Microsoft published a [paper](https://arxiv.org/pdf/2503.23817) “MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM Acceleration”. General matrix-vector multiplication (GeMV) remains a critical latency bottleneck in large language model (LLM) inference, even with quantized low-bit models. Processing-Using-DRAM (PUD), an analog in-DRAM computing technique, has the potential to repurpose on-device DRAM as a GeMV engine, offering additional high-throughput processing capabilities to widespread consumer devices without DRAM modifications. However, applying PUD to GeMV operations in the LLM inference pipeline incurs significant overheads before and after in-DRAM computation, diminishing the benefits of its high-throughput processing capabilities. This paper presents MVDRAM, the first practical system to accelerate GeMV operations for low-bit LLM inference using unmodified DRAM. By leveraging the data sharing patterns and mathematical linearity in GeMV operations, MVDRAM orchestrates the processor and DRAM to eliminate the costs associated with pre-arranging inputs and bit-transposition of outputs required in conventional PUD approaches. Experimental evaluation with four DDR4 DRAM modules shows that MVDRAM achieves comparable or even better inference speed than the processor-based implementation for GeMV operations in low-bit (under 4-bit) LLM. In particular, MVDRAM achieves up to 7.29× speedup and 30.5× energy efficiency for low-bit GeMV operations. For end-to-end LLM inference, MVDRAM achieves 2.18× and 1.31× throughput improvements, along with 3.04× and 2.35× energy efficiency, for 2-bit and 4-bit quantized low-bit models, respectively. MVDRAM has the potential to redefine the AI hardware landscape by demonstrating the feasibility of standard DRAM as an LLM accelerator.  <br>  <br>

43. ***Contradiction Detection Evaluated in RAG Systems:   <br>Amazon addresses the challenge of contradictory information in RAG systems, presenting a data generation framework to simulate different contradiction types and evaluating LLMs' ability to detect them, finding that context validation remains challenging even for state-of-the-art models.***  <br>  <br>
    Mar 31, Amazon published a [paper](https://arxiv.org/abs/2504.00180) “Contradiction Detection in RAG Systems: Evaluating LLMs as Context Validators for Improved Information Consistency”. Retrieval Augmented Generation (RAG) systems have emerged as a powerful method for enhancing large language models (LLMs) with up-to-date information. However, the retrieval step in RAG can sometimes surface documents containing contradictory information, particularly in rapidly evolving domains such as news. These contradictions can significantly impact the performance of LLMs, leading to inconsistent or erroneous outputs. This study addresses this critical challenge in two ways. First, the work presents a novel data generation framework to simulate different types of contradictions that may occur in the retrieval stage of a RAG system. Second, the study evaluates the robustness of different LLMs in performing as context validators, assessing their ability to detect contradictory information within retrieved document sets. Experimental results reveal that context validation remains a challenging task even for state-of-the-art LLMs, with performance varying significantly across different types of contradictions. While larger models generally perform better at contradiction detection, the effectiveness of different prompting strategies varies across tasks and model architectures. The work finds that chain-of-thought prompting shows notable improvements for some models but may hinder performance in others, highlighting the complexity of the task and the need for more robust approaches to context validation in RAG systems.  <br>  <br>

45. ***Interpretability in Machine Learning for Physics Reviewed:   <br>The University of Waterloo and others review the role of interpretability in machine learning applied to physics, categorizing different aspects of interpretability, discussing machine learning models in terms of both interpretability and performance, and exploring the philosophical implications of interpretability in scientific inquiry.***  <br>  <br>
    Mar 30, Uni of Waterloo et al published a [paper](https://arxiv.org/pdf/2503.23616) “Interpretable Machine Learning in Physics: A Review”. Machine learning is increasingly transforming various scientific fields, enabled by advancements in computational power and access to large data sets from experiments and simulations. As artificial intelligence (AI) continues to grow in capability, these algorithms will enable many scientific discoveries beyond human capabilities. Since the primary goal of science is to understand the world around us, fully leveraging machine learning in scientific discovery requires models that are interpretable -- allowing experts to comprehend the concepts underlying machine-learned predictions. Successful interpretations increase trust in black-box methods, help reduce errors, allow for the improvement of the underlying models, enhance human-AI collaboration, and ultimately enable fully automated scientific discoveries that remain understandable to human scientists. This review examines the role of interpretability in machine learning applied to physics. The authors categorize different aspects of interpretability, discuss machine learning models in terms of both interpretability and performance, and explore the philosophical implications of interpretability in scientific inquiry. Additionally, the work highlights recent advances in interpretable machine learning across many subfields of physics. By bridging boundaries between disciplines -- each with its own unique insights and challenges -- aiming to establish interpretable machine learning as a core research focus in science.  <br>  <br>

47. ***Challenges and Paths Towards AI for Software Engineering Discussed:   <br>MIT and others discuss progress, challenges, and promising research directions for AI in software engineering, emphasizing tasks beyond code generation and completion and aiming for high levels of automation in routine development efforts.***  <br>  <br>
    Mar 28, MIT et al published a [paper](https://arxiv.org/pdf/2503.22625) “Challenges and Paths Towards AI for Software Engineering”. AI for software engineering has made remarkable progress recently, becoming a notable success within generative AI. Despite this, there are still many challenges that need to be addressed before automated software engineering reaches its full potential. It should be possible to reach high levels of automation where humans can focus on the critical decisions of what to build and how to balance difficult tradeoffs while most routine development effort is automated away. Reaching this level of automation will require substantial research and engineering efforts across academia and industry. This study aims to discuss progress towards this in a threefold manner. First, the study provides a structured taxonomy of concrete tasks in AI for software engineering, emphasizing the many other tasks in software engineering beyond code generation and completion. Second, the work outlines several key bottlenecks that limit current approaches. Finally, the work provides an opinionated list of promising research directions toward making progress on these bottlenecks, hoping to inspire future research in this rapidly maturing field.  <br>  <br>

49. ***Entity Frequency Influences Hallucinations in LLMs:   <br>Researchers from the University of Oxford, LMU Munich, and others demonstrate that an asymmetry exists in the recognition of logically equivalent facts, which can be attributed to frequency discrepancies of entities appearing as subjects versus objects in pre-training data, influencing LLM hallucinations.***  <br>  <br>
    Mar 28, Uni of Oxford, LMU Munich et al published a [paper](https://arxiv.org/pdf/2503.22362) “Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs”. Understanding and mitigating hallucinations in Large Language Models (LLMs) is crucial for ensuring reliable content generation. While previous research has primarily focused on "when" LLMs hallucinate, the work explains "why" and directly links model behaviour to the pre-training data that forms their prior knowledge. Specifically, the study demonstrates that an asymmetry exists in the recognition of logically equivalent facts, which can be attributed to frequency discrepancies of entities appearing as subjects versus objects. Given that most pre-training datasets are inaccessible, the work leverages the fully open-source OLMo series by indexing its Dolma dataset to estimate entity frequencies. Using relational facts (represented as triples) from Wikidata5M, the study constructs probing datasets to isolate this effect. Experiments reveal that facts with a high-frequency subject and a low-frequency object are better recognised than their inverse, despite their logical equivalence. The pattern reverses in low-to-high frequency settings, and no statistically significant asymmetry emerges when both entities are high-frequency. These findings highlight the influential role of pre-training data in shaping model predictions and provide insights for inferring the characteristics of pre-training data in closed or partially closed LLMs.  <br>  <br>

51. ***CoT-VLA Enables Visual Chain-of-Thought Reasoning for VLAs:   <br>Nvidia, Stanford University and MIT introduce CoT-VLA, a method that incorporates explicit visual chain-of-thought (CoT) reasoning into vision-language-action models (VLAs) by predicting future image frames autoregressively as visual goals before generating a short action sequence.***  <br>  <br>
    Mar 27, Nvidia, Stanford Uni and MIT published a [paper](https://arxiv.org/pdf/2503.22020) “CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models”. Vision-language-action models (VLAs) have shown potential in leveraging pretrained vision-language models and diverse robot demonstrations for learning generalizable sensorimotor control. While this paradigm effectively utilizes large-scale data from both robotic and non-robotic sources, current VLAs primarily focus on direct input--output mappings, lacking the intermediate reasoning steps crucial for complex manipulation tasks. As a result, existing VLAs lack temporal planning or reasoning capabilities. This work introduces a method that incorporates explicit visual chain-of-thought (CoT) reasoning into vision-language-action models (VLAs) by predicting future image frames autoregressively as visual goals before generating a short action sequence to achieve these goals. The study introduces CoT-VLA, a state-of-the-art 7B VLA that can understand and generate visual and action tokens. Experimental results demonstrate that CoT-VLA achieves strong performance, outperforming the state-of-the-art VLA model by 17% in real-world manipulation tasks and 6% in simulation benchmarks. Project website: https://cot-vla.github.io/  <br>  <br>

53. ***CodeScientist Automates Scientific Discovery with Code-Based Experimentation:   <br>The Allen Institute for AI and others introduce CodeScientist, a novel ASD system that frames ideation and experiment construction as a genetic search jointly over research articles and codeblocks, generating discoveries in the domain of agents and virtual environments.***  <br>  <br>
    Mar 20, Allen Inst. for AI et al published a [paper](https://arxiv.org/pdf/2503.22708) “CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation”. Despite the surge of interest in autonomous scientific discovery (ASD) of software artifacts (e.g., improved ML algorithms), current ASD systems face two key limitations: (1) they largely explore variants of existing codebases or similarly constrained design spaces, and (2) they produce large volumes of research artifacts (such as automatically generated papers and code) that are typically evaluated using conference-style paper review with limited evaluation of code. This study introduces CodeScientist, a novel ASD system that frames ideation and experiment construction as a form of genetic search jointly over combinations of research articles and codeblocks defining common actions in a domain (like prompting a language model). The work uses this paradigm to conduct hundreds of automated experiments on machine-generated ideas broadly in the domain of agents and virtual environments, with the system returning 19 discoveries, 6 of which were judged as being both at least minimally sound and incrementally novel after a multi-faceted evaluation beyond that typically conducted in prior work, including external (conference-style) review, code review, and replication attempts. Moreover, the discoveries span new tasks, agents, metrics, and data, suggesting a qualitative shift from benchmark optimization to broader discoveries.

  <br>  <br>  <br>


***Mar 30, 2025***

1. ***Language Model Embeddings Share Global and Local Geometric Structures.   <br>Researchers from Harvard University and Google have discovered that token embeddings in language models exhibit common geometric structures, including similar relative orientations ("global" similarities) and shared local geometry characterized by intrinsic dimensionality. The study shows that tokens with lower intrinsic dimensions tend to form semantically coherent clusters. Surprisingly, this alignment persists through hidden states, enabling the transfer of steering vectors between language models with different dimensions, which has implications for interpretability.***  <br>  <br>
   Mar 27, Harvard Uni and Google published a [paper](https://www.arxiv.org/pdf/2503.21073) “Shared Global and Local Geometry of Language Model Embeddings”. Researchers have recently suggested that models share common representations. This work finds that the token embeddings of language models exhibit common geometric structure. First, the study finds “global” similarities: token embeddings often share similar relative orientations. Next, the study characterizes local geometry in two ways: (1) by using Locally Linear Embeddings, and (2) by defining a simple measure for the intrinsic dimension of each token embedding. The intrinsic dimension measure demonstrates that token embeddings lie on a lower dimensional manifold. The study qualitatively shows that tokens with lower intrinsic dimensions often have semantically coherent clusters, while those with higher intrinsic dimensions do not. Both characterizations allow to find similarities in the local geometry of token embeddings. Perhaps most surprisingly, the study finds that alignment in token embeddings persists through the hidden states of language models, allowing us to develop an application for interpretability. Namely, the work empirically demonstrates that steering vectors from one language model can be transferred to another, despite the two models having different dimensions.  <br>  <br>

3. ***MCTS-RAG Enhances Small LLM Reasoning with Iterative Retrieval and Search.   <br>Yale University and NYU have introduced MCTS-RAG, a novel approach that improves the reasoning capabilities of small language models on knowledge-intensive tasks. It combines retrieval-augmented generation (RAG) for relevant context with Monte Carlo Tree Search (MCTS) to refine reasoning paths through an iterative decision-making process. This integration of structured reasoning and adaptive retrieval leads to enhanced decision-making, reduced hallucinations, and improved factual accuracy, allowing smaller LMs to achieve performance comparable to frontier LLMs.***  <br>  <br>
   Mar 26, Yale Uni and NYU published a [paper](https://arxiv.org/pdf/2503.20757) “MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search”. The study introduces MCTS-RAG, a novel approach that enhances the reasoning capabilities of small language models on knowledge-intensive tasks by leveraging retrieval-augmented generation (RAG) to provide relevant context and Monte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically integrates retrieval and reasoning through an iterative decision-making process. Unlike standard RAG methods, which typically retrieve information independently from reasoning and thus integrate knowledge suboptimally, or conventional MCTS reasoning, which depends solely on internal model knowledge without external facts, MCTS-RAG combines structured reasoning with adaptive retrieval. This integrated approach enhances decision-making, reduces hallucinations, and ensures improved factual accuracy and response consistency. The experimental results on multiple reasoning and knowledge-intensive datasets datasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that the method enables small-scale LMs to achieve performance comparable to frontier LLMs like GPT-4o by effectively scaling inference-time compute, setting a new standard for reasoning in small-scale models. https://github.com/yale-nlp/MCTS-RAG  <br>  <br>

5. ***Open Deep Search Democratizes Search with Open-Source Reasoning Agents.   <br>Sentient, the University of Washington, Princeton University, and UC Berkeley have presented Open Deep Search (ODS), a framework aiming to bridge the gap between proprietary and open-source search AI solutions. ODS augments open-source LLMs with reasoning agents that can strategically use web search tools. It comprises the Open Search Tool, a novel web search tool outperforming proprietary alternatives, and the Open Reasoning Agent, which orchestrates actions using this tool, enabling open-source LLMs to achieve near state-of-the-art performance on question-answering benchmarks.***  <br>  <br>
   Mar 26, Sentient, Uni of Washington, Princeton Uni and UC Berkeley published a [paper](https://arxiv.org/pdf/2503.20201) “Open Deep Search: Democratizing Search with Open-source Reasoning Agents”. The study introduces Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and OpenAI's GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with a base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating a sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is a novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is a general framework for seamlessly augmenting any LLMs -- for example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES. https://github.com/sentient-agi/OpenDeepSearch  <br>  <br>

7. ***Entropy-Guided Reward Aggregation (ENCORE) Improves LLM Safety Alignment.   <br>Researchers from Harvard University, NYU, UCLA, and MIT have found that safety rules with high rating entropy are less reliable in identifying preferred LLM responses. Leveraging this, they introduce ENCORE, a training-free approach that improves the alignment of LLMs with safety guidelines by downweighting reward rules exhibiting high entropy during multi-head reward aggregation. Theoretical analysis supports this entropy-based penalization, and experiments on safety tasks demonstrate that ENCORE significantly outperforms various competitive baselines while maintaining interpretability.***  <br>  <br>
   Mar 26, Harvard Uni, NYU, UCLA and MIT published a [paper](https://arxiv.org/pdf/2503.20995) “Multi-head Reward Aggregation Guided by Entropy”. Aligning large language models (LLMs) with safety guidelines typically involves reinforcement learning from human feedback (RLHF), relying on human-generated preference annotations. However, assigning consistent overall quality ratings is challenging, prompting recent research to shift towards detailed evaluations based on multiple specific safety criteria. This paper uncovers a consistent observation: safety rules characterized by high rating entropy are generally less reliable in identifying responses preferred by humans. Leveraging this finding, the study introduces ENCORE, a straightforward entropy-guided approach that composes multi-head rewards by downweighting rules exhibiting high rating entropy. Theoretically, the study demonstrates that rules with elevated entropy naturally receive minimal weighting in the Bradley-Terry optimization framework, justifying the entropy-based penalization. Through extensive experiments on RewardBench safety tasks, the method significantly surpasses several competitive baselines, including random weighting, uniform weighting, single-head Bradley-Terry models, and LLM-based judging methods. The proposed approach is training-free, broadly applicable to various datasets, and maintains interpretability, offering a practical and effective solution for multi-attribute reward modeling.  <br>  <br>

9. ***Google Releases Gemini 2.5 Pro Experimental with Advanced Reasoning and a Million-Token Context.   <br>Google has launched Gemini 2.5 Pro Experimental, the first release of their latest AI model, Gemini 2.5. This model excels in complex problem-solving with advanced reasoning and coding capabilities, currently ranking #1 on the LMArena benchmark. Gemini 2.5 builds upon techniques like reinforcement learning and chain-of-thought prompting, featuring a 1 million token context window, multimodality, and strong performance across coding, math, and science benchmarks. It is now available in Google AI Studio and the Gemini app.***  <br>  <br>
    Mar 25, Goole [released Gemini 2.5](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/). Gemini 2.5 is Google's latest AI model, designed to handle complex problems with advanced reasoning and coding capabilities. The first release, Gemini 2.5 Pro Experimental, leads benchmarks and ranks #1 on LMArena. These "thinking models" analyze information, draw logical conclusions, and make informed decisions, enhancing performance and accuracy. Building on techniques like reinforcement learning and chain-of-thought prompting, Gemini 2.5 combines an enhanced base model with improved post-training. The model excels in coding, math, and science benchmarks, and is available in Google AI Studio and the Gemini app, with Vertex AI support coming soon. Gemini 2.5 features a 1 million token context window, multimodality, and strong performance across various data types. Developers and enterprises can start experimenting with it now, with pricing details to be announced soon.  <br>  <br>

11. ***Language Models Can Verbatim Complete Text They Weren't Explicitly Trained On.   <br>A study by Google and Stanford has shown that large language models can sometimes complete text verbatim even if those specific sequences were not explicitly present in their training data according to n-gram overlap definitions. The authors demonstrate that this n-gram based definition of training data membership can be gamed, with completion tests succeeding even when target sequences were removed from the training set. This highlights the limitations of relying solely on n-gram overlap to define training data membership.***  <br>  <br>
    Mar 25, Google and Stanford published a [paper](https://arxiv.org/pdf/2503.17514) “Language Models May Verbatim Complete Text They Were Not Explicitly Trained On”. An important question today is whether a given text was used to train a large language model (LLM). A completion test is often employed: check if the LLM completes a sufficiently complex text. This, however, requires a ground-truth definition of membership; most commonly, it is defined as a member based on the n-gram overlap between the target text and any text in the dataset. This study demonstrates that this n-gram based membership definition can be effectively gamed. The authors study scenarios where sequences are non-members for a given n and we find that completion tests still succeed. The study finds many natural cases of this phenomenon by retraining LLMs from scratch after removing all training samples that were completed; these cases include exact duplicates, near-duplicates, and even short overlaps. They showcase that it is difficult to find a single viable choice of n for membership definitions. Using these insights, the work designs adversarial datasets that can cause a given target sequence to be completed without containing it, for any reasonable choice of n. The findings highlight the inadequacy of n-gram membership, suggesting membership definitions fail to account for auxiliary information available to the training algorithm.  <br>  <br>

13. ***Jensen's Lower Bound Enables Reinforcement Learning for Chain-of-Thought Optimization.   <br>Meta researchers have proposed a method to optimize chain-of-thought reasoning in language models using reinforcement learning without an external reward function. The algorithm treats chain-of-thought as a latent variable within a probabilistic inference framework and utilizes a simpler Jensen's lower bound instead of the full evidence lower bound. This approach yields tractable objectives with straightforward algorithmic components, making it suitable for large-scale training and naturally interpolating between supervised fine-tuning and online reinforcement learning, showing effectiveness in mathematical reasoning.***  <br>  <br>
    Mar 25, Meta published a [paper](https://arxiv.org/pdf/2503.19618) “Learning to chain-of-thought with Jensen's evidence lower bound”. The study proposes a way to optimize chain-of-thought with reinforcement learning, but without external reward function. The algorithm relies on viewing chain-of-thought as latent variable as part of a probabilistic inference problem. Contrary to the full evidence lower bound, the study proposes to apply a much simpler Jensen's lower bound, which derives tractable objectives with simple algorithmic components (e.g., without the need for parametric approximate posterior), making it more conducive to modern large-scale training. The lower bound approach naturally interpolates other methods such as supervised fine-tuning and online reinforcement learning, whose practical trade-offs will be illustrated. Finally, the study shows that on mathematical reasoning problems, optimizing with Jensen's lower bound is as effective as policy gradient with external reward. Taken together, the results showcase as a proof of concept to this new algorithmic paradigm's potential to more generic applications.  <br>  <br>

15. ***Vision-Language Models Still Struggle with Real-Time Face-to-Face Question Answering.   <br>A study by Qualcomm and the University of Toronto introduces the Qualcomm Interactive Video Dataset (IVD) to assess the ability of vision-language models to answer questions about live, unfolding scenes in real-time. The research reveals that current models significantly lag behind human performance on this task, identifying key areas for improvement. However, the study also indicates that fine-tuning on this type of interactive video data can substantially reduce the performance gap for many perceptual skills.***  <br>  <br>
    Mar 25, Qualcomm and Uni of Toronto published a [paper](Can Vision-Language Models Answer Face to Face Questions in the Real-World?) “Can Vision-Language Models Answer Face to Face Questions in the Real-World?”. AI models have made significant strides in recent years in their ability to describe and answer questions about real-world images. They have also made progress in the ability to converse with users in real-time using audio input. This raises the question: have people reached the point where AI models, connected to a camera and microphone, can converse with users in real-time about scenes and events that are unfolding live in front of the camera? This has been a long-standing goal in AI and is a prerequisite for real-world AI assistants and humanoid robots to interact with humans in everyday situations. This work introduces a new dataset and benchmark, the Qualcomm Interactive Video Dataset (IVD), which allows to assess the extent to which existing models can support these abilities, and to what degree these capabilities can be instilled through fine-tuning. The dataset is based on a simple question-answering setup, where users ask questions that the system has to answer, in real-time, based on the camera and audio input. The study shows that existing models fall far behind human performance on this task; and identifies the main sources for the performance gap. However, the work also shows that for many of the required perceptual skills, fine-tuning on this form of data can significantly reduce this gap.  <br>  <br>

17. ***Google's Gemma 3 Introduces Multimodality, Extended Context, and Architectural Improvements.   <br>Google has released the Gemma 3 Technical Report, detailing the multimodal addition to the Gemma family of open models, ranging from 1 to 27 billion parameters. Gemma 3 introduces vision understanding, broader language coverage (over 128K tokens), and a new architecture with an increased ratio of local to global attention layers to reduce KV-cache memory usage for long contexts. Trained with distillation, Gemma 3 models outperform Gemma 2, with significant improvements in math, chat, instruction-following, and multilingual abilities.***  <br>  <br> 
    Mar 25, Google published a [paper](https://arxiv.org/pdf/2503.19786) “Gemma 3 Technical Report”. The report introduces Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context - at least 128K tokens. The report also changes the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, the novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. Models are open to the community.  <br>  <br>

19. ***Reasoning to Learn from Latent Thoughts Improves Language Model Pretraining Efficiency.   <br>Researchers from Stanford University, the University of Toronto, and the Vector Institute propose that explicitly modeling and inferring latent thoughts underlying text generation can enhance the data efficiency of language model pretraining in data-constrained scenarios. Their approach views web text as a compressed outcome of human thought processes, with latent thoughts containing crucial contextual knowledge and reasoning steps. Empirical results in math demonstrate that synthetic data approaches for inferring latent thoughts significantly improve data efficiency, and a 1B LM can bootstrap its performance through iterative refinement of thought-augmented pretraining data.***  <br>  <br>
    Mar 24, Stanford Uni, Uni of Toronto and Vector Inst published a [paper](https://arxiv.org/pdf/2503.18866) “Reasoning to Learn from Latent Thoughts”. Compute scaling for language model (LM) pretraining has outpaced the growth of human-written texts, leading to concerns that data will become the bottleneck to LM scaling. To continue scaling pretraining in this data-constrained regime, we propose that explicitly modeling and inferring the latent thoughts that underlie the text generation process can significantly improve pretraining data efficiency. Intuitively, our approach views web text as the compressed final outcome of a verbose human thought process and that the latent thoughts contain important contextual knowledge and reasoning steps that are critical to data-efficient learning. We empirically demonstrate the effectiveness of our approach through data-constrained continued pretraining for math. We first show that synthetic data approaches to inferring latent thoughts significantly improve data efficiency, outperforming training on the same amount of raw data (5.7% → 25.4% on MATH). Furthermore, we demonstrate latent thought inference without a strong teacher, where an LM bootstraps its own performance by using an EM algorithm to iteratively improve the capability of the trained LM and the quality of thought-augmented pretraining data. We show that a 1B LM can bootstrap its performance across at least three iterations and significantly outperform baselines trained on raw data, with increasing gains from additional inference compute when performing the E-step. The gains from inference scaling and EM iterations suggest new opportunities for scaling data-constrained pretraining.  <br>  <br>

21. ***SimpleRL-Zoo Investigates Zero Reinforcement Learning for Diverse Open Base Models.   <br>HKUST, TikTok, and BUPT have explored zero reinforcement learning training, where long chain-of-thought reasoning emerges directly from base language models using rule-based rewards, across ten diverse open base models. The study identifies key design strategies for achieving substantial improvements in reasoning accuracy and response length. Notably, they observed the "aha moment" in small models outside the Qwen family, providing valuable insights and open-sourcing their code, models, and analysis tools.***  <br>  <br>
    Mar 24, HKUST, TikTok and BUPT published a [paper](https://arxiv.org/pdf/2503.18892) “SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild”. DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as people find the base models already exhibit strong instruction-following and self-reflection abilities. This study investigates zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty - the work achieves substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, the study observes that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the "aha moment"). Notably, the work observes the "aha moment" for the first time in small models not from the Qwen family. The study shares the key designs that enable successful zero RL training, along with the findings and practices. To facilitate further research, the authors open-source the code, models, and analysis tools at https://github.com/hkust-nlp/simpleRL-reason  <br>  <br>

23. ***FFN Fusion Optimizes LLM Inference by Parallelizing Feed-Forward Network Layers.   <br>Nvidia has introduced FFN Fusion, an architectural optimization technique that reduces sequential computation in large language models by parallelizing sequences of Feed-Forward Network (FFN) layers, particularly after removing specific attention layers. Applying this to Llama-3.1-405B-Instruct resulted in Llama-Nemotron-Ultra-253B-Base, an efficient model achieving a 1.71x speedup in inference latency and significantly lower per-token cost while maintaining strong benchmark performance.***  <br>  <br>
    Mar 24, Nvidia published a [paper](https://arxiv.org/pdf/2503.18908) “FFN Fusion: Rethinking Sequential Computation in Large Language Models”. The study introduces FFN Fusion, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. The key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining after the removal of specific attention layers, can often be parallelized with minimal accuracy impact. The study develops a principled methodology for identifying and fusing such sequences, transforming them into parallel operations that significantly reduce inference latency while preserving model behavior. Applying these techniques to Llama-3.1-405B-Instruct, the study creates Llama-Nemotron-Ultra-253B-Base (Ultra-253B-Base), an efficient and soon-to-be publicly available model that achieves a 1.71X speedup in inference latency and 35X lower per-token cost while maintaining strong performance across benchmarks. Through extensive experiments on models from 49B to 253B parameters, the work demonstrates that FFN Fusion becomes increasingly effective at larger scales and can complement existing optimization techniques like quantization and pruning. Most intriguingly, the work finds that even full transformer blocks containing both attention and FFN layers can sometimes be parallelized, suggesting new directions for neural architecture design.  <br>  <br>

25. ***xKV: Cross-Layer SVD for Efficient KV-Cache Compression in Long-Context LLMs.   <br>Researchers from Cornell University, the University of Washington, and NYMCT University have proposed xKV, a post-training method for compressing the KV-Cache in large language models with long context windows. xKV applies Singular Value Decomposition (SVD) on the KV-Cache of grouped layers, consolidating it into a shared low-rank subspace. Evaluations on long-context benchmarks show xKV achieving higher compression rates with improved accuracy compared to existing inter-layer techniques and demonstrating compatibility with Multi-Head Latent Attention.***  <br>  <br>
    Mar 24, Cornell Uni, Uni of Washington and NYMCT Uni published a [paper](https://arxiv.org/pdf/2503.18893) “xKV: Cross-Layer SVD for KV-Cache Compression”. Large Language Models (LLMs) with long context windows enable powerful applications but come at the cost of high memory consumption to store the Key and Value states (KV-Cache). Recent studies attempted to merge KV-cache from multiple layers into shared representations, yet these approaches either require expensive pretraining or rely on assumptions of high per-token cosine similarity across layers which generally does not hold in practice. The work finds that the dominant singular vectors are remarkably well-aligned across multiple layers of the KV-Cache. Exploiting this insight, the study proposes xKV, a simple post-training method that applies Singular Value Decomposition (SVD) on the KV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers into a shared low-rank subspace, significantly reducing KV-Cache sizes. Through extensive evaluations on the RULER long-context benchmark with widely-used LLMs (e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates than state-of-the-art inter-layer technique while improving accuracy by 2.7%. Moreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA) (e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding tasks without performance degradation. These results highlight xKV's strong capability and versatility in addressing memory bottlenecks for long-context LLM inference. Code is publicly available at: https://github.com/abdelfattah-lab/xKV.  <br>  <br>

27. ***AgentRxiv: A Framework for Collaborative Autonomous Research with LLM Agents.   <br>Johns Hopkins University and ETH have introduced AgentRxiv, a framework enabling LLM agent laboratories to collaborate on research by uploading and retrieving reports from a shared preprint server. Experiments show that agents with access to their prior research perform better, and multiple agent laboratories sharing research through AgentRxiv achieve higher overall accuracy, suggesting a potential role for autonomous agents in future AI system design.***  <br>  <br>
    Mar 23, Johns Hopkins Uni and ETH published a [paper](https://arxiv.org/pdf/2503.18102) “AgentRxiv: Towards Collaborative Autonomous Research”. Progress in scientific discovery is rarely the result of a single "Eureka" moment, but is rather the product of hundreds of scientists incrementally working together toward a common goal. While existing agent workflows are capable of producing research autonomously, they do so in isolation, without the ability to continuously improve upon prior research results. To address these challenges, the study introduces AgentRxiv-a framework that lets LLM agent laboratories upload and retrieve reports from a shared preprint server in order to collaborate, share insights, and iteratively build on each other's research. The work tasks agent laboratories to develop new reasoning and prompting techniques and find that agents with access to their prior research achieve higher performance improvements compared to agents operating in isolation (11.4% relative improvement over baseline on MATH-500). The study finds that the best performing strategy generalizes to benchmarks in other domains (improving on average by 3.3%). Multiple agent laboratories sharing research through AgentRxiv are able to work together towards a common goal, progressing more rapidly than isolated laboratories, achieving higher overall accuracy (13.7% relative improvement over baseline on MATH-500). These findings suggest that autonomous agents may play a role in designing future AI systems alongside humans. The authors hope that AgentRxiv allows agents to collaborate toward research goals and enables researchers to accelerate discovery. https://github.com/SamuelSchmidgall/AgentLaboratory  <br>  <br>

29. ***Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models.   <br>MIT and Google researchers have found that large language models do not update their beliefs according to Bayesian principles. To address this, they propose Bayesian Teaching, training LLMs to mimic the predictions of an optimal Bayesian model. This approach significantly improves performance on recommendation tasks and enables generalization to other tasks, suggesting that LLMs can learn and generalize reasoning strategies effectively.***  <br>  <br>
    Mar 21, MIT and Google published a [paper](https://arxiv.org/pdf/2503.17523) “Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models”. Artificial intelligence systems based on large language models (LLMs) are increasingly used as agents that interact with users and with the world. To do so successfully, LLMs need to construct internal representations of the world and form probabilistic beliefs about those representations. To provide a user with personalized recommendations, for example, the LLM needs to gradually infer the user's preferences, over the course of multiple interactions. To evaluate whether contemporary LLMs are able to do so, the study uses the Bayesian inference framework from probability theory, which lays out the optimal way to update an agent's beliefs as it receives new information. The study first shows that the LLMs do not update their beliefs as expected from the Bayesian framework, and that consequently their predictions do not improve as expected as more information becomes available, even less so than the authors find is the case for humans. To address this issue, the authors teaches the LLMs to reason in a Bayesian manner by training them to mimic the predictions of an optimal Bayesian model. The study finds that this approach not only significantly improves the LLM's performance on the particular recommendation task it is trained on, but also enables generalization to other tasks. This suggests that this method endows the LLM with broader Bayesian reasoning skills. More generally, the results indicate that LLMs can learn about reasoning strategies effectively and generalize those skills to new domains, which in part explains LLMs' empirical success.  <br>  <br>

31. ***Reward Features Enable Capturing Individual Human Preferences in LLM Training.   <br>Google researchers argue that standard reinforcement learning from human feedback models preferences without considering individual differences. They propose a method to specialize reward models to specific individuals or groups by capturing preferences as a linear combination of general reward features. Experiments with large language models show that this approach either significantly outperforms non-adaptive and other adaptive baselines or matches their performance with a simpler and more stable architecture, especially in scenarios with high disagreement.***  <br>  <br>
    Mar 21, Google published a [paper](https://www.arxiv.org/pdf/2503.17338) “Capturing Individual Human Preferences with Reward Features”. Reinforcement learning from human feedback usually models preferences using a reward model that does not distinguish between people. The study argues that this is unlikely to be a good design choice in contexts with high potential for disagreement, like in the training of large language models. The work proposes a method to specialise a reward model to a person or group of people. The approach builds on the observation that individual preferences can be captured as a linear combination of a set of general reward features. The study shows how to learn such features and subsequently use them to quickly adapt the reward model to a specific individual, even if their preferences are not reflected in the training data. The authors present experiments with large language models comparing the proposed architecture with a non-adaptive reward model and also adaptive counterparts, including models that do in-context personalisation. Depending on how much disagreement there is in the training data, the model either significantly outperforms the baselines or matches their performance with a simpler architecture and more stable training.  <br>  <br>

33. ***Curriculum Extraction from a Fully Trained Teacher Enables Efficient Knowledge Distillation.   <br>Researchers from the University of Texas at Austin and Microsoft have shown that a curriculum for efficient knowledge distillation can be extracted from just the fully trained teacher network, offering similar benefits to progressive distillation without needing to store intermediate checkpoints. Their method uses a random projection of the teacher's hidden representations to progressively train the student network before using the full network output, outperforming one-shot distillation and achieving comparable performance to progressive distillation.***  <br>  <br>
    Mar 21, Uni of Texas at Austin and Microsoft published a [paper](https://www.arxiv.org/pdf/2503.17494) “Efficient Knowledge Distillation via Curriculum Extraction”. Knowledge distillation is a technique used to train a small student network using the output generated by a large teacher network, and has many empirical advantages. While the standard one-shot approach to distillation only uses the output of the final teacher network, recent work has shown that using intermediate checkpoints from the teacher's training process as an implicit “curriculum” for progressive distillation can significantly speed up training. However, such schemes require storing these checkpoints, and often require careful selection of the intermediate checkpoints to train on, which can be impractical for large-scale training. This study shows that a curriculum can be extracted from just the fully trained teacher network, and that this extracted curriculum can give similar efficiency benefits to those of progressive distillation. The extraction scheme is natural; the authors use a random projection of the hidden representations of the teacher network to progressively train the student network, before training using the output of the full network. The study shows that the scheme significantly outperforms one-shot distillation and achieves a performance similar to that of progressive distillation for learning sparse parities with two-layer networks, and provide theoretical guarantees for this setting. Additionally, the study shows that the method outperforms one-shot distillation even when using transformer-based architectures, both for sparse-parity learning, and language modeling tasks.  <br>  <br>

35. ***Weight Rescaling Techniques Improve Variance Control in LLM Pre-training.   <br>BluOrion has introduced Layer Index Rescaling (LIR) and Target Variance Rescaling (TVR), novel weight initialization and variance control strategies for large language model pre-training. Experiments on a 1B parameter LLaMA model demonstrate that better variance management using these techniques leads to substantial improvements in downstream task performance and reduces extreme activation values, mitigating challenges related to quantization and low-precision training.***  <br>  <br>
    Mar 21, BluOrion published a [paper](https://arxiv.org/pdf/2503.17500) “Variance Control via Weight Rescaling in LLM Pre-training”. The outcome of Large Language Model (LLM) pre-training strongly depends on weight initialization and variance control strategies. Although the importance of initial variance control has been well documented in neural networks in general, the literature on initialization and management of its growth during LLM pre-training, specifically, is somewhat sparse. This work introduces the Layer Index Rescaling (LIR) weight initialization scheme, and the Target Variance Rescaling (TVR) variance control strategy. Experiments on a 1B parameter LLaMA model demonstrate that better variance management using these techniques yields substantial improvements in downstream task performance (up to 4.6% on common pre-training benchmarks) and reduces extreme activation values, thus mitigating challenges associated with quantization and low-precision training. Code is available at: https://github.com/bluorion-com/weight_rescaling.  <br>  <br>

37. ***The KoLMogorov Test: Compression by Code Generation as an Intelligence Benchmark.   <br>Meta and Tel Aviv University have introduced the KoLMogorov-Test (KT), a compression-as-intelligence test for code-generating LLMs. KT challenges models to generate the shortest program that outputs a given sequence of data. Evaluation using audio, text, DNA, and synthetic program outputs reveals that current flagship models perform poorly, suggesting that new innovations are needed to better approximate Kolmogorov compression.***  <br>  <br>
    Mar 18, Meta and Tel Aviv Uni published a [paper](https://arxiv.org/pdf/2503.13992v1) “The KoLMogorov Test: Compression by Code Generation”. Compression is at the heart of intelligence. A theoretically optimal way to compress any sequence of data is to find the shortest program that outputs that sequence and then halts. However, such Kolmogorov compression is uncomputable, and code generating LLMs struggle to approximate this theoretical ideal, as it requires reasoning, planning and search capabilities beyond those of current models. In this work, we introduce the KoLMogorov-Test (KT), a compression-as-intelligence test for code generating LLMs. In KT a model is presented with a sequence of data at inference time, and asked to generate the shortest program that produces the sequence. The study identifies several benefits of KT for both evaluation and training: an essentially infinite number of problem instances of varying difficulty is readily available, strong baselines already exist, the evaluation metric (compression) cannot be gamed, and pretraining data contamination is highly unlikely. To evaluate current models, the study uses audio, text, and DNA data, as well as sequences produced by random synthetic programs. Current flagship models perform poorly – both GPT4-o and Llama-3.1-405B struggle on the natural and synthetic sequences. On the synthetic distribution, the authors are able to train code generation models with lower compression rates than previous approaches. Moreover, the study shows that gains on synthetic data generalize poorly to real data, suggesting that new innovations are necessary for additional gains on KT.  <br>  <br>

39. ***A Multi-Modal Multi-Agent Framework for Enhanced Document Understanding.   <br>Researchers from UNC-Chapel Hill and Adobe have presented MDocAgent, a novel retrieval-augmented generation and multi-agent framework for Document Question Answering (DocQA). MDocAgent integrates both textual and visual cues from documents using five specialized agents that collaborate to achieve a more comprehensive understanding, leading to improved accuracy on multi-modal document understanding benchmarks.***  <br>  <br>
    Mar 18, UNC-Chapel Hill and Adobe published a [paper](https://arxiv.org/pdf/2503.13964) “MDocAgent: A Multi-Modal Multi-Agent Framework for Document Understanding”. Document Question Answering (DocQA) is a very common task. Existing methods using Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and Retrieval Augmented Generation (RAG) often prioritize information from a single modal, failing to effectively integrate textual and visual cues. These approaches struggle with complex multi-modal reasoning, limiting their performance on real-world documents. The study presents MDocAgent (A Multi-Modal Multi-Agent Framework for Document Understanding), a novel RAG and multi-agent framework that leverages both text and image. The system employs five specialized agents: a general agent, a critical agent, a text agent, an image agent and a summarizing agent. These agents engage in multi-modal context retrieval, combining their individual insights to achieve a more comprehensive understanding of the document's content. This collaborative approach enables the system to synthesize information from both textual and visual components, leading to improved accuracy in question answering. Preliminary experiments on five benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of MDocAgent, achieve an average improvement of 12.1% compared to current state-of-the-art method. This work contributes to the development of more robust and comprehensive DocQA systems capable of handling the complexities of real-world documents containing rich textual and visual information. https://github.com/aiming-lab/mdocagent













































