***Dec 8th***

1. ***Release of OpenAI o1 Model Series <br>
OpenAI introduced the o1 model series, leveraging reinforcement learning and chain-of-thought reasoning for enhanced safety and robustness. The models achieve top performance on safety-related benchmarks, such as avoiding unsafe advice and stereotypes, while reducing errors by 34% compared to preview versions. Designed for complex problem-solving and multimedia understanding, the release emphasizes the importance of alignment, stress-testing, and risk management.*** <br> <br>
   Dec 6, OpenAI released its o1 and o1’s [System Card](https://cdn.openai.com/o1-system-card-20241205.pdf). The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of the models. In particular, the models can reason about the safety policies in context when responding to potentially unsafe prompts. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. The results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations. Other features not listed in the report include: research scientists on the livestream said an internal evaluation indicated it made major mistakes about 34% less often than the o1 preview mode; the model seems geared toward scientists, engineers, and coders, is designed to solve thorny problems; read and understand multi-media inputs such a photo of a hand-drawn system for a data center in space, and answer tough questions to a layperson. <br> <br>

3. ***Meta’s Llama 3.3 Launch <br>
Meta released Llama 3.3, a compact yet powerful open-source model with 70B parameters, rivaling larger models in performance at a fraction of the cost. Licensed under a community agreement, it ensures responsible use and attribution. Outperforming prior models in NLP benchmarks like multilingual dialogue and reasoning, Llama 3.3 offers accessible high-quality AI with reduced computational requirements.*** <br> <br>
   Dec 6, according to [venturebeat](https://venturebeat.com/ai/meta-launches-open-source-llama-3-3-shrinking-powerful-bigger-model-into-smaller-size/), Meta launches open source Llama 3.3, shrinking powerful bigger model into smaller size. With 70 billion parameters — or settings governing the model’s behavior — Llama 3.3 delivers results on par with Meta’s 405B parameter model from the Llama 3.1 from the summer, but at a fraction of the cost and computational overhead — e.g., the GPU capacity needed to run the model in an inference. It’s designed to offer top-tier performance and accessibility yet in a smaller package than prior foundation models. Meta’s Llama 3.3 is offered under the Llama 3.3 Community License Agreement, which grants a non-exclusive, royalty-free license for use, reproduction, distribution, and modification of the model and its outputs. Developers integrating Llama 3.3 into products or services must include appropriate attribution, such as “Built with Llama,” and adhere to an Acceptable Use Policy that prohibits activities like generating harmful content, violating laws, or enabling cyberattacks. According to Meta AI on X, the Llama 3.3 model handedly outperforms the identically sized Llama 3.1-70B as well as Amazon’s new Nova Pro model in several benchmarks such as multilingual dialogue, reasoning, and other advanced natural language processing (NLP) tasks (Nova outperforms it in HumanEval coding tasks). <br> <br>

5. ***Nvidia's NVILA for Efficient Visual Language Models (VLMs) <br>
Nvidia unveiled NVILA, an open VLM series focusing on efficiency and accuracy. By scaling and compressing visual data, NVILA processes high-res images and long videos efficiently, reducing training costs and latency. It matches or outperforms existing VLMs while significantly lowering resource consumption, facilitating advancements in image and video analysis.*** <br> <br>
   Dec 5, Nvidia published a [paper](https://arxiv.org/pdf/2412.04468) “NVILA: Efficient Frontier Visual Language Models”. Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and accuracy. Building on top of VILA, the study improves its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens. This "scale-then-compress" approach enables NVILA to efficiently process high-resolution images and long videos. The authors also conduct a systematic investigation to enhance the efficiency of NVILA throughout its entire lifecycle, from training and fine-tuning to deployment. NVILA matches or surpasses the accuracy of many leading open and proprietary VLMs across a wide range of image and video benchmarks. At the same time, it reduces training costs by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by 1.6-2.2X, and decoding latency by 1.2-2.8X. [Code](https://github.com/NVlabs/VILA) and models are available to facilitate reproducibility. <br> <br>

7. ***Task Scaling Laws from Model Ladders <br>
A new study introduces task scaling laws using compute-efficient model ladders to predict language model task performance. By training smaller "ladder" models, researchers forecast larger model accuracy with minimal computational cost. While predictions are accurate on low-variance tasks, challenges remain for high-variance tasks. The approach underscores the value of efficient, predictive methods in AI scaling.*** <br> <br>
   Dec 5, Aillen Inst of AI, Uni of Washington and Princeton Uni published a [paper](https://arxiv.org/pdf/2412.04403) “Establishing Task Scaling Laws via Compute-Efficient Model Ladders”. The study develops task scaling laws and model ladders to predict the individual task performance of pretrained language models (LMs) in the overtrained setting. Standard power laws for language modeling loss cannot accurately model task performance. Therefore, the work leverages a two-step prediction approach: first use model and data size to predict a task-specific loss, and then use this task loss to predict task performance. The study trains a set of small-scale "ladder" models, collect data points to fit the parameterized functions of the two prediction steps, and make predictions for two target models: a 7B model trained to 4T tokens and a 13B model trained to 5T tokens. Training the ladder models only costs 1% of the compute used for the target models. On four multiple-choice tasks written in ranked classification format, the study can predict the accuracy of both target models within 2 points of absolute error. The authors have higher prediction error on four other tasks (average absolute error 6.9) and find that these are often tasks with higher variance in task metrics. The study also finds that using less compute to train fewer ladder models tends to deteriorate predictions. Finally, the research empirically shows that the design choices and the two-step approach lead to superior performance in establishing scaling laws.  <br> <br>

9. ***Challenges in Human Evaluations of Chatbots <br>
A Cornell study highlights flaws in open human evaluation platforms like Chatbot Arena. Bad annotations, either from apathetic or adversarial users, can significantly distort model rankings. Ensuring reliable annotations requires robust guardrails, as even minor issues can misrepresent model capabilities and hinder trustworthiness.*** <br> <br>
    Dec 5, Cornell Uni published a [paper](https://arxiv.org/pdf/2412.04363) “Challenges in Trustworthy Human Evaluation of Chatbots”. Open community-driven platforms like Chatbot Arena that collect user preference data from site visitors have gained a reputation as one of the most trustworthy publicly available benchmarks for LLM performance. While now standard, it is tricky to implement effective guardrails to collect high-quality annotations from humans. This work demonstrates that three sources of bad annotations, both malicious and otherwise, can corrupt the reliability of open leaderboard rankings. In particular, the work shows that only 10% of poor quality votes by apathetic (site visitors not appropriately incentivized to give correct votes) or adversarial (bad actors seeking to inflate the ranking of a target model) annotators can change the rankings of models by up to 5 places on the leaderboard. Finally, the paper discuss open challenges in ensuring high-quality human annotations. <br> <br>

11. ***Google’s GenCast for Probabilistic Weather Forecasting <br>
Google's GenCast introduces a breakthrough in ML-based probabilistic weather forecasting, outperforming traditional models like ENS in skill and speed. Leveraging decades of data, it produces global forecasts for 80+ variables within minutes, excelling in predicting extreme weather and renewable energy planning. This represents a leap forward in operational weather prediction.*** <br> <br>
    Dec 4, Nature published a [paper](https://www.nature.com/articles/s41586-024-08252-9) from Google “Probabilistic weather forecasting with machine learning”. Weather forecasts are fundamentally uncertain, so predicting the range of probable weather scenarios is crucial for important decisions, from warning the public about hazardous weather to planning renewable energy use. Traditionally, weather forecasts have been based on numerical weather prediction (NWP), which relies on physics-based simulations of the atmosphere. Recent advances in machine learning (ML)-based weather prediction (MLWP) have produced ML-based models with less forecast error than single NWP simulations. However, these advances have focused primarily on single, deterministic forecasts that fail to represent uncertainty and estimate risk. Overall, MLWP has remained less accurate and reliable than state-of-the-art NWP ensemble forecasts. Here the study introduces GenCast, a probabilistic weather model with greater skill and speed than the top operational medium-range weather forecast in the world, ENS, the ensemble forecast of the European Centre for Medium-Range Weather Forecasts. GenCast is an ML weather prediction method, trained on decades of reanalysis data. GenCast generates an ensemble of stochastic 15-day global forecasts, at 12-h steps and 0.25° latitude–longitude resolution, for more than 80 surface and atmospheric variables, in 8 min. It has greater skill than ENS on 97.2% of 1,320 targets evaluated and better predicts extreme weather, tropical cyclone tracks and wind power production. This work helps open the next chapter in operational weather forecasting, in which crucial weather-dependent decisions are made more accurately and efficiently. <br> <br>

13. ***PaliGemma 2 Vision-Language Model Upgrade <br>
Google enhanced the PaliGemma model with PaliGemma 2, combining versatile vision encoders and scalable language models across resolutions. Designed for diverse tasks, including OCR and radiography reports, PaliGemma 2 achieves state-of-the-art results in transfer learning, showcasing the interplay between task types, model size, and resolution in performance optimization.*** <br> <br>
    Dec 4, Google published a [paper](https://arxiv.org/pdf/2412.03555) “PaliGemma 2: A Family of Versatile VLMs for Transfer”. PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. The work combines the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. The study trains these models at three resolutions (224px2 , 448px2 and 896px2) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. The work further increases the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results. <br> <br>

15. ***Evaluating LMs as Synthetic Data Generators <br>
A study proposes AgoraBench to evaluate LMs’ synthetic data generation, finding distinct strengths like problem generation (GPT-4o) and enhancement (Claude-3.5). Key insights show data quality, not problem-solving ability, dictates generation effectiveness. Strategic formats and cost-efficient models optimize synthetic data for downstream AI applications.*** <br> <br>
    Dec 4, CMU, Uni of Washington, NEC et al. published a [paper](https://arxiv.org/pdf/2412.03679) “Evaluating Language Models as Synthetic Data Generators”. Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic comparison of different LMs as data generators in a unified setting. To address this gap, the study proposes AgoraBench, a benchmark that provides standardized settings and metrics to evaluate LMs' data generation abilities. Through synthesizing 1.26 million training instances using 6 LMs and training 99 student models, the study uncovers key insights about LMs' data generation capabilities. First, LMs exhibit distinct strengths. For instance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnet performs better at enhancing existing ones. Furthermore, the analysis reveals that an LM's data generation ability doesn't necessarily correlate with its problem-solving ability. Instead, multiple intrinsic features of data quality-including response quality, perplexity, and instruction difficulty-collectively serve as better indicators. Finally, the work demonstrates that strategic choices in output format and cost-conscious model selection significantly impact data generation effectiveness. <br> <br>

17. ***Best-of-N Jailbreaking Algorithm <br>
Researchers introduce Best-of-N (BoN) Jailbreaking, which exploits prompt variations to bypass AI safeguards across modalities. Achieving high attack success rates, BoN demonstrates vulnerabilities in both proprietary and open-source defenses. The method’s scalability highlights the persistent challenge of ensuring AI robustness against adversarial exploitation.*** <br> <br>
    Dec 4, Speechmatics, MATS, UCL, Stanford Uni, Uni of Oxford, Tangentic and Anthropic published a [paper](https://arxiv.org/pdf/2412.03556) “Best-of-N Jailbreaking”. The study introduces Best-of-N (BoN) Jailbreaking, a simple black-box algorithm that jailbreaks frontier AI systems across modalities. BoN Jailbreaking works by repeatedly sampling variations of a prompt with a combination of augmentations - such as random shuffling or capitalization for textual prompts - until a harmful response is elicited. The work finds that BoN Jailbreaking achieves high attack success rates (ASRs) on closed-source language models, such as 89% on GPT-4o and 78% on Claude 3.5 Sonnet when sampling 10,000 augmented prompts. Further, it is similarly effective at circumventing state-of-the-art open-source defenses like circuit breakers. BoN also seamlessly extends to other modalities: it jailbreaks vision language models (VLMs) such as GPT-4o and audio language models (ALMs) like Gemini 1.5 Pro, using modality-specific augmentations. BoN reliably improves when sampled more augmented prompts. Across all modalities, ASR, as a function of the number of samples (N), empirically follows power-law-like behavior for many orders of magnitude. BoN Jailbreaking can also be composed with other black-box algorithms for even more effective attacks - combining BoN with an optimized prefix attack achieves up to a 35% increase in ASR. Overall, the work indicates that, despite their capability, language models are sensitive to seemingly innocuous changes to inputs, which attackers can exploit across modalities. <br> <br>

19. ***The Path to Artificial General Intelligence (AGI) <br>
An article discusses the limitations of LLMs in achieving AGI, citing challenges like data dependency, poor generalization, and lack of feedback mechanisms. Progressing toward AGI requires innovative architectures and careful ethical considerations to manage risks and prioritize societal well-being.*** <br> <br>
    Dec 3, Nature published an [article](https://www.nature.com/articles/d41586-024-03905-1) “How close is AI to human-level intelligence”. The recent advancements in large language models (LLMs) like OpenAI's o1 have reignited the debate about artificial general intelligence (AGI), an AI system capable of human-level cognition. While LLMs have shown remarkable capabilities in various tasks, they are not sufficient to achieve AGI on their own. The limitations of LLMs include their reliance on vast amounts of data, their inability to generalize effectively, and their lack of internal feedback mechanisms. To progress towards AGI, researchers are exploring new architectures and training techniques, such as generative flow networks and world model construction. However, the development of AGI raises significant ethical concerns. It is crucial to ensure that AI systems are developed responsibly and that their potential risks are mitigated. Researchers and policymakers must work together to establish guidelines and regulations for AI development, prioritizing safety and societal well-being. <br> <br>

21. ***Small Language Models (SLMs) for Businesses <br>
Forbes advocates for SLMs over LLMs for business use, emphasizing their efficiency, cost-effectiveness, and domain-specific advantages. SLMs secure data, reduce resource consumption, and offer tailored solutions, highlighting the importance of selecting the right AI for optimizing business operations and fostering trust in AI systems.*** <br> <br>
    Dec 3, Forbes published an [article](https://www.forbes.com/sites/deandebiase/2024/11/25/why-small-language-models-are-the-next-big-thing-in-ai/) “Why Small Language Models Are The Next Big Thing In AI”. The article argues that while large language models (LLMs) from tech giants like Microsoft, Google, and Amazon are powerful, they may not be the best fit for every business due to their high costs and resource demands. Instead, small language models (SLMs) and domain-specific LLMs offer more tailored, efficient, and cost-effective solutions. SLMs are trained on specific data types, keeping data secure within a company's firewall and reducing energy consumption. Domain-specific LLMs focus on specialized knowledge, providing more accurate and contextually relevant responses. These models require less computing power, can run on-premises, and offer greater control over data. The article highlights the importance of choosing the right AI model for specific business needs to optimize efficiency and reduce costs, emphasizing that trusted AI and data are crucial for future business solutions. <br> <br>

23. ***Multi-Agent LLM Training (MALT) <br>
A study introduces MALT, a collaborative AI training approach where multiple specialized LLMs jointly solve reasoning tasks. Employing sequential roles like generator and verifier, MALT improves accuracy in math and reasoning benchmarks, paving the way for multi-agent AI systems with cooperative problem-solving capabilities.*** <br> <br>
    Dec 2, Uni of Oxford, Cooperative AI Foundation, MBZUI and UC Berkeley published a [paper](https://arxiv.org/pdf/2412.01928) “MALT: Improving Reasoning with Multi-Agent LLM Training”. Enabling effective collaboration among LLMs is a crucial step toward developing autonomous systems capable of solving complex problems. While LLMs are typically used as single-model generators, where humans critique and refine their outputs, the potential for jointly-trained collaborative models remains largely unexplored. Despite promising results in multi-agent communication and debate settings, little progress has been made in training models to work together on tasks. This study presents a first step toward "Multi-agent LLM training" (MALT) on reasoning problems. The proposed approach employs a sequential multi-agent setup with heterogeneous LLMs assigned specialized roles: a generator, verifier, and refinement model iteratively solving problems. The study proposes a trajectory-expansion-based synthetic data generation process and a credit assignment strategy driven by joint outcome based rewards. This enables the post-training setup to utilize both positive and negative trajectories to autonomously improve each model's specialized capabilities as part of a joint sequential system. The study evaluates the approach across MATH, GSM8k, and CQA, where MALT on Llama 3.1 8B models achieves relative improvements of 14.14%, 7.12%, and 9.40% respectively over the same baseline model. This demonstrates an early advance in multi-agent cooperative capabilities for performance on mathematical and common sense reasoning questions. More generally, the work provides a concrete direction for research around multi-agent LLM training approaches. <br> <br>

25. ***Reverse-Enhanced Thinking in LLMs <br>
Reverse thinking, modeled after human reasoning, is introduced in LLMs via RevThink, a training framework combining forward and backward reasoning. The approach improves accuracy, sample efficiency, and generalization across reasoning tasks, offering a novel paradigm for enhancing AI reasoning.*** <br> <br>
    Nov 29, UNC and Google published a [paper](https://arxiv.org/pdf/2411.19865) “Reverse Thinking Makes LLMs Stronger Reasoners”. Reverse thinking plays a crucial role in human reasoning. Humans can reason not only from a problem to a solution but also in reverse, i.e., start from the solution and reason towards the problem. This often enhances overall reasoning performance as it enables consistency checks between their forward and backward thinking. To enable Large Language Models (LLMs) to perform reverse thinking, the study introduces Reverse-Enhanced Thinking (RevThink), a framework composed of data augmentation and learning objectives. In RevThink, the work augments the dataset by collecting structured forward-backward reasoning from a teacher model, consisting of: (1) the original question, (2) forward reasoning, (3) backward question, and (4) backward reasoning. The study then employs three objectives to train a smaller student model in a multi-task learning fashion: (a) generate forward reasoning from a question, (b) generate a backward question from a question, and (c) generate backward reasoning from the backward question. Experiments across 12 datasets covering commonsense, math, and logical reasoning show an average 13.53% improvement over the student model's zero-shot performance and a 6.84% improvement over the strongest knowledge distillation baselines. Moreover, the method demonstrates sample efficiency -- using only 10% of the correct forward reasoning from the training data, it outperforms a standard fine-tuning method trained on 10x more forward reasoning. RevThink also exhibits strong generalization to out-of-distribution held-out datasets. <br> <br>

27. ***Decoupled Momentum Optimization (DeMo) <br>
DeMo, a novel optimizer, reduces the need for high-speed interconnects in distributed training by decoupling momentum updates. Supporting scalable, bandwidth-efficient training, DeMo matches state-of-the-art performance while lowering resource demands, enabling cost-effective neural network pretraining.*** <br> <br>
    Nov 29, Nous Research published a [paper](https://arxiv.org/pdf/2411.19870) “DeMo: Decoupled Momentum Optimization”. Training large neural networks typically requires sharing gradients between accelerators through specialized high-speed interconnects. Drawing from the signal processing principles of frequency decomposition and energy compaction, the study demonstrates that synchronizing full optimizer states and model parameters during training is unnecessary. By decoupling momentum updates and allowing controlled divergence in optimizer states across accelerators, the study achieves improved convergence compared to state-of-the-art optimizers. The work introduces {De}coupled {Mo}mentum (DeMo), a fused optimizer and data parallel algorithm that reduces inter-accelerator communication requirements by several orders of magnitude. This enables training of large neural networks even with limited network bandwidth and heterogeneous hardware. The method is topology-agnostic and architecture-independent and supports scalable clock-synchronous distributed training with negligible compute and memory overhead. Empirical results show that models trained with DeMo match or exceed the performance of equivalent models trained with AdamW, while eliminating the need for high-speed interconnects when pre-training large scale foundation models. An open source reference PyTorch implementation is published on GitHub at https://github.com/bloc97/DeMo <br> <br>

29. ***AI2T: Building Trustable AI Tutors <br>
Carnegie Mellon University introduced AI2T, a system enabling efficient creation of intelligent tutoring systems (ITSs) via interactive teaching. AI2T learns through step-by-step solutions and self-assesses using STAND, a novel algorithm outperforming methods like XGBoost. The system can reliably induce rules for problem-solving with just 20–30 minutes of training, reducing the labor-intensive process of ITS programming. AI2T’s self-aware capabilities ensure more accurate and trustworthy outcomes compared to large language models, with promising implications for data-efficient, scalable ITS development.*** <br> <br>
    Nov 26, CMU published a [paper](https://arxiv.org/pdf/2411.17924) “AI2T: Building Trustable AI Tutors by Interactively Teaching a Self-Aware Learning Agent”. AI2T is an interactively teachable AI for authoring intelligent tutoring systems (ITSs). Authors tutor AI2T by providing a few step-by-step solutions and then grading AI2T's own problem-solving attempts. From just 20-30 minutes of interactive training, AI2T can induce robust rules for step-by-step solution tracking (i.e., model-tracing). As AI2T learns it can accurately estimate its certainty of performing correctly on unseen problem steps using STAND: a self-aware precondition learning algorithm that outperforms state-of-the-art methods like XGBoost. The user study shows that authors can use STAND's certainty heuristic to estimate when AI2T has been trained on enough diverse problems to induce correct and complete model-tracing programs. AI2T-induced programs are more reliable than hallucination-prone LLMs and prior authoring-by-tutoring approaches. With its self-aware induction of hierarchical rules, AI2T offers a path toward trustable data-efficient authoring-by-tutoring for complex ITSs that normally require as many as 200-300 hours of programming per hour of instruction. <br> <br>

31. ***AI in Scientific Discovery and Innovation <br>
MIT analyzed AI's impact on scientific and product innovation through a study involving the deployment of materials discovery AI in a corporate R&D setting. The findings revealed significant productivity boosts for high-performing scientists, with a 44% increase in discoveries, 39% more patent filings, and 17% greater product innovation. AI automated routine tasks, enabling researchers to focus on evaluating results. However, this benefit was unevenly distributed, primarily aiding top researchers. Despite the productivity gains, 82% of participants reported diminished job satisfaction due to reduced creativity and underutilization of skills, highlighting the trade-offs of AI augmentation.*** <br> <br>
    Nov 6, MIT published a [paper](https://aidantr.github.io/files/AI_innovation.pdf) “Artificial Intelligence, Scientific Discovery, and Product Innovation”. This paper studies the impact of artificial intelligence on innovation, exploiting the randomized introduction of a new materials discovery technology to 1,018 scientists in the R&D lab of a large U.S. firm. AI-assisted researchers discover 44% more materials, resulting in a 39% increase in patent filings and a 17% rise in downstream product innovation. These compounds possess more novel chemical structures and lead to more radical inventions. However, the technology has strikingly disparate effects across the productivity distribution: while the bottom third of scientists see little benefit, the output of top researchers nearly doubles. Investigating the mechanisms behind these results, the study shows that AI automates 57% of “idea-generation” tasks, reallocating researchers to the new task of evaluating model-produced candidate materials. Top scientists leverage their domain knowledge to prioritize promising AI suggestions, while others waste significant resources testing false positives. Together, these findings demonstrate the potential of AI-augmented research and highlight the complementarity between algorithms and expertise in the innovative process. Survey evidence reveals that these gains come at a cost, however, as 82% of scientists report reduced satisfaction with their work due to decreased creativity and skill underutilization.
<br><br><br>


***Dec 1st***

1. ***Microsoft's Magentic-One Multi-Agent System <br>
Microsoft introduced Magentic-One, a versatile multi-agent system that excels in managing complex multi-step tasks across domains like software development, data analysis, and web navigation. It features an Orchestrator coordinating four specialized agents (WebSurfer, FileSurfer, Coder, ComputerTerminal) and is model-agnostic, supporting LLMs like GPT-4o. Tested on benchmarks like GAIA and AutoGenBench, it demonstrated high workflow accuracy and emphasizes safety via guidelines, human oversight, and red-teaming exercises. The system's open-source release highlights industry trends toward modular AI architectures.*** <br> <br>
   Nov 30, according to [InfoQ](https://www.infoq.com/news/2024/11/microsoft-magentic-one/), Microsoft has introduced Magentic-One, a versatile multi-agent system designed to handle complex, multi-step tasks across various domains, enhancing efficiency in areas like software development, data analysis, and web navigation. The system features a multi-agent architecture led by an Orchestrator agent, coordinating four specialized agents: WebSurfer for browser-based tasks, FileSurfer for file operations, Coder for writing and analyzing code, and ComputerTerminal for executing code and system-level operations. Built on the open-source Microsoft AutoGen framework, Magentic-One is model-agnostic and compatible with various large language models (LLMs), including GPT-4o. Tested on benchmarks such as GAIA, AssistantBench, and WebArena using AutoGenBench, the system demonstrated competitive accuracy in managing complex workflows. Microsoft has addressed potential risks like unintended actions and system misuse by incorporating guidelines for safe deployment, red-teaming exercises, and human oversight recommendations. The release has garnered interest within the AI community, with experts noting its potential impact on LLM-based applications and the innovative approach to web browsing. The code for Magentic-One and its evaluation tool, AutoGenBench, is available as open-source resources, encouraging collaboration to enhance agentic AI systems. This development reflects a broader industry trend towards modular and collaborative AI architectures, with major companies like AWS, IBM, and OpenAI also contributing to this field. <br> <br>

3. ***NeuroAI for AI Safety <br>
A paper explores neuroscience's role in AI safety, emphasizing the brain's architecture as a model for robust, cooperative, and pragmatic intelligence. The research outlines paths for AI safety inspired by neuroscience, including brain emulation, robust sensory systems, and interpretability via neuroscience methods. Recommendations include scaling cognitively inspired architectures and leveraging brain data for safer AI development.*** <br> <br>
   Nov 27, Amaranth Foundation, Princeton Uni, MIT, Stanford Uni et al. published a [paper](https://arxiv.org/pdf/2411.18526) “NeuroAI for AI Safety”. As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, the researchers highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. The work makes several concrete recommendations for how neuroscience can positively impact AI safety. <br> <br>

5. ***LLMs Surpass Human Experts in Neuroscience Predictions <br>
A study highlights the potential of LLMs, particularly BrainGPT, to predict neuroscience results better than human experts by synthesizing decades of research. Using BrainBench, a benchmark for neuroscience prediction, LLMs demonstrated superior accuracy, especially when confident in their predictions. This approach is transferable to other knowledge-intensive fields, suggesting LLMs as valuable discovery tools.*** <br> <br>
   Nov 27, Nature Human Behaviour published a [paper](https://www.nature.com/articles/s41562-024-02046-9) “Large language models surpass human experts in predicting neuroscience results”. Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. Here, to evaluate this possibility, the study created BrainBench, a forward-looking benchmark for predicting neuroscience results. The study finds that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs indicated high confidence in their predictions, their responses were more likely to be correct, which presages a future where LLMs assist humans in making discoveries. The approach is not neuroscience specific and is transferable to other knowledge-intensive endeavours. <br> <br>

7. ***Nvidia's Star Attention for Efficient LLM Inference <br>
Nvidia introduced Star Attention, an efficient mechanism for long-sequence LLM inference that reduces computational costs and memory requirements. It uses block-sparse approximations and sequence-global attention, improving inference time by up to 11x while retaining 95-100% accuracy. The innovation integrates seamlessly with most Transformer-based LLMs and is available as open-source code.*** <br> <br>
   Nov 26, Nvidia published a [paper](https://arxiv.org/pdf/2411.17116) “Star Attention: Efficient LLM Inference over Long Sequences”. Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. The study introduces Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 95-100% of accuracy. Code is here. <br> <br>
   
9. ***Critic-RM: Self-Generated Critiques in Reward Modeling <br>
Meta and Georgia Tech proposed Critic-RM, a framework that enhances reward modeling for LLMs by integrating self-generated critiques alongside scalar rewards. This two-stage process improves modeling accuracy and rectifies flawed reasoning steps, with experiments showing a 3.7%-7.3% accuracy improvement compared to standard models.*** <br> <br>
    Nov 26, Meta and Georgia Inst of Tech published a [paper](https://arxiv.org/pdf/2411.16646) “Self-Generated Critiques Boost Reward Modeling for Language Models”. Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce scalar scores and struggle to incorporate critiques in a natural language format. The study hypothesizes that predicting both critiques and the scalar reward would improve reward modeling ability. Motivated by this, the study proposes Critic-RM, a framework that improves reward models using self-generated critiques without extra supervision. Critic-RM employs a two-stage process: generating and filtering high-quality critiques, followed by joint fine-tuning on reward prediction and critique generation. Experiments across benchmarks show that Critic-RM improves reward modeling accuracy by 3.7%-7.3% compared to standard reward models and LLM judges, demonstrating strong performance and data efficiency. Additional studies further validate the effectiveness of generated critiques in rectifying flawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy. <br> <br>
   
11. ***Limits of LLM Resampling with Imperfect Verifiers <br>
Princeton researchers showed that inference scaling using resampling has limitations when verifiers, like unit tests, are imperfect. False positives in coding tasks impose accuracy limits, and resampling cannot surpass the verifier's flaws. The study finds optimal sampling rates and highlights the need for strong models to reduce false positives and coding inaccuracies.*** <br> <br>
    Nov 26, Princeton Uni published a [paper](https://arxiv.org/pdf/2411.17501) “Inference Scaling FLaws: The Limits of LLM Resampling with Imperfect Verifiers”. Recent research has generated hope that inference scaling could allow weaker language models to match or exceed the accuracy of stronger models, such as by repeatedly sampling solutions to a coding problem until it passes unit tests. The central thesis of this paper is that there is no free lunch for inference scaling: indefinite accuracy improvement through resampling can only be realized if the "verifier" (in this case, a set of unit tests) is perfect. When the verifier is imperfect, as it almost always is in domains such as reasoning or coding (for example, unit tests have imperfect coverage), there is a nonzero probability of false positives: incorrect solutions that pass the verifier. Resampling cannot decrease this probability, so it imposes an upper bound to the accuracy of resampling-based inference scaling even with an infinite compute budget. The study finds that there is a very strong correlation between the model's single-sample accuracy (i.e. accuracy without unit tests) and its false positive rate on coding benchmarks HumanEval and MBPP, whose unit tests have limited coverage. Therefore, no amount of inference scaling of weaker models can enable them to match the single-sample accuracy of a sufficiently strong model. When considering that false positives have a negative utility compared to abstaining from producing a solution, it bends the inference scaling curve further downward. Empirically, the study finds that the optimal number of samples can be less than 10 under realistic assumptions. Finally, the work shows that beyond accuracy, false positives may have other undesirable qualities, such as poor adherence to coding style conventions. <br> <br>
   
13. ***Extractive-Abstractive Spectrum in Information Sharing <br>
Stanford research introduced the extractive-abstractive spectrum to assess trade-offs in verifiability and utility in LLM outputs versus search engines. As outputs become more abstractive, utility improves, but verifiability decreases. The findings suggest domain-specific balancing of utility and verifiability for high-stakes LLM applications.*** <br> <br>
    Nov 26, Stanford Uni published a [paper](https://arxiv.org/pdf/2411.17375) “The Extractive-Abstractive Spectrum: Uncovering Verifiability Trade-offs in LLM Generations”. Across all fields of academic study, experts cite their sources when sharing information. While large language models (LLMs) excel at synthesizing information, they do not provide reliable citation to sources, making it difficult to trace and verify the origins of the information they present. In contrast, search engines make sources readily accessible to users and place the burden of synthesizing information on the user. Through a survey, the study finds that users prefer search engines over LLMs for high-stakes queries, where concerns regarding information provenance outweigh the perceived utility of LLM responses. To examine the interplay between verifiability and utility of information-sharing tools, the study introduces the extractive-abstractive spectrum, in which search engines and LLMs are extreme endpoints encapsulating multiple unexplored intermediate operating points. Search engines are extractive because they respond to queries with snippets of sources with links (citations) to the original webpages. LLMs are abstractive because they address queries with answers that synthesize and logically transform relevant information from training and in-context sources without reliable citation. The study defines five operating points that span the extractive-abstractive spectrum and conduct human evaluations on seven systems across four diverse query distributions that reflect real-world QA settings: web search, language simplification, multi-step reasoning, and medical advice. As outputs become more abstractive, the study finds that perceived utility improves by as much as 200%, while the proportion of properly cited sentences decreases by as much as 50% and users take up to 3 times as long to verify cited information. The findings recommend distinct operating points for domain-specific LLM systems and the failure analysis informs approaches to high-utility LLM systems that empower users to verify information. <br> <br>

15. ***ShowUI: Advancing GUI Visual Agents <br>
The National University of Singapore and Microsoft presented ShowUI, a vision-language-action model for GUI tasks, achieving high efficiency and accuracy in zero-shot screenshot grounding. Innovations like UI-guided token selection and interleaved vision-language-action streaming enhance training and performance. ShowUI achieves 75.1% accuracy and is available as open-source software.*** <br> <br>
    Nov 26, National Uni of Singapore and Microsoft published a [paper](https://arxiv.org/pdf/2411.17465) “ShowUI: One Vision-Language-Action Model for GUI Visual Agent”. Building Graphical User Interface (GUI) assistants holds significant promise for enhancing human workflow productivity. While most agents are language-based, relying on closed-source API with text-rich meta-information (e.g., HTML or accessibility tree), they show limitations in perceiving UI visuals as humans do, highlighting the need for GUI visual agents. This work develops a vision-language-action model in digital world, namely ShowUI, which features the following innovations: (i) UI-Guided Visual Token Selection to reduce computational costs by formulating screenshots as an UI connected graph, adaptively identifying their redundant relationship and serve as the criteria for token selection during self-attention blocks; (ii) Interleaved Vision-Language-Action Streaming that flexibly unifies diverse needs within GUI tasks, enabling effective management of visual-action history in navigation or pairing multi-turn query-action sequences per screenshot to enhance training efficiency; (iii) Small-scale High-quality GUI Instruction-following Datasets by careful data curation and employing a resampling strategy to address significant data type imbalances. With above components, ShowUI, a lightweight 2B model using 256K data, achieves a strong 75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection further reduces 33% of redundant visual tokens during training and speeds up the performance by 1.4x. Navigation experiments across web Mind2Web, mobile AITW, and online MiniWob environments further underscore the effectiveness and potential of the model in advancing GUI visual agents. The models are available at https://github.com/showlab/ShowUI. <br> <br>

17. ***OneDiffusion: Versatile Large-Scale Diffusion Model <br>
OneDiffusion supports bidirectional image synthesis and understanding, handling tasks like text-to-image generation, depth estimation, and multi-view generation. Its unified training framework eliminates the need for specialized architectures, enabling scalable, multi-task training. The model demonstrates competitive performance across tasks and is open-sourced.*** <br> <br>
    Nov 25, AI2, Uni of California, and Uni of Washington published a [paper](https://arxiv.org/pdf/2411.16318) “One Diffusion to Generate Them All”. The work introduces OneDiffusion, a versatile, large-scale diffusion model that seamlessly supports bidirectional image synthesis and understanding across diverse tasks. It enables conditional generation from inputs such as text, depth, pose, layout, and semantic maps, while also handling tasks like image deblurring, upscaling, and reverse processes such as depth estimation and segmentation. Additionally, OneDiffusion allows for multi-view generation, camera pose estimation, and instant personalization using sequential image inputs. The model takes a straightforward yet effective approach by treating all tasks as frame sequences with varying noise scales during training, allowing any frame to act as a conditioning image at inference time. The unified training framework removes the need for specialized architectures, supports scalable multi-task training, and adapts smoothly to any resolution, enhancing both generalization and scalability. Experimental results demonstrate competitive performance across tasks in both generation and prediction such as text-to-image, multiview generation, ID preservation, depth estimation and camera pose estimation despite relatively small training dataset. Code and checkpoint are freely available at https://github.com/lehduong/OneDiffusion <br> <br>

19. ***LLM-as-a-Judge Framework <br>
A comprehensive study on LLM-based judgment and assessment introduces a taxonomy for scoring, ranking, and selection tasks. The research compiles benchmarks and highlights challenges in using LLMs for nuanced judgment, offering insights for advancing this emerging paradigm in AI.*** <br> <br>
    Nov 25, Arizona State Uni, Uni of Illinois Chicago, Uni of Maryland, UC Berkeley et al published a [paper](https://arxiv.org/pdf/2411.16594) “From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge” . Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). However, traditional methods, whether matching-based or embedding-based, often fall short of judging subtle attributes and delivering satisfactory results. Recent advancements in Large Language Models (LLMs) inspire the "LLM-as-ajudge" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection across various tasks and applications. This paper provides a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to advance this emerging field. The paper begins by giving detailed definitions from both input and output perspectives. Then the paper introduces a comprehensive taxonomy to explore LLM-as-a-judge from three dimensions: what to judge, how to judge and where to judge. Finally, the authors compile benchmarks for evaluating LLM-as-a-judge and highlight key challenges and promising directions, aiming to provide valuable insights and inspire future research in this promising research area. <br> <br>

21. ***Predicting Emergent Capabilities via Finetuning <br>
UC Berkeley researchers proposed a method to predict emergent LLM capabilities by finetuning smaller models on specific tasks. This approach anticipates when capabilities will emerge in future models, validated on benchmarks like GSM8K and CommonsenseQA. Emergence laws derived from this method offer practical predictive insights.*** <br> <br>
    Nov 25, UC Berkeley published a [paper](https://arxiv.org/pdf/2411.16035) “Predicting Emergent Capabilities by Finetuning”. A fundamental open challenge in modern LLM scaling is the lack of understanding around emergent capabilities. In particular, language model pretraining loss is known to be highly predictable as a function of compute. However, downstream capabilities are far less predictable -- sometimes even exhibiting emergent jumps -- which makes it challenging to anticipate the capabilities of future models. This study first poses the task of emergence prediction: given access to current LLMs that have random few-shot accuracy on a task, can people predict whether future models (GPT-N+1) will have non-trivial accuracy on that task? The study then discovers a simple insight for this problem: finetuning LLMs on a given task can shift the point in scaling at which emergence occurs towards less capable models. To operationalize this insight, the authors can finetune LLMs with varying amounts of data and fit a parametric function that predicts when emergence will occur (i.e., "emergence laws"). The study validates this approach using four standard NLP benchmarks where large-scale open-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and CoLA). Using only small-scale LLMs, the study finds that, in some cases, the authors can accurately predict whether models trained with up to 4x more compute have emerged. Finally, the work presents a case study of two realistic uses for emergence prediction. <br> <br>

23. ***LLM Embeddings for Regression <br>
Stanford and Google explored the use of LLM embeddings for regression tasks, finding them superior to traditional feature engineering for high-dimensional data. The study explains this performance through Lipschitz continuity and identifies that model size and language understanding do not consistently improve regression outcomes.*** <br> <br>
    Nov 22, Stanford Uni and Google published a [paper](https://arxiv.org/pdf/2411.14708) “Understanding LLM Embeddings for Regression”. With the rise of large language models (LLMs) for flexibly processing information as strings, a natural application is regression, specifically by preprocessing string representations into LLM embeddings as downstream features for metric prediction. The study provides one of the first comprehensive investigations into embedding-based regression and demonstrate that LLM embeddings as features can be better for high-dimensional regression tasks than using traditional feature engineering. This regression performance can be explained in part due to LLM embeddings over numeric data inherently preserving Lipschitz continuity over the feature space. Furthermore, the study quantifies the contribution of different model effects, most notably model size and language understanding, which was found surprisingly do not always improve regression performance. <br> <br>

25. ***TÜLU 3: Open Post-Training for LLMs <br>
TÜLU 3 introduces open methods for post-training LLMs, surpassing models like Llama 3.1 and GPT-4o-mini in performance. Techniques include supervised fine-tuning, Direct Preference Optimization, and a novel RLVR method. The release includes comprehensive training recipes, datasets, and evaluations for reproducibility and adaptation.*** <br> <br>
    Nov 22, Allen Inst and Uni of Washington published a [paper](https://arxiv.org/pdf/2411.15124) “TÜLU 3: Pushing Frontiers in Open Language Model Post-Training”. Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, the study introduces TÜLU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. TÜLU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for the models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method which is called Reinforcement Learning with Verifiable Rewards (RLVR). With TÜLU 3, the study builds a multi-task evaluation scheme for posttraining with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. The work concludes with analysis and discussion of training methods that did not reliably improve performance. The TÜLU 3 release includes model weights, a demo, and the complete recipe — datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the TÜLU 3 approach to more domains. <br> <br>

27. ***Google Scholar's Role in the AI Era <br>
As Google Scholar marks 20 years, it faces competition from AI-driven tools but remains a vital resource due to its extensive database and community integration. Despite criticisms of its algorithm transparency, innovations in AI-powered features aim to sustain its relevance amidst evolving scholarly search technologies.*** <br> <br>
    Nov 19, Nature published an [article](https://www.nature.com/articles/d41586-024-03746-y) “Can Google Scholar survive the AI revolution?”. Google Scholar, the largest scholarly search engine, celebrates its 20th anniversary amid rising competition from AI-driven tools. Over the years, it has become a crucial resource for researchers due to its free access, extensive database, and sophisticated search capabilities. However, new AI-powered platforms like ChatGPT, Semantic Scholar, and OpenAlex are challenging its dominance by offering enhanced search experiences and data accessibility. Despite these advancements, Google Scholar's comprehensive coverage and deep integration in the scientific community make it difficult to displace. Co-founder Anurag Acharya emphasizes that Google Scholar's primary goal is to aid scholars in finding valuable research, and it continues to innovate with AI features like article ranking and search suggestions. Yet, it faces criticism for its lack of transparency regarding its search algorithms and content coverage. As AI tools evolve, the future of scholarly search engines remains dynamic, with Google Scholar striving to maintain its leading position while welcoming innovations that advance scientific research. <br> <br>

29. ***RPN 2: Unified Model for Complex Data <br>
RPN 2 introduces interdependence functions to enhance learning performance for complex, dependent data. By unifying backbones like CNNs, RNNs, GNNs, and Transformers, the model offers a broader canonical representation and potential for innovative architecture designs surpassing existing methods.*** <br> <br>
    Nov 17, Uni of California, Davis published a [paper](https://arxiv.org/pdf/2411.11162) “RPN 2: On Interdependence Function Learning Towards Unifying and Advancing CNN, RNN, GNN, and Transformer”. This paper builds upon the previous work on the Reconciled Polynomial Network (RPN). The original RPN model was designed under the assumption of input data independence, presuming the independence among both individual instances within data batches and attributes in each data instance. However, this assumption often proves invalid for function learning tasks involving complex, interdependent data such as language, images, time series, and graphs. Ignoring such data interdependence may inevitably lead to significant performance degradation. To overcome these limitations, the study introduces the new Reconciled Polynomial Network (version 2), namely RPN 2. By incorporating data and structural interdependence functions, RPN 2 explicitly models data interdependence via new component functions in its architecture. This enhancement not only significantly improves RPN 2's learning performance but also substantially expands its unifying potential, enabling it to encompass a broader range of contemporary dominant backbone models within its canonical representation. These backbones include, but are not limited to, convolutional neural networks (CNNs), recurrent neural networks (RNNs), graph neural networks (GNNs), and Transformers. The analysis reveals that the fundamental distinctions among these backbone models primarily stem from their diverse approaches to defining the interdependence functions. Furthermore, this unified representation opens up new opportunities for designing innovative architectures with the potential to surpass the performance of these dominant backbones. <br> <br>

31. ***Self-Taught Optimizer (STOP): Recursive Code Improvement <br>
Stanford, Microsoft, and OpenAI demonstrated that a scaffolding program infused with LLMs can recursively improve itself via strategies like beam search and genetic algorithms. Although not fully recursive self-improvement, this method significantly enhances code performance, highlighting potential advancements and risks in self-improving technologies.*** <br> <br>
    Aug 16, Stanford Uni, Microsoft and OpenAI published a [paper](https://arxiv.org/abs/2310.02304) “Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation”. Several recent advances in AI systems solve problems by providing a "scaffolding" program that structures multiple calls to language models (LMs) to generate better outputs. A scaffolding program is written in a programming language such as Python. This study uses a language-model-infused scaffolding program to improve itself. The work starts with a seed "improver" that improves an input program according to a given utility function by querying an LM several times and returning the best solution. The authors then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. A variety of self-improvement strategies are proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not full recursive self-improvement. Nonetheless, it demonstrates that a modern language model, GPT-4 in the experiments, is capable of writing code that can call itself to improve itself. The authors consider concerns around the development of self-improving technologies and evaluate the frequency with which the generated code bypasses a sandbox.
 <br> <br> <br>


***Nov 24***

1. ***AI-Powered Citizen Revolution:  <br>The Forbes article describes a growing trend where employees, aided by AI and user-friendly tools, are becoming technology creators across various roles. Dubbed the "AI-Powered Citizen Revolution," this shift sees workers like marketing managers and nurses leveraging low-code platforms, workflow automation, and data insights to solve problems. Despite initial IT resistance, companies like Shell are embracing this approach with dual IT roles and governance models, enabling rapid innovation by tapping into employees' domain expertise. This trend is reshaping the future of work, blending AI with human ingenuity.*** <br> <br>
   Nov 22, Forbes published an [article](https://www.forbes.com/sites/bernardmarr/2024/11/22/the-ai-powered-citizen-revolution-how-every-employee-is-becoming-a-technology-creator/) “The AI-Powered Citizen Revolution: How Every Employee Is Becoming A Technology Creator”. The article highlights a significant shift in organizations where employees across various departments are becoming technology creators, leveraging AI and user-friendly tools. This movement, termed the "AI-Powered Citizen Revolution," sees marketing managers, nurses, and finance teams developing their own tech solutions. Tom Davenport and Ian Barkin, co-authors of 'All Hands on Tech,' explain that this trend is driven by the increasing ease of using technology and the powerful devices everyone now possesses. The revolution includes three types of citizen creators: developers using low-code/no-code platforms, automators creating workflows, and data scientists deriving insights from data. An example from Shell illustrates this shift, where an employee transitioned from manual tasks to becoming a citizen developer. Despite initial resistance from IT departments, progressive organizations are embracing this change, recognizing the need for dual IT roles: one for maintaining systems and another for supporting citizen developers. Successful companies implement systems like Shell's "red, amber, green" model to balance innovation and control. This revolution is transforming work and innovation, enabling faster and more effective solutions by tapping into employees' domain expertise. The future of work will involve a blend of AI and human ingenuity, with organizations providing the necessary tools, training, and governance to empower their workforce. <br> <br>

3. ***AIMV2 Vision Encoders:  <br>Apple's paper introduces AIMV2, a new family of vision encoders pre-trained in a multimodal setting. By integrating image and text modalities, these encoders achieve outstanding performance in tasks like localization and classification. AIMV2 surpasses state-of-the-art models such as CLIP in multimodal evaluations, showcasing scalability and robust capabilities, including a record-breaking 89.5% ImageNet-1k accuracy.*** <br> <br>
   Nov 21, Apple published a [paper](https://arxiv.org/pdf/2411.14402) “Multimodal Autoregressive Pre-training of Large Vision Encoders”. The paper introduces a novel method for pre-training of large-scale vision encoders. Building on recent advancements in autoregressive pre-training of vision models, the study extends this framework to a multimodal setting, i.e., images and text. The study presents AIMV2, a family of generalist vision encoders characterized by a straightforward pre-training process, scalability, and remarkable performance across a range of downstream tasks. This is achieved by pairing the vision encoder with a multimodal decoder that autoregressively generates raw image patches and text tokens. The encoders excel not only in multimodal evaluations but also in vision benchmarks such as localization, grounding, and classification. Notably, the AIMV2-3B encoder achieves 89.5% accuracy on ImageNet-1k with a frozen trunk. Furthermore, AIMV2 consistently outperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in multimodal image understanding across diverse settings. [Code is here](https://github.com/apple/ml-aim). <br> <br>

5. ***Platform Engineering Evolution:  <br>Forbes highlights the rise of platform engineering as a more structured alternative to DevOps, which has become ambiguous over time. This approach separates operational and development tasks, allowing teams to focus on their strengths. Popularized at events like KubeCon, platform engineering leverages cloud-native technologies and offers a scalable framework for modern infrastructure and development needs.*** <br> <br>
   Nov 21, Forbes published an [article](https://www.forbes.com/sites/justinwarren/2024/11/21/platform-engineering-is-the-new-devops/) “Platform Engineering Is The New DevOps”. The article discusses how platform engineering is emerging as a preferred term over DevOps in the cloud-native community. Initially, DevOps aimed to bridge the gap between developers and operations, promoting collaboration. However, over time, the term became ambiguous and overused, leading to confusion and inefficiency. DevOps often resulted in developers handling operations tasks they were not interested in, causing friction and dissatisfaction. Platform engineering offers a solution by clearly separating operations from application development, allowing each team to focus on their strengths. This shift is seen as a natural evolution in the cloud-native ecosystem, with platform engineering gaining popularity at events like KubeCon. Unlike DevOps, which lacked a formal definition and became a catch-all term, platform engineering is viewed as a more structured and scalable approach. It emphasizes using cloud-native technologies rather than building them, aligning with the operational needs of modern organizations. Companies are encouraged to reassess their DevOps structures to ensure they are still effective, as platform engineering may provide a more efficient and focused framework for managing infrastructure and development tasks. <br> <br>

7. ***OpenScholar for Literature Synthesis:  <br>Researchers introduce OpenScholar, a retrieval-augmented language model designed for synthesizing scientific literature. Leveraging a datastore of 45 million papers, it provides citation-backed responses to scientific queries. OpenScholar outperforms GPT-4o in correctness and citation accuracy and is well-received in expert evaluations, signaling significant advancements in research assistance.*** <br> <br>
   Nov 21, Uni of Washington, Allen Inst for AI, UIUC, CMU, Meta, UNCC, and Stanford Uni published a [paper](https://arxiv.org/pdf/2411.14199) “OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs”. Scientific progress depends on researchers' ability to synthesize the growing body of literature. Can large language models (LMs) assist scientists in this task? The study introduce OpenScholar, a specialized retrieval-augmented LM that answers scientific queries by identifying relevant passages from 45 million open-access papers and synthesizing citation-backed responses. To evaluate OpenScholar, the authors develop ScholarQABench, the first large-scale multi-domain benchmark for literature search, comprising 2,967 expert-written queries and 208 long-form answers across computer science, physics, neuroscience, and biomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and PaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o hallucinates citations 78 to 90% of the time, OpenScholar achieves citation accuracy on par with human experts. OpenScholar's datastore, retriever, and self-feedback inference loop also improves off-the-shelf LMs: for instance, OpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations, experts preferred OpenScholar-8B and OpenScholar-GPT4o responses over expert-written ones 51% and 70% of the time, respectively, compared to GPT4o's 32%. The authors open-source all of the [code](https://github.com/AkariAsai/OpenScholar), [models](https://huggingface.co/collections/OpenScholar/openscholar-v1-67376a89f6a80f448da411a6), datastore, data and a public demo. <br> <br>

9. ***Knowledge Awareness in LLMs:  <br>A study explores how large language models recognize entities and avoid hallucinations using sparse autoencoders. By identifying causal directions in model representation spaces, researchers demonstrate that models can self-regulate their responses based on known or unknown entities. These findings provide insights into improving model reliability and interpretability.*** <br> <br>
    Nov 21, UPC and ETH Zurich published a [paper](https://arxiv.org/pdf/2411.14257) “Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models”. Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting the ability to solve this problem. Using sparse autoencoders as an interpretability tool, the study discovers that a key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about. Sparse autoencoders uncover meaningful directions in the representation space, these detect whether the model recognizes an entity, e.g. detecting it doesn't know about an athlete or a movie. This suggests that models can have self-knowledge: internal representations about their own capabilities. These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse. The study demonstrates that despite the sparse autoencoders being trained on the base model, these directions have a causal effect on the chat model's refusal behavior, suggesting that chat finetuning has repurposed this existing mechanism. Furthermore, the work provides an initial exploration into the mechanistic role of these directions in the model, finding that they disrupt the attention of downstream heads that typically move entity attributes to the final token. <br> <br>

11. ***Quantum Error Decoding with AI:  <br>Google presents a transformer-based neural network decoder for quantum error correction. This decoder surpasses traditional approaches in accuracy and adapts to complex error distributions, even with limited experimental data. The work underscores the potential of machine learning to revolutionize quantum computing by learning from data rather than human-designed algorithms.*** <br> <br>
    Nov 20, Google published a [paper](https://www.nature.com/articles/s41586-024-08148-8) on Nature “Learning high-accuracy error decoding for quantum processors”. Building a large-scale quantum computer requires effective strategies to correct errors that inevitably arise in physical quantum systems. Quantum error-correction codes present a way to reach this goal by encoding logical information redundantly into many physical qubits. A key challenge in implementing such codes is accurately decoding noisy syndrome information extracted from redundancy checks to obtain the correct encoded logical information. Here the researchers develop a recurrent, transformer-based neural network that learns to decode the surface code, the leading quantum error-correction code. The decoder outperforms other state-of-the-art decoders on real-world data from Google’s Sycamore quantum processor for distance-3 and distance-5 surface codes. On distances up to 11, the decoder maintains its advantage on simulated data with realistic noise including cross-talk and leakage, utilizing soft readouts and leakage information. After training on approximate synthetic data, the decoder adapts to the more complex, but unknown, underlying error distribution by training on a limited budget of experimental samples. The work illustrates the ability of machine learning to go beyond human-designed algorithms by learning from data directly, highlighting machine learning as a strong contender for decoding in quantum computers. <br> <br>

13. ***Hymba for Small Language Models:  <br>Nvidia's Hymba architecture combines attention mechanisms with state space models to enhance efficiency in small language models. It introduces innovations like learnable meta tokens and cross-layer sharing, achieving state-of-the-art performance among sub-2B models and offering significant improvements in cache size and throughput.*** <br> <br>
    Nov 20, Nvidia published a [paper](https://arxiv.org/pdf/2411.13676) “Hymba: A Hybrid-head Architecture for Small Language Models”. The study proposes Hymba, a family of small language models featuring a hybrid-head parallel architecture that integrates transformer attention mechanisms with state space models (SSMs) for enhanced efficiency. Attention heads provide high-resolution recall, while SSM heads enable efficient context summarization. Additionally, the study introduces learnable meta tokens that are prepended to prompts, storing critical information and alleviating the "forced-to-attend" burden associated with attention mechanisms. This model is further optimized by incorporating cross-layer key-value (KV) sharing and partial sliding window attention, resulting in a compact cache size. During development, the study conducted a controlled study comparing various architectures under identical settings and observed significant advantages of the proposed architecture. Notably, Hymba achieves state-of-the-art results for small LMs: the Hymba-1.5B-Base model surpasses all sub-2B public models in performance and even outperforms Llama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size reduction, and 3.49x throughput. <br> <br>

15. ***RedPajama Open Datasets:  <br>The RedPajama project tackles challenges in transparency and quality of datasets for training large language models. By releasing RedPajama-V1 and V2, the study provides over 100 trillion tokens with metadata and quality signals, fostering the development of transparent and high-performing open-source language models.*** <br> <br>
    Nov 19, Together AI, Stanford Uni, Uni of Chicago, EleutherAI, Ontocord.ai, Princeton Uni, ETH Zurich, Mila, Uni of Montreal et al published a [paper](https://arxiv.org/pdf/2411.12372) “RedPajama: an Open Dataset for Training Large Language Models”. Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top-performing models lack transparency in their dataset curation and model development processes, posing an obstacle to the development of fully open language models. This work identifies three core data-related challenges that must be addressed to advance open-source language models. These include (1) transparency in model development, including the data curation process, (2) access to large quantities of high-quality data, and (3) availability of artifacts and metadata for dataset curation and analysis. To address these challenges, the work releases RedPajama-V1, an open reproduction of the LLaMA training dataset. In addition, we release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata. Together, the RedPajama datasets comprise over 100 trillion tokens spanning multiple domains and with their quality signals facilitate the filtering of data, aiming to inspire the development of numerous new datasets. To date, these datasets have already been used in the training of strong language models used in production, such as Snowflake Arctic, Salesforce's XGen and AI2's OLMo. To provide insight into the quality of RedPajama, the study presents a series of analyses and ablation studies with decoder-only language models with up to 1.6B parameters. The findings demonstrate how quality signals for web data can be effectively leveraged to curate high-quality subsets of the dataset, underscoring the potential of RedPajama to advance the development of transparent and high-performing language models at scale. <br> <br>

17. ***Scaling Laws Across Datasets:  <br>Harvard's study extends scaling laws to predict model loss across different datasets and tasks. By identifying shifted power law relationships, it demonstrates reliable loss predictions across diverse pre-training and downstream scenarios. These findings advance understanding of generalization in large-scale AI models.*** <br> <br>
    Nov 19, Harvard Uni published a [paper](https://arxiv.org/pdf/2411.12925) “Loss-to-Loss Prediction: Scaling Laws for All Datasets”. While scaling laws provide a reliable methodology for predicting train loss across compute scales for a single data distribution, less is known about how these predictions should change as the distribution is changed. This study derives a strategy for predicting one loss from another and apply it to predict across different pre-training datasets and from pre-training data to downstream task data. The predictions extrapolate well even at 20x the largest FLOP budget used to fit the curves. More precisely, the work finds that there are simple shifted power law relationships between (1) the train losses of two models trained on two separate datasets when the models are paired by training compute (train-to-train), (2) the train loss and the test loss on any downstream distribution for a single model (train-to-test), and (3) the test losses of two models trained on two separate train datasets (test-to-test). The results hold up for pre-training datasets that differ substantially (some are entirely code and others have no code at all) and across a variety of downstream tasks. Finally, the study finds that in some settings these shifted power law relationships can yield more accurate predictions than extrapolating single-dataset scaling laws. <br> <br>

19. ***Generative World Explorer (Genex):  <br>Johns Hopkins introduces Genex, a framework enabling AI agents to mentally explore large 3D worlds and update beliefs with imagined observations. This approach improves decision-making and planning in partial-observation scenarios, showcasing the potential of generative methods for embodied AI.*** <br> <br>
    Nov 19, Johns Hopkins Uni published a [paper](https://arxiv.org/pdf/2411.11844) “Generative World Explorer”. Planning with partial observation is a central challenge in embodied AI. A majority of prior works have tackled this challenge by developing agents that physically explore their environment to update their beliefs about the world state. In contrast, humans can imagine unseen parts of the world through a mental exploration and revise their beliefs with imagined observations. Such updated beliefs can allow them to make more informed decisions, without necessitating the physical exploration of the world at all times. To achieve this human-like ability, the study introduces the Generative World Explorer (Genex), an egocentric world exploration framework that allows an agent to mentally explore a large-scale 3D world (e.g., urban scenes) and acquire imagined observations to update its belief. This updated belief will then help the agent to make a more informed decision at the current step. To train Genex, the study creates a synthetic urban scene dataset, Genex-DB. Experimental results demonstrate that (1) Genex can generate high-quality and consistent observations during long-horizon exploration of a large virtual physical world and (2) the beliefs updated with the generated observations can inform an existing decision-making model (e.g., an LLM agent) to make better plans. <br> <br>

21. ***Procedural Knowledge in LLMs:  <br>UCL and collaborators reveal that LLM reasoning relies on procedural knowledge rather than retrieval-like strategies. Through pretraining data analysis, the study highlights how models synthesize procedural insights to solve reasoning tasks, differentiating them from fact-based question answering.*** <br> <br>
    Nov 19, UCL, Cohere et al. published a [paper](https://arxiv.org/pdf/2411.12580) “Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models”. The capabilities and limitations of Large Language Models have been sketched out in great detail in recent years, providing an intriguing yet conflicting picture. On the one hand, LLMs demonstrate a general ability to solve problems. On the other hand, they show surprising reasoning gaps when compared to humans, casting doubt on the robustness of their generalisation strategies. The sheer volume of data used in the design of LLMs has precluded the authors from applying the method traditionally used to measure generalisation: train-test set separation. To overcome this, the authors study what kind of generalisation strategies LLMs employ when performing reasoning tasks by investigating the pretraining data they rely on. For two models of different sizes (7B and 35B) and 2.5B of their pretraining tokens, the study identifies what documents influence the model outputs for three simple mathematical reasoning tasks and contrast this to the data that are influential for answering factual questions. The study finds that, while the models rely on mostly distinct sets of data for each factual question, a document often has a similar influence across different reasoning questions within the same task, indicating the presence of procedural knowledge. The study further finds that the answers to factual questions often show up in the most influential data. However, for reasoning questions the answers usually do not show up as highly influential, nor do the answers to the intermediate reasoning steps. When characterising the top ranked documents for the reasoning questions qualitatively, the study confirms that the influential documents often contain procedural knowledge, like demonstrating how to obtain a solution using formulae or code. The findings indicate that the approach to reasoning the models use is unlike retrieval, and more like a generalisable strategy that synthesises procedural knowledge from documents doing a similar form of reasoning. <br> <br>

23. ***Pixtral Large Multimodal Model:  <br>Mistral unveils Pixtral Large, a state-of-the-art multimodal model excelling in image understanding and reasoning. It outperforms leading models in benchmarks like MathVista and DocVQA and introduces advancements in handling complex visual data, reaffirming its leadership in multimodal AI.*** <br> <br>
    Nov 18, Mistral [released Pixtral Large](https://mistral.ai/news/pixtral-large/?utm_source=substack&utm_medium=email), a 124B open-weights multimodal model built on top of Mistral Large 2. Pixtral Large is the second model in the multimodal family and demonstrates frontier-level image understanding. Particularly, the model is able to understand documents, charts and natural images, while maintaining the leading text-only understanding of Mistral Large 2. Mistral evaluates Pixtral Large against frontier models on a set of standard multimodal benchmarks, through a common testing harness. On MathVista, which evaluates complex mathematical reasoning over visual data, the model achieves 69.4%, outperforming all other models. To assess reasoning capabilities over complex charts and documents, Mistral evaluates performance using ChartQA and DocVQA, where Pixtral Large surpasses GPT-4o and Gemini-1.5 Pro. Finally, Pixtral Large demonstrates competitive capabilities on MM-MT-Bench, outperforming all of Claude-3.5 Sonnet (new), Gemini-1.5 Pro and GPT-4o (latest). MM-MT-Bench is an open-source, judge-based evaluation intended to reflect real-world use cases of multimodal LLMs (see the Pixtral 12B [technical report](https://arxiv.org/abs/2410.07073) for details). Along with Pixtral Large, Mistral Large, the state-of-the-art text model, also gets an update. The model is available as pixtral-large-latest on our API, as well as for self-deployment as Mistral Large 24.11 on HuggingFace under the Mistral Research License (MRL) for research, or with a commercial license from Mistral AI for commercial use. <br> <br>

25. ***Challenges in Reranker Scaling:  <br>Databricks identifies limitations in reranker performance as the number of documents increases. Contrary to expectations, rerankers show diminishing returns and can degrade retrieval quality when overburdened, calling for research into scalable reranking methods.*** <br> <br>
    Nov 18, Databricks and UIUC published a [paper](https://arxiv.org/pdf/2411.11767) “Drowning in Documents: Consequences of Scaling Reranker Inference”. Rerankers, typically cross-encoders, are often used to re-score the documents retrieved by cheaper initial IR systems. This is because, though expensive, rerankers are assumed to be more effective. The study challenges this assumption by measuring reranker performance for full retrieval, not just re-scoring first-stage retrieval. Experiments reveal a surprising trend: the best existing rerankers provide diminishing returns when scoring progressively more documents and actually degrade quality beyond a certain limit. In fact, in this setting, rerankers can frequently assign high scores to documents with no lexical or semantic overlap with the query. The authors hope that the findings will spur future research to improve reranking. <br> <br>

27. ***LLMs’ Flexibility Beyond Linguistics:  <br>A study finds that LLMs perform equally well on forward and backward scientific text, suggesting their success stems from their architecture's ability to learn structured patterns rather than human-like linguistic processing. This highlights the generality of transformers across domains.*** <br> <br>
    Nov 17, Uni College London and Uni of Tubingen published a [paper](https://arxiv.org/pdf/2411.11061v1) “Beyond Human-Like Processing: Large Language Models Perform Equivalently on Forward and Backward Scientific Text”. The impressive performance of large language models (LLMs) has led to their consideration as models of human language processing. Instead, the work suggests that the success of LLMs arises from the flexibility of the transformer learning architecture. To evaluate this conjecture, the work trained LLMs on scientific texts that were either in a forward or backward format. Despite backward text being inconsistent with the structure of human languages, the study found that LLMs performed equally well in either format on a neuroscience benchmark, eclipsing human expert performance for both forward and backward orders. The results are consistent with the success of transformers across diverse domains, such as weather prediction and protein design. This widespread success is attributable to LLM’s ability to extract predictive patterns from any sufficiently structured input. Given their generality, we suggest caution in interpreting LLM’s success in linguistic tasks as evidence for human-like mechanisms. <br> <br>

29. ***AI for Chip Design:  <br>Google defends its deep reinforcement learning method, AlphaChip, against critiques questioning its chip design capabilities. The response underscores the method’s impact, which has already achieved widespread adoption, and addresses flaws in critical evaluations, reaffirming the method's validity.***
    Nov 15, Google and Stanford published a [paper](https://arxiv.org/pdf/2411.10053) “That Chip Has Sailed: A Critique of Unfounded Skepticism Around AI for Chip Design”. In 2020, Google introduced a deep reinforcement learning method capable of generating superhuman chip layouts, which Google then published in Nature and open-sourced on GitHub. AlphaChip has inspired an explosion of work on AI for chip design, and has been deployed in state-of-the-art chips across Alphabet and extended by external chipmakers. Even so, a non-peer-reviewed invited paper at ISPD 2023 questioned its performance claims, despite failing to run the method as described in Nature. For example, it did not pre-train the RL method (removing its ability to learn from prior experience), used substantially fewer compute resources (20x fewer RL experience collectors and half as many GPUs), did not train to convergence (standard practice in machine learning), and evaluated on test cases that are not representative of modern chips. Recently, Igor Markov published a meta-analysis of three papers: Google’s peer-reviewed Nature paper, the non-peer-reviewed ISPD paper, and Markov's own unpublished paper (though he does not disclose that he co-authored it). Although AlphaChip has already achieved widespread adoption and impact, the authors publish this response to ensure that no one is wrongly discouraged from innovating in this impactful area.

31. ***Microsoft and MIT explore prompt formatting's impact on LLM performance: <br>
Prompt optimization is crucial for LLM effectiveness, yet the role of prompt templates has been underexplored. This study evaluates how formatting the same contexts into templates like plain text, Markdown, JSON, and YAML affects LLM performance across tasks like reasoning, code generation, and translation. Results show significant variability: GPT-3.5-turbo’s performance fluctuated by up to 40% in code translation tasks depending on the prompt template, while GPT-4 proved more robust. The findings challenge the reliance on fixed prompt formats and call for more flexibility in their design.*** <br> <br>
    Nov 15, Microsoft, and MIT published a [paper](https://arxiv.org/pdf/2411.10541v1) “Does Prompt Formatting Have Any Impact on LLM Performance?”. In the realm of Large Language Models (LLMs), prompt optimization is crucial for model performance. Although previous research has explored aspects like rephrasing prompt contexts, using various prompting techniques (like in-context learning and chain-of-thought), and ordering few-shot examples, the understanding of LLM sensitivity to prompt templates remains limited. Therefore, this paper examines the impact of different prompt templates on LLM performance. The study formatted the same contexts into various human-readable templates, including plain text, Markdown, JSON, and YAML, and evaluated their impact across tasks like natural language reasoning, code generation, and translation using OpenAI’s GPT models. Experiments show that GPT-3.5-turbo’s performance varies by up to 40% in a code translation task depending on the prompt template, while larger models like GPT-4 are more robust to these variations. The analysis highlights the need to reconsider the use of fixed prompt templates, as different formats can significantly affect model performance. <br> <br>

33. ***University of Oxford investigates AI feedback guided by constitutions" <br>
AI models increasingly rely on "constitutions"—guidelines used for feedback and training. This study examined four constitutions designed to improve patient-centered communication in medical interviews. Evaluations by 215 human raters revealed that detailed constitutions enhanced emotive qualities but failed to outperform baselines in practical skills like information gathering. The research suggests that while detailed constitutions can improve certain aspects of AI feedback, their efficacy as a reward signal for practical applications may have limitations.*** <br> <br>
    Nov 15, Uni of Oxford published a [paper](https://arxiv.org/pdf/2411.10168) “Evaluating the role of ‘Constitutions’ for learning from AI feedback”. The growing capabilities of large language models (LLMs) have led to their use as substitutes for human feedback for training and assessing other LLMs. These methods often rely on ‘constitutions’, written guidelines which a critic model uses to provide feedback and improve generations. The work investigates how the choice of constitution affects feedback quality by using four different constitutions to improve patient-centered communication in medical interviews. In pairwise comparisons conducted by 215 human raters, the work found that detailed constitutions led to better results regarding emotive qualities. However, none of the constitutions outperformed the baseline in learning more practically-oriented skills related to information gathering and provision. The findings indicate that while detailed constitutions should be prioritised, there are possible limitations to the effectiveness of AI feedback as a reward signal in certain areas. <br> <br>

35. ***Collaborative study simulates behaviors of 1,000 individuals using generative agents: <br>
Stanford, Northwestern, University of Washington, and Google introduced a novel agent architecture simulating the behaviors and attitudes of 1,052 real individuals. By applying LLMs to qualitative interviews, the agents replicated participants’ survey responses with 85% accuracy and performed comparably in predicting personality traits and experimental outcomes. The architecture also reduced biases across racial and ideological groups compared to demographic-based models. This work lays the groundwork for tools to analyze individual and collective behavior for applications in policymaking and social science.*** <br> <br>
    Nov 14, Stanford Uni, Northwestern Uni, Uni of Washington, and Google published a [paper](https://arxiv.org/pdf/2411.10109) “Generative Agent Simulations of 1,000 People”. The promise of human behavioral simulation--general-purpose computational agents that replicate human behavior across domains--could enable broad applications in policymaking and social science. The study presents a novel agent architecture that simulates the attitudes and behaviors of 1,052 real individuals--applying large language models to qualitative interviews about their lives, then measuring how well these agents replicate the attitudes and behaviors of the individuals that they represent. The generative agents replicate participants' responses on the General Social Survey 85% as accurately as participants replicate their own answers two weeks later, and perform comparably in predicting personality traits and outcomes in experimental replications. The architecture reduces accuracy biases across racial and ideological groups compared to agents given demographic descriptions. This work provides a foundation for new tools that can help investigate individual and collective behavior.
 <br> <br>


***Nov 17***

1. ***Evo bridges DNA, RNA, and protein scales.  <br>The research introduces Evo, a genomic foundation model trained on extensive datasets of prokaryotic and phage genomes. Evo predicts biological functions, codesigns protein-DNA and protein-RNA systems, and generates genome-scale sequences. These capabilities advance understanding across molecular and genomic complexities, with implications for biology and genetic engineering.*** <br> <br>
   Nov 15, Arc Inst, Stanford Uni, UC Berkeley et al published a [paper](https://www.science.org/doi/10.1126/science.ado9336) on Science “Sequence modeling and design from molecular to genome scale with Evo”. The genome is a sequence that encodes the DNA, RNA, and proteins that orchestrate an organism’s function. The research presents Evo, a long-context genomic foundation model with a frontier architecture trained on millions of prokaryotic and phage genomes, and report scaling laws on DNA to complement observations in language and vision. Evo generalizes across DNA, RNA, and proteins, enabling zero-shot function prediction competitive with domain-specific language models and the generation of functional CRISPR-Cas and transposon systems, representing the first examples of protein-RNA and protein-DNA codesign with a language model. Evo also learns how small mutations affect whole-organism fitness and generates megabase-scale sequences with plausible genomic architecture. These prediction and generation capabilities span molecular to genomic scales of complexity, advancing the understanding and control of biology. <br> <br>

3. ***AI agents redefine productivity.  <br>Deloitte reports that multiagent AI systems are reshaping industries by enabling collaborative and complex tasks. Although technical challenges remain, the report forecasts rapid advancements, driven by growing investments and breakthroughs in reasoning capabilities. Businesses are urged to prepare for transformative impacts within a year.*** <br> <br>
   Nov 14, Deloitte published a [report](https://www2.deloitte.com/content/dam/Deloitte/us/Documents/consulting/us-ai-institute-generative-ai-agents-multiagent-systems.pdf) “Prompting for action How AI agents are reshapingthe future of work”. The main points of the report are: 1) AI agents are reshaping industries by expanding the potential applications of Generative AI (GenAI) and typical language models. 2) Multiagent AI systems can significantly enhance the quality of outputs and complexity of work performed by single AI agents. 3) Forward-thinking businesses and governments are already implementing AI agents and multiagent AI systems across a range of use cases. 4) Executive leaders should make moves now to prepare for and embrace this next era of intelligent organizational transformation. The report indicates that the era of AI agent collaboration is still in its early stages. Interest is growing among businesses and technology providers, but comprehensive solutions are not yet common. There is much technical work to be done—particularly in terms of the reasoning and planning capabilities that will enable AI agents. Improvements are likely to come fast. In recent months GenAI tools have shown significant improvements in reasoning and agent orchestration capabilities. Many venture capital firms are investing heavily across the spectrum of AI agent-related technologies, as are many of today’s leading GenAI and technology providers. What is available today is only a glimpse of what’s to come. People anticipate a significant evolution of core language models, AI agents, and agent orchestration platforms within the next 12 months. <br> <br>

5. ***LLMs lack human-like understanding.  <br>A study assessing LLMs on comprehension tasks reveals they perform at chance levels and exhibit non-human errors. Despite their usefulness, LLMs lack compositional understanding and grammatical regulation, emphasizing their limitations compared to humans in interpreting underlying meaning.*** <br> <br>
   Nov 14, researcher from Spain, German and USA published a [paper](https://www.nature.com/articles/s41598-024-79531-8#:~:text=We%20interpret%20this%20evidence%20as,regulating%20grammatical%20and%20semantic%20information.) on scientific reports “Testing AI on language comprehension tasks reveals insensitivity to underlying meaning”. Large Language Models (LLMs) are recruited in applications that span from clinical assistance and legal support to question answering and education. Their success in specialized tasks has led to the claim that they possess human-like linguistic capabilities related to compositional understanding and reasoning. Yet, reverse-engineering is bound by Moravec’s Paradox, according to which easy skills are hard. The study systematically assesses 7 state-of-the-art models on a novel benchmark. Models answered a series of comprehension questions, each prompted multiple times in two settings, permitting one-word or open-length replies. Each question targets a short text featuring high-frequency linguistic constructions. To establish a baseline for achieving human-like performance, the study tested 400 humans on the same prompts. Based on a dataset of n = 26,680 datapoints, the study discovered that LLMs perform at chance accuracy and waver considerably in their answers. Quantitatively, the tested models are outperformed by humans, and qualitatively their answers showcase distinctly non-human errors in language understanding. The study interprets this evidence as suggesting that, despite their usefulness in various tasks, current AI models fall short of understanding language in a way that matches humans, and the authors argue that this may be due to their lack of a compositional operator for regulating grammatical and semantic information. <br> <br>

7. ***ChatGPT aids academic growth.  <br>OpenAI’s guide highlights strategies for using ChatGPT to enhance writing skills, from generating citations to testing logic and refining ideas. It emphasizes ethical usage, encouraging transparency in academic practices to balance ChatGPT’s assistance with independent learning.*** <br> <br>
   Nov 14, OpenAI released an [article](https://openai.com/chatgpt/use-cases/student-writing-guide/) “A Student’s Guide to Writing with ChatGPT” to help students becoming better writing and thinkers. The guide includes: 1) Delagate citation grunt work ChatGPT; 2) Quickly get up to speed on a new topic; 3) Get a roadmap of relevant sources; 4) Complete your understanding by asking specific questions; 5) Improve your flow by getting feedback on structure; 6) Test your logic with reverse outlining; 7) Develop your ideas through Socratic dialogue; 8) Pressure-test your thesis by asking for counterarguments; 9) Compare your ideas against history’s greatest thinkers; 10) Elevate your writing through iterative feedback; 11) Use Advanced Voice Mode as a reading companion; 12) Don’t just go through the motions—hone your skills. One last point: When you use ChatGPT to deepen your understanding, develop your ideas, or come to insights you might not otherwise have had, it should fall within the bounds of acceptable academic practices. But since ChatGPT can also be used in unethical ways, your professors will likely feel more comfortable if they can see exactly how it’s contributing to your thinking. <br> <br>

11. ***AI agents level the playing field for SMBs.  <br>AI agents are poised to transform small and mid-sized businesses by automating tasks and boosting efficiency. Companies like Microsoft and Salesforce are driving these innovations, enabling SMBs to compete with larger enterprises while addressing concerns about job displacement through workforce retraining.*** <br> <br>
    Nov 14, Forbes published [an article](https://www.forbes.com/sites/quickerbettertech/2024/11/14/how-ai-agents-will-disrupt-small-and-mid-sized-business-in-2025/) “How AI Agents Will Disrupt Small And Mid-Sized Business In 2025”. In 2025, small and mid-sized businesses will see a transformative shift with the introduction of AI agents, enabling them to automate tasks similarly to large corporations. Unlike current generative AI chatbots, AI agents will perform complex tasks, initiate transactions, and solve problems, acting more like human assistants. Companies like Microsoft, Salesforce, and Intuit are developing AI agents to enhance productivity and profitability, offering features that will keep customers engaged and subscribed. These agents will handle various functions, from qualifying leads and managing finances to scheduling meetings and processing invoices. The healthcare and software development sectors are also seeing advancements with AI agents taking on roles traditionally held by humans. While there is concern about job displacement, business owners can mitigate this by training employees to work alongside AI. The adoption of AI agents will move AI from corporate environments to everyday business operations, making it crucial for small business owners to engage with their software vendors to leverage these new tools effectively. The big worry is the potential job loss, but for businesses facing labor shortages, AI agents will be invaluable. Business owners must address employee concerns by providing training and demonstrating the benefits of AI. The reality is that AI agents will start moving AI from the corporate boardroom to Main Street, and while the transition won't be immediate or flawless, it will be significant. Small business owners should proactively discuss AI agents with their software vendors to understand and apply these capabilities in 2025. <br> <br>

13. ***FinDVer benchmarks LLM claim verification.  <br>Yale's FinDVer evaluates LLMs' ability to verify claims in complex financial documents. Results show significant gaps between LLM and human performance, but the benchmark offers insights into advancing LLM capabilities for expert-domain applications.*** <br> <br>
    Nov 13, Yale Uni published a [paper](https://aclanthology.org/2024.emnlp-main.818.pdf) “FinDVer: Explainable Claim Verification over Long and Hybrid-Content Financial Documents”. The work introduces FinDVer, a comprehensive benchmark specifically designed to evaluate the explainable claim verification capabilities of LLMs in the context of understanding and analyzing long, hybrid-content financial documents. FinDVer contains 4,000 expert-annotated examples across four subsets, each focusing on a type of scenario that frequently arises in real-world financial domains. The study assesses a broad spectrum of 25 LLMs under long-context and RAG settings. The results show that even the current best-performing system (i.e., GPT-4o) significantly lags behind human experts. The detailed findings and insights highlight the strengths and limitations of existing LLMs in this new task. The researchers believe FinDVer can serve as a valuable benchmark for evaluating LLM capabilities in claim verification over complex, expert-domain documents.  <br> <br>

15. ***CCE slashes training memory needs.  <br>Apple proposes the Cut Cross-Entropy (CCE) method to dramatically lower memory usage in LLM training. By computing loss on-the-fly, CCE reduces the memory footprint without affecting performance, enabling efficient training for large vocabulary models.*** <br> <br>
    Nov 13, Apple published a [paper](https://arxiv.org/pdf/2411.09009) “Cut Your Losses in Large-Vocabulary Language Models”. As language models grow ever larger, so do their vocabularies. This has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation. Cross-entropy builds up a logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined. The work proposes Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory. Rather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly. The study implements a custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has a dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, the study leverages the inherent sparsity of softmax and propose to skip elements of the gradient computation that have a negligible (i.e., below numerical precision) contribution to the gradient. Experiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence. Here is [code](https://github.com/apple/ml-cross-entropy). <br> <br>

17. ***LLMs simulate journalistic planning.  <br>Research investigates how journalists select sources in news writing and applies Bayesian methods to predict source-selection schemas. The study provides a framework for understanding planning in long-form text generation, with implications for improving LLM-generated content.*** <br> <br>
    Nov 13, Uni of Southern California, UC Berkeley and Stanford Uni published a [paper](https://aclanthology.org/2024.findings-emnlp.930.pdf) “Explaining Mixtures of Sources in News Articles”. Human writers plan, then write. For large language models (LLMs) to play a role in longer-form article generation, people must understand the planning steps humans make before writing. The study explores one kind of planning, source-selection in news, as a case-study for evaluating plans in long-form generation. The authors ask: why do specific stories call for specific kinds of sources? The study imagines a generative process for story writing where a source-selection schema is first selected by a journalist, and then sources are chosen based on categories in that schema. Learning the article’s plan means predicting the schema initially chosen by the journalist. Working with professional journalists, the study adapt five existing schemata and introduce three new ones to describe journalistic plans for the inclusion of sources in documents. Then, inspired by Bayesian latent-variable modeling, the study develops metrics to select the most likely plan, or schema, underlying a story, which is used to compare schemata. The study finds that two schemata: stance and social affiliation best explain source plans in most documents. However, other schemata like textual entailment explain source plans in factually rich topics like “Science”. Finally, the study finds the authors can predict the most suitable schema given just the article’s headline with reasonable accuracy. This can been seen as an important case-study for human planning, and provides a framework and approach for evaluating other kinds of plans. The authors release a corpora, NewsSources, with annotations for 4M articles. <br> <br>

19. ***TTT enhances reasoning in novel tasks.  <br>MIT demonstrates that test-time training (TTT) significantly improves LLM performance on abstract reasoning benchmarks like ARC. By adapting model parameters during inference, TTT achieves state-of-the-art results, suggesting an alternative to symbolic reasoning.*** <br> <br>
    Nov 12, MIT published a [paper](https://ekinakyurek.github.io/papers/ttt.pdf) “The Surprising Effectiveness of Test-Time Training for Abstract Reasoning”. Language models have shown impressive performance on tasks within their training distribution, but often struggle with novel problems requiring complex reasoning. The study investigates the effectiveness of test-time training (TTT)—updating model parameters temporarily during inference using a loss derived from input data—as a mechanism for improving models’ reasoning capabilities, using the Abstraction and Reasoning Corpus (ARC) as a benchmark. Through systematic experimentation, the study identifies three crucial components for successful TTT: (1) initial finetuning on similar tasks (2) auxiliary task format and augmentations (3) per-instance training. TTT significantly improves performance on ARC tasks, achieving up to 6× improvement in accuracy compared to base fine-tuned models; applying TTT to a 8B-parameter language model, the study achieves 53% accuracy on the ARC’s public validation set, improving the state-of-the-art by nearly 25% for public and purely neural approaches. By ensembling the method with recent program generation approaches, the study gets SoTA public validation accuracy of 61.9%, matching the average human score. The findings suggest that explicit symbolic search is not the only path to improved abstract reasoning in neural language models; additional test-time applied to continued training on few-shot examples can also be extremely effective. <br> <br>

21. ***Bigger models aren't always better teachers.  <br>A study challenges assumptions that larger LLMs are better for instruction tuning. Findings show that compatibility between teacher and base models is more crucial than size, prompting the development of new evaluation metrics like Compatibility-Adjusted Reward (CAR).*** <br> <br>
    Nov 12, Uni of Washington and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2411.07133) “Stronger Models are NOT Stronger Teachers for Instruction Tuning”. Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. This study challenges this commonly-adopted assumption. The extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. The authors refer to this phenomenon as the Larger Models' Paradox and observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. The study thus develops a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Experiments across five base models demonstrate that CAR outperforms almost all baselines. Main finds include1) larger response generators not equal improved instruction-following capabilities; 2) learning from response generators within the same model family leads to higher performance; 3) Open-source LLMs can outperform close-source LLMs as response generators; 4) Higher temperature and top-p enhance instruction-following capabilities. <br> <br>

23. ***LLMs show limited implicit communication skills.  <br>Arizona State's study introduces ExpressivityArena to assess LLMs’ ability to convey implicit cues in tasks like poetry and coding. While LLMs demonstrate some expressive capabilities, limitations remain, guiding future research on enhancing conversational nuance.*** <br> <br>
    Nov 12, Arizona State Uni published a [paper](https://arxiv.org/pdf/2411.08010v1) “ExpressivityArena: Can LLMs Express Information Implicitly?”. While Large Language Models (LLMs) have demonstrated remarkable performance in certain dimensions, their ability to express implicit language cues that human use for effective communication remains unclear. This paper presents ExpressivityArena, a Python library for measuring the implicit communication abilities of LLMs. The study provides a comprehensive framework to evaluate expressivity of arbitrary LLMs and explore its practical implications. To this end, the authors refine the definition and measurements of “expressivity,” and use the framework in a set of small experiments. These experiments test LLMs in creative and logical tasks such as poetry, coding, and emotion-based responses. They are then evaluated by an automated grader, through ExpressivityArena, which the work verifies to be the most pragmatic for testing expressivity. Building on these experiments, the work deepen the understanding of the expressivity of LLMs by assessing their ability to remain expressive in conversations. The findings indicate that LLMs are capable of generating and understanding expressive content, however, with some limitations. These insights will inform the future development and deployment of expressive LLMs.  <br> <br>

25. ***WAM enables precise watermarking.  <br>The Watermark Anything Model (WAM) introduces advanced localized image watermarking. Capable of embedding imperceptible messages into small areas, WAM offers robust watermarking even under challenging conditions, broadening applications in digital content security.*** <br> <br>
    Nov 11, Meta, Ecole Polytechnique and Inria Rennes published a [paper](https://arxiv.org/pdf/2411.07231) “Watermark Anything with Localized Messages”. Image watermarking methods are not tailored to handle small watermarked areas. This restricts applications in real-world scenarios where parts of the image may come from different sources or have been edited. The study introduces a deep-learning model for localized image watermarking, dubbed the Watermark Anything Model (WAM). The WAM embedder imperceptibly modifies the input image, while the extractor segments the received image into watermarked and non-watermarked areas and recovers one or several hidden messages from the areas found to be watermarked. The models are jointly trained at low resolution and without perceptual constraints, then post-trained for imperceptibility and multiple watermarks. Experiments show that WAM is competitive with state-of-the art methods in terms of imperceptibility and robustness, especially against inpainting and splicing, even on high-resolution images. Moreover, it offers new capabilities: WAM can locate watermarked areas in spliced images and extract distinct 32-bit messages with less than 1 bit error from multiple small regions - no larger than 10% of the image surface - even for small 256times 256 images. <br> <br>

27. ***Fine-tuning struggles to update LLMs reliably.  <br>Stanford’s FineTuneBench reveals that commercial fine-tuning APIs are inefficient at learning new or updated knowledge, with low generalization accuracy. The findings highlight limitations in current methods and underscore the need for more effective fine-tuning strategies.*** <br> <br>
    Nov 11, Stanford Uni published a [paper](https://arxiv.org/pdf/2411.05059) “FineTuneBench: How well do commercial fine-tuning APIs infuse knowledge into LLMs?”. There is great interest in fine-tuning frontier large language models (LLMs) to inject new information and update existing knowledge. While commercial LLM fine-tuning APIs from providers such as OpenAI and Google promise flexible adaptation for various applications, the efficacy of fine-tuning remains unclear. This study introduces FineTuneBench, an evaluation framework and dataset for understanding how well commercial fine-tuning APIs can successfully learn new and updated knowledge. The work analyzes five frontier LLMs with commercially available fine-tuning APIs, including GPT-4o and Gemini 1.5 Pro, on their effectiveness in two settings: (1) ingesting novel information, such as recent news events and new people profiles, and (2) updating existing knowledge, such as updated medical guidelines and code frameworks. The results reveal substantial shortcomings in all the models' abilities to effectively learn new information through fine-tuning, with an average generalization accuracy of 37% across all models. When updating existing knowledge, such as incorporating medical guideline updates, commercial fine-tuning APIs show even more limited capability (average generalization accuracy of 19%). Overall, fine-tuning GPT-4o mini is the most effective for infusing new knowledge and updating knowledge, followed by GPT-3.5 Turbo and GPT-4o. The fine-tuning APIs for Gemini 1.5 Flesh and Gemini 1.5 Pro are unable to learn new knowledge or update existing knowledge. These findings underscore a major shortcoming in using current commercial fine-tuning services to achieve reliable knowledge infusion in common scenarios. The authors open source the FineTuneBench dataset at this https URL. <br> <br>

29. ***REMIX mitigates knowledge loss.  <br>Princeton introduces REMIX, a strategy for preventing LLM forgetting during continual training. By mixing unrelated data into the learning process, REMIX reduces interference and enhances memory retention, paving the way for more robust knowledge updates.*** <br> <br>
    Nov 11, Princeton Uni published a [paper](https://arxiv.org/pdf/2411.07175) “Continual Memorization of Factoids in Large Language Models”. Large language models can absorb a massive amount of knowledge through pretraining, but pretraining is inefficient for acquiring long-tailed or specialized facts. Therefore, fine-tuning on specialized or new knowledge that reflects changes in the world has become popular, though it risks disrupting the model's original capabilities. The authors study this fragility in the context of continual memorization, where the model is trained on a small set of long-tail factoids (factual associations) and must retain these factoids after multiple stages of subsequent training on other datasets. Through extensive experiments, the study shows that LLMs suffer from forgetting across a wide range of subsequent tasks, and simple replay techniques do not fully prevent forgetting, especially when the factoid datasets are trained in the later stages. The work posits that there are two ways to alleviate forgetting: 1) protect the memorization process as the model learns the factoids, or 2) reduce interference from training in later stages. With this insight, the study develops an effective mitigation strategy: REMIX (Random and Generic Data Mixing). REMIX prevents forgetting by mixing generic data sampled from pretraining corpora or even randomly generated word sequences during each stage, despite being unrelated to the memorized factoids in the first stage. REMIX can recover performance from severe forgetting, often outperforming replay-based methods that have access to the factoids from the first stage. The authors then analyze how REMIX alters the learning process and find that successful forgetting prevention is associated with a pattern: the model stores factoids in earlier layers than usual and diversifies the set of layers that store these factoids. The efficacy of REMIX invites further investigation into the underlying dynamics of memorization and forgetting, opening exciting possibilities for future research. <br> <br>

31. ***OpenAI's Challenge with Slowing AI Improvement.  <br>
OpenAI is reportedly encountering a slowdown in performance improvements for its models, with its upcoming model, "Orion," showing less advancement compared to previous transitions (e.g., GPT-3 to GPT-4). Orion's performance might even lag in specific areas like coding. OpenAI has formed a foundational team to address this by exploring strategies such as using synthetic data for training and enhancing post-training processes. The company has denied plans to release Orion this year.*** <br> <br>
    Nov 9, according to [techcrunch.com](https://techcrunch.com/2024/11/09/openai-reportedly-developing-new-strategies-to-deal-with-ai-improvement-slowdown/), OpenAI reportedly developing new strategies to deal with AI improvement slowdown. OpenAI’s next flagship model might not represent as big a leap forward as its predecessors, according to a new report in The Information. Employees who tested the new model, code-named Orion, reportedly found that even though its performance exceeds OpenAI’s existing models, there was less improvement than they’d seen in the jump from GPT-3 to GPT-4. In other words, the rate of improvement seems to be slowing down. In fact, Orion might not be reliably better than previous models in some areas, such as coding. In response, OpenAI has created a foundations team to figure out how the company can continue to improve its models in the face of a dwindling supply of new training data. These new strategies reportedly include training Orion on synthetic data produced by AI models, as well as doing more to improve models during the post-training process. OpenAI did not immediately respond to a request for comment. In response to previous reports about plans for its flagship model, the company said, “We don’t have plans to release a model code-named Orion this year.” <br> <br>

33. ***Efficient Inference with Recycled Attention.  <br>
A study by NYU, Cornell, and UT Austin introduces "Recycled Attention," an inference method for long-context language models. This approach alternates between full-context and partial attention, recycling patterns from prior computations to improve efficiency. It demonstrates superior speed and performance on long-context tasks compared to previous methods and explores dynamic attention-switching strategies and continued pretraining for further optimization.*** <br> <br>
    Nov 8, NYU, Uni of Cornell and Uni of Texas at Austin published a [paper](https://arxiv.org/pdf/2411.05787v1) “Recycled Attention: Efficient inference for long-context language models”. Generating long sequences of tokens given a long-context input imposes a heavy computational burden for large language models (LLMs). One of the computational bottleneck comes from computing attention over a long sequence of input at each generation step. This study proposes Recycled Attention, an inference-time method which alternates between full context attention and attention over a subset of input tokens. When performing partial attention, the approach recycles the attention pattern of a previous token that has performed full attention and attend only to the top K most attended tokens, reducing the cost of data movement and attention computation. Compared to previously proposed inference-time acceleration method which attends only to local context or tokens with high accumulative attention scores, the proposed approach flexibly chooses tokens that are relevant to the current decoding step. The study evaluate the methods on RULER, a suite of tasks designed to comprehensively evaluate long-context abilities, and long-context language modeling tasks. Applying the method to off-the-shelf LLMs achieves comparable speedup to baselines which only consider local context while improving the performance by 2x. The study further explores two ideas to improve performance-efficiency trade-offs: (1) dynamically decide when to perform recycled or full attention step based on the query similarities and (2) continued pre-training the model with Recycled Attention. <br> <br>

35. ***Precision-Aware Scaling Laws for LLMs.  <br>
Harvard, Stanford, MIT, Databricks, and CMU propose precision-aware scaling laws for low-precision training and inference, addressing cost and quality trade-offs in LLMs. These laws predict the quality degradation from reduced precision and show how post-training quantization degrades performance as training data scales. This unified model helps optimize training and inference across precisions.*** <br> <br>
    Nov 7, Harvard Uni, Stanford Uni, MIT, Databricks and CMU published a [paper](https://arxiv.org/pdf/2411.04330) “Scaling Laws for Precision”. Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. This study devises "precision-aware" scaling laws for both training and inference. The study proposes that training in lower precision reduces the model's "effective parameter count," allowing to predict the additional loss incurred from training in low precision and post-train quantization. For inference, the study finds that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, the scaling laws allow to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision may be compute optimal. The authors unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. The study fits on over 465 pretraining runs and validate the predictions on model sizes up to 1.7B parameters trained on up to 26B tokens. <br> <br>

37. ***Identifying LLM Hosting Platforms.  <br>
Imperial College, Cambridge, and Google develop a method to verify the hardware and software platforms used in LLM inference. The technique, called Hardware and Software Platform Inference (HSPI), analyzes output patterns to identify GPU architectures and software stacks. Testing shows up to 100% accuracy in white-box scenarios and significant success in black-box cases, addressing concerns over mismatched services in AI hosting.*** <br> <br>
    Nov 7, Imperial College London, Uni of Cambridge and Google published a [paper](https://arxiv.org/pdf/2411.05197) “Hardware and Software Platform Inference”. It is now a common business practice to buy access to large language model (LLM) inference rather than self-host, because of significant upfront hardware infrastructure and energy costs. However, as a buyer, there is no mechanism to verify the authenticity of the advertised service including the serving hardware platform, e.g. that it is actually being served using an NVIDIA H100. Furthermore, there are reports suggesting that model providers may deliver models that differ slightly from the advertised ones, often to make them run on less expensive hardware. That way, a client pays premium for a capable model access on more expensive hardware, yet ends up being served by a (potentially less capable) cheaper model on cheaper hardware. This work introduces hardware and software platform inference (HSPI) -- a method for identifying the underlying GPU architecture and software stack of a (black-box) machine learning model solely based on its input-output behavior. The method leverages the inherent differences of various GPU architectures and compilers to distinguish between different GPU types and software stacks. By analyzing the numerical patterns in the model's outputs, the study proposes a classification framework capable of accurately identifying the GPU used for model inference as well as the underlying software configuration. The findings demonstrate the feasibility of inferring GPU type from black-box models. The authors evaluate HSPI against models served on different real hardware and find that in a white-box setting the researcher can distinguish between different GPUs with between 83.9% and 100% accuracy. Even in a black-box setting the model is able to achieve results that are up to three times higher than random guess accuracy. <br> <br>

39. ***Unlocking Latent Reasoning in LLMs.  <br>
Salesforce introduces LaTent Reasoning Optimization (LaTRO), a framework enhancing LLM reasoning through self-reward mechanisms rather than external feedback. LaTRO achieves significant accuracy gains on reasoning datasets like GSM8K, demonstrating LLMs' untapped reasoning potential. This approach offers a scalable, self-improvement pathway for reasoning tasks, with code available on GitHub.*** <br> <br>
    Nov 6, Saleforce published a [paper](https://arxiv.org/pdf/2411.04282) “Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding”. Large language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time, optimizing reasoning capabilities during training remains challenging. The work introduces LaTent Reasoning Optimization (LaTRO), a principled framework that formulates reasoning as sampling from a latent distribution and optimizes it via variational approaches. LaTRO enables LLMs to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring external feedback or reward models. The study validates LaTRO through experiments on GSM8K and ARC-Challenge datasets using multiple model architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of 12.5% over base models and 9.6% over supervised fine-tuning across Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. The findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked and enhanced through the proposed optimization approach in a self-improvement manner. The code of LaTRO is available at https://github.com/SalesforceAIResearch/LaTRO. <br> <br>
    
41. ***Counting Abilities and Tokenization in LLMs.  <br>
A study by UBC and Stony Brook University examines how Transformers' architectural limitations affect counting tasks, a critical reasoning component. While Chain of Thought (CoT) reasoning partially addresses these issues, tokenization strategies (e.g., byte-level vs. character-level) significantly impact performance. The findings suggest revisiting tokenization methods to enhance LLM reasoning capabilities.*** <br> <br>
    21.	Oct 29, Uni of British Columbia and Stony Brook Uni published a paper “Counting Ability of Large Language Models and Impact of Tokenization”. Transformers, the backbone of modern large language models (LLMs), face inherent architectural limitations that impede their reasoning capabilities. Unlike recurrent networks, Transformers lack recurrent connections, confining them to constant-depth computation. This restriction places them in the complexity class TC0, making them theoretically incapable of solving tasks that demand increasingly deep reasoning as input length grows. Counting, a fundamental component of many reasoning tasks, also requires reasoning depth to grow linearly to be performed inductively. While previous studies have established the upper limits of counting ability in Transformer-based expert models (i.e., models specifically trained for counting tasks), these findings do not directly extend to general-purpose LLMs due to differences in reasoning mechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning can help alleviate some of the architectural limitations of Transformers in counting tasks. However, little attention has been paid to the role of tokenization in these models. Unlike expert models that often use character-level tokenization, LLMs typically rely on byte-level (BPE) tokenizers, which fundamentally alters the way reasoning is processed. This work investigates the impact of tokenization on the counting abilities of LLMs, uncovering substantial performance variations based on input tokenization differences. The work provides both theoretical and experimental analyses, offering insights into how tokenization choices can undermine models' theoretical computability, thereby inspiring the design of new tokenization methods to enhance reasoning in LLMs.
 <br> <br> <br>


***Nov 10***

1. ***Meta and Stanford propose a sparse multi-modal transformer architecture to optimize computational efficiency:  <br>The paper introduces the Mixture-of-Transformers (MoT), which decouples non-embedding parameters for different modalities, improving multi-modal processing efficiency. In tests, MoT achieves similar performance to dense models with significantly reduced FLOPs and faster processing times on large datasets.*** <br> <br>
   Nov 8, Meta and Stanford Uni published a [paper](https://arxiv.org/pdf/2411.04996) “Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models”.  The development of large language models (LLMs) has expanded to multi-modal systems capable of processing text, images, and speech within a unified framework. Training these models demands significantly larger datasets and computational resources compared to text-only LLMs. To address the scaling challenges, the study introduces Mixture-of-Transformers (MoT), a sparse multi-modal transformer architecture that significantly reduces pretraining computational costs. MoT decouples non-embedding parameters of the model by modality -- including feed-forward networks, attention matrices, and layer normalization -- enabling modality-specific processing with global self-attention over the full input sequence. The study evaluates MoT across multiple settings and model scales. In the Chameleon 7B setting (autoregressive text-and-image generation), MoT matches the dense baseline's performance using only 55.8% of the FLOPs. When extended to include speech, MoT reaches speech performance comparable to the dense baseline with only 37.2% of the FLOPs. In the Transfusion setting, where text and image are trained with different objectives, a 7B MoT model matches the image modality performance of the dense baseline with one third of the FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image generation metrics. System profiling further highlights MoT's practical benefits, achieving dense baseline image quality in 47.2% of the wall-clock time and text quality in 75.6% of the wall-clock time (measured on AWS p4de.24xlarge instances with NVIDIA A100 GPUs). <br> <br>

3. ***MIT, Harvard, and Johns Hopkins present an inverse generative model for few-shot task learning:  <br>Their Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM) learns new task concepts with few examples using pre-trained generative models, improving agent behavior prediction across varied domains like navigation and manipulation.*** <br> <br>
   Nov 7, MIT, Harvard Uni and Johns Hopkins Uni published a [paper](https://arxiv.org/pdf/2411.04987v1) “Few-Shot Task Learning through Inverse Generative Modeling”. Learning the intents of an agent, defined by its goals or motion style, is often extremely challenging from just a few examples. The study refers to this problem as task concept learning and present an approach, Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM), which learns new task concepts by leveraging invertible neural generative models. The core idea is to pretrain a generative model on a set of basic concepts and their demonstrations. Then, given a few demonstrations of a new concept (such as a new goal or a new action), the method learns the underlying concepts through backpropagation without updating the model weights, thanks to the invertibility of the generative model. The authors evaluate the method in five domains – object rearrangement, goal-oriented navigation, motion caption of human actions, autonomous driving, and real-world table-top manipulation. The experimental results demonstrate that via the pretrained generative model, the study successfully learns novel concepts and generate agent plans or motion corresponding to these concepts in (1) unseen environments and (2) in composition with training concepts. <br> <br>

5. ***MIT and partners hypothesize that language models share semantic representations across languages and modalities:  <br>Known as the "semantic hub hypothesis," this shared space enables language models to process inputs from different languages and modalities, creating predictable cross-modal effects in model outputs.*** <br> <br>
   Nov 7, MIT, Uni of Southern California and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2411.04986v1) “The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities”. Modern language models can process inputs across diverse languages and modalities. The study hypothesizes that models acquire this capability through learning a shared representation space across heterogeneous data types (e.g., different languages and modalities), which places semantically similar inputs near one another, even if they are from different modalities/languages. The authors term this the semantic hub hypothesis, following the hub-and-spoke model from neuroscience (Patterson et al., 2007) which posits that semantic knowledge in the human brain is organized through a transmodal semantic “hub” which integrates information from various modality-specific “spokes” regions. The study first shows that model representations for semantically equivalent inputs in different languages are similar in the intermediate layers, and that this space can be interpreted using the model’s dominant pretraining language via the logit lens. This tendency extends to other data types, including arithmetic expressions, code, and visual/audio inputs. Interventions in the shared representation space in one data type also predictably affect model outputs in other data types, suggesting that this shared representations space is not simply a vestigial byproduct of large-scale training on broad data, but something that is actively utilized by the model during input processing. <br> <br>

7. ***Cambridge and HKU explore LLMs' capabilities in following threads in extensive contexts:  <br>Their research finds that while many LLMs handle multiple threads well, accuracy declines as the context window grows. The study also shows that token counts from different tokenizers vary significantly, impacting context limits.*** <br> <br>
   Nov 7, Uni of Cambridge and The Uni of Hong Kong published a [paper](https://arxiv.org/pdf/2411.05000) “Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?”. As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context LLMs appear well-suited to this form of complex information retrieval and reasoning, which has traditionally proven costly and time-consuming. However, although the development of longer context models has seen rapid gains in recent years, the understanding of how effectively LLMs use their context has not kept pace. To address this, the study conducts a set of retrieval experiments designed to evaluate the capabilities of 17 leading LLMs, such as their ability to follow threads of information through the context window. Strikingly, the work finds that many models are remarkably threadsafe: capable of simultaneously following multiple threads without significant loss in performance. Still, for many models, the study finds the effective context limit is significantly shorter than the supported context length, with accuracy decreasing as the context window grows. The study also highlights the important point that token counts from different tokenizers should not be directly compared -- they often correspond to substantially different numbers of written characters. The authors release the code and long-context experimental data. <br> <br>

9. ***Meta, UNC, and NYU introduce Self-Consistency Preference Optimization (ScPO) for unsupervised model improvement:  <br>ScPO iteratively trains models to prioritize consistent over inconsistent answers, closing the gap with supervised models on reasoning tasks and showing notable improvements in reasoning accuracy.*** <br> <br>
    Nov 6, Meta, UNC Chapel Hill and NYU published a [paper](https://arxiv.org/pdf/2411.04109) “Self-Consistency Preference Optimization”.  Self-alignment, whereby models learn to improve themselves without human annotation, is a rapidly growing research area. However, existing techniques often fail to improve complex reasoning tasks due to the difficulty of assigning correct rewards. An orthogonal approach that is known to improve correctness is self-consistency, a method applied at inference time based on multiple sampling in order to find the most consistent answer. This work extends the self-consistency concept to help train models. The study thus introduces self-consistency preference optimization (ScPO), which iteratively trains consistent answers to be preferred over inconsistent ones on unsupervised new problems. The work shows ScPO leads to large improvements over conventional reward model training on reasoning tasks such as GSM8K and MATH, closing the gap with supervised training with gold answers or preferences, and that combining ScPO with standard supervised learning improves results even further. On ZebraLogic, ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and Claude-3 Haiku. <br> <br>

11. ***UCL, Boston University, and Meta analyze the effects of evaluation data contamination in LLMs:  <br>Using a novel ConTAM metric, the study shows how contamination impacts benchmark scores and suggests that longer contaminated strings provide clearer insights into performance inflation.*** <br> <br>
    Nov 6, UCL, Boston Uni, Cohere and Meta published a [paper](https://arxiv.org/pdf/2411.03923v1) “Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?”. Evaluation data contamination, the inadvertent mixing of samples from evaluation benchmarks into pre-training corpora, constitutes a recently growing and important concern in the field of evaluating large language models (LLMs). The resulting ‘training on the test set’ makes it difficult to interpret evaluation benchmark scores, and an active area of research studies its effects. However, while evaluation data contamination is easily understood intuitively, it is surprisingly difficult to define precisely which examples should be considered contaminated and, consequently, to what extent contamination inflates the corresponding benchmark scores. This work proposes that these questions should be addressed together and that contamination metrics can be assessed based on whether the examples they mark contaminated indeed give models an undue advantage. The study proposes a novel analysis method called ConTAM, and shows – in a large scale survey of existing and novel n-gram based contamination metrics across 13 benchmarks and 7 models from 2 different families – that ConTAM can be used to better understand evaluation data contamination as well as its effects on benchmark scores. The study finds that contamination may have a much larger effect than reported in recent LLM releases and that there are differences in the extent to which models at different scale are impacted by contamination. Furthermore, the authors find that considering only the longest contaminated substring generally provides a better signal than considering a union of all contaminated substrings, as was common practice in previous studies, and that doing model and benchmark specific threshold analysis greatly increases the specificity of the results. Lastly, the study investigates the impact of various hyperparameter choices of contamination studies, finding that – among other things – both using larger values of n and disregarding contaminated strings that are infrequent in the pre-training data lead to many false negatives. With ConTAM, the work provides a method to empirically ground evaluation data contamination metrics in downstream effects as well as measure their magnitude. With the exploration, the work sheds light on how evaluation data contamination can impact LLMs and provide insight into the various considerations important when doing contamination analysis. The authors end the paper by discussing these in more detail and providing concrete suggestions for future work. <br> <br>

13. ***Peking University examines large language models' numerical understanding with the Number Cookbook benchmark:  <br>The study highlights LLMs' weaknesses in basic numerical processing, offering a new benchmark to evaluate models and exploring techniques like chain-of-thought to improve numerical understanding.*** <br> <br>
    Nov 6, Peking Uni published a [paper](https://arxiv.org/pdf/2411.03766v1) “Number Cookbook: Number Understanding of Language Models and How to Improve It”. Large language models (LLMs) can solve an increasing number of complex reasoning tasks while making surprising mistakes in basic numerical understanding and processing (such as 9.11> 9.9). The latter ability is essential for tackling complex arithmetic and mathematical problems and serves as a foundation for most reasoning tasks, but previous work paid little attention to it or only discussed several restricted tasks (like integer addition). This paper comprehensively investigates the numerical understanding and processing ability (NUPA) of LLMs. Firstly, the study introduces a benchmark covering four common numerical representations and 17 distinct numerical tasks in four major categories, resulting in 41 meaningful combinations in total. These tasks are derived from primary and secondary education curricula, encompassing nearly all everyday numerical understanding and processing scenarios, and the rules of these tasks are very simple and clear. Through the benchmark, the work finds that current LLMs fail frequently in many of the tasks. To study the problem, the work trains small models with existing and potential techniques for enhancing NUPA (such as special tokenizers, PEs, and number formats), comprehensively evaluating their effectiveness using the testbed. The study also finetunes practical-scale LLMs on the proposed NUPA tasks and find that 1) naive finetuning can improve NUPA a lot on many but not all tasks, and 2) surprisingly, techniques designed to enhance NUPA prove ineffective for finetuning pretrained models. The study further explores the impact of chain-of-thought techniques on NUPA. The work takes a preliminary step towards understanding and improving NUPA of LLMs. The benchmark and code are released at https://github.com/GraphPKU/number_cookbook. <br> <br>

15. ***UCL and Google propose a soft parameter reset for non-stationary learning in neural networks:  <br>This technique adapts to non-stationary distributions by reverting parameters towards initial values, improving performance in non-stationary learning scenarios like continual learning and contextual bandits.*** <br> <br>
    Nov 6, UCL and Google published a [paper](https://openreview.net/pdf?id=fDiZJ7mmOV) on NeurIPS “Non-Stationary Learning of Neural Networks with Automatic Soft Parameter Reset”. Neural networks are traditionally trained under the assumption that data come from a stationary distribution. However, settings which violate this assumption are becoming more popular; examples include supervised learning under distributional shifts, reinforcement learning, continual learning and non-stationary contextual bandits. This study introduces a novel learning approach that automatically models and adapts to non-stationarity, via an Ornstein-Uhlenbeck process with an adaptive drift parameter. The adaptive drift tends to draw the parameters towards the initialisation distribution, so the approach can be understood as a form of soft parameter reset. The authors show empirically that the approach performs well in non-stationary supervised and off-policy reinforcement learning settings. <br> <br>

17. ***Stanford introduces a gradient method with online scaling for accelerated convergence:  <br>This approach optimally scales gradients at each iteration, achieving faster convergence in optimization tasks compared to traditional methods, especially in smooth convex optimization.*** <br> <br>
    Nov 5, Stanford Uni published a [paper](https://arxiv.org/pdf/2411.01803) “Gradient Methods with Online Scaling”. The study introduces a framework to accelerate the convergence of gradient-based methods with online learning. The framework learns to scale the gradient at each iteration through an online learning algorithm and provably accelerates gradient-based methods asymptotically. In contrast with previous literature, where convergence is established based on worst-case analysis, this framework provides a strong convergence guarantee with respect to the optimal scaling matrix for the iteration trajectory. For smooth strongly convex optimization, the results provide an O(κ⋆log(1/ε)) complexity result, where κ⋆ is the condition number achievable by the optimal preconditioner, improving on the previous O(√nκ⋆log(1/ε)) result. In particular, a variant of the method achieves superlinear convergence on convex quadratics. For smooth convex optimization, the work shows for the first time that the widely-used hypergradient descent heuristic improves on the convergence of gradient descent. <br> <br>

19. ***CMU and Bosch suggest VLMs can perform optimally with fewer visual tokens and larger models:  <br>They find that reducing visual tokens while maximizing model size minimizes inference compute without sacrificing performance, challenging current token reduction practices.*** <br> <br>
    Nov 5, CMU and Bosch published a [paper](https://arxiv.org/pdf/2411.03312) “Inference Optimal VLMs Need Only One Visual Token but Larger Models”. Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks. However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM. To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression. However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance. The study first characterizes this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. The results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., 5-10times), the results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, the study takes some initial steps towards building approaches tailored for high token compression settings. Code is available at https://github.com/locuslab/llava-token-compression. <br> <br>

21. ***University of Tokyo develops ADOPT, an adaptive gradient method ensuring convergence without bounded gradient assumptions:  <br>ADOPT improves on Adam's convergence across tasks by altering momentum calculations, proving effective in tasks like image classification and NLP.*** <br> <br>
    Nov 5, The of Tokyo published a [paper](https://arxiv.org/pdf/2411.02853) “ADOPT: Modified Adam Can Converge with Any β2 with the Optimal Rate”. Adaptive gradient methods based on exponential moving averages, such as Adam and RMSprop, are widely used for deep learning. However, it is known that they do not converge unless choosing hyperparameters in a problem-dependent manner. There have been many attempts to fix their convergence (e.g., AMSGrad), but they require an impractical assumption that the stochastic gradient is uniformly bounded. This study propose as new adaptive gradient method named ADOPT, which achieves the optimal convergence rate of O(1/√T)  with any hyperparameter choice without the bounded stochastic gradient assumption. ADOPT addresses the non-convergence issue of Adam by removing the current gradient from the second moment estimate and changing the order of the momentum calculation and the scaling operation by the second moment estimate. The study also conducts intensive numerical experiments, and verify that ADOPT achieves competitive or even better results compared to Adam and its variants across a wide range of tasks, including image classification, generative modeling, natural language processing, and deep reinforcement learning. <br> <br>

23. ***Databricks examines long-context RAG performance in LLMs and its limitations:  <br>Findings reveal that few models maintain accuracy above 64k tokens, identifying long-context scenarios where RAG remains beneficial but challenging.*** <br> <br>
    Nov 5, Databricks published a [paper](https://arxiv.org/pdf/2411.03538v1) “Long Context RAG Performance of Large Language Models”. Retrieval Augmented Generation (RAG) has emerged as a crucial technique for enhancing the accuracy of Large Language Models (LLMs) by incorporating external information. With the advent of LLMs that support increasingly longer context lengths, there is a growing interest in understanding how these models perform in RAG scenarios. Can these new long context models improve RAG performance? This paper presents a comprehensive study of the impact of increased context length on RAG performance across 20 popular open source and commercial LLMs. The study ran RAG workflows while varying the total context length from 2,000 to 128,000 tokens (and 2 million tokens when possible) on three domain-specific datasets, and reports key insights on the benefits and limitations of long context in RAG applications. The findings reveal that while retrieving more documents can improve performance, only a handful of the most recent state of the art LLMs can maintain consistent accuracy at long context above 64k tokens. The authors also identify distinct failure modes in long context scenarios, suggesting areas for future research. <br> <br>

25. ***Neural Magic and IST Austria study trade-offs in LLM quantization for faster inference:  <br>Evaluating formats like FP8 and INT8, they find that lower-bit quantization maintains performance well, with INT4 offering cost-efficient deployment, especially in asynchronous scenarios.*** <br> <br>
    Nov 4, Neural Magic and Inst of Sci and Tech Austria published a [paper](https://arxiv.org/pdf/2411.02355) “"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization”. Despite the popularity of large language model (LLM) quantization for inference acceleration, significant uncertainty remains regarding the accuracy-performance trade-offs associated with various quantization formats. The study presents a comprehensive empirical study of quantized accuracy, evaluating popular quantization formats (FP8, INT8, INT4) across academic benchmarks and real-world tasks, on the entire Llama-3.1 model family. Additionally, the study examines the difference in text generated by quantized models versus their uncompressed counterparts. Beyond benchmarks, the work also presents a couple of quantization improvements which allowed to obtain state-of-the-art accuracy recovery results. The investigation, encompassing over 500,000 individual evaluations, yields several key findings: (1) FP8 weight and activation quantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and activation quantization (W8A8-INT), when properly tuned, incurs surprisingly low 1-3% accuracy degradation, and (3) INT4 weight-only quantization (W4A16-INT) is competitive with 8-bit integer weight and activation quantization. To address the question of the "best" format for a given deployment environment, the authors conduct inference performance analysis using the popular open-source vLLM framework on various GPU architectures. The study finds that W4A16 offers the best cost-efficiency for synchronous deployments, and for asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel in asynchronous "continuous batching" deployment of mid- and large-size models on high-end GPUs. The results provide a set of practical guidelines for deploying quantized LLMs across scales and performance requirements. <br> <br>

27. ***AMD launches open-source language models for efficient local use on AMD platforms:  <br>The 1B parameter models, trained on AMD GPUs, excel in reasoning and instruction-following, offering privacy-focused, energy-efficient solutions for specialized applications.*** <br> <br>
    Nov 4, [according to AM Community](https://community.amd.com/t5/ai/introducing-the-first-amd-1b-language-models-amd-olmo/ba-p/721253), AMD released its IB language models. AMD has introduced its first series of fully open 1 billion parameter language models, AMD OLMo, continuing its tradition of open-sourcing models and code. These models are designed to be pre-trained and fine-tuned for domain-specific applications, allowing for better alignment with unique use cases. The AMD OLMo models are pre-trained with 1.3 trillion tokens on AMD Instinct GPUs and include three checkpoints: AMD OLMo 1B, AMD OLMo 1B SFT, and AMD OLMo 1B SFT DPO. These models demonstrate improved performance in reasoning, instruction-following, and chat capabilities compared to other similar-sized open-source models. AMD has also made these models accessible for local use on AMD Ryzen AI PCs, emphasizing privacy, efficiency, and lower power consumption. The initiative aims to empower the community to innovate and advance AI research. <br> <br>

29. ***Duke and Google enhance LLM factual accuracy with Self Logits Evolution Decoding (SLED):  <br>SLED improves output truthfulness by refining logits across model layers, achieving up to 20% better factuality without additional latency and supporting various model types and sizes.*** <br> <br>
    Nov 1, Duke Uni and Google published a [paper](https://arxiv.org/pdf/2411.02433) “SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models”. Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, the study introduces Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, the SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (LLaMA 2, LLaMA 3, Gemma) and scales (from 2B to 70B), including more advanced architectural configurations such as the mixture of experts (MoE). The evaluation spans a wide variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thought reasoning tasks. The results demonstrate that SLED consistently improves factual accuracy by up to 20% compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance. <br> <br>

31. ***Specialized Sparse Autoencoders (SSAEs) for Foundation Models:  <br>To better interpret rare concepts in foundation models, CMU introduced SSAEs, which focus on specific subdomains. These models outperform general-purpose SAEs in capturing subdomain-tail concepts. In a Bias in Bios case study, SSAEs reduced spurious gender information, enhancing classification accuracy by 12.5%.*** <br> <br>
    Nov 1, CMU published a [paper](https://arxiv.org/pdf/2411.00743) “Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models”. Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability methods. Sparse Autoencoders (SAEs) have emerged as a promising tool for disentangling FM representations, but they struggle to capture rare, yet crucial concepts in the data. The study introduces Specialized Sparse Autoencoders (SSAEs), designed to illuminate these elusive dark matter features by focusing on specific subdomains. The study presents a practical recipe for training SSAEs, demonstrating the efficacy of dense retrieval for data selection and the benefits of Tilted Empirical Risk Minimization as a training objective to improve concept recall. The evaluation of SSAEs on standard metrics, such as downstream perplexity and L_0 sparsity, show that they effectively capture subdomain tail concepts, exceeding the capabilities of general-purpose SAEs. The study showcases the practical utility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs achieve a 12.5% increase in worst-group classification accuracy when applied to remove spurious gender information. SSAEs provide a powerful new lens for peering into the inner workings of FMs in subdomains. <br> <br> 

33. ***DynaMath Benchmark for Mathematical Reasoning in VLMs:  <br>The DynaMath benchmark was introduced to evaluate Vision-Language Models' (VLMs) mathematical reasoning, assessing robustness with question variants. Tests on 14 models revealed lower worst-case versus average-case accuracy, emphasizing the need to strengthen VLMs' reasoning abilities.*** <br> <br>
    Oct 29, UIUC and UC Berkeley published a [paper](https://arxiv.org/pdf/2411.00836) “DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models”. The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, the study found that SOTA VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. This work investigates the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, the study introduces DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. The work evaluated 14 SOTA VLMs with 5,010 generated concrete questions. The results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. The analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.

35. ***Cost-Benefit of Semantic Chunking in RAG Systems   <br>This study challenges the efficacy of semantic chunking in Retrieval-Augmented Generation systems, finding no consistent performance gain over fixed-size chunking, thus questioning its computational cost.*** <br><br>
    Oct 16, Vectara Inc and Uni of Wisconsin-Madison published a [paper](https://arxiv.org/pdf/2410.13070) “Is Semantic Chunking Worth the Computational Cost?”. Recent advances in Retrieval-Augmented Generation (RAG) systems have popularized semantic chunking, which aims to improve retrieval performance by dividing documents into semantically coherent segments. Despite its growing adoption, the actual benefits over simpler fixed-size chunking, where documents are split into consecutive, fixed-size segments, remain unclear. This study systematically evaluates the effectiveness of semantic chunking using three common retrieval-related tasks: document retrieval, evidence retrieval, and retrieval-based answer generation. The results show that the computational costs associated with semantic chunking are not justified by consistent performance gains. These findings challenge the previous assumptions about semantic chunking and highlight the need for more efficient chunking strategies in RAG systems. <br> <br>

37. ***Numerical Representation in Language Models:  <br>Research found LLMs use a digit-wise representation in base 10, which contributes to their numerical errors. This representation explains LLM inaccuracies in basic numerical reasoning and highlights the need for alternative approaches in numerical tasks.*** <br> <br>
    Oct 15, Uni of Oxford and Tel Aviv Uni published [paper](https://arxiv.org/pdf/2410.11781) “Language Models Encode Numbers Using Digit Representations in Base 10”. Large language models (LLMs) frequently make errors when handling even simple numerical problems, such as comparing two small numbers. A natural hypothesis is that these errors stem from how LLMs represent numbers, and specifically, whether their representations of numbers capture their numeric values. The work tackles this question from the observation that LLM errors on numerical tasks are often distributed across the digits of the answer rather than normally around its numeric value. Through a series of probing experiments and causal interventions, the study shows that LLMs internally represent numbers with individual circular representations per-digit in base 10. This digit-wise representation, as opposed to a value representation, sheds light on the error patterns of models on tasks involving numerical reasoning and could serve as a basis for future studies on analyzing numerical mechanisms in LLMs. <br> <br>

39. ***Loss of Plasticity in Deep Continual Learning:  <br>The study showed that standard deep learning methods lose plasticity in continual learning tasks. Only algorithms with diversity mechanisms, such as continual backpropagation, maintained indefinite plasticity, suggesting that non-gradient elements are essential for sustained learning.*** <br> <br>
    Aug 21, Nature published a [paper](https://www.nature.com/articles/s41586-024-07711-7) from Alberta Uni “Loss of plasticity in deep continual learning”. Artificial neural networks, deep-learning methods and the backpropagation algorithm1 form the foundation of modern machine learning and artificial intelligence. These methods are almost always used in two phases, one in which the weights of the network are updated and one in which the weights are held constant while the network is used or evaluated. This contrasts with natural learning and many applications, which require continual learning. It has been unclear whether or not deep learning methods work in continual learning settings. Here the study shows that they do not—that standard deep-learning methods gradually lose plasticity in continual-learning settings until they learn no better than a shallow network. The study shows such loss of plasticity using the classic ImageNet dataset and reinforcement-learning problems across a wide range of variations in the network and the learning algorithm. Plasticity is maintained indefinitely only by algorithms that continually inject diversity into the network, such as the continual backpropagation algorithm, a variation of backpropagation in which a small fraction of less-used units are continually and randomly reinitialized. The results indicate that methods based on gradient descent are not enough—that sustained deep learning requires a random, non-gradient component to maintain variability and plasticity. <br> <br>

41. ***Evaluating Implicit World Models in Generative Models:  <br>This study introduced evaluation metrics for assessing generative models' implicit world models in deterministic domains like game-playing and navigation. Findings revealed that despite good performance on diagnostics, models' world representations were incoherent, highlighting fragility in real-world applications.*** <br> <br>
    Jun 22, Harvard Uni, MIT, Cornell Uni and Uni of Chicago Booth published a [paper](https://arxiv.org/pdf/2406.03689) “Evaluating the World Model Implicit in a Generative Model”. Recent work suggests that large language models may implicitly learn world models. How should people assess this possibility? The study formalizes this question for the case where the underlying reality is governed by a deterministic finite automaton. This includes problems as diverse as simple logical reasoning, geographic navigation, game-playing, and chemistry. The study proposes new evaluation metrics for world model recovery inspired by the classic Myhill-Nerode theorem from language theory. The authors illustrate their utility in three domains: game playing, logic puzzles, and navigation. In all domains, the generative models considered do well on existing diagnostics for assessing world models, but the evaluation metrics reveal their world models to be far less coherent than they appear. Such incoherence creates fragility: using a generative model to solve related but subtly different tasks can lead it to fail badly. Building generative models that meaningfully capture the underlying logic of the domains they model would be immensely valuable; the results suggest new ways to assess how close a given model is to that goal.
 <br> <br>


***Nov 2***

1. ***AI Identifies Database Vulnerability:  <br>Google’s AI project, “Big Sleep,” discovered a previously unknown bug in SQLite, showcasing the ability of AI agents to find software vulnerabilities. This achievement emphasizes the potential of large language models in improving software security.*** <br> <br>
   Nov 2, [according to PCMag](https://au.pcmag.com/ai/108079/googles-big-sleep-ai-project-uncovers-real-software-vulnerabilities), Google’s AI project, “Big Sleep,” recently discovered a previously unknown and exploitable bug in SQLite, an open-source database engine. This marks the first public instance of an AI agent identifying a new memory-safety issue in widely used software. The AI’s success highlights the potential of large language models in finding software vulnerabilities, offering a significant advantage in defending against hackers. Designed to mimic human security researchers, Big Sleep was able to perform a root-cause analysis by triggering and investigating the bug. This achievement suggests that AI can enhance vulnerability research, making it more efficient and effective. <br> <br>

3. ***Benchmarking Code Generation:  <br>Purdue University introduced REPOCOD, a new code generation benchmark that highlights the limitations of existing large language models (LLMs) in real-world software development. Evaluations showed no model exceeded 30% accuracy on this challenging benchmark, indicating the need for more advanced LLMs.*** <br> <br>
   Oct 31, Purdue Uni published a [paper](https://arxiv.org/pdf/2410.21647) “Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'”. Large language models (LLMs) have shown remarkable ability in code generation with more than 90 pass@1 in solving Python coding problems in HumanEval and MBPP. Such high accuracy leads to the question: can LLMs replace human programmers? Existing manual crafted, simple, or single-line code generation benchmarks cannot answer this question due to their gap with real-world software development. To answer this question, the study proposes REPOCOD, a code generation benchmark with 980 problems collected from 11 popular real-world projects, with more than 58% of them requiring file-level or repository-level context information. In addition, REPOCOD has the longest average canonical solution length (331.6 tokens) and the highest average cyclomatic complexity (9.00) compared to existing benchmarks. In the evaluations on ten LLMs, none of the models can achieve more than 30 pass@1 on REPOCOD, disclosing the necessity of building stronger LLMs that can help developers in real-world software development. <br> <br>

5. ***Layer Gradients in LLMs:  <br>A study from the University of Maryland explored how fast and slow thinking affects gradient patterns in LLMs during training. It found that slow thinking methods result in greater stability and better discrimination of correct reasoning paths, enhancing our understanding of LLM training efficiency.*** <br> <br>
   Oct 31, Uni of Maryland published a [paper](https://arxiv.org/pdf/2410.23743) “What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective”. What makes a difference in the post-training of LLMs? The work investigates the training patterns of different layers in large language models (LLMs), through the lens of gradient, when training with different responses and initial models. The authors are specifically interested in how fast vs. slow thinking affects the layer-wise gradients, given the recent popularity of training LLMs on reasoning paths such as chain-of-thoughts (CoT) and process rewards. In this study, fast thinking without CoT leads to larger gradients and larger differences of gradients across layers than slow thinking (Detailed CoT), indicating the learning stability brought by the latter. Moreover, pre-trained LLMs are less affected by the instability of fast thinking than instruction-tuned LLMs. Additionally, the authors study whether the gradient patterns can reflect the correctness of responses when training different LLMs using slow vs. fast thinking paths. The results show that the gradients of slow thinking can distinguish correct and irrelevant reasoning paths. As a comparison, the work conducts similar gradient analyses on non-reasoning knowledge learning tasks, on which, however, trivially increasing the response length does not lead to similar behaviors of slow thinking. The study strengthens fundamental understandings of LLM training and sheds novel insights on its efficiency and stability, which pave the way towards building a generalizable System-2 agent. The code, data, and gradient statistics can be found in: https://github.com/MingLiiii/Layer_Gradient. <br> <br>

7. ***Real-Time Information Integration:  <br>OpenAI launched ChatGPT Search, which enhances the chatbot's responses by incorporating real-time web data, including citations to reliable news sources. This new feature aims to improve the accuracy and relevance of information provided by ChatGPT.*** <br> <br>
   Oct 31, OpenAI [released ChatGPT Search](https://openai.com/index/introducing-chatgpt-search/?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=chatgpt-search-goes-live&_bhlid=913044e6704018f496b05f6913ccb437329e2d96), aiming at offering faster, more accurate answers by pulling real-time data from the web. With links to relevant news, stock quotes, sports scores, and other information, it merges natural language responses with up-to-date details one typically get from a search engine. Each chat response now includes citations to sources, like news articles or blog posts, making it easy to dive deeper. You can click the Sources button to view the references directly. OpenAI partnered with global news organizations, including Associated Press, Le Monde, Reuters, and News Corp, to offer reliable information and extend the reach of high-quality journalism. Publishers now have the option to appear in ChatGPT’s search results, opening up new avenues for audience engagement. OpenAI plans to expand this new search experience to areas like shopping and travel, while also making it available in Advanced Voice and canvas modes. Future updates will bring ChatGPT’s search capabilities to Free and guest users, further broadening access. <br> <br>

9. ***Evaluating Factual Knowledge:  <br>OpenAI’s new benchmark, SimpleQA, assesses LLMs' abilities to answer short factual questions accurately. Designed to challenge models like GPT-4, SimpleQA offers a straightforward grading system, with the best-performing model achieving a score of 42.4.*** <br> <br>
    Oct 30, OpenAI published a [paper](https://cdn.openai.com/papers/simpleqa.pdf) “Measuring short-form factuality in large language models”. The work presents SimpleQA, a benchmark that evaluates the ability of language models to answer short, fact-seeking questions. The study prioritized two properties in designing this eval. First, SimpleQA is challenging, as it is adversarially collected against GPT-4 responses. Second, responses are easy to grade, because questions are created such that there exists only a single, indisputable answer. Each answer in SimpleQA is graded as either correct, incorrect, or not attempted. A model with ideal behavior would get as many questions correct as possible while not attempting the questions for which it is not confident it knows the correct answer. SimpleQA is a simple, targeted evaluation for whether models “know what they know,” and the authors hope is that this benchmark will remain relevant for the next few generations of frontier models. According to this test, the highest score of AI models is 42.4.  SimpleQA can be found at https://github.com/openai/simple-evals. <br> <br>

11. ***Memorization vs. Reasoning in LLMs:  <br>A study from multiple institutions investigated the role of memorization in LLMs' logical reasoning capabilities. Results indicated that while LLMs perform well on familiar puzzles, they often fail on modified versions, highlighting the balance between memorization and genuine reasoning skills.*** <br> <br>
    Oct 30, Google, UIUC, Princeton Uni, and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2410.23123) “On Memorization of Large Language Models in Logical Reasoning”. Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. This study systematically investigates this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&K) puzzles. The study found that LLMs could interpolate the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet fail when those puzzles are slightly perturbed, suggesting that the models heavily rely on memorization to solve those training puzzles. On the other hand, the study shows that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. In-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers suggest that the LLMs learn to reason on K&K puzzles despite training data memorization. This phenomenon indicates that LLMs exhibit a complex interplay between memorization and genuine reasoning abilities. Finally, analysis with per-sample memorization score sheds light on how LLMs switch between reasoning and memorization in solving logical puzzles. Code and data are available at https://memkklogic.github.io. <br> <br>

13. ***Scaling Transformers Efficiently:  <br>The introduction of TokenFormer proposes a scalable architecture for transformers that allows for efficient model parameter sharing without retraining. This approach reduces computational costs while maintaining performance comparable to traditional transformer models.*** <br> <br>
    Oct 30, MPII, Google and Peking Uni published a [paper](https://arxiv.org/pdf/2410.23168) “TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters”. Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce TokenFormer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at https://github.com/Haiyang-W/TokenFormer. <br> <br>

15. ***LLMs in Data Science:  <br>A benchmark developed by Snowflake and the Polish Academy of Sciences assesses LLMs in generating feature engineering code. The study demonstrates that LLMs can effectively transform datasets, providing a cost-effective method for evaluating their capabilities.*** <br> <br>
    Oct 30, Snowflake and Polish Academy Sciences published a [paper](https://arxiv.org/pdf/2410.23331) “Can Models Help Us Create Better Models? Evaluating LLMs as Data Scientists”. The study presents a benchmark for large language models designed to tackle one of the most knowledge-intensive tasks in data science: writing feature engineering code, which requires domain knowledge in addition to a deep understanding of the underlying problem and data structure. The model is provided with a dataset description in a prompt and asked to generate code transforming it. The evaluation score is derived from the improvement achieved by an XGBoost model fit on the modified dataset compared to the original data. By an extensive evaluation of state-of-the-art models and comparison to well-established benchmarks, the study demonstrates that the FeatEng of the proposal can cheaply and efficiently assess the broad capabilities of LLMs, in contrast to the existing methods. <br> <br>

17. ***AI in Research Support:  <br>The AAAR-1.0 benchmark, introduced by researchers from various institutions, evaluates LLMs' performance in complex research tasks. This new dataset aims to assess LLMs' capabilities in conducting specialized research activities, highlighting their potential and limitations.*** <br> <br>
    Oct 29, Pennsylvania State Uni, Netflix, Uni of California Davis, Uni of Illinois Chicago et al published a [paper](https://arxiv.org/pdf/2410.22394) “AAAR-1.0: Assessing AI's Potential to Assist Research”. Numerous studies have assessed the proficiency of AI systems, particularly large language models (LLMs), in facilitating everyday tasks such as email writing, question answering, and creative content generation. However, researchers face unique challenges and opportunities in leveraging LLMs for their own work, such as brainstorming research ideas, designing experiments, and writing or reviewing papers. This study introduces AAAR-1.0, a benchmark dataset designed to evaluate LLM performance in three fundamental, expertise-intensive research tasks: (i) EquationInference, assessing the correctness of equations based on the contextual information in paper submissions; (ii) ExperimentDesign, designing experiments to validate research ideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper submissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews is deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis. An evaluation of both open-source and proprietary LLMs reveals their potential as well as limitations in conducting sophisticated research tasks. The authors will keep iterating AAAR-1.0 to new versions. <br> <br>

19. ***Innovations in Parameter Sharing:  <br>The study on Relaxed Recursive Transformers presents a novel approach to parameter sharing in LLMs, enabling efficient model size reduction without sacrificing performance. This method improves inference speed while maintaining model effectiveness.*** <br> <br>
    Oct 28, KAIST and Google published a [paper](https://arxiv.org/pdf/2410.20672) “Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA”. Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. This work revisits "layer tying" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller "Recursive Transformers" that share parameters across layers, with minimal loss of performance. Here, the Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. The work further improves performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. The work shows that the recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original "full-size" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, the work shows that this has the potential to lead to significant (2-3x) gains in inference throughput. <br> <br>

21. ***Compression Error Compensation:  <br>Nvidia's research introduces EoRA, a training-free method for compensating errors in compressed LLMs. This approach optimizes performance without requiring extensive retraining, demonstrating significant improvements in various tasks.*** <br> <br>
    Oct 28, Nvidia published a [paper](https://arxiv.org/pdf/2410.21271) “EoRA: Training-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation”. This study re-formulates the model compression problem into the customized compensation problem: Given a compressed model, the study aims to introduce residual low-rank paths to compensate for compression errors under customized requirements from users (e.g., tasks, compression ratios), resulting in greater flexibility in adjusting overall capacity without being constrained by specific compression formats. However, naively applying SVD to derive residual paths causes suboptimal utilization of the low-rank representation capacity. Instead, the work proposes Training-free Eigenspace Low-Rank Approximation (EoRA), a method that directly minimizes compression-induced errors without requiring gradient-based training, achieving fast optimization in minutes using a small amount of calibration data. EoRA projects compression errors into the eigenspace of input activations, leveraging eigenvalues to effectively prioritize the reconstruction of high-importance error components. Moreover, EoRA can be seamlessly integrated with fine-tuning and quantization to further improve effectiveness and efficiency. EoRA consistently outperforms previous methods in compensating errors for compressed LLaMA2/3 models on various tasks, such as language generation, commonsense reasoning, and math reasoning tasks (e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and MathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4 sparsity). EoRA offers a scalable, training-free solution to compensate for compression errors, making it a powerful tool to deploy LLMs in various capacity and efficiency requirements. <br> <br>

23. ***Modular Framework for Social Reasoning:  <br>The SocialGPT framework combines vision and language models for social relation reasoning. It achieves competitive performance without additional training and offers interpretable results, enhancing understanding of image content.*** <br> <br>
    Oct 28, Harvard Uni, IBM et al. published a [paper](https://arxiv.org/pdf/2410.21411) “SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization”. Social relation reasoning aims to identify relation categories such as friends, spouses, and colleagues from images. While current methods adopt the paradigm of training a dedicated network end-to-end using labeled image data, they are limited in terms of generalizability and interpretability. To address these issues, the study first presents a simple yet well-crafted framework named SocialGPT, which combines the perception capability of Vision Foundation Models (VFMs) and the reasoning capability of Large Language Models (LLMs) within a modular framework, providing a strong baseline for social relation recognition. Specifically, the study instructs VFMs to translate image content into a textual social story, and then utilize LLMs for text-based reasoning. SocialGPT introduces systematic design principles to adapt VFMs and LLMs separately and bridge their gaps. Without additional model training, it achieves competitive zero-shot results on two databases while offering interpretable answers, as LLMs can generate language-based explanations for the decisions. The manual prompt design process for LLMs at the reasoning phase is tedious and an automated prompt optimization method is desired. As the work essentially converts a visual classification task into a generative task of LLMs, automatic prompt optimization encounters a unique long prompt optimization issue. To address this issue, the study further proposes the Greedy Segment Prompt Optimization (GSPO), which performs a greedy search by utilizing gradient information at the segment level. Experimental results show that GSPO significantly improves performance, and the method also generalizes to different image styles. The code is available at https://github.com/Mengzibin/SocialGPT. <br> <br>

25. ***Relevance Feedback in Retrieval:  <br>MIT's study presents ReDE-RF, a method for improving zero-shot dense retrieval by focusing on relevance estimation rather than hypothetical document generation. This approach enhances efficiency and accuracy in document retrieval tasks.*** <br> <br>
    Oct 28, MIT published [paper](https://arxiv.org/pdf/2410.21242) “Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback”. Building effective dense retrieval systems remains difficult when relevance supervision is not available. Recent work has looked to overcome this challenge by using a Large Language Model (LLM) to generate hypothetical documents that can be used to find the closest real document. However, this approach relies solely on the LLM to have domain-specific knowledge relevant to the query, which may not be practical. Furthermore, generating hypothetical documents can be inefficient as it requires the LLM to generate a large number of tokens for each query. To address these challenges, the work introduces Real Document Embeddings from Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF proposes to re-frame hypothetical document generation as a relevance estimation task, using an LLM to select which documents should be used for nearest neighbor search. Through this re-framing, the LLM no longer needs domain-specific knowledge but only needs to judge what is relevant. Additionally, relevance estimation only requires the LLM to output a single token, thereby improving search latency. Experiments show that ReDE-RF consistently surpasses state-of-the-art zero-shot dense retrieval methods across a wide range of low-resource retrieval datasets while also making significant improvements in latency per-query. <br> <br>

27. ***Fine-Tuning Analysis:  <br>A study contrasts Low-Rank Adaptation (LoRA) and full fine-tuning in LLMs, revealing that while they may perform similarly, they access different parts of parameter space. This difference impacts generalization and adaptability in various tasks.*** <br> <br>
    Oct 28, MIT published a [paper](https://arxiv.org/pdf/2410.21228) “LoRA vs Full Fine-tuning: An Illusion of Equivalence”. Fine-tuning is a crucial paradigm for adapting pre-trained large language models to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA) have been shown to match the performance of fully fine-tuned models on various tasks with an extreme reduction in the number of trainable parameters. Even in settings where both methods learn similarly accurate models, are their learned solutions really equivalent? The work studies how different fine-tuning methods change pre-trained models by analyzing the model's weight matrices through the lens of their spectral properties. The work finds that full fine-tuning and LoRA yield weight matrices whose singular value decompositions exhibit very different structure; moreover, the fine-tuned models themselves show distinct generalization behaviors when tested outside the adaptation task's distribution. More specifically, the study first shows that the weight matrices trained with LoRA have new, high-ranking singular vectors, which is called intruder dimensions. Intruder dimensions do not appear during full fine-tuning. Second, the work shows that LoRA models with intruder dimensions, despite achieving similar performance to full fine-tuning on the target task, become worse models of the pre-training distribution and adapt less robustly to multiple tasks sequentially. Higher-rank, rank-stabilized LoRA models closely mirror full fine-tuning, even when performing on par with lower-rank LoRA models on the same tasks. These results suggest that models updated with LoRA and full fine-tuning access different parts of parameter space, even when they perform equally on the fine-tuned distribution. The study concludes by examining why intruder dimensions appear in LoRA fine-tuned models, why they are undesirable, and how their effects can be minimized. <br> <br>

29. ***Iterative Context Retrieval:  <br>The FACT method introduces an iterative approach to improve multi-fact retrieval in LLMs, addressing the challenges of losing critical information. This technique significantly enhances performance in multi-fact tasks, highlighting the need for better retrieval strategies.*** <br> <br>
    Oct 28, DeepWisdom, Uni de Montreal & Mila and Google published a [paper](https://arxiv.org/pdf/2410.21012) “FACT: Examining the Effectiveness of Iterative Context Rewriting for Multi-fact Retrieval”. Large Language Models (LLMs) are proficient at retrieving single facts from extended contexts, yet they struggle with tasks requiring the simultaneous retrieval of multiple facts, especially during generation. This paper identifies a novel "lost-in-the-middle" phenomenon, where LLMs progressively lose track of critical information throughout the generation process, resulting in incomplete or inaccurate retrieval. To address this challenge, the study introduces Find All Crucial Texts (FACT), an iterative retrieval method that refines context through successive rounds of rewriting. This approach enables models to capture essential facts incrementally, which are often overlooked in single-pass retrieval. Experiments demonstrate that FACT substantially enhances multi-fact retrieval performance across various tasks, though improvements are less notable in general-purpose QA scenarios. The findings shed light on the limitations of LLMs in multi-fact retrieval and underscore the need for more resilient long-context retrieval strategies. <br> <br>

31. ***Relaxed Recursive Transformers:  <br>This paper introduces a method for parameter sharing in large language models (LLMs) through Recursive Transformers, which reuse layers to reduce size without significant performance loss. The approach, enhanced by Relaxed Recursive Transformers using LoRA modules, achieves better performance than traditional models and proposes a new inference method that could improve throughput significantly.*** <br> <br>
    Oct 28, KAIST AI and Google published a [paper](https://arxiv.org/pdf/2410.20672) “Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA”. Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. This work revisits "layer tying" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller "Recursive Transformers" that share parameters across layers, with minimal loss of performance. Here, the Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. The work further improves performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. The study shows that the recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original "full-size" model (e.g., Gemma 2B with no shared parameters). Finally, the work proposes Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, the work shows that this has the potential to lead to significant (2-3x) gains in inference throughput. <br> <br>

33. ***Beyond Autoregression:  <br>This research explores diffusion language models that can generate multiple tokens simultaneously, outperforming autoregressive models in quality and speed. A novel distillation method reduces inference steps drastically, leading to faster generation rates.*** <br> <br>
    Oct 28, EPFL published a [paper](https://arxiv.org/pdf/2410.21035) “Beyond Autoregression: Fast LLMs via Self-Distillation Through Time”. Autoregressive (AR) Large Language Models (LLMs) have demonstrated significant success across numerous tasks. However, the AR modeling paradigm presents certain limitations; for instance, contemporary autoregressive LLMs are trained to generate one token at a time, which can result in noticeable latency. Recent advances have indicated that search and repeated sampling can enhance performance in various applications, such as theorem proving, code generation, and alignment, by utilizing greater computational resources during inference. This paper demonstrates that diffusion language models are capable of generating at least 32 tokens simultaneously, while exceeding the performance of AR models in text quality and on the LAMBADA natural language understanding benchmark. This outcome is achieved through a novel distillation method for discrete diffusion models, which reduces the number of inference steps by a factor of 32-64. Practically, the models, even without caching, can generate tokens at a rate that is up to 8 times faster than AR models employing KV caching, and it is anticipated further improvements with the inclusion of caching. Moreover, the paper demonstrates the efficacy of the approach for diffusion language models with up to 860M parameters. <br> <br>

35. ***Chain-of-Thought Prompting:  <br>Investigating when chain-of-thought prompting can harm model performance, this study identifies tasks where reasoning can lead to poorer outcomes, drawing parallels with human cognition. It shows significant drops in performance on specific tasks when using CoT.*** <br> <br>
    Oct 27, Princeton Uni and NYU published a [paper](https://arxiv.org/pdf/2410.21333) “Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse”. Chain-of-thought (CoT) prompting has become a widely used strategy for working with large language and multimodal models. While CoT has been shown to improve performance across many tasks, determining the settings in which it is effective remains an ongoing effort. In particular, it is still an open question in what settings CoT systematically reduces model performance. This study seeks to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, looking at cases where (i) verbal thinking or deliberation hurts performance in humans, and (ii) the constraints governing human performance generalize to language models. Three such cases are implicit statistical learning, visual recognition, and classifying with patterns containing exceptions. In extensive experiments across all three settings, the study finds that a diverse collection of state-of-the-art models exhibit significant drop-offs in performance (e.g., up to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using inference-time reasoning compared to zero-shot counterparts. The work also identifies three tasks that satisfy condition (i) but not (ii), and find that while verbal thinking reduces human performance in these tasks, CoT retains or increases model performance. Overall, the results show that while there is not an exact parallel between the cognitive processes of models and those of humans, considering cases where thinking has negative consequences for human performance can help to identify settings where it negatively impacts models. By connecting the literature on human deliberation with evaluations of CoT, the work offers a new tool that can be used in understanding the impact of prompt choices and inference-time reasoning. <br> <br>

37. ***Centaur: A Unified Model of Cognition:  <br>The Centaur model aims to simulate human cognition through extensive behavioral data. It shows promise in predicting human responses across diverse experimental settings, potentially transforming cognitive science.*** <br> <br>
    Oct 26, Helmholtz Munich, Uni of Tuebingen, Uni of Oxford, NYU, MPI, Google, Princeton Uni, Uni of Cambridge et al published 140-page [paper](https://arxiv.org/pdf/2410.20268) “Centaur: a foundation model of human cognition”. Establishing a unified theory of cognition has been a major goal of psychology. While there have been previous attempts to instantiate such theories by building computational models, we currently do not have one model that captures the human mind in its entirety. Here the study introduces Centaur, a computational model that can predict and simulate human behavior in any experiment expressible in natural language. The study derived Centaur by finetuning a state-of-the-art language model on a novel, large-scale data set called Psych-101. Psych-101 reaches an unprecedented scale, covering trial-by-trial data from over 60,000 participants performing over 10,000,000 choices in 160 experiments. Centaur not only captures the behavior of held-out participants better than existing cognitive models, but also generalizes to new cover stories, structural task modifications, and entirely new domains. Furthermore, the study finds that the model's internal representations become more aligned with human neural activity after finetuning. Taken together, Centaur is the first real candidate for a unified model of human cognition. The authors anticipate that it will have a disruptive impact on the cognitive sciences, challenging the existing paradigm for developing computational models. <br> <br>

39. ***Fast Best-of-N Decoding:  <br>This paper presents Speculative Rejection, a method for efficient inference-time alignment of LLMs, achieving the effectiveness of Best-of-N alignment with significantly lower computational costs.*** <br> <br>
    Oct 26, CMU, Uni of Virginia, UC Berkeley, Princeton, Fudan Uni published a [paper](https://arxiv.org/pdf/2410.20290) “Fast Best-of-N Decoding via Speculative Rejection”. The safe and effective deployment of Large Language Models (LLMs) involves a critical step called alignment, which ensures that the model's responses are in accordance with human preferences. Prevalent alignment techniques, such as DPO, PPO and their variants, align LLMs by changing the pre-trained model weights during a phase called post-training. While predominant, these post-training methods add substantial complexity before LLMs can be deployed. Inference-time alignment methods avoid the complex post-training step and instead bias the generation towards responses that are aligned with human preferences. The best-known inference-time alignment method, called Best-of-N, is as effective as the state-of-the-art post-training procedures. Unfortunately, Best-of-N requires vastly more resources at inference time than standard decoding strategies, which makes it computationally not viable. This study introduces Speculative Rejection, a computationally-viable inference-time alignment algorithm. It generates high-scoring responses according to a given reward model, like Best-of-N does, while being between 16 to 32 times more computationally efficient. <br> <br>

41. ***RARe: Retrieval Augmented Retrieval:  <br>RARe demonstrates how in-context examples can improve retrieval model performance by finetuning models on similar query pairs, leading to enhanced generalization and retrieval accuracy.*** <br> <br>
    Oct 26, Uni of Texas at Austin et al. published a [paper](https://arxiv.org/pdf/2410.20088) “RARe: Retrieval Augmented Retrieval with In-Context Examples”. The work investigates whether in-context examples, widely used in decoder-only language models (LLMs), can improve embedding model performance in retrieval tasks. Unlike in LLMs, naively prepending in-context examples (query-document pairs) to the target query at inference time does not work out of the box. The study introduces a simple approach to enable retrievers to use in-context examples. The approach, RARe, finetunes a pre-trained model with in-context examples whose query is semantically similar to the target query. This can be applied to adapt various base architectures (i.e., decoder-only language models, retriever models) and consistently achieves performance gains of up to +2.72% nDCG across various open-domain retrieval datasets (BeIR, RAR-b). In particular, the study finds RARe exhibits stronger out-of-domain generalization compared to models using queries without in-context examples, similar to what is seen for in-context learning in LLMs. The study further provides analysis on the design choices of in-context example augmentation and lay the foundation for future work in this space. <br> <br>

43. ***Rethinking Uncertainty:  <br>This review proposes a comprehensive framework for understanding and measuring uncertainty in LLMs, aiming to improve reliability in mission-critical applications.*** <br> <br>
    Oct 26, Virginia Tech, UIUC, The Uni of Texas at Dallas and Uni of California Davis published a [paper](https://arxiv.org/pdf/2410.20199) “Rethinking the Uncertainty: A Critical Review and Analysis in the Era of Large Language Models”. In recent years, Large Language Models (LLMs) have become fundamental to a broad spectrum of artificial intelligence applications. As the use of LLMs expands, precisely estimating the uncertainty in their predictions has become crucial. Current methods often struggle to accurately identify, measure, and address the true uncertainty, with many focusing primarily on estimating model confidence. This discrepancy is largely due to an incomplete understanding of where, when, and how uncertainties are injected into models. This paper introduces a comprehensive framework specifically designed to identify and understand the types and sources of uncertainty, aligned with the unique characteristics of LLMs. The proposed framework enhances the understanding of the diverse landscape of uncertainties by systematically categorizing and defining each type, establishing a solid foundation for developing targeted methods that can precisely quantify these uncertainties. The study also provides a detailed introduction to key related concepts and examine the limitations of current methods in mission-critical and safety-sensitive applications. The paper concludes with a perspective on future directions aimed at enhancing the reliability and practical adoption of these methods in real-world scenarios. <br> <br>

45. ***Model Equality Testing:  <br>The study formalizes methods for detecting changes in model behavior from APIs, showing how statistical tests can identify whether models serve altered outputs compared to their original versions.*** <br>v
    Oct 26, Stanford Uni published a [paper](https://arxiv.org/pdf/2410.20247) “Model Equality Testing: Which Model Is This API Serving?”. Users often interact with large language models through black-box inference APIs, both for closed- and open-weight models (e.g., Llama models are popularly accessed via Amazon Bedrock and Azure AI Studio). In order to cut costs or add functionality, API providers may quantize, watermark, or finetune the underlying model, changing the output distribution -- often without notifying users. The study formalizes detecting such distortions as Model Equality Testing, a two-sample testing problem, where the user collects samples from the API and a reference distribution and conducts a statistical test to see if the two distributions are the same. The study finds that tests based on the Maximum Mean Discrepancy between distributions are powerful for this task: a test built on a simple string kernel achieves a median of 77.4% power against a range of distortions, using an average of just 10 samples per prompt. The authors then apply this test to commercial inference APIs for four Llama models, finding that 11 out of 31 endpoints serve different distributions than reference weights released by Meta. <br> <br>

47. ***Measuring Memorization in LLMs:  <br>This research introduces a probabilistic method for assessing LLM memorization rates, providing a clearer understanding of how different sampling techniques influence data extraction capabilities.*** <br> <br>
    Oct 25, Google and Boston Uni published a [paper](https://arxiv.org/pdf/2410.19482) “Measuring memorization through probabilistic discoverable extraction”. Large language models (LLMs) are susceptible to memorizing training data, raising concerns due to the potential extraction of sensitive information. Current methods to measure memorization rates of LLMs, primarily discoverable extraction (Carlini et al., 2022), rely on single-sequence greedy sampling, potentially underestimating the true extent of memorization. This paper introduces a probabilistic relaxation of discoverable extraction that quantifies the probability of extracting a target sequence within a set of generated samples, considering various sampling schemes and multiple attempts. This approach addresses the limitations of reporting memorization rates through discoverable extraction by accounting for the probabilistic nature of LLMs and user interaction patterns. Experiments demonstrate that this probabilistic measure can reveal cases of higher memorization rates compared to rates found through discoverable extraction. The work further investigates the impact of different sampling schemes on extractability, providing a more comprehensive and realistic assessment of LLM memorization and its associated risks. The contributions include a new probabilistic memorization definition, empirical evidence of its effectiveness, and a thorough evaluation across different models, sizes, sampling schemes, and training data repetitions.  <br> <br>

49. ***GPT-4o System Card:  <br>This document outlines the capabilities and safety evaluations of the GPT-4o model, which integrates text, audio, and visual inputs and outputs. It highlights the model's speed and performance improvements over predecessors.*** <br> <br>
    Oct 25, OpenAI published a [paper](https://arxiv.org/pdf/2410.21276) “GPT-4o System Card”. GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with the commitment to building AI safely and consistent with the voluntary commitments to the White House, OpenAI is sharing the GPT-4o System Card, which includes the Preparedness Framework evaluations. This System Card provides a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures implemented to ensure the model is safe and aligned. The paper also includes third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities. <br> <br>

51. ***COAT: Memory-Efficient FP8 Training:  <br>COAT introduces a method to reduce memory usage in FP8 training by optimizing optimizer states and activations, achieving significant training speedups while maintaining performance.*** <br> <br>
    Oct 25, UC Berkeley, Nvidia, MIT and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2410.19313) “COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training”. FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (Compressing Optimizer States and Activations for FP8 Training), a novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1) Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2) Mixed-Granularity Activation Quantization, which optimizes activation memory using a combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by 1.54x compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves a 1.43x end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngine's speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing a practical solution for scaling large-scale model training. The code is available at https://github.com/NVlabs/COAT. <br> <br>

53. ***Mixture of Parrots:  <br>This study analyzes the trade-offs in MoE architectures, finding that while they excel at memorization, their reasoning capabilities may not scale similarly.*** <br> <br>
    Oct 24, Harvad Uni, MIT and Microsoft published a [paper](https://arxiv.org/pdf/2410.19034) “Mixture of Parrots: Experts improve memorization more than reasoning”. The Mixture-of-Experts (MoE) architecture enables a significant increase in the total number of model parameters with minimal computational overhead. However, it is not clear what performance tradeoffs, if any, exist between MoEs and standard dense transformers. This study shows that as increasing the number of experts (while fixing the number of active parameters), the memorization performance consistently increases while the reasoning capabilities saturate. The study begins by analyzing the theoretical limitations of MoEs at reasoning. The study proves that there exist graph problems that cannot be solved by any number of experts of a certain width; however, the same task can be easily solved by a dense model with a slightly larger width. On the other hand, the study finds that on memory-intensive tasks, MoEs can effectively leverage a small number of active parameters with a large number of experts to memorize the data. The study empirically validates these findings on synthetic graph problems and memory-intensive closed book retrieval tasks. Lastly, the study pre-trains a series of MoEs and dense transformers and evaluate them on commonly used benchmarks in math and natural language. The authors find that increasing the number of experts helps solve knowledge-intensive tasks, but fails to yield the same benefits for reasoning tasks. <br> <br>

55. ***Detecting Label Errors in Datasets:  <br>The paper demonstrates how LLMs can detect label errors in training datasets, suggesting that many perceived model failures may stem from mislabeled data.*** <br> <br>
    Oct 24, Technion Inst of Tech and Google published a [paper](https://arxiv.org/pdf/2410.18889) “Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance”. NLP benchmarks rely on standardized datasets for training and evaluating models and are crucial for advancing the field. Traditionally, expert annotations ensure high-quality labels; however, the cost of expert annotation does not scale well with the growing demand for larger datasets required by modern models. While crowd-sourcing provides a more scalable solution, it often comes at the expense of annotation precision and consistency. Recent advancements in large language models (LLMs) offer new opportunities to enhance the annotation process, particularly for detecting label errors in existing datasets. This study considers the recent approach of LLM-as-a-judge, leveraging an ensemble of LLMs to flag potentially mislabeled examples. Through a case study of four datasets from the TRUE benchmark, covering different tasks and domains, the study empirically analyzes the labeling quality of existing datasets, and compare expert, crowd-sourced, and the LLM-based annotations in terms of agreement, label quality, and efficiency, demonstrating the strengths and limitations of each annotation method. The findings reveal a substantial number of label errors, which, when corrected, induce a significant upward shift in reported model performance. This suggests that many of the LLMs so-called mistakes are due to label errors rather than genuine model failures. Additionally, the work discusses the implications of mislabeled data and propose methods to mitigate them in training to improve model performance. <br> <br>

57. ***Read-ME: Router-Decoupled Mixture of Experts:  <br>This research presents a framework to transform dense LLMs into efficient MoE models, addressing inference challenges and improving latency and accuracy through better system integration.*** <br> <br>
    Oct 24, The Uni of Texas at Austin and Qualcomm AI Research published a [paper](https://arxiv.org/pdf/2410.19123) “Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design”. The proliferation of large language models (LLMs) has led to the adoption of Mixture-of-Experts (MoE) architectures that dynamically leverage specialized subnetworks for improved efficiency and performance. Despite their benefits, MoE models face significant challenges during inference, including inefficient memory management and suboptimal batching, due to misaligned design choices between the model architecture and the system policies. Furthermore, the conventional approach of training MoEs from scratch is increasingly prohibitive in terms of cost. This work proposes a novel framework Read-ME that transforms pre-trained dense LLMs into smaller MoE models (in contrast to "upcycling" generalist MoEs), avoiding the high costs of ground-up training. The approach employs activation sparsity to extract experts. To compose experts, the work examines the widely-adopted layer-wise router design and show its redundancy, and thus introducing the pre-gating router decoupled from the MoE backbone that facilitates system-friendly pre-computing and lookahead scheduling, enhancing expert-aware batching and caching. The codesign therefore addresses critical gaps on both the algorithmic and system fronts, establishing a scalable and efficient alternative for LLM inference in resource-constrained settings. Read-ME outperforms other popular open-source dense models of similar scales, achieving improvements of up to 10.1% on MMLU, and improving mean end-to-end latency up to 6.1%. Codes are available at: https://github.com/VITA-Group/READ-ME.

 <br> <br> <br>
