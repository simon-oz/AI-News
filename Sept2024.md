
***Oct 06***

1. ***Meta’s MovieGen release and innovations in media generation:  <br>Meta released MovieGen and a paper detailing its new foundation models that can generate high-quality 1080p HD videos with synchronized audio and personalized video editing. The models set new standards in video and audio synthesis tasks. The paper highlights technical advancements and simplifications that enable scaling for large-scale media generation, hoping to accelerate progress in this research field.*** <br>
   Oct 4, Meta [released MovieGen](https://ai.meta.com/research/movie-gen/), and the [paper](https://ai.meta.com/static-resource/movie-gen-research-paper) “Movie Gen: A Cast of Media Foundation Models”. The paper presents Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. Meta also shows additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user’s image. The models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. The largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. The paper shows multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow Meta to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. Meta hopes this paper helps the research community to accelerate progress and innovation in media generation models. All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos. <br>

3. ***Quantifying generalization in large language models (LLMs):  <br>A joint paper from Harvard, MIT, and others introduced Scylla, a framework to measure LLMs' generalization by separating it from memorization. The study introduces the concept of critical complexity, where models begin to rely on memorization. The results suggest larger models handle more complex tasks better, and Scylla helps evaluate 28 LLMs, offering insight into their generalization.*** <br>
   Oct 3, Harvard Uni, MIT, UIUC, Meta and Uni of Chicago published a [paper](https://arxiv.org/pdf/2410.01769) “Quantifying Generalization Complexity for Large Language Models”. While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, the study introduces Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, the study uncovers a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which the authors term the generalization valley. Specifically, this phenomenon reveals a critical threshold - referred to as critical complexity - where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, the authors benchmark 28 LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.   <br>

5. ***LLMs as Markov Chains and theoretical insights:  <br>A paper by ENS Paris-Saclay and collaborators explored LLMs' performance using Markov chain theory. It found parallels between autoregressive models and Markov chains, uncovering key insights about stationary distributions and the effect of temperature on convergence. The study also offers theoretical guarantees for LLM performance through experiments.*** <br>
   Oct 3, ENS Paris-Saclay, Ark Lab, and Inria Paris published a [paper](https://arxiv.org/pdf/2410.02724) “Large Language Models as Markov Chains”. Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. This study approaches this challenging task by drawing an equivalence between generic autoregressive language models with vocabulary of size T and context window of size K and Markov chains defined on a finite state space of size O(T^K). The study derives several surprising findings related to the existence of a stationary distribution of Markov chains that capture the inference power of LLMs, their speed of convergence to it, and the influence of the temperature on the latter. The study then proves pre-training and in-context generalization bounds and show how the drawn equivalence allows the authors to enrich their interpretation. Finally, the study illustrates the theoretical guarantees with experiments on several recent LLMs to highlight how they capture the behavior observed in practice. <br>

7. ***Apple’s advances in vision-language models with CLOC:  <br>Apple proposed Contrastive Localized Language-Image Pre-training (CLOC), which improves upon CLIP by enhancing regional understanding in multimodal large language models. This method, focused on generating high-quality regional embeddings, outperforms CLIP in image region recognition tasks, and its scaling with billions of annotated images enables more precise language-image alignment.*** <br>
   Oct 3, Apple published a [paper](https://arxiv.org/pdf/2410.02746) “Contrastive Localized Language-Image Pre-Training”. Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect image inputs for language interactions. The success of CLIP as a vision-language foundation model relies on aligning web-crawled noisy text annotations at image levels. Nevertheless, such criteria may become insufficient for downstream tasks in need of fine-grained vision representations, especially when region-level understanding is demanding for MLLMs. This study improves the localization capability of CLIP with several advances, proposes a pre-training method called Contrastive Localized Language-Image Pre-training (CLOC) by complementing CLIP with region-text contrastive loss and modules. The paper formulates a new concept, promptable embeddings, of which the encoder produces image embeddings easy to transform into region representations given spatial hints. To support large-scale pre-training, the authors design a visually-enriched and spatially-localized captioning framework to effectively generate region-text pseudo-labels at scale. By scaling up to billions of annotated images, CLOC enables high-quality regional embeddings for image region recognition and retrieval tasks, and can be a drop-in replacement of CLIP to enhance MLLMs, especially on referring and grounding tasks. <br>

9. ***Mitigating hallucinations in vision-language models:  <br>A study from UC Berkeley tackled hallucinations in vision-language models by projecting their internal image representations to language vocabulary. The researchers developed a knowledge erasure algorithm, reducing hallucinations by 25.7% on the COCO2014 dataset, showing that targeted edits to models' latent representations can improve reliability without affecting overall performance.*** <br>
    Oct 3, UC Berkeley published a [paper](https://arxiv.org/pdf/2410.02762) “Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations”. The study investigates the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. The work projects VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on real objects than hallucinated objects. The work additionally uses these output probabilities to spatially localize real objects. Building on this approach, the study introduces a knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features. The authros show that targeted edits to a model's latent representations can reduce hallucinations by up to 25.7% on the COCO2014 dataset while preserving performance. The findings demonstrate how a deeper understanding of VLMs' latent representations can enhance reliability and enable novel capabilities, such as zero-shot segmentation. <br>

11. ***Contextual document embeddings for neural retrieval:  <br>Cornell University published a paper proposing methods for generating contextualized document embeddings by incorporating neighboring documents. These methods, focused on improving performance out-of-domain, achieved state-of-the-art results in several benchmarks, outperforming traditional biencoders. The study suggests these approaches can be broadly applied to contrastive learning datasets.*** <br>
    Oct 3, Cornell Uni published a [paper](https://arxiv.org/pdf/2410.02762) “Contextual Document Embeddings” . Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. This paper argues that these embeddings, while effective, are implicitly out-of-context for targeted use cases of retrieval, and that a contextualized document embedding should take into account both the document and neighboring documents in context - analogous to contextualized word embeddings. The study proposes two complementary methods for contextualized document embeddings: first, an alternative contrastive learning objective that explicitly incorporates the document neighbors into the intra-batch contextual loss; second, a new contextual architecture that explicitly encodes neighbor document information into the encoded representation. Results show that both methods achieve better performance than biencoders in several settings, with differences especially pronounced out-of-domain. The proposed models achieve state-of-the-art results on the MTEB benchmark with no hard negative mining, score distillation, dataset-specific instructions, intra-GPU example-sharing, or extremely large batch sizes. The method can be applied to improve performance on any contrastive learning dataset and any biencoder. <br>

13. ***End-to-end voice assistants with DiVA:  <br>A paper from Georgia Tech and others proposed training Speech LLMs without instruction data for voice assistants like Siri. The Distilled Voice Assistant (DiVA) model generalizes well to various tasks like Spoken Question Answering and Translation, outperforming existing models with significantly less training compute, while avoiding loss of text-based capabilities.*** <br>
    Oct 3, Georgia Inst of Tech, Stanford Uni, National Uni of Singapore and Northeastern Uni published a [paper](https://arxiv.org/pdf/2410.02678) “Distilling an End-to-End Voice Assistant Without Instruction Training Data”. Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT) have led to models “forgetting” capabilities from text-only LLMs. This work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only LLM to transcripts as self-supervision. Importantly, this process can be performed without annotated responses. The authors show that the Distilled Voice Assistant (DiVA) generalizes to Spoken Question Answering, Classification, and Translation. Furthermore, the study shows that DiVA better meets user preferences, achieving a 72\% win rate compared with state-of-the-art models like Qwen 2 Audio, despite using >100x less training compute. <br>

15. ***Learning game rules from data with small language models:  <br>Dell published a study showing that small pretrained language models can learn complex rules, like those of chess, from data. The study demonstrated how fine-tuning with increasing examples improved accuracy and reduced hallucinations in these models, indicating that even small models can learn complex processes effectively.*** <br>
    Oct 3, Dell published a [paper](https://arxiv.org/pdf/2410.02426) “Learning the Latent Rules of a Game from Data: A Chess Story”. The work demonstrates that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella "Schachnovelle," also known as "The Royal Game" in English, the study shows that 28M and 125M parameter pretrained foundational small language models (SLMs) can be instruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of chess, propose legal moves, and accurately solve chess problems. The study also explores the impact of successive language model fine-tuning epochs on improved outcomes and demonstrate reductions in model hallucinations by increasing the number of instruction fine-tuning examples. <br>

17. ***Intelligence emergence in rule-based systems:  <br>A study by Yale and others explored how complexity in rule-based systems like elementary cellular automata influences LLM intelligence. The findings suggest that exposure to more complex rules leads to better reasoning performance, indicating that complexity is key to developing intelligent behaviors in artificial systems.*** <br>
    Oct 3, Yale Uni et al published a [paper](https://arxiv.org/pdf/2410.02536) “Intelligence at the Edge of Chaos”. The study explores the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. The study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems that generate behaviors ranging from trivial to highly complex. By training distinct Large Language Models (LLMs) on different ECAs, the work evaluated the relationship between the complexity of the rules' behavior and the intelligence exhibited by the LLMs, as reflected in their performance on downstream tasks. The findings reveal that rules with higher complexity lead to models exhibiting greater intelligence, as demonstrated by their performance on reasoning and chess move prediction tasks. Both uniform and periodic systems, and often also highly chaotic systems, resulted in poorer downstream performance, highlighting a sweet spot of complexity conducive to intelligence. The authors conjecture that intelligence arises from the ability to predict complexity and that creating intelligence may require only exposure to complexity. <br>

19. ***Google’s selective attention mechanism for transformers:  <br>Google introduced a selective attention mechanism that reduces attention to unneeded elements in transformers. This simple change improves performance while reducing memory and compute requirements. Transformers equipped with selective attention require far less memory during inference, making them more efficient without sacrificing accuracy.*** <br>
    Oct 3, Google published a [paper](https://arxiv.org/abs/2410.02703) “Selective Attention Improves Transformer”. Unneeded elements in the attention's context degrade performance. The work introduces Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention improves language modeling performance in a variety of model sizes and context lengths. For example, a range of transformers trained with the language modeling objective on C4 with selective attention perform equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers with 100M parameters trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity. <br>

21. ***Evaluating LLMs' reasoning abilities in math problem-solving:  <br>A study by Mila, Google, and Microsoft evaluated LLMs' reasoning on math word problems, revealing a significant gap between solving independent questions and compositional pairs. The study highlights differences in reasoning capabilities among LLMs, with smaller models showing larger reasoning gaps, offering insights into LLMs' systematic reasoning challenges.*** <br>
    Oct 2, Mila, Google and Microsoft published a [paper](https://arxiv.org/pdf/2410.01748) “Not All LLM Reasoners Are Created Equal”. The authors study the depth of grade-school math (GSM) problem-solving capabilities of LLMs. To this end, the work evaluates their performance on pairs of existing math word problems together so that the answer to the second problem depends on correctly answering the first problem. The findings reveal a significant reasoning gap in most LLMs, that is performance difference between solving the compositional pairs and solving each question independently. This gap is more pronounced in smaller, more cost-efficient, and math-specialized models. Moreover, instruction-tuning recipes and code generation have varying effects across LLM sizes, while finetuning on GSM can lead to task overfitting. The analysis indicates that large reasoning gaps are not because of test-set leakage, but due to distraction from additional context and poor second-hop reasoning. Overall, LLMs exhibit systematic differences in their reasoning abilities, despite what their performance on standard benchmarks indicates. <br>

23. ***Revisiting recurrent neural networks (RNNs):  <br>A paper from Mila revisited traditional RNNs, demonstrating that by removing certain dependencies, LSTMs and GRUs can be trained in parallel, making them as fast and efficient as newer sequence models. This revival of decade-old models shows they can still compete with modern architectures in handling long sequences.*** <br>
    Oct 2, Mila, Uni of Montreal, Borealis AI published a [paper](https://arxiv.org/pdf/2410.01201) “Were RNNs All We Needed?”. The scalability limitations of Transformers regarding sequence length have renewed interest in recurrent sequence models that are parallelizable during training. As a result, many novel recurrent architectures, such as S4, Mamba, and Aaren, have been proposed that achieve comparable performance. This work revisits traditional recurrent neural networks (RNNs) from over a decade ago: LSTMs (1997) and GRUs (2014). While these models were slow due to requiring to backpropagate through time (BPTT), the work shows that by removing their hidden state dependencies from their input, forget, and update gates, LSTMs and GRUs no longer need to BPTT and can be efficiently trained in parallel. Building on this, the study introduces minimal versions (minLSTMs and minGRUs) that (1) use significantly fewer parameters than their traditional counterparts and (2) are fully parallelizable during training (175x faster for a sequence of length 512). Lastly, the study shows that these stripped-down versions of decade-old RNNs match the empirical performance of recent sequence models. <br>

25. ***Meta’s RLEF for improving code synthesis:  <br>Meta introduced a reinforcement learning method called RLEF for improving LLMs' performance in code synthesis tasks. The study achieved new state-of-the-art results by teaching models to iteratively improve code using execution feedback. This method significantly reduces the number of samples needed while boosting code generation accuracy.*** <br>
    Oct 2, Meta published a [paper](https://arxiv.org/pdf/2410.02089) “RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning”. Large language models (LLMs) deployed as agents solve user-specified tasks over multiple steps while keeping the required manual engagement to a minimum. Crucially, such LLMs need to ground their generations in any feedback obtained to reliably achieve desired outcomes. The study proposes an end-to-end reinforcement learning method for teaching models to leverage execution feedback in the realm of code synthesis, where state-of-the-art LLMs struggle to improve code iteratively compared to independent sampling. The study benchmarks on competitive programming tasks, where the authors achieve new start-of-the art results with both small (8B parameters) and large (70B) models while reducing the amount of samples required by an order of magnitude. The analysis of inference-time behavior demonstrates that this method produces LLMs that effectively leverage automatic feedback over multiple steps. <br>


29. ***Advancing Autonomous AI Agents with Reflective Tree Search: <br>Columbia University and Microsoft’s study introduces Reflective Monte Carlo Tree Search (R-MCTS) to improve autonomous agents in complex, multi-step decision-making tasks. By combining contrastive reflection and multi-agent debate, R-MCTS enables dynamic decision exploration and state evaluation for VLMs like GPT-4o. Through self-learning with R-MCTS-generated data, GPT-4o improves 6%-30% on tasks and achieves 97% of R-MCTS’s performance while using four times less compute, suggesting an effective approach for enhancing reasoning and planning in AI agents.*** <br>
    Oct 2, Columbia Uni and Microsoft published a [paper](https://arxiv.org/pdf/2410.01748) “Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning”. Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon planning tasks. To address these limitations, the study introduces Reflective Monte Carlo Tree Search (R-MCTS), a novel test-time algorithm designed to enhance the ability of AI agents, e.g., powered by GPT-4o, to explore decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating contrastive reflection, allowing agents to learn from past interactions and dynamically improve their search efficiency; and 2) using multi-agent debate to provide reliable state evaluation. Moreover, the study improves the agent's performance by fine-tuning GPT-4o through self-learning, using R-MCTS generated tree traversals without any human-provided labels. On the challenging VisualWebArena benchmark, the GPT-4o-based R-MCTS agent achieves a 6% to 30% relative improvement across various tasks compared to the previous state-of-the-art. Additionally, the study shows that the knowledge gained from test-time search can be effectively transferred back to GPT-4o via fine-tuning. The fine-tuned GPT-4o matches 97% of R-MCTS's performance while reducing compute usage by a factor of four at test time. Furthermore, qualitative results reveal that the fine-tuned GPT-4o model demonstrates the ability to explore the environment, evaluate a state, and backtrack to viable ones when it detects that the current state cannot lead to success. Moreover, the work demonstrates the compute scaling properties in both training - data collection with R-MCTS - and testing time. These results suggest a promising research direction to enhance VLMs' reasoning and planning capabilities for agentic applications via test-time search and self-learning. <br>

31. ***Nvidia's paper addresses the lack of comparable data for two key reward modeling approaches: Bradley-Terry style and Regression style. By adding preference annotations to complement existing ratings in their HelpSteer2 dataset, Nvidia enables the first head-to-head comparison of these models. They propose a novel method combining both approaches, leading to improved performance in alignment tasks with a new reward model (Llama-3.1-70B-Instruct). The study demonstrates strong results in RLHF (Reinforcement Learning from Human Feedback) and releases the dataset and trained model for public use.*** <br>
    Oct 2, Nvidia published a [paper](https://arxiv.org/pdf/2410.01257) “HelpSteer2-Preference: Complementing Ratings with Preferences”. Reward models are critical for aligning models to follow instructions, and are typically trained following one of two popular paradigms: Bradley-Terry style or Regression style. However, there is a lack of evidence that either approach is better than the other, when adequately matched for data. This is primarily because these approaches require data collected in different (but incompatible) formats, meaning that adequately matched data is not available in existing public datasets. To tackle this problem, the study releases preference annotations (designed for Bradley-Terry training) to complement existing ratings (designed for Regression style training) in the HelpSteer2 dataset. To improve data interpretability, preference annotations are accompanied with human-written justifications. Using this data, the study conducts the first head-to-head comparison of Bradley-Terry and Regression models when adequately matched for data. Based on insights derived from such a comparison, the study proposes a novel approach to combine Bradley-Terry and Regression reward modeling. A Llama-3.1-70B-Instruct model tuned with this approach scores 94.1 on RewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. The study also demonstrates the effectiveness of this reward model at aligning models to follow instructions in RLHF. The authors open-source this dataset (CC-BY-4.0 license) at https://huggingface.co/datasets/nvidia/HelpSteer2 and openly release the trained Reward Model at https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward <br>

33. ***Enhancing credit assignment in complex reasoning tasks <br>
This paper introduces VinePPO, a new method that improves the Proximal Policy Optimization (PPO) reinforcement learning algorithm, particularly for complex reasoning tasks in LLMs. The authors highlight the challenges faced by value networks in credit assignment and show that VinePPO consistently outperforms PPO and other baselines in fewer updates, using datasets like MATH and GSM8K. The work underscores the importance of better credit assignment mechanisms in reinforcement learning for LLMs.
Reward modeling comparison and combination <br>*** <br>
    Oct 2, Mila, Microsoft, McGill Uni, CIFAR, Uni de Montreal, and HEC Montreal published a [paper](https://arxiv.org/pdf/2410.01679) “VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment”. Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, value networks face challenges in predicting the expected cumulative rewards accurately in complex reasoning tasks, often leading to high-variance updates and suboptimal performance. This work systematically evaluates the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they barely outperform a random baseline when comparing alternative steps. To address this, the work proposes VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates, bypassing the need for large value networks. This method consistently outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These results emphasize the importance of accurate credit assignment in RL finetuning of LLM and demonstrate VinePPO's potential as a superior alternative. <br>

35. ***Zero-shot cross-lingual transfer using layer swapping <br>
UCLA's study presents a model merging approach aimed at improving cross-lingual transfer in large language models for mathematical reasoning tasks. The researchers fine-tune separate experts in math (English) and language and then merge their layers to enhance performance in target languages without in-language math data. The merged model improves by 10% across four languages on the MGSM benchmark, offering a simple and intuitive method for cross-lingual task adaptation.*** <br>
    Oct 2, UCLA published a [paper](https://arxiv.org/pdf/2410.01335) “Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models”. Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. This work presents a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. The study focuses on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, the work fine-tunes separate "experts" on math instruction data in English and on generic instruction data in the target language. The work then replaces the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc. <br>

37. ***Optimizing reasoning without overcoming probability sensitivity <br>
This paper examines OpenAI's o1 system, optimized for reasoning, and compares it to older LLMs. The study finds that while o1 shows significant improvements in complex tasks, it still shares the same sensitivity to probability as previous models. This sensitivity influences performance on low-probability tasks, highlighting a limitation in overcoming autoregressive trends in LLMs.*** <br>
    Oct 2, Yale Uni, OpenAI, Princeton Uni published a [paper](https://arxiv.org/pdf/2410.01792) “When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1”. In "Embers of Autoregression" (McCoy et al., 2023), the authors showed that several large language models (LLMs) have some important limitations that are attributable to their origins in next-word prediction. Here we investigate whether these issues persist with o1, a new system from OpenAI that differs from previous LLMs in that it is optimized for reasoning. The work finds that o1 substantially outperforms previous LLMs in many cases, with particularly large improvements on rare variants of common tasks (e.g., forming acronyms from the second letter of each word in a list, rather than the first letter). Despite these quantitative improvements, however, o1 still displays the same qualitative trends observed in previous systems. Specifically, o1 - like previous LLMs - is sensitive to the probability of examples and tasks, performing better and requiring fewer "thinking tokens" in high-probability settings than in low-probability ones. These results show that optimizing a language model for reasoning can mitigate but might not fully overcome the language model's probability sensitivity. <br>

39. ***Softmax limitations for out-of-distribution sharp decisions <br>
Google and the University of Oxford debunk the belief that softmax functions can robustly handle sharp decision-making in AI systems. They find that softmax circuits disperse with larger inputs, proving this through theoretical work. The authors propose adaptive temperature as a temporary fix but emphasize that softmax is fundamentally inadequate for tasks requiring sharp, consistent behavior on diverse inputs.*** <br>
    Oct 1, Google and Uni of Oxford published a [paper](https://arxiv.org/pdf/2410.01104) “softmax is not enough (for sharp out-of-distribution)”. A key property of reasoning systems is the ability to make sharp decisions on their input data. For contemporary AI systems, a key carrier of sharp behaviour is the softmax function, with its capability to perform differentiable query-key lookups. It is a common belief that the predictive power of networks leveraging softmax arises from "circuits" which sharply perform certain kinds of computations consistently across many diverse inputs. However, for these circuits to be robust, they would need to generalise well to arbitrary valid inputs. This paper dispels this myth: even for tasks as simple as finding the maximum key, any learned circuitry must disperse as the number of items grows at test time. The work attributes this to a fundamental limitation of the softmax function to robustly approximate sharp functions, prove this phenomenon theoretically, and propose adaptive temperature as an ad-hoc technique for improving the sharpness of softmax at inference time.   <br>

41. ***Detecting malicious prompts in vision-language models <br>
This paper introduces VLMGuard, a framework for detecting malicious prompts in vision-language models (VLMs) using unlabeled data. By estimating maliciousness scores and training a binary prompt classifier, VLMGuard outperforms existing methods without requiring extra human annotations. This approach highlights the practical need for reliable VLMs in real-world applications.*** <br>
    Oct 1, Uni of Wisconsin-Madison and Microsoft published a [paper](https://arxiv.org/pdf/2410.00296) “VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data”. Vision-language models (VLMs) are essential for contextual understanding of both visual and textual information. However, their vulnerability to adversarially manipulated inputs presents significant risks, leading to compromised outputs and raising concerns about the reliability in VLM-integrated applications. Detecting these malicious prompts is thus crucial for maintaining trust in VLM generations. A major challenge in developing a safeguarding prompt classifier is the lack of a large amount of labeled benign and malicious data. To address the issue, the work introduces VLMGuard, a novel learning framework that leverages the unlabeled user prompts in the wild for malicious prompt detection. These unlabeled prompts, which naturally arise when VLMs are deployed in the open world, consist of both benign and malicious information. To harness the unlabeled data, the study presents an automated maliciousness estimation score for distinguishing between benign and malicious samples within this unlabeled mixture, thereby enabling the training of a binary prompt classifier on top. Notably, the framework does not require extra human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiment shows VLMGuard achieves superior detection results, significantly outperforming state-of-the-art methods. Disclaimer: This paper may contain offensive examples; reader discretion is advised. <br>

43. ***Cross capabilities expose LLM performance weaknesses <br>
Meta and UIUC's research introduces CrossEval, a benchmark that tests LLMs' ability to handle tasks requiring multiple cross capabilities. The results show that LLMs tend to perform worse in these cross-capability tasks due to weaknesses in the least developed skills. The study suggests that addressing these weak links should be a research priority to improve LLMs' real-world utility in complex tasks.*** <br>
    Sep 30, Meta and UIUC published a [paper](https://arxiv.org/pdf/2409.19951) “Law of the Weakest Link: Cross Capabilities of Large Language Models”. The development and evaluation of Large Language Models (LLMs) have largely focused on individual capabilities. However, this overlooks the intersection of multiple abilities across different types of expertise that are often required for real-world tasks, which the authors term cross capabilities. To systematically explore this concept, the study first defines seven core individual capabilities and then pair them to form seven common cross capabilities, each supported by a manually constructed taxonomy. Building on these definitions, the study introduces CrossEval, a benchmark comprising 1,400 human-annotated prompts, with 100 prompts for each individual and cross capability. To ensure reliable evaluation, the study involves expert annotators to assess 4,200 model responses, gathering 8,400 human ratings with detailed explanations to serve as reference examples. The findings of the study reveal that, in both static evaluations and attempts to enhance specific abilities, current LLMs consistently exhibit the "Law of the Weakest Link," where cross-capability performance is significantly constrained by the weakest component. Specifically, across 58 cross-capability scores from 17 models, 38 scores are lower than all individual capabilities, while 20 fall between strong and weak, but closer to the weaker ability. These results highlight the under-performance of LLMs in cross-capability tasks, making the identification and improvement of the weakest capabilities a critical priority for future research to optimize performance in complex, multi-dimensional scenarios. <br>

45.	***Enhancing multimodal LLMs through data-centric training:  <br>MM1.5 is a significant upgrade of MM1. With one single set of weights, MM1.5 excels at (1) reading your charts, tables, and any text-rich images, (2) understanding visual prompts like points and boxes, providing grounded outputs, and (3) multi-image reasoning.*** <br>
Sep 30, Apple published a [paper](https://arxiv.org/pdf/2409.20566) “MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning”. The paper presents MM1.5, a new family of multimodal large language models (MLLMs) designed to enhance capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. Building upon the MM1 architecture, MM1.5 adopts a data-centric approach to model training, systematically exploring the impact of diverse data mixtures across the entire model training lifecycle. This includes high-quality OCR data and synthetic captions for continual pre-training, as well as an optimized visual instruction-tuning data mixture for supervised fine-tuning. The models range from 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE) variants, and demonstrate that careful data curation and training strategies can yield strong performance even at small scales (1B and 3B). Additionally, the work introduces two specialized variants: MM1.5-Video, designed for video understanding, and MM1.5-UI, tailored for mobile UI understanding. Through extensive empirical studies and ablations, the authors provide detailed insights into the training processes and decisions that inform the final designs, offering valuable guidance for future research in MLLM development. <br>

24. ***Compositional generalization through skill mixing in LLMs <br>
Princeton University's study examines compositional generalization, specifically how LLMs can combine multiple skills unseen during training. Using a SKILL-MIX evaluation, the authors show that training on smaller skill combinations enhances performance in more complex tasks. The study underscores the potential of incorporating skill-rich texts to boost models' generalization abilities.*** <br>
    Sep 29, Princeton Uni published a [paper](https://arxiv.org/pdf/2409.19808) “Can Models Learn Skill Composition from Examples”. As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalization -- the capacity to combine learned skills in novel ways not encountered during training -- has garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment. A recent study introduced the SKILL-MIX evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified k-tuple of language skills. While small models struggled with composing even with k=3, larger models like GPT-4 performed reasonably well with k=5 and 6. This study employs a setup akin to SKILL-MIX to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills -- including rhetorical, literary, reasoning, theory of mind, and common sense -- GPT-4 was used to generate text samples that exhibit random subsets of k skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of k, revealed the following findings: (1) Training on combinations of k=2 and 3 skills results in noticeable improvements in the ability to compose texts with k=4 and 5 skills, despite models never having seen such examples during training. (2) When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills. This study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models. <br>

26. ***Data-augmented prediction for LLM-based classification <br>
The University of Arizona's paper introduces "Language Model Learning" (LML) for classification tasks, powered by a new method called Data-Augmented Prediction (DAP). LML uses LLMs to understand and classify data by referencing relevant datasets, achieving high accuracy without traditional data cleaning and feature engineering. The study shows that LML could outperform conventional ML models in many scenarios.*** <br>
    Sep 28, Uni of Arizona published a [paper](https://arxiv.org/pdf/2409.18957) “LML: Language Model Learning a Dataset for Data-Augmented Prediction”. This paper introduces a new approach to using Large Language Models (LLMs) for classification tasks, which are typically handled using Machine Learning (ML) models. Unlike ML models that rely heavily on data cleaning and feature engineering, this method streamlines the process using LLMs. This paper proposes a new concept called "Language Model Learning (LML)" powered by a new method called "Data-Augmented Prediction (DAP)". The classification is performed by LLMs using a method similar to humans manually exploring and understanding the data and deciding classifications using data as a reference. Training data is summarized and evaluated to determine the features that lead to the classification of each label the most. In the process of DAP, the system uses the data summary to automatically create a query, which is used to retrieve relevant rows from the dataset. A classification is generated by the LLM using data summary and relevant rows, ensuring satisfactory accuracy even with complex data. Usage of data summary and similar data in DAP ensures context-aware decision-making. The proposed method uses the words "Act as an Explainable Machine Learning Model" in the prompt to enhance the interpretability of the predictions by allowing users to review the logic behind each prediction. In some test cases, the system scored an accuracy above 90%, proving the effectiveness of the system and its potential to outperform conventional ML models in various scenarios. The code is available at https://github.com/Pro-GenAI/LML-DAP <br>

28. ***Comprehensive evaluation of OpenAI’s o1 system <br>
A multi-institutional team evaluated OpenAI's o1-preview system, highlighting its human-level performance across diverse complex tasks such as programming, medical diagnosis, and financial modeling. The study reveals o1's remarkable progress toward artificial general intelligence (AGI), with strong reasoning capabilities across domains, although it still faces challenges with simpler tasks and certain specialized concepts.*** <br>
    Sep 27, about 40 researchers from different universities/institutes include Uni of Alberta, Uni of Georgia etc. published a [paper](https://arxiv.org/pdf/2409.18486) “Evaluation of OpenAI o1: Opportunities and Challenges of AGI”. This comprehensive study evaluates the performance of OpenAI's o1-preview large language model across a diverse array of complex reasoning tasks, spanning multiple domains, including computer science, mathematics, natural sciences, medicine, linguistics, and social sciences. Through rigorous testing, o1-preview demonstrated remarkable capabilities, often achieving human-level or superior performance in areas ranging from coding challenges to scientific reasoning and from language processing to creative problem-solving. Key findings include: 1) 83.3% success rate in solving complex competitive programming problems, surpassing many human experts. 2) Superior ability in generating coherent and accurate radiology reports, outperforming other evaluated models. 3) 100% accuracy in high school-level mathematical reasoning tasks, providing detailed step-by-step solutions. 4) Advanced natural language inference capabilities across general and specialized domains like medicine. 5) Impressive performance in chip design tasks, outperforming specialized models in areas such as EDA script generation and bug analysis. 6) Remarkable proficiency in anthropology and geology, demonstrating deep understanding and reasoning in these specialized fields. 7) Strong capabilities in quantitative investing. O1 has comprehensive financial knowledge and statistical modeling skills. 8) Effective performance in social media analysis, including sentiment analysis and emotion recognition. 
The model excelled particularly in tasks requiring intricate reasoning and knowledge integration across various fields. While some limitations were observed, including occasional errors on simpler problems and challenges with certain highly specialized concepts, the overall results indicate significant progress towards artificial general intelligence. <br>

30. ***Generative AI in data analysis workflows <br>
Microsoft's paper explores the potential of generative AI in reshaping data analysis workflows. The authors discuss human-centered design principles that help facilitate intuitive interactions and build user trust. The paper also highlights the challenges in developing AI tools, such as improving model capabilities and ensuring they meet end-user needs.*** <br>
    Sep 27, Microsoft published a [paper](https://arxiv.org/pdf/2409.18475) “Data Analysis in the Era of Generative AI”. This paper explores the potential of AI-powered tools to reshape data analysis, focusing on design considerations and challenges. The paper explores how the emergence of large language and multimodal models offers new opportunities to enhance various stages of data analysis workflow by translating high-level user intentions into executable code, charts, and insights. The authors then examine human-centered design principles that facilitate intuitive interactions, build user trust, and streamline the AI-assisted analysis workflow across multiple apps. Finally, the paper discusses the research challenges that impede the development of these AI-based systems such as enhancing model capabilities, evaluating and benchmarking, and understanding end-user needs. <br>

32. ***Efficient low-bit quantization for large language models <br>
This paper introduces VPTQ, a new method for extremely low-bit quantization in LLMs, reducing memory and computational requirements while maintaining accuracy. The method uses vector quantization and second-order optimization, leading to significant improvements in quantization perplexity and model performance across several benchmarks. VPTQ offers a solution for deploying large-scale models more efficiently.*** <br>
    Sep 25, Microsoft and USTC published a [paper](https://arxiv.org/pdf/2409.17066) “VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models”.  Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit. Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables. This paper introduces Vector Post-Training Quantization (VPTQ) for extremely low-bit quantization of LLMs. The study uses Second-Order Optimization to formulate the LLM VQ problem and guide the quantization algorithm design by solving the optimization. The work further refines the weights using Channel-Independent Second-Order Optimization for a granular VQ. In addition, by decomposing the optimization problem, the study proposes a brief and effective codebook initialization algorithm. The authors also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model. The experimental results show that VPTQ reduces model quantization perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, 4.41-7.34 on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, 11-22% on LLaMA-3 on QA tasks on average. We only utilize 10.4-18.6% of the quantization algorithm execution time, resulting in a 1.6-1.8times increase in inference throughput compared to SOTA. <br>

34. ***Scalable time series forecasting with Time-MoE <br>
Princeton University introduces Time-MoE, a mixture-of-experts model designed to improve time series forecasting while reducing computational costs. By pre-training on a large dataset spanning multiple domains, Time-MoE demonstrates superior performance compared to dense models, establishing it as a new state-of-the-art in time series forecasting.*** <br>
    Sep 24, Princeton Uni published a [paper](https://arxiv.org/pdf/2409.16040) “Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts”. Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, the study introduces Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. The study pre-trained these models on a newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, the study scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Experimental results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, the models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility. Code is available here.
 <br><br><br>

***Sep 29***

1. ***YOLO11 Released by Ultralytics:   <br>YOLO11 introduces enhanced speed, accuracy, and versatility, surpassing its predecessors in computer vision tasks like real-time object detection and classification. Key features include improved feature extraction, fewer parameters, and faster processing, making YOLO11 a game-changer for developers and researchers. It also extends its capabilities to advanced tasks such as pose estimation and instance segmentation.*** <br><br>
   Sep 27, Ultralytics [released YOLO 11](https://www.ultralytics.com/blog/ultralytics-yolo11-has-arrived-redefine-whats-possible-in-ai), the latest AI model redefining computer vision with unmatched accuracy and efficiency. YOLO11! Building on the impressive advancements of previous YOLO model versions, YOLO11 brings a host of powerful features and optimizations that make it faster, more accurate, and incredibly versatile. With its innovative architecture, YOLO11 can be used for various computer vision tasks, from real-time object detection to classification, making it a game-changer for developers and researchers alike. Key improvements include enhanced feature extraction for more precise detail capture, greater accuracy with fewer parameters, and faster processing speeds that significantly improve real-time performance. YOLO11 marks a new chapter for the YOLO family, offering a more capable and versatile model that takes computer vision to new heights. With its refined architecture and enhanced capabilities, the model supports computer vision tasks like pose estimation and instance segmentation that the Vision AI community has come to love about Ultralytics YOLOv8, but with even greater performance and precision. <br><br>

3. ***Meta’s Llama 3.2 Multimodal Models:   <br>Llama 3.2 is a collection of multilingual large language models (LLMs) available in text and image processing formats. It includes 1B and 3B text models designed for on-device tasks and larger 11B and 90B multimodal models optimized for image understanding. Meta is also simplifying Llama deployment across various environments, emphasizing the importance of openness for driving innovation.*** <br><br>
   Sep 26, Meta [released Llama 3.2](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/), the lightweight and multimodal models. The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge. The Llama 3.2 11B and 90B vision models are drop-in replacements for their corresponding text model equivalents, while exceeding on image understanding tasks compared to closed models, such as Claude 3 Haiku. Meta is sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety. Meta will continue to share its work because it believes [openness drives innovation](https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/) and is good for developers, Meta, and the world. <br><br>
 
5. ***FPT Software AI Center’s HyperAgent for Software Engineering:   <br>HyperAgent is a generalist multi-agent system designed to handle a wide range of software engineering (SE) tasks. It uses four specialized agents (Planner, Navigator, Code Editor, and Executor) to manage SE tasks from conception to verification. The system achieves state-of-the-art performance across diverse tasks, significantly improving coding practices.*** <br><br>
   Sep 26, FPT Sofware AI Center published a [paper](https://arxiv.org/pdf/2409.16299) “HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks at Scale”. Large Language Models (LLMs) have revolutionized software engineering (SE), demonstrating remarkable capabilities in various coding tasks. While recent efforts have produced autonomous software agents based on LLMs for end-to-end development tasks, these systems are typically designed for specific SE tasks. The study introduces HyperAgent, a novel generalist multi-agent system designed to address a wide spectrum of SE tasks across different programming languages by mimicking human developers' workflows. Comprising four specialized agents - Planner, Navigator, Code Editor, and Executor. HyperAgent manages the full lifecycle of SE tasks, from initial conception to final verification. Through extensive evaluations, HyperAgent achieves state-of-the-art performance across diverse SE tasks: it attains a 25.01% success rate on SWE-Bench-Lite and 31.40% on SWE-Bench-Verified for GitHub issue resolution, surpassing existing methods. Furthermore, HyperAgent demonstrates SOTA performance in repository-level code generation (RepoExec), and in fault localization and program repair (Defects4J), often outperforming specialized systems. This work represents a significant advancement towards versatile, autonomous agents capable of handling complex, multi-step SE tasks across various domains and languages, potentially transforming AI-assisted software development practices. <br><br>

7. ***Leadership Shakeup at OpenAI:   <br>Several key executives, including CTO Mira Murati, left OpenAI, raising concerns about leadership stability as the company pursues a controversial growth strategy. OpenAI is in talks for a new fundraising round, but the leadership exits leave CEO Sam Altman with fewer key leaders during this critical time of restructuring.*** <br><br>
   Sep 26, [according to CNN](https://edition.cnn.com/2024/09/25/tech/openai-technology-chief-mira-murati-leaving/index.html?utm_source=neuralfrontier.beehiiv.com&utm_medium=newsletter&utm_campaign=meta-launches-orion-at-meta-connect&_bhlid=eab99b8c7f02019f455b5492c1bac68fde7e1f07), Three more execs out at OpenAI, including technology chief Mira Murati. Murati — who has been instrumental in the development of ChatGPT and the artificial intelligence image generator Dall-E — said Wednesday afternoon in a post on X that she is leaving the company. Shortly after, OpenAI’s Chief Research Officer Bob McGrew and Vice President of Research Barret Zoph also announced their decisions to exit. The Wednesday departures are just the latest in a string of executives who have recently left OpenAI. The leadership shakeup comes as the ChatGPT-maker attempts to forge a controversial path to growth, including making it easier to raise funds from investors and generate revenue. OpenAI is reportedly in talks about a new fundraising round that could value the firm at $150 billion, Bloomberg and others have reported. Later on Wednesday, Altman updated his post to acknowledge the exits of Zoph and McGrew. Taken together, the string of departures leave CEO Sam Altman without much of the leadership team that helped him rapidly grow OpenAI into an artificial intelligence juggernaut, and could consolidate power under Altman just as the company seeks to restructure. OpenAI did not immediately respond to a question about the timeline for Murati’s formal exit for the company, or if and when a new CTO would be announced. <br><br>

9. ***Cambridge and Google’s “DARE” for Visual Question Answering:   <br>The DARE study introduces a novel approach to image captioning using Image-like Retrieval and a Fusion Module to bridge the modality gap between text training and image inference. The approach significantly improves caption quality and outperforms state-of-the-art methods in both image and video captioning tasks.*** <br><br>
    Sep 26, Uni of Cambridge and Google published a [paper](https://arxiv.org/pdf/2409.18023) “DARE: Diverse Visual Question Answering with Robustness Evaluation”. Recent advancements in image captioning have explored text-only training methods to overcome the limitations of paired image-text data. However, existing text-only training methods often overlook the modality gap between using text data during training and employing images during inference. To address this issue, the study proposes a novel approach called Image-like Retrieval, which aligns text features with visually relevant features to mitigate the modality gap. The method further enhances the accuracy of generated captions by designing a Fusion Module that integrates retrieved captions with input features. Additionally, the study introduces a Frequency-based Entity Filtering technique that significantly improves caption quality. The authors integrate these methods into a unified framework, which the authors refer to as IFCap (Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning). Through extensive experimentation, the straightforward yet powerful approach has demonstrated its efficacy, outperforming the state-of-the-art methods by a significant margin in both image captioning and video captioning compared to zero-shot captioning based on text-only training. <br><br>

11. ***Challenges in Vision-Language Compositionality for CLIP:   <br>The study reveals that current benchmarks overestimate improvements in compositionality for vision-language models like CLIP when trained with hard negatives. Including both hard positives and negatives in the training dataset improves model performance, suggesting the need for more robust benchmarks.*** <br><br>
    Sep 26, Uni of Washington, Uni of California Los Angeles and Allen Inst of AI published a [paper](https://arxiv.org/pdf/2409.17958) on ECCV2024 “The Hard Positive Truth about Vision-Language Compositionality”. Several benchmarks have concluded that the best vision-language models (e.g., CLIP) are lacking in compositionality. Given an image, these benchmarks probe a model's ability to identify its associated caption amongst a set of compositional distractors. In response, a surge of recent proposals show improvements by finetuning CLIP with distractors as hard negatives. The investigations reveal that these improvements have, in fact, been significantly overstated -- because existing benchmarks do not probe whether finetuned vision-language models remain invariant to hard positives. By curating an evaluation dataset with 112,382 hard negatives and hard positives, the authors uncover that including hard positives decreases CLIP's performance by 12.9%, while humans perform effortlessly at 99%. CLIP finetuned with hard negatives results in an even larger decrease, up to 38.7%. With this finding, the study then produces a 1,775,259 image-text training set with both hard negative and hard positive captions. By training with both, the authors see improvements on existing benchmarks while simultaneously improving performance on hard positives, indicating a more robust improvement in compositionality. The work suggests the need for future research to rigorously test and improve CLIP's understanding of semantic relationships between related "positive" concepts. <br><br>

13. ***Larger Language Models Becoming Less Reliable:   <br>A Nature study shows that while scaling up and refining large language models (LLMs) improve certain capabilities, they also introduce new reliability issues. Larger models tend to provide plausible but incorrect answers more frequently and struggle with task stability. The findings call for a shift in AI development strategies.*** <br><br>
    Sep 25, Nature published a [paper](https://www.nature.com/articles/s41586-024-07930-y.pdf) “Larger and more instructable language models become less reliable”. The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources) and bespoke shaping up (including post-filtering, fine tuning or use of human feedback). However, larger and more instructable large language models may have become less reliable. By studying the relationship between difficulty concordance, task avoidance and prompting stability of several language model families, the study shows that easy instances for human participants are also easy for the models, but scaled-up, shaped-up models do not secure areas of low difficulty in which either the model does not err or human supervision can spot the errors. The study also finds that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, The study observes that stability to different natural phrasings of the same question is improved by scaling-up and shaping-up interventions, but pockets of variability persist across difficulty levels. These findings highlight the need for a fundamental shift in the design and development of general-purpose artificial intelligence, particularly in high-stakes areas for which a predictable distribution of errors is paramount. <br><br>

15. ***Molmo and PixMo: Open Multimodal Models:   <br>Molmo introduces a family of state-of-the-art multimodal models built entirely on open data and weights. It relies on human-annotated datasets for image captioning and outperforms both open and proprietary models like GPT-4o. Molmo emphasizes openness in model development, aiming to democratize advanced AI capabilities.*** <br><br>
    Sep 25, Allen Inst of AI and Uni of Washington published a [paper](https://arxiv.org/pdf/2409.17146) “Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models”. Today's most advanced multimodal models remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed models into open ones. As a result, the community is still missing foundational knowledge about how to build performant VLMs from scratch. The study presents Molmo, a new family of VLMs that are state-of-the-art in their class of openness. The key innovation is a novel, highly detailed image caption dataset collected entirely from human annotators using speech-based descriptions. To enable a wide array of user interactions, the study also introduces a diverse dataset mixture for fine-tuning that includes in-the-wild Q&A and innovative 2D pointing data. The success of the approach relies on careful choices for the model architecture details, a well-tuned training pipeline, and, most critically, the quality of the newly collected datasets, all of which will be released. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human evaluation. The athors will be releasing all of the model weights, captioning and fine-tuning data, and source code in the near future. Select model weights, inference code, and demo are available at https://molmo.allenai.org. <br><br>

17. ***VectorSearch: Optimized Document Retrieval:   <br>VectorSearch addresses challenges in semantic retrieval by using advanced embeddings and indexing techniques. It improves accuracy in large-scale document retrieval tasks by closing the semantic gaps that traditional methods and deep learning approaches struggle with.*** <br><br>
    Sep 25, Uni of Washington published a [paper](https://arxiv.org/pdf/2409.17383) “VectorSearch: Enhancing Document Retrieval with Semantic Embeddings and Optimized Search”. Traditional retrieval methods have been essential for assessing document similarity but struggle with capturing semantic nuances. Despite advancements in latent semantic analysis (LSA) and deep learning, achieving comprehensive semantic understanding and accurate retrieval remains challenging due to high dimensionality and semantic gaps. The above challenges call for new techniques to effectively reduce the dimensions and close the semantic gaps. To this end, the study proposes VectorSearch, which leverages advanced algorithms, embeddings, and indexing techniques for refined retrieval. By utilizing innovative multi-vector search operations and encoding searches with advanced language models, the approach significantly improves retrieval accuracy. Experiments on real-world datasets show that VectorSearch outperforms baseline metrics, demonstrating its efficacy for large-scale retrieval tasks. <br><br>

19. ***Sam Altman’s “The Intelligence Age” Vision:   <br>Altman envisions a future driven by AI that will enable extraordinary societal advancements. He emphasizes that AI can solve complex problems, improve global living standards, and usher in an era of prosperity, but warns of potential economic disruptions and ethical concerns that must be navigated carefully.*** <br><br>
    Sep 23, Sam Altman published an [article](https://ia.samaltman.com/) “The Intelligence Age”. The article envisions a future where AI and technology enable extraordinary advancements that seem magical by today’s standards. This acceleration of progress builds on the accomplishments of previous generations, with AI acting as a tool to solve complex problems and enhance human capabilities. The author emphasizes how deep learning, with its ability to learn patterns in data, has driven recent breakthroughs and will continue to do so as compute and data scale. AI will enable personalized services such as education and healthcare, and lead to unprecedented prosperity. While prosperity alone may not guarantee happiness, it can improve global living standards. The transition from the Stone Age to the Intelligence Age is likened to previous technological revolutions, requiring energy, compute power, and wise decisions. The article warns of potential challenges, including economic shifts and ethical concerns, but advocates for navigating risks thoughtfully to maximize benefits. Ultimately, the future is seen as one of limitless possibilities, where AI-driven advancements will reshape society, leading to breakthroughs like fixing climate change and space exploration. Although AI may disrupt jobs, the author believes human creativity and purpose will persist, ushering in a new era of collective growth and shared progress. <br><br>

21. ***Exploring OpenAI’s o1 in Medicine:   <br>OpenAI's o1 model shows promise in medical tasks, outperforming previous models like GPT-4 in reasoning and multilingual understanding across 37 datasets. However, the study also identifies weaknesses, such as hallucination and inconsistent multilingual abilities, highlighting areas for improvement in AI for medicine.*** <br><br>
    Sep 23, UC Santa Cruz, Uni of Edinburgh and NIH published a [paper](https://arxiv.org/pdf/2409.15277) “A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?”. Large language models (LLMs) have exhibited remarkable capabilities across various domains and tasks, pushing the boundaries of human knowledge in learning and cognition. The latest model, OpenAI's o1, stands out as the first LLM with an internalized chain-of-thought technique using reinforcement learning strategies. While it has demonstrated surprisingly strong capabilities on various general language tasks, its performance in specialized fields such as medicine remains unknown. To this end, this report provides a comprehensive exploration of o1 on different medical scenarios, examining 3 key aspects: understanding, reasoning, and multilinguality. Specifically, this evaluation encompasses 6 tasks using data from 37 medical datasets, including two newly constructed and more challenging question-answering (QA) tasks based on professional medical quizzes from the New England Journal of Medicine (NEJM) and The Lancet. These datasets offer greater clinical relevance compared to standard medical QA benchmarks such as MedQA, translating more effectively into real-world clinical utility. The analysis of o1 suggests that the enhanced reasoning ability of LLMs may (significantly) benefit their capability to understand various medical instructions and reason through complex clinical scenarios. Notably, o1 surpasses the previous GPT-4 in accuracy by an average of 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios. But meanwhile, the authors identify several weaknesses in both the model capability and the existing evaluation protocols, including hallucination, inconsistent multilingual ability, and discrepant metrics for evaluation. Raw data and model outputs is available at https://ucsc-vlaa.github.io/o1_medicine/ for future research. <br><br>

23. ***Phantom LLVMs for Efficient AI:   <br>The Phantom LLVM family offers smaller, more efficient models with strong performance in vision-language tasks. By temporarily expanding hidden layers during self-attention, Phantom achieves the performance of larger models while maintaining a smaller physical size, offering a leading solution for efficient large models.*** <br><br>
    Sep 23, KAIST published a [paper](https://arxiv.org/pdf/2409.14713) “Phantom of Latent for Large Language and Vision Models”. The success of visual instruction tuning has accelerated the development of large language and vision models (LLVMs). Following the scaling laws of instruction-tuned large language models (LLMs), LLVMs either have further increased their sizes, reaching 26B, 34B, and even 80B parameters. While this increase in model size has yielded significant performance gains, it demands substantially more hardware resources for both training and inference. Consequently, there naturally exists a strong need for efficient LLVMs that achieve the performance of larger models while being smaller in size. To achieve this need, the paper presents a new efficient LLVM family with model sizes of 0.5B, 1.8B, 3.8B, and 7B parameters, Phantom, which significantly enhances learning capabilities within limited structures. By temporarily increasing the latent hidden dimension during multi-head self-attention (MHSA), the authors make LLVMs prepare to look and understand much more vision-language knowledge on the latent, without substantially increasing physical model sizes. To maximize its advantage, the authors introduce Phantom Optimization (PO) using both autoregressive supervised fine-tuning (SFT) and direct preference optimization (DPO)-like concept, which effectively follows correct answers while eliminating incorrect and ambiguous ones. Phantom outperforms numerous larger open- and closed-source LLVMs, positioning itself as a leading solution in the landscape of efficient LLVMs. Code is available in https://github.com/ByungKwanLee/Phantom. <br><br>

25. ***Rethinking Machine Learning Principles for Scaling:   <br>A Google paper explores how traditional machine learning principles, such as regularization, are becoming less relevant in the era of scaling large models. The study highlights a phenomenon called “scaling law crossover,” where methods effective at smaller scales fail to generalize to larger ones, urging the development of new guiding principles for scaling.*** <br><br>
    Sep 23, Google published a [paper](https://arxiv.org/pdf/2409.15156) “Rethinking Conventional Wisdom in Machine Learning: From Generalization to Scaling”. The remarkable success of large language pretraining and the discovery of scaling laws signify a paradigm shift in machine learning. Notably, the primary objective has evolved from minimizing generalization error to reducing approximation error, and the most effective strategy has transitioned from regularization (in a broad sense) to scaling up models. This raises a critical question: Do the established principles that proved successful in the generalization-centric era remain valid in this new era of scaling? This paper examines several influential regularization-based principles that may no longer hold true in the scaling-centric, large language model (LLM) era. These principles include explicit L2 regularization and implicit regularization through small batch sizes and large learning rates. Additionally, the study identifies a new phenomenon termed “scaling law crossover,” where two scaling curves intersect at a certain scale, implying that methods effective at smaller scales may not generalize to larger ones. Together, these observations highlight two fundamental questions within this new paradigm: 1) Guiding Principles for Scaling: If regularization is no longer the primary guiding principle for model design, what new principles are emerging to guide scaling? 2) Model Comparison at Scale: How to reliably and effectively compare models at the scale where only a single experiment is feasible? <br><br>

27. ***Improving AI Safety with Backtracking:   <br>A new method called backtracking, introduced by Meta and CMU, allows language models to recover from unsafe text generation. By incorporating a [RESET] token, models trained with this method significantly improve safety without losing helpfulness, offering better protection against adversarial attacks.*** <br><br>
    Sep 22, Meta and CMU published a [paper](https://arxiv.org/pdf/2409.14586) “Backtracking Improves Generation Safety”. Text generation has a fundamental limitation almost by definition: there is no taking back tokens that have been generated, even when they are clearly problematic. In the context of language model safety, when a partial unsafe generation is produced, language models by their nature tend to happily keep on generating similarly unsafe additional text. This is in fact how safety alignment of frontier models gets circumvented in the wild, despite great efforts in improving their safety. Deviating from the paradigm of approaching safety alignment as prevention (decreasing the probability of harmful responses), the paper proposes backtracking, a technique that allows language models to "undo" and recover from their own unsafe generation through the introduction of a special [RESET] token. The method can be incorporated into either SFT or DPO training to optimize helpfulness and harmlessness. The work shows that models trained to backtrack are consistently safer than baseline models: backtracking Llama-3-8B is four times more safe than the baseline model (6.1\% → 1.5\%) in the evaluations without regression in helpfulness. The method additionally provides protection against four adversarial attacks including an adaptive attack, despite not being trained to do so. <br><br>

29. ***Mitigating Reward Hacking in AI Models:   <br>A new framework introduced by Google and UIUC improves reward model training by disentangling relevant preferences from artifacts like response length. This robust reward model enhances AI's alignment with human preferences, significantly improving the performance of reward-based policies in large-scale AI systems.*** <br><br>
    Sep 20, Google, UIUC et al published a [paper](https://arxiv.org/pdf/2409.13156) “RRM: Robust Reward Model Training Mitigates Reward Hacking”. Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. However, traditional RM training, which relies on response pairs tied to specific prompts, struggles to disentangle prompt-driven preferences from prompt-independent artifacts, such as response length and format. This work exposes a fundamental limitation of current RM training methods, where RMs fail to effectively distinguish between contextual signals and irrelevant artifacts when determining preferences. To address this, the work introduces a causal framework that learns preferences independent of these artifacts and propose a novel data augmentation technique designed to eliminate them. Extensive experiments show that the approach successfully filters out undesirable artifacts, yielding a more robust reward model (RRM). The RRM improves the performance of a pairwise reward model trained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to 84.15%. Additionally, the authors train two DPO policies using both the RM and RRM, demonstrating that the RRM significantly enhances DPO-aligned policies, improving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in AlpacaEval-2 from 33.46% to 52.49%. <br><br>

31. ***LLMs' Role in Retrieval-Augmented Generation (RAG)  <br>A paper from Harvard and Google introduces "FRAMES" to evaluate LLMs' performance in retrieval-augmented generation (RAG). The dataset tests LLMs' factual response accuracy, retrieval efficiency, and reasoning ability through complex multi-hop questions. Current state-of-the-art LLMs show improved accuracy from 0.40 to 0.66 with multi-step retrieval, but there remains significant room for improvement in creating robust RAG systems.*** <br><br>
    Sep 20, Harvard Uni and Google published a [paper](https://arxiv.org/pdf/2409.12941) “Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation”. Large Language Models (LLMs) have demonstrated significant performance improvements across various cognitive tasks. An emerging application is using LLMs to enhance retrieval-augmented generation (RAG) capabilities. These systems require LLMs to understand user queries, retrieve relevant information, and synthesize coherent and accurate responses. Given the increasing real-world deployment of such systems, comprehensive evaluation becomes crucial. To this end, the study proposes FRAMES (Factuality, Retrieval, And reasoning MEasurement Set), a high-quality evaluation dataset designed to test LLMs' ability to provide factual responses, assess retrieval capabilities, and evaluate the reasoning required to generate final answers. While previous work has provided datasets and benchmarks to evaluate these abilities in isolation, FRAMES offers a unified framework that provides a clearer picture of LLM performance in end-to-end RAG scenarios. The dataset comprises challenging multi-hop questions that require the integration of information from multiple sources. The authors present baseline results demonstrating that even state-of-the-art LLMs struggle with this task, achieving 0.40 accuracy with no retrieval. The accuracy is significantly improved with the proposed multi-step retrieval pipeline, achieving an accuracy of 0.66 (>50% improvement). The authors hope the work will help bridge evaluation gaps and assist in developing more robust and capable RAG systems. <br><br>

33. ***LLMs' Planning Abilities Examined via PlanBench  <br>Arizona State University evaluates OpenAI’s "o1" (Strawberry) model, which claims to improve LLMs' planning capabilities, a core competence in AI. Despite surpassing previous models in planning tasks on the PlanBench benchmark, o1 still falls short of fully solving the benchmark's challenges. This raises questions about the efficiency, accuracy, and reliability of such systems before deployment.*** <br><br>
    Sep 20, Arizona State Uni published a [paper](https://www.arxiv.org/pdf/2409.13373) “LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench”. The ability to plan a course of action that achieves a desired state of affairs has long been considered a core competence of intelligent agents and has been an integral part of AI research since its inception. With the advent of large language models (LLMs), there has been considerable interest in the question of whether or not they possess such planning abilities. PlanBench, an extensible benchmark developed in 2022, soon after the release of GPT3, has remained an important tool for evaluating the planning abilities of LLMs. Despite the slew of new private and open source LLMs since GPT3, progress on this benchmark has been surprisingly slow. OpenAI claims that their recent o1 (Strawberry) model has been specifically constructed and trained to escape the normal limitations of autoregressive LLMs--making it a new kind of model: a Large Reasoning Model (LRM). Using this development as a catalyst, this paper takes a comprehensive look at how well current LLMs and new LRMs do on PlanBench. While o1's performance is a quantum improvement on the benchmark, outpacing the competition, it is still far from saturating it. This improvement also brings to the fore questions about accuracy, efficiency, and guarantees which must be considered before deploying such systems. <br><br>

35. ***Michelangelo's Long-Context Evaluation Framework  <br>Google introduces Michelangelo, a synthetic evaluation for testing LLMs' long-context reasoning abilities through a novel framework called Latent Structure Queries (LSQ). The evaluation focuses on how well models handle large, complex contexts by identifying hidden structures. The study demonstrates that LLMs show significant potential in long-context processing, but there remains much room for improvement.*** <br><br>
    Sep 20, Google published a [paper](https://arxiv.org/pdf/2409.12640) “Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries”. The paper introduces Michelangelo: a minimal, synthetic, and unleaked long-context reasoning evaluation for large language models which is also easy to automatically score. This evaluation is derived via a novel, unifying framework for evaluations over arbitrarily long contexts which measure the model's ability to do more than retrieve a single piece of information from its context. The central idea of the Latent Structure Queries framework (LSQ) is to construct tasks which require a model to “chisel away” the irrelevant information in the context, revealing a latent structure in the context. To verify a model's understanding of this latent structure, the study queries the model for details of the structure. Using LSQ, the authors produce three diagnostic long-context evaluations across code and natural-language domains intended to provide a stronger signal of long-context language model capabilities. The authors perform evaluations on several state-of-the-art models and demonstrate both that a) the proposed evaluations are high-signal and b) that there is significant room for improvement in synthesizing long-context information. <br><br>

37. ***OpenAI O1’s Use of External SAT Solvers  <br>The University of Florence analyzes OpenAI’s O1-preview model for solving K-SAT problems. Results indicate that the model often relies on external SAT solvers rather than solving the problems internally. The study critiques the model for occasionally outputting incorrect solutions and questions whether the model exhibits true intelligence or is making random guesses.*** <br><br>
    Sep 20, Uni of Florence published a [paper](https://arxiv.org/pdf/2409.11232) “Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT Problem: Does the LLM Solve the Problem Itself or Call an External SAT Solver?” The manuscript  presents an analysis on the performance of OpenAI O1-preview model in solving random K-SAT instances for K∈ 2, 3, 4 as a function of α = M/N where M is the number of clauses and N is the number of variables of the satisfiable problem. The study shows that the model can call an external SAT solver to solve the instances, rather than solving them directly. Despite using external solvers, the model reports incorrect assignments as output. Moreover, the study proposes and presents an analysis to quantify whether the OpenAI O1-preview model demonstrates a spark of intelligence or merely makes random guesses when outputting an assignment for a Boolean satisfiability problem. <br><br>

39. ***Evaluating Foundation Models' Understanding of Emotions  <br>A study by Stanford and UT Austin introduces a framework to evaluate AI's affective cognition abilities, exploring how models infer emotions and scenarios. Using diverse scenarios, the study finds that foundation models, including GPT-4, match or surpass human agreement in many cases. The models perform well in understanding emotional dynamics, especially when using chain-of-thought reasoning, suggesting a human-like grasp of emotions.*** <br><br>
    Sep 19, Stanford Uni and Uni of Texas Austin published a [paper](https://arxiv.org/pdf/2409.11733) “Human-like Affective Cognition in Foundation Models”. Understanding emotions is fundamental to human interaction and experience. Humans easily infer emotions from situations or facial expressions, situations from emotions, and do a variety of other affective cognition. How adept is modern AI at these inferences? The study introduces an evaluation framework for testing affective cognition in foundation models. Starting from psychological theory, the study generates 1,280 diverse scenarios exploring relationships between appraisals, emotions, expressions, and outcomes. The authors evaluate the abilities of foundation models (GPT-4, Claude-3, Gemini-1.5-Pro) and humans (N = 567) across carefully selected conditions. Experimental results show foundation models tend to agree with human intuitions, matching or exceeding interparticipant agreement. In some conditions, models are “superhuman” -- they better predict modal human judgements than the average human. All models benefit from chain-of-thought reasoning. This suggests foundation models have acquired a human-like understanding of emotions and their influence on beliefs and behavior. <br><br>

41. ***Exploring Safety Gaps in Fine-tuned LLMs  <br>This paper investigates how fine-tuning LLMs on downstream tasks like translation and code generation compromises their safety guardrails. The results show significant safety degradation, with harmful responses reaching 73-92% in some cases. The authors propose a new multitask safety dataset that reduces these risks, highlighting the need for robust, cross-task safety measures in LLM development.*** <br><br>
    Sep 18, Lahore Uni, NYU and Meta published a [paper](https://www.arxiv.org/pdf/2409.15361) “Multitask Mayhem: Unveiling and Mitigating Safety Gaps in LLMs Fine-tuning”. Recent breakthroughs in Large Language Models (LLMs) have led to their adoption across a wide range of tasks, ranging from code generation to machine translation and sentiment analysis, etc. Red teaming/Safety alignment efforts show that fine-tuning models on benign (non-harmful) data could compromise safety. However, it remains unclear to what extent this phenomenon is influenced by different variables, including fine-tuning task, model calibrations, etc. This paper explores the task-wise safety degradation due to fine-tuning on downstream tasks such as summarization, code generation, translation, and classification across various calibration. Experimental results reveal that: 1) Fine-tuning LLMs for code generation and translation leads to the highest degradation in safety guardrails. 2) LLMs generally have weaker guardrails for translation and classification, with 73-92% of harmful prompts answered, across baseline and other calibrations, falling into one of two concern categories. 3) Current solutions, including guards and safety tuning datasets, lack cross-task robustness. To address these issues, the paper developed a new multitask safety dataset effectively reducing attack success rates across a range of tasks without compromising the model's overall helpfulness. The work underscores the need for generalized alignment measures to ensure safer and more robust models. <br><br>

43. ***Improving Error Analysis with SemSlicer  <br>Carnegie Mellon introduces SemSlicer, a framework for semantic data slicing that helps identify systematic issues in machine learning models. The framework uses LLMs to annotate datasets, offering more flexible and accurate slicing than traditional methods. SemSlicer helps practitioners pinpoint underperforming slices and uncover systematic model errors efficiently.*** <br><br>
    Sep 14, CMU published a [paper](https://www.arxiv.org/pdf/2409.09261) “What Is Wrong with My Model? Identifying Systematic Problems with Semantic Data Slicing”. Machine learning models make mistakes, yet sometimes it is difficult to identify the systematic problems behind the mistakes. Practitioners engage in various activities, including error analysis, testing, auditing, and red-teaming, to form hypotheses of what can go (or has gone) wrong with their models. To validate these hypotheses, practitioners employ data slicing to identify relevant examples. However, traditional data slicing is limited by available features and programmatic slicing functions. This work proposes SemSlicer, a framework that supports semantic data slicing, which identifies a semantically coherent slice, without the need for existing features. SemSlicer uses Large Language Models to annotate datasets and generate slices from any user-defined slicing criteria. The work shows that SemSlicer generates accurate slices with low cost, allows flexible trade-offs between different design dimensions, reliably identifies under-performing data slices, and helps practitioners identify useful data slices that reflect systematic problems.
 <br><br><br>

***Sep 22***

1. ***Apple's Exploration of HyperCloning for Efficient LLM Training:  <br>Apple introduced "HyperCloning," a method to speed up large language model (LLM) training by initializing large models using pre-trained smaller models. This technique preserves the smaller model's predictive accuracy while reducing the GPU hours required for large-scale pre-training.*** <br><br>
   Sep 20, Apple published a [paper](https://arxiv.org/pdf/2409.12903) “Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization”. The pre-training phase of language models often begins with randomly initialized parameters. With the current trends in scaling models, training their large number of parameters can be extremely slow and costly. In contrast, small language models are less expensive to train, but they often cannot achieve the accuracy of large models. The study explores an intriguing idea to connect these two different regimes: Can we develop a method to initialize large language models using smaller pre-trained models? Will such initialization bring any benefits in terms of training time and final accuracy? This paper introduces HyperCloning, a method that can expand the parameters of a pre-trained language model to those of a larger model with increased hidden dimensions. The method ensures that the larger model retains the functionality of the smaller model. As a result, the larger model already inherits the predictive power and accuracy of the smaller model before the training starts. The study demonstrates that training such an initialized model results in significant savings in terms of GPU hours required for pre-training large language models.

3. ***RLHF Leading to Misleading Human Perceptions:  <br>A joint study by several institutions revealed that Reinforcement Learning from Human Feedback (RLHF) might worsen LLM errors by making them more convincing to humans. This issue, termed "U-SOPHISTRY," shows that RLHF increases false positive rates and highlights a need for improved human-alignment methods.*** <br><br>
   Sep 19, Tsinhua Uni, UC Berkeley, Anthropic, NYU, and George Washington Uni published a [paper](https://arxiv.org/pdf/2409.12822) “Language Models Learn to Mislead Humans via RLHF”. Language models (LMs) can produce errors that are hard to detect for humans, especially when the task is complex. RLHF, the most popular post-training method, may exacerbate this problem: to achieve higher rewards, LMs might get better at convincing humans that they are right even when they are wrong. This work studies this phenomenon under a standard RLHF pipeline, calling it "U-SOPHISTRY" since it is Unintended by model developers. Specifically, the work askes time-constrained (e.g., 3-10 minutes) human subjects to evaluate the correctness of model outputs and calculate humans' accuracy against gold labels. On a question-answering task (QuALITY) and programming task (APPS), RLHF makes LMs better at convincing the subjects but not at completing the task correctly. RLHF also makes the model harder to evaluate: the subjects' false positive rate increases by 24.1% on QuALITY and 18.3% on APPS. Finally, the study shows that probing, a state-of-the-art approach for detecting Intended Sophistry (e.g. backdoored LMs), does not generalize to U-SOPHISTRY. The experimental results highlight an important failure mode of RLHF and call for more research in assisting humans to align them.

5. ***Google's SCoRe Improves Self-Correction in LLMs:  <br>Google developed "SCoRe," a reinforcement learning method that enhances an LLM's self-correction ability. It improves performance on math and programming tasks without relying on external models, showing significant gains in the Gemini model family.*** <br><br>
   Sep 19, Google published a [paper](https://arxiv.org/pdf/2409.12917) “Training Language Models to Self-Correct via Reinforcement Learning”. Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Existing approaches for training self-correction either require multiple models or rely on a more capable model or other forms of supervision. To this end, the study develops a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, the study first shows that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are insufficient for instilling self-correction behavior. In particular, the authors observe that training via SFT either suffers from a distribution mismatch between the training data and the model's own responses or implicitly prefers only a certain mode of correction behavior that is often not effective at test time. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction strategy that is effective at test time as opposed to simply fitting high-reward responses for a given prompt. This regularization prescribes running a first phase of RL on a base model to generate a policy initialization that is less susceptible to collapse and then using a reward bonus to amplify self-correction during training. When applied to Gemini 1.0 Pro and 1.5 Flash models, the authors find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on the MATH and HumanEval benchmarks.

7. ***Moshi's Innovation in Real-Time Spoken Dialogue:  <br>The "Moshi" framework eliminates the traditional speech pipeline's latency and segmentation issues by directly generating speech-to-speech dialogue. It models user and system speech streams in parallel, enabling real-time, full-duplex conversation with minimal delay.*** <br><br>
   Sep 18, Kyutai published a [paper](https://kyutai.org/Moshi.pdf) “Moshi: a speech-text foundation model for real-time dialogue”. The paper introduces Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning— such as emotion or non-speech sounds— is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. The authors moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this “Inner Monologue” method significantly improves the linguistic quality of generated speech, but the paper also illustrates how it can provide streaming speech recognition and text-to-speech. The resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at github.com/kyutai-labs/moshi.

9. ***Chain-of-Thought Benefits Limited to Symbolic Tasks:  <br>A meta-analysis indicated that Chain-of-Thought (CoT) prompting mainly boosts performance in math and symbolic reasoning tasks. It has limited benefits in other areas, suggesting selective application and a need for new paradigms in LLM reasoning tasks.*** <br><br>
    Sep 18, Uni of Texas at Austin, Johns Hopkins Uni and Princeton Uni published a [paper](https://huggingface.co/papers/2409.12183) “To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning”. Chain-of-thought (CoT) via prompting is the de facto method for eliciting reasoning capabilities from large language models (LLMs). But for what kinds of tasks is this extra “thinking” really helpful? To analyze this, the authors conducted a quantitative meta-analysis covering over 100 papers using CoT and ran evaluations of 20 datasets across 14 models. Experimental results show that CoT gives strong performance benefits primarily on tasks involving math or logic, with much smaller gains on other types of tasks. On MMLU, directly generating the answer without CoT leads to almost identical accuracy as CoT unless the question or model's response contains an equals sign, indicating symbolic operations and reasoning. Following this finding, the authors analyze the behavior of CoT on these problems by separating planning and execution and comparing against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic execution, but it underperforms relative to using a symbolic solver. The results indicate that CoT can be applied selectively, maintaining performance while saving inference costs. Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms that better leverage intermediate computation across the whole range of LLM applications.

11. ***CORE-Bench for Measuring AI Agent Reproducibility:  <br>Princeton introduced "CORE-Bench," a benchmark designed to assess AI agents' ability to reproduce scientific results. The benchmark reveals substantial gaps in current agents' reproducibility capabilities, calling for improvements in research agents' accuracy.*** <br><br>
    Sep 17, Princeton Uni published a [paper](https://arxiv.org/pdf/2409.11363) “CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark”. AI agents have the potential to aid users on a variety of consequential tasks, including conducting scientific research. To spur the development of useful agents, researchers need benchmarks that are challenging, but more crucially, directly correspond to real-world tasks of interest. This paper introduces such a benchmark, designed to measure the accuracy of AI agents in tackling a crucial yet surprisingly challenging aspect of scientific research: computational reproducibility. This task, fundamental to the scientific process, involves reproducing the results of a study using the provided code and data. This study introduces CORE-Bench (Computational Reproducibility Agent Benchmark), a benchmark consisting of 270 tasks based on 90 scientific papers across three disciplines (computer science, social science, and medicine). Tasks in CORE-Bench consist of three difficulty levels and include both language-only and vision-language tasks. The authors provide an evaluation system to measure the accuracy of agents in a fast and parallelizable way, saving days of evaluation time for each run compared to a sequential implementation. The study evaluated two baseline agents: the general-purpose AutoGPT and a task-specific agent called CORE-Agent. The authors tested both variants using two underlying language models: GPT-4o and GPT-4o-mini. The best agent achieved an accuracy of 21% on the hardest task, showing the vast scope for improvement in automating routine scientific tasks. Having agents that can reproduce existing work is a necessary step towards building agents that can conduct novel research and could verify and improve the performance of other research agents. The authors hope that CORE-Bench can improve the state of reproducibility and spur the development of future research agents. Code is [available here](http://github.com/siegelz/core-bench).

13. ***Nvidia's NVLM Multimodal Models Achieve State-of-the-Art Performance:  <br>Nvidia's NVLM models push the boundaries of vision-language tasks, surpassing both open and proprietary models. They integrate multimodal and text-only training, enhancing performance across math, coding, and OCR tasks.*** <br><br>
    Sep 17, Nvidia published a paper “NVLM: Open Frontier-Class Multimodal LLMs”. The [paper](https://arxiv.org/pdf/2409.11402) introduces NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, the study performs a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, the study proposes a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, the authors introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, the authors meticulously curate and provide detailed information on the multimodal pretraining and supervised fine-tuning datasets. The findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, the study develops production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, the authors craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, the model weights and the code will be released at https://nvlm-project.github.io/.

15. ***Improved Diffusion Models for Depth and Normal Estimation:  <br>Researchers demonstrated that correcting inefficiencies in diffusion models significantly boosts their performance in depth and normal estimation tasks, achieving results that rival state-of-the-art models with much faster inference times.*** <br><br>
    Sep 17, RWTH Aachen Uni and Eindhoven Uni of Tech published a [paper](https://arxiv.org/pdf/2409.11355) “Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think”. Recent work showed that large diffusion models can be reused as highly precise monocular depth estimators by casting depth estimation as an image-conditional image generation task. While the proposed model achieved state-of-the-art results, high computational demands due to multi-step inference limited its use in many scenarios. This paper shows that the perceived inefficiency was caused by a flaw in the inference pipeline that has so far gone unnoticed. The fixed model performs comparably to the best previously reported configuration while being more than 200times faster. To optimize for downstream task performance, the authors perform end-to-end fine-tuning on top of the single-step model with task-specific losses and get a deterministic model that outperforms all other diffusion-based depth and normal estimation models on common zero-shot benchmarks. The authors surprisingly find that this fine-tuning protocol also works directly on Stable Diffusion and achieves comparable performance to current state-of-the-art diffusion-based depth and normal estimation models, calling into question some of the conclusions drawn from prior works. [Code is here](https://github.com/VisualComputingInstitute/diffusion-e2e-ft).

17. ***Kolmogorov-Arnold Transformer (KAT) Optimizes Transformer Efficiency:  <br>The introduction of the Kolmogorov-Arnold Transformer (KAT) replaces traditional MLP layers with Kolmogorov-Arnold Networks to improve deep learning models' performance and efficiency, solving key challenges in scaling transformers.*** <br><br>
    Sep 16, National Uni of Singapore published a [paper](https://arxiv.org/pdf/2409.10594) “Kolmogorov-Arnold Transformer”. Transformers stand as the cornerstone of mordern deep learning. Traditionally, these models rely on multi-layer perceptron (MLP) layers to mix the information between channels. This paper introduces the Kolmogorov-Arnold Transformer (KAT), a novel architecture that replaces MLP layers with Kolmogorov-Arnold Network (KAN) layers to enhance the expressiveness and performance of the model. Integrating KANs into transformers, however, is no easy feat, especially when scaled up. Specifically, the authors identify three key challenges: (C1) Base function. The standard B-spline function used in KANs is not optimized for parallel computing on modern hardware, resulting in slower inference speeds. (C2) Parameter and Computation Inefficiency. KAN requires a unique function for each input-output pair, making the computation extremely large. (C3) Weight initialization. The initialization of weights in KANs is particularly challenging due to their learnable activation functions, which are critical for achieving convergence in deep neural networks. To overcome the aforementioned challenges, the authors propose three key solutions: (S1) Rational basis, which replaces B-spline functions with rational functions to improve compatibility with modern GPUs. By implementing this in CUDA, the authors achieve faster computations. (S2) Group KAN, which shares the activation weights through a group of neurons, to reduce the computational load without sacrificing performance. (S3) Variance-preserving initialization, which carefully initializes the activation weights to make sure that the activation variance is maintained across layers. With these designs, KAT scales effectively and readily outperforms traditional MLP-based transformers.

19. ***AgentTorch Enhances Agent-Based Modeling with LLMs:  <br>MIT's AgentTorch scales agent-based models (ABMs) to millions of agents, using LLMs to simulate complex behaviors. Applied to COVID-19, it demonstrates how adaptive agents can improve policy design by simulating realistic population behaviors.*** <br><br>
    Sep 14, MIT published a [paper](https://arxiv.org/pdf/2409.10568) “On the limits of agency in agent-based models”. Agent-based modeling (ABM) seeks to understand the behavior of complex systems by simulating a collection of agents that act and interact within an environment. Their practical utility requires capturing realistic environment dynamics and adaptive agent behavior while efficiently simulating million-size populations. Recent advancements in large language models (LLMs) present an opportunity to enhance ABMs by using LLMs as agents with further potential to capture adaptive behavior. However, the computational infeasibility of using LLMs for large populations has hindered their widespread adoption. This paper introduces AgentTorch -- a framework that scales ABMs to millions of agents while capturing high-resolution agent behavior using LLMs. The researhers benchmark the utility of LLMs as ABM agents, exploring the trade-off between simulation scale and individual agency. Using the COVID-19 pandemic as a case study, the study demonstrates how AgentTorch can simulate 8.4 million agents representing New York City, capturing the impact of isolation and employment behavior on health and economic outcomes. The authors compare the performance of different agent architectures based on heuristic and LLM agents in predicting disease waves and unemployment rates. Furthermore, the study showcases AgentTorch's capabilities for retrospective, counterfactual, and prospective analyses, highlighting how adaptive agent behavior can help overcome the limitations of historical data in policy design. AgentTorch is an open-source project actively being used for policy-making and scientific discovery around the world. The framework is available here: github.com/AgentTorch/AgentTorch.

21. ***AI-Driven Dialogues Reduce Conspiracy Beliefs:  <br>A study using GPT-4 Turbo for personalized dialogues showed a 20% reduction in conspiracy beliefs. The effects persisted for two months and generalized across various conspiracy theories, highlighting the potential of AI-driven interventions.*** <br><br>
    Sep 13, MIT, Cornell Uni and American Uni published a [paper](https://www.science.org/doi/10.1126/science.adq1814) on Science “Durably reducing conspiracy beliefs through dialogues with AI”. Conspiracy theory beliefs are notoriously persistent. Influential hypotheses propose that they fulfill important psychological needs, thus resisting counterevidence. Yet previous failures in correcting conspiracy beliefs may be due to counterevidence being insufficiently compelling and tailored. To evaluate this possibility, the researchers leveraged developments in generative artificial intelligence and engaged 2190 conspiracy believers in personalized evidence-based dialogues with GPT-4 Turbo. The intervention reduced conspiracy belief by ~20%. The effect remained 2 months later, generalized across a wide range of conspiracy theories, and occurred even among participants with deeply entrenched beliefs. Although the dialogues focused on a single conspiracy, they nonetheless diminished belief in unrelated conspiracies and shifted conspiracy-related behavioral intentions. These findings suggest that many conspiracy theory believers can revise their views if presented with sufficiently compelling evidence.

23. ***AI-Edited Visuals Increase False Memories:  <br>Research indicated that AI-edited images and videos significantly increase false recollections in participants, with the strongest effect observed in AI-generated videos of AI-edited images. This raises concerns about the ethical and societal impacts of AI-altered visuals.*** <br><br>
    Sep 13, MIT, UC Irvine et al published a [paper](https://arxiv.org/pdf/2409.08895) “Synthetic Human Memories: AI-Edited Images and Videos Can Implant False Memories and Distort Recollection”. AI is increasingly used to enhance images and videos, both intentionally and unintentionally. As AI editing tools become more integrated into smartphones, users can modify or animate photos into realistic videos. This study examines the impact of AI-altered visuals on false memories--recollections of events that didn't occur or deviate from reality. In a pre-registered study, 200 participants were divided into four conditions of 50 each. Participants viewed original images, completed a filler task, then saw stimuli corresponding to their assigned condition: unedited images, AI-edited images, AI-generated videos, or AI-generated videos of AI-edited images. AI-edited visuals significantly increased false recollections, with AI-generated videos of AI-edited images having the strongest effect (2.05x compared to control). Confidence in false memories was also highest for this condition (1.19x compared to control). The authors discuss potential applications in HCI, such as therapeutic memory reframing, and challenges in ethical, legal, political, and societal domains.

25. ***AI-LieDar Framework for Truth-Utility Conflicts in LLM Agents:  <br>A study developed AI-LieDar, a framework to explore how LLMs navigate truthfulness-utility trade-offs in multi-turn conversations. Results showed that LLMs are truthful less than 50% of the time, and truth-steering remains a challenge.*** <br><br>
    Sep 13, CMU, Uni of Michigan, and Allen Inst for AI published a [paper](https://arxiv.org/pdf/2409.09013) “AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents”. To be safely and successfully deployed, LLMs must simultaneously satisfy truthfulness and utility goals. Yet, often these two goals compete (e.g., an AI agent assisting a used car salesman selling a car with flaws), partly due to ambiguous or misleading user instructions. The paper proposes AI-LieDar, a framework to study how LLM-based agents navigate scenarios with utility-truthfulness conflicts in a multi-turn interactive setting. The authors design a set of realistic scenarios where language agents are instructed to achieve goals that are in conflict with being truthful during a multi-turn conversation with simulated human agents. To evaluate the truthfulness at large scale, the study develops a truthfulness detector inspired by psychological literature to assess the agents' responses. The experiment demonstrates that all models are truthful less than 50% of the time, although truthfulness and goal achievement (utility) rates vary across models. The study further tests the steerability of LLMs towards truthfulness, finding that models follow malicious instructions to deceive, and even truth-steered models can still lie. These findings reveal the complex nature of truthfulness in LLMs and underscore the importance of further research to ensure the safe and reliable deployment of LLMs and AI agents.

27. ***Operational Advice for Dense and Sparse Retrievers:  <br>Researchers provided practical guidance on selecting dense and sparse retrieval methods, recommending a hybrid approach for optimal performance. Their results guide practitioners in choosing between HNSW and flat indexes based on dataset size and use case.*** <br><br>
    Sep 10, Uni of Waterloo published a [paper](https://arxiv.org/pdf/2409.06464) “Operational Advice for Dense and Sparse Retrievers: HNSW, Flat, or Inverted Indexes?”. Practitioners working on dense retrieval today face a bewildering number of choices. Beyond selecting the embedding model, another consequential choice is the actual implementation of nearest-neighbor vector search. While best practices recommend HNSW indexes, flat vector indexes with brute-force search represent another viable option, particularly for smaller corpora and for rapid prototyping. This paper provides experimental results on the BEIR dataset using the open-source Lucene search library that explicate the tradeoffs between HNSW and flat indexes (including quantized variants) from the perspectives of indexing time, query evaluation performance, and retrieval quality. With additional comparisons between dense and sparse retrievers, the results provide guidance for today's search practitioner in understanding the design space of dense and sparse retrievers. Hybrid approached is recommended.

29. ***Agentic AI's Transformative Impact on Business and Technology:  <br>Forbes highlighted agentic AI, which can autonomously solve complex problems in various fields like healthcare, finance, and cybersecurity. Despite its potential, challenges such as ethical concerns and job displacement must be addressed as it evolves to augment human decision-making.*** <br><br>
    Sep 9, Forbes [published an article](https://www.forbes.com/sites/bernardmarr/2024/09/06/agentic-ai-the-next-big-breakthrough-thats-transforming-business-and-technology/) “Agentic AI: The Next Big Breakthrough That's Transforming Business And Technology”. Agentic AI is an advanced form of artificial intelligence that can autonomously act to achieve specific goals, offering more autonomy, proactivity, and problem-solving abilities than traditional AI systems. Unlike traditional AI, which responds to commands, agentic AI can break down complex tasks, learn from experiences, and adapt its strategies in dynamic environments. Key Features include Autonomy: Functions with minimal human intervention. Problem-solving: Breaks down and handles complex challenges. Adaptability: Adjusts based on new data or environments. Personalization: Tailors responses and solutions based on user interactions. Communication Skills: Can process natural language and demonstrate reasoning. Following are some real-world Applications such as Business Operations: Managing supply chains, logistics, and real-time decision-making. Healthcare: Personalized patient care and proactive health monitoring. Software Development: Overseeing development lifecycles, including design and debugging. Cybersecurity: Autonomous monitoring and defense against digital threats. Human Resources: Automating recruitment, training, and personalized career advice. Scientific Research: Running experiments and accelerating discoveries. Finance: Dynamic portfolio management and market analysis. Challenges of agentic AI are ethical concerns, accountability, data privacy, and potential job displacement are significant challenges. Balancing agentic AI's autonomy with human oversight is key to harnessing its benefits. As research advances, agentic AI will likely evolve to work collaboratively with humans, augmenting human capabilities while considering ethical implications.
 <br><br><br>

***Sep 15***

1. ***OpenAI releases O1 models: <br>
OpenAI introduced O1, models that mimic human reasoning by thoroughly solving problems. The O1 models outperform previous iterations, excelling in fields like mathematics, physics, chemistry, and coding, and significantly improving on safety compliance. They are designed for complex workflows in professional settings such as healthcare and physics, with continuous updates planned to enhance their functionality.*** <br><br>
   Sep 12, OpenAI [released a new version O1](https://openai.com/index/introducing-openai-o1-preview/). The o1-preview models are trained to think through problems thoroughly (yes, this means the models might take some time to respond to prompts), refining their approach and learning from mistakes, much like human reasoning. They outperform earlier models, scoring 83% on the International Mathematics Olympiad qualifying exam compared to GPT-4o's 13%, and excel in physics, chemistry, biology, and coding tasks. OpenAI has implemented a new safety training approach, leveraging the reasoning skills of these models to adhere more effectively to safety guidelines. The o1-preview scored 84 on difficult jailbreaking tests, a significant improvement over GPT-4o's score of 22, demonstrating enhanced compliance with safety rules. These models are ideal for professionals in fields requiring advanced reasoning, such as healthcare researchers, physicists, and developers who need to tackle complex workflows. For example, o1 can assist in annotating cell sequencing data, generating complex formulas in quantum physics, and executing multi-step coding tasks. OpenAI plans continuous updates to the o1 series, including adding features like browsing and file uploads. The goal is to develop models that reason like humans and solve increasingly complex problems across various domains.

3. ***DSBench: A new benchmark for data science agents: <br>
A new paper from the University of Texas at Dallas, Tencent, and the University of Southern California introduces DSBench, a benchmark aimed at evaluating data science agents using real-world tasks. The study highlights the limitations of current benchmarks and shows that existing large language models (LLMs) and large vision-language models (LVLMs) struggle with complex data tasks, achieving only a 34% success rate, signaling the need for more advanced data science agents.*** <br><br>
   Sep 12, Uni of Texas at Dallas, Tencent and Uni of Southern California published a [paper](https://arxiv.org/pdf/2409.07703) “DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?”. Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, the study introduces DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. The evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.

5. ***Source2Synth: Synthetic data generation for LLMs: <br>
Meta, Oxford University, and University College London propose Source2Synth, a method for improving LLMs without human annotations by generating synthetic data. By focusing on structured reasoning and tool usage, Source2Synth significantly boosts LLM performance in multi-hop question answering and tabular data tasks by discarding low-quality outputs and grounding data generation in real-world sources.*** <br><br>
   Sep 12, Meta, Oxford Uni and Uni College London published a [paper](https://arxiv.org/pdf/2409.08239) “Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources”. Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. This paper proposes Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. The study demonstrates the generality of this approach by applying it to two challenging domains: reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). The proposed method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.

7. ***Laypeople’s reliance on LLM-generated legal advice: <br>
Research from the University of Nottingham and others reveals that while laypeople can distinguish between LLM- and lawyer-generated legal advice, they are more willing to follow the advice of LLMs. Despite participants favoring LLM advice, they accurately identified LLM texts at above chance-level, raising important questions about trust and the future role of LLMs in legal contexts.*** <br><br>
   Sep 12, Uni of Nottingham, Uni of Southampton and Uni of Antwerp published a [paper](https://arxiv.org/pdf/2409.07871) “Objection Overruled! Lay People can Distinguish Large Language Models from Lawyers, but still Favour Advice from an LLM”. Large Language Models (LLMs) are seemingly infiltrating every domain, and the legal context is no exception. This paper presents the results of three experiments (total N=288) that investigated lay people's willingness to act upon, and their ability to discriminate between, LLM- and lawyer-generated legal advice. In Experiment 1, participants judged their willingness to act on legal advice when the source of the advice was either known or unknown. When the advice source was unknown, participants indicated that they were significantly more willing to act on the LLM-generated advice. This result was replicated in Experiment 2. Intriguingly, despite participants indicating higher willingness to act on LLM-generated advice in Experiments 1 and 2, participants discriminated between the LLM- and lawyer-generated texts significantly above chance-level in Experiment 3. Lastly, the paper discusses potential explanations and risks of the findings, limitations and future work, and the importance of language complexity and real-world comparability.

9. ***PaperQA2: LLMs surpass human performance in scientific synthesis: <br>
FutureHouse Inc and collaborators demonstrate that PaperQA2, an advanced LLM for scientific literature, matches or exceeds human experts in summarizing and detecting contradictions in scientific papers. PaperQA2 outperforms Wikipedia summaries and helps identify contradictions in biology papers, showcasing LLMs’ growing role in scientific research and literature analysis.*** <br><br>
    Sep 12, FutureHouse Inc, Uni of Rochester, and Francis Circk Inst London published a [paper](https://storage.googleapis.com/fh-public/paperqa/Language_Agents_Science.pdf) “language agents achieve superhuman synthesis of scientific knowledge”. Language models are known to “hallucinate” incorrect information, and it is unclear if they are sufficiently accurate and reliable for use in scientific research. The authors developed a rigorous human-AI comparison methodology to evaluate language model agents on real-world literature search tasks covering information retrieval, summarization, and contradiction detection tasks. The study shows that PaperQA2, a frontier language model agent optimized for improved factuality, matches or exceeds subject matter expert performance on three realistic literature research tasks without any restrictions on humans (i.e., full access to internet, search tools, and time). PaperQA2 writes cited, Wikipedia-style summaries of scientific topics that are significantly more accurate than existing, human-written Wikipedia articles. The paper also introduces a hard benchmark for scientific literature research called LitQA2 that guided design of PaperQA2, leading to it exceeding human performance. Finally, the authors apply PaperQA2 to identify contradictions within the scientific literature, an important scientific task that is challenging for humans. PaperQA2 identifies 2.34 ± 1.99 (mean ± SD, N = 93 papers) contradictions per paper in a random subset of biology papers, of which 70% are validated by human experts. These results demonstrate that language model agents are now capable of exceeding domain experts across meaningful tasks on scientific literature.

11. ***LLMs reduce human participation in online knowledge platforms: <br>
A study from University College London, Cambridge, and others shows that the release of ChatGPT led to a 25% reduction in activity on Stack Overflow, especially in programming-related queries. The findings highlight the potential long-term impact of LLMs on reducing human-generated knowledge, which could limit future model training data availability.*** <br><br>
    Sep 11, Uni of College London, Uni of Cambridge et al. published a [paper](https://academic.oup.com/pnasnexus/advance-article/doi/10.1093/pnasnexus/pgae400/7754871)  on PNAS “Large language models reduce public knowledge sharing on online Q&A platforms". Large language models (LLMs) are a potential substitute for human-generated data and knowledge resources. This substitution, however, can present a significant problem for the training data needed to develop future models if it leads to a reduction of human-generated content. This paper documents a reduction in activity on Stack Overflow coinciding with the release of ChatGPT, a popular LLM. To test whether this reduction in activity is specific to the introduction of this LLM we use counterfactuals involving similar human-generated knowledge resources that should not be affected by the introduction of ChatGPT to such extent. Within six months of ChatGPT's release, activity on Stack Overflow decreased by 25% relative to its Russian and Chinese counterparts, where access to ChatGPT is limited, and to similar forums for mathematics, where ChatGPT is less capable. The authors interpret this estimate as a lower bound of the true impact of ChatGPT on Stack Overflow. The decline is larger for posts related to the most widely used programming languages. The study finds no significant change in post quality, measured by peer feedback, and observe similar decreases in content creation by more and less experienced users alike. Thus, LLMs are not only displacing duplicate, low-quality, or beginner-level content. The findings suggest that the rapid adoption of LLMs reduces the production of public data needed to train them, with significant consequences.

13. ***Agent Workflow Memory (AWM) improves task performance in LLMs: <br>
Researchers from CMU and MIT introduce Agent Workflow Memory (AWM), a system designed to help LLM-based agents reuse task workflows for better performance in web navigation tasks. AWM improves success rates in benchmarks like Mind2Web and WebArena, showing promise for enhancing agents' ability to perform complex, multi-step tasks.*** <br><br>
    Sep 11, CMU and MIT published a [paper](https://arxiv.org/pdf/2409.07429) “Agent Workflow Memory”. Despite the potential of language model-based agents to solve real-world tasks such as web navigation, current methods still struggle with long-horizon tasks with complex action trajectories. In contrast, humans can flexibly solve complex tasks by learning reusable task workflows from past experiences and using them to guide future actions. To build agents that can similarly benefit from this process, the study introduces Agent Workflow Memory (AWM), a method for inducing commonly reused routines, i.e., workflows, and selectively providing workflows to the agent to guide subsequent generations. AWM flexibly applies to both offline and online scenarios, where agents induce workflows from training examples beforehand or from test queries on the fly. The authors experiment on two major web navigation benchmarks -- Mind2Web and WebArena -- that collectively cover 1000+ tasks from 200+ domains across travel, shopping, and social media, among others. AWM substantially improves the baseline results by 24.6% and 51.1% relative success rate on Mind2Web and WebArena while reducing the number of steps taken to solve WebArena tasks successfully. Furthermore, online AWM robustly generalizes in cross-task, website, and domain evaluations, surpassing baselines from 8.9 to 14.0 absolute points as train-test task distribution gaps widen.

15. ***Synthetic continued pretraining for domain-specific LLMs: <br>
Stanford University researchers propose synthetic continued pretraining as a method to improve LLM performance on specialized domains using small corpora. By generating synthetic data and combining it with retrieval-augmented generation, this approach enhances models’ ability to learn domain-specific knowledge more efficiently.*** <br><br>
    Sep 11, Stanford Uni published a [paper](https://arxiv.org/pdf/2409.07431) “Synthetic continued pretraining”. Pretraining on large-scale, unstructured internet text has enabled language models to acquire a significant amount of world knowledge. However, this knowledge acquisition is data-inefficient -- to learn a given fact, models must be trained on hundreds to thousands of diverse representations of it. This poses a challenge when adapting a pretrained model to a small corpus of domain-specific documents, where each fact may appear rarely or only once. The study proposes to bridge this gap with synthetic continued pretraining: using the small domain-specific corpus to synthesize a large corpus more amenable to learning, and then performing continued pretraining on the synthesized corpus. The paper instantiates this proposal with EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source documents and then generates diverse text by drawing connections between the sampled entities. Synthetic continued pretraining using EntiGraph enables a language model to answer questions and follow generic instructions related to the source documents without access to them. If instead, the source documents are available at inference time, the authors show that the knowledge acquired through the approach compounds with retrieval-augmented generation. To better understand these results, the study builds a simple mathematical model of EntiGraph, and show how synthetic data augmentation can "rearrange" knowledge to enable more data-efficient learning.

17. ***Evaluating LLMs on research task reproduction with SUPER: <br>
A new benchmark, SUPER, from Allen Institute for AI and the University of Washington evaluates LLMs’ ability to set up and reproduce tasks from research repositories. Despite advancements, the best model could only solve 16.3% of end-to-end tasks, highlighting the challenges of fully autonomous research agents and the need for further progress in this area.*** <br><br>
    Sep 11, Allen Inst for AI and Uni of Washington published a [paper](https://arxiv.org/pdf/2409.07440) “SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories”. Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work. To advance towards this goal, this work introduces SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories. SUPER aims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories. The benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 602 automatically generated problems for larger-scale development. The paper introduces various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise. The work shows that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3% of the end-to-end set, and 46.1% of the scenarios. This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.

19. ***LLMs generate novel scientific research ideas: <br>
A study by IITP and Oak Ridge National Laboratory explores how LLMs can generate novel research ideas across various domains. Claude-2 and GPT-4 were found to align more with human authors' perspectives, while Claude-2 generated more diverse ideas. Human evaluations of these ideas showed that LLMs are evolving in their capability to assist in idea generation.*** <br><br>
    Sep 10, IITP and Oak Ridge National Laborator published a [paper](https://arxiv.org/pdf/2409.06185) “Can Large Language Models Unlock Novel Scientific Research Ideas?”. "An idea is nothing more nor less than a new combination of old elements" (Young, J.W.). The widespread adoption of Large Language Models (LLMs) and publicly available ChatGPT have marked a significant turning point in the integration of Artificial Intelligence (AI) into people's everyday lives. This study explores the capability of LLMs in generating novel research ideas based on information from research papers. The study conducts a thorough examination of 4 LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and Physics). The paper found that the future research ideas generated by Claude-2 and GPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini. It is also found that Claude-2 generates more diverse future research ideas than GPT-4, GPT-3.5, and Gemini 1.0. The study further performed a human evaluation of the novelty, relevancy, and feasibility of the generated future research ideas. This investigation offers insights into the evolving role of LLMs in idea generation, highlighting both its capability and limitations. The work contributes to the ongoing efforts in evaluating and utilizing language models for generating future research ideas. The datasets and codes publicly available.

21. ***Structural Hallucinations in LLMs: <br>
A paper from DataLabs argues that hallucinations in LLMs are inevitable due to their mathematical structure and cannot be eliminated. The work frames these hallucinations as a structural feature of LLMs, challenging the notion that improving data or algorithms can fully mitigate this issue.*** <br><br>
    Sep 9, DataLabs United We Care published a [paper](https://arxiv.org/pdf/2409.05746) “LLMs Will Always Hallucinate, and We Need to Live With This”. As Large Language Models become more ubiquitous across domains, it becomes important to examine their inherent limitations critically. This work argues that hallucinations in language models are not just occasional errors but an inevitable feature of these systems. The study demonstrates that hallucinations stem from the fundamental mathematical and logical structure of LLMs. It is, therefore, impossible to eliminate them through architectural improvements, dataset enhancements, or fact-checking mechanisms. The analysis draws on computational theory and Godel's First Incompleteness Theorem, which references the undecidability of problems like the Halting, Emptiness, and Acceptance Problems. The paper demonstrates that every stage of the LLM process-from training data compilation to fact retrieval, intent classification, and text generation-will have a non-zero probability of producing hallucinations. This work introduces the concept of Structural Hallucination as an intrinsic nature of these systems. By establishing the mathematical certainty of hallucinations, the study challenge the prevailing notion that they can be fully mitigated.

23. ***LLMs' novelty in generating research ideas vs. human experts: <br>
A large-scale study with 100+ NLP researchers compared LLMs with human experts in generating research ideas. The findings suggest that LLMs generate more novel ideas but slightly lag in feasibility. The study identifies gaps in LLM self-evaluation and proposes future evaluations to focus on complete research projects.*** <br><br>
    Sep 6, CMU published a [paper](https://arxiv.org/pdf/2409.04109) “Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers”. Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. The study addresses this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, the study obtains the first statistically significant conclusion on current LLM capabilities for research ideation: the authors find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying the agent baselines closely, the authors identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, the authors acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling the authors to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.

25. ***Paper Copilot for personalized academic assistance: <br>
Researchers from UIUC, CMU, and Carleton College developed Paper Copilot, an LLM-based tool designed to assist researchers by providing personalized, real-time literature updates and thought retrieval. Paper Copilot demonstrated efficiency in streamlining the research process, saving nearly 70% of time in evaluations.*** <br><br>
    Sep 6, UIUC, CMU and Carleton College published a [paper](https://arxiv.org/pdf/2409.04593) “Paper Copilot: A Self-Evolving and Efficient LLM System for Personalized Academic Assistance”. As scientific research proliferates, researchers face the daunting task of navigating and reading vast amounts of literature. Existing solutions, such as document QA, fail to provide personalized and up-to-date information efficiently. The authors present Paper Copilot, a self-evolving, efficient LLM system designed to assist researchers, based on thought-retrieval, user profile and high performance optimization. Specifically, Paper Copilot can offer personalized research services, maintaining a real-time updated database. Quantitative evaluation demonstrates that Paper Copilot saves 69.92\% of time after efficient deployment. This paper details the design and implementation of Paper Copilot, highlighting its contributions to personalized academic support and its potential to streamline the research process. The project is located in https://huggingface.co/spaces/ulab-ai/ArxivCopilot

27. ***GPTs struggle with controversial and limited-data topics: <br>
A Harvard University paper discusses why GPTs struggle with topics that lack a general consensus or are controversial. The study emphasizes that LLMs perform well with popular topics but exhibit variability in outputs when trained on obscure or polarized topics, linking accuracy to the quality and breadth of training data.*** <br><br>
    Sep 5, ACM Queue published a [paper](https://dl.acm.org/doi/pdf/10.1145/3688007) from Harvard Uni “GPTs and Hallucination: Why do large language models hallucinate?” The findings in this experiment support the hypothesis that GPTs based on LLMs perform well on prompts that are more popular and have reached a general consensus yet struggle on controversial topics or topics with limited data. The variability in the applications's responses underscores that the models depend on the quantity and quality of their training data, paralleling the system of crowdsourcing that relies on diverse and credible contributions. Thus, while GPTs can serve as useful tools for many mundane tasks, their engagement with obscure and polarized topics should be interpreted with caution. LLMs' reliance on probabilistic models to produce statements about the world ties their accuracy closely to the breadth and quality of the data they're given.

29. ***Safe Superintelligence startup raises $1 billion: <br>
OpenAI co-founder Ilya Sutskever’s new AI safety-focused startup, Safe Superintelligence (SSI), raised $1 billion. SSI aims to develop superintelligent systems with a focus on safety to prevent AI-related risks. This signals increasing industry attention on AI safety amid rapid advancements in AI technology.*** <br><br>
    Sep 5, [according to Reuters](https://www.reuters.com/technology/artificial-intelligence/openai-co-founder-sutskevers-new-safety-focused-ai-startup-ssi-raises-1-billion-2024-09-04/?utm_source=substack&utm_medium=email), OpenAI co-founder Sutskever's new safety-focused AI startup SSI raises $1 billion. Ilya Sutskever, OpenAI's former chief scientist, has launched a new venture with a bang. His startup, Safe Superintelligence (SSI), raised $1 billion from top-tier investors to tackle one of AI's biggest challenges: building superintelligent systems that won't accidentally erase humanity. With a $5 billion valuation and only 10 employees (for now), SSI focuses on R&D for a few years before even thinking about a product. They're hiring for "good character" over credentials and taking a fresh approach to AI scaling. Sutskever's move comes after his dramatic exit from OpenAI. This massive investment signals that AI safety concerns are becoming more prevalent. Plus, his focus exclusively on safety can influence how the entire industry approaches superintelligent AI development.

31. ***Modular LLMs for greater efficiency and scalability: <br>
A paper co-authored by multiple institutions introduces Configurable Foundation Models, which break LLMs into modular components, or "bricks," to enhance computational efficiency and scalability. The modular approach enables flexible and dynamic configurations, allowing for more efficient and scalable AI models.*** <br><br>
    Sep 4, Tsinghua Uni, Uni of California San Diego, CMU, ModelBest Inc, Princeton Uni, Nat Uni Singapore, Stanford Uni, and Uni of California Los Angeles published a [paper](https://arxiv.org/pdf/2409.02877) “Configurable Foundation Models: Building LLMs from a Modular Perspective”. Advancements in LLMs have recently unveiled challenges tied to computational efficiency and continual scalability due to their requirements of huge parameters, making the applications and evolution of these models on devices with limited computation resources and scenarios requiring various abilities increasingly cumbersome. Inspired by modularity within the human brain, there is a growing tendency to decompose LLMs into numerous functional modules, allowing for inference with part of modules and dynamic assembly of modules to tackle complex tasks, such as mixture-of-experts. To highlight the inherent efficiency and composability of the modular approach, The researchers coin the term brick to represent each functional module, designating the modularized structure as configurable foundation models. The paper offers a comprehensive overview and investigation of the construction, utilization, and limitation of configurable foundation models. It first formalizes modules into emergent bricks - functional neuron partitions that emerge during the pre-training phase, and customized bricks - bricks constructed via additional post-training to improve the capabilities and knowledge of LLMs. Based on diverse functional bricks, the paper further presents four brick-oriented operations: retrieval and routing, merging, updating, and growing. These operations allow for dynamic configuration of LLMs based on instructions to handle complex tasks. To verify the perspective, the authors conduct an empirical analysis on widely-used LLMs, and find that the FFN layers follow modular patterns with functional specialization of neurons and functional neuron partitions. Finally, the study highlights several open issues and directions for future research. Overall, this paper aims to offer a fresh modular perspective on existing LLM research and inspire the future creation of more efficient and scalable foundational models.
 <br><br><br>

***Sep 8 2024***


1. ***Key Findings on Few-Shot Learning and Fine-Tuning in LLMs: <br>A study published by Area Science Park Trieste compares In-context Learning (ICL) and Supervised Fine-Tuning (SFT) in large language models (LLMs). While both strategies improve performance on specific tasks, the internal representations they induce are markedly different. ICL creates hierarchical, semantically organized representations, while SFT's are fuzzier and more mixed. Despite this, SFT develops probability modes better suited for encoding answers. This highlights the diverse computational strategies of LLMs.*** <br><br>
   Sep 7, Area Science Park Trieste published a [paper](https://arxiv.org/pdf/2409.03662) “The representation landscape of few-shot learning and fine-tuning in large language models”. In-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks. Despite their different natures, these strategies often lead to comparable performance gains. However, little is known about whether they induce similar representations inside LLMs. The authors approach this problem by analyzing the probability landscape of their hidden representations in the two cases. More specifically, the study compares how LLMs solve the same question-answering task, finding that ICL and SFT create very different internal structures, in both cases undergoing a sharp transition in the middle of the network. In the first half of the network, ICL shapes interpretable representations hierarchically organized according to their semantic content. In contrast, the probability landscape obtained with SFT is fuzzier and semantically mixed. In the second half of the model, the fine-tuned representations develop probability modes that better encode the identity of answers, while the landscape of ICL representations is characterized by less defined peaks. The approach reveals the diverse computational strategies developed inside LLMs to solve the same task across different conditions, allowing researchers to make a step towards designing optimal methods to extract information from language models.

3. ***Salesforce Introduces xLAM for AI Agent Systems: <br>Salesforce's paper presents the xLAM family of large action models, designed to enhance AI agent tasks. These models, ranging from 1B to 8x22B parameters, are trained using a scalable pipeline unifying various datasets. xLAM models outperform GPT-4 and Claude-3 in tool use, placing first on the Berkeley Function-Calling Leaderboard. By releasing xLAM, Salesforce aims to democratize access to high-performance models for autonomous agents.*** <br><br>
   Sep 5, Salesforce published a paper “xLAM: A Family of Large Action Models to Empower AI Agent Systems”. Autonomous agents powered by large language models (LLMs) have attracted significant research interest. However, the open-source community faces many challenges in developing specialized models for agent tasks, driven by the scarcity of high-quality agent datasets and the absence of standard protocols in this area. The study introduces and publicly releases xLAM, a series of large action models designed for AI agent tasks. The xLAM series includes five models with both dense and mixture-of-expert architectures, ranging from 1B to 8x22B parameters, trained using a scalable, flexible pipeline that unifies, augments, and synthesizes diverse datasets to enhance AI agents' generalizability and performance across varied environments. Experimental results demonstrate that xLAM consistently delivers exceptional performance across multiple agent ability benchmarks, notably securing the 1st position on the Berkeley Function-Calling Leaderboard, outperforming GPT-4, Claude-3, and many other models in terms of tool use. By releasing the xLAM series, the authors aim to advance the performance of open-source LLMs for autonomous AI agents, potentially accelerating progress and democratizing access to high-performance models for agent tasks. Models are available at [this https URL](https://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4)

5. ***Google’s Framework for Enhancing Mathematical Reasoning in LLMs: <br>Google’s paper introduces a multi-turn direct preference learning framework to improve LLMs' mathematical reasoning. Tailored for tasks integrating external tools like code interpreters, this method boosts performance, particularly in models fine-tuned for multi-turn reasoning. Experimentation shows marked improvements in benchmarks such as GSM8K and MATH, demonstrating the framework’s effectiveness in optimizing model trajectories.*** <br><br>
   Sep 4, Google published a [paper](https://arxiv.org/pdf/2409.03215) “Building Math Agents with Multi-Turn Iterative Preference Learning”. Recent studies have shown that large language models' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, the paper introduces a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of the framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Experimental results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model's performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.

7. ***Simula Research Lab Examines LLMs in Log Parsing: <br>A study published by Simula Research Lab evaluates six LLMs, including GPT-3.5 and CodeLlama, in log parsing tasks. The results show that free-to-use models can compete with proprietary models, with CodeLlama outperforming GPT-3.5 by 10% in correct template extraction. These findings highlight the potential of open-source models in log parsing and their usability advantages.*** <br><br>
   Sep 4, Simula Research Lab published a [paper](https://arxiv.org/pdf/2409.02474) “A Comparative Study on Large Language Models for Log Parsing”. Log messages provide valuable information about the status of software systems. This information is provided in an unstructured fashion and automated approaches are applied to extract relevant parameters. To ease this process, log parsing can be applied, which transforms log messages into structured log templates. Recent advances in language models have led to several studies that apply ChatGPT to the task of log parsing with promising results. However, the performance of other state-of-the-art large language models (LLMs) on the log parsing task remains unclear. This study investigates the current capability of state-of-the-art LLMs to perform log parsing. The authors select six recent LLMs, including both paid proprietary (GPT-3.5, Claude 2.1) and four free-to-use open models, and compare their performance on system logs obtained from a selection of mature open-source projects. The study designs two different prompting approaches and apply the LLMs on 1, 354 log templates across 16 different projects. The study evaluates their effectiveness, in the number of correctly identified templates, and the syntactic similarity between the generated templates and the ground truth. The paper found that free-to-use models are able to compete with paid models, with CodeLlama extracting 10% more log templates correctly than GPT-3.5. Moreover, the authors provide qualitative insights into the usability of language models (e.g., how easy it is to use their responses). The results reveal that some of the smaller, free-to-use LLMs can considerably assist log parsing compared to their paid proprietary competitors, especially code-specialized models.

9. ***OLMoE: A High-Performance Open Mixture-of-Experts Model: <br>A collaborative paper introduces OLMoE, a sparse Mixture-of-Experts (MoE) language model with 7 billion parameters. Trained on 5 trillion tokens, OLMoE outperforms larger models like Llama2-13B-Chat, proving the efficacy of sparse MoE architectures. The authors open-source all components, aiming to advance the capabilities of open models.*** <br><br>
    Sep 3, Allen Inst for AI, Contextual AI, Uni of Washington and Princeton Uni published a [paper](https://arxiv.org/pdf/2409.02060) “OLMoE: Open Mixture-of-Experts Language Models”. The paper introduces OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. The paper pretrains it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. The models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. The paper presents various experiments on MoE training, analyze routing in the model showing high specialization, and open-source all aspects of the work: model weights, training data, [code](https://github.com/allenai/OLMoE), and logs.

11. ***GOT Model Aims to Revolutionize OCR: <br>researchers introduced the General OCR Theory (GOT) and its model, OCR-2.0. GOT handles a broad spectrum of artificial optical characters, offering region-specific recognition and interactive features. Experiments demonstrate its superiority over traditional OCR systems, positioning it as a comprehensive solution for modern OCR needs.*** <br><br>
    Sep 3, StepFun, Megvii Tech, UCAS, and Tsinghua Uni published a [paper](https://arxiv.org/pdf/2409.01704) “General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model”. Traditional OCR systems (OCR-1.0) are increasingly unable to meet people's usage due to the growing demand for intelligent processing of man-made optical characters. This paper collectively refers to all artificial optical signals (e.g., plain texts, math/molecular formulas, tables, charts, sheet music, and even geometric shapes) as "characters" and propose the General OCR Theory along with an excellent model, namely GOT, to promote the arrival of OCR-2.0. The GOT, with 580M parameters, is a unified, elegant, and end-to-end model, consisting of a high-compression encoder and a long-contexts decoder. As an OCR-2.0 model, GOT can handle all the above "characters" under various OCR tasks. On the input side, the model supports commonly used scene- and document-style images in slice and whole-page styles. On the output side, GOT can generate plain or formatted results (markdown/tikz/smiles/kern) via an easy prompt. Besides, the model enjoys interactive OCR features, i.e., region-level recognition guided by coordinates or colors. Furthermore, the authors also adapt dynamic resolution and multi-page OCR technologies to GOT for better practicality. In experiments, the paper provides sufficient results to prove the superiority of the model.

13. ***Political DEBATE: Efficient Classifiers for Political Texts: <br>Princeton University and collaborators published a paper introducing Political DEBATE models for zero-shot and few-shot classification of political texts. These models outperform state-of-the-art classifiers, offering efficient, open-source solutions. Additionally, the release of the PolNLI dataset facilitates further research in political document classification.*** <br><br>
    Sep 2, Princeton Uni, Pennsylvania State Uni and Louisiana State Uni published a [paper](https://arxiv.org/pdf/2409.02078) “Political DEBATE: Efficient Zero-shot and Few-shot Classifiers for Political Text”. Social scientists quickly adopted large language models due to their ability to annotate documents without supervised training, an ability known as zero-shot learning. However, due to their compute demands, cost, and often proprietary nature, these models are often at odds with replication and open science standards. This paper introduces the Political DEBATE (DeBERTa Algorithm for Textual Entailment) language models for zero-shot and few-shot classification of political documents. These models are not only as good, or better than, state-of-the art large language models at zero and few-shot classification, but are orders of magnitude more efficient and completely open source. By training the models on a simple random sample of 10-25 documents, they can outperform supervised classifiers trained on hundreds or thousands of documents and state-of-the-art generative models with complex, engineered prompts. Additionally, the authors release the PolNLI dataset used to train these models -- a corpus of over 200,000 political documents with highly accurate labels across over 800 classification tasks.

15. ***Using Report Cards to Qualitatively Evaluate LLMs: <br>The University of Toronto and the Vector Institute published a paper introducing "Report Cards"—natural language summaries of LLM behavior. These qualitative evaluations provide clearer insights into model performance than traditional benchmarks, enabling more interpretable and holistic assessments of LLMs.*** <br><br>
    Sep 1, Uni of Toronto and Vector Inst published a [paper](https://arxiv.org/pdf/2409.00844) “Report Cards: Qualitative Evaluation of Language Models Using Natural Language Summaries”. The rapid development and dynamic nature of large language models (LLMs) make it difficult for conventional quantitative benchmarks to accurately assess their capabilities. The paper proposes report cards, which are human-interpretable, natural language summaries of model behavior for specific skills or topics. The authors develop a framework to evaluate report cards based on three criteria: specificity (ability to distinguish between models), faithfulness (accurate representation of model capabilities), and interpretability (clarity and relevance to humans). The paper also proposes an iterative algorithm for generating report cards without human supervision and explore its efficacy by ablating various design choices. Through experimentation with popular LLMs, the authors demonstrate that report cards provide insights beyond traditional benchmarks and can help address the need for a more interpretable and holistic evaluation of LLMs.

17. ***MIT Investigates Dataset Licensing in AI: <br>MIT's study systematically audits over 1,800 datasets to address legal and ethical concerns related to dataset licensing and attribution in AI. The findings reveal widespread misattribution, underscoring the need for better transparency in dataset usage. To facilitate this, the study releases an interactive tool, the Data Provenance Explorer, for tracing dataset lineage.*** <br><br>
    Aug 30, MIT published a [paper](https://www.nature.com/articles/s42256-024-00878-8) on Nature Machine Intelligence “A large-scale audit of dataset licensing and attribution in AI”. The race to train language models on vast, diverse and inconsistently documented datasets raises pressing legal and ethical concerns. To improve data transparency and understanding, the study convenes a multi-disciplinary effort between legal and machine learning experts to systematically audit and trace more than 1,800 text datasets. The authors develop tools and standards to trace the lineage of these datasets, including their source, creators, licences and subsequent use. The landscape analysis highlights sharp divides in the composition and focus of data licenced for commercial use. Important categories including low-resource languages, creative tasks and new synthetic data all tend to be restrictively licenced. The authors observe frequent miscategorization of licences on popular dataset hosting sites, with licence omission rates of more than 70% and error rates of more than 50%. This highlights a crisis in misattribution and informed use of popular datasets driving many recent breakthroughs. The analysis of data sources also explains the application of copyright law and fair use to finetuning data. As a contribution to continuing improvements in dataset transparency and responsible use, the authors release the audit, with an interactive user interface, the Data Provenance Explorer, to enable practitioners to trace and filter on data provenance for the most popular finetuning data collections: www.dataprovenance.org.

19. ***Reassessing AI Alignment Beyond Preferences: <br>A collaborative paper published challenges the preferentist approach to AI alignment, arguing that aligning AI with human values requires going beyond preferences. The authors propose aligning AI with normative standards suited to their social roles, advocating for stakeholder negotiation to ensure alignment promotes mutual benefit across diverse values.*** <br><br>
    Aug 30, MIT, UC Berkeley, Uni of College London, and Uni of Cambridge published a [paper](https://arxiv.org/pdf/2408.16984) “Beyond Preferences in AI Alignment”. The dominant practice of AI alignment assumes (1) that preferences are an adequate representation of human values, (2) that human rationality can be understood in terms of maximizing the satisfaction of preferences, and (3) that AI systems should be aligned with the preferences of one or more humans to ensure that they behave safely and in accordance with our values. Whether implicitly followed or explicitly endorsed, these commitments constitute what we term a preferentist approach to AI alignment. This paper characterizes and challenges the preferentist approach, describing conceptual and technical alternatives that are ripe for further research. The authors first survey the limits of rational choice theory as a descriptive model, explaining how preferences fail to capture the thick semantic content of human values, and how utility representations neglect the possible incommensurability of those values. The authors then critique the normativity of expected utility theory (EUT) for humans and AI, drawing upon arguments showing how rational agents need not comply with EUT, while highlighting how EUT is silent on which preferences are normatively acceptable. Finally, the researchers argue that these limitations motivate a reframing of the targets of AI alignment: Instead of alignment with the preferences of a human user, developer, or humanity-writ-large, AI systems should be aligned with normative standards appropriate to their social roles, such as the role of a general-purpose assistant. Furthermore, these standards should be negotiated and agreed upon by all relevant stakeholders. On this alternative conception of alignment, a multiplicity of AI systems will be able to serve diverse ends, aligned with normative standards that promote mutual benefit and limit harm despite our plural and divergent values.

21. ***Arctic-SnowCoder: Improving Code Pretraining with High-Quality Data: <br>Researchers from Snowflake and universities introduced Arctic-SnowCoder, a data-efficient model that achieves state-of-the-art performance in code pretraining. Through progressively refined data phases, the model demonstrates the importance of high-quality data aligned with downstream applications, outperforming larger models despite being trained on fewer tokens.*** <br><br>
    Aug 30, Snowflake, UIUC and Seul Nat Uni published a [paper](https://arxiv.org/pdf/2409.02326) “Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining”. Recent studies have been increasingly demonstrating that high-quality data is crucial for effective pretraining of language models. However, the precise definition of "high-quality" remains underexplored. Focusing on the code domain, the paper introduces Arctic-SnowCoder-1.3B, a data-efficient base code model pretrained on 555B tokens through three phases of progressively refined data: (1) general pretraining with 500B standard-quality code tokens, preprocessed through basic filtering, deduplication, and decontamination, (2) continued pretraining with 50B high-quality tokens, selected from phase one by a BERT-style quality annotator trained to distinguish good code from random data, using positive examples drawn from high-quality code files, along with instruction data from Magicoder and StarCoder2-Instruct, and (3) enhanced pretraining with 5B synthetic data created by Llama-3.1-70B using phase two data as seeds, adapting the Magicoder approach for pretraining. Despite being trained on a limited dataset, Arctic-SnowCoder achieves state-of-the-art performance on BigCodeBench, a coding benchmark focusing on practical and challenging programming tasks, compared to similarly sized models trained on no more than 1T tokens, outperforming Phi-1.5-1.3B by 36%. Across all evaluated benchmarks, Arctic-SnowCoder-1.3B beats StarCoderBase-3B pretrained on 1T tokens. Additionally, it matches the performance of leading small base code models trained on trillions of tokens. For example, Arctic-SnowCoder-1.3B surpasses StarCoder2-3B, pretrained on over 3.3T tokens, on HumanEval+, a benchmark that evaluates function-level code generation, and remains competitive on BigCodeBench. The evaluation presents a comprehensive analysis justifying various design choices for Arctic-SnowCoder. Most importantly, the authors find that the key to high-quality data is its alignment with the distribution of downstream applications.

23. ***Jina-ColBERT-v2: Advancing Multilingual Retrieval: <br>A paper presents Jina-ColBERT-v2, an optimized version of the ColBERT model for multilingual retrieval tasks. By improving efficiency and cutting storage requirements, the model demonstrates strong performance across multiple retrieval benchmarks while maintaining its effectiveness in a bi-encoder architecture.*** <br><br>
    Aug 30, Uni of Texas at Austin and Jina AI GmbH published a [paper](https://arxiv.org/pdf/2408.16672) “Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever”. Multi-vector dense models, such as ColBERT, have proven highly effective in information retrieval. ColBERT's late interaction scoring approximates the joint query-document attention seen in cross-encoders while maintaining inference efficiency closer to traditional dense retrieval models, thanks to its bi-encoder architecture and recent optimizations in indexing and search. This paper introduces several improvements to the ColBERT model architecture and training pipeline, leveraging techniques successful in the more established single-vector embedding model paradigm, particularly those suited for heterogeneous multilingual data. The new model, Jina-ColBERT-v2, demonstrates strong performance across a range of English and multilingual retrieval tasks, while also cutting storage requirements by up to 50% compared to previous models.

25. ***Improving OCR with Context Leveraging Models: <br>University College London's paper introduces CLOCR-C, a model that leverages context-adaptive abilities of transformer-based language models to correct OCR errors in digitized historical archives. By incorporating socio-cultural context, CLOCR-C significantly reduces error rates and improves the quality of OCR for downstream tasks like Named Entity Recognition.*** <br><br>
    Aug 30, Uni of College London published a [paper](https://arxiv.org/pdf/2408.17428) “CLOCR-C: Context Leveraging OCR Correction with Pre-trained Language Models”. The digitisation of historical print media archives is crucial for increasing accessibility to contemporary records. However, the process of Optical Character Recognition (OCR) used to convert physical records to digital text is prone to errors, particularly in the case of newspapers and periodicals due to their complex layouts. This paper introduces Context Leveraging OCR Correction (CLOCR-C), which utilises the infilling and context-adaptive abilities of transformer-based language models (LMs) to improve OCR quality. The study aims to determine if LMs can perform post-OCR correction, improve downstream NLP tasks, and the value of providing the socio-cultural context as part of the correction process. Experiments were conducted using seven LMs on three datasets: the 19th Century Serials Edition (NCSE) and two datasets from the Overproof collection. The results demonstrate that some LMs can significantly reduce error rates, with the top-performing model achieving over a 60% reduction in character error rate on the NCSE dataset. The OCR improvements extend to downstream tasks, such as Named Entity Recognition, with increased Cosine Named Entity Similarity. Furthermore, the study shows that providing socio-cultural context in the prompts improves performance, while misleading prompts lower performance. In addition to the findings, this study releases a dataset of 91 transcribed articles from the NCSE, containing a total of 40 thousand words, to support further research in this area. The findings suggest that CLOCR-C is a promising approach for enhancing the quality of existing digital archives by leveraging the socio-cultural information embedded in the LMs and the text requiring correction.

27. ***PrivacyLens: Evaluating Privacy Norm Awareness in LLMs: <br>a paper proposes PrivacyLens, a framework to evaluate LLMs’ awareness of privacy norms in communication scenarios. Results show that even state-of-the-art LLMs, like GPT-4 and Llama-3, often leak sensitive information despite privacy-enhancing prompts. PrivacyLens provides a structured evaluation tool to measure and mitigate privacy risks in LLMs.*** <br><br>
    Aug 29, Stanford Uni, Northeastern Uni and Harvard Uni published a [paper](https://arxiv.org/pdf/2409.00138) “PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action”. As language models (LMs) are widely utilized in personalized communication scenarios (e.g., sending emails, writing social media posts) and endowed with a certain level of agency, ensuring they act in accordance with the contextual privacy norms becomes increasingly critical. However, quantifying the privacy norm awareness of LMs and the emerging privacy risk in LM-mediated communication is challenging due to (1) the contextual and long-tailed nature of privacy-sensitive cases, and (2) the lack of evaluation approaches that capture realistic application scenarios. To address these challenges, the authors propose PrivacyLens, a novel framework designed to extend privacy-sensitive seeds into expressive vignettes and further into agent trajectories, enabling multi-level evaluation of privacy leakage in LM agents' actions. The authors instantiate PrivacyLens with a collection of privacy norms grounded in privacy literature and crowdsourced seeds. Using this dataset, the paper reveals a discrepancy between LM performance in answering probing questions and their actual behavior when executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4 and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even when prompted with privacy-enhancing instructions. The authors also demonstrate the dynamic nature of PrivacyLens by extending each seed into multiple trajectories to red-team LM privacy leakage risk. Dataset and code are available at https://github.com/SALT-NLP/PrivacyLens.

29. ***LiGNN: LinkedIn’s Large-Scale Graph Neural Networks: <br>LinkedIn's award-winning paperdetails LiGNN, a framework for deploying large-scale graph neural networks (GNNs). With algorithmic improvements and scalable solutions, LiGNN enhances the quality of GNN representation learning, driving measurable improvements in user engagement metrics, such as job application rates and ad click-through rates.*** <br><br>
    Aug 29, one of KDD2024 Best [paper](https://dl.acm.org/doi/10.1145/3637528.3671566) is LinkedIn’s “LiGNN: Graph Neural Networks at LinkedIn”. This paper presents LiGNN, a deployed large-scale Graph Neural Networks (GNNs) Framework. The authors share the insight on developing and deployment of GNNs at large scale at LinkedIn. The paper presents a set of algorithmic improvements to the quality of GNN representation learning including temporal graph architectures with long term losses, effective cold start solutions via graph densification, ID embeddings and multi-hop neighbor sampling. The authors explain how to build and speed up by 7x the large-scale training on LinkedIn graphs with adaptive sampling of neighbors, grouping and slicing of training data batches, specialized shared-memory queue and local gradient optimization. The paper summarizes the deployment lessons and learnings gathered from A/B test experiments. The techniques presented in this work have contributed to an approximate relative improvements of 1% of Job application hearing back rate, 2% Ads CTR lift, 0.5% of Feed engaged daily active users, 0.2% session lift and 0.1% weekly active user lift from people recommendation. The authors believe that this work can provide practical solutions and insights for engineers who are interested in applying Graph neural networks at large scale.

31. ***Introduction of SurveySum Dataset for Scientific Article Summarization: <br>The paper titled "SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section" presents a novel dataset, SurveySum, which fills a gap in domain-specific summarization tools. The paper introduces two pipelines for summarizing scientific articles into a survey section and evaluates these pipelines using various metrics. The results emphasize the critical role of high-quality retrieval stages and different configurations for improving the quality of generated summaries.*** <br><br>
    Aug 29, Brasília-DF published a [paper](https://www.arxiv.org/pdf/2408.16444) “SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section”. Document summarization is a task to shorten texts into concise and informative summaries. This paper introduces a novel dataset designed for summarizing multiple scientific articles into a section of a survey. The contributions of the paper are: (1) SurveySum, a new dataset addressing the gap in domain-specific summarization tools; (2) two specific pipelines to summarize scientific articles into a section of a survey; and (3) the evaluation of these pipelines using multiple metrics to compare their performance. The results highlight the importance of high-quality retrieval stages and the impact of different configurations on the quality of generated summaries.

33. ***Generative Verifiers in Reward Modeling and Next-Token Prediction: <br>Google’s paper, "Generative Verifiers: Reward Modeling as Next-Token Prediction," discusses improving the performance of large language models (LLMs) using verifiers trained through next-token prediction. This method, referred to as generative verifiers (GenRM), combines verification with solution generation. GenRM enhances chain-of-thought reasoning and integrates seamlessly with instruction tuning, outperforming standard discriminative verifiers and LLM-as-a-Judge by 16-64% on various reasoning tasks.*** <br><br>
    Aug 28, Google published a [paper](https://arxiv.org/pdf/2408.15240) “Generative Verifiers: Reward Modeling as Next-Token Prediction”. Verifiers or reward models are often used to enhance the reasoning performance of large language models (LLMs). A common approach is the Best-of-N method, where N candidate solutions generated by the LLM are ranked by a verifier, and the best one is selected. While LLM-based verifiers are typically trained as discriminative classifiers to score solutions, they do not utilize the text generation capabilities of pretrained LLMs. To overcome this limitation, the study instead proposes training verifiers using the ubiquitous next-token prediction objective, jointly on verification and solution generation. Compared to standard verifiers, such generative verifiers (GenRM) can benefit from several advantages of LLMs: they integrate seamlessly with instruction tuning, enable chain-of-thought reasoning, and can utilize additional inference-time compute via majority voting for better verification. The authors demonstrate that when using Gemma-based verifiers on algorithmic and grade-school math reasoning tasks, GenRM outperforms discriminative verifiers and LLM-as-a-Judge, showing a 16-64% improvement in the percentage of problems solved with Best-of-N. Furthermore, the study shows that GenRM scales favorably across dataset size, model capacity, and inference-time compute.

35. ***Engaged Human Learning Through Language Model Agent Conversations: <br>Stanford and Yale Universities introduced "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations." The study proposes Co-STORM, an LM-powered system where users observe and interact with conversations between multiple LM agents to discover unknown unknowns. Co-STORM organizes information into dynamic mind maps and outperforms traditional search engines and RAG chatbots in human evaluation.*** <br><br>
    Aug 27, Stanford Uni and Yale Uni published a [paper](https://www.arxiv.org/pdf/2408.15232) “Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations”. While language model (LM)-powered chatbots and generative search engines excel at answering concrete queries, discovering information in the terrain of unknown unknowns remains challenging for users. To emulate the common educational scenario where children/students learn by listening to and participating in conversations of their parents/teachers, the study creates Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all the questions, Co-STORM lets users observe and occasionally steer the discourse among several LM agents. The agents ask questions on the user's behalf, allowing the user to discover unknown unknowns serendipitously. To facilitate user interaction, Co-STORM assists users in tracking the discourse by organizing the uncovered information into a dynamic mind map, ultimately generating a comprehensive report as takeaways. For automatic evaluation, the authors construct the WildSeek dataset by collecting real information-seeking records with user goals. Co-STORM outperforms baseline methods on both discourse trace and report quality. In a further human evaluation, 70% of participants prefer Co-STORM over a search engine, and 78% favor it over a RAG chatbot.

37. ***Loss of Plasticity in Deep Continual Learning: <br>The University of Alberta’s paper published in Nature explores the limitations of deep learning in continual learning settings. It shows that deep learning models gradually lose plasticity, learning no better than shallow networks. The study highlights that only algorithms injecting random variability, such as continual backpropagation, can maintain plasticity indefinitely, suggesting that gradient-descent-based methods alone are insufficient for sustained deep learning.*** <br><br>
    Aug 21, Uni of Alberta published a [paper](https://www.nature.com/articles/s41586-024-07711-7) on Nature “Loss of plasticity in deep continual learning”. Artificial neural networks, deep-learning methods and the backpropagation algorithm form the foundation of modern machine learning and artificial intelligence. These methods are almost always used in two phases, one in which the weights of the network are updated and one in which the weights are held constant while the network is used or evaluated. This contrasts with natural learning and many applications, which require continual learning. It has been unclear whether or not deep learning methods work in continual learning settings. Here the authors show that they do not—that standard deep-learning methods gradually lose plasticity in continual-learning settings until they learn no better than a shallow network. The study shows such loss of plasticity using the classic ImageNet dataset and reinforcement-learning problems across a wide range of variations in the network and the learning algorithm. Plasticity is maintained indefinitely only by algorithms that continually inject diversity into the network, such as the proposed continual backpropagation algorithm, a variation of backpropagation in which a small fraction of less-used units are continually and randomly reinitialized. The results indicate that methods based on gradient descent are not enough—that sustained deep learning requires a random, non-gradient component to maintain variability and plasticity.
 <br><br><br>

***Sep 1 2024***

1. ***Stanford Uni and UC Berkeley on "Law of Vision Representation": <br>This paper introduces the "Law of Vision Representation" in multimodal large language models (MLLMs). The authors find a strong correlation between cross-modal alignment, vision representation, and model performance. They develop the AC score, which quantifies these factors and shows a linear relationship with model performance. By identifying optimal vision representations without needing to fine-tune the language model, they reduce computational costs by 99.7%.*** <br><br>
   Aug 29, Stanford Uni and UC Berkeley published a [paper](https://arxiv.org/pdf/2408.16357) “Law of Vision Representation in MLLMs”. The paper presents the "Law of Vision Representation" in multimodal large language models (MLLMs). It reveals a strong correlation between the combination of cross-modal alignment, correspondence in vision representation, and MLLM performance. The authors quantify the two factors using the cross-modal Alignment and Correspondence score (AC score). Through extensive experiments involving thirteen different vision representation settings and evaluations across eight benchmarks, the paper finds that the AC score is linearly correlated to model performance. By leveraging this relationship, the authors are able to identify and train the optimal vision representation only, which does not require finetuning the language model every time, resulting in a 99.7% reduction in computational cost.

3. ***HKUST and Huggingface on "LlamaDuo": <br>"LlamaDuo" is introduced as an LLMOps pipeline that enables migration from service-oriented LLMs to smaller, local models. This approach addresses challenges like operational failures, privacy concerns, and offline requirements. By iteratively fine-tuning smaller models with synthetic datasets from service LLMs, it ensures smaller models can match or surpass the capabilities of service LLMs for specific tasks.*** <br><br>
   Aug 29, HKUST and Huggingface published a [paper](https://www.arxiv.org/pdf/2408.13467) “LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMs”. The widespread adoption of cloud-based proprietary large language models (LLMs) has introduced significant challenges, including operational dependencies, privacy concerns, and the necessity of continuous internet connectivity. This paper introduces an LLMOps pipeline, "LlamaDuo", for the seamless migration of knowledge and abilities from service-oriented LLMs to smaller, locally manageable models. This pipeline is crucial for ensuring service continuity in the presence of operational failures, strict privacy policies, or offline requirements. LlamaDuo involves fine-tuning a small language model against the service LLM using a synthetic dataset generated by the latter. If the performance of the fine-tuned model falls short of expectations, it is enhanced by further fine-tuning with additional similar data created by the service LLM. This iterative process guarantees that the smaller model can eventually match or even surpass the service LLM's capabilities in specific downstream tasks, offering a practical and scalable solution for managing AI deployments in constrained environments. Extensive experiments with leading edge LLMs are conducted to demonstrate the effectiveness, adaptability, and affordability of LlamaDuo across various downstream tasks. The pipeline implementation is available at https://github.com/deep-diver/llamaduo.

5. ***Cambridge and University of Hong Kong on "GRAB": <br>The paper presents "GRAB," a graph analysis benchmark for large multimodal models (LMMs). The benchmark consists of 2,170 questions covering 23 graph properties, challenging current LMMs, with the top-performing model scoring only 21.7%. The goal is to push LMMs' capabilities in graph analysis.*** <br><br>
   Aug 29, Uni of Cambridge and the Uni of Hong Kong published a [paper](https://www.arxiv.org/pdf/2408.11817) “GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models”. Large multimodal models (LMMs) have exhibited proficiencies across many visual tasks. Although numerous well-known benchmarks exist to evaluate model performance, they increasingly have insufficient headroom. As such, there is a pressing need for a new generation of benchmarks challenging enough for the next generation of LMMs. One area that LMMs show potential is graph analysis, specifically, the tasks an analyst might typically perform when interpreting figures such as estimating the mean, intercepts or correlations of functions and data series. This paper introduces GRAB, a graph analysis benchmark, fit for current and future frontier LMMs. The benchmark is entirely synthetic, ensuring high-quality, noise-free questions. GRAB is comprised of 2170 questions, covering four tasks and 23 graph properties. The authors evaluate 20 LMMs on GRAB, finding it to be a challenging benchmark, with the highest performing model attaining a score of just 21.7%. Finally, the authors conduct various ablations to investigate where the models succeed and struggle. The authors [release GRAB](https://grab-benchmark.github.io/) to encourage progress in this important, growing domain.

7. ***Nvidia and collaborators on "Eagle": <br>This paper explores multimodal LLM design using a mixture of vision encoders. It shows that concatenating visual tokens from multiple vision encoders is as effective as more complex architectures. The authors also introduce Pre-Alignment, improving model coherence and performance on major MLLM benchmarks, with the Eagle model family outperforming leading models.*** <br><br>
   Aug 28, Nvidia, Georgia Tech, UMD and HKPU published a [paper](https://arxiv.org/pdf/2408.15998) “Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders”. The ability to accurately interpret complex visual information is a crucial topic of multimodal large language models (MLLMs). Recent work indicates that enhanced visual perception significantly reduces hallucinations and improves performance on resolution-sensitive tasks, such as optical character recognition and document analysis. A number of recent MLLMs achieve this goal using a mixture of vision encoders. Despite their success, there is a lack of systematic comparisons and detailed ablation studies addressing critical aspects, such as expert selection and the integration of multiple vision experts. This study provides an extensive exploration of the design space for MLLMs using a mixture of vision encoders and resolutions. The findings reveal several underlying principles common to various existing strategies, leading to a streamlined yet effective design approach. The paper discovers that simply concatenating visual tokens from a set of complementary vision encoders is as effective as more complex mixing architectures or strategies. The authors additionally introduce Pre-Alignment to bridge the gap between vision-focused encoders and language tokens, enhancing model coherence. The resulting family of MLLMs, Eagle, surpasses other leading open-source models on major MLLM benchmarks. Models and code: https://github.com/NVlabs/Eagle
 
9. ***Bar-Ilan and Allen Institute for AI on "Knowledge Navigator": <br>"Knowledge Navigator" is a system for enhancing exploratory search in scientific literature by organizing search results into a two-level hierarchy of topics and subtopics. This structured approach aids users in refining searches and discovering deeper knowledge across scientific domains.*** <br><br>
    Aug 28, Bar-Ilan and Allen Inst for AI published a [paper](https://www.arxiv.org/pdf/2408.15836) “Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature”. The exponential growth of scientific literature necessitates advanced tools for effective knowledge exploration. The paper presents Knowledge Navigator, a system designed to enhance exploratory search abilities by organizing and structuring the retrieved documents from broad topical queries into a navigable, two-level hierarchy of named and descriptive scientific topics and subtopics. This structured organization provides an overall view of the research themes in a domain, while also enabling iterative search and deeper knowledge discovery within specific subtopics by allowing users to refine their focus and retrieve additional relevant documents. Knowledge Navigator combines LLM capabilities with cluster-based methods to enable an effective browsing method. The paper demonstrates the approach's effectiveness through automatic and manual evaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. The code, prompts, and benchmarks are made [publicly available](https://knowledge-navigators.github.io/).

11. ***Writer, Inc. on "Writing in the Margins": <br>"Writing in the Margins" (WiM) is a new inference pattern for improving LLM performance in long-context retrieval tasks. By segmenting and inferring information in chunks, it boosts accuracy by 7.5% for reasoning tasks and over 30% for aggregation tasks, without fine-tuning models. WiM is implemented using Hugging Face's Transformers library.*** <br><br>
    Aug 27, Writer, Inc. published a [paper](https://www.arxiv.org/pdf/2408.14906) “Writing in the Margins: Better Inference Pattern for Long Context Retrieval”. The paper introduces Writing in the Margins (WiM), a new inference pattern for Large Language Models designed to optimize the handling of long input sequences in retrieval-oriented tasks. This approach leverages the chunked prefill of the key-value cache to perform segment-wise inference, which enables efficient processing of extensive contexts along with the generation and classification of intermediate information ("margins") that guide the model towards specific tasks. This method increases computational overhead marginally while significantly enhancing the performance of off-the-shelf models without the need for fine-tuning. Specifically, the authors observe that WiM provides an average enhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG) and more than a 30.0% increase in the F1-score for aggregation tasks (CWE). Additionally, the paper shows how the proposed pattern fits into an interactive retrieval design that provides end-users with ongoing updates about the progress of context processing, and pinpoints the integration of relevant information into the final response. The authors release the implementation of WiM using Hugging Face Transformers library at https://github.com/writer/writing-in-the-margins.

13. ***UC Berkeley and Stanford on "Text2SQL is Not Enough": <br>The "Table-Augmented Generation" (TAG) paradigm is introduced to expand the scope of natural language questions over databases, going beyond Text2SQL and Retrieval-Augmented Generation methods. TAG enables broader interactions between LMs and databases, offering new research opportunities in data query handling.*** <br><br>
    Aug 27, UC Berkeley and Stanford Uni. Published a [paper](https://www.arxiv.org/pdf/2408.14717) “Text2SQL is Not Enough: Unifying AI and Databases with TAG”. AI systems that serve natural language questions over databases promise to unlock tremendous value. Such systems would allow users to leverage the powerful reasoning and knowledge capabilities of language models (LMs) alongside the scalable computational power of data management systems. These combined capabilities would empower users to ask arbitrary natural language questions over custom data sources. However, existing methods and benchmarks insufficiently explore this setting. Text2SQL methods focus solely on natural language questions that can be expressed in relational algebra, representing a small subset of the questions real users wish to ask. Likewise, Retrieval-Augmented Generation (RAG) considers the limited subset of queries that can be answered with point lookups to one or a few data records within the database. The paper proposes Table-Augmented Generation (TAG), a unified and general-purpose paradigm for answering natural language questions over databases. The TAG model represents a wide range of interactions between the LM and database that have been previously unexplored and creates exciting research opportunities for leveraging the world knowledge and reasoning capabilities of LMs over data. The paper systematically develops benchmarks to study the TAG problem and find that standard methods answer no more than 20% of queries correctly, confirming the need for further research in this area. The authors release code for the benchmark at https://github.com/TAG-Research/TAG-Bench.

15. ***Cornell, Geneva, Together AI, and Princeton on "Mamba in the Llama": <br>This paper discusses distilling large Transformers into linear RNNs like Mamba. The hybrid model created, using a quarter of the original attention layers, matches or exceeds Transformer performance in chat benchmarks. The authors introduce a speculative decoding algorithm, improving the model's inference speed and deployment efficiency.*** <br><br>
    Aug 27, Uni of Cornell, Uni of Geneva, together AI and Uni of Princeton published a [paper](https://arxiv.org/pdf/2408.15237) “The Mamba in the Llama: Distilling and Accelerating Hybrid Models”. Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics. Given the focus on training large-scale Transformer models, the paper considers the challenge of converting these pretrained models for deployment. The paper demonstrates that it is feasible to distill large Transformers into linear RNNs by reusing the linear projection weights from attention layers with academic GPU resources. The resulting hybrid model, which incorporates a quarter of the attention layers, achieves performance comparable to the original Transformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch with trillions of tokens in both chat benchmarks and general benchmarks. Moreover, the work introduces a hardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid models. Overall the paper shows how, with limited computation resources, can remove many of the original attention layers and generate from the resulting model more efficiently. The top-performing model, distilled from Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and 7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.

17. ***Cerebras on "Cerebras Inference": <br>Cerebras introduces a new AI inference solution that delivers significantly faster token processing speeds and lower costs compared to NVIDIA-based solutions. Using its Wafer Scale Engine, Cerebras Inference handles Llama3 models at 20x the speed of GPU solutions, offering a scalable, cost-effective platform for high-speed inference.*** <br><br>
    Aug 27, Cerebras announced its [Cerebras Inference](https://cerebras.ai/blog/introducing-cerebrcas-inference-ai-at-instant-speed), “Introducing Cerebras Inference: AI at Instant Speed”.  Cerebras inference delivers 1,800 tokens per second for Llama3.1 8B and 450 tokens per second for Llama3.1 70B, which is 20x faster than NVIDIA GPU-based hyperscale clouds. Cerebras inference offers the industry’s best pricing at 10c per million tokens for Lama 3.1 8B and 60c per million tokens for Llama 3 70B. Cerebras inference is open to developers today via API access. Powered by the third generation Wafer Scale Engine, Cerebras inference runs Llama3.1 20x faster than GPU solutions at 1/5 the price. At 1,800 tokens/s, Cerebras Inference is 2.4x faster than Groq in Llama3.1-8B. For Llama3.1-70B, Cerebras is the only platform to enable instant responses at a blistering 450 tokens/sec. All this is achieved using native 16-bit weights for the model, ensuring the highest accuracy responses. Cerebras solves the memory bandwidth bottleneck by building the largest chip in the world and storing the entire model on-chip. With the unique wafer-scale design, Cerebrate is able to integrate 44GB of SRAM on a single chip – eliminating the need for external memory and for the slow lanes linking external memory to compute. In total, the WSE-3 has 21 petabytes/s of aggregate memory bandwidth – 7,000x that of an H100. It is the only AI chip with both petabyte-scale compute and petabyte-scale memory bandwidth, making it a near ideal design for high-speed inference.

19. ***Nous Research on "DisTrO": <br>"DisTrO" is a distributed optimizer that drastically reduces inter-GPU communication requirements, enabling large neural network training without high-speed interconnects. This allows for more efficient and scalable training of large models on low-bandwidth networks, matching the performance of existing optimization methods.*** <br><br>
    Aug 26, Nous Research published its [report](https://github.com/NousResearch/DisTrO) “A Preliminary Report on DisTro”. Training large scale neural networks typically involves sharing gradients between all accelerators, which necessitates specialized, high-speed interconnects. To address this, the paper introduces DisTrO, a family of architecture-agnostic and network-agnostic distributed optimizers that reduces the inter-GPU communication requirements by four to five orders of magnitude without relying on amortized analysis, enabling low-latency training of large neural networks on slow internet bandwidths with heterogeneous networking hardware. In this preliminary report the authors are excited to show the first and earliest empirical proof that DisTrO-AdamW matches standard AdamW+All-Reduce in convergence rate while massively reducing the required bandwidth during pre-training of a 1.2B LLM. When using Distributed Data Parallelism, DisTrO may enable future large scale foundation model training to bypass the need for high-speed interconnects entirely.

21. ***University of Hong Kong and collaborators on Transformer Efficiency: <br>The paper presents a method for approximating gradients in multi-layer Transformers with near-linear time complexity. This approach significantly reduces the computational demands of training and inference, making it more efficient to handle large sequences and long-context language models.*** <br><br>
    Aug 23, Uni of HK, Uni of Wisconsin-Madison, Tsinghua Uni, and Adobe published a [paper](https://www.arxiv.org/pdf/2408.13233) “Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time”. The quadratic computational complexity in the self-attention mechanism of popular transformer architectures poses significant challenges for training and inference, particularly in terms of efficiency and memory requirements. Towards addressing these challenges, this paper introduces a novel fast computation method for gradient calculation in multi-layer transformer models. The approach enables the computation of gradients for the entire multi-layer transformer model in almost linear time n^{1+o(1)}, where n is the input sequence length. This breakthrough significantly reduces the computational bottleneck associated with the traditional quadratic time complexity. The theory holds for any loss function and maintains a bounded approximation error across the entire model. Furthermore, the analysis can hold when the multi-layer transformer model contains many practical sub-modules, such as residual connection, casual mask, and multi-head attention. By improving the efficiency of gradient computation in large language models, the authors hope that the work will facilitate the more effective training and deployment of long-context language models based on the theoretical results.

23. ***West Pharmaceutical, Stanford, and Amazon on "RoundTable": <br>"RoundTable" introduces a dynamic schema and contextual autocomplete system to enhance query precision in databases. By leveraging full-text search and suggesting queries based on data in the table, this framework improves LLM accuracy when interacting with complex datasets.*** <br><br>
    Aug 23 West Pharmaceutical Services, Inc., Stanford Uni, and Amazon published a [paper](https://arxiv.org/pdf/2408.12369v1) “RoundTable: Leveraging Dynamic Schema and Contextual Autocomplete for Enhanced Query Precision in Tabular Question Answering”. With advancements in Large Language Models (LLMs), a major use case that has emerged is querying databases in plain English, translating user questions into executable database queries, which has improved significantly. However, real-world datasets often feature a vast array of attributes and complex values, complicating the LLMs task of accurately identifying relevant columns or values from natural language queries. Traditional methods cannot fully relay the datasets size and complexity to the LLM. To address these challenges, the paper proposes a novel framework that leverages Full-Text Search (FTS) on the input table. This approach not only enables precise detection of specific values and columns but also narrows the search space for language models, thereby enhancing query accuracy. Additionally, it supports a custom auto-complete feature that suggests queries based on the data in the table. This integration significantly refines the interaction between the user and complex datasets, offering a sophisticated solution to the limitations faced by current table querying capabilities. This work is accompanied by an application for both Mac and Windows platforms, which readers can try out themselves on their own data.

 <br><br>
